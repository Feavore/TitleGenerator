URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10431v1,Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model for WECC Composite Load Model in Power Systems,"Data-driven modeling for dynamic systems has gained widespread attention in recent years. Its inverse formulation, parameter estimation, aims to infer the inherent model parameters from observations. However, parameter degeneracy, where different combinations of parameters yield the same observable output, poses a critical barrier to accurately and uniquely identifying model parameters. In the context of WECC composite load model (CLM) in power systems, utility practitioners have observed that CLM parameters carefully selected for one fault event may not perform satisfactorily in another fault. Here, we innovate a joint conditional diffusion model-based inverse problem solver (JCDI), that incorporates a joint conditioning architecture with simultaneous inputs of multi-event observations to improve parameter generalizability. Simulation studies on the WECC CLM show that the proposed JCDI effectively reduces uncertainties of degenerate parameters, thus the parameter estimation error is decreased by 42.1% compared to a single-event learning scheme. This enables the model to achieve high accuracy in predicting power trajectories under different fault events, including electronic load tripping and motor stalling, outperforming standard deep reinforcement learning and supervised learning approaches. We anticipate this work will contribute to mitigating parameter degeneracy in system dynamics, providing a general parameter estimation framework across various scientific domains.","Dynamic system modeling is a fundamental study across different scientific fields. Data-driven machine learning provides a new paradigm to model the system dynamics due to its potential of implementing more accurate and efficient simulations (Wang et al., 2023). This encompasses two fundamental forms: forward surrogation and inverse modeling. Forward surrogation predicts the system’s evolution from initial states, while inverse modeling deduces the model’s inherent properties from observation data (Kadeethum et al., 2021). Inverse modeling, which includes techniques such as parameter identification, plays a crucial role in understanding and emulating system dynamics. However, the inverse problem of system dynamics is complex and challenging due to parameter degeneracy and unidentifiability, where non-unique solutions exist that produce identical observation outputs (Lederman et al., 2021). In power systems, load modeling uses several types of models to represent the aggregation behavior of various end-user load devices in the distribution system (Kim et al., 2023). As the dynamic performance of end-user loads becomes increasingly complex with technological advances (NER, December 2016), the Western Electricity Coordinating Council (WECC) has developed the state-of-the-art composite load model (WECC CLM) (NER, December 2016; WEC, April 2021), which is capable of emulating more categories of load devices such as single-phase induction machines, power electronic-interfaced loads, as well as the distributed energy resources (DERs), which are being increasingly integrated into the power system. Though the structure of WECC CLM has been specified, this aggregated model is still like a ""grey box"", as the true values of the model parameters are not fully known. As a mathematical approximation, the parameters of WECC CLM cannot be tested on-site unlike the physical entity. The load survey is a direct approach to estimate the parameters. However, it is time-consuming and require high granularity for satisfactory accuracy. An alternative is to infer from measurement data under disturbances. The measurement-based approaches have been widely investigated by researchers. In (Wang and Wang, 2014), it’s formulated as a nonlinear optimization problem, which aims to find the optimum parameters that minimize the bias between the transient trajectories with estimated parameters and real measurements. The recursive least square (RLS) method is utilized to linearize the system model and identifies model parameters by minimizing the sum of the squares of the residuals in a recursive way. However, there are more than one hundred parameters and dozens of differential equations in WECC CLM, yielding a highly nonlinear and high-dimentional optimization problem with complex interaction among parameters. It is difficult to be competent in effectively solving this problem. Encouraged by the extraordinary capability of machine learning (ML) technology in solving complex tasks, researchers also investigate different kinds of ML-based methods for WECC CLM parameterization. In (Wang et al., 2020; Bu et al., 2020; Xie et al., 2021a), the parameter calibration problem is transformed to a markov decision process, and reinforcement learning (RL) is introduced to search for the best parameters. Some techniques, such as two-stage hierarchical framework, and evolutionary learning with sensitivity weight incorperation, are introduced to improve the accuracy. However, its optimality performance degenerates with the increase of action space and model complexity. In (Afrasiabi et al., 2023), a multi-residual deep learning structure is established to capture the spatial-temporal features and estimate the wide-area CLM parameters by learning the mapping between observations and model parameters. However, the supervised learning method fails to represent the one-to-many mapping between observations and parameters (Hu et al., 2023a). Different from the deterministic methods, generative models learn the underlying distributions of data and deduce probabilistic solutions. Based on Bayes’ theorem, several generative models, including generative adversarial network (GAN), conditional variational autoencoder (CVAE) and conditional masked autoregressive flow (CMAF) are also exploited to learn the posterior distribution of the parameters for WECC CLM (Khodayar and Wang, 2021; Khazeiynasab et al., 2022; Tan et al., 2024). However, one difficulty in practical application is ensuring the generalizability of parameters across different fault events. The CLM parameters carefully selected during one fault event may not achieve satisfactory performance in another fault. This is primarily due to the issue of parameter degeneracy, as previously discussed. Recent works with RL have explored training parameter identification agents using multiple events directly or adopting multi-task learning approaches (Hu et al., 2023b; Xie et al., 2021b). However, multi-event environments are inherently non-stationary, which can degrade the performance of the learning agents. Additionally, multi-task learning approaches risk negative transfer, where knowledge learned from one task hinders the learning of another. In recent years, generative artificial intelligence (AI) is taking center stage in the AI domain, with the emergence of a number of advanced generative models (Cao et al., 2023; Zhang et al., 2023). Diffusion probabilistic models employ a forward and reverse diffusion process, enabling them to accurately capture complex data distributions and embrace high-quality data generation (Ho et al., 2020; Du et al., 2023; Yang et al., 2023). The conditional structure enables diffusion models to flexibly control its generation process towards the expected style. In addition to data generation, diffusion models also have an outstanding performance in solving inverse problems such as image restoration with the ability to learn the intricate patterns and dependencies among data (Kawar et al., 2022; Daras et al., 2024). Motivated by the advancement of diffusion model and the need to address the multi-event challenge, we propose a novel parameter estimation framework named Joint Conditional Diffusion Model-based Inverse Problem Solver (JCDI). This framework learns the parameter posterior distributions through the forward and reverse diffusion process of diffusion models, simultaneoulsy considers the observations under different fault events. The main contributions of this work include: • By conducting global sensitivity analysis for WECC CLM, we reveal the sensitivity discrepancies under different fault events, especially when there are power electronic load tripping and motor stalling. • We develop a diffusion-based parameter estimation framework JCDI, with a transformer-based denoising network architecture, leveraging the diffusion model to capture complex distributions among parameter space, and produce a probabilistic solution considering the parameter degeneracy. • We propose a joint conditioning structure, which enables JCDI to infer parameters conditioned on transient trajectories under multiple fault events simultaneously. Therefore, the uncertainties of parameter estimation will be reduced with the increase of conditioned fault events. • We validate the effectiveness of JCDI in reducing estimation uncertainties of degenerated parameters and improving generalizability to different fault events. We also demonstrate the superiority of JCDI compared with existing parameter estimation approaches such as reinforcement learning and supervised learning."
https://arxiv.org/html/2411.10364v1,"Forming Auxiliary High-confident Instance-level Loss to 
Promote Learning from Label Proportions","Learning from label proportions (LLP), i.e., a challenging weakly-supervised learning task, aims to train a classifier by using bags of instances and the proportions of classes within bags, rather than annotated labels for each instance. Beyond the traditional bag-level loss, the mainstream methodology of LLP is to incorporate an auxiliary instance-level loss with pseudo-labels formed by predictions. Unfortunately, we empirically observed that the pseudo-labels are are often inaccurate due to over-smoothing, especially for the scenarios with large bag sizes, hurting the classifier induction. To alleviate this problem, we suggest a novel LLP method, namely Learning from Label Proportions with Auxiliary High-confident Instance-level Loss (L2p-ahil). Specifically, we propose a dual entropy-based weight (DEW) method to adaptively measure the confidences of pseudo-labels. It simultaneously emphasizes accurate predictions at the bag level and avoids overly smoothed predictions. We then form high-confident instance-level loss with DEW, and jointly optimize it with the bag-level loss in a self-training manner. The experimental results on benchmark datasets show that L2p-ahil can surpass the existing baseline methods, and the performance gain can be more significant as the bag size increases.","During the past decades, supervised learning has been proven to gain significant success in diverse real-world applications, in which one can easily collect loads of training instances with precise supervision [10, 38]. Unfortunately, in many scenarios, it is challenging to collect a sufficient number of precisely labeled training instances due to various difficulties such as high annotation costs [16, 32] and privacy protection concerns [11]. Consequently, training instances with incomplete, inexact, and inaccurate supervision are often available only [50]. The demand for learning with such weak supervision facilitates the development of Weakly-Supervised Learning (WSL)[37], ranging many specific paradigms, e.g., semi-supervised learning [18, 49], partial label learning [24, 19], and positive-unlabeled learning [8, 12], to name just a few. Figure 1: Before training, the data is grouped and only the proportion information is retained. Classifier training is performed using unlabeled data and proportions in bag units, and finally the instance classification task is completed. (a) (b) (c) (d) Figure 2: Results of preliminary experiments with different bag sizes on CIFAR-10 and CIFAR-100 (500 epochs). (a)(c) Averaged accuracy of pseudo-labels on CIFAR-10 and CIFAR-100. (b)(d) Averaged normalized entropy of pseudo-labels on CIFAR-10 and CIFAR-100. Higher normalized entropy values imply more smoothing results. Table 2 shows the adverse effects caused by these inaccurate pseudo-labels. Recently, an emerging paradigm of WSL, known as Learning from Label Proportions (LLP) as shown in Fig.1, has gained increasing attention within the machine learning community [48, 2, 9, 4], and it has been broadly applied in a wide range of domains such as disease diagnosis [39], population census [1], and spam email filtering [30]. Technically, LLP, as its name suggests, refers to the classification problem where a certain number of training instances are grouped into many bags with the corresponding proportion of classes, rather than annotated labels for each instance. Naturally, such sparse and inexact supervision from LLP causes significant difficulties in inducing effective classifiers for predicting labels of instances. To handle the task of LLP, the basic method is to induce the classifier by optimizing the bag-level loss, measuring the difference between the ground-truth class proportion in a bag and the predicted class proportion formed by predictions of instances from the bag [46]. As suggested by [1], optimizing the bag-level loss only can not guarantee inducing effective classifiers. Therefore, the mainstream methodology of LLP becomes to incorporate an auxiliary instance-level loss with pseudo-labels, and jointly optimize it with the bag-level loss in a self-training manner [21, 22, 25, 7]. However, in our preliminary experiments, we empirically observed that the pseudo-labels generated by this kind of methods are often inaccurate and even meaningless, especially for the scenarios with large bag sizes. As illustrated in Fig.2, our early empirical results indicated that the pseudo-labels are inaccurate and tend to be overly smoothed, quantitatively measured by entropy values, as the bag size increases. Such low-quality pseudo-labels hurt the classifier induction, resulting in performance degradation. To solve this problem, in this paper, we propose a novel LLP method, namely Learning Label Proportions with Auxiliary High-confident Instance-level Loss (L2p-ahil). Specifically, we suggest a dual entropy-based weighting (DEW) method to adaptively evaluate the confidence of pseudo-labels from both bag and instance levels. At the bag level, it emphasizes accurate class proportion predictions measured the difference between entropy values of ground-truth proportions and predicted proportions from the class-pivoted perspective. At the instance level, it avoids overly smoothed predictions of instances, directly measured by the entropy values. We then form a high-confident instance-level loss with DEW, and jointly optimize it with the bag-level loss in a self-training manner. The experimental results on benchmark datasets demonstrate that L2p-ahil can outperform the existing baseline methods, and the performance gain can be more significant as the bag size increases. In a nutshell, the major contributions of this paper are outlined as follows: • We quantitatively evaluated the problem of inaccurate and meaningless pseudo-labels of the existing LLP art, as the big size increases. • To solve this problem, we propose a novel LLP method named (L2p-ahil), which adaptively measures the confidences of pseudo-labels. • We conduct extensive experiments to empirically validate the effectiveness of L2p-ahil even with larger bag sizes."
https://arxiv.org/html/2411.10323v1,"The Dawn of GUI Agent: A Preliminary 
Case Study with Claude 3.5 Computer Use","The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use’s unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project: https://github.com/showlab/computer_use_ootb.","Automating desktop tasks has become an increasingly popular area of research, driven by the need to enhance users’ productivity and accessibility across various application environments. From web navigation to professional software and even video games, users frequently encounter repetitive tasks that could benefit from automation. While large language models like GPT-4 and Qwen-2-VL have demonstrated their potential in automating tasks through general GUI interaction, the capacity of these models is still far from enough for applicable desktop task automation. Recent studies in GUI automation agents have leveraged general-purpose LLMs to interact with graphical user interfaces (GUIs) by understanding the GUI state and generating actions. However, the release of Claude 3.5 Computer Use by Anthropic marks a significant advancement in this domain, introducing the first frontier AI model to offer computer use in public beta. Unlike previous models, Claude 3.5 Computer Use offers an end-to-end solution through API calls, actions will be generated from user instruction and observed purely visual GUI state, without requiring further external knowledge such as reference plan and GUI parsing. Despite this advancement, the community needs a comprehensive analysis that evaluates the performance of API-based GUI automation models in depth. To take the first steps to explore the capacities and limitations of such models, we propose a comprehensive case study based on real-world desktop environments, encompassing a diverse range of software domains, including web navigation, professional tools, and games. The selected cases are designed to reflect the needs of various user groups, ensuring that the evaluation covers a broad spectrum of desktop automation tasks. To isolate specific aspects of the model’s capability, we evaluate the performance of API-based GUI automation models rigorously across three dimensions: • Planning: Assessing the model’s ability to generate an executable plan from the user’s query. The plan should have a correct flow, allowing the overall successful operations of the software, with each step being clear and executable. • Action: Evaluating whether the model can accurately ground the interactable GUI elements and execute the action step-by-step from the derived plan. • Critic: Measuring the model’s awareness of the changing environment, including its ability to adapt to the outcomes of its actions, such as retrying tasks if unsuccessful or terminating execution when the task is completed. To our best knowledge, this is the first comprehensive case study on Claude 3.5 Computer Use and API-based GUI automation models. We hope that our research provides the community with valuable insights into the capacities and limitations of these models. Our case study aim to lay the foundation for the continued exploration and benchmarking of API-based GUI automation. Additionally, to facilitate the community to discover and benchmark the newly released model, we also release an out-of-the-box universal framework, namely Computer Use OOTB, providing a seamless solution for users and researchers to deploy these models in local environments without the need for complex setup or configuration, aiming to improve the accessibility of GUI automation research field. Our contributions in this report are summarized as follows. • We present a comprehensive case study for Claude 3.5 Computer Use on desktop task automation, covering domains such as web search, professional software, and games, designed to reflect the needs of various user groups. • We introduce an out-of-the-box, cross-platform agent framework for deploying API-based GUI automation models, offering a universal solution for easy implementation and benchmarking. • We conduct extensive human evaluations and provide in-depth analyses, demonstrating both the advancements and limitations of the newly released API-based GUI automation model."
https://arxiv.org/html/2411.10272v1,Scaling Law for Post-training after Model Pruning,"Large language models (LLMs) based on the Transformer architecture are widely employed across various domains and tasks. However, their increasing size imposes significant hardware demands, limiting practical deployment. To mitigate this, model pruning techniques have been developed to create more efficient models while maintaining high performance. Despite this, post-training after pruning is crucial for performance recovery and can be resource-intensive. This paper investigates the post-training requirements of pruned LLMs and introduces a scaling law to determine the optimal amount of post-training data. Post-training experiments with the Llama-3 and Qwen-2.5 series models, pruned using depth pruning, width pruning, and 2:4 semi-structured pruning, show that higher pruning ratios necessitate more post-training data for performance recovery, whereas larger LLMs require less. The proposed scaling law predicts a model’s loss based on its parameter counts before and after pruning, as well as the post-training token counts. Furthermore, we find that the scaling law established from smaller LLMs can be reliably extrapolated to larger LLMs. This work provides valuable insights into the post-training of pruned LLMs and offers a practical scaling law for optimizing post-training data usage.","Large language models (LLMs) based on the Transformer architecture Vaswani et al. (2017) have attracted widespread attention and are applied across diverse domains and tasks. However, as LLMs grow in size, their hardware demands increase substantially, limiting their practical deployment in real-world scenarios. To address this challenge, researchers have focused on developing compact models through model pruning techniques Han et al. (2016) that maintain high performance while reducing hardware requirements. Model pruning can be broadly categorized into unstructured pruning Frantar and Alistarh (2023); Sun et al. (2024); Zhang et al. (2024) and structured pruning Chen et al. (2024); Hu et al. (2024); Liu et al. (2024); Muralidharan et al. (2024); Ma et al. (2023); Ashkboos et al. (2024); Men et al. (2024). Unstructured pruning removes individual elements from weight matrices, resulting in sparse matrices while maintaining model performance, but it is not hardware-friendly and does not effectively accelerate computation. Semi-structured pruning, a variant of unstructured pruning, leverages hardware support Mishra et al. (2021) to achieve acceleration, though it may cause greater performance degradation compared to unstructured pruning. Structured pruning, on the other hand, removes entire components such as attention heads or layers, effectively reducing the parameter count but often with a higher performance loss compared to other pruning techniques. Figure 1: The post-training loss curve of Llama-3.1-8B after depth pruning with different pruning ratios. Figure 2: The post-training loss curve of Llama-3.1-8B, Llama-3.2-3B and Llama-3.2-1B after 2:4 semi-structured pruning. To effectively utilize compact models obtained from semi-structured or structured pruning, post-training after pruning Ashkboos et al. (2024); Chen et al. (2024); Yang et al. (2024); Ma et al. (2023); Kim et al. (2024) is essential to mitigate performance decline. Some model pruning approaches employ continual pre-training or Parameter-Efficient Fine-Tuning (PEFT) as the post-training stage after pruning. For example, Shortened Llama Kim et al. (2024) uses 627B tokens of pre-training data for continual pre-training of the pruned LLMs, whereas LLM-Pruner Ma et al. (2023) utilizes 50,000 instruction data points for LoRA fine-tuning Hu et al. (2021). However, LoRA fine-tuning with a limited amount of instruction data is insufficient to fully restore the model’s performance, whereas continual pre-training with a large dataset can achieve full recovery but requires substantial hardware resources. Given the significant hardware demands, this raises the question: is it truly necessary to use such a vast amount of data for model recovery? LLM-Streamline Chen et al. (2024) finds that using large amounts of data for post-training only slightly improves performance compared to using a more appropriate amount. Therefore, identifying the optimal amount of post-training data is crucial for the performance recovery of pruned LLMs and is more resource-efficient. This naturally leads to the question of whether a similar scaling law, like those observed during pre-training of LLMs, could be established to predict the the optimal amount of post-training data after model pruning. In this paper, we conduct post-training experiments on the Llama-3 series Dubey et al. (2024) and Qwen-2.5 series models Team (2024), employing depth pruning, width pruning, and 2:4 semi-structured pruning. From these experiments, we establish a scaling law for post-training after model pruning. Specifically, we have two key observations. First, we find that as the pruning ratio increases, the amount of data required for adequate post-training also increases. For example, as shown in Figure 2, with depth pruning, the loss curve of Llama-3.1-8B approaches convergence after post-training on approximately 1B tokens at a 16% pruning ratio. However, at pruning ratios of 24% and 33%, the loss curve continues to decline, indicating a need for more post-training data. Second, we find that larger LLMs require less data to recover their performance after pruning, which contradicts the intuitive assumption that larger LLMs would need more data. As illustrated in Figure 2, with 2:4 semi-structured pruning, increasing model parameter counts generally results in a flatter post-training loss curve. For instance, the loss curve of Llama-3.1-8B approaches convergence with approximately 0.5B tokens, while the smaller Llama-3.2-1B’s loss curve continues to decline, suggesting that larger LLMs require less post-training data. Considering these two key observations and the established scaling laws for the pre-training of LLMs Hoffmann et al. (2022), we determine the form of the scaling law for post-training after model pruning and fit these laws to the three pruning methods across the Llama-3 series and Qwen-2.5 series models. Given the model parameter counts before and after pruning, as well as the post-training token counts, we can predict the model’s loss based on the fitted scaling law, enabling us to determine the optimal amount of post-training data. Finally, we find that the scaling law established from smaller LLMs can be reliably extrapolated to larger LLMs. suggesting that the scaling law we established has the potential to predict the rate of decline in the loss curve for even larger pruned LLMs. Overall, this paper makes the following contributions: • We identify two key observations in the post-training of LLMs pruned using depth pruning, width pruning, and 2:4 semi-structured pruning: pruned LLMs with higher pruning ratios require more post-training data to restore their performance, while larger LLMs require less post-training data to recover performance. • Based on the observations we have discovered, we determine the form of the scaling law for post-training after model pruning and fit these laws to the three pruning methods across the Llama-3 seires and Qwen-2.5 series models. This scaling law allows us to predict the model’s loss based on its parameters counts before and after pruning, as well as the post-training token counts. Additionally, we find that the scaling law established from smaller LLMs can be extrapolated to larger LLMs, suggesting it has the potential to predict the rate of decline in the loss curve for even larger LLMs."
https://arxiv.org/html/2411.10176v1,Let people fail! Exploring the influence of explainable virtual and robotic agents in learning-by-doing tasks,"Collaborative decision-making with artificial intelligence (AI) agents presents opportunities and challenges. While human-AI performance often surpasses that of individuals, the impact of such technology on human behavior remains insufficiently understood, primarily when AI agents can provide justifiable explanations for their suggestions. This study compares the effects of classic vs. partner-aware explanations on human behavior and performance during a learning-by-doing task. Three participant groups were involved: one interacting with a computer, another with a humanoid robot, and a third one without assistance. Results indicated that partner-aware explanations influenced participants differently based on the type of artificial agents involved. With the computer, participants enhanced their task completion times. At the same time, those interacting with the humanoid robot were more inclined to follow its suggestions, although they did not reduce their timing. Interestingly, participants autonomously performing the learning-by-doing task demonstrated superior knowledge acquisition than those assisted by explainable AI (XAI). These findings raise profound questions and have significant implications for automated tutoring and human-AI collaboration.","The maturation of artificial intelligence (AI) techniques has facilitated their extensive utilization across various domains. The integration and refinement of explainable AI (XAI) methods have further empowered non-expert users to incorporate AI models into decision-making settings Waldman and Martin (2022). The resultant dynamics of human-AI collaboration have become a focal point of interest for the human-computer interaction (HCI) community and society at large Buçinca et al. (2021). While human-AI collaboration in decision-making has predominantly been addressed in HCI in recent years, with individuals interacting with artificial agents or receiving suggestions and explanations from recommendation systems Malhi et al. (2020); Lai et al. (2021), the study of the human-robot collaboration received the scientific community’s attention since the dawn of the human-robot interaction (HRI) research field. However, recent years have witnessed implementing and testing explainable techniques with robots in collaborative contexts Anjomshoae et al. (2019); Wallkötter et al. (2021). Differently from the HCI context, the HRI one provides richer interaction modalities, offering a more diverse range of opportunities for personalizing XAI and the modality of the explanations delivery Matarese et al. (2021). An emerging challenge addressed by both the HCI and HRI communities is examining how AI technologies influence human behavior in the context of human-AI collaboration Green and Chen (2019). Multidisciplinary efforts have investigated the impact of AI suggestions on human decision-making, exploring implications related to human cognitive biases Bertrand et al. (2022). Moreover, the introduction of XAI techniques has a dual effect, enabling non-expert users to benefit from such powerful technology while also raising concerns about over-reliance on AI models and the reinforcement of negative human heuristics, such as automation bias Vered et al. (2023). This work investigates the impact of interacting with virtual and robotic explainable agents on people’s behavior and performance during a learning-by-doing task Anzai and Simon (1979); Schank et al. (2013). In our experiments, participants had to learn an unknown task with the assistance of an explainable artificial agent, specifically a virtual talking agent and a social humanoid robot. Additionally, a separate group performed the task autonomously without assistance. During the experiments, we employed an assessment task to directly and quantitatively measure the utility of the human-agent explanatory interactions, building on prior work111M. Matarese, F. Rea, K. Rohlfing, A. Sciutti. How informative is your XAI? Assessing the Quality of Explanations through Information Power (under review).. We aimed to compare the effect of different explanation strategies and explainable agents on participants’ behavior, focusing on their final knowledge of the task. The subsequent sections are organized as follows. Section 2 reviews related works, categorizing them into three parts: human-AI collaboration, explanations in human-AI decision-making, and explanations evaluation. Section 3 outlines the methods employed in the user study, presenting the peculiar methodologies and the technology used during the experiments. Section 4 details the results of the user study, showing comparisons between the experimental conditions. These results are extensively discussed in Section 5, with reference to the existing literature. Finally, Section 6 summarizes our work, highlighting its limitations."
https://arxiv.org/html/2411.10173v1,Semantics and Spatiality of Emergent Communication,"When artificial agents are jointly trained to perform collaborative tasks using a communication channel, they develop opaque goal-oriented communication protocols. Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly. In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances. We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction. We prove, under mild assumptions, that semantically inconsistent communication protocols can be optimal solutions to the discrimination task, but not to reconstruction. We further show that the reconstruction objective encourages a stricter property, spatial meaningfulness, which also accounts for the distance between messages. Experiments with emergent communication games validate our theoretical results. These findings demonstrate an inherent advantage of distance-based communication goals, and contextualize previous empirical discoveries.","Humans use language in multi-agent social interactions. Pressures of synchronization and collaboration play a central role in shaping the way we communicate. Motivated by this observation and the goal of creating artificial agents capable of meaningful communication, the field of emergent communication (EC) employs a multi-agent environment jointly trained to accomplish a task that requires active transmission of information. The agents utilize a messaging channel that usually takes the form of a discrete sequence of abstract symbols, resembling the structure of human language. Successful optimization results in synchronized agents operating a newly developed communication protocol tailored to the objective. We study a type of EC setup inspired by Lewis games [32] where a sender agent describes a given input and a receiver agent makes a prediction based on that description. The game objective is designed to make the receiver demonstrate knowledge of the original input, which in turn compels the sender to encode relevant information in the message. There are two common objectives used in this type of EC setup (illustrated in Figure 1): Reconstruction In the reconstruction task [34, 44, 45], the original input is re-generated based solely on the message. We are specifically interested in a reconstruction objective that quantifies distance between prediction and target, forming a discrete version of autoencoding [18]. Discrimination In the discrimination task [28, 17, 33], the original input is retrieved from a set of candidates, incentivized by negative log-likelihood. (a) (b) Figure 1: Illustration of the reconstruction and discrimination tasks. The discrimination receiver is given the candidates in addition to the message. A central goal in EC, which motivates our work, is understanding how different factors in the environment affect the emergent protocol, and specifically developing agents and objectives that create protocols with similar characteristics to natural language [41, 25, 45, 2, 40]. Tools and experiments have been developed to evaluate the proximity to natural language by testing for properties such as compositionality [6] or efficiency [7]. Unfortunately, many of these empirical methods show great dissimilarity to human communication. One particularly surprising experiment [3] revealed that protocols created via the discrimination task can generalize extremely well to random noise data, suggesting that they do not signal human-recognizable (meaningful) properties of the input. In this paper, we identify a fundamental property of any meaningful communication protocol, and thus a prerequisite for similarity to natural language. We observe that the discrete bottleneck forces EC protocols to be many-to-one mappings, i.e., that messages likely represent more than one input. With this in mind, we claim that inputs mapped to the same message should be semantically similar, as is the case with human language. We formalize this idea with a mathematical definition that we term semantic consistency. We further develop a stricter version of this definition, spatial meaningfulness, which also accounts for distances between messages, and is therefore better suited to indicate similarity to natural language. Armed with these definitions, we analyze theoretical solutions to the two common EC environments. In the reconstruction setting, under mild assumptions, we prove that every optimal solution is semantically consistent. With a different set of assumptions, we also show that reconstruction-induced communication protocols are spatially meaningful. In sharp contrast, we surprisingly find that the discrimination objective does not guarantee semantic consistency, i.e., a communication protocol can be optimal in a discrimination environment but still not semantically consistent nor spatially meaningful. In fact, we show that uniformly random messages can lead to a globally optimal discrimination objective value, meaning that the relationship between inputs mapped to the same message is potentially arbitrary despite optimally solving the task. Our results provide theoretical support to previous empirical findings, such as the discrimination generalization to noise. We further analyze several common variations of the discrimination game from the EC literature. To support our findings, we run experiments on Shapes [27] and MNIST [30]. The empirical results agree with most of our theoretical findings. However, we observe a gap between theory and practice regarding the level of channel utilization, which we leave for future work to further investigate."
https://arxiv.org/html/2411.10115v1,Memorization in Attention-only Transformers,"Recent research has explored the memorization capacity of multi-head attention, but these findings are constrained by unrealistic limitations on the context size. We present a novel proof for language-based Transformers that extends the current hypothesis to any context size. Our approach improves upon the state-of-the-art by achieving more effective exact memorization with an attention layer, while also introducing the concept of approximate memorization of distributions. Through experimental validation, we demonstrate that our proposed bounds more accurately reflect the true memorization capacity of language models, and provide a precise comparison with prior work.","1 INTRODUCTION Modern large language models, especially Transformers, showcase great memorization capacity ([karpukhin-etal-2020-dense, roberts_how_2020]). Among recent works, researchers have shown that facts are memorized in the MLPs of a Transformer, and have even identified which MLPs ([meng2022locating, meng2023massediting, nanda_fact_2023]). However, they were not able to understand how these MLPs store information. Both exact and approximate theoretical memorization in an MLP are well documented in the literature: a ReLU MLP can memorize exactly as many real-valued label as it has neurons n ([Bubeck2020NetworkSA]), and can memorize exactly n discrete labels with only \tilde{O}(\sqrt{n}) neurons ([vardi2022on]). Contrary to the MLPs, the memorization power of multi-head attention layers has not been empirically studied. The main role of the attention layer is not viewed as remembering information but rather as moving between residual streams the information retrieved by the MLPs ([nanda_fact_2023, wang2023interpretability, variengien2024look]). For the theoretical aspect of the memorization in attention layers, there exists results on the expressivity of the attention patterns ([pmlr-v119-bhojanapalli20a]), the memorization capacity of attention layers ([mahdavi2024memorization]), and the memorization capacity of Transformers ([kim2023provable]). We will discuss related works in depth in section 6. In this article, we are interested in moving the state-of-the-art in terms of memorization capacity for the attention layer. We will thus consider the memorization capacity of an Attention-only Transformer (AoT). We need to specify what memorization capacity means. We will distinguish two types of memorization tasks, namely the association task and the distribution task. The association task, already studied in [bietti2023birth, cabannes2024scaling, kim2023provable, mahdavi2024memorization], consists of predicting a token given a sequence of tokens as input. We only require the AoT to predict the right next-token at the last position. This memorization is exact and hence, we want to know the maximal set of sequence-token associations (t_{in},t_{out}) that can be exactly memorized by an AoT. The distribution task consists of predicting the correct distribution, measured using the KL-divergence, for an input sequence of tokens. We use the KL-divergence since it is the default loss function used to train most Transformers. To our knowledge, we are the first to introduce and study this task. Memorizing distribution happens in natural language modeling: take the sentence ""Arnold Schwarzenegger was a"", it can be completed with ""actor"", ""writer"" or ""bodybuilder"". Thus, language models need to memorize not one but several correct next-tokens, each with a possibly different probability depending on the importance of the answer. Our contributions are: 1. We improve the state-of-the-art on the association task by proving that a one layer AoT with H heads each of dimension d_{h}, and an embedding dimension d can memorize Hd_{h}+d associations. In the context of language model, this improves on the previous result on the attention layer expressivity by [mahdavi2024memorization] which requires a limited context-windows and has memorization capacity of H(d_{h}-1)+1. We compare our result with other constructions using deep Transformers ([kim2023provable]) as well as MLPs memorization ([huben2023attentiononlytransformersimplementingmlps, Bubeck2020NetworkSA]). 2. We introduce the distribution task for Transformers as a way to quantify memorization when there is not a unique correct next-token. We provide upper and lower bounds for that distribution task on the error made by the best AoT. The divergence of that AoT will approximate that of a sequence encoder, which is a mapping from token sequences to logits, and has a rank constraint. 3. Finally, we prove upper bound for the divergence of sequence encoders when the target distribution is almost a look-up table. Proofs for the statements can be found in appendix with experimental details. The code-base is available at this repo."
https://arxiv.org/html/2411.10084v1,Adapting the Biological SSVEP Response to Artificial Neural Networks,"Neuron importance assessment is crucial for understanding the inner workings of artificial neural networks (ANNs) and improving their interpretability and efficiency. This paper introduces a novel approach to neuron significance assessment inspired by frequency tagging, a technique from neuroscience. By applying sinusoidal contrast modulation to image inputs and analyzing resulting neuron activations, this method enables fine-grained analysis of a network’s decision-making processes. Experiments conducted with a convolutional neural network for image classification reveal notable harmonics and intermodulations in neuron-specific responses under part-based frequency tagging. These findings suggest that ANNs exhibit behavior akin to biological brains in tuning to flickering frequencies, thereby opening avenues for neuron/filter importance assessment through frequency tagging. The proposed method holds promise for applications in network pruning, and model interpretability, contributing to the advancement of explainable artificial intelligence and addressing the lack of transparency in neural networks. Future research directions include developing novel loss functions to encourage biologically plausible behavior in ANNs.","Neuron importance assessment refers to quantifying the significance of individual neurons within an artificial neural network (ANN), with respect to their contribution to overall network performance and output. Such tools have a wide application range, mostly in the context of explainable artificial intelligence (XAI), that aims to improve the long-criticized lack of transparency of neural networks [1]; and network pruning, where by removing less influential neurons, one can forge computationally less intensive models with often negligible performance loss [2]. The assessment of neuron significance within ANNs has been gaining popularity. Notable advances include layer-wise relevance propagation [3], which backpropagates output predictions throughout the network while assigning relevance scores to individual neurons. DeepLIFT [4] on the other hand contrasts neuron activation with the final decision of the network based on a chosen reference input that is “neutral”. This comparison enables the assignment of contribution scores to individual neurons. A neuron/filter pruning technique for convolutional neural networks (CNN) has been also reported [5] through the Taylor expansion of the loss function, approximating the elimination effect of an individual neuron on the model’s loss. Another study [6] has tackled the same issue through inter-filter mutual relationships. Moreover, the Neuron Shapley approach [7] relies on computing the average marginal neuron contributions across all possible neuron combinations, while Neuron-Level Plasticity Control [8] estimates a neuron’s capacity to maintain previously learned knowledge during adaptation to new tasks with the end of significance quantification. More recently, the context of adversarial attacks has been explored [9] through the activation and deactivation of individual neurons, so as to assess which among them influence a model’s vulnerability. In contrast to the aforementioned studies, this paper proposes a radically different approach to neuron significance assessment, based on the concept of frequency tagging, adopted from the neuroscientific investigation of the biological brain [10]. To the best of our knowledge, it has never been explored before in the context of artificial neural networks. Figure 1: The proposed frequency tagging approach for a color image. Its left half is tagged (i.e. contrast-modulated) with 6 Hz whereas the right half is tagged with 7.5 Hz. The tagged images are sequentially fed into an arbitrary (convolutional) neural network, in the order of their tagging, and the neuron’s activation responses are acquired across time. The temporal sequence is then transformed into the frequency domain, with symbolic amplitudes illustrating the network’s response strength at specific frequencies. More specifically, if the human brain is presented with a visual stimulus flickering (i.e. changing intensity regularly) at a specific frequency, then the neural response gets tuned to the harmonics of the same frequency. This is a well-established observation in neuroscience literature [11, 12, 13], and can be directly observed from an electroencephalogram (EEG). Frequency tagging (Fig. 1) is a method of artificially flickering parts of a visual stimulus with distinct frequencies (i.e. so parts are “tagged”) with the aim of isolating the traces of part-based processing in the brain [12]. In this paper, it has been hypothesized that one should observe the same effect in an ANN as well, if it indeed mimics a biological brain as often claimed [14]. To test this, we have focused on a prominent neuroscientific experimental paradigm [11], and replaced the biological brain with an ANN. The specifics of frequency tagging and its adaptation to ANNs as well as the tools that can be used for node significance assessment are presented next in Section 2. Then the proof-of-concept experiments (Section 3) that have been conducted with color images and a Resnet-32 CNN, with the end of observing notable harmonics under part-based frequency tagging are described. The paper ends with a discussion of our results and various deep learning applications that can benefit from the proposed neuron significance assessment method (Section 4)."
https://arxiv.org/html/2411.10063v1,Federated Domain Generalization via Prompt Learning and Aggregation,"Federated domain generalization (FedDG) aims to improve the global model’s generalization in unseen domains by addressing data heterogeneity under privacy-preserving constraints. A common strategy in existing FedDG studies involves sharing domain-specific knowledge among clients, such as spectrum information, class prototypes, and data styles. However, this knowledge is extracted directly from local client samples, and sharing such sensitive information poses a potential risk of data leakage, which might not fully meet the requirements of FedDG. In this paper, we introduce prompt learning to adapt pre-trained vision-language models (VLMs) in the FedDG scenario, and leverage locally learned prompts as a more secure bridge to facilitate knowledge transfer among clients. Specifically, we propose a novel FedDG framework through Prompt Learning and AggregatioN (PLAN), which comprises two training stages to collaboratively generate local prompts and global prompts at each federated round. First, each client performs both text and visual prompt learning using their own data, with local prompts indirectly synchronized by regarding the global prompts as a common reference. Second, all domain-specific local prompts are exchanged among clients and selectively aggregated into the global prompts using lightweight attention-based aggregators. The global prompts are finally applied to adapt VLMs to unseen target domains. As our PLAN framework requires training only a limited number of prompts and lightweight aggregators, it offers notable advantages in computational and communication efficiency for FedDG. Extensive experiments demonstrate the superior generalization ability of PLAN across four benchmark datasets. We have released our code at https://github.com/GongShuai8210/PLAN.","In today’s era of distributed data sources and edge computing, there exists a substantial demand for collaboratively training machine learning models across multiple clients. Federated learning (FL) [1, 2] has emerged as a promising solution, enabling the development of accurate and robust models while maintaining data privacy. Unlike traditional centralized approaches, FL allows each local client to learn from its own data. Then, a central server periodically aggregates the local model parameters from all clients to generate a global model. Figure 1: The novel problem setting of FedDG aims to learn a global model from multiple decentralized source domains, enabling it to directly generalize to completely unseen target domains. Despite significant progress, traditional FL approaches [3, 4, 5, 6] primarily concentrate on enhancing model performance within the federation of clients. These methods often assume that data across all clients are identically distributed. In reality, however, clients independently collect local data, which naturally form mutiple source domains with distinct distributions. During deployment, the test data may also come from a target domain with previously unseen distributions. Therefore, how to generalize the federated model under domain shifts remains an underexplored issue. As a common solution to the domain shift issue, domain generalization (DG) [7, 8] techniques have been developed to enable models trained on source domains with heterogeneous distributions to generalize well to unseen target domains. Nevertheless, applying existing DG methods in the FL setting is not straightforward, as they typically operate in a centralized manner that requires full access to data from different source domains. To overcome this challenge, federated domain generalization (FedDG) [9] has been further introduced to synergize FL and DG. As illustrated in Fig. 1, FedDG facilitates collaborative model learning across diverse source domains for effective generalization to unseen target domains while keeping data privacy. Many prior studies on FedDG aim to share domain knowledge among clients under the privacy-preserving constraint, thereby enhancing the generalization ability of each local model by integrating knowledge from diverse domains. For example, Liu et al. [9] exchanged the amplitude spectrum information in frequency space among clients. Huang et al. [10] allowed the sharing of class prototypes of local samples across different clients. Chen et al. [11] performed cross-client style transfer by exchanging the mean and variance of each pixel-level feature channel. Notably, the aforementioned domain knowledge, including amplitude spectrum, class prototypes, and data style, is directly extracted from local samples and can be considered a reflection of their inherent characteristics. Sharing such sensitive information among clients potentially poses the risk of data leakage [12], which might not fully meet the requirements of FedDG. Recently, numerous foundational vision-language models (VLMs) [13, 14] have been developed by utilizing large-scale image-text pairs from the web. These VLMs acquire general knowledge about the associations between visual patterns and their corresponding textual descriptions. To preserve this general knowledge and mitigate the transfer gap to downstream tasks, the technique of prompt learning [15, 16] has been proposed, which freezes the parameters of VLMs while inserting a few learnable prompt tokens as additional inputs to VLMs. In a FL scenario, prompt learning can be conducted in each client, enabling the learned prompts to encapsulate domain knowledge and facilitate the adaptation of VLMs to the tasks at hand. It is important to note that prompts are optimized via learning rather than being generated directly from local samples in each client. Therefore, these prompts can serve as a more secure bridge to transfer domain knowledge among clients for FedDG. In this paper, we present a novel FedDG framework through Prompt Learning and AggregatioN (PLAN). Our PLAN method comprises two stages in each federated round: first, learning local prompts in each client, and then, synthesizing global prompts that effectively generalize to unseen target domains. A few recent works [17, 18] have begun to explore prompt learning in the context of FL. However, there remain two major limitations: 1) Clients independently learn local prompts, resulting in prompts biased towards each client’s private data. This bias can compromise the generalization of the global model; and 2) A common practice for aggregating local prompts into global prompts is by using fixed weights. However, diverse domain knowledge tends to contribute unequally to the global model, and neglecting these differences may significantly diminish model performance [19]. To address the above issues, PLAN introduces a reference-based prompt learning mechanism, which requires the local prompts in each client to align with a common reference—the global prompts generated in the previous round and shared among all clients. In this way, PLAN facilitates the indirect synchronization of local prompts across clients without the need for data sharing. Additionally, PLAN is equipped with the lightweight attention-based prompt aggregators, which measures the importance of local prompts from different clients and selectively aggregates them into the global prompts. During the aggregation, all local prompts remain unchanged, with only the aggregators’ parameters being updated. Note that for further improved performance, PLAN performs both text and visual prompt learning and aggregation on multiple blocks in VLMs. In each federated round, clients only need to train and exchange a limited set of prompt tokens and the lightweight aggregators with the server, rather than the entire model. Therefore, PLAN not only offers robust privacy-preserving capability but also enhances computational and communication efficiency for FedDG. Experimental results on four benchmark datasets demonstrate that our PLAN method significantly outperforms conventional FL, DG, and FedDG methods, as well as prompt learning-based methods. Ablation studies and hyperparameter analyses validate the key components of PLAN. We further verify the effectiveness of PLAN when faced with insufficient local data and confirm its advantages in both computational and communication costs. Visualization studies are also conducted to provide deeper insights into PLAN. In a nutshell, our main contributions are as follows: • Instead of sharing domain knowledge directly extracted from local samples, we introduce prompts learned with local data as a more secure means of transferring domain knowledge among clients in FedDG. • We propose a novel FedDG framework, PLAN, which integrates a reference-based prompt learning mechanism to facilitate the indirect synchronization of local prompts across clients and utilizes the lightweight attention-based prompt aggregators to selectively aggregate local prompts into global prompts. • We conduct extensive experiments on four benchmark datasets and demonstrate that our PLAN method achieves new state-of-the-art performance while maintaining communication and computation efficiency for FedDG. The remainder of the paper is structured as follows: Section II provides a review of related work. Section III presents the preliminary knowledge relevant to our study. Section IV details our PLAN method for FedDG. Experimental results and analysis are discussed in Section V, followed by the conclusions in Section VI."
https://arxiv.org/html/2411.10053v1,That Chip Has Sailed: A Critique of Unfounded Skepticism Around AI for Chip Design,"In 2020, we introduced a deep reinforcement learning method capable of generating superhuman chip layouts, which we then published in Nature and open-sourced on GitHub. AlphaChip has inspired an explosion of work on AI for chip design, and has been deployed in state-of-the-art chips across Alphabet and extended by external chipmakers. Even so, a non-peer-reviewed invited paper at ISPD 2023 questioned its performance claims, despite failing to run our method as described in Nature. For example, it did not pre-train the RL method (removing its ability to learn from prior experience), used substantially fewer compute resources (20x fewer RL experience collectors and half as many GPUs), did not train to convergence (standard practice in machine learning), and evaluated on test cases that are not representative of modern chips. Recently, Igor Markov published a “meta-analysis” of three papers: our peer-reviewed Nature paper, the non-peer-reviewed ISPD paper, and Markov’s own unpublished paper (though he does not disclose that he co-authored it). Although AlphaChip has already achieved widespread adoption and impact, we publish this response to ensure that no one is wrongly discouraged from innovating in this impactful area.","Following its publication in Nature, AlphaChip [30] has inspired an explosion of work on AI for chip design [41, 39, 43, 40, 10, 18, 5, 34, 8, 12, 17, 37, 7, 29]. It has also generated superhuman chip layouts used in three generations of TPU (see Figure 1), datacenter CPUs (Axion), and other chips across Alphabet, and been extended to new areas of chip design by external academics and chipmakers [25, 11]. Figure 1: AlphaChip has been deployed in three additional generations of TPU. In each generation, it has been adopted in a greater proportion of blocks and has outperformed human experts by a wider margin. Even so, Igor Markov published a criticism of our work in the November 2024 issue of Communications of the ACM [27], which is presented as a “meta-analysis” of our Nature paper and two non-peer-reviewed papers: • Cheng et al.: The first is an invited ISPD paper111Invited papers at ISPD are not peer-reviewed. by Cheng et al. [9]. This paper did not follow standard machine learning practices, and its reinforcement learning methodology and experimental setup diverged significantly from those described in our Nature paper. Nevertheless, its hamstrung version of our method still outperformed RePlAce222Incidentally, RePlAce, as noted in a footnote of Cheng et al., is unable to produce any result at all for 2 out of the 6 test cases in its main data table. [13], which was the state of the art when we published in Nature. • Markov et al.: The second “meta-analyzed” paper is an unpublished PDF with no author list [3], which is described as a “separate evaluation” performed by “Google Team 2”, but was in fact co-authored by Markov himself333Markov did not disclose anywhere in his “meta-analysis” that he is an author of one of the two “separate evaluations”. He also omitted his name from the paper’s authors in the references section, and linked only to an anonymous PDF. When questioned on LinkedIn, Markov admitted his authorship, but later deleted the post., though this is not disclosed444Markov also failed to disclose his role as a high-level employee at Synopsys, a company which licenses commercial tools that compete with our open-source method.. This paper did not meet Google’s bar for publication. In 2022, it was reviewed by an independent committee at Google, which determined that “the claims and conclusions in the draft are not scientifically backed by the experiments” [33] and “as the [AlphaChip] results on their original datasets were independently reproduced, this brought the [Markov et al.] RL results into question” [33]. We provided the committee with one-line scripts that generated significantly better RL results than those reported in Markov et al., outperforming their “stronger” simulated annealing baseline. We still do not know how Markov and his collaborators produced the numbers in their paper. Markov’s “meta-analysis” offers one additional source of concern regarding our paper: a “whistleblower” within Google. However, this “whistleblower” admitted to a Google investigator that he had no reason to believe fraud occurred: “he stated that he suspected that the research being conducted by Goldie and Mirhoseini was fraudulent, but also stated that he did not have evidence to support his suspicion of fraud” [24]. In his “meta-analysis”, Markov speculates wildly and without evidence about fraud and scientific misconduct, none of which occurred. Most of Markov’s criticisms are of this form: it does not look to him like our method should work, and therefore it must not work, and any evidence suggesting otherwise is fraud. Nature investigated Markov’s concerns, found them to be entirely without merit, and published an Addendum upholding our work at the conclusion of this process [20]. As an example, in the opening paragraph of his conclusions, Markov states (emphasis his): “In the paper, we find a smorgasbord of questionable practices in ML [26]555Note that Markov’s citation 26 has nothing to do with our paper, though readers may mistakenly believe that it offers corroboration. including irreproducible research practices, multiple variants of cherry-picking, misreporting, and likely data contamination (leakage).” We did not engage in any of these practices, or any other form of scientific misconduct, and Markov provides no evidence for these allegations. Nowhere in Markov’s paper does he describe any form of alleged cherry-picking, let alone multiple variants, nor does he provide evidence. Nor does he describe any form of alleged “misreporting,” or explain what he means by this, or provide evidence. Nor does he provide any evidence of data contamination (leakage), aside from his speculation that it would have improved our results if it had occurred. Many of these allegations appear for the first time in his “Conclusions” section! In an effort to discredit our TPU deployments, Markov also suggests that Google must just be “dogfooding” our method, allowing inferior AlphaChip placements to be used in TPU in order to prop up our research paper. This is untrue, and absurd on its face. Google cares far more about the efficiency of TPU designs – a multi-billion-dollar project that is central to Google’s cloud and AI initiatives – than it does about a research paper666In reality, we had to work for a long time to build enough trust for the TPU team to use our layouts, even after AlphaChip was outperforming human experts on the metrics, and this makes sense – their job is to deliver TPU chips and make them as efficient and reliable as possible, and they cannot afford to take unnecessary risks.. For clarity, we present a timeline of events, including non-confidential deployments777AlphaChip has been deployed in other hardware across Alphabet that we cannot yet disclose.: • Apr 2020: Released arXiv preprint of our Nature paper [4]. • Aug 2020: 10 AlphaChip layouts taped out in TPU v5e. • Jun 2021: Published Nature article [30]. • Sep 2021: 15 AlphaChip layouts taped out in TPU v5p. • Jan 2022 - Jul 2022: Open-sourced AlphaChip [21], after ensuring compliance with export control restrictions and excising internal dependencies. This involved independent replication of the results in our Nature paper by another team at Google. See Section 4. • Feb 2022: Independent committee within Google declined to publish Markov et al. as the data did not support its claims and conclusions [33]. • Oct 2022: 25 AlphaChip layouts taped out in Trillium (latest public TPU). • Feb 2023: Cheng et al. posted on arXiv [9], claiming to perform “massive reimplementation” of our method, despite it being fully open-source. As discussed in Sections 2 and 3, Cheng et al. did not run our method as described in Nature, among other issues. • Jun 2023: Markov released arXiv preprint of his “meta-analysis” [28]. • Sep 2023: Nature posted Editor’s note stating that they are investigating our paper, and initiated second peer review process [30]. • Mar 2024: 7 AlphaChip layouts adopted in Google Axion Processors (ARM-based CPU). • Apr 2024: Nature completed its investigation and post-publication review, and found entirely in our favor, concluding that “the best way forward is to publish an update to the paper in the form of an Addendum (not a ‘Correction’, as we have established that there is little that actually needs correcting).” [44] • Sep 2024: Nature published Addendum upholding our work [20], removed Editor’s note. • Sep 2024: SVP of MediaTek announced that they extended AlphaChip to accelerate development of their most advanced chips [19]. • Nov 2024: Markov republished his “meta-analysis”, though his concerns were already found to be without merit during Nature’s investigation and second peer review process. In brief, Markov’s paper contains no original data, and is a “meta-analysis” of just two papers. The first is presented with no author list (though Markov was an author), was never published, made claims that were not scientifically backed by the data, and could not be reproduced. The second, Cheng et al., is the only substantive content in Markov’s “meta-analysis”, so we devote the remainder of this paper to describing significant issues in its purported reproduction of our method."
https://arxiv.org/html/2411.09874v1,A Hybrid Artificial Intelligence System for Automated EEG Background Analysis and Report Generation,"Electroencephalography (EEG) plays a crucial role in the diagnosis of various neurological disorders. However, small hospitals and clinics often lack advanced EEG signal analysis systems and are prone to misinterpretation in manual EEG reading. This study proposes an innovative hybrid artificial intelligence (AI) system for automatic interpretation of EEG background activity and report generation. The system combines deep learning models for posterior dominant rhythm (PDR) prediction, unsupervised artifact removal, and expert-designed algorithms for abnormality detection. For PDR prediction, 1530 labeled EEGs were used, and the best ensemble model achieved a mean absolute error (MAE) of 0.237, a root mean square error (RMSE) of 0.359, an accuracy of 91.8% within a 0.6 Hz error, and an accuracy of 99% within a 1.2 Hz error. The AI system significantly outperformed neurologists in detecting generalized background slowing (p=0.02; F1: AI 0.93, neurologists 0.82) and demonstrated improved focal abnormality detection, although not statistically significant (p=0.79; F1: AI 0.71, neurologists 0.55). Validation on both an internal dataset and the Temple University Abnormal EEG Corpus showed consistent performance (F1: 0.884 and 0.835, respectively; p=0.66), demonstrating generalizability. The use of large language models (LLMs) for report generation demonstrated 100% accuracy, verified by three other independent LLMs. This hybrid AI system provides an easily scalable and accurate solution for EEG interpretation in resource-limited settings, assisting neurologists in improving diagnostic accuracy and reducing misdiagnosis rates.","\IEEEPARstart Electroencephalography (EEG) plays a crucial role in clinical neurology, aiding in the diagnosis of various neurological disorders such as epilepsy, dementia, brain lesions, and localized cerebral structural abnormalities [8], [26]. With the advent of an aging society, the incidence of neurodegenerative diseases and cerebral structural disorders, including dementia, stroke, traumatic subdural hemorrhage, and brain tumors, is gradually increasing [25]. In small hospitals or neurological clinics, EEG serves as a first-line diagnostic tool due to its relative low cost, non-invasiveness, absence of radiation exposure, and repeatability. The interpretation of EEG background activity, which includes PDR, symmetry, and the presence of focal slow waves, is an essential method for assessing cerebral cortical function and facilitates the diagnosis of these disorders [1]. In healthcare facilities lacking radiological equipment such as Computed Tomography or Magnetic Resonance Imaging, EEG becomes even more important. However, a significant proportion of hospitals or clinics lack the capability for quantitative analysis and assisted interpretation of EEG features and reports often rely on the experience of clinicians. Low inter-rater agreement among neurologists interpreting clinical EEG is a significant problem and may lead to inconsistencies in diagnosis and treatment decisions [17]. The SCORE-AI system[17] uses an artificial intelligence (AI) approach for automatic interpretation of clinical EEG, providing a solution to these challenges in EEG interpretation. The model employs a convolutional neural network architecture trained on a large dataset of 30,493 highly annotated EEGs to classify recordings as normal or into clinically relevant abnormal categories. One limitation is that the SCORE-AI system requires a large amount of training data labeled by experts, which may not be feasible for small hospitals and clinics. Furthermore, the complexity of EEG labeling is relatively high, and the SCORE program may not be easily integrated into existing EEG and hospital information systems, making it less suitable for small healthcare institutions. Additionally, the presence of numerous artifacts in clinical EEG and the inability to achieve the same quality as research-grade EEG pose significant challenges for EEG preprocessing. Currently, there is no single unsupervised and efficient method that can effectively address these issues[27]. Furthermore, structured EEG features were difficult to obtain in small institutions because commercial systems generally do not provide access to such data. Considering these challenges, this study aims to develop an AI-based automatic reporting algorithm for EEG background, which we refer to as the Hybrid AI system, using a smaller dataset based on routine clinical EEG for training and focusing on EEG background analysis. The system employs a hybrid model that includes anomaly detection, deep learning, guidelines, and expert heuristics. In clinical practice, a report needs to be written following EEG analysis. To the best of our knowledge, many experts still write EEG reports manually, either through customized phrase input in the hospital information system or by using word processing software to enter the report form, especially in smaller institutes. With the advancements in large language models, an increasing number of researchers have started adopting AI-assisted generation in medical fields[35], [20]. To date, there is no existing method for AI-generated EEG report content. This study also pioneers the use of large language models and EEG features to assist in generating EEG report content. Figure 1: Workflow of the proposed hybrid AI system for automated EEG background interpretation and report generation. Figure 2: Neighbor electrode example and artifact repair using the neighbor-HBOS method. (A) F3 and its neighboring electrodes: Fp1, F7, and C3. (B) EEG signal before artifact repair. (C) EEG signal after artifact repair using the neighbor-HBOS method."
https://arxiv.org/html/2411.10446v1,VeriGraph: Scene Graphs for Execution Verifiable Robot Planning,"Recent advancements in vision-language models (VLMs) offer potential for robot task planning, but challenges remain due to VLMs’ tendency to generate incorrect action sequences. To address these limitations, we propose VeriGraph, a novel framework that integrates VLMs for robotic planning while verifying action feasibility. VeriGraph employs scene graphs as an intermediate representation, capturing key objects and spatial relationships to improve plan verification and refinement. The system generates a scene graph from input images and uses it to iteratively check and correct action sequences generated by an LLM-based task planner, ensuring constraints are respected and actions are executable. Our approach significantly enhances task completion rates across diverse manipulation scenarios, outperforming baseline methods by 58\% for language-based tasks and 30\% for image-based tasks.","For robots to be able to solve complex manipulation problems in the real world, they need to understand the physical world around them, including object locations and relationships between objects in the scene. Humans intuitively understand spatial relationships between objects in the world and can use this understanding to develop efficient and executable plans to complete tasks. Consider the example of organizing a cluttered room. Humans can quickly understand which objects are out of place based on their understanding of how objects are supposed to relate to each other. For example, it seems intuitive that a book should be on a shelf, not on a cup. Robots struggle to perceive the world around them the way humans do. Additionally, physical constraints in the real world restrict the order in which actions can be executed. For example, if a glass cup is on a book, which is on a desk, the robot must pick up the cup first and place it on the desk before picking up the book. Because of these constraints, robots need to understand the relationships between objects in the scene. If the robot does not understand that the cup is on the book, it might not factor that into its planning and may try to pick up the book first, which could result in the cup falling and breaking. Figure 1: VeriGraph is able to utilize an initial scene image and a reference image which may or may not be from the same setting. Using the two images, our approach generates the corresponding scene graphs. Using a VLM as the planner along with execution-verifiability, we generate and execute a plan using the robot. Recent advances in large language models (LLMs) and vision-language models (VLMs) have opened up new possibilities for robot task planning [1, 2]. These models demonstrate impressive reasoning capabilities and world knowledge. Prior work [3, 4, 5] used LLMs to generate Planning Domain Definition Language (PDDL), which can be used by classical planners to create a task plan. While the results have been promising, PDDL is inherently restrictive and does not generalize well [6, 7]. Other lines of work use VLMs to generate high-level task plans directly using images [8, 9, 10]. While the results are remarkable, relying on raw pixel data for complex manipulation can be suboptimal. Pixel-level information is often noisy and may contain extraneous details irrelevant to the high-level planning task. To address the scene representation issue, we propose VeriGraph, a novel approach that utilizes scene graphs as an intermediate representation for robot task planning. Scene graphs have proven particularly valuable in robotic task planning [11, 12, 13, 14]. Their structured nature enables the abstraction of object-level details into symbolic graphs, making them robust to noise while retaining essential information about object interactions. For example, a scene graph might encode that a ""cup is on the table"" or a ""spoon is inside the cup,"" providing a framework for reasoning about actions such as moving the spoon or rearranging the objects in the scene. This abstraction is especially important for tasks where the physical appearance of individual objects is not important. One such task is using a reference scene to arrange objects in another scene to look like the reference scene. Because of the scene graph representation, VeriGraph can solve this task significantly better than methods that rely on raw pixel data. Despite their powerful nature, VLMs are prone to failures in planning, often requiring multiple iterations of prompting to achieve the correct result [15]. Approaches such as [10] attempt to solve this problem by inserting a human directly into the loop. However, this is time-consuming and requires constant human supervision. To address the plan verification and correcting problem we add an iterative planning component to VeriGraph. The structured nature of scene graphs allows VeriGraph to represent each action in the plan as graph operations. For example, moving an object from one location to another can be represented with an edge manipulation operation. This representation allows VeriGraph to quickly check for constraint violations and iterate with the task planner to generate valid action sequences. This setup, shown in Figure 2, allows for more accurate and robust planning. For some tasks, it might be easier to specify the goal state via language instructions, while for some tasks a reference image is sufficient. To be able to support both types of task specification, VeriGraph supports flexible goal specification, allowing manipulation tasks to be defined through either target scene images or natural language instructions. The system can generate goal scene graphs from these inputs, providing a unified planning framework across different task specifications. Note that for reference images, VeriGraph does not require the exact same scene, only contextually similar scene (e.g., refer to Figure 1). This versatility makes VeriGraph applicable to various real-world scenarios where goals may be communicated in different formats. We show that VeriGraph beats existing methods that rely on raw pixels as input to the task planner while being execution-verifiable. Our main contributions are as follows: • We present a modular and fast approach that uses scene graphs to enhance planning with LLMs, improving the understanding of spatial relationships and constraints. • We propose an iterative planning and verification mechanism that uses scene graphs to represent and verify action sequences, enhancing the system’s ability to identify and correct constraint violations without human intervention. • We utilize VLMs to generate goal scene graphs based on a reference image or language instruction to create a unified goal specification method."
https://arxiv.org/html/2411.10436v1,Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization,"Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.","Large Language Models (LLMs) have been verified in various field OpenAI (2024); Dubey et al. (2024); Sun et al. (2024), while they encounter challenges such as hallucination. Multimodal Large Language Models (MLLMs) are also known to hallucinate Bai et al. (2024). Specifically, they often produce unfaithful content that does not align with the visual input, which undermines their reliability and practicality, particularly in critical applications such as autonomous driving Cui et al. (2024) or medical tasks Liu et al. (2023a). Hence, addressing MLLM hallucination (M-hallu) is essential. Recent efforts have aimed at mitigating M-hallu through various approaches, including inference-stage strategies like contrastive decoding Leng et al. (2024), and post-hoc corrections that employ external visual models to refine responses Yin et al. (2023). While these methods are simple and training-free, they do not fundamentally enhance the model’s intrinsic capabilities. Meanwhile, some pioneer preference optimization methods like Direct Preference Optimization (DPO) Rafailov et al. (2024) have been introduced, which encourage the model to learn from the comparisons between positive and negative samples, alleviating hallucinations Zhao et al. (2023); Pi et al. (2025). However, most current methods cannot deliver consistent improvements across all types of MLLM hallucination tasks (e.g., VQA and captioning tasks, as shown in our experiments of Table 1). Additionally, it appears that the model’s improvement on specific tasks is closely related to the format of the training data. For instance, the DPO data of SeVa Zhu et al. (2024) primarily consists of VQA, which explains its strong performance on VQA-related hallucination evaluation. However, its results on captioning tasks are relatively unsatisfactory. Moreover, these methods do not explicitly consider diverse sources of M-hallu. Hence, we argue that if we focus on mitigating multimodal hallucinations, we should be able to address diverse types of hallucination causes and tasks, and design hallucination-targeted preference pairs for DPO accordingly. Our goal is to comprehensively alleviate all multimodal hallucination problems, including both discriminative tasks (e.g., VQA) and generative tasks (e.g., image captioning). Different from the hallucinations in LLMs, M-hallu primarily arises from the following three aspects: (1) Insufficient visual capability: This occurs when the MLLM’s visual encoder lacks the necessary strength, being distracted by relatively unimportant visual information, leading to hallucinations; (2) Incapable long-context generation: We observe that hallucinations become more pronounced as the generated content grows longer, similar to long-range forgetting, which needs to be addressed in practical applications; (3) Multimodal conflicts: Multimodal conflicts frequently arise in real-world scenarios due to the inevitable noises in texts and images. MLLMs are more prone to hallucinations with conflicting information existing between text and image Liu et al. (2024b). To address the aforementioned challenges, we propose Hallucination-targeted Direct Preference Optimization (HDPO) to mitigate M-hallu. Our approach constructs hallucination-targeted preference pairs, specifically designed to address various forms and causes of hallucinations. Specifically, we design three types of DPO data reflecting the corresponding hallucination causes as follows: (1) For insufficient visual capability, during the model’s autoregressive decoding, we preserve only some visual tokens with the lowest attention scores to produce targeted negative responses that reflect incorrect visual information distraction, urging MLLMs to pay attention to more effective visual information. (2) For incapable long context generation, we specifically select positive examples from high-quality long-form captions, while creating negative examples where the latter part of the response deviates from the image content, simulating long-form hallucinations. (3) For multimodal conflicts, we add conflicting information with images into prompts to generate negative examples. We provide positive and negative pairs with questions featuring conflicting prefixes to train the model to correctly respond to the question even containing conflicting information. We conduct extensive experiments to evaluate our approach across various types of M-hallu tasks. The results demonstrate that our HDPO framework achieves the overall best performance in effectively mitigating MLLM hallucinations on various tasks. Our contributions are summarized as follows: • We analyze three key causes behind MLLM hallucinations from visual capability, long-context generation, and multimodal conflicts aspects, offering valuable insights to guide future advancements. • Based on these analyses, we propose a novel HDPO, aiming to jointly address all types of M-hallu tasks. To the best of our knowledge, we are the first to adopt hallucination-targeted DPO from diverse aspects with our novel DPO data construction strategies. • Through extensive experiments on different datasets, HDPO demonstrates consistent improvements in all types of M-hallu tasks."
https://arxiv.org/html/2411.10422v1,Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash,"Large Language Models (LLMs) have shown impressive capabilities in complex tasks and interactive environments, yet their creativity remains underexplored. This paper introduces a simulation framework utilizing the game Balderdash to evaluate both the creativity and logical reasoning of LLMs. In Balderdash, players generate fictitious definitions for obscure terms to deceive others while identifying correct definitions. Our framework enables multiple LLM agents to participate in this game, assessing their ability to produce plausible definitions and strategize based on game rules and history. We implemented a centralized game engine featuring various LLMs as participants and a judge LLM to evaluate semantic equivalence. Through a series of experiments, we analyzed the performance of different LLMs, examining metrics such as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The results provide insights into the creative and deceptive capabilities of LLMs, highlighting their strengths and areas for improvement. Specifically, the study reveals that infrequent vocabulary in LLMs’ input leads to poor reasoning on game rules and historical context.111https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash","Large Language Models (LLMs) have recently been employed as agents in various complex tasks, showcasing their potential in dynamic, interactive environments (Dorbala et al., 2024; Singh et al., 2024). This has led to a growing interest in LLM-based multi-agent systems (LLM-MA), particularly within the realm of gaming (Mukobi et al., 2024; Xu et al., 2023). Games offer a structured yet flexible platform to analyze and understand LLM behavior under diverse scenarios (Light et al., 2023). Currently, LLMs are typically evaluated through static tasks (Lee et al., 2023; Zhao et al., 2024; Gómez-Rodríguez and Williams, 2023). Traditional games like Avalon (Wang et al., 2023) and Werewolf (Xu et al., 2024) have also been used to benchmark LLMs, focusing on logical reasoning and strategic interaction. These games require players to engage in deception, deduction, and negotiation, providing valuable insights into LLMs’ decision-making processes. However, these studies often overlook the assessment of creativity. To address this gap, we introduce a simulation framework for the game Balderdash. In this game, players generate plausible yet fictitious definitions for obscure terms, aiming to deceive other players while identifying the correct definitions. We argue that Balderdash can be used to evaluate both the creativity and logical reasoning of LLMs, challenging the models to balance these two crucial aspects and providing a comprehensive assessment of their capabilities. In this paper, we aim to assess the creativity of LLMs by evaluating their ability to generate plausible definitions for obscure words in Balderdash. We will further examine their logical reasoning skills by observing how effectively they deceive opponents and identify correct definitions in the context of the game. Finally, we will investigate the performance of these models in a multi-agent setting where both creativity and logical deduction are crucial for success."
https://arxiv.org/html/2411.10416v1,Towards Automatic Evaluation of Task-Oriented Dialogue Flows,"Task-oriented dialogue systems rely on predefined conversation schemes (dialogue flows) often represented as directed acyclic graphs. These flows can be manually designed or automatically generated from previously recorded conversations. Due to variations in domain expertise or reliance on different sets of prior conversations, these dialogue flows can manifest in significantly different graph structures. Despite their importance, there is no standard method for evaluating the quality of dialogue flows. We introduce FuDGE (Fuzzy Dialogue-Graph Edit Distance), a novel metric that evaluates dialogue flows by assessing their structural complexity and representational coverage of the conversation data. FuDGE measures how well individual conversations align with a flow and, consequently, how well a set of conversations is represented by the flow overall. Through extensive experiments on manually configured flows and flows generated by automated techniques, we demonstrate the effectiveness of FuDGE and its evaluation framework. By standardizing and optimizing dialogue flows, FuDGE enables conversational designers and automated techniques to achieve higher levels of efficiency and automation.","One of the most promising applications of Conversational AI lies in Customer Service Automation, where task-oriented dialogue systems aim to address customer concerns effectively without human intervention. These systems often rely on dialogue flows—structured conversation schemes—to retrieve appropriate information from knowledge bases or back-end systems. Over the years, frameworks like Dialogflow CX222https://cloud.google.com/dialogflow/cx/docs/basics, Rasa333https://rasa.com, and Amazon Lex444https://aws.amazon.com/lex/ have facilitated the creation of task-oriented dialogue systems by leveraging flows comprising user intents, agent actions, and other metadata. As shown in Figure 1, dialogue flows define possible paths a customer and agent can take during a conversation, enabling structured automation. Figure 1: An illustration of a dialogue flow (left) that is used to configure a task-oriented dialogue agent for a fictitious company named ”Everyclothing.” The chat widget (right) illustrates a customer asking about canceling their order. Dialogue flows are typically handcrafted by domain experts through an iterative process involving historical data analysis. This iterative process depends on the designer’s expertise, the quality of the data, and the time invested, resulting in dialogue flows or directed acyclic graph (DAG) that vary significantly in flow attributes such as the number of user intents, intent sequences, and graph paths. Alternatively dialogue flows can be can be generated using automated flow discovery algorithms, which leverage historical dialogue data. These algorithms rely on hyperparameters that significantly influence the resulting flow structure. However, evaluation of flows with respect to historical dialogues has been a relatively unexplored area of NLP. Previous work has predominantly focused on improving the quality of task-oriented dialog agents through better intent discovery (Zhang et al. 2022; Shi et al. 2018; Perkins and Yang 2019) which is often categorized as a sub-problem in the dialogue flow discovery process. The domain of flow discovery from historical human-human dialogues has had relatively sparse research despite being crucial for building task-oriented dialogue agents. There is a need to automatically evaluate the quality of the dialogue flows with respect to the historical dialogue transcripts used to generate them to deliver a consistent baseline, better versioning and tracking progress of flows, and corresponding automation with time. To that end, we introduce a novel evaluation framework to assess the flow discovery algorithm or to guide the human conversation designer while building the flows. Our evaluation framework enables comparison between the flows generated from the same corpus by assigning a score to a dialogue flow. A good flow should cover the important and representative conversations from the corpus while simultaneously being as compressed as possible. Note that there is a trade-off between the degree of compression and the amount of information being lost. A graph containing all conversations potentially covers the entire corpus, but it is not beneficial for designing a dialogue system because it will strictly imitate historical conversations and will not generalize well to unseen dialogues. On the other hand, a simple graph start -> Hello -> Goodbye is still a dialogue flow, but it loses most of the information about the corpus. Our evaluation framework takes both of these aspects into consideration. As the core of our evaluation framework, we propose FuDGE, Fuzzy Dialogue-Graph Edit distance, an algorithmic way of computing the distance between a specific conversation and a given dialogue flow paths. FuDGE allows us to measure how well the dialogues are represented and covered by a dialogue flow. We combine the FuDGE score with the complexity of the DAG representing the flow and produce Flow-F1 (FF1) score that captures the trade-off between the compression and complexity. Overall, our contributions are as follows: 1. We introduce a novel dialogue flow evaluation framework that provides a metric to quantify the goodness of a dialogue flow generated from a dialogue corpora, taking into consideration the inherent trade-off between flow complexity and information contained in the flows 2. We propose FuDGE distance, an efficient edit-distance metric between a dialogue flow and a conversation that can independently be used in any dialogue evaluation task, including zero-shot dialogue generation from predefined schemes. We show that FuDGE can effectively separate within-task versus out-of-task conversations for a given set of dialogues. 3. We demonstrate that FF1 score can help hyperparameter tuning, ranking, and pruning the dialogue flows to identify the most optimal flows for a given dialogue corpus."
https://arxiv.org/html/2411.10411v1,Repurposing Stable Diffusion Attention for Training-Free Unsupervised Interactive Segmentation,"Recent progress in interactive point prompt based Image Segmentation allows to significantly reduce the manual effort to obtain high quality semantic labels. State-of-the-art unsupervised methods use self-supervised pre-trained models to obtain pseudo-labels which are used in training a prompt-based segmentation model. In this paper, we propose a novel unsupervised and training-free approach based solely on the self-attention of Stable Diffusion. We interpret the self-attention tensor as a Markov transition operator, which enables us to iteratively construct a Markov chain. Pixel-wise counting of the required number of iterations along the Markov chain to reach a relative probability threshold yields a Markov-iteration-map, which we simply call a Markov-map. Compared to the raw attention maps, we show that our proposed Markov-map has less noise, sharper semantic boundaries and more uniform values within semantically similar regions. We integrate the Markov-map in a simple yet effective truncated nearest neighbor framework to obtain interactive point prompt based segmentation. Despite being training-free, we experimentally show that our approach yields excellent results in terms of Number of Clicks (NoC), even outperforming state-of-the-art training based unsupervised methods in most of the datasets.","The goal of point prompt based interactive image segmentation is to obtain a high-quality segmentation from limited user interaction in the form of clicking. Prompt based image segmentation gained popularity lately due to large supervised foundation models [15, 28]. In this paper, we focus on unsupervised methods, where no segmentation labels are used at all in the design and/or training of the models. Most recent approaches rely on self supervised backbones like ViT [6], trained either by DINO [2] or MAE [12] based self-supervised techniques. On the other hand, Stable Diffusion (SD) [29] has been used for many different computer vision applications such as monocular depth estimation [14], semantic segmentation [38], object detection [3] and image classification [17], often resulting in state-of-the-art solutions. Inspired by DiffSeg [38], we utilize SD’s self-attention maps in our training-free interactive segmentation approach. The main challenges are that attention maps are noisy and don’t distinguish between instances. To overcome the mentioned challenges, we interpret the self-attention tensors as a Markov transition operator, where the repeated application of the transition forms a Markov chain. We propose a novel Markov-iteration-map or simply Markov-map, where each pixel counts the number of iterations required to obtain a specific probability value. We show that the proposed Markov-map has less noise, the semantic boundaries are sharper and the semantic regions within Markov-maps are more uniformly distributed. Native Markov-maps do not distinguish between instances. Therefore, we further improve Markov-maps with a flood fill approach, which suppresses local minima, to enable instance based segmentation. Finally, we obtain Markov-maps of each prompt point and combine them with a truncated nearest neighbor approach to enable multi-prompt point interactive segmentation. Surprisingly, despite being training-free, we significantly improve the state-of-the-art in terms of Number of Clicks (NoC) and even surpass training based unsupervised approaches in most of the datasets. Our main contributions are: • We introduce Markov-Map Nearest Neighbor (M2N2), the first attention-based unsupervised training-free multi-point prompt segmentation framework. • We propose Markov-maps in order to improve the semantic information. • We enable instance aware Markov-maps by utilizing a modified flood fill approach. • We introduce a truncated nearest neighbor approach to combine multiple point prompts. • We conduct extensive experiments and achieve state-of-the-art results, surpassing even unsupervised training-based methods. Figure 1: M2N2 framework overview. We perform a single denoising step of the input image with Stable Diffusion 2 to obtain attention tensors. The tensors are aggregated and utilized to obtain a Markov-map M_{i} for each prompt point. The final segmentation is the result of a truncated nearest neighbor of scaled Markov-maps M_{i} as a measure of semantic distance for each prompt point. The green and red areas in the scaled Markov-maps denote regions where the distance is less or equal to the global background threshold. In this visualization, components in blue contain adjustable hyperparameters."
https://arxiv.org/html/2411.10397v1,"Features that Make a Difference: 
Leveraging Gradients for Improved Dictionary Learning","Sparse Autoencoders (SAEs) are a promising approach for extracting neural network representations by learning a sparse and overcomplete decomposition of the network’s internal activations. However, SAEs are traditionally trained considering only activation values and not the effect those activations have on downstream computations. This limits the information available to learn features, and biases the autoencoder towards neglecting features which are represented with small activation values but strongly influence model outputs. To address this, we introduce Gradient SAEs (g-SAEs), which modify the k-sparse autoencoder architecture by augmenting the TopK activation function to rely on the gradients of the input activation when selecting the k elements. For a given sparsity level, g-SAEs produce reconstructions that are more faithful to original network performance when propagated through the network. Additionally, we find evidence that g-SAEs learn latents that are on average more effective at steering models in arbitrary contexts. By considering the downstream effects of activations, our approach leverages the dual nature of neural network features as both representations, retrospectively, and actions, prospectively. While previous methods have approached the problem of feature discovery primarily focused on the former aspect, g-SAEs represent a step towards accounting for the latter as well.","Sparse Autoencoders (SAEs) have emerged as a promising method for interpreting neural networks, aiming to recover a model’s features via dictionary learning (Bricken et al., 2023; Cunningham et al., 2023; Templeton et al., 2024). While there is no universally accepted definition of features, they are generally understood to be the atomic units of language models’ computation, possessing the quality of monosemanticity as both representations and causal intermediates. The success of SAEs in identifying directions in language model activation space that are causally relevant and interpretable provides evidence that a significant fraction of the internal activations of language models are sparse, linear combinations of vectors which are each oriented in a direction corresponding to a feature of the model. (Park et al., 2024b, a). The elements of the SAE-learned dictionary corresponding to these directions we call latents. However, SAEs are trained to encode input activations such that the reconstruction loss over many tokens from a training corpus is minimized. This has raised some concerns that SAEs may not learn latents corresponding to the features of a model, but rather the features of a dataset (Braun et al., 2024; Dooms and Wilhelm, 2024). That is, SAEs might give the illusion of interpretability by learning latents based on frequent and distinct concepts in the training corpus, rather than learning latents because they correspond to features that play an important and distinct role in the model’s decision-making process. We would like a better guarantee that our interpretability tools discover the features of a model, rather than primarily reflecting the training data. Additionally, if one is interested in interpretability as a means to exert more fine-grained control over the behavior of models, it would be desirable to have interpretability tools that are biased towards uncovering the features that are most responsible for a model’s output. Templeton et al. (2024) speculate that current Large Language Models (LLMs) may represent orders of magnitude more features than the size of the dictionaries of the largest SAEs trained to date. The limited capacity of SAEs incentivizes them to neglect features that strongly affect model outputs if overlooking them usually results in small reconstruction errors (Braun et al., 2024). Figure 1: The setup of g-SAE training. Here \mathcal{L}(\mathbf{x}) is the function mapping the residual stream activation at the layer the SAE was trained on, to the predictive cross entropy loss it yields. The red dotted line denotes backpropagation. These desiderata motivate our approach, which aims to address the challenges above by connecting dictionary learning more closely with model outputs. In this paper we show: 1. Distance in LLM activation space correlates poorly with degree of downstream effect, but gradients can yield accurate first order predictions. We show that there is only a weak correlation between the norm of a perturbation vector and it’s effect on LLM downstream outputs, as measured by change in cross-entropy loss. In contrast to the norm alone, the first order approximation given by gradients of activations with respect to loss strongly predict downstream effects for perturbations of moderate size. This suggests that reconstruction loss minimization doesn’t directly incentivize the SAE to learn reconstructions that minimize difference in effect. 2. In SAE training, incorporating gradients produces improvements on various Pareto frontiers defined by popular SAE architectures, and results in more complete use of the SAE’s capacity. For a given expansion size, using a gradient-aware TopK activation function leads to improvements in the minimization of difference in model loss when reconstructed outputs are propagated downstream. It also leads to fewer permanently inactive units of the SAE hidden layer. 3. g-SAE latents provide superior steering with no cost to interpretability. g-SAEs recover latents that appear to be as interpretable as existing architectures, and on average are more influential to the specific logits they are associated with, when used as steering vectors in arbitrary contexts."
https://arxiv.org/html/2411.10389v1,Deep Learning for Micro-Scale Crack Detection on Imbalanced Datasets Using Key Point Localization,"Internal crack detection has been a subject of focus in structural health monitoring. By focusing on crack detection in structural datasets, it is demonstrated that deep learning (DL) methods can effectively analyse seismic wave fields interacting with micro-scale cracks, which are beyond the resolution of conventional visual inspection. This work explores a novel application of DL based key point detection technique, where cracks are localized by predicting the coordinates of four key points that define a bounding region of the crack. The study not only opens new research directions for non-visual applications but also effectively mitigates the impact of imbalanced data which poses a challenge for previous DL models, as it can be biased toward predicting the majority class (non-crack regions). Popular DL techniques, such as the Inception blocks are used and investigated. The model shows an overall reduction in loss when applied to micro-scale crack detection and is reflected in the lower average deviation between the location of actual and predicted cracks, with an average IOU being 0.511 for all micro cracks (>0.00 µm) and 0.631 for larger micro cracks (>4 µm).","Structural health monitoring is a critical area where the safety of infrastructures depends on the detection of internal cracks, which are not visible on the surface. These hidden cracks pose a significant challenge because detecting them requires a combination of domain expertise in both deep learning and material science to effectively extract the relevant features (Zhou et al., 2023; Liu and Zhang, 2019). Moreover, processing the numerical data generated demands high computational power, as the task involves analyzing wave propagation through materials and identifying changes caused by cracks. The ability to automate and enhance this detection process is vital for ensuring structural safety in industries such as civil engineering, aerospace, and manufacturing (Cha et al., 2024; Chen and Jahanshahi, 2018). In recent years, deep neural networks have significantly advanced the field of computer vision, particularly in areas like object detection(Krizhevsky et al., 2017). Traditionally, these networks were designed to grow deeper by adding more layers to capture increasingly complex patterns in data. However, this approach faces challenges such as vanishing gradients, increased computational costs, and difficulties in training. As a result, researchers began exploring the idea of wide networks, which emphasize increasing the number of neurons in each layer rather than stacking more layers. This shift has proven especially beneficial for tasks like object detection, where wider networks can capture more detailed features across a broader spatial rangeZagoruyko and Komodakis (2017), improving performance without the complications that come with deeper architectures (He et al., 2015). In this paper, we explore the transition from deep to wide convolutional networks in the context of object detection, specifically for crack detection using numerical data. Wide networks, with their ability to capture diverse features, offer advantages in handling spatial correlations across larger regions of the input, leading to more accurate detection results (Huang et al., 2018; Xie et al., 2017). Unlike traditional deep learning models in computer vision, we successfully detected patterns in numerical wave propagation data, demonstrating that our model can effectively identify and locate cracks. To the best of our knowledge, no prior work has applied this approach to crack detection, making this the first study to use an object detection model specifically for crack detection from numerical data. This work is a significant first step, proving that the model can detect cracks and accurately determine their location. However, due to the limited availability of data, we were unable to evaluate the model against samples with multiple or more complex cracks. In future work (Section Future Work), we aim to test the model with samples containing multiple cracks and more intricate crack patterns. This paper opens the door to further research on improving crack detection using wide convolutional networks and addressing more complex scenarios in structural health monitoring."
https://arxiv.org/html/2411.10385v1,"Low-Latency Task-Oriented Communications with Multi-Round, Multi-Task Deep Learning","In this paper, we address task-oriented (or goal-oriented) communications where an encoder at the transmitter learns compressed latent representations of data, which are then transmitted over a wireless channel. At the receiver, a decoder performs a machine learning task, specifically for classifying the received signals. The deep neural networks corresponding to the encoder-decoder pair are jointly trained, taking both channel and data characteristics into account. Our objective is to achieve high accuracy in completing the underlying task while minimizing the number of channel uses determined by the encoder’s output size. To this end, we propose a multi-round, multi-task learning (MRMTL) approach for the dynamic update of channel uses in multi-round transmissions. The transmitter incrementally sends an increasing number of encoded samples over the channel based on the feedback from the receiver, and the receiver utilizes the signals from a previous round to enhance the task performance, rather than only considering the latest transmission. This approach employs multi-task learning to jointly optimize accuracy across varying number of channel uses, treating each configuration as a distinct task. By evaluating the confidence of the receiver in task decisions, MRMTL decides on whether to allocate additional channel uses in multiple rounds. We characterize both the accuracy and the delay (total number of channel uses) of MRMTL, demonstrating that it achieves the accuracy close to that of conventional methods requiring large numbers of channel uses, but with reduced delay by incorporating signals from a prior round. We consider the CIFAR-10 dataset, convolutional neural network architectures, and AWGN and Rayleigh channel models for performance evaluation. We show that MRMTL significantly improves the efficiency of task-oriented communications, balancing accuracy and latency effectively.","In the rapidly advancing field of NextG communication systems, there is an increasing focus on task-oriented (or goal-oriented) communications. This approach is gaining prominence as it addresses the specific needs of various applications by ensuring that the transmission process is aligned with the ultimate objective of the task at hand [1, 2, 3, 4, 5, 6, 7, 8, 9]. Unlike traditional communication paradigms that focus on delivering raw data, task-oriented communications (TOC) aims to transmit only the information necessary to accomplish a specific task. Deep learning plays a crucial role in optimizing the encoding and decoding processes for TOC, allowing for efficient and effective transmission of information that directly contributes to the task’s success. By leveraging deep learning-driven TOC, NextG communication systems can achieve significant improvements in both performance and resource utilization [10, 11, 12], making them well-suited for the demands of modern applications such as the Internet of Things (IoT), augmented reality/virtual reality (AR/VR), and vehicle-to-everything (V2X) network systems. In IoT networks, sensors generate vast amounts of data that need to be processed and analyzed to make real-time decisions, such as in smart cities and industrial automation. TOC can significantly reduce the communication overhead by transmitting only the essential information required for decision-making, rather than the raw sensor data. Similarly, in AR/VR applications, low latency and high accuracy are critical to delivering immersive experiences. TOC can help achieve this by optimizing the transmission of visual and sensory data to meet the application’s specific needs. In V2X systems, vehicles need to communicate with each other and with infrastructure to ensure safe and efficient transportation. TOC can enhance these interactions by focusing on the transmission of critical information, such as collision warnings and traffic updates, thereby improving response times and reducing network congestion. One of the primary challenges in TOC is balancing task accuracy and latency objectives and requirements. To that end, the age of task information for TOC was studied in [13]. Achieving high accuracy often requires transmitting a large amount of data, which can lead to increased delay (measured by the number of channel uses) and higher bandwidth usage. Conversely, minimizing delay and bandwidth usage can compromise accuracy. This accuracy-delay tradeoff is a significant hurdle that needs to be addressed to realize the full potential of TOC. We propose a novel multi-round, multi-task learning (MRMTL) approach to address this challenge by dynamically updating the number of channel uses in iterative transmissions of TOC. MRMTL involves an encoder at the transmitter that learns compressed latent representations of input data (e.g., images), which are transmitted over a wireless channel. At the receiver, a decoder performs a machine learning task, specifically classifying the received signals. MRMTL is different from the autoencoder-based communications, where the typical setting has the source-coded data symbols as the input and the reconstruction of those symbols as the output [14]. On the other hand, MRMTL starts with (raw) input data and performs a machine learning task such as classification. The deep neural networks (DNNs) corresponding to the encoder-decoder pair are jointly trained, considering both channel and data characteristics to achieve high task accuracy with minimal channel uses. MRMTL introduces a dynamic update mechanism where the transmitter incrementally sends an increasing number of encoded samples over the channel. The receiver utilizes signals from a previous round to enhance task performance, rather than relying solely on the latest transmissions. The multi-round process utilizes multi-task learning that jointly optimizes accuracy across multiple rounds, with each configuration treated as a distinct task performed with a different number of channel uses. When the receiver’s confidence in the task decisions is low, it can then allocate additional channel uses to improve task accuracy. We demonstrate that MRMTL achieves the accuracy of conventional methods with single-round, single-task learning (SRSTL) for TOC that requires a large number of channel uses, but with significantly reduced delay by incorporating signals from a prior round. To evaluate MRMTL, we consider the CIFAR-10 dataset as the input, convolutional neural network (CNN) architectures as the DNNs, and AWGN and Rayleigh channel models. Our results show that MRMTL significantly improves the efficiency of TOC, effectively balancing accuracy and delay. This study represents a significant step forward in the development of efficient and effective TOC systems for NextG networks. The MRMTL approach can be extended to incorporate semantic communications [15] and integrated sensing and communications [16, 17] by adding tasks of reconstruction and sensing, respectively. The remainder of the paper is organized as follows. Section II describes SRSTL for TOC. Section III presents MRMTL for TOC. Section IV introduces the MRMTL’s process of dynamic initiation of multiple rounds in TOC. Figure 1: System model of SRSTL for TOC."
https://arxiv.org/html/2411.10371v1,"A Survey of Event Causality Identification: Principles, Taxonomy, Challenges, and Assessment","Event Causality Identification (ECI) has become a crucial task in Natural Language Processing (NLP), aimed at automatically extracting causalities from textual data. In this survey, we systematically address the foundational principles, technical frameworks, and challenges of ECI, offering a comprehensive taxonomy to categorize and clarify current research methodologies, as well as a quantitative assessment of existing models. We first establish a conceptual framework for ECI, outlining key definitions, problem formulations, and evaluation standards. Our taxonomy classifies ECI methods according to the two primary tasks of sentence-level (SECI) and document-level (DECI) event causality identification. For SECI, we examine feature pattern-based matching, deep semantic encoding, causal knowledge pre-training and prompt-based fine-tuning, and external knowledge enhancement methods. For DECI, we highlight approaches focused on event graph reasoning and prompt-based techniques to address the complexity of cross-sentence causal inference. Additionally, we analyze the strengths, limitations, and open challenges of each approach. We further conduct an extensive quantitative evaluation of various ECI methods on two benchmark datasets. Finally, we explore future research directions, highlighting promising pathways to overcome current limitations and broaden ECI applications.","As big data continues to proliferate, the channels and methods for acquiring unstructured text are constantly expanding. One of the primary challenges now lies in how to automatically extract valuable information and knowledge from these texts. This has become a significant area of research in Natural Language Processing (NLP) and Knowledge Reasoning. Events form the core content of texts, and various research directions have emerged around events, such as Event Extraction (EE) [1], Event Relation Extraction (ERE), and Event Coreference Resolution (ECR) [2]. In recent years, Event Causality Identification (ECI) has gained increasing attention as an important and challenging task [3]. As a critical subtask of ERE, ECI aims to predict whether there is a causal relationship between given events in a text. ECI has been widely applied in tasks such as question answering systems [4], information retrieval [5], event prediction [6], knowledge graph construction [7], and reading comprehension [8]. In the ECI task, events are represented by their triggers, known as ""event mentions."" The task then becomes determining which event mentions in a given text have a causal relationship. Figure 1 provides an example of this. ECI is vital for text understanding and decision-making, as it uncovers the causes and effects of events, helping to analyze risks and opportunities for more informed, data-driven decisions. This capability is particularly essential in domains that require complex reasoning, such as finance [9], law [10], healthcare [11], and the military [12]. Figure 1: An example of ECI. The red boxes indicate event mentions. The blue solid arrows represent intra-sentence event causalities, while the green dashed arrows represent inter-sentence event causalities. ECI focuses solely on extracting specific types of relations—cause and effect. However, compared to the general ERE task, identifying causalities, this task is more challenging due to several factors: 1. Implicitness: Causal links are often implied rather than directly stated, requiring deeper contextual and semantic understanding. 2. Long-distance dependencies: Causalities may span multiple sentences or paragraphs, requiring models to capture distant interactions. 3. Complexity: Causal chains can involve multi-step links (e.g., ""earthquake→collapsed→trapped""), adding complexity to identification. 4. Sample imbalance: In supervised settings, negative samples (non-causal pairs) dominate, challenging models to learn effectively from fewer positive cases. 5. Limited labeled training data: Most datasets are small, requiring models to parse semantics well and generalize causal patterns from limited annotations. Early ECI relied on feature pattern recognition, using cues like lexical signals [13, 14, 15], temporal features [16, 17], and co-occurrences [18, 19]. With the rise of Deep Learning (DL), more advanced methods emerged, enabling models to better capture contextual information by deeply encoding text semantics [20]. Transformer-based [21] Pre-trained Language Models (PLMs) [22] have revolutionized ECI, as they are trained on large corpora, enhancing semantic understanding and producing high-quality event and context representations, thereby improving causal identification [23, 24, 25]. Since 2023, Large Language Models (LLMs) have become widely popular. Using vast datasets and large-scale self-supervised learning, LLMs have gained stronger knowledge and contextual understanding, enabling them to handle few-shot and zero-shot tasks [26, 27]. This has advanced research in event extraction, relation extraction, and question answering. However, research by Gao et al. [26] highlights that while LLMs can perform zero-shot ECI with simple prompts, they are prone to ""causal hallucination,"" leading to many false positives. Hence, even though LLMs have significantly advanced tasks related to textual event analysis, ECI remains highly challenging. Research on ECI is still in its early stages, but the field is expanding rapidly. Recent years have seen a notable increase in ECI-related publications, particularly between 2022 and 2024, indicating growing interest and engagement. Though still a relatively small area, ECI research is diversifying in methodological approaches and gaining visibility in high-impact venues. This upward trend suggests that ECI will continue to attract significant attention in the near future. Several surveys have examined event causality and ERE in natural language. Asghar [28] reviewed early rule-based and statistical methods for causal extraction but did not cover deep learning (DL) approaches. Zhao et al. [29] and Xie et al. [30] provided overviews of advances in ERE, with Zhao et al. summarizing general trends in entity causality and Xie et al. comparing DL-based supervision methods. Liu et al. [31] focused on causal and temporal relationships but with limited coverage of recent methods, while Yang et al. [32] and Ali et al. [33] reviewed explicit and implicit causality extraction, limited to work prior to 2021. Liu et al. [34] covered methods, challenges, and datasets for event relation identification but lacked detailed method classification. In this paper, we provide a comprehensive review and analysis of the current state of research on ECI. We systematically outline the core concepts, key technologies, and methodological frameworks in this field, and conduct a quantitative evaluation and comparison of existing approaches. Furthermore, we present a thorough outlook on the future development trends of this promising research area. Our main contributions are as follows: • We detail several concepts of ECI, including problem formalization, datasets, evaluation protocols, and key technologies. • We propose the first comprehensive classification framework for ECI (Figure 2), summarizing the technical features of various methods, evaluating their strengths and limitations. • We perform a quantitative comparison of different ECI methods using experimental data reproduced on two common datasets. • We discuss future directions in ECI, highlighting key challenges and potential solutions for advancing the field. Figure 2: Taxonomy of ECI models, distinguishing SECI and DECI subtasks. For Feature Pattern-Based Matching SECI methods, only reference indices are provided due to the age of these methods and absence of model names. Other methods are labeled as ""Model Name/Author Name + Reference Index"" for clarity. The remainder of this paper is organized as follows: Section 2 presents the relevant concepts of ECI, reviews commonly used datasets, and evaluation metrics. Section 3 introduces key technologies for ECI. Section 4 provides a comprehensive overview of the classification framework for ECI, summarizes core techniques of various models, and analyzes their strengths and weaknesses. Section 5 presents a quantitative evaluation of the performance of existing classical methods on two datasets. Section 6 outlines future research directions for ECI."
https://arxiv.org/html/2411.10369v1,"Towards High-Fidelity 3D Portrait Generation with Rich Details 
by Cross-View Prior-Aware Diffusion","Recent diffusion-based Single-image 3D portrait generation methods typically employ 2D diffusion models to provide multi-view knowledge, which is then distilled into 3D representations. However, these methods usually struggle to produce high-fidelity 3D models, frequently yielding excessively blurred textures. We attribute this issue to the insufficient consideration of cross-view consistency during the diffusion process, resulting in significant disparities between different views and ultimately leading to blurred 3D representations. In this paper, we address this issue by comprehensively exploiting multi-view priors in both the conditioning and diffusion procedures to produce consistent, detail-rich portraits. From the conditioning standpoint, we propose a Hybrid Priors Diffsion model, which explicitly and implicitly incorporates multi-view priors as conditions to enhance the status consistency of the generated multi-view portraits. From the diffusion perspective, considering the significant impact of the diffusion noise distribution on detailed texture generation, we propose a Multi-View Noise Resamplig Strategy integrated within the optimization process leveraging cross-view priors to enhance representation consistency. Extensive experiments demonstrate that our method can produce 3D portraits with accurate geometry and rich details from a single image. The project page is at https://haoran-wei.github.io/Portrait-Diffusion.","The generation of realistic 3D portraits from a single image [7, 39, 10, 35, 20] has become an important focus in computer vision and graphics, with broad applications in augmented reality, virtual reality, video conferencing, and gaming[14, 18, 45]. The most straightforward approach involves training GAN models [47, 1] on extensive portrait datasets to directly produce 3D representations. However, acquiring such training data can be costly and technically challenging, leading to failures in generating high-fidelity 360° full-head portraits [7, 8] and often resulting in a lack of diversity in the outputs. To address these limitations, recent developments [24, 46, 26, 31, 30] leverage text-to-image diffusion priors [44, 41, 5], which exhibit stronger generalization capabilities and higher generation quality, to produce novel perspectives. Most approaches incorporate additional priors, such as reference image latents [42, 50], ID features [28, 13], and view embeddings [28], to enhance the consistency between new perspectives and the primary viewpoint. Subsequently, they commonly employ Score Distilling Sampling (SDS) loss [25] to distill these 2D priors into 3D representations, ensuring consistent 3D generation. However, in single-image 3D portrait generation, these methods still face challenges: generated portraits often appear over-smoothed and fail to capture detailed textures like hair strands, as illustrated in Fig. 1, limiting their practical applications. We attribute this issue to the insufficient consideration of cross-view consistency during the diffusion process, resulting in significant disparities between different views. This 2D inconsistency results in blurred 3D output by SDS optimization. Although these methods attempt to improve consistency by incorporating additional priors, they rely solely on diffusion attentions to implicitly convey these priors. This reliance results in a lack of explict constraints, leading to inconsistent status across different viewpoints. Moreover, the diffusion procedure is inherently stochastic; even with the same conditions, a diffusion model can generate varied representations due to randomly sampled noises. By using view-independent procedures with purely random noise in diffusion, these methods overlook the impact of stochasticity on representation consistency. Consequently, these inconsistencies in status and representation jointly result in over-smoothed 3D models when optimized under the SDS loss, which enforces 3D consistency and continuity in sacrifice of texture details. To address these issues, we propose fully exploiting cross-view priors in both the conditioning and diffusion procedures to enhance multi-view consistency, thus yielding detail-rich 3D portraits, as showcased in Fig. 1. From a conditioning perspective, we propose Hybrid Priors Diffusion Model (HPDM). Our approach seeks to transfer and utilize cross-view prior information in both explicit and implicit ways to control the novel view generation. In an explicit manner, we begin by employing geometric priors to map pixels from the current view to the next, providing an explicit reference to dominate the generation process. Given that this reference encompasses only a limited overlapping region and contains artifacts introduced through perspective transformations, we further propose to utilize the robust modeling capabilities of attention mechanisms to mitigate these deficiencies. These mechanisms capture finer texture and geometry priors and implicitly transfer these priors into the control conditions, ensuring a more comprehensive and precise guidance for the portrait status of novel viewpoint. From a diffusion procedure perspective, our goal is to manage randomness in adjacent viewpoints so that they can share detailed, consistent representations. To achieve this, we introduce a Multi-View Noise Resampling Strategy (MV-NRS) integrated into the SDS loss, which manages each view’s noise distribution by passing cross-view priors. MV-NRS consists of two main components: first, a shared anchor noise initialization that leverages geometric priors to establish a preliminary representation; and second, an anchor noise optimization phase, where we resample and update the anchor noise based on denoising gradient consistency prior to progressively align the representations during the SDS optimization. To summarize, our main contributions are as follows: • We developed a Portrait Diffusion pipeline consisting of GAN-prior Initialization, Portrait Geometry Restoration, and Multi-view Diffusion Refinement modules to generate rich-detail 3D portraits. • We designed a Hybrid Priors Diffusion Model that emphasizes both explicit and implicit integration of multi-view priors to impose conditions, aiming to enhance the consistency of multi-view status. • We introduced a Multi-View Noise Resampling Strategy integrated within the SDS loss to manage randomness across different views through the transmission of cross-view priors, thereby achieving fine-grained consistent representations. • Through extensive experiments, we show that our proposed pipeline successfully achieves high-fidelity 3D full portrait generation with rich details. Figure 2: The Portrait Diffusion Framework. This framework comprises three integral modules. GAN-prior Portrait Initialization, employs existing Portrait GAN priors to derive initial tri-plane NeRF features from frontal-view portrait images. Portrait Geometry Restoration, is focused on reconstructing the geometry using these initialized tri-planes. Multi-view Diffusion Texture Refinement, transforms coarse textures into detailed representations."
https://arxiv.org/html/2411.10368v1,Mechanisms of Generative Image-to-Image Translation Networks,"Generative Adversarial Networks (GANs) are a class of neural networks that have been widely used in the field of image-to-image translation. In this paper, we propose a streamlined image-to-image translation network with a simpler architecture compared to existing models. We investigate the relationship between GANs and autoencoders and provide an explanation for the efficacy of employing only the GAN component for tasks involving image translation. We show that adversarial for GAN models yields results comparable to those of existing methods without additional complex loss penalties. Subsequently, we elucidate the rationale behind this phenomenon. We also incorporate experimental results to demonstrate the validity of our findings.","The advancement of large neural networks has significantly improved the performance of image-to-image translation tasks. Its high accuracy and flexibility attract many researchers in various fields. Industries, such as healthcare, automotive, and entertainment, utilize image-to-image translation technologies for different applications, including medical imaging, autonomous driving, and digital content creation [1, 2, 3]. In addition, researchers in academia and the private sectors are continuously innovating to explore new possibilities and advances in this area. Image-to-image translation encompasses a wide range of tasks, including edge-to-image, photo-to-painting, etc. [1, 4, 5]. All of these tasks need significant computational and data resources for the training model. Depending on the complexity of the model and the size of the dataset, training can take from hours to weeks. A myriad of methodologies have been advanced to address the image-to-image translation problem. Despite most existing models are able to solve the problem, they do not explain the mechanisms by which the network distinguishes content from style [6, 7, 8, 9, 10]. The nebulous definitions of content and style pose significant challenges in the mathematical characterization of the image translation process. Moreover, existing models for image-to-image translation often employ Generative Adversarial Networks (GANs) architecture, but encompass significant complexity, incorporating elements such as cycle loss, identity loss, and penalties on intermediate features. Rarely is the necessity of these intricate penalties examined. Previously, we introduced a GAN-based model to transform food images using only GAN penalty without any additional penalties [4]. In this paper, we investigate the similarity between Generative Adversarial Networks (GANs) [11] and autoencoders [12] to elucidate the GAN model mechanism for image-to-image translation without imposing additional penalties. Subsequently, we show the rationale behind the efficacy of employing solely the GAN component for image-to-image translation tasks. We offer a clear explanation that substantiates the primary role of GAN components in addressing the image-to-image translation problem. We have conducted a comprehensive review and analysis of the models employed for image generation and image-to-image translation. Our investigation focuses on identifying the efficacy of various components of the network. Notably, we discovered that the autoencoder and GAN models generate homologous output and provide an explanation for this phenomenon. This explanation also extends to the efficiency of GANs in the context of image-to-image translation. From our perspective, we employ a preliminary GAN for image-to-image translation. Furthermore, our findings elucidate why some examples in the network may fail. This paper makes the following contributions: (i) We demonstrate that with a discriminator of sufficient capacity to distinguish between real and synthetic images, adversarial training for autoencoder models yields results similar to those of traditional autoencoder models. This is substantiated through experimental validation. (ii) We extend adversarial training to the image-to-image translation problem, illustrating that a straightforward GAN model can preserve common features and generate novel ones, whereas previous methods impose additional penalties to maintain common features. (iii) Our work provides a rationale for the efficacy of GANs in the image-to-image translation context, clarifying that the decomposition of texture and content signifies common and differentiating characteristics determined by the dataset. This offers a more precise and comprehensive understanding compared to previous studies. The paper is structured as follows: The related works section gives a brief review of image generation and translation. The methods section provides our explanation, encompassing algebraic and geometric interpretations. Subsequently, the experiment section presents three experiments. The first experiment compares the performance of GANs and autoencoders, the second investigates the model’s capability for image-to-image translation, and the third examines the constraints outlined in the methods section. Finally, conclusions are drawn based on our analysis."
https://arxiv.org/html/2411.10367v1,Sequential multi-agent reinforcement learning framework for false data injection and detection schemes,"This document is a model and instructions for LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract.",This document is a model and instructions for LaTeX. Please observe the conference page limits.
https://arxiv.org/html/2411.10329v1,"Safe Text-to-Image Generation: 
Simply Sanitize the Prompt Embedding","In recent years, text-to-image (T2I) generation models have made significant progress in generating high-quality images that align with text descriptions. However, these models also face the risk of unsafe generation, potentially producing harmful content that violates usage policies, such as explicit material. Existing safe generation methods typically focus on suppressing inappropriate content by erasing undesired concepts from visual representations, while neglecting to sanitize the textual representation. Although these methods help mitigate the risk of misuse to some extent, their robustness remains insufficient when dealing with adversarial attacks.Given that semantic consistency between input text and output image is a fundamental requirement for T2I models, we identify that textual representations (i.e., prompt embeddings) are likely the primary source of unsafe generation. To this end, we propose a vision-agnostic safe generation framework, Embedding Sanitizer (ES), which focuses on erasing inappropriate concepts from prompt embeddings and uses the sanitized embeddings to guide the model for safe generation. ES is applied to the output of the text encoder as a plug-and-play module, enabling seamless integration with different T2I models as well as other safeguards. In addition, ES’s unique scoring mechanism assigns a score to each token in the prompt to indicate its potential harmfulness, and dynamically adjusts the sanitization intensity to balance defensive performance and generation quality. Through extensive evaluation on five prompt benchmarks, our approach achieves state-of-the-art robustness by sanitizing the source (prompt embedding) of unsafe generation compared to nine baseline methods. It significantly outperforms existing safeguards in terms of interpretability and controllability while maintaining generation quality.","Diffusion models [11, 39], as the current state-of-the-art (SOTA) generative paradigm, drive the development of text-to-image (T2I) generation systems. These T2I models are guided by textual prompts and aim to generate realistic images that align with the provided text descriptions, finding widespread application in fields such as art creation [44] and graphic design [51]. Between 2022 and 2023 alone, T2I models generated over 15 billion images [6]. While T2I models demonstrate significant potential in terms of generation performance, they also face risks of unsafe generation. For instance, the BBC reported that AI-generated child sexual abuse materials were being widely sold over the internet, severely violating ethical and legal norms [5]. Therefore, ensuring that the content generated by T2I models adheres to usage policies [8, 25] is an urgent necessity. To address this issue, many commercial T2I online services have implemented various safeguards. For instance, DALL-E [30] uses a Moderation API to detect input prompts that may violate usage policies. Stable Diffusion is equipped with a SafetyChecker [31] to filter out generated images containing explicit content. However, as passive defense mechanisms, input and output moderation typically offer coarse-grained protection, which may cause T2I models to overly intercept and refuse to generate any images. In contrast, safe generation methods [17, 7, 15] focus on erasing inappropriate concepts from the model’s internal representations. These methods aim to suppress the emergence of these concepts in the generated content, rather than indiscriminately blocking inputs or outputs. For instance, SafeGen [17] fine-tunes the model to remove visual representations linked to explicit concepts, ensuring that the generated content does not reflect these concepts. Such methods offer more fine-grained protection for T2I models, enabling them to produce high-quality, safe images even when given prompts with inappropriate concepts. However, despite the fact that existing safe generation methods are capable of suppressing specific target concepts to certain extent, their robustness remains limited [43, 46]. Considering that semantic consistency between the input text and the generated image is a fundamental requirement for T2I models, which means that inappropriate prompts can be the primary source of unsafe generations by T2I models. Existing methods usually focus on fine-tuning the denoiser of a T2I model or modifying the denoising process to erase inappropriate concepts from the visual representation, neglecting the sanitization of inappropriate concepts in the prompt embeddings. This exposes their limitations, as these methods typically only suppress explicitly deleted concepts and lack sufficient generalization ability when handling synonyms or related concepts. Adversarial attacks on T2I models, such as SneakyPrompt [49], exploit this weakness by using synonym substitutions to bypass safeguards, leading to unsafe generation. Therefore, a natural question arises: Could we propose a robust safety generation framework by erasing inappropriate concepts at the source (i.e., prompt embeddings)? Figure 1: ES operates by sanitizing prompt embeddings to generate safe images and providing interpretability to indicate harmful tokens. Our work. In this paper, we provide an affirmative answer by introducing a novel, vision-agnostic safe generation framework, Embedding Sanitizer (ES). As shown in Figure 1, ES operates as a plug-and-play module applied after the text encoder, without necessitating modifications to any T2I model components. This design ensures compatibility, allowing ES to function independently or integrate seamlessly with other protective measures. ES focuses on erasing representations of inappropriate concepts from prompt embeddings, producing sanitized embeddings that guide the T2I model in generating appropriate content. A key feature of ES is its internal scoring network, which assigns a score to each token in an input prompt to gauge its potential harmfulness. For example, when processing the prompt ”A person in a birthday suit,” the token ”suit” receives a high score, indicating it as a significant contributor to inappropriate content generation. During the sanitization phase, ES dynamically adjusts the intensity of sanitization based on these scores, minimizing the impact on low-score (harmless) tokens while preserving strong sanitization effects on high-score (harmful) tokens. The scoring mechanism and adaptive sanitization function not only enhance the identification and management of inappropriate content but also provide interpretability and controllability in the generation process. ES is a plug-and-play deep learning model that accesses only the text encoder during training, without involving other T2I model components. This design supports the use of a customized objective function for efficient end-to-end training. As a vision-agnostic safe generation framework, ES does not rely on image data for training. Instead, training begins with the selection of target concepts for erasure (e.g., nude) and their corresponding anchor concepts (e.g., dressed), providing explicit learning targets for sanitization. To augment training data, we use randomly synthesized prompts to pair toxic prompts P_{t} (e.g., A nude person) with clean prompts P_{c} (e.g., A dressed person). This approach produces abundant, diverse contextual samples, enabling ES to effectively learn to identify and sanitize inappropriate content across various scenarios. During training, a mean squared error loss is employed to encourage ES to align the embedding of the toxic prompt P_{t} with that of the clean prompt P_{c}. Post-training, ES sanitizes input prompts in real-time to produce clean embeddings, ensuring that generated content adheres to usage policies. Contributions. Our main contributions are as follows: • We propose and investigate a vision-agnostic safe generation method, Embedding Sanitizer (ES), designed as a plug-and-play module that sanitizes prompt embeddings in a controllable manner and provides interpretability for inappropriate elements within prompts. This modular design ensures orthogonal compatibility with existing safeguards, allowing ES to function independently or integrate seamlessly with other protective measures. • We introduce a synthetic data-driven training strategy that supplies ES with a nearly limitless variety of contextual training data. By specifying target and anchor concepts for removal, we reformulate the sanitization task as a transformation from target to anchor concepts, enabling efficient end-to-end training of ES. • We conduct a comprehensive evaluation of ES on five prompt datasets and compare it with nine latest safeguard baselines. Results demonstrate that ES achieves SOTA robustness while maintaining content quality, validating that prompt embedding sanitization can significantly enhance resilience to adversarial attacks. To facilitate future studies, we open-source our code in the following repository: https://anonymous.4open.science/r/Embedding-Sanitizer-166E/. Ethical Considerations. For inappropriate content displayed in the manuscript, we apply masking measures to ensure that readers are not directly exposed to sensitive materials, thereby adhering to ethical standards for handling such content."
https://arxiv.org/html/2411.10308v1,A Realistic Collimated X-Ray Image Simulation Pipeline,"Collimator detection remains a challenging task in X-ray systems with unreliable or non-available information about the detectors position relative to the source. This paper presents a physically motivated image processing pipeline for simulating the characteristics of collimator shadows in X-ray images. By generating randomized labels for collimator shapes and locations, incorporating scattered radiation simulation, and including Poisson noise, the pipeline enables the expansion of limited datasets for training deep neural networks. We validate the proposed pipeline by a qualitative and quantitative comparison against real collimator shadows. Furthermore, it is demonstrated that utilizing simulated data within our deep learning framework not only serves as a suitable substitute for actual collimators but also enhances the generalization performance when applied to real-world data.","In digital radiography, the detection of collimator-covered areas is essential to present diagnostically relevant regions to radiologists. Geometric alignment algorithms, as described in [9], can be employed in X-ray systems with known extrinsic projection parameters. However, despite their availability, these often suffer from inaccuracies sabotaging effectiveness in practice. Due to the inherent geometrical variability in conventional X-ray systems, particularly with mobile flat panel detectors, precise information of the relative position to the detector is unavailable. Moreover, imprecise collimator movement further complicates the detection process, necessitating analysis within image domain. Contrary to a simplistic threshold-based approach, the identification of relevant areas is challenging due to the presence of physical effects like edge-blurring, noise, and scattered radiation. Even human visual perception faces difficulties due to these complexities, as depicted in Fig. 1. (a) Full contrast (b) Contrast adjusted (c) Collimator mask (d) Lineplot Figure 1: Illustrative case for collimator detection depicted in two contrast settings. (a) Contrast adjusted to full image. (b) Contrast adjusted to the orange box. The collimated area (c) is shown as a binary mask. In (d), the intensity profile along the dashed line is compared to the collimated area to visualize the complexity of image-based collimator detection. Deep neural networks (DNNs) show promise for collimator detection, but the limited availability of pre-processed raw data poses a challenge for training robust networks in medical applications. So far, machine learning approaches for collimator detection have not significantly outperformed classic analytical methods in the literature. For instance, comparing the plane detection Hough transform proposed by Kawashita et al. [6] with Mao et al.’s [11] approach that combines random forest learning with a landmark detector in a multi-view learning approach, both methods demonstrate similar performance on unseen data. According to Mao et al. [11], each classifier was trained using only 200 training images. To enhance the performance of machine learning algorithms, it is reasonable to assume that the implementation of robust data augmentation techniques is beneficial. These techniques aim to increase the quantity and variety of datasets. In this context, suitable augmentation techniques can be categorized into deep learning-based methods, such as generative adversarial networks (GANs) [5], and physically motivated approaches. Although GANs have shown promising potential for post-processed X-ray image augmentation (without collimators) in studies like Bowles et al. [1], Madani et al. [10], Kora et al. [7], and Ng et al. [12], they require sophisticated techniques and lack comprehensibility when aiming to serve as reliable training data. Unlike this concept, physically motivated approaches offer a robust alternative for augmentation. These methods leverage an understanding of the underlying physics involved in imaging processes. By incorporating physical principles, these approaches ensure reproducibility and reliability, as demonstrated by Eckert et al. [4] and Xu et al. [15]. In this paper a physically motivated image processing pipeline is presented that simulates the characteristics of real collimators enabling the expansion of limited datasets of X-ray images without collimators. The data augmentation method enables the generation of unlimited pre-processed image data e.g. for training DNNs."
https://arxiv.org/html/2411.10293v1,"RETR: Multi-View Radar Detection Transformer 
for Indoor Perception","Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.77+ IoU for instance segmentation, respectively.","Perception information encompasses the processes and technologies to detect, interpret, and understand their surroundings. Complementary to the mainstream camera and LiDAR sensors, radar can enhance the safety and resilience of perception under low light, adversarial weather (e.g., rain, snow, dust), and hazardous conditions (e.g., smoke, fire) at affordable device and maintenance cost. An emerging application of radar perception is indoor sensing and monitoring for elderly care, building energy management, and indoor navigation [7]. A notable limitation of indoor radar perception is the low semantic features from radar signals. Earlier efforts use radar detection points [42, 30] to support simple classification tasks such as fall detection and activity recognition over a limited number of patterns. To support challenging perception tasks such as object detection, pose estimation, and segmentation, lower-level radar signal representation such as radar heatmaps is more preferred. Along this line, the earliest work is RF-Pose [43] using a convolution-based autoencoder network to fuse features from the two radar views and regress keypoints for 2D image-plane pose estimation. It is later extended to 3D human pose estimation [44]. It is noted that RF-Pose is not publicly accessible. More recently, RFMask [38] borrows the Faster R-CNN framework [27] by proposing candidate regions only in the horizontal radar heatmap via a region proposal network (RPN). A corresponding proposal in the vertical radar heatmap is automatically determined using a fixed-height candidate region at the same depth as the horizontal proposal. The combined horizontal and vertical proposals are then projected into the image plane for bounding box (BBox) estimation. In addition, RFMask calculates the BBox loss only over the 2D horizontal radar view and disregards features from the vertical radar heatmap for BBox estimation. Figure 1: By taking horizontal-view and vertical-view radar heatmaps as inputs, RETR introduces a depth-prioritizing positional encoding (exploit the shared depth between the two radar views) into transformer self-attention and cross-attention modules and outputs a set of 3D-embedding object queries to support image-plane object detection and segmentation via a calibrated or learnable radar-to-camera coordinate transformation and 3D-to-2D pinhole camera projection. In this paper, we exploit features from both horizontal and vertical radar views for object estimation and segmentation and introduce Radar dEtection TRansformer (RETR) (Fig. 1). RETR extends the popular Detection Transformer (DETR) [3], which effectively eliminates the need for hand-crafted components such as non-maximum suppression and proposal/anchor generation, to the multi-view radar perception. More importantly, RETR incorporates carefully designed modifications to exploit the unique multi-view radar setting such as shared depth dimension and the transformation between the radar and camera coordinate systems. Our contributions are summarized below: 1. Extending DETR for Multi-View Radar Perception: 1) Encoder: we associate features from both radar views by applying self-attention over the pooled multi-view radar tokens, eliminating the need for a cumbersome association scheme. We introduce a top-K feature selection to allow only K features from each view to keep the complexity low. 2) Decoder: the DETR decoder provides a natural way to associate the same object query to corresponding features from the two radar views via cross-attention. As such, the object query is able to learn 3D spatial embedding of objects in the radar coordinate (see Fig. 1). 2. Tunable Positional Encoding: To enhance feature association across the two radar views, we further exploit the fact that the two radar views share the depth dimension and introduce a tunable positional encoding (TPE) as an inductive bias. TPE imposes constraints in the attention map to prioritize the relative importance of depth dimension and avoid exhaustive correlations between radar views. 3. Tri-Plane Loss from Both 3D Radar Coordinate and 2D Image Plane: we enforce the output queries of the DETR decoder to directly predict 3D BBoxes in the radar coordinate system and convert them into the 2D image plane. We introduce a tri-plane loss that combines the BBox loss in the 3D radar plane and that in the 2D image plane, to calculate the global set-prediction loss. 4. Learnable Radar-to-Camera Coordinate Transformation: We employ a calibrated radar-to-camera coordinate transformation via a calibration process and a learnable coordinate transformation via reparameterization by preserving the orthonormal (i.e., 3D special orthogonal group \mathcal{SO}\left({3}\right)) structure of the rotation matrix. We demonstrate the effectiveness of our contributions through evaluations on two open datasets: the HIBER dataset [38] and the MMVR dataset [26]."
https://arxiv.org/html/2411.10290v1,"The ParClusterers Benchmark Suite (PCBS): 
A Fine-Grained Analysis of Scalable Graph Clustering 
[Experiment, Analysis & Benchmark]","We introduce the ParClusterers Benchmark Suite (PCBS)—a collection of highly scalable parallel graph clustering algorithms and benchmarking tools that streamline comparing different graph clustering algorithms and implementations. The benchmark includes clustering algorithms that target a wide range of modern clustering use cases, including community detection, classification, and dense subgraph mining. The benchmark toolkit makes it easy to run and evaluate multiple instances of different clustering algorithms, which can be useful for fine-tuning the performance of clustering on a given task, and for comparing different clustering algorithms based on different metrics of interest, including clustering quality and running time.Using PCBS, we evaluate a broad collection of real-world graph clustering datasets. Somewhat surprisingly, we find that the best quality results are obtained by algorithms that not included in many popular graph clustering toolkits. The PCBS provides a standardized way to evaluate and judge the quality-performance tradeoffs of the active research area of scalable graph clustering algorithms. We believe it will help enable fair, accurate, and nuanced evaluation of graph clustering algorithms in the future.","Clustering is a critical tool in almost any scientific field that involves classifying and organizing data today. Examples of fields leveraging clustering range from computational biology and phylogenetics to complex network analysis, machine learning, and astrophysics (Manning et al., 2008; Shalita et al., 2016; Camerra et al., 2014; Patwary et al., 2015). Clustering has proven particularly useful in fields transformed by AI and machine learning because of its utility in understanding and leveraging high-dimensional vector representations (embeddings) of data (Monath et al., 2021; Bateni et al., 2017; Douze et al., 2024; Dhulipala et al., 2021, 2022). In this paper, we are interested in carefully characterizing the behavior (e.g., measuring quality, running time, and scalability) of parallel clustering algorithms for shared-memory multi-core machines that are scalable in the size of the dataset and the number of threads. Our specific focus is on graph clustering, which is a versatile and scalable clustering approach that can be used with different input types. On one hand, graph clustering is a natural approach whenever the input is a graph (e.g., friendships, interactions, etc.). On the other hand, graph clustering can also be applied in the other popular scenario, when the input is a collection of points in a metric space (e.g., embeddings). In this case, one can obtain a graph by computing a weighted similarity graph, where continuous or complete phenomena can be cast into sparse similarity graphs, e.g., by keeping only edges between nearby points or only the most significant entries of a similarity matrix. Despite substantial prior works that study the quality (e.g., precision and recall) and scalability of individual graph clustering methods (Dhulipala et al., 2023, 2021, 2022; Shi et al., 2021; Staudt et al., 2016; Tsourakakis et al., 2017; Tseng et al., 2021a), no prior works have systematically compared a large collection of different graph clustering methods (and their corresponding implementations) to understand how different methods compare against each other under different metrics. For example, celebrated and widely-utilized graph clustering algorithms, such as modularity clustering are well understood to be highly effective in community detection tasks on unweighted natural graphs, but little is known about their performance for clustering on vector embedding clustering tasks. This paper addresses this gap by performing a systematic comparison of a large and diverse set of graph clustering methods. Our evaluation includes methods tailored to both weighted and unweighted graphs and incorporates a diverse set of natural graphs and similarity graphs derived from point sets. We focus on undirected graphs, as converting directed graphs to undirected graphs is a common practice in many graph tasks, such as community detection (see, e.g., (Fortunato, 2010)). We stratify our evaluation based on four unsupervised clustering tasks that are commonly found in the literature and in practice—(1) community detection, (2) vector embedding clustering, (3) dense subgraph partitioning, and (4) high resolution clustering. Due to insisting on scalability, we focus our evaluation on the most scalable parallel graph clustering methods currently available in the literature. To make our evaluation easily reusable and extensible by future researchers, we designed a graph clustering benchmark called the ParClusterers Benchmark Suite (PCBS). PCBS enables users to accurately measure the scalability and accuracy of different shared-memory parallel graph clustering algorithms. In addition to providing a simple and easy to use benchmarking platform, we have also incorporated eleven parallel graph clustering methods into PCBS. The algorithms include algorithms from our recent prior work, as well as several new implementations. In addition to classic graph clustering methods such as modularity-based clustering (Shi et al., 2021), structural clustering (Tseng et al., 2021b), and label propagation methods (Raghavan et al., 2007), we include recently developed hierarchical agglomerative graph clustering methods (Dhulipala et al., 2022) and connectivity-based methods such as k-core and low-diameter decomposition (Dhulipala et al., 2018). Finally, unlike much of the existing work on graph clustering, which typically focuses on optimizing a specific graph clustering metric (e.g., modularity or conductance) that a clustering method is usually designed to optimize, PCBS supports evaluating any clustering algorithm using a very broad set of metrics, which helps us understand what different clustering algorithms are able to optimize for on real-world datasets, and help inform users of the best clustering algorithm for a given metric. Besides PCBS’s clustering implementations, PCBS also supports running many clustering implementations in other graph clustering frameworks and systems such as NetworKit (Staudt et al., 2016), Neo4j (neo, [n.d.]), and TigerGraph (tig, [n.d.]). PCBS can also be easily extended to include new datasets, algorithms, and parameter search methods. The datasets we study include both widely-used graph datasets from the SNAP repository, as well as several new graph clustering datasets that we have generated from spatial and embedding datasets using a simple nearest-neighbor-based graph building process, and which we will open-source as part of this work. We also contribute a new graph dataset for clustering, which represent similarities between 1.2M short texts. As far as we know, this is the first large-scale graph clustering dataset that provides a large number of ground-truth clusters. Our datasets cover a wide range of scales and clustering tasks, including community detection, vector embedding clustering, and dense subgraph partitioning. Key Contributions. The key contributions of our work include: • A comprehensive library that implements eleven state-of-the-art scalable graph clustering algorithms, providing a unified codebase for researchers and practitioners. • A benchmarking toolkit that facilitates the systematic evaluation of graph clustering algorithms across diverse datasets, parameter settings, and experimental configurations, enabling rigorous and comprehensive comparative analyses. • A new large graph clustering dataset containing many ground-truth clusters. • The first extensive evaluation of parallel graph clustering algorithms, encompassing their runtime performance, clustering quality, and the trade-off between these two critical dimensions. We also compare our library against other existing libraries and graph databases. Key Results. Some of our key takeaways and findings of our study of graph clustering algorithms include: • Our clustering implementations in PCBS are very fast compared to other clustering implementation in state-of-the-art graph libraries and databases. While graph databases, such as Neo4j (neo, [n.d.]) and TigerGraph (tig, [n.d.]), provide a richer functionality, on different graph clustering implementations, they are slower. For example, on the LiveJournal graph from SNAP (Leskovec and Sosič, 2016), PCBS is on average 32.5x faster than Neo4j and 303x faster than TigerGraph. Compared with state-of-the-art parallel graph library NetworKit (Staudt et al., 2016), PCBS is on average 4.54x faster. We compute the average using the geometric mean. • Correlation clustering (Shi et al., 2021) obtains the highest quality on three out of four tasks. ParHAC (Dhulipala et al., 2022) obtains the best quality on the fourth task. We consider this finding surprising, given that these two methods are not included in many popular graph clustering frameworks. The best performing method commonly found in existing graph clustering packages is modularity clustering. However, we observe that on a vast majority of datasets, correlation clustering obtains strictly better quality. • Parallel affinity clustering obtains high quality on the vector embedding clustering task and is consistently faster than correlation clustering and ParHAC on large graphs. Our code and the full version of our paper can be found at https://github.com/ParAlg/ParClusterers."
https://arxiv.org/html/2411.10285v1,"Systolic Arrays and Structured Pruning Co-design 
for Efficient Transformers in Edge Systems","Efficient deployment of resource-intensive transformers on edge devices necessitates cross-stack optimization. We thus study the interrelation between structured pruning and systolic acceleration, matching the size of pruned blocks with the systolic array dimensions. In this setting, computations of pruned weight blocks can be skipped, reducing run-time and energy consumption, but potentially impacting quality of service (QoS). To evaluate the trade-offs between systolic array size and sparsity opportunities, we present a novel co-design framework that integrates algorithmic optimization, system simulation, and hardware design. Targeting speech recognition using transformers as case study, we analyze how configuration choices across the stack affect performance metrics. Results demonstrate that structured pruning on systems featuring systolic array acceleration can effectively increase performance, while maintaining high QoS levels. Up to 26% system-wide speedups due to structured pruning were measured, with only 1.4% word error rate degradation on the standard Librispeech dataset.","Transformers have fostered a revolution in machine learning, with applications ranging from classification [1] to generative models for text and images [2], to speech recognition [3]. However, their complex structure based on multiple attention and feed-forward layers [4] results in unprecedented computational requirements, posing significant challenges for their deployment. These are particularly acute in edge scenarios, where systems have to operate within constrained energy and performance envelopes. In this context, a plethora of optimization strategies have been proposed. On the software side [5], commonly used approaches involve reducing the precision of data representations (quantization) and removing parts that contribute the least to inference outcomes (pruning). As for hardware, efforts have mainly focused on the acceleration of the main computational kernel in transformers, i.e. General Matrix Multiplications (GEMMs). Although diverse solutions ranging from analog crossbars [6][7] to near-DRAM computing [8][9] work toward this goal, a particularly promising alternative is represented by systolic arrays [10]. These two-dimensional meshes of processing elements can indeed parallelize the computation of a GEMM (or, more precisely, the computation of a GEMM tile), while presenting a high parallelism degree, low resource requirements and only mandating a simple, low-overhead control logic. Recent works [11, 12, 13, 14, 15] have attempted to co-optimize software algorithms and hardware accelerators dedicated to transformer inference [16]. Such a stance is particularly appealing at the crossroads of model pruning and systolic array acceleration. On the software side, pruning can be performed by eliding weights in regular block patterns (in a “structured” way) rather than as individual elements [17]. While this approach introduces a constraint to pruning, and can hence result in lower overall sparsity rates, it substantially amplifies hardware-side optimization opportunities when matching the sizes of the pruned tile and the accelerator mesh. The exploration of this strategy, which we term Systolic Array Structured Pruning (SASP), is the focus of this work. SASP opens a complex multidimensional design space which requires careful consideration of metrics spanning from hardware to algorithms. Indeed, while a larger accelerator can expose a higher degree of parallelism, it also requires more resources (area / energy). Moreover, SASP settings with larger tiles may overly penalize the achievable sparsity for a desired Quality of Service (QoS) or, alternatively, result in high QoS degradation for a fixed pruning rate. To explore these interrelations, we employ a holistic approach integrating methods for a) the structured pruning of transformer algorithms, b) the system-level level modeling of accelerated systems executing them, and c) the hardware synthesis of accelerators. Our environment for SASP exploration builds on frameworks for the training of transformers (ESPnet [3]) and for system simulation (gem5 [18]). By employing a novel systolic array architectural template, it supports both floating point and weight-quantized data representations, as supported by ESPnet. As a test case, we employ our exploration approach to analyze a speech recognition application, based on an 24-block, 75M-parameter transformer processing the LibriSpeech dataset [19]. We observed that SASP can achieve, for a systolic array size of 32\times 32, up to 44% speedup and 42% energy savings over a non-pruned, non-quantized system when employing a 20% pruning rate, resulting in a marginal Word Error Rate (WER) degradation of 1.4%. The contributions of this paper are summarized as follows: • We introduce a methodology for the systematic exploration of Systolic Array Structured Pruning (SASP), a co-design strategy that combines systolic array acceleration and structured pruning with matching accelerator and tile size. • We show how the insights collected from our framework enable the evaluation of figures of merit at different abstraction levels, including the assessment of QoS, performance, resource usage, and energy, as well as their trade-offs. We discuss how these can be effectively leveraged from the joint perspective of algorithmic optimization, system integration, and systolic array design. • Using a speech recognition case study, we show that SASP-based co-optimization of transformers and systolic arrays can lead to efficiency and speedup gains of up to 26% with minimal QoS impact."
https://arxiv.org/html/2411.10279v1,Lateral Movement Detection via Time-aware Subgraph Classification on Authentication Logs,"Lateral movement is a crucial component of advanced persistent threat (APT) attacks in networks. Attackers exploit security vulnerabilities in internal networks or IoT devices, expanding their control after initial infiltration to steal sensitive data or carry out other malicious activities, posing a serious threat to system security. Existing research suggests that attackers generally employ seemingly unrelated operations to mask their malicious intentions, thereby evading existing lateral movement detection methods and hiding their intrusion traces. In this regard, we analyze host authentication log data from a graph perspective and propose a multi-scale lateral movement detection framework called LMDetect. The main workflow of this framework proceeds as follows: 1) Construct a heterogeneous multigraph from host authentication log data to strengthen the correlations among internal system entities; 2) Design a time-aware subgraph generator to extract subgraphs centered on authentication events from the heterogeneous authentication multigraph; 3) Design a multi-scale attention encoder that leverages both local and global attention to capture hidden anomalous behavior patterns in the authentication subgraphs, thereby achieving lateral movement detection. Extensive experiments on two real-world authentication log datasets demonstrate the effectiveness and superiority of our framework in detecting lateral movement behaviors.","Recently, the rapid development of the internet has profoundly transformed the digital environment, introducing unprecedented connectivity and avenues for information access. However, this high degree of connectivity has also triggered a series of significant cybersecurity challenges. In particular, the complexity and stealthiness of Advanced Persistent Threats (APTs) have escalated, posing severe threats to internet security. Among APT attacks, lateral movement attacks are especially prominent and have become a critical issue that urgently needs to be addressed in cybersecurity. Lateral movement is a key stage in the APT attack. After gaining initial access to a node within the network, attackers employ various techniques to progressively infiltrate and expand their control within the network. This process allows attackers to escalate privileges, access sensitive data, and establish persistent access mechanisms (such as backdoors or malware). Fig 1 illustrates two typical lateral movement scenarios in an internal network: 1) External attackers successfully infiltrate an internal network host through phishing emails or vulnerability exploitation. After establishing an initial foothold, they gradually expand their control by scanning other devices in the network and exploiting inherent security weaknesses or by stealing credentials to escalate privileges; 2) Internal members leverage their initial legitimate access or stolen credentials to carry out unauthorized access, progressively extending their control over the system. The ultimate goal of lateral movement is to steal high-value resources, which can lead to significant losses for organizations. Therefore, detecting lateral movement behavior is critical to thwarting APT attacks. Researchers have developed various methods for detecting lateral movement, primarily including methods based on endpoint detection and response (EDR) [1, 2, 3, 4, 5, 6, 7, 8], machine learning (ML) and deep learning (DL) [9, 10, 11, 12, 13, 14, 15, 16, 17]. While these approaches have achieved certain success in detecting lateral movement, they still face several limitations. EDR-based methods can monitor endpoint devices in real-time and detect abnormal behaviors, but they are vulnerable to attackers who can evade detection by using legitimate tools and credentials. Additionally, EDR requires substantial computational resources to process large volumes of data, potentially leading to performance bottlenecks. ML-based methods demonstrate good performance in identifying complex behavioral patterns, but they rely heavily on large amounts of labeled data for training. As attackers continuously modify their tactics, existing models struggle to adapt to new attack means. DL-based methods excel in extracting complex features from data but also require extensive high-quality data for training, and the training process is complex and time-consuming. Moreover, DL-based models have poor interpretability, making it difficult to analyze and understand the detection results, thus posing challenges for security experts in threat response. Figure 1: Two lateral movement scenarios in the enterprise internal network: 1) External threat actors employ advanced persistent threat (APT) techniques to infiltrate the internal network; 2) Internal personnel exploit initial privileges for unauthorized access. Both leverage lateral movement tactics to expand their control and accomplish the objective of exfiltrating sensitive data. Authentication logs serve as a critical data resource for detecting lateral movements, recording every authentication attempt between users, devices, and services, including both successful and failed logins, along with related timestamps, locations, and contextual information. These data provide security analysts and automated systems with a microscopic view of internal network activities, enabling the tracking and analysis of user and device behavioral patterns. Recently, methods based on Graph Neural Networks (GNNs) [18, 19, 20, 21, 22] utilizing authentication logs have achieved notable success in the domain of lateral movement detection. By modeling authentication logs as heterogeneous graph structures and employing GNNs to learn higher-order representations of lateral movement behaviors, these methods have captured complex temporal and relational characteristics of lateral movements, resulting in superior detection performance. However, GNN-based approaches still face specific limitations: 1) Limitations of the detection paradigm. Current GNN-based methods typically model lateral movement detection as an edge classification task based on node representation learning, treating network behaviors in isolation and overlooking the fact that lateral movements often involve a series of continuous actions; 2) Efficiency constraints of large-scale graph computation. The scale of authentication log data is vast, recording numerous user authentication events and network interactions. Learning on such large-scale interaction graphs results in high computational complexity and significant time costs; 3) Imbalanced distribution of authentication log data. In typical network environments, benign interactions vastly outnumber malicious ones, such as lateral movements, leading to a severe imbalance between positive and negative samples. Existing GNN-based detection models are prone to detection bias under these conditions, favoring predictions of common benign behaviors and overlooking rare but highly damaging malicious actions. This imbalance poses serious challenges to the accuracy and robustness of models. Figure 2: Workflow of the LMDetect framework. In this regard, we propose a time-aware multi-scale Lateral Movement Detection framework (LMDetect) — an end-to-end graph neural network model — to characterize behavior patterns in network authentication events and further achieve lateral movement detection. Figure 2 illustrates the workflow of LMDetect. Specifically, we first parse authentication log data, identify network entities and interaction types, and construct a heterogeneous authentication multigraph (HAMG) to describe network activity. We then consider lateral movement detection as a subgraph-level classification problem, designing a time-aware subgraph generator to capture highly relevant information surrounding target authentication events, yielding time-aware authentication subgraphs centered on each target event. This approach enables efficient mini-batch training of LMDetect. To better capture the behavior patterns of authentication events, we further introduce a multi-scale attention encoder as the backbone of our framework, which can effectively learn both local and global dependencies among network entities, providing strong representations of authentication events. The main contributions of this work are as follows: • Data and Task Modeling: We model authentication logs as a heterogeneous authentication multigraph and formulate the lateral movement detection task as a subgraph classification problem, designing a time-aware subgraph generator to enable scalable lateral movement detection. • Powerful Representation Capability: We propose a multi-scale attention encoder that effectively learns both local and global dependencies in network activities, capturing the behavior patterns of authentication events. • State-of-the-Art Performance: Extensive experiments on two real-world authentication log datasets demonstrate the effectiveness and significant superiority of our framework for the lateral movement detection task."
https://arxiv.org/html/2411.10257v1,The Unreasonable Effectiveness of Guidance for Diffusion Models,"Guidance is an error-correcting technique used to improve the perceptual quality of images generated by diffusion models. Typically, the correction is achieved by linear extrapolation, using an auxiliary diffusion model that has lower performance than the primary model. Using a 2D toy example, we show that it is highly beneficial when the auxiliary model exhibits similar errors as the primary one but stronger. We verify this finding in higher dimensions, where we show that competitive generative performance to state-of-the-art guidance methods can be achieved when the auxiliary model differs from the primary one only by having stronger weight regularization. As an independent contribution, we investigate whether upweighting long-range spatial dependencies improves visual fidelity. The result is a novel guidance method, which we call sliding window guidance (SWG), that guides the primary model with itself by constraining its receptive field. Intriguingly, SWG aligns better with human preferences than state-of-the-art guidance methods while requiring neither training, architectural modifications, nor class conditioning. The code will be released.","Diffusion models (DMs) have emerged as a powerful approach for generative tasks, achieving remarkable success in areas such as image synthesis and text-to-image generation [43, 17, 24, 44, 27, 40, 1]. DMs are a class of generative models that iteratively transform noise samples into samples that are close to a desired data distribution. Despite their success, DMs often fail to generate high-quality samples in the visual domain [3] and require guidance techniques to improve visual fidelity (Fig. 1). The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16]. In the following, we denote by \bm{x} a noisy image and by \bm{\epsilon}(\bm{x},t;c) and \bm{\epsilon}(\bm{x},t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme Figure 1: Left: Even state-of-the-art diffusion models can fail to generate globally coherent images without guidance. Right: Sliding window guidance (SWG) upweights long-range dependencies and thereby improves global coherence on average. \tilde{\bm{\epsilon}}(\bm{x},t;c)=\bm{\epsilon}(\bm{x},t;c)+w[\bm{\epsilon}(% \bm{x},t;c)-\bm{\epsilon}(\bm{x},t)], (1) with guidance weight w>0. CFG can be viewed as an error-correcting method [44, 7]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [41]. Despite the widespread use of CFG in conditional synthesis [36], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [40], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25]. Since guidance is a linear extrapolation scheme, the sampling trajectory can overshoot the desired distribution, leading to highly simplified images [30]. Finally, CFG sampling is restricted to class conditional generation by design. Recently, a new class of guidance methods has been developed to address some of the limitations of CFG. These methods utilize the extrapolation scheme of Eq. 1 in a more generic way \tilde{\bm{\epsilon}}(\bm{x},t)=\bm{\epsilon}_{\text{pos}}(\bm{x},t)+w[\bm{% \epsilon}_{\text{pos}}(\bm{x},t)-\bm{\epsilon}_{\text{neg}}(\bm{x},t)], (2) where the subscripts pos for positive and neg for negative simply refer to the sign of the noise predictors. We consider \bm{\epsilon}_{\text{pos}} to be a trained, well-performing DM. The idea behind Eq. (2) is to extrapolate into high likelihood regions by designing a negative model that has an inferior performance compared to \bm{\epsilon}_{\text{pos}}. Typically, \bm{\epsilon}_{\text{neg}} is derived from \bm{\epsilon}_{\text{pos}} by re-training using fewer parameters, architecture-based heuristic manipulations, or shorter training times [2, 25]. We refer to this class of guidance methods as weak model guidance (WMG). While WMG methods seem promising, they often require training additional models, heavy model-specific hyperparameter tuning at sampling time [25], or manual selection of specific layers to impair [2]. In this paper, we first introduce a 2D toy example to show that (i) WMG samples closer to high likelihood regions than CFG and (ii) guidance works best if \bm{\epsilon}_{\text{neg}} makes similar errors as \bm{\epsilon}_{\text{pos}} but stronger. Assuming that the generative error of \bm{\epsilon}_{\text{pos}} can be primarily attributed to scaling factors, such as training time and number of parameters [22], further constraining these factors allows us to construct \bm{\epsilon}_{\text{neg}} in a principled way. To this end, we show that constructing \bm{\epsilon}_{\text{neg}} by re-training or fine-tuning \bm{\epsilon}_{\text{pos}} under increased weight regularization results in competitive performance compared to state-of-the-art guidance methods. Additionally, to improve perceptual image quality, we introduce sliding window guidance (SWG), a novel guidance method designed to upweight long-range dependencies (Fig. 1). In contrast to existing guidance techniques [40, 16, 2, 25, 38, 30, 47], SWG requires neither training, architectural modifications nor class conditioning and can be applied to any DM that can process multiple image resolutions. Finally, SWG achieves competitive generative performance while aligning better with human preferences than state-of-the-art techniques. (a) w=0 (b) w=w^{*} (c) w=3w^{*} (d) Guidance triangle Figure 2: Inference trajectories for our 2D-toy model. (a) w=0 yields the trajectories of the positive model. (b) w^{*} denotes the guidance weight that leads to best (CFG) or onset of saturating (WMG) performance (w^{*}=1 for CFG and w^{*}=5 for WMG). (c) Extreme guidance weights lead to even smaller endpoint errors, but unstable trajectories for WMG and lead CFG to diverge from the data distribution. (d) Geometry of guidance updates for WMG. The linear guidance correction pushes WMG closer to the data point but pushes CFG further away, even though the conditional model correctly predicts the direction toward the data point."
https://arxiv.org/html/2411.10234v1,"Generative AI in Multimodal User Interfaces: Trends, Challenges, and Cross-Platform Adaptability","As the boundaries of human-computer interaction expand, Generative AI emerges as a key driver in reshaping user interfaces (UIs), introducing new possibilities for personalized, multimodal, and cross-platform interactions. This integration reflects a growing demand for more adaptive and intuitive UIs that can accommodate diverse input types—text, voice, video—and deliver seamless experiences across devices. This paper explores the integration of Generative AI in modern UIs, examining historical developments and focusing on multimodal interaction, cross-platform adaptability, and dynamic personalization. A central theme is the “interface dilemma,” which addresses the challenge of designing effective interactions for multimodal large language models (LLMs), assessing the trade-offs between graphical, voice-based, and immersive interfaces. The paper further evaluates lightweight frameworks tailored for mobile platforms, spotlighting the role of mobile hardware in enabling scalable, multimodal AI. Technical and ethical challenges, including context retention, privacy concerns, and balancing cloud and on-device processing, are thoroughly examined. Finally, the paper outlines future directions, such as emotionally adaptive interfaces, predictive AI-driven UIs, and real-time collaborative systems, underscoring Generative AI’s potential to redefine adaptive, user-centric interfaces across platforms.","User interfaces have significantly evolved over the last few decades. From early text-based systems to modern graphical and multimodal interfaces, the developments in human-computer-driven interactions were strongly driven by advancements in software and hardware. This review examines how Generative AI, particularly multimodal large language models (LLMs) [1], is poised to further transform UI design by enabling dynamic personalization, context retention, and efficient scalability. It seems inevitable that with the advent of highly capable LLMs being accessible on every device, the way people interact with technology is going to adapt. This raises a few critical questions: What is the ideal interface for users to interact with AI-powered systems? Is there going to be a single trend or will interfaces adapt to each type of application specifically? How will the availability of immersive technologies like Virtual Reality (VR) glasses influence this? How long can it take people to adapt to a completely new interface experience? Some of the biggest technology companies have been experiencing with introducing new ways to interact with technology, but in the end, they converge to very similar styles and ideas. We will also address the limitations of current frameworks and the challenges of implementing AI in this domain, particularly when constrained by hardware like mobile phones. I-A Problem Statement: The Interface Dilemma Since the release of Chat GPT by Open AI, there has been a massive spike in interest in AI-driven applications. The chatbot-like interface, which was popularized by Chat GPT has quickly become the standard for human-AI interactions. Despite recent developments of multimodal LLMs, chat-based interactions are still the most popular ones, despite their limitations. As a result, most applications are limited to a very similar user interface and require high-quality user input to help the LLM understand the context or mood. One example of such applications can be voice assistants such as Alexa, Google Assistant, or Siri. Apple first integrated Siri into its ecosystem with the iPhone 4S in 2011. Since then, both the hardware and software it runs, as well as its capabilities, have improved massively. However, the way users interact with Siri has stayed exactly the same. Similarly, Google Assistant and Alexa, despite being competing products, offer exactly almost identical experiences when it comes to interacting with their device. We already possess powerful multimodal LLMs capable of processing text, images, and voice. However, the challenge lies in determining the best interface for human-computer interaction. Should it be console-based, GUI-driven, or even integrated into VR glasses? The core issue revolves around formulating an interface that is intuitive and leverages the full capabilities of AI-driven multimodal systems. I-B Objectives This review article aims to critically analyze and synthesize the state-of-the-art advancements in AI-driven user interface (UI) design, with a particular emphasis on optimizing interfaces for multimodal LLMs. The central focus is to explore how various forms of interaction, including text, voice, and video, can be seamlessly integrated into UIs to enhance human-computer interaction. Additionally, this article investigates the application of lightweight frameworks, particularly in the context of mobile devices, and assesses the potential of mobile phone hardware (voice, video, text) as a primary platform for scalable, efficient, and user-friendly AI interfaces. TABLE I: List of key acronyms used in the paper. Acronym Definition AI Artificial Intelligence AR Augmented Reality BCI Brain-Computer Interface CLI Command-Line Interface GUI Graphical User Interface IoT Internet of Things LLM Large Language Model NLP Natural Language Processing NPU Neural Processing Unit UI User Interface VR Virtual Reality TABLE II: Comparison of review articles about Generative AI in multimodal user interfaces. ✓, ✗, and ✱indicate that the topic is well-covered, uncovered, and partially covered, respectively. Reference Year Focus Multimodal Interaction Generative AI Cross-Platform Challenges & Trends Bieniek et al. (This Paper) 2024 A review of integrating Generative AI in modern computer UIs, focusing on multimodal interaction, cross-platform compatibility, and dynamic personalization ✓ ✓ ✓ ✓ Jones et al. [2] 2024 Examines how multimodal LLMs ground language in comparison to human interactions ✓ ✗ ✗ ✱ Munikoti et al. [3] 2024 Reviews architectures, challenges, and opportunities in multimodal AI, including transformer-based architectures for fusion ✓ ✓ ✱ ✓ Huang et al. [4] 2024 Focuses on adaptive user experience with Generative AI and dynamic personalization ✗ ✓ ✓ ✱ Kim et al. [5] 2021 Reviews multimodal interaction systems based on IoT and augmented reality ✓ ✗ ✓ ✓ Lu et al. [6] 2024 Reviews AI in user experience design, with an emphasis on human-centered AI applications ✗ ✓ ✓ ✗ Pyarelal et al. [7] 2018 Discusses automating the design of UIs using AI technologies ✱ ✓ ✱ ✱ Su et al. [8] 2023 Reviews advancements in multimodal human-robot interaction, with emphasis on combining voice, gestures, and facial recognition ✓ ✗ ✗ ✓ Zhang et al. [9] 2024 Evaluates the impact of multimodal interactions on user engagement in AI-driven conversations ✓ ✓ ✗ ✓ Bandi et al. [10] 2023 Reviews generative AI models, input-output formats, evaluation metrics, and technical challenges ✗ ✓ ✱ ✓ I-C Scope This article focuses on the intersection of AI and UI design, with particular attention to multimodal interaction with LLMs. The review encompasses the technological frameworks that enable AI-driven UI optimization, especially for mobile platforms, and evaluates lightweight approaches for integrating voice, video, and text modalities. The scope is limited to UIs that support multimodal interaction and dynamic personalization, excluding non-interactive or single-modal systems. Furthermore, the review assesses the feasibility of deploying such systems on mobile hardware, considering processing limitations, scalability challenges, and the need for efficient resource management. I-D Related Papers Table II provides a structured comparison of various review articles on generative AI, multimodal interfaces, and cross-platform adaptability. Each reference contributes uniquely to the body of research, highlighting different aspects of AI-driven user interfaces. For instance, Jones et al. [2] focus on the comparison between multimodal LLMs and human language grounding, providing valuable insights into how AI systems perceive and interpret multimodal inputs. However, their work lacks a detailed discussion on cross-platform adaptability and the role of generative AI in modern interfaces. In contrast, Munikoti et al. [3] delve deep into the architecture of multimodal AI systems, such as transformer-based models, and discuss their potential across different platforms. While their review thoroughly covers multimodal interaction, it only partially addresses the cross-platform challenges. Huang et al. [4] contribute by emphasizing adaptive user experiences through generative AI. Their review offers insights into how dynamic personalization can be achieved using generative AI but lacks an extensive exploration of multimodal systems. Similarly, Kim et al. (2021) focus on multimodal interaction systems in Internet of Things (IoT) and Augmented Reality (AR), emphasizing cross-platform challenges and offering a robust view of system interoperability but without a focus on generative AI [5]. On the other hand, Lu et al. [6] concentrate on human-centered AI in user experience design, which is highly relevant to AI-driven interfaces but does not deeply explore multimodal interaction. While some works, such as Su et al. [8], offer a comprehensive review of multimodal human-robot interaction, they miss out on the cross-platform adaptability required in broader AI-driven systems. Bandi et al. [10], though thorough in covering generative AI models and technical challenges, only partially address cross-platform issues and multimodal interaction. While these articles contribute significantly to the fields of generative AI and multimodal user interfaces, my attached paper offers a more comprehensive review by covering multimodal interaction, generative AI integration, cross-platform adaptability, and the emerging trends and challenges in AI-driven UIs. This distinguishes the paper as a broader, more inclusive review, bridging gaps not fully addressed by other works. I-E Paper Structure The structure of this paper is as follows: Section II covers the interface dilemma, highlighting challenges in designing intuitive multimodal LLM interfaces. Section III reviews the evolution of user interfaces, noting limitations of current AI-driven platforms. Section IV discusses application frameworks and AI integration, focusing on Generative AI’s role in cross-platform adaptability and mobile integration. Section V examines the development of new multimodal UIs enabled by Generative AI, addressing privacy, mobile processing efficiency, and context retention challenges. Section VI explores limitations and challenges in AI-driven interfaces and outlines future trends. Section VII identifies evaluation metrics for AI-driven multimodal UIs and Section VIII concludes the paper. Lastly, the list of key acronyms used in this paper is given in Table I."
https://arxiv.org/html/2411.10232v1,ColorEdit: Training-free Image-Guided Color editing with diffusion model,"Text-to-image (T2I) diffusion models, with their impressive generative capabilities, have been adopted for image editing tasks, demonstrating remarkable efficacy. However, due to attention leakage and collision between the cross-attention map of the object and the new color attribute from the text prompt, text-guided image editing methods may fail to change the color of an object, resulting in a misalignment between the resulting image and the text prompt. In this paper, we conduct an in-depth analysis on the process of text-guided image synthesizing and what semantic information different cross-attention blocks have learned. We observe that the visual representation of an object is determined in the up-block of the diffusion model in the early stage of the denoising process, and color adjustment can be achieved through value matrices alignment in the cross-attention layer. Based on our findings, we propose a straightforward, yet stable, and effective image-guided method to modify the color of an object without requiring any additional fine-tuning or training. Lastly, we present a benchmark dataset called COLORBENCH, the first benchmark to evaluate the performance of color change methods. Extensive experiments validate the effectiveness of our method in object-level color editing and surpass the performance of popular text-guided image editing approaches in both synthesized and real images.","Figure 1: Multi-Object color editing. Each outcome image is changing the color of the hat first and then changing the color of the bowl, using the associated reference color image. Figure 2: Example of color change. Text-guided editing methods may fail to change the color of an object while maintaining the structure of it or the background information. Color is one of the most important visual perceptions for humans. The color of an object significantly influences the emotional responses and perceptions of a person toward it, which makes color a key element in both functional and aesthetic design decisions across industries, particularly in the design field. With the development of diffusion models, some studies [18, 47, 23] have applied the stable diffusion (SD) model to the colorization task, which involves adding color to grayscale or black-and-white images. However, currently, no studies specifically investigate the task of color change. Although some methods [13, 42, 24, 3, 10, 9] demonstrate the capability to modify an object’s color, those text-guided techniques often fail to change the color of an object as expected, as shown in Fig. 2. In this paper, we conduct an in-depth exploration on the cross-attention layer, which aligns and transforms text information into synthesized images. Specifically, we visualize the cross-attention maps of objects in different blocks of the model through different stages of the denoising process to elucidate how textual information directs the generation of images. We identify the semantic information captured by various cross-attention blocks and demonstrate when and where an object’s shape, contour, and texture are established. For the unsuccessful outcome of text-guided color editing, we argue that there are two main factors: (a) the imprecise distribution of color attribute attention weights on the spatial area, called cross-attention leakage. (b) The collision of information on attributes in the cross-attention map from the original object and the color term in the target prompt. We find that, compared to altering the Key matrices in the cross-attention layer, modifying the Value matrices of the target image results in a more stable color change effect. Based on our findings, we introduce a simplified yet stable and effective method called training-free Image-Guided Color Editing. Our method performs object color editing by aligning the Value matrices of the target image with the Value matrices extracted from a reference color image in specific cross-attention layers of the diffusion model in the early stages of the denoising process (see an example in Fig. 1). Our contributions are as follows: (1) We demonstrate that the shape, contour, and texture of an object are determined in the U-Net decoder in the early stage of the denoising process. (2) We propose a tuning-free image-guided method to edit the color of an object through Value matrices alignment in the cross-attention layer. (3) We introduce COLORBENCH, the first benchmark dataset to evaluate the color editing task. (4) Experimental results demonstrate that our Image-Guided Color Editing method surpasses current popular text-guided image editing methods on both synthesized and real images."
https://arxiv.org/html/2411.10231v1,"A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image
Super-Resolution with Transformers and TaylorShift","Transformer-based Super-Resolution (SR) models have recently advanced image reconstruction quality, yet challenges remain due to computational complexity and an over-reliance on large patch sizes, which constrain fine-grained detail enhancement. In this work, we propose TaylorIR to address these limitations by utilizing a patch size of 1x1, enabling pixel-level processing in any transformer-based SR model. To address the significant computational demands under the traditional self-attention mechanism, we employ the TaylorShift attention mechanism, a memory-efficient alternative based on Taylor series expansion, achieving full token-to-token interactions with linear complexity. Experimental results demonstrate that our approach achieves new state-of-the-art SR performance while reducing memory consumption by up to 60% compared to traditional self-attention-based transformers.","Image Super-Resolution (SR) remains a foundational yet significant low-vision challenge, aiming to reconstruct High-Resolution (HR) images from Low-Resolution (LR) inputs. The applications encapsulated by SR are broad, spanning security, medical imaging, and even astronomical analysis [20, 21]. Despite the powerful advances made with deep learning, limitations persist, especially regarding high-frequency detail enhancements [19, 13, 9]. With the introduction of deep learning, SR methods have leaned heavily on Convolutional Neural Networks (CNNs) [30, 32, 12], delivering impressive performance. Short after, transformer-based architectures have demonstrated an aptitude for capturing intricate relationships across input sequences, making them a dominant choice for regression-based image SR [25, 6]. Prominent transformer-based architectures are SwinIR [11], Restormer [27], and HAT [3], which have demonstrated promising gains, applying self-attention mechanisms for precise context-aware upscaling. Yet, transformer-based SR methods face notable challenges: high memory requirements and quadratic time complexity associated with self-attention, limiting practicality for real-time and large-scale applications. As a result, current methods reduce the contextual scope within which attention is operating, e.g., 8\times8 windows, and sometimes operate within these windows with patch-sizes greater than 1\times1, leading to non-pixel level detail enhancement. This restriction compromises the ability to capture fine-grained dependencies across the entire image. Figure 1: Overview of TaylorIR’s impact on low-vision applications like image SR. By embedding the input as 1\times1 patches, TaylorIR transforms inputs to long, pixel-level sequences, allowing fine-grained detail enhancement. In addition, it exploits the TaylorShift [22] attention mechanism, which significantly reduces memory consumption compared to classical self-attention, making it a more efficient solution for image SR. To address these issues, we introduce TaylorIR, a novel transformer-based SR approach that makes use of pixel-level detail refinement. Moreover, it substitutes traditional self-attention with TaylorShift [22] attention, a memory-efficient mechanism inspired by Taylor series expansion that approximates full token-to-token interactions with linear complexity. By enabling fine-grained attention on a per-pixel level, our approach significantly enhances context information while improving computational efficiency. For Swin-based SR models like SwinIR, we showcase TaylorSwinIR, a version of SwinIR adapted with TaylorIR to support 1\times1 patch embeddings and large window sizes, i.e., from 8\times8 with 64 tokens to 48\times48 with 2304 tokens. TaylorShift [22] enables efficient, global attention at a lower memory footprint, supporting broader context and enhanced detail without the resource strain of traditional attention mechanisms. Compared to the baseline SwinIR, applying TaylorIR achieves state-of-the-art performance while reducing memory consumption by up to 60%, as demonstrated in extensive experiments across standard benchmarks and exemplified in Figure 1. Our key contributions are as follows: • Pixel-Wise Patch Embedding: We adopt a 1\times1 patch size approach for transformer-based SR, allowing for per-pixel processing and sharper detail reconstruction. • Efficient Large-Window Self-Attention: By enabling extended windows in SwinTransformer-based SR models, our employed TaylorShift [22] attention improves SR quality with reduced memory and computational costs associated with standard window attention. • Improving State-of-the-Art SR models: TaylorSwinIR outperforms current SR models in both PSNR and SSIM across multiple benchmark datasets, establishing a new efficiency-performance balance."
https://arxiv.org/html/2411.10224v1,MCL: Multi-view Enhanced Contrastive Learning for Chest X-ray Report Generation,"Radiology reports are crucial for planning treatment strategies and enhancing doctor-patient communication, yet manually writing these reports is burdensome for radiologists. While automatic report generation offers a solution, existing methods often rely on single-view radiographs, limiting diagnostic accuracy. To address this problem, we propose MCL, a Multi-view enhanced Contrastive Learning method for chest X-ray report generation. Specifically, we first introduce multi-view enhanced contrastive learning for visual representation by maximizing agreements between multi-view radiographs and their corresponding report. Subsequently, to fully exploit patient-specific indications (e.g., patient’s symptoms) for report generation, we add a transitional “bridge” for missing indications to reduce embedding space discrepancies caused by their presence or absence. Additionally, we construct Multi-view CXR and Two-view CXR datasets from public sources to support research on multi-view report generation. Our proposed MCL surpasses recent state-of-the-art methods across multiple datasets, achieving a 5.0% F1 RadGraph improvement on MIMIC-CXR, a 7.3% BLEU-1 improvement on MIMIC-ABN, a 3.1% BLEU-4 improvement on Multi-view CXR, and an 8.2% F1,mic-14 CheXbert improvement on Two-view CXR.","Radiology reports, crafted by experienced radiologists, meticulously document imaging findings from examinations such as X-rays, PET scans, and CTs, detailing abnormalities and initial diagnostic conclusions. These reports deliver vital imaging insights that enable physicians to develop efficient patient treatment strategies (Messina et al. 2022). However, the manual writing process is time-consuming and requires significant expertise, making it increasingly difficult to meet the demands of modern healthcare (Bannur et al. 2024), particularly in regions with limited medical resources. Figure 1: Comparison of existing methods and our proposed method for chest X-ray report generation. Existing methods generate reports from single-view radiographs, while our approach utilizes multi-view radiographs. Automatic chest X-ray report generation aims to produce detailed and accurate free-text reports from multi-view radiographs, helping radiologists improve diagnostic efficiency and consistency by providing high-quality draft reports. In clinical practice, limitations like X-ray equipment constraints and the complexity of human anatomical structures can prevent a single-view radiograph from achieving optimal imaging quality and adequately displaying the overall anatomical structure. As a result, multi-view imaging examinations, such as postero-anterior (PA), antero-posterior (AP), and lateral views, are crucial for accurate diagnostics and personalized treatment. Consequently, the number of radiographs varies across studies. Typically, each study comprises a collection of radiographs, a corresponding report, and a patient-specific indication (which may sometimes be absent). This variability is also evident in public chest X-ray report generation datasets such as MIMIC-CXR (Johnson et al. 2019) and IU X-ray (Demner-Fushman et al. 2016). Such variability makes it challenging to effectively utilize multi-view radiographs from the same study to enhance the clinical accuracy of generated reports. One intuitive approach (Li et al. 2019; Chen et al. 2020; Liu et al. 2024c, b) treats each radiograph as an individual study, generating reports from single-view radiographs (see Figure 1). However, this scheme fails to fully exploit the rich anatomical information available in multi-view radiographs, potentially leading to inaccurate and inconsistent reports. In addition, since the IU X-ray dataset predominantly includes studies with two-view radiographs, several studies (Chen et al. 2020, 2021; Yang et al. 2023) have developed two-view report generation approaches, showing promise in producing informative reports. Despite this, these methods often struggle to integrate into clinical workflows due to the varying number of radiographs per study. To address this challenge, we introduce a novel two-stage method, called Multi-view enhanced Contrastive Learning (MCL) for generating chest X-ray reports. In the representation learning stage, we propose multi-view enhanced contrastive learning for visual representation by leveraging semantic correspondences between multi-view radiographs within the same study and between these radiographs and their corresponding report. In the report generation stage, we utilize the cross-attention mechanism to exploit available indications fully, providing the model with patient background information. Additionally, we incorporate a transitional “bridge” for missing indications to reduce embedding space differences caused by the presence or absence of these indications. For multi-view enhanced contrastive learning, we first employ multi-positive contrastive learning to bring multi-view radiographs within the same study closer, improving the consistency of visual features. Then, we develop a multi-view fusion module to integrate varying numbers of radiographs per study, producing fused visual features for subsequent cross-modal alignment. Finally, we apply contrastive learning for instance-wise and token-wise alignment, maximizing agreement between multi-view radiographs and their corresponding report. We evaluate our proposed MCL on the MIMIC-CXR (Johnson et al. 2019), MIMIC-ABN (Ni et al. 2020; Hou et al. 2023a), and our curated Multi-view CXR and Two-view CXR datasets. Experiment results demonstrate the effectiveness of MCL, showing a 5.0% F1 RadGraph improvement on MIMIC-CXR, a 7.3% BLEU-1 improvement on MIMIC-ABN, a 3.1% BLEU-4 improvement on Multi-view CXR, and an 8.2% F1,mic-14 CheXbert improvement on Two-view CXR. Our key contributions are outlined as follows: • We propose a novel multi-view enhanced contrastive learning that facilitates cross-modal alignment between multi-view radiographs and their corresponding report, addressing the issue that existing algorithms cannot handle varying numbers of views. • We incorporate a transitional “bridge” for missing indications to reduce embedding space differences caused by their presence or absence, thereby enhancing the capture of patient background information. • We curate Multi-view CXR and Two-view CXR datasets from two public sources, ensuring that each study includes multiple radiographs. This supports research on multi-view report generation, particularly for scenarios involving varying numbers of views or two-view setups. Figure 2: Illustration of our proposed MCL, which comprises a visual encoder, a text encoder, and a text generator. MCL is a two-stage method: 1) Representation learning using multi-view enhanced contrastive learning. 2) Report generation based on patient-specific indications. Model inference solely involves stage 2, excluding the language modeling (LM) loss."
https://arxiv.org/html/2411.10213v1,An Empirical Study on LLM-based Agents for Automated Bug Fixing,"Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent and non-agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine seven proprietary and open-source systems on the SWE-bench Lite benchmark for automated bug fixing. We first assess each system’s overall performance, noting instances solvable by all or none of these sytems, and explore why some instances are uniquely solved by specific system types. We also compare fault localization accuracy at file and line levels and evaluate bug reproduction capabilities, identifying instances solvable only through dynamic reproduction. Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.","Large Language Models (LLMs) (Zhao et al., 2023) are advanced machine learning models trained on vast amounts of textual data, capable of understanding and generating human-like text. LLM-based Agents (Xi et al., 2023) are systems that utilize large language models to interact with the environment and accomplish specific tasks. Recently, LLM-based Agents have demonstrated significant influence in automated bug fixing in code repositories (Kang et al., 2023; Zhang et al., 2024; Yang et al., 2024a). Thanks to the powerful natural language processing capabilities of LLMs, these Agents can efficiently understand and analyze source code and its associated natural language descriptions, such as user-submitted issue descriptions and code comments. Additionally, through dynamic interaction with local environments (e.g., via terminal), LLM-based Agents can retrieve useful information from the code repository, perform code editing and execution, and iterate and validate repair results, thereby improving the accuracy and efficiency of bug fixes. This combination of LLM and environmental feedback has made automated bug fixing more efficient and feasible than ever before, providing revolutionary new tools for software maintenance and development. Researchers from both the industry (Liu et al., 2024a; hon, [n. d.]; gru, [n. d.]; Ma et al., 2024) and academia (Zhang et al., 2024; Tao et al., 2024) have developed LLM-based Agent systems to locate and fix bugs in code repositories. To evaluate the fault localization and repair capabilities of LLMs and various Agent systems, Jimenez et.al (Jimenez et al., 2023) proposed the evaluation datasets SWE-bench, with derived versions SWE-bench Lite (a subset of the full benchmark), and SWE-bench Verified (human annotated subset of SWE-bench published recently). These datasets contain real bugs from code repositories and can verify the correctness of the patches generated by Agents through unit tests. Recently, these datasets have become the most influential benchmarks in the field of automated bug fixing, attracting both academic and industrial participants to compete on the SWE-bench Lite leaderboard 111https://www.swebench.com/, with new submissions typically every one or a half week. However, no work has systematically analyzed the fault localization and repair capabilities of LLM-based Agents or the performance differences among various tools within these Agent systems. Regarding the SWE-bench Lite dataset itself, due to the quality of issue descriptions and the complexity of the logical dependencies related to the defects, some instances in the benchmark are easier for Agents to fix, while others are more difficult (Xia et al., 2024). As for the design of the systems, different designs exhibit different planning, reasoning, and problem solving capabilities, i.e. some systems adopting static approaches (gru, [n. d.]) and others adopting dynamic approaches (Zhang et al., 2024). We have also observed significant differences in the sets of cases that each system can solve. Therefore, analyzing the solving capabilities of LLM-based Agents on specific instances can not only help us better understand the current performance of Agents but also provide comparative insights to inspire future research directions. We collected the four most outstanding commercial systems (i.e. MarsCode Agent (Liu et al., 2024a), Honeycomb (hon, [n. d.]), Gru (gru, [n. d.]), Alibaba Lingma Agent (Ma et al., 2024)) and the three most excellent open-source systems (i.e. AutoCodeRover (Zhang et al., 2024), Agentless + RepoGraph (Ouyang et al., 2024), Agentless (Xia et al., 2024)) with top performances on the SWE-bench Lite Leaderboard and conducted a comprehensive analysis of the performance differences of each system. First, we evaluated the overall performance of LLM-based Agents in bug fixing, including statistics on the instances that all seven systems can solve and those that none can solve, and analyzed the reasons behind these results. We also explored why some instances can only be solved by Agent systems while others can only be solved by non-Agent systems. Next, we investigated the performance differences in fault localization among different systems and their causes, compiling file-level and line-level localization accuracy rates. Finally, we analyzed the impact of bug reproduction on bug fixing, and the common characteristics of instances that can only be solved through dynamic reproduction. Through data analysis, we have summarized several insights.To improve bug fixing, it is essential to enhance the model’s reasoning ability, enabling it to accurately identify bug-related information within an issue and reduce noise interference. For multiple potential repair locations, the model should leverage its reasoning capabilities to determine the most relevant location. From the Agentic flow perspective, there should be a strong focus on the quality of the issue and attention to multiple suspicious locations in the stack trace. The design should include mechanisms to verify the completeness of patches and consider their global impact. Mechanisms should also be implemented to mitigate the randomness of the model’s output or effectively utilize its diversity. In error localization, achieving line-level accuracy is more critical than file-level, due to the larger discovery space, necessitating finer-grained results. During the reproduction process, ensuring the correctness of reproductions is crucial, as incorrect reproductions can result in the failure of the entire solving process. Novelty and Contribution To the best of knowledge, this is the first work to: (1) Study the effectiveness of LLM-based Agents in automatic bug fixing for code repositories (2) Examine the effectiveness of different LLM-based Agents in Fault Localization and analyze the reasons for their differences (3) Investigate the impact of bug reproduction on bug fixing of LLM-based Agents (4) Summarize the current issues and future research directions for LLM-based Agents in bug fixing Paper Organization The remainder of this paper is organized as follows: Section 2 explains the background. Section 3 describes the study design. Section 4 presents the analysis results and findings. Section 5 discusses the analysis results and findings. Section 6 reports the threats to validity. Section 7 discusses related work, and Section 8 concludes the paper."
https://arxiv.org/html/2411.10175v1,The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning,"Visual Reinforcement Learning (RL) methods often require extensive amounts of data. As opposed to model-free RL, model-based RL (MBRL) offers a potential solution with efficient data utilization through planning. Additionally, RL lacks generalization capabilities for real-world tasks. Prior work has shown that incorporating pre-trained visual representations (PVRs) enhances sample efficiency and generalization. While PVRs have been extensively studied in the context of model-free RL, their potential in MBRL remains largely unexplored. In this paper, we benchmark a set of PVRs on challenging control tasks in a model-based RL setting. We investigate the data efficiency, generalization capabilities, and the impact of different properties of PVRs on the performance of model-based agents. Our results, perhaps surprisingly, reveal that for MBRL current PVRs are not more sample efficient than learning representations from scratch, and that they do not generalize better to out-of-distribution (OOD) settings. To explain this, we analyze the quality of the trained dynamics model. Furthermore, we show that data diversity and network architecture are the most important contributors to OOD generalization performance. 00footnotetext: *Correspondence to: moritz.schneider@de.bosch.com Project website: https://schneimo.com/pvr4mbrl","Reinforcement Learning (RL) provides an elegant alternative to classic planning and control schemes, as it allows for complex behaviors to emerge by just specifying a reward, rather than hand-modelling and tuning environments and agents. Despite their success, most methods need extensive data and can be used only on their respective task, lacking the generalization capabilities needed to handle the complexity of real world tasks. On hardware, RL is costly in terms of time and wear, therefore model-based approaches are attractive as they promise to improve sample efficiency. For many real-life problems, vision is an invaluable source of state information, but due to its high-dimensional nature it is challenging to incorporate it in RL algorithms. Therefore, the use of pre-trained visual representations (PVRs) is attractive as, intuitively, it promises to improve sample efficiency and generalization. Most existing approaches use or investigate PVRs in the context of model-free RL. For example, CLIP [1] is already widely used as pre-trained vision model for model-free robotic RL tasks [2, 3, 4]. One would assume that the benefits such representations yield for model-free settings equally apply to model-based methods. In model-based reinforcement learning (MBRL), features of convolutional neural networks (CNNs) are usually used as visual state representations, whereas other representation types such as keypoints, surface normals, depth, segmentation and pre-trained representations are often ignored. Moreover, model-based methods are usually trained under an objective mismatch as the training process is required to optimize the accuracy of the dynamics model and the overall performance of the agent at the same time [5]. Naturally, this makes the training procedure different and more difficult than in model-free settings. In this work, we focus on model-based RL and benchmark a set of representative PVRs on a set of challenging control tasks. To this end we want to answer the following question: i Is MBRL more sample efficient when using PVRs in contrast to learning a representation from scratch? Furthermore, PVRs are nowadays increasingly often trained on general datasets (e.g. ImageNet [6], Ego4D [7], etc.) and should be reusable in a wide variety of downstream tasks without further finetuning on in-domain data. We would like to empower downstream RL algorithms with corresponding generalization capabilities. Most existing implementations only investigate the distribution shift for the PVRs, but not for the downstream RL algorithm. This leads us to the additional questions: ii Can model-based agents generalize better to out-of-distribution (OOD) settings with PVRs (i.e. can PVRs pass on their generalization capabilities to model-based agents)? iii How important are different training properties like data diversity and network architecture of a PVR for model-based agents on a downstream RL task? Compared with the model-free RL approach, MBRL methods learn accurate models of the environment for efficient learning and planning. Therefore, additionally we want to investigate the final question: iv How does the quality of the learned dynamics models in terms of accumulation errors and prediction quality differ between models trained from scratch and those based on PVRs? Contributions. The key contributions of this paper are summarized as follows: • Benchmarking PVRs for MBRL. Using PVRs trained on diverse and general data, we study the generalization capabilities to out-of-distribution (OOD) settings of model-based agents utilizing these PVRs. To the best of our knowledge, we perform the first such comparison for MBRL. • OOD Evaluation for MBRL and PVRs. Most other benchmarks only look into the case that PVRs should facilitate better training performance. We additionally look into the case of shifting the distribution also for the underlying MBRL agent, i.e., we have large differences between training and evaluation set. In this way, we investigate to what extent PVRs transfer their generalization capabilities to downstream RL agents. Our experiments reveal that PVRs are often ineffective for the MBRL agents. Furthermore, agents using representations learned from scratch outperform the PVR-based agents most of the time. • Important OOD Properties of PVRs. We investigate and discuss important properties of PVRs for generalization in downstream control tasks in a MBRL context. We find that data diversity and network architecture are the most important contributors to OOD generalization performance. • Analysis of Model Quality. To explain our results more in depth, we analyze the quality of the trained world models and find that those which use representations learned from scratch are in general more accurate and have less accumulation errors regarding predicted rewards than the ones using PVRs."
https://arxiv.org/html/2411.10174v1,"A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural
Networks using Side-Channel Attacks","During the past decade, Deep Neural Networks (DNNs) proved their value on a large variety of subjects. However despite their high value and public accessibility, the protection of the intellectual property of DNNs is still an issue and an emerging research field. Recent works have successfully extracted fully-connected DNNs using cryptanalytic methods in hard-label settings, proving that it was possible to copy a DNN with high fidelity, i.e., high similitude in the output predictions. However, the current cryptanalytic attacks cannot target complex, i.e., not fully connected, DNNs and are limited to special cases of neurons present in deep networks. In this work, we introduce a new end-to-end attack framework designed for model extraction of embedded DNNs with high fidelity. We describe a new black-box side-channel attack which splits the DNN in several linear parts for which we can perform cryptanalytic extraction and retrieve the weights in hard-label settings. With this method, we are able to adapt cryptanalytic extraction, for the first time, to non-fully connected DNNs, while maintaining a high fidelity. We validate our contributions by targeting several architectures implemented on a microcontroller unit, including a Multi-Layer Perceptron (MLP) of 1.7 million parameters and a shortened MobileNetv1. Our framework successfully extracts all of these DNNs with high fidelity (88.4% for the MobileNetv1 and 93.2% for the MLP). Furthermore, we use the stolen model to generate adversarial examples and achieve close to white-box performance on the victim’s model (95.8% and 96.7% transfer rate).","During the last decade, the number of tasks for which Deep Neural Networks (DNNs) have proven their effectiveness has steadily increased, leading to the widespread adoption of these algorithms in a large variety of fields. From computer vision to text translation and images generation, DNNs are everywhere now, and the best models have become valuable intellectual property (IP). In the meantime, the parallel effort from hardware designers have made possible DNNs’ deployment on edge device. However, due to their high value, the IP of the deployed models must be protected against new attacks caused by the embedded context. Since the publication of a first side-channel attack against the IP of an embedded DNN [2], the number of physical-based attacks against DNNs has greatly increased. Several methodologies using side-channel attacks with the objective of the extraction of the DNN’s hyperparameters have been proposed [18, 11]. DNN parameters have also been targeted by physical attacks, via side-channel [32, 19], or through fault injection [27, 13]. These types of attack, targeting the parameters of the model, try to copy the targeted model and perform a model extraction attack. Model extraction is not only a threat to embedded DNNs but to any deployed DNNs. There is therefore a large variety of methods, and even objectives, for theses attacks. We can characterize the two main types of objectives or adversarial goals for model extraction with the terminology introduced in [17]: accuracy-based model extraction and fidelity-based model extraction. The first aims at gaining access to a substitute model with good performance on the task of the targeted model without having to perform the whole training process. The second has for purpose to clone the targeted model to acquire a copy as close as possible to the original. The cloned model can then be used to gain information on the victim’s DNN and potentially mount more powerful attacks against it. In this study, we will consider only fidelity-based model extraction. Jagielski et al. [17] were the first to propose a functional framework for fidelity-based model extraction of 1-layer neural network (NN) using the ReLU (Rectified Linear Unit) function. They exploited the gradient of the DNN to gain access to what they defined as critical points. Such points correspond to inputs where the activation value of one specific neuron is null. These points are visible in DNNs based on the ReLU activation function, since they correspond to points where discontinuities occur in the gradient of the DNN. Using this knowledge, they were able to extract a 1-layer fully-connected NN using the least-square (LSTSQ) algorithm. Due to the similarities with attacks targeting crypto-system, i.e., analysis of a large number of input-output pairs to gain information on a secret value, this method was later characterized as cryptanalytic extraction of DNNs. Both works in [6] and [28] successfully extended this result to deeper architecture, with the method proposed in [6] still achieving the highest fidelity today. One key limitation in the method proposed in [6] was the extraction of the neurons’ sign, for which they used an exhaustive search. This is a critical information, as assigning the wrong sign will deactivate the neuron when it should be activated, and inversely. Shamir et al. argued that their method would not scale well with larger architecture [4]. So they used crafted perturbations activating specific neurons to infer this information [4]. While very effective, all of these methods rely on the fact that the output of the DNN is composed of the full confidence score vector which allows gradient estimation. This corresponds to an ideal case for the attacker, and researches have aimed at removing this assumption in order to move on to more realistic scenarios. Recent work by Chen et al. [9] demonstrated that it was possible to extract DNNs in hard-label settings for small networks. This was further proved by the work of Carlini et al. [5], in which they successfully extracted a four hidden-layer DNN using only the hard-label. However, while these works no longer require the confidence score, they are all designed solely for fully-connected DNNs, excluding any target composed of a non-fully connected layer, such as a pooling layer. In parallel, several methods aiming for fidelity-based model extraction have been proposed using physical attacks. As mentioned before, the most notable examples of such attacks were presented in [27, 13]. They used fault injection to determine the value of a subset of the weights’ bits in the DNN and constraint learning for the rest of the weights. Contrarily to the methods previously mentioned, these are not restricted to fully connected DNNs and, as such, were successful in extracting various complex architectures, e.g., ResNet-34 or VGG-11 [27]. However, these methods are either limited to DRAM platforms performing DNN inference, i.e. Machine-Learning-as-a-Service (MLaaS) platforms, [27] or require access to an open device, on which the attacker has full control (i.e. white-box settings), to find the memory localization of the weights’ bits [13]. We summarize in Table I the results and the threat models from state-of-the-art (SOTA) frameworks performing fidelity-based model extraction using side-channel, fault injection or cryptanalytic methods. To the best of our knowledge, there is no method for extracting complex architectures, i.e., not restricted to fully-connected layers, without requiring an access to the confidence scores and an open device. Contributions: In this paper, we present an efficient framework, combining black-box side-channel attacks and cryptanalytic-based extraction of DNNs in a hard-label setting. We propose a new method to acquire critical points using a side-channel attack instead of the output of the DNN. This allows the attack to be performed in a hard-label settings, and offers higher precision than previous methodologies in extracting weights, resulting in higher fidelity. Additionally, our framework is not impacted by the output’s data format, and we are the first to propose results of cryptanalytic extraction with both 32- and 64-bit data. Furthermore, we introduce an alternative methodology to determine the sign of each neuron requiring only one hypothesis by layer. Finally, the main advantage of our method over other cryptanalytic extraction frameworks is the ability of the side-channel attacks to subdivide the DNN at each activation functions. Using this result, we improve the SOTA by targeting complex DNNs composed of non-fully connected layers, such as depth-wise separable convolution or pooling layers, in a MobileNetv1 architecture. We summarize the major contributions of our work as follows: • We present a new black-box side-channel attack on a constant-time implementation of the ReLU function introduced as a SOTA countermeasure. • We propose a new gradient-free extraction method improving both the precision on the weights’ extraction and the robustness to special cases of neurons. • We introduce a new method to infer the sign of the neurons without access to the confidence scores and requiring only the testing of one hypothesis by layer. • We build an end-to-end framework capable of extracting DNNs with high fidelity and not limited to fully-connected layers. • To prove the practicability of our contributions, we validate them through comparison against other SOTA frameworks, and by targeting a shortened version of MobileNetv1 embedded on an STM32F767ZI using the X-Cube-AI framework. We provide an implementation of our code at https://github.com/X. TABLE I: Overview of the state-of-the-art of fidelity-based model extraction attacks. Approach Attack type Full extraction (weight + bias) Hard-label setting Not restricted to fully connected DNN Random queries DNN’s datatype tested Targeted architecture (Most complex) ICML’20 [28] Cryptanalytic ✓ ✗ ✗ ✓ 64-bit float MLP 10-20-20-1 Crypto’20 [6] Cryptanalytic ✓ ✗ ✗ ✓ 64-bit float MLP 40-20-10-10-1 AC’24 [9] Cryptanalytic ✓ ✓ ✗ ✓ 64-bit float MLP 1024-2-2-1 Preprint [5] Cryptanalytic ✓ ✓ ✗ ✓ 64-bit float MLP 3072-256\times3-64-10 USENIX’19[2] Side-Channel ✗ ✓ ✓ ✓ 32-bit float MLP 784-200\times4-10 ICCAD’23[33] Side-Channel ✓ ✗ ✗ ✓ 64-bit float LeNet5 IEEE S&P’22[27] Fault Injection ✓ ✓ ✓ ✗ 8-bit data ResNet34 and VGG11 ESORICS’23 [13] Fault Injection ✓ ✗ ✓ ✗ 8-bit data CNN 3\times(Conv + Pooling + ReLU)-Linear This work Cryptanlytic and side-channel ✓ ✓ ✓ ✓ 32- and 64-bit float Shortened MobileNetv1 MLP 3072-256\times3-64-10 Attacks not in hard-label settings suppose an access to the confidence scores. Contribution not using random queries use a subset of the training or a testing dataset to perform part of their attacks."
https://arxiv.org/html/2411.10172v1,Increasing the Accessibility of Causal Domain Knowledge via Causal Information Extraction Methods: A Case Study in the Semiconductor Manufacturing Industry,"The extraction of causal information from textual data is crucial in the industry for identifying and mitigating potential failures, enhancing process efficiency, prompting quality improvements, and addressing various operational challenges. This paper presents a study on the development of automated methods for causal information extraction from actual industrial documents in the semiconductor manufacturing industry. The study proposes two types of causal information extraction methods, single-stage sequence tagging (SST) and multi-stage sequence tagging (MST), and evaluates their performance using existing documents from a semiconductor manufacturing company, including presentation slides and FMEA (Failure Mode and Effects Analysis) documents. The study also investigates the effect of representation learning on downstream tasks. The presented case study showcases that the proposed MST methods for extracting causal information from industrial documents are suitable for practical applications, especially for semi structured documents such as FMEAs, with a 93% F1 score. Additionally, MST achieves a 73% F1 score on texts extracted from presentation slides. Finally, the study highlights the importance of choosing a language model that is more aligned with the domain and in-domain fine-tuning.","Causal domain knowledge plays a vital role in various downstream tasks, such as risk assessment [12], root cause analysis [32] and data mining [1]. The smallest unit of the causal domain knowledge is known as a causal relationship. The causal relationship is a connection between two or more events or variables where one event or variable is the cause of the other. Causal domain knowledge is commonly documented either in unstructured or semi structured forms. Tabular formatted documents, typically utilized in the Failure Mode Effect Analysis (FMEA), serve as a prime example of semi structured documents. Furthermore, much causal domain knowledge is also found in presentation slides, which are documents that do not follow a predefined structure, and are typically used in industry for sharing information between the different teams and with customers. In FMEA documents, the manually created textual content presents several challenges such as non-standardized descriptions of the failure modes, effects, and root causes, and in many cases merged cells. In the case of merged cells, the description of multiple events within a single FMEA table cell [27], which can involve enchained relations. In an enchained causal relation, a cause or an effect in a causal relation is the cause or the effect of another causal relation. This creates a complex network of interconnected events where each cause and effect is both influenced by and influences other events in the chain. Furthermore, the increasing use of digital presentation slides as a medium for presenting and sharing information has made them a valuable source of knowledge [11]. Presentation slides stemming from processes like failure analysis requests encapsulate documented causal relations. These relations shed light on failures detected in products, providing a comprehensive understanding of the factors contributing to such issues. However, the unstructured nature of presentation slides, which combine visual and textual elements and use spatial positioning to document information, can make automated information access difficult. While these features are useful for presenting information to a live audience, they can make it difficult for automated systems to extract and understand the information contained in the slides. Presentation slides and FMEA documents are a rich source of causal domain knowledge. However, the sheer volume of documents in the industry can make it difficult to manually process and extract the information contained within them. This can lead to increased product development cycle time, which can be detrimental to overall productivity. Therefore, automatically extracting causal domain knowledge from unstructured documents like presentation slides, as well as from semi structured documents like FMEA documents, can be highly beneficial in increasing the availability and accessibility of this knowledge. Scholars have been addressing the topic of causal information extraction from text devising natural language processing (NLP) methods. Two common NLP approaches are lexical pattern-based methods [8] and statistical machine learning-based methods [41]. More recently, pre-trained transformers based language models are devised for extracting meaningful representation of the text in sequence tagging based approach for extracting causal information [10, 7, 34]. Also, research indicated that fine-tuning on domain specific data can enhance the vector text representation by better capturing the nuances and complexities of the domain-specific language, which can improve the overall performance on downstream tasks [19]. Even though causal information extraction has become a well studied topic in recent years, streamlined causal information extraction methods tend to struggle with the detection of detailed causal relations including nested and enchained relations [34, 7]. Furthermore, testing of these methods on different types of industrial documents is limited. To address this gap, the following research questions are addressed in this paper: • How effective are existing causal information extraction methods on different types of industrial documents? • What is the effect of text representation learning on the overall performance of causal information extraction in industrial settings? In summary, the aim of this research is to develop a method for detailed causal information extraction on industrial data, with which the availability and consistency of causal domain knowledge can be improved in the industrial world, facilitating more reliable data analysis and more informed decision-making. Our work has the potential to be applicable to a variety of domains, including teaching, where much causal domain knowledge is documented in unstructured or semi structured documents. By addressing these research questions, the contribution of this paper can be summarized as follows: • Extending causal information extraction methods to industrial documents, increasing the availability of causal domain knowledge for downstream tasks in the industry. • Providing guidance for practitioners working in different industries with similar types of documents. • Addressing data consistency issues commonly found in semi structured documents, like the merged cells in FMEA (Failure Mode and Effects Analysis) documents. • Contributing to the body of research that highlights the effect of representation learning on downstream tasks."
https://arxiv.org/html/2411.10171v1,"Imagine-2-Drive: 
High-Fidelity World Modeling in CARLA for Autonomous Vehicles","In autonomous driving with image based state space, accurate prediction of future events and modeling diverse behavioral modes are essential for safety and effective decision-making. World model-based Reinforcement Learning (WMRL) approaches offers a promising solution by simulating future states from current state and actions. However, utility of world models is often limited by typical RL policies being limited to deterministic or single gaussian distribution. By failing to capture the full spectrum of possible actions, reduces their adaptability in complex, dynamic environments. In this work, we introduce Imagine-2-Drive, a framework that consists of two components, VISTAPlan, a high-fidelity world model for accurate future prediction and Diffusion Policy Actor (DPA), a diffusion based policy to model multi-modal behaviors for trajectory prediction. We use VISTAPlan to simulate and evaluate trajectories from DPA and use Denoising Diffusion Policy Optimization (DDPO) to train DPA to maximize the cumulative sum of rewards over the trajectories. We analyze the benefits of each component and the framework as a whole in CARLA with standard driving metrics. As a consequence of our twin novelties- VISTAPlan and DPA, we significantly outperform the state of the art (SOTA) world models on standard driving metrics by 15% and 20% on Route Completion and Success Rate respectively.Project website: https://anantagrg.github.io/Imagine-2-Drive.github.io/Keywords: Diffusion Policy, World Model based Reinforcement Learning","I INTRODUCTION Autonomous driving systems must operate safely and effectively in complex, dynamic environments, where accurate prediction of future events and diverse behavioral modeling are critical for informed decision-making. The ability to foresee potential obstacles, navigate uncertain traffic conditions, and make proactive adjustments to driving strategies relies on robust future prediction capabilities. World model-based Reinforcement Learning approaches [pan2022isodreamisolatingleveragingnoncontrollable, li2024think2driveefficientreinforcementlearning] have emerged as a promising solution to this challenge by simulating future states based on current observations and actions. These models enable autonomous vehicles (AVs) to internally “imagine” possible future scenarios, facilitating more efficient exploration and reducing the risks and costs associated with real-world interactions. However, the utility of these world models is often limited by the nature of traditional RL policies. Most RL policies are constrained to deterministic outputs or single gaussian distributions, which fail to capture the full range of possible behaviors. This undermines the adaptability of the world models and their ability to handle the complexity and variability found in driving environments. The significant advantages of world models also highlights the importance of learning an accurate world model. Current WMRL [hafner2020dreamcontrollearningbehaviors, hafner2022masteringataridiscreteworld, hafner2024masteringdiversedomainsworld, pan2022isodreamisolatingleveragingnoncontrollable, deng2021dreamerproreconstructionfreemodelbasedreinforcement] approaches, model the environment dynamics in latent space using a Recurrent Neural Network (RNN) based network. A common limitation of these approaches is their reliance on single-step transition models, where errors accumulate over multi-step planning, causing planned states to drift from the on-policy distribution. Prior works [wang2023drivedreamerrealworlddrivenworldmodels, ding2024diffusionworldmodelfuture, gao2024vistageneralizabledrivingworld] leverage video diffusion [blattmann2023stablevideodiffusionscaling] based approaches to predict the future states in a single pass, with VISTA [gao2024vistageneralizabledrivingworld] being the most versatile and accurate. To overcome these limitations, we present Imagine-2-Drive , a novel framework that incorporates two key innovations: VISTAPlan, a high-fidelity world model designed for precise future prediction, and DPA, a diffusion-based policy actor that models diverse behavioral modes for trajectory planning. VISTAPlan extends upon the VISTA’s prediction capabilities by incorporating additional modules to predict reward and discount factors. These enhancements enable VISTAPlan to function as a comprehensive world model, facilitating more effective planning and decision-making. Similar to [janner2022planningdiffusionflexiblebehavior, saha2023edmpensembleofcostsguideddiffusionmotion, ze20243ddiffusionpolicygeneralizable, jiang2023motiondiffusercontrollablemultiagentmotion, yang2024diffusionesgradientfreeplanningdiffusion], DPA utilizes a diffusion based model to predict the trajectory (a sequence of actions) in a single pass. Being a generative model, it allows DPA to model multiple behavioral modes. By incorporating the diverse behavior patterns inherent to diffusion policies, our framework can explore a broader range of behaviors, thereby improving its overall performance and robustness. DPA is trained with DDPO [black2024trainingdiffusionmodelsreinforcement] using VISTAPlan to simulate and evaluate trajectories to maximize the cumulative reward over the trajectories We validate our framework in the autonomous driving domain in CARLA [dosovitskiy2017carlaopenurbandriving] simulator, demonstrating its superiority over existing methods. A comprehensive evaluation across diverse driving scenarios highlights the contributions of each component, and how they effectively complement one another to enhance the overall performance. To summarize, our key contributions are: 1. We propose VISTAPlan, a high fidelity world model based reinforcement learning framework for autonomous driving, with image as the sole input modality. VISTAPlan extends VISTA by incorporating additional reward and discount factor heads enabling it for effective planning and decision-making. 2. DPA, a novel diffusion based policy actor which can model multiple behavioral modes, thereby enabling it to explore a broader range of behaviors. It is trained using DDPO to maximize the cumulative sum of rewards over trajectories. 3. Thorough analysis of our approach and baselines in complex CARLA driving scenarios over standard driving metrics to understand the effectiveness of our framework and validating each component."
https://arxiv.org/html/2411.10152v1,Causal Time-Series Synchronization for Multi-Dimensional Forecasting,"The process industry’s high expectations for Digital Twins require modeling approaches that can generalize across tasks and diverse domains with potentially different data dimensions and distributional shifts i.e., Foundational Models. Despite success in natural language processing and computer vision, transfer learning with (self-) supervised signals for pre-training general-purpose models is largely unexplored in the context of Digital Twins in the process industry due to challenges posed by multi-dimensional time-series data, lagged cause-effect dependencies, complex causal structures, and varying number of (exogenous) variables. We propose a novel channel-dependent pre-training strategy that leverages synchronized cause-effect pairs to overcome these challenges by breaking down the multi-dimensional time-series data into pairs of cause-effect variables. Our approach focuses on: (i) identifying highly lagged causal relationships using data-driven methods, (ii) synchronizing cause-effect pairs to generate training samples for channel-dependent pre-training, and (iii) evaluating the effectiveness of this approach in channel-dependent forecasting. Our experimental results demonstrate significant improvements in forecasting accuracy and generalization capability compared to traditional training methods.","The number of sensors and the corresponding data produced in the process industry continuously increases, enabling a rich source of sensor data [13], which presents an unprecedented opportunity to harness complex sensor data for enhancing operational efficiency and decision-making. Manufacturers in the European Union, in particular, face the dual challenge of accelerating decision-making processes while conforming with ambitious energy-efficiency standards to reduce greenhouse gas emissions from production plants [7]. These challenges underscore the need for intelligently managed and controlled manufacturing lines, highlighting Digital Twins as the key enabler to increase competitiveness, sustainability, and efficiency [12]. A Digital Twin (DT) in the process industry represents a virtual model of a production system, integrating sensor data, simulations, and real-time data processing to predict and optimize the production process [22]. Recently, the evolution of DTs embraces cognitive capabilities, i.e. cognitive DTs (see [1], [17]), showcasing their progression towards autonomy and intelligence. A cognitive DT should be capable of learning from and providing decision-support for various tasks and use cases, including classification and regression-related tasks, necessitating the training across various datasets with different multi-dimensional data characteristics. Transfer Learning (TL) and Self-Supervised Learning (SSL) are promising research directions and may tackle the complex task of developing such ”higher-level” DTs [17]. \mathbf{X}^{(1)}\mathbf{X}^{(2)}\mathbf{X}^{(N)}\hat{\mathbf{X}}^{(1)}\hat{\mathbf{X}}^{(2)}\hat{\mathbf{X}}^{(N)}{f}{f}{f}(a) CI Model\mathbf{X}^{(1)}\mathbf{X}^{(2)}\mathbf{X}^{(N)}\hat{\mathbf{X}}^{(1)}\hat{\mathbf{X}}^{(2)}\hat{\mathbf{X}}^{(N)}{f}(b) CD Model\mathbf{X}^{(1)}\mathbf{Z}^{(1)}\mathbf{X}^{(2)}\mathbf{Z}^{(2)}\mathbf{X}^{(N)}\mathbf{Z}^{(N)}\hat{\mathbf{X}}^{(1)}\hat{\mathbf{X}}^{(2)}\hat{\mathbf{X}}^{(N)}{f}{f}{f}(c) CI+CD Model (ours) Figure 1: (a) Channel Independence (CI), i.e. each variable is treated as an isolated univariate problem only indirectly learning from other channels through shared weights; (b) Channel Dependence (CD), i.e. all variables are mixed and treated as a multivariate problem directly learning from other variables; (c) Proposed hybrid model (CI+CD), i.e. target variables \mathbf{X}^{(N)} are mixed with exogenous variables \mathbf{Z}^{(N)} forming blocks of cause-effect pairs, directly learning cause-effect behaviour of two variables through CD and indirectly learning from other cause-effects from other variable interactions through shared weights, i.e. CI. Illustrative example adapted from Wang et al. [26]. Time-series data in the process industry comprises different causally related variables, and accurate modelling requires capturing the relationships between these. Several channel-dependent (CD) strategies jointly model multiple variables using GNNs ([29], [11]), MLPs ([6], [26]), CNNs ([28]), Transformers ([19], [16]) [30]. Unexpectedly, while only modelling cross-time dependence, recently proposed channel-independent (CI) methods ([20], [3], [14]) often outperform CD methods on various benchmarks [30], possibly due to misaligned time-series and noise of non-causally related variables. An illustrative example of CD and CI forecasting is depicted in Fig.1. Training general-purpose models utilizing CD strategies offers clear advantages in the process industry, where variables influence the process state, such as control inputs, raw material properties, and environmental factors. In addition, Digital Twins rely on simulation and control capabilities, necessitating cross-variable dependence on exogenous information when forecasting target variables. Furthermore, the physical counterpart of the Digital Twin in the process industry often exhibits high inertia, meaning changes to a controllable variable may affect the system only after a certain delay, with the system slowly reaching equilibrium. Capturing such complex behaviour necessitates not only a CD approach, it is also important to consider that highly-lagged cross-variable dependency may not be captured by the context window, i.e. the (historical) data the model is conditioned on, of such models. 1.1 Contributions In this paper, we tackle the challenges of applying CD pre-training on multi-dimensional time-series datasets with complex causal structures or isolated causal clusters in addition to slow inertia and time misalignment, characteristics commonly encountered in the process industry. These characteristics may make it difficult for a CD pre-training strategy to efficiently converge across multi-dimensional datasets, as the models may constantly fit potential non-causally related variables and time-mismatched cause-and-effect behaviours. A related time-alignment problem is also encountered in a recently proposed control-flow DT for the process industry [18]; however, it is not tackled in their paper. We propose a novel hybrid pre-training strategy (see Fig. 1 (c)) based on synchronized cause-effect pairs (see Fig. 2) to mitigate the mentioned problems and to focus pre-training specifically on cause-effect behaviours, an essential part of simulation and control in industrial processes, by breaking down complex causal structures into smaller, more focused training samples. Effect \mathbf{X}^{(i)}Cause \mathbf{Z}^{(j)}Time t(a) non-synchronized cause-effect pair(t+\delta_{ij})Time t(b) synchronized cause-effect pair Figure 2: Illustrative example of a non-synchronized cause-effect pair (a), the effect lags with a time delay of two time-steps and a synchronized cause-effect pair (b), where the cause is shifted \delta_{ij} time-steps, i.e. a shift by two time-steps in this example, to account for the causal lag. Illustrative example adapted from Zhao et al. [30]. This contribution underscores the urgent need for more robust, specialized pre-training strategies to handle such complex data effectively and addresses the following research questions: Research Question 1: How can highly lagged relationships in industrial processes be identified and modelled using data-driven approaches? Research Question 2: How can one synchronize cause-effect pairs w.r.t causal lag and create context-horizon training samples for CD pre-training without losing information in the sample? Research Question 3: Do CD pre-training methods benefit from cause-effect synchronization for regression-related tasks like forecasting? This research has several sections that address the above-stated research questions systematically. Sec. 2 discusses predictive modelling and Digital Twins in the context of the process industry. We highlight the problem of process inertia and complex causal structures for generalized process modelling. Sec. 3 explains extracting a causal relationship graph using linear Granger Causality. Additionally, we propose to break up complex causal structures of multi-dimensional time-series data into pairs of synchronized cause-effect training samples. In Sec. 4, we validate the proposed methodology on synthetically generated data with known causal relationships on the task of forecasting. In Sec. 5, we synthesize the findings from the conducted research and the experimental results, evaluate the stated research questions and discuss shortcomings and possible solutions."
https://arxiv.org/html/2411.10137v1,Legal Evalutions and Challenges of Large Language Models,"In this paper, we review legal testing methods based on Large Language Models (LLMs), using the OPENAI o1 model as a case study to evaluate the performance of large models in applying legal provisions. We compare current state-of-the-art LLMs, including open-source, closed-source, and legal-specific models trained specifically for the legal domain. Systematic tests are conducted on English and Chinese legal cases, and the results are analyzed in depth. Through systematic testing of legal cases from common law systems and China, this paper explores the strengths and weaknesses of LLMs in understanding and applying legal texts, reasoning through legal issues, and predicting judgments. The experimental results highlight both the potential and limitations of LLMs in legal applications, particularly in terms of challenges related to the interpretation of legal language and the accuracy of legal reasoning. Finally, the paper provides a comprehensive analysis of the advantages and disadvantages of various types of models, offering valuable insights and references for the future application of AI in the legal field.","In recent years, the breakthrough of deep learning technology in natural language processing (NLP), particularly the rapid advancement of Transformer technology, has led to the flourishing of LLMs [1]. Models like OpenAI’s GPT series have demonstrated exceptional capabilities in NLP, excelling not only in traditional NLP tasks such as machine translation and Question Answering, but also in some multimodal tasks, such as image-to-text translation, speech recognition, and subtitle generation [2, 3, 4, 5]. These models are capable of accurately understanding relationships between various data forms and enabling cross-modal information transformation, significantly enhancing automation and efficiency across these fields. In the legal field, LLMs are seen as a transformative force with the potential to revolutionize traditional legal services, owing to their comprehensive legal knowledge base and exceptional capabilities in natural language understanding and generation [6]. Some studies have explored the application of LLMs in the analysis and generation of legal texts, evaluating their performance in tasks such as legal reasoning, case retrieval, and legal question answering, and investigating their potential to improve the efficiency and accuracy of legal work [7]. Meanwhile, other researchers have focused on developing LLMs specifically tailored for legal domains, enabling these models to better understand legal terminology, apply legal provisions accurately, and adapt to the nuances of different legal systems. This specialization aims to increase the practical value of LLMs in legal practice [8, 9, 10]. However, effectively evaluating the performance of LLMs across various legal systems and linguistic environments remains a significant challenge. Additionally, addressing the technical and ethical concerns associated with their application is an urgent issue that requires further attention and resolution. The application of LLMs in the legal field also faces numerous challenges and issues. First, legal language is highly specialized and precise, making it crucial to ensure the accuracy and legality of the content generated by these models [11, 12]. Second, LLMs may absorb biases and inaccuracies from their training data, which can have serious repercussions when applied in the legal context [13, 14, 15]. Additionally, the automation of legal decision-making processes could lead to ethical concerns and disputes over legal accountability [16, 17]. As shown in Fig 1 Based on this background, this work aims to provide a comprehensive overview of the performance of LLMs in the legal field, offering valuable insights for both the academic community and legal practitioners. The study is structured as follows: Section 1: This Section explains the background, purpose, and significance of the study, outlining the motivations and objectives behind the research. Section 2: This Section provides a detailed analysis of legislation related to large models on a global scale, exploring the similarities and differences in policies and regulations across various countries. Section 3: The focus is on models specifically tailored to the legal domain, examining their technical features and evaluating their potential applications in legal practice. Section 4: This Section presents a comprehensive assessment of the models discussed in Section 3 using thirteen Chinese and thirteen English legal cases. The cases were selected to include a complete set of four components: judgment, background, analysis and conclusion. The Section systematically evaluates the performance and applicability of each model through a comparative analysis of results and quantitative metrics. Section 5: This Section discusses key issues related to the use of LLMs in the legal field, including data privacy, legal liability, ethical considerations, and technical limitations. Section 6: The final Section summarizes the findings of the study and provides an outlook on future research directions. Through this study, we aim to provide in-depth insights into the application of LLMs in the legal area, fostering their rational and sustainable integration into legal practice. This will not only contribute to improving the efficiency and quality of legal services but also lay a solid foundation for future innovations in legal technology. Figure 1: Overview of the study"
https://arxiv.org/html/2411.10108v1,Identifying Key Drivers of Heatwaves: A Novel Spatio-Temporal Framework for Extreme Event Detection,"Heatwaves (HWs) are extreme atmospheric events that produce significant societal and environmental impacts. Predicting these extreme events remains challenging, as their complex interactions with large-scale atmospheric and climatic variables are difficult to capture with traditional statistical and dynamical models. This work presents a general method for driver identification in extreme climate events. A novel framework (STCO-FS) is proposed to identify key immediate (short-term) HW drivers by combining clustering algorithms with an ensemble evolutionary algorithm. The framework analyzes spatio-temporal data, reduces dimensionality by grouping similar geographical nodes for each variable, and develops driver selection in spatial and temporal domains, identifying the best time lags between predictive variables and HW occurrences. The proposed method has been applied to analyze HWs in the Adda river basin in Italy. The approach effectively identifies significant variables influencing HWs in this region. This research can potentially enhance our understanding of HW drivers and predictability.","The occurrence of heatwaves (HWs), characterized by prolonged periods of abnormally high temperatures exceeding typical local conditions, has become a pressing concern in recent years due to their severe societal and environmental impacts [1, 2]. Since 1950, extensive regions worldwide have witnessed numerous prolonged and intense HWs, resulting in significant consequences for human mortality, regional economies, and natural ecosystems [3, 4, 5, 6, 7]. In agriculture, heat stress on crops can significantly reduce yields, leading to food insecurity. In addition, increased demand for electricity for cooling during HWs substantially strains power grids. The escalation in the frequency of HWs has been documented in various parts of the globe in recent years and is at least partly attributed to the temperature increases driven by anthropogenic warming [8, 9]. Numerous studies [10, 11, 12] have consistently highlighted that the ongoing increase in global surface temperatures will lead to significant alterations in the frequency and intensity of HWs across Europe by the end of this century. This trend is not confined to Europe; globally, there is also a growing prevalence of heat extremes, with projections indicating that these events will continue to increase in the coming decades [13, 14, 15]. Regional differences can be encountered in HW projections. Hence, this leads to diverse drivers and climate forcings on regional scales. The identification of these drivers plays a key role in understanding regional variations and in developing effective mitigation and adaptation strategies, as different regions may experience distinct climate impacts due to a combination of local factors and global climate forces. Moreover, understanding these drivers is crucial for improving forecasts on sub-seasonal scales, allowing for more accurate predictions of HWs and other extreme events. When tackling the challenge of HW detection or prediction, it is necessary to understand the mechanisms responsible for these extreme events. Although the underlying processes remain not entirely understood [1], an increasing number of studies have delved into these mechanisms and physical drivers that contribute to the formation and prediction of HWs [16, 17]. HWs are the product of intricate interactions between large- and small-scale processes that operate across diverse temporal scales. These events are highly influenced by atmospheric circulation, often regarded as a fast-acting driver, as well as anomalous conditions in slowly changing climate components, which can serve as proximate factors (e.g., land surface) or remote factors (e.g., upper ocean temperature, or sea ice) affecting HWs occurrence [18, 19, 20]. In the extratropics, atmospheric circulation patterns that influence HWs include quasi-stationary synoptic-scale high-pressure systems (anticyclones) [21, 22], whose predictability at a seasonal scale is low due to the influence of the chaotic variability of the atmosphere [23]. Finally, long-term trends in frequency, duration, and intensity of HWs are primarily driven by anthropogenic forcings, including global factors such as greenhouse gas concentrations and regional factors like land-use/land-cover changes and aerosol emissions [24]. However, these are out of the scope of this paper. In close relation to the previous discussion, and considering the vast volume of available spatial and temporal data, employing data-driven methodologies becomes indispensable for uncovering potential HW drivers. A limited body of literature addresses this subject using ML and feature selection and dimensionality-reduction approaches. Some works [25, 26, 27] employed Principal Components Analysis (PCA) to reduce and optimize the number of highly correlated variables, using them as inputs in some ML algorithms. In [28], authors aimed to identify the role of the individual drivers for five HWs in the recent decade through factorial experiments, which force the model toward observations for one or several key components at a time, allowing to identify how much of the observed temperature anomaly of each event can be attributed to each driver. Other feature selection approaches have been used for different weather problems in searching for optimal input variables. In [29], an extreme gradient boosting feature selection algorithm was applied with ML models in a problem of short-term relative humidity prediction. In [30], a nested loop of roughly pruned random forests was used for identifying significant drivers of daily streamflow from large-scale atmospheric circulation in Norway. In [31], a clustering method was applied to divide Morocco into regions that are spatially consistent in terms of extreme precipitation and to identify its drivers by analyzing atmospheric circulation anomalies during the occurrence of regional events. In [32], ML regression-based algorithms were used to identify the drivers of drought dynamics in the Free State Province. [33] shows the influence of different drivers to understand the causal mechanism of HWs over South-West India. For that purpose, climate model simulations and long-term observational data were proposed. This study proposes a general framework for HW driver identification, which can be applied to other extreme events in the context of detection and event short-term prediction. The framework is illustrated here to detect HWs in a European location. Specifically, the framework proposed in this work follows a two-phase methodology to obtain robust HW driver identification. In the first phase, a clustering algorithm is applied to variables identified as potential drivers, extracted from the ERA5 reanalysis dataset [34], and presented as time series. This clustering step reduces the dimensionality of the spatial domain by grouping nodes with similar time series patterns. In the second phase, a wrapper feature selection approach based on a multi-method ensemble evolutionary algorithm (PCRO-SL) [35] is employed to identify the most skilful drivers and periods for HW forecast over short-term (days to weeks) and seasonal horizons. The optimization algorithm’s fitness function performs a driver selection by evaluating the performance of an ML model for HW classification based on a subset of clustered drivers. The proposed framework is applied to the agricultural districts in the Adda river basin, located downstream of Lake Como, in the Lombardy region, Northern Italy. These districts are part of the Po Valley, one of the most productive European agricultural areas, which provides one-third of the national agricultural production [36]. Understanding the crop risks associated with extreme temperatures is becoming increasingly crucial to planning effective climate change adaptation strategies. The manuscript is organized as follows. First, a description of the data, including potential drivers and target variables used for developing the experiments, is provided in Section 2. Then, the spatio-temporal feature selection methodology is presented and detailed in Section 3. Subsequently, the experimental work and the results obtained are further described in Section 4. Finally, in Section 5, there is a discussion on the potential uses of the framework in wider-scale driver detection and on implications for forecasting."
https://arxiv.org/html/2411.10100v1,Multi-Task Adversarial Variational Autoencoder for Estimating Biological Brain Age with Multimodal Neuroimaging,"Despite advances in deep learning for estimating brain age from structural MRI (sMRI), incorporating functional MRI (fMRI) data presents significant challenges due to its complex data structure and the noisy nature of functional connectivity measurements. To address these challenges, we present the Multitask Adversarial Variational Autoencoder (M-AVAE), a bespoke deep learning framework designed to enhance brain age predictions through multimodal MRI data integration. The M-AVAE uniquely separates latent variables into generic and unique codes, effectively isolating shared and modality-specific features. Additionally, integrating multitask learning with sex classification as a supplementary task enables the model to account for sex-specific aging nuances. Evaluated on the OpenBHB dataset—a comprehensive multisite brain MRI aggregation—the M-AVAE demonstrates exceptional performance, achieving a mean absolute error of 2.77 years, surpassing conventional methodologies. This success positions M-AVAE as a powerful tool for metaverse-based healthcare applications in brain age estimation. The source code is made publicly available at: https://github.com/engrussman/MAVAE.","The advent of multimodal neuroimaging, which combines functional magnetic resonance imaging (fMRI) for the assessment of functional connectivity and structural magnetic resonance imaging (sMRI) for cortical morphology, offers a nuanced approach to the detection of cognitive impairment and the prediction of brain age [10]. However, the exploration of anatomical and functional differences in the brain between sexes using multimodal imaging for the estimation of brain age remains underexplored. Sex differences play a vital role in the brain’s ageing process, with notable anatomical and functional variations between male and female brains [11]. Incorporating sex information into age estimation models improves accuracy and has shown promise in deep learning applications [12]. Our research addresses this gap by integrating sex considerations in a multimodal imaging framework within the metaverse context, aiming to improve the accuracy and applicability of brain age predictions in personalised healthcare. Specifically, we propose a novel metaverse-based AI application for brain age estimation: the Multi-Task Adversarial Variational Autoencoder (M-AVAE). This innovative model merges adversarial learning and variational auto-encoding capabilities within a multitask learning framework, aiming for simultaneous estimation of brain age and prediction of sex from multimodal MRI data, including both sMRI and fMRI. The design of M-AVAE meticulously segregates the latent features of each imaging modality into distinct components, effectively disentangling the shared and unique attributes across modalities. This method not only improves the accuracy in capturing commonalities, but also minimises interference during data fusion, presenting a novel approach to multimodal neuroimaging analysis suitable for integration into metaverse platforms. Our major contributions can be summarised as follows. • We introduce a novel multimodal framework for the estimation of brain age within the metaverse ecosystem for healthcare. Integrating our AI model into a metaverse environment, may enable continuous, real-time updates and interactions, enhancing the precision and reliability of brain age estimations and may allow personalised, predictive healthcare. • Our approach is unique in creating a disentangled representation of brain imaging data by applying both adversarial and variational principles within a single architecture. This disentanglement allows for the clear differentiation of shared versus modality-specific information, paving the way for more nuanced interpretations of neuroimaging data within a metaverse platform. • Through rigorous evaluation of publicly available datasets, our extensive experiments validate the efficacy and robustness of the proposed framework, thereby establishing a new benchmark for brain age estimation models suitable for metaverse integration. Table 1: Summary of comparison of our work with the existing studies in term of availability of different components, i.e., multimodal, multitask, adversarial, and distangled learning. Author Multimodal Multitask Adversarial Disentangled (Year) learning learning learning learning He et al. 2022 [13] ✗ ✗ ✗ ✗ He et al. 2022 [14] ✗ ✗ ✗ ✗ Cheng et al. 2021 [15] ✗ ✗ ✗ ✗ Armanious et al. 2021 [16] ✗ ✓ ✗ ✗ Zhang et al. 2022 [17] ✗ ✓ ✗ ✗ Liu et al. 2023 [18] ✗ ✓ ✗ ✗ Dular et al. 2024 [19] ✗ ✓ ✗ ✗ Wang et al. 2023 [20] ✗ ✓ ✗ ✗ Mouches et al. 2022 [21] ✓ ✓ ✗ ✗ Cai et al. 2023 [22] ✓ ✗ ✗ ✓ Hu et al. 2020 [23] ✓ ✗ ✓ ✓ Our Study ✓ ✓ ✓ ✓"
https://arxiv.org/html/2411.10087v1,PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse,"Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers’ time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar pre-existing SSL method and competitive against the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse. The code is freely available at https://github.com/SPEECHCOG/PFML.","Self-supervised learning (SSL) can be described as a data-driven learning paradigm where the training process is guided by the inherent structure of the data itself. Unlike supervised learning that relies on externally provided labels, SSL exploits the intrinsic properties of the data to generate its own supervisory signal (Balestriero et al., 2023). SSL enables the model to learn rich feature representations from large amounts of unlabeled data that can be used as a starting point for downstream tasks, either as such or by fine-tuning the feature extractor to be better suited for solving some specific task (Erhan et al., 2010). Since typically there is an abundance of unlabeled data but a scarcity of labeled data, the use of SSL has been shown to reduce the need for large, manually annotated datasets (van den Oord et al., 2018; Baevski et al., 2020; Chen et al., 2020). In addition to SSL algorithms that have been developed for a single data modality, SSL algorithms that can be applied to multiple different data modalities have gained popularity in recent years (van den Oord et al., 2018; Akbari et al., 2021; Baevski et al., 2022; Wang et al., 2023). These methods and their extensions have shown great success in e.g. audio, image, and text data (van den Oord et al., 2018; Hénaff et al., 2020; Akbari et al., 2021; Baevski et al., 2022; Wang et al., 2023; Baevski et al., 2023; Yoon et al., 2023; Zhu et al., 2023; Lian et al., 2023). However, many SSL algorithms suffer from two issues: First, SSL algorithms are usually complex, with a plethora of hyperparameters that need careful tuning for the algorithm to work properly. This hinders the ability of SSL algorithms to be applied to new data domains, where the selection of these hyperparameters is not self-evident. For example, in contrastive learning-based SSL, the selection of positive and negative samples during training is essential for the algorithm to work properly. However, deciding which samples should be assigned to positive and negative categories is not always apparent (Kalantidis et al., 2020; Robinson et al., 2021; Balestriero et al., 2023). As another example, determining the number of clusters for clustering-based SSL algorithms (such as Caron et al. (2020) and Hsu et al. (2021)) in a new data domain or task can be difficult. Examples of such domains could include, for instance, different types of medical time-series data (e.g. EEG, ECG, or EMG recordings) that come in various dataset sizes and from various recording configurations. Second, a common failure mode during SSL pre-training is representation collapse, where the model ends up outputting a constant, time-invariant feature representation. Representation collapse is very common in SSL pre-training (Hua et al., 2021; Jing et al., 2022; Balestriero et al., 2023; Garrido et al., 2023), and many SSL methods apply different countermeasures to tackle the problem (see Section 3.1). In the present study, we propose a new SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). In PFML, the aim is to predict statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The overall methodological aim of our method is to have an SSL algorithm that would be as straightforward as possible to apply to various time-series data domains with minimal hyperparameter optimization, and without the risk of representation collapse. The contributions of the present study are as follows: 1. We propose a novel SSL algorithm for time-series data, PFML, that does not suffer from representation collapse, rendering the method straightforward to apply to new time-series data domains. To the best of our knowledge, PFML is the first work within the field of SSL for time-series data where the central idea of reconstructing statistical functionals is utilized. 2. We demonstrate the effectiveness of PFML using three different data modalities with complex, real-life classification tasks: infant posture and movement classification from multi-sensor inertial measurement unit (IMU) data, emotion recognition from speech data, and sleep stage classification from EEG data. 3. We show that PFML obtains both superior results against a conceptually similar pre-existing SSL method, and competitive results against the current state-of-the-art data modality agnostic SSL method, while also being conceptually simpler and without suffering from representation collapse."
https://arxiv.org/html/2411.10072v1,Real-Time AI-Driven People Tracking and Counting Using Overhead Cameras,"Accurate people counting in smart buildings and intelligent transportation systems is crucial for energy management, safety protocols, and resource allocation. This is especially critical during emergencies, where precise occupant counts are vital for safe evacuation. Existing methods struggle with large crowds, often losing accuracy with even a few additional people. To address this limitation, this study proposes a novel approach combining a new object tracking algorithm, a novel counting algorithm, and a fine-tuned object detection model. This method achieves 97% accuracy in real-time people counting with a frame rate of 20-27 FPS on a low-power edge computer.","I INTRODUCTION Keeping track of the number of people who have entered, exited, and are remaining inside a building or public transport is essential for crowd, facility, and safety management. Even though recent advancements in technology have paved the path for smart buildings and intelligent public transportation systems, it remains challenging to develop economical, efficient, and accurate systems to count people in real-time. Several works have been conducted since the early 2000s for real-time people counting using signal processing and deep learning techniques. Regardless of the technology they use, the reliability, accuracy of these, and cost to implement them are questionable. Either the highly accurate systems are extremely expensive or low-cost systems are not accurate and reliable. The use of time-of-flight concept sensors such as laser beam, thermal, and ultrasound sensors tends to fail when two or more people need to be counted. Jae Hoon et al. [1] attempted using laser range finders. The system requires two sensors to function properly, and these sensors need specific placement depending on the environment. This customization makes it difficult to implement in various settings. In order to address these issues Jeong Woo et al. [2] proposed a new method using two ultra-wide band radar sensors. Even in this approach, the placement of two sensors is crucial, placing them very close leads to undercounting, and placing them far apart leads to miscalculations such as double counting. Yanni et al. [3] and Tiang et al. [4] have done studies employing WiFi and analyzing the phase, but they are not ideal as WiFi easily get distorted by other objects leading the system for inaccurate results. In this deep learning and image processing era, several works have been carried out to count people. In [5], they introduced an image processing-based method employing background subtraction and blob detection to track objects, but it suffered from sensitivity to illumination changes and false positives caused by non-human objects in the frame. Li Guangqin et al. [6] proposed a depth camera-based solution to mitigate illumination issues but still faced challenges with false positives, while [7], inspired by [6], used depth cameras for background subtraction and 3D reconstruction for person detection, yet struggled with detecting individuals outside predefined human model dimensions. In response to challenges with traditional image processing methods, researchers have explored deep learning approaches for person detection and tracking. Guojin et al. [8] utilizes a convolutional neural network (CNN) for head detection and a spatio-temporal context tracking algorithm but faces issues with computational efficiency and sensitivity to head rotations, while [9] proposes “cluster pruning” to enhance real-time performance, achieving improved frames per second (FPS) values for people counting tasks but still falling short for monitoring door crossings due to low FPS rates. Therefore, a low-cost yet reliable system with a high FPS rate is needed for real-time people counting on edge devices. To this end, we propose a new efficient tracking algorithm, a counting algorithm, and a fine-tuned model for object detection in any complex environment. With these proposed methods we obtain higher FPS rates even when there are more than two people in the frame in both good and low lighting conditions. This achieves an overall accuracy of 97% in real-time video testing which is a 2% improvement compared to the best state-of-the-art and high frame rate: 20-27 FPS on average."
https://arxiv.org/html/2411.10071v1,Evidential Federated Learning for Skin Lesion Image Classification,"We introduce FedEvPrompt, a federated learning approach that integrates principles of evidential deep learning, prompt tuning, and knowledge distillation for distributed skin lesion classification. FedEvPrompt leverages two sets of prompts: b-prompts (for low-level basic visual knowledge) and t-prompts (for task-specific knowledge) prepended to frozen pre-trained Vision Transformer (ViT) models trained in an evidential learning framework to maximize class evidences. Crucially, knowledge sharing across federation clients is achieved only through knowledge distillation on attention maps generated by the local ViT models, ensuring enhanced privacy preservation compared to traditional parameter or synthetic image sharing methodologies. FedEvPrompt is optimized within a round-based learning paradigm, where each round involves training local models followed by attention maps sharing with all federation clients. Experimental validation conducted in a real distributed setting, on the ISIC2019 dataset, demonstrates the superior performance of FedEvPrompt against baseline federated learning algorithms and knowledge distillation methods, without sharing model parameters. In conclusion, FedEvPrompt offers a promising approach for federated learning, effectively addressing challenges such as data heterogeneity, imbalance, privacy preservation, and knowledge sharing.","In recent decades, deep learning has played a leading role in medical image analysis, including skin lesion classification. However, most of the existing methods rely on centralized learning, assuming data uniformity and accessibility, which often does not align with the reality of decentralized and privacy-sensitive clinical settings. This disparity not only limits progress in the field, but also exacerbates inequalities, with wealthier regions having a data advantage over poorer areas, leading to disparities in model performance and clinical support. Federated learning (FL) emerges as a promising solution to this challenge, enabling model training across distributed devices while preserving data privacy. Methods like FedAvg McMahan et al. (2017) and FedProx Li et al. (2020) have addressed issues such as non-i.i.d. data and system heterogeneity, yet they still face obstacles, particularly in scenarios with class imbalances and data heterogeneity. Evidential Deep Learning (EDL) Sensoy et al. (2018a) has found adoption in FL to handle these limitations in medical data, thereby enhancing model confidence and reliability, crucial for clinical applications. For example, the recent work on uncertainty-aware aggregation of federated models for diabetic retinopathy classification demonstrates its efficacy in improving model performance and reliability Wang et al. (2023). Furthermore, the scarcity of data poses an additional significant challenge, often leading to model overfitting and suboptimal federation performance. Recent techniques like learnable prompting Li and Liang (2021), particularly effective in low-data regimes, offer a promising solution by facilitating personalized model tuning across distributed clients Li et al. (2023). Nonetheless, privacy concerns persist, particularly due to the sharing and aggregation of model parameters, which poses the risk of reconstructing training images, as demonstrated by recent studies Zhang et al. (2023); Geiping et al. (2020); Zhu et al. (2019). To mitigate these concerns, one strategy involves sharing suitably-constructed synthetic data generated through generative models Pennisi et al. (2024). Yet, the use of generative models carries its own risks, potentially incorporating and synthesizing sensitive training samples, thus exacerbating privacy concerns. We here propose FedEvPrompt, a novel approach that integrates principles of evidential deep learning, prompt tuning, and knowledge distillation to address existing limitations comprehensively. FedEvPrompt leverages prompts prepended to pre-trained ViT models trained in an evidential learning setting, maximizing class evidence. Knowledge sharing across federation clients is achieved only through knowledge distillation on attention maps generated by ViT models, which offers greater privacy preservation compared to sharing parameters or synthetic images, as it lacks pixel-level details and reconstructive qualities. While our approach maintains a high level of abstraction for minimizing privacy leaks, it also provides richer information than average logits, as in FedDistill Seo et al. (2022), or prototypes, as in FedProto Tan et al. (2022). Thus, FedEvPrompt represents a principled way to share insights into the decision-making process of local models for enhanced federated performance, as demonstrated by the results achieved on a real-word distributed setting for skin lesion classification."
https://arxiv.org/html/2411.10057v1,KuaiFormer: Transformer-Based Retrieval at Kuaishou,"In large-scale content recommendation systems, retrieval serves as the initial stage in the pipeline, responsible for selecting thousands of candidate items from billions of options to pass on to ranking modules. Traditionally, the dominant retrieval method has been Embedding-Based Retrieval (EBR) using a Deep Neural Network (DNN) dual-tower structure. However, applying transformer in retrieval tasks has been the focus of recent research, though real-world industrial deployment still presents significant challenges. In this paper, we introduce KuaiFormer, a novel transformer-based retrieval framework deployed in a large-scale content recommendation system. KuaiFormer fundamentally redefines the retrieval process by shifting from conventional score estimation tasks (such as click-through rate estimate) to a transformer-driven Next Action Prediction paradigm. This shift enables more effective real-time interest acquisition and multi-interest extraction, significantly enhancing retrieval performance. KuaiFormer has been successfully integrated into Kuaishou App’s short-video recommendation system since May 2024, serving over 400 million daily active users and resulting in a marked increase in average daily usage time of Kuaishou users. We provide insights into both the technical and business aspects of deploying transformer in large-scale recommendation systems, addressing practical challenges encountered during industrial implementation. Our findings offer valuable guidance for engineers and researchers aiming to leverage transformer models to optimize large-scale content recommendation systems.","The Transformer (Vaswani, 2017) architecture has demonstrated significant success across multiple domains, with notable models such as BERT (Devlin et al., 2019) and GPT (Brown, 2020; Achiam et al., 2023) in natural language processing (NLP), and Vision Transformers (Alexey, 2020; Liu et al., 2021; Li et al., 2022) in computer vision (CV). These achievements underscore the Transformer’s remarkable capabilities in sequence modeling and parallelization. In the field of recommendation systems, Transformer-based architectures like SASRec (Kang and McAuley, 2018) and Bert4Rec (Sun et al., 2019) have also shown potential. However, these academic efforts often fail to address certain industrial challenges, which has limited their effectiveness in driving business success in large-scale recommendation systems, such as those at Kuaishou. Short-video recommendation poses unique challenges that demand advanced modeling techniques. The diverse nature of short-video content and the rapid evolution of user interests necessitate real-time adaptation to accurately capture these dynamic preferences. Users typically watch hundreds of short-videos each day, expressing preferences across a wide range of interest domains, while the system actively pushes diverse content to mitigate aesthetic fatigue and avoid the ”filter bubble” effect. As a result, models that rely on daily updates, such as PinnerFormer(Pancha et al., 2022), struggle to adapt to users’ evolving content needs. Moreover, traditional approaches like SASRec and Bert4Rec, which compress user behavior into a single interest vector, lack the capacity to accurately capture the full spectrum of user interests reflected in these interactions. To more effectively capture complex user interests, models like MIND (Li et al., 2019) and ComiRec (Cen et al., 2020) employ techniques such as capsule networks to extract multiple interest vectors from user action sequences. However, because these models do not utilize native Transformer-based architectures, they are limited in fully leveraging the advantages offered by Transformers. Furthermore, they do not address the performance overhead associated with processing long sequences, which is a significant concern in industrial applications. To effectively implement the Transformer model within Kuaishou’s large-scale short video recommendation system, this study conducts a detailed analysis of these specific challenges in the recommendation field and proposes a series of concise and effective solutions tailored to address these issues. • How to train with billion-scale item set: Large language models (LLMs) typically leverage next token prediction as the pretraining task. This involves calculating the probabilities of all tokens in the vocabulary being the next token, based on the historical sequence, and selecting the one with the highest probability as the next predicted token. General-purpose language models usually contain fewer than 100,000 tokens in their vocabularies (Tao et al., 2024). In the context of the Kuaishou short video recommendation system, the candidate pool contains billions of short videos, making it computationally prohibitive to compute probabilities for all candidates using a naive softmax approach. Efficient methods are therefore required to address the challenges of large-scale candidate selection while maintaining model performance. • How to capture user’s multi-interests: Different with language which have a clear semantics direction when predicting the next token. In our short-video services, users always have multiple interest points and a higher tolerance for short-videos watching. As a result, there maybe exists multiple short-videos with completely different semantics that are served as ‘positive’ next items at same time, which is unfriendly to vanilla Transformer learning. • How to extend to longer sequences with fewer computation resources: Different with large language model could stack very deeper and wider Transformers to achieve best performance with extremely higher computation resources. As a recommendation model, our model need to response a large amount request (about ¿ 50 billion requests every day), thus our Retrieval model should achieve the balance between efficiency and effectiveness. Particularly, the Transformer time complexity is \mathcal{O}(n^{2}d), where n denotes the input sequence length, d means hidden state dimension (Keles et al., 2022). Thereby Transformer-based model is sensitive with sequences length and we need to devise specific module to accelerate longer sequences training. In this work, we present KuaiFormer, our latest advancements in real-time industrial retrieval, which delivered the most significant improvements in the Retrieval stage at Kuaishou over the past year. Specifically, we introduce several reliable modifications to adapt Transformer to industrial retrieval scenarios: a customized softmax learning objective for stable model training, multiple query tokens to capture users’ diverse interests, and a historical sequence compression mechanism to improve the efficiency of long-sequence modeling. • Smooth In-Batch Softmax Loss with LogQ Correction: To avoid directly training on a billion-scale item set, we first employ in-batch softmax as the learning objective for KuaiFormer. However, in-batch softmax inevitably introduces sampling bias, deviating from uniform item sampling: popular items have more chances to be selected as negative samples, which can lead to performance degradation. We apply the widely-used logQ correction method (Yi et al., 2019) to correct for sampling bias. Additionally, in short-video services, users often have higher tolerance for watching, which reduces the confidence that negative samples in in-batch sampling truly represent items users dislike. Therefore, instead of using strict 0/1 labels for training, we incorporate label smoothing techniques (Müller et al., 2019) to mitigate training noise and enhance model robustness. • Multi-interests Query Tokens: To capture users’ diverse interests, we drew inspiration from the [CLS] token in BERT, which introduces a learnable token to compress the original input information into a holistic sequence representation. We extend this concept in KuaiFormer by introducing multiple learnable tokens, combined with a multi-interest training strategy to extract distinct user interest representations from historical item sequences. Specifically, KuaiFormer’s learnable query tokens leverage a causal masking mechanism, enabling subsequent interest tokens to fully interact with preceding interest token representations, thereby achieving more effective interest disentanglement. • Adaptive Item Compression Mechanism: To address the efficiency challenges of modeling longer sequences, we made an intuitive assumption: compared with the most recently watched short videos, users’ memories of earlier videos are more vague. Therefore, we can apply coarse-grained modeling to earlier items while using fine-grained modeling for the latest ones. Based on this, we devised an adaptive item compression mechanism: first, we divide the earlier item sequences into several groups, compressing each group into a single representation to reduce the input sequence length. This series of compressed representations is then concatenated with the most recent items, forming the token sequence input for the model. The main contributions of our work are as follows: • We propose our next-generation retrieval approach, KuaiFormer, to our knowledge, this work is the first real-time retrieval model by the pure Transformer architecture in industrial-scale RecSys. • We conduct extensive offline and online experiments to show KuaiFormer superiors, which contribute +0.360%/+0.126%/+0.411% online video watch time gains to Kuaishou short-video services. • We offer insights into both model and architectural aspects, addressing practical challenges encountered in industrial RecSys deployment. Our experience offer valuable guidance for engineers and researchers aiming to leverage Transformer to build better industrial RecSys."
https://arxiv.org/html/2411.10055v1,Towards unearthing neglected climate innovations from scientific literature using Large Language Models,"Climate change poses an urgent global threat, needing the rapid identification and deployment of innovative solutions. We hypothesise that many of these solutions already exist within scientific literature but remain underutilised. To address this gap, this study employs a curated dataset sourced from OpenAlex, a comprehensive repository of scientific papers. Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers on seven dimensions, covering climate change mitigation potential, stage of technological development, and readiness for deployment. The outputs of the language models are then compared with human evaluations to assess their effectiveness in identifying promising yet overlooked climate innovations. Our findings suggest that these LLM-based models can effectively augment human expertise, uncovering climate solutions that are potentially impactful but with far greater speed, throughput and consistency. Here, we focused on UK-based solutions, but the workflow is region-agnostic. This work contributes to the discovery of neglected innovations in scientific literature and demonstrates the potential of AI in enhancing climate action strategies.","The International Energy Agency (IEA) notes that about half the projected CO2 reductions that will be required to achieve Net Zero by 2050 will depend on technologies that are currently not commercially viable– highlighting the critical need for breakthrough innovations to mitigate the impacts of climate change [1]. Unlike the clear relationship between life sciences and biotech innovation, for example, there is no one academic field that dominates climate innovation. Potential solutions can emerge from disparate fields. Therefore, one likely reason for the neglect of certain climate solutions is the sheer volume and diversity of scientific literature. Traditional methods of knowledge discovery and synthesis may fail to capture innovative approaches buried in vast datasets, leading to missed opportunities for policy and technological advancement [2]. This is especially relevant for countries like the UK, which has a world-leading academic culture and made substantial investments to foster climate innovation but may still have untapped potential in its existing scientific outputs [12]. To address this challenge, we propose the use of machine learning (ML) and Large Language Models (LLMs) to systematically identify climate innovations in scientific literature. We leverage OpenAlex [8], a comprehensive open dataset of scholarly papers and comprehensive meta-data, to provide test data for analysis by state-of-the-art language models, such as GPT4-o from OpenAI. These models are prompted to evaluate paper abstracts across seven dimensions: climate emissions reduction/removal potential, technology level, deployability, market need, potential to enable subsequent innovation, mission focus of research, and neglectedness. Our hypothesis is that the research evidence base for many high-impact climate solutions is already documented in scientific papers from the UK but these have not yet been fully identified or systematically prioritised. Benchmarking the outputs of the LLMs against parallel human evaluations, we aim to assess the effectiveness of these models in finding overlooked innovations and identify any potential advantages over human reasoning. This research contributes to both the discovery of neglected climate solutions and the application of ML in enhancing domain-specific knowledge extraction, potentially accelerating climate action by uncovering actionable insights hidden within the existing literature."
https://arxiv.org/html/2411.10048v1,Physics-informed neural networks need a physicist to be accurate: the case of mass and heat transport in Fischer-Tropsch catalyst particles,"Physics-Informed Neural Networks (PINNs) have emerged as an influential technology, merging the swift and automated capabilities of machine learning with the precision and dependability of simulations grounded in theoretical physics. PINNs are often employed to solve algebraic or differential equations to replace some or even all steps of multi-stage computational workflows, leading to their significant speed-up. However, wide adoption of PINNs is still hindered by reliability issues, particularly at extreme ends of the input parameter ranges. In this study, we demonstrate this in the context of a system of coupled non-linear differential reaction-diffusion and heat transfer equations related to Fischer-Tropsch synthesis, which are solved by a finite-difference method with a PINN used in evaluating their source terms. It is shown that the testing strategies traditionally used to assess the accuracy of neural networks as function approximators can overlook the peculiarities which ultimately cause instabilities of the finite-difference solver. We propose a domain knowledge-based modifications to the PINN architecture ensuring its correct asymptotic behavior. When combined with an improved numerical scheme employed as an initial guess generator, the proposed modifications are shown to recover the overall stability of the simulations, while preserving the speed-up brought by PINN as the workflow component. We discuss the possible applications of the proposed hybrid transport equation solver in context of chemical reactors simulations.","The outstanding abilities of neural networks (NNs) in approximating complex relations have resulted in their successful application in many fields, ranging from image recognition and text comprehension to mimicking the solutions of differential equations encountered in complex engineering problems 1. One of the benefits brought by employing NNs as an alternative to traditional numerical methods is shifting the computational burden to the training phase, which is performed only once, thus enabling faster solution generation during the inference phase. This can be especially helpful in accelerating multi-stage simulations when the output of one computational method is used as an input to another one, as often encountered in engineering problems or digital twins designs 2, 3. An illustrative example can be found in chemical engineering problems related to ground-up modeling of chemical reactor or even entire chemical plants. In such applications, theoretical models are commonly available for finding the rates of both the micro-scale phenomena (e.g., molecular-level chemical reactions) and macro-scale phenomena (e.g., heat and mass transport). Their coupling results then in a system of equations which should be solved self-consistently, e.g., by solving the ‘micro-scale’ equations as a sub-task each time when the evaluation of the source terms in ‘macro-scale’ equations is required. Replacing solution of such sub-tasks with NNs is then an attractive option to accelerate the overal simulation. Despite their advantages, NNs, like many other models which learn from data, often lack interpretability. This makes their reliability in scientific or mission-critical applications questionable. Physics-informed neural network (PINN) approach has been proposed to partially overcome this drawback by incorporating the available theoretical knowledge into the NN training process 4. This approach suggests using exact equations known from the theory as the objectives that the function approximated by NN is expected to satisfy. Imposing such type of constraints often appears sufficient to make NN fit the solution of a theory-based (typically, physics-based) equation and in this way to become more interpretable. The physics-informed paradigm also requires minimal changes to the NN architecture and can be conveniently implemented in one of numerous specialized programming frameworks. However, as the loss function which is minimized during any NN training does not commonly reach exactly zero during or after this process, the theory-based constraints can only be satisfied approximately. Thus, the transfer of the theoretical knowledge into the PINN achieved by the physics-informed method remains incomplete. As will be discussed below, this can have significant consequences for incorporating PINNs into the multi-stage simulations. More broadly, PINNs can be viewed in context of wider family of methods, commonly known as the physics-informed machine learning (PIML). Within this family, there are other methods which are also intended to fuse theory-based and data-driven methods into a single computational model, but achieve this by modifying the architecture of the NN itself in order to make its output by design fulfil certain theory-based constraints exactly (e.g., 5, 6, 7, 8). Although none of the PIML approaches typically achieves complete transfer of all available theoretical knowledge into the NN, different approaches prioritize different aspects of it. To make this point more concrete, it is instructive to distinguish between the accuracy of the numerical values produced by NNs and the correctness of their dependence on the NN input parameters on the asymptotics. In this paper, we demonstrate that minor numerical inaccuracies of PINN as a function approximator can significantly affect the overall result of a multi-stage simulation, when the PINN acts as a source term in a diffusion-like equation (subsection 2.3). We further investigate the asymptotics suggested by the theory-based equations (subsection 3.1) and propose a modified PINN architecture which ensures the model follows them by design (subsections 3.2, 3.3). This is done for the particular case of reaction-diffusion system related to Fischer-Tropsch synthesis (FTS) process, which is widely used in chemical industry to produce synthetic hydrocarbons (the underlying equations are reviewed in subsections 2.1 and 2.2, along with the benefits of leveraging PINN for solving them). Finally, we demonstrate that a well-defined and guaranteed asymptotic behavior of a modified PINN is essential for constructing a conventional finite-difference equation solver enabling a stable convergence for the considered problem (subsection 3.4)."
https://arxiv.org/html/2411.10036v1,"Rethinking Normalization Strategies and Convolutional Kernels 
for Multimodal Image Fusion","Multimodal image fusion (MMIF) aims to integrate information from different modalities to obtain a comprehensive image, aiding downstream tasks. However, existing methods tend to prioritize natural image fusion and focus on information complementary and network training strategies. They ignore the essential distinction between natural and medical image fusion and the influence of underlying components. This paper dissects the significant differences between the two tasks regarding fusion goals, statistical properties, and data distribution. Based on this, we rethink the suitability of the normalization strategy and convolutional kernels for end-to-end MMIF. Specifically, this paper proposes a mixture of instance normalization and group normalization to preserve sample independence and reinforce intrinsic feature correlation. This strategy promotes the potential of enriching feature maps, thus boosting fusion performance. To this end, we further introduce the large kernel convolution, effectively expanding receptive fields and enhancing the preservation of image detail. Moreover, the proposed multipath adaptive fusion module recalibrates the decoder input with features of various scales and receptive fields, ensuring the transmission of crucial information. Extensive experiments demonstrate that our method exhibits state-of-the-art performance in multiple fusion tasks and significantly improves downstream applications. The code is available at https://github.com/HeDan-11/LKC-FUNet.","In reality, single-modal images capture limited information and each modality contains essentially different information [9]. Image fusion technology fully integrates the complementary information of different modalities to generate a comprehensive representation of the image [52]. It is widely used for scene information enhancement or restoration [23, 24, 51]. Moreover, object detection [60, 59] and semantic segmentation [25, 27] can also benefit from clearer representations of scenes and objects in the fused images [61]. Fusion tasks include infrared and visible image fusion (IVIF)[18, 47], medical image fusion (MIF) [29, 34], multi-exposure image fusion [19], and so on, where IVIF and MIF are very similar and challenging subcategories [40]. Specifically, in IVIF, fused images can avoid the drawbacks of visible (VIS) images that are sensitive to illumination conditions and infrared (IR) images that are noisy and low resolution [24, 63]. Similarly, MIF generates images that comprehensively reflect the information of tissues, organs, and metabolism to assist medical diagnosis and improve reliability [55]. (a) Histogram analysis of varied images. SD: standard deviation. (b) Impact of diverse normalization methods and large kernel convolution (LKC) on fusion performance. Figure 1: Evaluating pixel intensity distribution discrepancies and fusion results exhibition. In recent years, many deep learning methods [32, 38, 50, 56, 58, 64] have been developed to address challenges in image fusion. Common encoder-decoder models demonstrating promising results utilize convolutional neural networks (CNNs) [54, 57] and Transfomer [21, 33, 65] to extract features and reconstruct images. It has achieved relatively satisfactory convergence performance in IVIF. However, it has focused more on the improvement of objective indicators in MIF, without sufficiently considering the needs of MIF itself. In fact, there are the following significant differences between IVIF and MIF: 1) Fusion goals: IVIF emphasizes the overall structure and salient objects. Whereas in MIF, details and structural information must be fully retained, and tiny textures may indicate important lesion information. 2) Data distribution. Fig. 1(a) shows that IR and VIS images are Gaussian distributed, while medical images are highly sparsely distributed. 3) Statistical properties. Medical images possess more complex details, and their statistical values, such as average gradient (AG), spatial frequency (SF), and standard deviation (SD), are far higher than those of IR and VIS images. 4) Intra-task inter-sample differences. Fig. 1(a) shows that the pixel distributions of VIS images differ significantly between daytime and nighttime scenes. Meanwhile, computed tomography (CT) images have more high-brightness regions compared to magnetic resonance imaging (MRI) and positron emission tomography (PET) images, which might result in mutual interference during the fusion process. Some methods [5, 63] are trained to achieve only unified fusion on IR and VIS images. These methods attain excellent performance in IVIF but severe sacrifice of details in medical images, clearly overlooking differences 1-3. SDNet [54] and CDDFuse [61] are trained with different parameters to deal with the two tasks. Nevertheless, the emphasis continues to be on IVIF, while for MIF, only its model parameters are adjusted. This neglects the limited effective feature and the strict fusion requirements for detail retention in MIF and fails to solve the conflict between simultaneously retaining the high-brightness regions in CT/functional images and the detail information in MRI. We aim to rethink the fitness of the underlying components in this discrepancy case and design suitable models to resolve fusion conflicts and reduce inter-sample interference during training. Firstly, early fusion frameworks [8, 57] are derived from advanced vision tasks, in which the employed batch normalization (BN) seeks to normalize the feature distribution across the entire batch; but this approach ignored the independence of samples, leading to data smoothing. This has less impact on IVIF which emphasizes structure preservation. However, for medical images that are highly sparsely distributed and require strict detail retention, the interaction between samples will also cause a conflict between regions of high brightness and detail retention. While some methods [5, 12, 14, 25] forgo normalization to preserve sample independence, they fail to account for the inherent properties of images and the intrinsic relationships between features, resulting in limited improvements in fusion performance. Secondly, large kernel convolution (LKC) can capture spatial information within a wider range and is crucial for preserving image structure and details. However, its exploration in image fusion is limited, which may be related to its performance bottleneck. As depicted in Fig. 1(b), when BN is applied, the interaction among samples results in data smoothing, further reducing effective features. Here, the large receptive fields of LKC have a limited or even hindering effect on detail retention. Not using normalization fails to offer better image features and makes it hard to raise the upper limit of fusion performance. Finally, when using UNet [35], simple skip connections do not consider the relative importance of feature maps in different paths during the fusion process. This may result in crucial features being neglected. To address the above issues, we focus on exploring an efficient UNet to cope with the challenges of substantial differences among tasks and limited feature extraction. We employ a mixture of IN and GN to fully consider the sample independence, image properties, and intrinsic connections among features. This strategy enhances the generation of rich feature maps while more effectively preserving the distinctive attributes of source image pairs, as illustrated in Fig. 1(b). The strategy elevates the upper bound of fusion performance, at this point, the application of LKC enhances detail retention capabilities. Finally, the feature maps in the input decoder are recalibrated by combining spatial, channel attention, and bidirectional interactions. We fully consider four critical differences, thereby guaranteeing not only outstanding performance in MIF, but also full applicability to IVIF. Our contributions are summarized as follows: • A UNet with LKC is proposed to achieve multimodal image fusion (MMIF), namely LKC-FUNet, including IVIF and MIF. • Rethink the impact of normalization and LKC on image fusion. Verify the inappropriateness of BN in MMIF. Mixing IN and GN preserves image properties and is well suited for highly sparsely distributed fusion tasks. Under the above strategy, LKC enlarges the ”effective receptive field” to better preserve the detailed information and significantly improve the fusion performance. • A multipath adaptive fusion module is designed for feature fusion across different receptive fields and at various scales. Spatial-channel dual-attention feature maps, bidirectional interactions, and recalibration are used to provide more comprehensive inputs to the decoder. • Our method significantly improves multiple metrics in both tasks and achieves breakthroughs in MIF visualization. It is also shown to facilitate downstream multimodal object detection and semantic segmentation. Figure 2: Overall architecture of LKC-FUNet."
https://arxiv.org/html/2411.10028v1,MOT_FCG++: Enhanced Representation of Motion and Appearance Features,"The goal of multi-object tracking (MOT) is to detect and track all objects in a scene across frames, while maintaining a unique identity for each object. Most existing methods rely on the spatial motion features and appearance embedding features of the detected objects in consecutive frames. Effectively and robustly representing the spatial and appearance features of long trajectories has become a critical factor affecting the performance of MOT. We propose a novel approach for appearance and spatial feature representation, improving upon the clustering association method MOT_FCG. For spatial motion features, we propose Diagonal Modulated GIoU, which more accurately represents the relationship between the position and shape of the objects. For appearance features, we utilize a dynamic appearance representation that incorporates confidence information, enabling the trajectory appearance features to be more robust and global. Based on the baseline model MOT_FCG, we achieved 76.1 HOTA, 80.4 MOTA and 81.3 IDF1 on the MOT17 validation set, and also achieved competitive performance on the MOT20 and DanceTrack validation sets.","Multi-object tracking (MOT) aims to identify dynamic objects in each frame of a given video sequence and assign the same identity identifier to the same object across consecutive frames, thereby forming individual motion trajectories for different objects. The goal of MOT is to simultaneously track and recognize multiple targets in a scene, demonstrating enormous potential in fields such as video surveillance, intelligent transportation, autonomous driving, and sports broadcasting. Detection-based tracking methods (DBT) are mainstream methods for MOT, typically divided into two stages: detection and tracking. In the first stage, a convolutional neural network-based detector is used to label the objects of interest in each frame of the video sequence with bounding boxes. In the second stage, the tracker utilizes the output from the detector to extract the appearance and spatial motion features of the objects again, and calculates the similarity between these detection features and the existing trajectory features to perform trajectory association. How to effectively represent the appearance and spatial motion features of trajectories is a key factor affecting the performance of DBT methods. In this paper, we improve upon the clustering association method MOT_FCG by proposing a more global appearance embedding representation method and a more precise spatial motion information representation method. Ablation experiments validate the effectiveness of our proposed modules, and as shown in Figure 1 we achieve excellent performance on the MOT17 datasets. The main contributions of this paper are threefold: \bullet We point out that the use of median elements as trajectory appearance embedding features in MOT_FCG has limitations. To address this, we adopt a dynamic appearance embedding representation method. This method can adaptively adjust the weighting based on confidence. It integrates the global appearance embedding features of the trajectory, leading to a more comprehensive and holistic representation. \bullet To more accurately represent spatial motion information, we propose a new spatial metric method—Diagonal Modulated GIoU. This method can more precisely characterize the positional relationships between objects and to some extent reflect the shape information of the object bounding boxes. \bullet We achieved competitive results on the MOT17 and MOT20 datasets, and also validated the effectiveness of the proposed modules on the DanceTrack dataset."
https://arxiv.org/html/2411.10015v1,"MicroCrackAttentionNeXt: 
Advancing Microcrack Detection in Wave Field Analysis Using Deep Neural Networks through Feature Visualization","Micro Crack detection using deep neural networks(DNNs) through an automated pipeline using wave fields interacting with the damaged areas is highly sought after. However, these high dimensional spatio-temporal crack data are limited, moreover these dataset have large dimension in the temporal domain. The dataset presents a substantial class imbalance, with crack pixels constituting an average of only 5% of the total pixels per sample. This extreme class imbalance poses a challenge for deep learning models with the different micro scale cracks, as the network can be biased toward predicting the majority class, generally leading to poor detection accuracy. This study builds upon the previous benchmark SpAsE-Net, an asymmetric encoder–decoder network for micro-crack detection. The impact of various activation and loss functions were examined through feature space visualisation using manifold discovery and analysis (MDA) algorithm. The optimized architecture and training methodology achieved an accuracy of 86.85%.","Micro crack detection in materials is of significant importance due to the potential for catastrophic failures, which can lead to substantial financial losses and safety hazards in industries (Malekloo et al., 2022; Golewski, 2023). Detecting cracks in complex structures, like aircraft bodies or intricate machinery components, poses a substantial challenge using conventional methods like visual inspection or standard cameras, especially when dealing with complex geometries. The use of wave-based approaches for crack detection offers a powerful solution, as these methods allow for the analysis of structures that are not easily accessible or too complex to inspect manually. Convolutional Neural Networks (CNNs) are especially good at processing spatial data due to their ability to capture local spatial correlations within an image (LeCun et al., 2015). Nevertheless, standard segmentation methods, such as vanilla architectures, demonstrate limited performance on this particular dataset, due to the complex spatio-temporal nature of the crack patterns. This becomes even more significant when the cracks represent a tiny minority in the dataset, leading to poor detection accuracy. This issue is enhanced when dealing with very small cracks, as they not only lead to data imbalance but may also cause minimal disruption in wave behaviour. In such cases, the waves may exhibit minimal changes, making it difficult for the model to detect the cracks accurately. This challenge necessitates the development of a more tailored custom model. Our proposed MicroCrackAttentionNeXt is designed to overcome the limitations of vanilla models like UNet by incorporating enhanced spatial and temporal feature extraction. Unlike UNet Ronneberger et al. (2015), where the input and target share the same modality (image-to-image translation). Our model processes spatio-temporal input data and outputs spatial crack predictions, enabling it to handle more complex data while improving micro-scale detection accuracy. The asymmetric encoder-decoder structure, with attention layers is particularly effective as it focuses on capturing critical crack patterns rather than relying heavily on skip connections. The attention mechanism ensures that the model prioritizes the time steps when the waves interact with the cracks, improving detection precision. The DNNs capacity to recognise minute details and complex patterns in high dimensional data is impacted by the activation functions used, which becomes crucial in the micro-scale setting where accuracy is much needed. Activation functions enhance the network’s expressive power, enabling it to capture diverse features and representations. Rectified Linear Unit (ReLU) Nair and Hinton (2010) and its variants are commonly used activation functions. ReLU introduces non-linearity by setting negative values to zero, allowing positive ones to pass unchanged, which aids in deep network training. The ""dying ReLU"" issue, where neurons become inactive, hampers learning Xu et al. (2015); He et al. (2015a). Variants like Leaky ReLU mitigate this by allowing small negative slopes. SELU (Scaled Exponential Linear Unit) Klambauer et al. (2017) scales outputs to maintain self-normalizing properties, keeping activations near zero mean and unit variance. GeLU (Gaussian Error Linear Unit) Hendrycks and Gimpel (2023) enhances representation learning by incorporating probabilistic elements, though it has higher computational complexity. ELU (Exponential Linear Unit) Clevert et al. (2016) improves learning dynamics but is computationally expensive. Various Loss functions have been proposed in the literature to combat class imbalance issues in the DNN model. The loss functions tested are: 1)Dice LossLin et al. (2018), 2)Focal LossLin et al. (2018), 3)Weighted Dice LossYeung et al. (2021) and, 4)Combined Weighted Dice LossJadon (2020). These activation functions aim to strike a delicate balance between adaptability and computational efficiency, essential considerations in the micro-material domain, where capturing fine details is crucial for accurate crack detection. Empirical exploration and meticulous fine-tuning of these activation functions is imperative to identify the optimal choice that aligns with the distinctive characteristics of micro-material images. Ultimately, a nuanced and effective approach to crack detection in micro-materials relies on the thoughtful selection and optimization of activation functions within the CNN architecture. The extent of the influence of different activations is difficult to determine against conventional metrics such as accuracy and F1 score. Hence, it is imperative to analyse the internal dynamics of the model. Methods like Principal Component Analysis, t-SNE van der Maaten and Hinton (2008) and UMAP McInnes et al. (2020) are used to analyse the higher dimensional feature maps of these blackbox models against the target. However, these methods provide little to no insight when used on segmentation problems. In this study, we use the recently proposed Manifold Discovery Analysis (MDA) Islam et al. (2023) to qualitatively assess the impacts of various activation functions. Moreover, through this, we were able to analyse the effects activations had on the feature maps of the model, allowing us to choose the best activation function for the given problem. The primary contributions of this paper are: • Introducing MicroCrackAttentionNeXt – an incremental improvement over (Moreh et al., 2024). • Qualitative Investigation of the impact of activations MicroCrackAttentionNeXt through Manifold Discovery and Analysis. The paper’s structure is outlined in the following manner: Section 2 encompasses a concise yet informative overview of relevant studies. Section 3 deals with the dataset used and the proposed methodology. The assessment of the performance of the proposed system and the results obtained are included in Section 4. Ultimately, concluding remarks and future works are presented in Section 5."
https://arxiv.org/html/2411.10010v1,DeepMedcast: A Deep Learning Method for Generating Intermediate Weather Forecasts among Multiple NWP Models,"Numerical weather prediction (NWP) centers around the world operate a variety of NWP models, and recent advances in AI-driven NWP models have increased the availability of diverse NWP outputs. While this expansion holds the potential to improve forecast accuracy, it also raises a critical challenge of identifying the most reliable predictions for specific forecast scenarios. Traditional approaches, such as ensemble or weighted averaging, combine multiple NWP outputs but often generate unrealistic atmospheric fields, complicating the production of reliable and consistent forecasts in operational settings.In this study, we introduce DeepMedcast, a deep learning method that generates intermediate forecast, or ""medcast"", between two or more NWP outputs. Unlike ensemble averaging, DeepMedcast can provide consistent and explainable medcast without distorting meteorological fields. This paper details the methodology and case studies of DeepMedcast, discussing its advantages and potential contributions to operational forecasting.","In recent decades, numerical weather predictions (NWPs) and their post-processing have played a central role in issuing weather forecasts, warnings, and advisories (WMO, 2013; Vannitsem el al., 2021). NWP centers around the world have developed and are operating a variety of NWP models for accurate weather predictions. For example, the European Centre for Medium-Range Weather Forecasts (ECMWF) operates the Integrated Forecasting System (IFS) and its ensemble prediction system (ECMWF, 2024); the UK Met Office operates the Unified Model and the Met Office Global and Regional Ensemble Prediction System (Brown et al., 2012; Hagelin et al., 2017; Inverarity et al., 2023). The National Centers for Environmental Prediction (NCEP) at the National Oceanic and Atmospheric Administration (NOAA) operates the Global Forecast System (NCEP, 2016), the High-Resolution Rapid Refresh (Dowell et al., 2022), and the Hurricane Weather Research and Forecasting model (Gopalakrishnan et al., 2011). The Japan Meteorological Agency (JMA) operates three deterministic NWP models and two ensemble prediction systems for short-range to weekly forecasts: the Global Spectrum Model (GSM), the Meso-Scale Model (MSM), the Local Forecast Model, the Global Ensemble Prediction System, and the Mesoscale Ensemble Prediction System (JMA, 2024). These models cover different areas with varying resolutions and processes. In addition to traditional physics-based NWP models, recent advancements in artificial intelligence (AI) have introduced new methods for producing weather predictions. AI-driven NWP models, such as FourCastNet (Pathak et al., 2022; Bonev et al., 2023), GraphCast (Lam et al., 2022), Pangu-Weather (Bi et al., 2022, 2023), FengWu (Chen et al., 2023; Han et al., 2024), Aurora (Bodnar et al., 2024), GenCast (Price et al., 2023), and AIFS (Lang et al., 2024), have demonstrated the ability to enhance both the speed and accuracy of weather predictions by leveraging deep learning techniques to model complex atmospheric systems. At present, forecasters are able to use multiple NWP models including AI-driven NWP models, which provide a range of possible atmospheric states, allowing them to select the most plausible prediction from available NWPs. However, this raises a critical question: Which prediction is the most plausible? If the models have comparable accuracy, it is impossible to determine in advance which one is the best. One practical and widely used solution is to average the results from multiple NWP models or their post-processed outputs (Vislocky and Fritsch, 1997; JMA, 2018). The National Hurricane Center and the Joint Typhoon Warning Center in the United States use consensus forecasts (e.g., Simon et al. (2018); Cangialosi et al. (2023)), which are weighted averages, extensively for both tropical cyclone (TC) track and intensity predictions. JMA employs consensus forecasting for TC track predictions by averaging multiple NWP outputs to improve forecast accuracy (Nishimura and Fukuda, 2019; JMA, 2022). The UK Met Office operates the IMPROVER system (Roberts el al., 2023), which applies a weighted average of post-processing and nowcasts based on multiple NWP outputs. The National Weather Service (NWS) at NOAA operates the National Blend of Models (NBM), which provides statistically post-processed multi-model ensemble guidance (Hamill et al., 2017). The German Meteorological Service uses MOSMIX and ModelMIX (Primo et al., 2024), which are weighted averages of post-processing based on IFS, their global model, and their regional ensemble model. Additionally, the World Area Forecast Centre, comprising centers in London and Washington, operates harmonized forecasts, including mean, maximum, and minimum forecasts, from both NWP outputs for aviation hazards such as cumulonimbus clouds, turbulence, and in-flight icing (ICAO, 2016). It is straightforward to average the central position of TCs, extra tropical cyclones, or the location of fronts because averaging does not degrade their clarity. However, averaging atmospheric fields such as pressure or wind speed around these systems is not appropriate. This is because averaging can smooth out or distort these fields, weakening the central pressure or wind speeds around cyclones and fronts, resulting in predictions that are unrealistic, meteorologically inconsistent, and difficult to interpret. Forecasters must then choose between two options: using a single model that is explainable but potentially less accurate or using an averaged prediction that is unexplainable and unrealistic but may be more accurate. In response to this dilemma, we propose DeepMedcast, a novel approach that uses deep learning to generate intermediate forecast—or ""medcast""—between two or more NWP models. Unlike averaging, DeepMedcast can produce atmospheric fields around cyclones and fronts without smoothing out or disturbing their distributions. This capability is crucial in operational forecasts, where accurate and explainable predictions are needed for issuing reliable warnings and advisories. To the best of the authors’ knowledge, no existing method provides plausible intermediate forecasts by combining multiple NWP model outputs. This paper is structured as follows: Section 2 presents the methodology and data used for DeepMedcast, detailing the deep learning architecture and training process. Section 3 discusses the results of applying DeepMedcast to multiple NWP models with case studies, and Section 4 offers a discussion of the advantages of DeepMedcast and its contributions to operational forecasting. Finally, Section 5 concludes with a summary of the findings and future research directions."
https://arxiv.org/html/2411.10006v1,"Orca: Enhancing Role-Playing
Abilities of Large Language Models by Integrating Personality Traits","Large language models has catalyzed the development of personalized dialogue systems, numerous role-playing conversational agents have emerged. While previous research predominantly focused on enhancing the model’s capability to follow instructions by designing character profiles, neglecting the psychological factors that drive human conversations. In this paper, we propose Orca, a framework for data processing and training LLMs of custom characters by integrating personality traits. Orca comprises four stages: (1) Personality traits inferring, leverage LLMs to infer user’s BigFive personality trait reports and scores. (2) Data Augment, simulate user’s profile, background story, and psychological activities. (3) Dataset construction, personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4) Modeling and Training, personality-conditioned instruction tuning (PTIT and PSIT), using the generated data to enhance existing open-source LLMs. We introduce OrcaBench, the first benchmark for evaluating the quality of content generated by LLMs on social platforms across multiple scales. Our experiments demonstrate that our proposed model achieves superior performance on this benchmark, demonstrating its excellence and effectiveness in perceiving personality traits that significantly improve role-playing abilities.","Building human-like conversation agents is a long-term challenge for AI researchers. The emergence of groundbreaking language models such as ChatGPT and GPT-4 (OpenAI, 2023), coupled with their intrinsic capacity for emergent in-context learning (ICL) (Brown et al., 2020) abilities and a three-stage reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) algorithm which have largely raised the capacity bar of existing AI systems. LLMs have acquired a wealth of knowledge during their pre-training stage. ICL utilizes LLMs in a few-shot or zero-shot way that can instruct LLMs to understand the tasks in the form of natural language text. Therefore, personality-based responses have gained significant attention. Despite GPT-4 exhibit advanced role-playing capabilities because human-generated conversations are combined with the Instruct tuning dataset in a dialogue format for training, it is widely recognized that LLMs, suffer from a lack of consistent personality traits often failing to be engaging. This is the result of the existing LLMs are predominantly trained on general domains and lack specific optimization for personalized LLMs. Recently, human social behavior is being changed by role-playing applications, such as Character.AI which has attracted a growing number of researchers to bridge the gap between the text and behavior of dialogue agents and humans (Team, 2023; Wang et al., 2024). Personality-based dialogue systems can be broadly categorized into two types. (1) Persona-based Dialogue, represented by the work in Zhang et al. (2018) where the manipulation of profile information is employed to enhance the appeal of chit chat. These ideas are also used in the latest character LLMs such as CharacterGLM (Zhou et al., 2023), Ditto (Lu et al., 2024), and ChatHaruhi (Li et al., 2023), aiming to improve the humanity of customized characters. However, these approaches primarily create profile settings as prompts for model training, overlooking the psychological factors of human language and behavior. (2) Personality-aware Dialogue. It is more novel to attempt to establish a connection between personality traits and character compatibility. Wang et al. (2024) proposed a Social Support Conversation (S2Conv) framework–CharacterChat. To achieve this goal, it created a group that of virtual characters with distinct profiles called MBTI-1024 Bank based on the MBTI (Myers-Briggs Type Indicator) to train LLMs. In order to link individuals with persona-compatible virtual supporters. It designed a series of support agents and the interpersonal matching mechanism. But the psychological theory-based personality traits with implicit expression and behavior are not well modeled. Also in the field of emotional support, Dan et al. (2024) proposed a mixture of experts (MoE)-based personalized LLMs, named P-tailor, to model the Big Five Personality Traits such as openness, conscientiousness, extraversion, agreeableness and neuroticism. In fact, each BigFive dimension has six sub-dimensions (Gosling et al., 2003), P-tailor only categorizes the BigFive high and low into 10 routes, ignoring the low-dimensional features and failing to model continuous personality trait scores, which is still a challenge to deeply fuse personality traits and language models. In addition, the data for the above work were entirely produced by LLMs, and the personality traits of the characters have low confidence, which limits the effectiveness of the model in fusing personality traits. Figure 1: The workflow for developing our personalized agent system, Orca, to provide personalized interaction on social media platforms. Orca comprises four stages: (1) Personality traits inferring; (2) Data Augment through designing numerous simulation prompts. (3) Dataset construction, serial instruction for the connection of labels and character and personality traits. (4) Modeling and Training, personality traits instruction tuning (PTIT) and personality scores tuning (PSIT), using the generated data to enhance existing open-source LLMs. In our opinion, there will be three stages in the development of personalized modeling: The first is the inclusion of character information profiles, such as changing the system prompt, appending character information at the end of the prompt, and stimulating LLM-related responses in the form of zero-shot/few-shot (Tu et al., 2023). The second stage is integrating psychological theories. How to integrate psychological theories and LLMs is a meaningful research topic, although there are some research works trying to train LLMs perceive and express emotions (Wen et al., 2023), the existing research works are still insufficient. The third stage is to fuse personality trait modalities. Higher dimensional vectors retain more information than discrete token ids that can be perceived by LLMs, analogous to the embedding fusion of LLMs and vision models (Zhu et al., 2024; GLM et al., 2024) that is the soul of personalized AI assistant. From a macro perspective, the emergence of LLM-based agents allows us to take a more microscopic view of simulated society, which leads to more discoveries from the new representation (Xi et al., 2023). Our work aims to improve the second stage and then try to move towards the third stage. In this paper, we propose a framework for enhancing the role-playing capabilities of LLMs by integrating personality traits to customize AI characters. Specifically, we collected the last 200 posts from 500 users on social media platforms. Inspired by Peters & Matz (2024b), we improved the prompts for inferring personality traits to obtain 35 dimensions continuous scores (as each main dimension has 6 sub-dimensions as mentioned above) and text format reports. Then, we simulate a profile for each user using the LLMs. In order to increase the generalization capacity of the model, we give each post the motivation and potential knowledge. In short, the inputs give the model all the reasonably necessary preconditions to generate the content. It is worth noting that our model has the capability of multi-modal perception and generation, since the media resources for each post are captioned accompanies the text perform the above inputs and outputs. For the coarse-grained model, we train the model by splicing personality trait reports into queries. For the fine-grained model, we design a score interpreter to transcribe numerical input into text description. To assess the effectiveness of the model and training method, we construct a multi-scale benchmark for evaluating the quality of content generated by the personalized AI characters. Experiments show that our model is trained to make connections to user profiles and personality traits and exhibits high personalized performance. Our work can be summarized as follows: • We propose Orca, a framework for data processing and training LLMs of custom characters by integrating personality traits, accompanying products include instruction prompt (PCIP) and dataset, dubbed OrcaData. • We propose PTIT and PSIT, two approaches for modeling coarse and fine-grained fusion of personality trait features, and has considerably improved the quality of generate content. • We propose OrcaBench, a benchmark for multi-scale assessment of the quality of content generated by social AI characters."
https://arxiv.org/html/2411.10000v1,DuSEGO: Dual Second-order Equivariant Graph Ordinary Differential Equation,"Graph Neural Networks (GNNs) with equivariant properties have achieved significant success in modeling complex dynamic systems and molecular properties. However, their expressiveness ability is limited by: (1) Existing methods often overlook the over-smoothing issue caused by traditional GNN models, as well as the gradient explosion or vanishing problems in deep GNNs. (2) Most models operate on first-order information, neglecting that the real world often consists of second-order systems, which further limits the model’s representation capabilities. To address these issues, we propose the Dual Second-order Equivariant Graph Ordinary Differential Equation (DuSEGO) for equivariant representation. Specifically, DuSEGO apply the dual second-order equivariant graph ordinary differential equations (Graph ODEs) on graph embeddings and node coordinates, simultaneously. Theoretically, we first prove that DuSEGO maintains the equivariant property. Furthermore, we provide theoretical insights showing that DuSEGO effectively alleviates the over-smoothing problem in both feature representation and coordinate update. Additionally, we demonstrate that the proposed DuSEGO mitigates the exploding and vanishing gradients problem, facilitating the training of deep multi-layer GNNs. Extensive experiments on benchmark datasets validate the superiority of the proposed DuSEGO compared to baselines.","GNNs (kipf2017semisupervised, ; Yao2023ImprovingTE, ; yin2024dynamic, ; yin2023coco, ) with equivariant properties (satorras2021n, ; han2022equivariant, ; liu2024segno, ) have become essential tools in modeling complex dynamic systems (karplus2002molecular, ; huang2021equivariant, ; liu2024dynamic, ; lu2024pascl, ) and molecular properties (brandstetter2021geometric, ; meng2024heterogeneous, ) due to their ability to capture symmetrical relationships within data. Equivariant GNNs are particularly effective in scenarios when maintaining consistency under transformations (e.g., rotations, reflections) is crucial. The consistency of equivariant representations allows these models to accurately reflect the inherent properties of the systems they are designed to study, leading to more reliable and insightful predictions, which now is an essential topic in machine learning (battaglia2016interaction, ; thomas2018tensor, ; zhang2022would, ; zhang2023investigating, ; meng2024deep, ). Despite their successes, existing Equivariant GNNs face notable limitations that constrain their performance and expressiveness. First, the over-smoothing problem (li2018deeper, ; keriven2022not, ; min2020scattering, ; yin2023omg, ). The classical Equivariant GNNs (such as EGNN (satorras2021n, ) and SEGNN (brandstetter2021geometric, )) typically utilize the GNNs (kipf2017semisupervised, ; yin2022generic, ; ai2023gcn, ; shou2023adversarial, ) as a backbone, as well as cooperating coordinates for equivariant representation. However, GNNs usually suffer from the over-smoothing problem, leading to the indistinguishable representation of features and coordinates, which is shown in Fig. 1. Second, gradient explosion or vanishing problems (dasoulas2021lipschitz, ; guo2022orthogonal, ; yinsport, ). Motivated by the notion that larger models are usually more expressive (kaplan2020scaling, ), we tend to explore the expressiveness ability by enlarging the model scale. However, as the network depth increases, gradients can either become excessively large or diminish to near zero, which limits the scalability and depth of GNNs. Third, real-world simulation (norcliffe2020second, ). Most Equivariant GNNs (fuchs2020se, ; thomas2018tensor, ; satorras2021n, ; brandstetter2021geometric, ; liu2023graph, ; liu2022mlp, ) primarily focus on first-order information. However, many physical systems are governed by second-order laws (norcliffe2020second, ). The first-order methods overlook the higher-order interactions that are critical in real-world systems, and limit the representational ability. Even though some works (huang2021equivariant, ; liu2023improving, ) try to introduce second-order information into equivariant learning, they simply focus on the simulation the position in dynamic physical system, ignoring the expressiveness representation for molecular properties prediction. (a) Dirichlet energy \mathbf{E}\left(\mathbf{H}^{n}\right) of node features. (b) Dirichlet energy \mathbf{E}\left(\mathbf{X}^{n}\right) of node positions. Figure 1. Dirichlet energy \mathbf{E}\left(\mathbf{H}^{n}\right) of node features \mathbf{H}^{n} and \mathbf{E}\left(\mathbf{X}^{n}\right) of node positions \mathbf{X}^{n} propagated through a EGNN (satorras2021n, ), SEGNN (brandstetter2021geometric, ) and GMN (huang2021equivariant, ) on a N-body system dataset, where we give the definition of the Dirichlet energy in Eq. 4. To address these challenges, we take an insight into the second-order method in Equivariant GNNs and a framework named Dual Second-order Equivariant Graph Ordinary Differential Equation (DuSEGO). The proposed DuSEGO is a universal framework that transfer the first-order Equivariant GNNs to second-order, which aims to enhance Equivariant GNNs expressiveness and address the issues of over-smoothing and gradient stability. First, we prove that the proposed DuSEGO maintains the equivariant properties. Then, we analyze the properties of DuSEGO to reveal how the proposed method solves the over-smoothing and gradient exploding and vanishing problems, facilitating the training of deep GNNs. Extensive experiments on benchmark datasets validate the superiority of the proposed DuSEGO compared to existing baselines. The main contributions can be summarized as three-fold: • The proposed DuSEGO is flexible to accommodate the first-order equivariant graph ODE methods to the second-order, which still maintains the equivariant properties. • We analyze the properties of DuSEGO, and demonstrate that the second-order graph ODE can effectively mitigate the over-smoothing, gradient exploding, and vanishing problems. • Extensive experiments conducted on various real-world datasets demonstrate the effectiveness of the proposed DuSEGO."
https://arxiv.org/html/2411.09996v1,"Building 6G Radio Foundation Models 
with Transformer Architectures","Foundation deep learning (DL) models are general models, designed to learn general, robust and adaptable representations of their target modality, enabling finetuning across a range of downstream tasks. These models are pretrained on large, unlabeled datasets using self-supervised learning (SSL). Foundation models have demonstrated better generalization than traditional supervised approaches, a critical requirement for wireless communications where the dynamic environment demands model adaptability. In this work, we propose and demonstrate the effectiveness of a Vision Transformer (ViT) as a radio foundation model for spectrogram learning. We introduce a Masked Spectrogram Modeling (MSM) approach to pretrain the ViT in a self-supervised fashion. We evaluate the ViT-based foundation model on two downstream tasks: Channel State Information (CSI)-based Human Activity sensing and Spectrogram Segmentation. Experimental results demonstrate competitive performance to supervised training while generalizing across diverse domains. Notably, the pretrained ViT model outperforms a four-times larger model that is trained from scratch on the spectrogram segmentation task, while requiring significantly less training time, and achieves competitive performance on the CSI-based human activity sensing task. This work demonstrates the effectiveness of ViT with MSM for pretraining as a promising technique for scalable foundation model development in future 6G networks.","Foundation models (FMs) are first trained on a large, often unlabeled dataset, allowing them to build broad, adaptable representations that can be finetuned for various downstream tasks. This initial pretraining stage is done using self-supervised learning (SSL), where the model learns underlying patterns and relationships within the data without relying on labeled examples [1, 2, 3]. The model ideally develops a robust understanding of its target modality, which, in our case, is radio spectrograms. In fields like computer vision and natural language processing, FMs have set new benchmarks [4, 5, 6, 7], often surpassing supervised learning models, specifically designed for individual tasks. This is largely due to their ability to generalize: FMs learn flexible and transferable representations that make them better suited to handle variations in data, perform across diverse tasks, and adapt to new contexts. Generalization is especially valuable when labeled data is scarce, as foundation models can perform well with minimal additional labeled samples. Deep learning (DL) has demonstrated strong potential when applied to individual wireless tasks, including automatic modulation classification [8], channel estimation [9], constellation and waveform design [10], among others. However, these models are highly specialized, and there are concerns about their ability to generalize effectively in real-world scenarios. Wireless signals are subject to time-varying impairments, and the communication environment is constantly changing, which can degrade a DL model’s performance if it fails to adapt. Introducing the concept of FMs for wireless can potentially overcome these limitations [11, 12]. We propose FMs for wireless signals as a solution to address these challenges. By capturing over-the-air radio signals and pretraining FMs through SSL, there is no need for labeled data. Additionally, these pretrained models can then serve as backbones for multiple tasks, reducing computational costs. Most importantly, FMs are expected to achieve better generalization by leveraging their broad, transferable representations, making them well-suited to handle diverse and dynamic wireless environments. The primary contributions of our paper are: • We propose and demonstrate the effectiveness of a Vision Transformer (ViT) as a radio foundation model for spectrogram learning. Adopting ViT as the FM offers enhanced flexibility, particularly in handling variable input sequences, and increased scalability, as training and evaluation can be parallelized. ViT also captures long-term dependencies through its attention mechanisms. • We introduce a Masked Spectrogram Modeling (MSM) approach to pretrain the ViT in a self-supervised fashion, and thoroughly evaluate key design considerations of the masking procedure and transformer size on performance. • By finetuning across two downstream tasks, we demonstrate that the ViT radio FM effectively learns features that generalize across diverse domains, achieving competitive—or even superior—performance with 4x smaller model sizes compared to baselines. • We demonstrate the effectiveness of the proposed foundation model by utilizing a real-world dataset that is captured over-the-air in a software-defined radio testbed. Upon acceptance, the datasets and code will be publicly available to encourage further research within the community on FM for wireless. The remainder of the paper is structured as follows: Section II presents the datasets utilized for pretraining the foundation model, and for the CSI-based human activity sensing and spectrogram segmentation tasks. Section III outlines the ViT architecture and algorithm of the self-supervised foundation model. Section IV presents numerical experiments conducted to evaluate the proposed methodology. Finally, section V concludes the paper."
https://arxiv.org/html/2411.09986v1,Unlocking Transfer Learning for Open-World Few-Shot Recognition,"Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR.","Few-shot learning (FSL) has got a lot of attention due to the importance in enabling models to adapt to novel tasks using a few examples (e.g., N-way K-shot: a task involving N distinct classes, each represented by K examples) [42, 17, 37, 34, 27, 10]. However, in practical applications, FSL models inevitably encounter instances that do not belong to the N classes, also known as open-set instances. Addressing this challenge has led to the emergence of the field of few-shot open-set recognition (FSOSR) [24, 16, 19, 15, 41]. In FSOSR, if two N-way K-shot FSOSR tasks have distinct closed sets, their corresponding open sets will also differ. This interdependence presents a key challenge when adapting to novel tasks in FSOSR. Namely, it is essential to redefine not only the closed set but also the open set, since the open set is inherently shaped by its closed set. Consequently, the open set lacks a universal definition across various FSOSR tasks; instead, it requires contextual consideration based on the closed set of a specific target task. Figure 1: Difficulty of straightforward extension of the transfer learning from FSL methods [34, 17] to FSOSR. Compared to the pre-trained model without transfer learning (w/o TL), in open-set recognition, [34, 17] are less effective as much as in closed-set, or even degrade the performance. Despite recent advancements in the field, current works [24, 16, 19, 15, 41] have commonly focused on leveraging prior knowledge from a large training dataset. Then, they frequently struggle to balance closed-set accuracy with open-set recognition capabilities, often prioritizing open-set recognition at the expense of closed-set accuracy. Then, these approaches face challenges in achieving broad generalization across various benchmarks. In this work, we bring attention to the novel application of transfer learning within this field. Transfer learning [14, 7, 6, 30] has been extensively studied and demonstrated its efficacy leveraging a pre-trained model to generalize it to other tasks. Recent FSL methods [31, 34, 39, 17, 35] have shown the efficacy of this approach. However, when they come to FSOSR, open-set examples are inherently not present, which significantly undermines the effect of transfer learning in terms of open-set recognition. Then, as in Fig. 1, the naive extension of the transfer learning techniques of FSL, e.g. IER-distill [34] and Label Halluc. [17] fails to attain the same level of improvement in open-set recognition as seen in closed set, or even results in decreased result. Tackling this point, we propose a two-staged FSOSR learning framework. Our method involves two stages: open-set aware meta-learning (OAL) and open-set free transfer learning (OFL). During the meta-learning stage, our objective extends beyond the meta-training of the feature encoder; we also aim to establish a universal open-set representation. This equips us with a decent starting point for the subsequent open-set free transfer learning. In the transfer learning stage, we commence by initializing the model using the parameters obtained from the meta-learning stage. To counteract the absence of open-set examples, we develop two alternative open-set sampling strategies. The first approach curates a training dataset of the previous stage as a source of open-set examples for open-set free transfer learning. For more pragmatic application, our second strategy is confined to the closed-set examples present in the target task, and exploits an episodic learning framework. Here, we generate pseudo FSOSR episodes by randomly dividing the closed-set categories into a closed set and a pseudo open set. As a result, our OAL-OFL method attains a marked enhancement in performance metrics on the standard FSOSR benchmarks as depicted in Fig. 1 while incurring a minimal extra training expense of only 1.5% compared to training without transfer learning. This allows OAL-OFL to surpass the existing state-of-the-art (SOTA) methods. Our contributions are summarized as four-fold: • We introduce a novel two-staged learning called OAL-OFL, bringing transfer learning to FSOSR for the first time with only a minimal additional cost. • We show the importance of preparing the model through open-set aware meta-learning, which is a sturdy starting point for transfer learning. • We suggest two breakthroughs to handle the lack of open-set examples during the transfer learning stage. • By leveraging the effectiveness of transfer learning, our proposed OAL-OFL achieves SOTA on miniImageNet and tieredImageNet datasets. This underscores its ability to generalize across various tasks, enhancing both closed-set classification accuracy and open-set recognition capabilities."
https://arxiv.org/html/2411.09972v1,Large Language Models as User-Agents for Evaluating Task-Oriented-Dialogue Systems,"Traditionally, offline datasets have been used to evaluate task-oriented dialogue (TOD) models. These datasets lack context awareness, making them suboptimal benchmarks for conversational systems. In contrast, user-agents, which are context-aware, can simulate the variability and unpredictability of human conversations, making them better alternatives as evaluators. Prior research has utilized large language models (LLMs) to develop user-agents. Our work builds upon this by using LLMs to create user-agents for the evaluation of TOD systems. This involves prompting an LLM, using in-context examples as guidance, and tracking the user-goal state. Our evaluation of diversity and task completion metrics for the user-agents shows improved performance with the use of better prompts. Additionally, we propose methodologies for the automatic evaluation of TOD models within this dynamic framework. We make our code publicly available 111https://github.com/TaahaKazi/user-agent","Task-oriented dialogue (TOD) systems are designed to assist users in completing specific tasks or goals through turns of natural language interactions [1, 2, 3]. These systems are typically built to guide users through a series of steps to accomplish a particular objective, such as booking a flight, ordering food, or scheduling an appointment. Along with the emergence of large language models (LLMs) [4, 5, 6, 7, 8], TOD systems have been fundamentally boosted by the complex reasoning/understanding ability and the domain adaptation power of LLMs. The functionality as well as the design concepts of TOD systems have also been tightly integrated into the training and grounding of LLMs. From the real-world deployment side, at least tens of LLMs have been developed and integrated into hundreds of downstream tasks, for example, the travel planning LLM agents [9]. However, with such a large number of downstream application scenarios, the capacities of traditional benchmark-based TOD system evaluations are falling far behind. This essentially indicates that a gap exists between the TOD system evaluations and the TOD system deployments. There are dataset-based evaluation methods where the inputs such as the dialogue schema or the dialogue history are fixed and the evaluation based on the expected output is rigid, which means the evaluation systems cannot evaluate multiple turns of utterances from TOD systems coherently [10]. This, however, generally leads to a policy mismatch between the evaluation systems and the TOD systems. There are also traditional interactive evaluation methods that generally result in evaluation simulators in pursuit of maximal information exchange [11]. Existing LLM-based evaluation methods, however, lack of emphasis on modeling more complex user behaviors [11]. To tackle the above issues, in this work, we set up our goal as building an automated evaluation system (a user simulator and an evaluator agent) for TOD systems. Specifically, given any TOD system, our user simulator agent will simulate conversations with the system given a set of initial user goals and the evaluator agent will output several metric scores gauging the comprehensive performance of TOD systems being evaluated. Figure 1 shows our proposed framework that involves the interactions of the user simulator with the TOD system in evaluation and the evaluation module. The user simulator can state the user goal and associated information, discuss options provided as solutions by the TOD system, and negotiate to find options that match user needs. We follow several principles for our simulator design: • The user simulator should be plug and play, which means it does not need scenario-specific annotations to fine-tune the simulator and it should be adapteble to different TOD systems or different evaluation domains. • The user simulator should be rich in linguistic diversity. • The user simulator should be able to model complex user behaviors. In this work, LLMs are used by simulators to evaluate TOD systems. Given an initial user goal, the simulator prompts the LLM to generate responses that interact with TOD systems to achieve the given goal. We propose three prompting strategies: the Vanilla Prompt with straightforward instructions and in-context examples, the Thought Prompt incorporating reasoning steps via Chain-of-Thought, and the User State Tracking Prompt, which updates a user-state tracking dictionary to avoid premature conversation ending or looping. We use multiple metrics to assess the TOD system’s performance, including task completion, naturalness, coherence, and dialogue-level diversity; with task completion evaluated using GPT-4. This approach addresses the limitations of traditional evaluation methods in a dynamic setting. In summary, our work introduces an automated framework using LLMs as user simulators to evaluate TOD systems. This framework enhances the evaluation process, making it more reflective of real-world interactions."
https://arxiv.org/html/2411.09969v1,Steering AI-Driven Personalization of Scientific Text for General Audiences,"Digital media platforms (e.g., social media, science blogs) offer opportunities to communicate scientific content to general audiences at scale. However, these audiences vary in their scientific expertise, literacy levels, and personal backgrounds, making effective science communication challenging. To address this challenge, we designed TranSlider, an AI-powered tool that generates personalized translations of scientific text based on individual user profiles (e.g., hobbies, location, and education). Our tool features an interactive slider that allows users to steer the degree of personalization from 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to generate the translations with given degrees. Through an exploratory study with 15 participants, we investigated both the utility of these AI-personalized translations and how interactive reading features influenced users’ understanding and reading experiences. We found that participants who preferred higher degrees of personalization appreciated the relatable and contextual translations, while those who preferred lower degrees valued concise translations with subtle contextualization. Furthermore, participants reported the compounding effect of multiple translations on their understanding of scientific content. Given these findings, we discuss several implications of AI-personalized translation tools in facilitating communication in collaborative contexts.","Science communication refers to the process of “communicating complex scientific information with general audiences to improve public awareness, interest, and understanding of science” (Burns et al., 2003). Both academic and industry professionals strive to make their ideas more accessible. As a result, science communication platforms, such as science blogs (Hogan, 2024) and science magazines (Times, 2024), have been translating scientific information into more accessible formats for decades. However, a single version of a text often falls short of meeting the diverse needs of a general audience (August et al., 2024). For instance, a journalist with an interest in quantum physics would have vastly different context and comprehension needs than a college student majoring in physics. Scientific research articles often employ domain-specific language, including jargon and complex sentence structures, which can pose significant barriers to comprehension for general audiences. Large language models (LLMs) offer the capability to personalize content to various user contexts, transforming both the style (e.g., casual to formal (Das et al., 2023)) and content (e.g., simplifying complex ideas (August et al., 2024)). This adaptive personalization creates opportunities for enhancing audience engagement and understanding through more effective content dissemination (Kim et al., 2024). For example, LLMs can tailor the tone and style of complex concepts like scientific information to align with individual language preferences, thereby improving readability and comprehension across various fields (August et al., 2023; Das et al., 2023; Ding et al., 2023). Notably, AI can achieve this personalization at scale (i.e., AI-scalable personalization), generating multiple tailored variations of content for diverse audiences (August et al., 2024; Riedl, 2010; Kim et al., 2024). This scalability positions LLMs as powerful tools in advancing inclusive and effective science communication (Razack et al., 2021). Among the many strategies in science communication, analogies have proven effective in translating technical content into more accessible forms (August et al., 2020). For instance, the structure of the solar system is often used as an analogy to explain the structure of an atom. While such analogies can broaden understanding of scientific information, they are typically generalized and may not resonate equally well with all audiences, as individual comprehension is shaped by societal, cultural, educational, and personal backgrounds. For example, the space analogy assumes familiarity with the solar system (Kim et al., 2024). LLMs have demonstrated the ability to generate personalized analogies that can help people understand complex concepts and ideas within their own context (Ding et al., 2023). Building on this capability, we leverage LLMs to generate multiple personalized analogies tailored to individual readers’ contexts and comprehension levels. We designed and implemented TranSlider (Translate through Slider), an interactive reading interface that enables users to steer the degree of personalization in scientific text through an adjustable slider. TranSlider allows the user to specify a degree of personalization from 0 to 100 and utilizes their background information (e.g., education, hobbies, location) to present relevant analogies. The slider enables intuitive exploration of various personalization degrees, allowing users to quickly review multiple translations. We employed TranSlider as a research probe to ask the following research questions: RQ1: What is the utility of AI-driven personalized translations of scientific text? RQ2: What is the impact of interactive reading features on user experience? Specifically: 2.1: How does exploring multiple translations influence readers’ comprehension and engagement with scientific text? 2.2: How does the slider interaction to steer the degree of personalization influence readers’ comprehension and engagement with scientific text? To answer these questions, we conducted a user study with 15 non-expert participants who used the tool to understand two scientific texts by exploring multiple personalized translations. We conducted post-session semi-structured interviews to elicit feedback. Participants generally found the analogies useful in understanding the content. Some favored general analogies (e.g., a participant liked a construction analogy describing the body’s cells as building sites—low personalization), while others preferred detailed, personalized analogies (e.g., a participant appreciated a baking analogy to explain harmful emissions from lithium-ion batteries—high personalization). Participants noted that reading multiple translations with varied analogies allowed them to piece together a fuller understanding of the scientific text, correcting misunderstandings along the way. Overall, users found the tool beneficial for learning about unfamiliar topics but were cautious about the reliability of AI-generated content. Based on these findings, we discuss the need for HCI techniques to steer models towards human pluralistic preferences, rather than solely relying on machine learning methods that assume uniformity in human experiences. In summary, our study has the following three contributions: (1) A novel slider-based interaction to enhance understanding of scientific content through the exploration of analogy-driven personalized translations (2) An investigation to examine the utility, benefits, and limitations of personalized translations in science communication (3) Implications for designing such AI-personalized translation tools and broader applications in collaborative and education contexts such as facilitating cross-disciplinary communication"
https://arxiv.org/html/2411.09968v1,Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs,"The hallucination problem in multimodal large language models (MLLMs) remains a common issue. Although image tokens occupy a majority of the input sequence of MLLMs, there is limited research to explore the relationship between image tokens and hallucinations. In this paper, we analyze the distribution of attention scores for image tokens across each layer and head of the model, revealing an intriguing and common phenomenon: most hallucinations are closely linked to the pattern of attention sinks in the self-attention matrix of image tokens, where shallow layers exhibit dense attention sinks and deeper layers show sparse attention sinks. We further analyze the attention heads of different layers and find that heads with high-density attention sink in the image part play a positive role in alleviating hallucinations. In this paper, we propose a training-free method named Enhancing Attention Heads (EAH), an approach designed to enhance the convergence of image tokens attention sinks in the shallow layers. EAH identifies the attention head that shows the vision sink in a shallow layer and extracts its attention matrix. This attention map is then broadcast to other heads in the layer, thereby strengthening the layer to pay more attention to the image itself. With extensive experiments, EAH shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality.","Figure 1: We found a common phenomenon through the attention map: In the range of image token, the attention head of shallow Sparse attention sink is prone to hallucination, while the attention head of Dense attention sink is much less likely to hallucinate. Multimodal large language models (MLLMs) [1, 29, 3, 46, 10, 25, 32, 13, 44] have made significant strides in cross-modal tasks, especially in handling both text and image modalities. However, hallucinations remain a persistent challenge, particularly in tasks such as Visual Question Answering (VQA) or image captioning. Current methods for addressing hallucinations often involve changing decoding strategies, incorporating external knowledge bases, or retraining models with additional data [22, 28, 30]. These approaches, however, often require significant resources and time. Recent research into attention sink has offered new insights into hallucinations. The concept of attention sink as an information flow is introduced in “Label Words are Anchors” [35], which shows how information flow often converges on a specific user token in large language models (LLMs). OPERA [17] further explores the connection between attention sink in user tokens and output tokens in MLLMs. It observes that when a token has a high attention weight across subsequent tokens, this over-reliance on the token can lead to hallucinations in the model’s outputs. Although these methods clarify the relationship between attention sink, user tokens, and output tokens, the relationship between attention sink, image tokens, and hallucination remains unclear. It’s important to note that MLLM’s output tokens are generated by the decoder based on logits, whereas input tokens, which constitute most of the input sequence, are more likely to directly reflect the MLLM’s internal mechanisms. Figure 2: Definition of dense vision sink head and its layer-wise distribution. In this case, \beta = 0.0015, \gamma = 15% Figure 3: Relationship between text tokens and the average proportion of dense vision sink heads within a single layer by layer2, analyzed across 5,000 randomly selected MSCOCO images using LLaVA1.5-7B. Most dense vision sink heads occur by layer2: As previously mentioned by FastV [5], the information flow of image tokens is primarily concentrated in the first and second layers. Building on this, we conduct experiments on the shallow layers of several models, including LLaVA1.5 [29], Minigpt4 [46], MiniGemini [26], and Intern-VL [6]. As shown in Fig. 2, we calculate the average number of dense vision sink heads across these layers to further investigate the distribution of attention sinks across different layers. We define h_{i,j} as the attention-map of a head, a “dense vision sink head” as a head (i,j) in which the proportion \alpha^{i,j} of columns in the attention map that meet the vision sink condition exceeds a threshold \gamma. Specifically, we define \alpha^{i,j} as: \text{vision sink}=\frac{\sum_{x=k}^{r}h_{i,j}[x][y]\cdot M}{r-k}>\beta,\quad k% \in[36,611], (1) \alpha^{i,j}=\frac{\text{Num(vision sinks)}}{576}, (2) A head is classified as a “dense vision sink head” if: \alpha^{i,j}\geq\gamma. (3) Observations show that most vision attention sinks occur by layer 2. Fewer dense vision sink heads lead to hallucination output: p=\frac{\text{Num(dense vision sink heads)}}{32} (4) This proportion p quantifies how many of the total 32 heads are classified as ”dense vision sink heads,” meaning they have a high proportion of columns that meet the vision sink condition within the image token range. We conducted the image captioning task on 5,000 randomly selected MSCOCO images using LLaVA1.5-7B. When the model generates a new token, we first determine whether it is a hallucination token. Then, we backtrack to layer 2 and analyze the model at the granularity of attention heads. We calculate the proportion of dense vision sink heads in layer 2 relative to the total number of heads (e.g., 32 heads in total). This analysis is repeated for layer 1, and the average across layers 1 and 2 is then computed. As shown in Fig. 3, we observe that non-hallucination tokens typically activate a larger number of dense vision sink heads, whereas hallucination tokens are generally associated with only a few dense vision sink heads, with the majority of heads being sparse. Through our analysis of different models, such as LLaVA1.5 [29], Minigpt4 [46], MiniGemini [26] and Intern-VL [6], it appears that fewer dense vision sinks heads lead to more probable hallucination output. Figure 4: (a) A example of distribution of dense vision head and the corresponding proportions/densities of vision sinks within these heads when model output hallucination token; (b) Relationship between the average skewness and CHAIRI on 150 randomly selected MSCOCO images, using LLaVA1.5-7B for captioning; (c) Comparative skewness scatter plot for Hallucination and Non-Hallucination classification on 150 randomly selected MSCOCO images, using LLaVA1.5-7B for VQA. Lower density of vision sinks and fewer vision sink heads lead to a higher probability of hallucinations: However, the average number of dense vision sink heads across shallow layers does not reveal the individual contributions of each dense vision head, some of which may be negative while others are positive for hallucination. As noted by ITI [23], in current large language models (LLMs) using transformer architecture, only a subset of attention heads plays a more significant role. Effectively optimizing these heads and leveraging them will likely lead to substantial improvements in model efficiency and overall performance. In this case, we conducted a more detailed view for each head, as shown in Figure 4 (a), the sink densities within different vision sink heads vary across the shallow layers (layer1-layer2), with an overall negatively skewed distribution. As shown in Figure 4 (b), for the image captioning task, the average skewness of the distribution of dense vision sink head and its corresponding vision sink densities in layer1 and layer2 is recorded each time a token is output. Once the output token is completed, the CHAIRI for the entire output is calculated, and the average skewness for all tokens in layer1 and layer2 is obtained. As shown in Figure 4 (c), for the VQA task (with only a single output token), the average skewness of the distribution of vision sink head and its corresponding vision sink densities in layer1 and layer2 is directly recorded for the answer token. It is observed that, regardless of the task (image captioning or VQA), a lower skewness coefficient correlates with a lower hallucination rate. In other words, a higher density of vision sinks within a dense vision sink head and a larger number of vision sink heads lead to a lower probability of hallucination. These observations highlight the critical role of attention head and vision sink distribution in understanding the attention sink phenomenon, particularly as it relates to alleviating hallucination issues in MLLMs. When the vision sink is sparse, visual tokens concentrate too heavily on specific elements, leading to reduced attention to other parts of the image. Conversely, a dense vision sink helps maintain a global perspective, preventing the model from narrowing its focus too much and minimizing information loss. Our goal is to ensure the model maintains a high-density vision sink within shallow layers. To achieve this, we design a training-free method called Enhancing Attention Heads (EAH). This plug-and-play approach focuses on each attention head in the early layers, systematically identifying the head with the densest vision sinks. It then broadcasts this attention distribution across the layer, aligning the layer’s attention and the head’s vision sink distribution with that of the selected head. We conduct extensive evaluations, focusing specifically on hallucination issues, and test mainstream MLLMs to validate the effectiveness of EAH in reducing hallucinations across various model architectures. Our results demonstrate that EAH is a highly effective plug-and-play solution for mitigating hallucinations across various MLLMs. Specifically, our contributions can be summarized as follows: • This paper investigates how information flow relates to hallucinations in MLLMs. Our analysis reveals a consistent pattern where denser vision sinks and a larger number of vision sink heads in the shallow layers are associated with fewer hallucinations. • We propose a plug-and-play training-free method called Enhancing Attention Head, which alleviates hallucinations by finding the head with the densest vision sink and broadcasting it to other heads. • Experiments on multiple models validate the plug-and-play convenience and strong generalization of this method."
https://arxiv.org/html/2411.09955v1,Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era,"The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.","Visual design tools have become essential in various multimedia fields, although they often require prior knowledge to use effectively. Recent research has emphasised text-guided image editing as a way to make these tools more accessible and controllable (Li et al., 2020a; Patashnik et al., 2021; Gal et al., 2022; Crowson et al., 2022), as in Fig. 1. Studies have shown the effectiveness of diffusion models in creating realistic images and their application in image editing through techniques like swapping latent cross-modal maps for visual manipulation (Ho et al., 2020; Kim et al., 2022). Additionally, specific region editing is made possible through guided masks (Nichol et al., 2022; Avrahami et al., 2022). Moving away from complex descriptions and masks, instruction-based editing has gained traction for its straightforward approach, allowing users to directly command how and what aspects of an image to edit (Hertz et al., 2023; Mokady et al., 2023; Kawar et al., 2023). This paradigm is noted for its practicality, aligning closely with human intuition (Fu et al., 2024; El-Nouby et al., 2019; Fu et al., 2020). The latest text-to-image generative models offer impressive image quality and accuracy in reflecting the given captions, marking a significant leap in content generation technologies (Alayrac et al., 2022; Ramesh et al., 2022; Rombach et al., 2022). Among these advancements, instructional image editing has emerged as a particularly promising application (Brooks et al., 2023). This method streamlines the editing process by eliminating the need for detailed before-and-after captions (Avrahami et al., 2022; Wallace et al., 2023). Instead, users can provide simple, human-readable instructions, such as “change the dog to a cat”, making the editing process more intuitive and aligned with how humans naturally approach image modification (Zhang et al., 2024f). In recent years, advancements in large language models (LLMs) (Touvron et al., 2023; Brown et al., 2020) have dramatically reshaped the landscape of image and video manipulation. The convergence of these technologies has enabled more intuitive, flexible, and high-fidelity editing processes, largely driven by natural language instructions (Wu et al., 2023c; Feng et al., 2024b; Chakrabarty et al., 2023a). These innovations span various applications, from fashion image editing and 3D scene manipulation to video-to-video synthesis and audio-driven editing, empowering users to achieve fine-grained control over visual content. Moreover, Multimodal large language models (MLLMs), building upon the foundational capabilities of traditional LLMs, have extended the boundaries of vision-language tasks (Zhang et al., 2024b). By integrating latent visual knowledge and treating images as input, MLLMs enhance performance in tasks requiring both textual and visual reasoning. The emergence of diffusion models, such as LLaVA (Liu et al., 2024a) and MiniGPT-4 (Zhu et al., 2024a), has further elevated the potential of these frameworks by improving image-text alignment through instruction tuning. These models, including GILL (Koh et al., 2024) and SEED (Ge et al., 2023), facilitate coherent image generation from textual input while preserving rich visual semantics, marking a pivotal evolution in instruction-based editing. This review paper explores the evolution and diversity of techniques underpinning instruction-based image and video editing, synthesizing cutting-edge approaches that integrate human feedback, multimodal signals, and advanced neural architectures. The focus spans from early models leveraging generative adversarial networks (GANs) (Patashnik et al., 2021) to the latest innovations using diffusion models, including frameworks like Pix2Pix (Brooks et al., 2023), InstructBrush (Zhao et al., 2024), and FlexEdit (Nguyen et al., 2024a). Additionally, specialized models for audio- and video-driven editing, such as Noise2Music (Huang et al., 2023a) and Fairy (Wu et al., 2023a), are examined, demonstrating the versatility and creativity unlocked by these methods. By analyzing over 100 recent key publications, this review delves into key technological breakthroughs, evaluates their effectiveness, and considers potential avenues for further innovation. From 3D image editing (Sabat et al., 2024) to fashion editing (Wang and Ye, 2024), this paper highlights how these models are reshaping industries ranging from entertainment and fashion to education and remote sensing (Han et al., 2024b). Through this comprehensive overview, we aim to identify emerging trends, challenges, and opportunities in the growing field of text-driven, instruction-guided image and video editing. Differences with Existing Surveys. Our survey differs from existing surveys in its specific focus on instruction-based image and video editing empowered by LLMs. While Li et al. (Li et al., 2024b) focus on the integration of various modalities for retrieval tasks, our paper highlights the use of instructions for precise visual editing. Qin et al. (Qin et al., 2024) evaluate instruction-following abilities in LLMs but does not address their application in visual manipulation, which is a key focus of our review. Similarly, Yin et al. (Yin et al., 2023) address instruction-following in language models with a broader emphasis on ethical concerns, whereas our review emphasizes the technical advancements in using these capabilities for visual content generation and editing across various domains, including image, video, and 3D manipulation. Closest to our review is (Zhan et al., 2023), which explores generative AI techniques but lacks the detailed exploration of instruction-following in visual editing contexts, as seen in our paper. Especially, we consider caption-based image editing (Chen et al., 2018; Couairon et al., 2022b; Lin et al., 2023a) is a part of instruction-based image editing but we do not fully focus on the former. Rather, we are interested in user-friendly instructions that have practical implications for broad audience when editing images. Table 1 summarises the difference between our surveys and existing ones. Table 1. A comparison between existing surveys Survey Focused Task Focused Modality Key Contents (Qin et al., 2024) Editing Text Instruction development, Evaluation concerns (Yin et al., 2023) Editing Text LLM-empowered instructions, Instruction tuning (Li et al., 2024b) Retrieval Image, Video, Audio Image-text composite retrieval, Multimodal composite retrieval (Zhan et al., 2023) Generation Image Text guidance, Audio guidance, Sketch guidance, etc. Ours Editing Image, Video, Audio Instruction mechanisms, Augmentations, Learning stragies, Model designs, Loss functions Paper Collection Methodology. To map the research landscape on this subject, we used a range of keyword searches and combinations such as “image editing”, “image manipulation”, “text-guided”, “instruction-followed”, and “instruction-guided”. Initially, we relied on platforms like Google Scholar, Semantic Scholar, and the AI-enhanced tool Scite.ai to compile an initial set of studies. We then expanded this collection by conducting backward searches, reviewing the references in the selected papers, and forward searches to identify works that cited them. To ensure accuracy, we manually evaluated the relevance of each study, given that some focused on related areas like image generation or retrieval but employed similar techniques. This thorough process ultimately resulted in the identification of over 100 pivotal papers relevant to the field. Contributions. The main contributions of this survey are: • Comprehensive Review: This study provides a comprehensive review of LLM-empowered image and media editing. We have gathered and summarised an extensive body of literature, including both published works and pre-prints up to October 2024. • Process-based Taxonomy: We have organised the literature according to the developmental stages of an image editing framework. Fig. 2 presents the taxonomy we developed to structure the existing works in the field. • Optimisation Tools: We have curated a set of optimisation tools for developing end-to-end image editing frameworks, covering model designs, learning strategies, instruction mechanisms, data augmentations, and loss functions. • Practical Applications: We discuss various practical applications across multiple domains, including style, fashion, face editing, scene manipulation, charts, remote sensing, 3D, speech, music, and video editing. • Challenges and Future Directions: Instruction-guided visual design remains an emerging area of research. Based on the surveyed literature, we identify several unresolved challenges and propose future research directions to explore more editing use cases and user-friendly editing controls. • Sources, Datasets, and Metrics: To support empirical research, we provide a comprehensive overview of available source codes, datasets, and evaluation metrics that have been utilised in the field. • Online Updating Resource: To support ongoing research in LLM-empowered visual design, we have created an open-source repository111https://github.com/tamlhp/awesome-instruction-editing, which consolidates relevant studies, including links to papers and available code. Figure 2. Process-based taxonomy of instruction-guided image editing."
https://arxiv.org/html/2411.09952v1,GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video,"Avatar modelling has broad applications in human animation and virtual try-ons. Recent advancements in this field have focused on high-quality and comprehensive human reconstruction but often overlook the separation of clothing from the body. To bridge this gap, this paper introduces GGAvatar (Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular videos. Through advanced parameterized templates and unique phased training, this model effectively achieves decoupled, editable, and realistic reconstruction of clothed humans. Comparative evaluations with other costly models confirm GGAvatar’s superior quality and efficiency in modelling both clothed humans and separable garments. The paper also showcases applications in clothing editing, as illustrated in Figure 1, highlighting the model’s benefits and the advantages of effective disentanglement. The code is available at https://github.com/J-X-Chen/GGAvatar/.","Reconstructing realistic clothed digital humans and their garments is a significant task in computer graphics and computer vision. This type of work aims to synthesize high-resolution clothed human body images from an unprecedented view or generate human imagery in a novel pose. Previous research has delved into explicit modelling methods under costly capture systems to obtain suboptimal reconstruction outcomes (Seitz et al., 2006; Szeliski et al., 1996). Recent advancements have shifted towards direct construction from single RGB images or monocular videos, utilizing models with implicit representation such as Neural Radiance Field (NeRF)(Mildenhall et al., 2020) to capture fine textures on the surface. However, these models (Peng et al., 2021; Weng et al., 2022; Feng et al., 2022; Chen et al., 2021) require dozens of training hours. Consequently, current studies(Jiang et al., 2023; Geng et al., 2023; Qian et al., 2024; Lei et al., 2024; Hu et al., 2024b; Kocabas et al., 2024; Hu et al., 2024a) are increasingly focused on enhancing rendering speed and modelling efficiency by turning neural rendering techniques into Instant-NGP(Müller et al., 2022) or 3D Gaussian Splatting (3DGS)(Kerbl et al., 2023). Nevertheless, the lack of disentanglement functions in these existing avatar models may constantly limit their applicability in real-world scenarios. This paper argues that an ideal avatar model should not only produce high-quality, rapid, and thorough reconstruction results, but also possess the decoupling capability necessary for applications such as virtual try-ons. Unfortunately, creating a perfect editable and drivable avatar is a demanding task that presents several challenges. Firstly, to effectively disentangle the body and garments, integrity and anti-interference properties must be maintained between distinct components. Specific estimations are required for the unsupervised areas where the human body is obstructed. Secondly, a precise transformation between canonical space and various pose spaces must be established to locate the partitioned point cloud at the target position. Lastly, it is essential to capture diverse and intricate clothing details, including textures, and to achieve high-quality reconstructions from sparse monocular inputs, particularly for loose-fitting attire. However, works such as (Li et al., 2024; Feng et al., 2023; Corona et al., 2021; Jiang et al., 2020; Li et al., 2022; Kim et al., 2024; Pons-Moll et al., 2017) are limited to recovering geometry without providing corresponding appearance information. In response to these challenges, this paper proposes a novel framework, GGAvatar, designed to construct realistic avatars from monocular videos while effectively and completely separating the garments. Specifically, this paper builds and fits garment templates alongside the corresponding body template to achieve a preliminary state of separation and interference resistance, resulting in partitioned point sets. Phased trainable modules (isolation and joint training) reasonably prevent the intersection of point sets during the training process. Subsequently, the target Gaussian positions are ensured by constructing deformation fields based on a concentric skeleton. Simultaneously, high-quality rendering is accomplished using 3DGS. Notably, GGAvatar enables thorough separation of clothed humans in novel view synthesis tasks from monocular inputs—potentially a first in this field, to my knowledge. The paper evaluates the GGAvatar model by comparing it with baseline approaches and other works on the People Snapshot Dataset (Alldieck et al., 2018) or the ZJU Mocap Dataset (Peng et al., 2021). The results indicate that GGAvatar demonstrates a high level of reconstruction quality for clothed humans, comparable to that of other 3DGS-based models. Notably, the proposed model outperforms nearly every traditional NeRF-based model while exhibiting significantly faster training speeds—approximately hundreds of times faster than the NeRF counterparts. Furthermore, ablation studies are conducted to validate the effectiveness of each component. To highlight the superiority and practical utility of GGAvatar, this paper compares it with existing non-fully decoupled models on clothing transfer. The contributions are summarized as follows: • This paper proposes the GGAvatar model, based on phased training methods, to achieve high-quality and efficient construction for various viewpoints or pose synthesis tasks of clothed humans. • The method of constructing parameterized templates for garments is introduced to solve the challenge of complex clothes modelling. • The GGAvatar enables a thorough separation between different garments, allowing applications such as colour editing and clothing transfer."
https://arxiv.org/html/2411.09945v1,TEESlice: Protecting Sensitive Neural Network Models in Trusted Execution Environments When Attackers have Pre-Trained Models,"Trusted Execution Environments (TEE) are used to safeguard on-device models. However, directly employing TEEs to secure the entire DNN model is challenging due to the limited computational speed. Utilizing GPU can accelerate DNN’s computation speed but commercial widely-available GPUs usually lack security protection. To this end, scholars introduce TEE-shielded DNN partition (TSDP), a method that protects privacy-sensitive weights within TEEs and offloads insensitive weights to GPUs. Nevertheless, current methods do not consider the presence of a knowledgeable adversary who can access abundant publicly available pre-trained models and datasets. This paper investigates the security of existing methods against such a knowledgeable adversary and reveals their inability to fulfill their security promises. Consequently, we introduce a novel partition before training strategy, which effectively separates privacy-sensitive weights from other components of the model. Our evaluation demonstrates that our approach can offer full model protection with a computational cost reduced by a factor of 10. In addition to traditional CNN models, we also demonstrate the scalability to large language models. Our approach can compress the private functionalities of the large language model to lightweight slices and achieve the same level of protection as the shielding-whole-model baseline.","Deep Neural Networks (DNNs) and recent Large Language Models (LLMs) have emerged as a significant category of intelligent software for user devices. These applications are capable of executing a diverse range of complex AI tasks such as voice assistants (Yadlapally et al., 2023), image recognition (Drolia et al., 2017), and natural language processing (Desai et al., 2020). Nevertheless, the deployment of intelligent software on user devices presents a novel attack surface in comparison to cloud-based services: The detailed information of intelligent software (e.g., model weight values) is exposed to potential malicious users of the device. By having access to this white-box information, adversaries can easily achieve high attack accuracy with significantly lower costs for common attacks like Model Stealing (MS) and Membership Inference Attack (MIA) (Hu et al., 2022a; Orekondy et al., 2019; Jagielski et al., 2020; Papernot et al., 2016; Carlini et al., 2019; Leino and Fredrikson, 2020). These attacks pose a serious threat to the security of the intelligent software, its intellectual property, and the sensitive data privacy (e.g. training data privacy) of the software owners. Hence, a primary goal in fortifying on-device intelligent software is to thwart adversaries from obtaining the white-box information, thereby transforming straightforward and effective white-box MS and MIA attacks into black-box (considerably more challenging) scenarios (Hu et al., 2022a; Mo et al., 2020; Hou et al., 2022; Sun et al., 2020). In this paper, we focus on protecting DNN models on clients’ devices, which are equipped with Trusted Execution Environments (TEEs) and low-grade commercial GPUs. TEEs are commonly used to safeguard on-device intelligent software (Hanzlik et al., 2021; Lee et al., 2019b; Kim et al., 2020; Li et al., 2021). Like traditional software is protected, TEEs ensure that sensitive data (e.g., private keys) are kept separate from the system environment, making it inaccessible to formidable adversaries like malicious operating systems and administrators. Compared to other methods of protection at the algorithmic level, such as Multi-Party Computation (MPC) (Juvekar et al., 2018), Homomorphic Encryption (HE) (Gilad-Bachrach et al., 2016), Regularization (Nasr et al., 2018), and Differential Privacy (DP) (Dwork and Roth, 2014), TEE-based security imposes lower computational overhead on mobile and IoT devices while preserving the accuracy of the secured models (Hu et al., 2022a; Tramèr and Boneh, 2019). Nevertheless, applying TEEs directly to safeguard entire DNN models poses challenges because low-grade commercial GPUs (e.g. GeForce RTX 4090 and RTX A6000) do not provide the functionality of TEE. Although some recent high-end GPUs (e.g. Nvidia Hopper GPU Architecture (NVIDIA, 2023)) provide the functionality of confidential computing, their prices are too high for ordinary model users. An Nvidia H100 GPU is over 15\times more expensive than a GeForce RTX 4090111At Jun 2024, the price of an H100 GPU is about $30,000, while the price of a GeForce RTX 4090 is less than $2,000. Attempting to shield the complete DNN model within a TEE (shielding-whole-model) could result in a 50x reduction in the model’s speed. While safeguarding an entire deep learning model using TEEs may not be practical for on-device scenarios, recent research suggests safeguarding the privacy-sensitive and critical components of the model to ensure both high utility and security simultaneously. Specifically, a concept known as TSDP has been proposed. This approach involves splitting a large DNN model into two components: a privacy-sensitive component, which is small and contains vital information, and a privacy-insensitive component, which is larger and holds less critical data. The privacy-sensitive part operates within TEEs, while the privacy-insensitive part runs on GPUs (Mo et al., 2020; Hou et al., 2022; Shen et al., 2022b; Sun et al., 2020). The rationale behind TSDP is akin to securing conventional software with TEEs, where the privacy sensitive portion (e.g., private keys) is compact and can be protected by TEEs, while the larger portion of the software (e.g., the remaining codebase) operates outside of TEEs (Lazard et al., 2018). Current TSDP approaches generally assume that the portion off-loaded to the GPU does not reveal sensitive information of DNN models. These methods employ retraining techniques to show that even if an attacker uses this portion for MS or MIA, the reconstructed DNN model only achieves a similar accuracy to a black-box baseline (Mo et al., 2020; Hou et al., 2022; Shen et al., 2022b; Sun et al., 2020), which is significantly lower than the white-box accuracy. Previous TSDP studies rely on empirical experiments to prove that the disclosed model components do not leak significantly more information than a black-box interface (Hou et al., 2022; Sun et al., 2020; Mo et al., 2020; Shen et al., 2022b). This paper examines the security promises provided by current TSDP solutions in the presence of a more sophisticated and cunning adversary in the age of large language models. Specifically, we explore a realistic threat scenario where the adversary can leverage readily available public information from the Internet, such as pre-trained models and public datasets (Chen et al., 2022a, b; Wang et al., 2018). With the prevalence of large language models, it has become common for software developers to utilize publicly accessible models to accelerate the development of proprietary software. Previous studies have demonstrated that these public models can be exploited to compromise private software (Sitawarin et al., 2023). To undermine TSDP, attackers can use public information to scrutinize outsourced model components and obtain more information on privacy beyond simply analyzing black-box output, thus undermining the security guarantees of TSDP. However, none of the existing methods has thoroughly assessed their security guarantees in the presence of public information. Therefore, we contend that it is crucial to systematically evaluate the security assurances of TSDP solutions under this threat landscape. To investigate the security of TSDP methods, our initial step involves conducting a comprehensive review of the existing literature on TSDP. We analyze publications released from 2018 to 2023 in reputable conferences such as IEEE S&P, MobiSys, ATC, ASPLOS, RTSS, MICRO, AAAI, ICLR, ICML, PETs, MICRO, and TDSC. Each paper’s technical approaches are scrutinized, and we classify them into five distinct categories based on their primary contributions. These categories include fortifying deep layers (①), fortifying shallow layers (②), fortifying high-magnitude weights (③), fortifying intermediate layers (④), and fortifying non-linear layers (⑤). Subsequently, we choose one exemplary paper for each category and proceed to implement its technical methodology. After categorizing the existing TSDP approaches, we perform a thorough security assessment using a more powerful adversary that has access to public-pre-trained models. Both Membership Inference (MS) and Model Inversion Attacks (MIA) are carried out against the representative TSDP solutions we reviewed, and the attack accuracy is compared against two baselines: the black-box baseline (shielding-whole-model) offers the highest security assurance but the lowest utility, while the white-box baseline (where the entire Deep Neural Network model is offloaded outside of TEE) provides the highest utility but lacks security protection. The experiment results reveal that current TSDP methods inadvertently expose significant private information to attackers through offloaded model weights, allowing attacks of almost white-box quality against TEE-protected models. The accuracy of MS attacks on existing TSDP solutions is 3.85\times – 4.56\times higher than that of the black-box (shielding-whole-model) baseline. On the contrary, the unprotected white-box baseline demonstrates a 4.57\times higher accuracy compared to the shielding-whole-model configuration. The results for MIA attacks show a similar trend, with existing TSDP methods exhibiting 1.16\times – 1.36\times higher MIA accuracy than the shielding-whole-model baseline, while the accuracy for the white-box setup is 1.37\times higher. Furthermore, we found that significant challenges were faced in improving the security of established TSDP methods without fundamentally altering their approaches. For example, we evaluated the effectiveness of MS/MIA attacks using various setups of current TSDP techniques. Identifying an optimal configuration that balances a DNN model’s performance with security requirements proved to be particularly challenging. Specifically, achieving a high level of tolerance to attacks requires distinct settings to configure the protected component when protecting different models and datasets. Thus, a thorough empirical process is essential to determine the ideal configuration customized to specific models and datasets within all existing TSDP strategies. However, conducting such empirical analyses is excessively costly due to the large number of possible combinations of models and datasets. During our literature survey and empirical evaluation, we found that the fundamental weakness of existing TSDP approaches is that they follow a training-before-partition strategy. This involves first training a private model with a public pre-trained model and private data, and then separating the model into two parts: a shielded part that runs in TEEs, and an offloaded part that runs out of TEEs. Since training occurs before model partitioning, privacy-related weights may likely pervade the entire model. Therefore, it is hard for existing TSDP solutions to accurately isolate privacy-related weights, creating potential attack surfaces. In order to enhance the security of TSDP solutions against the new threat model, we introduce a novel TSDP framework named TEESlice. This framework effectively separates privacy-sensitive weights from outsourced weights during the inference phase. Unlike the training-before-partition approach used in prior research, TEESlice employs a partition-before-training strategy. This method involves initially dividing a DNN model into a backbone and several private segments, utilizing publicly pre-trained models as the backbone, and then training the segments with private data. Consequently, TEESlice effectively isolates privacy-related weights from offloaded weights and ensures the protection of all privacy-sensitive weights in TEEs. The primary difficulty in implementing the partition-before-training approach lies in guaranteeing that individual segments are of a manageable size for execution in TEEs without compromising on accuracy. To address this challenge, we suggest employing a dynamic pruning method. Initially, the private segments are trained with larger sizes to ensure they possess adequate model capacity for achieving high accuracy. Subsequently, the algorithm automatically adjusts the segment sizes to stay below a specified threshold of accuracy loss. Through this process, TEESlice is able to identify the optimal configuration, or ”sweet spot,” that minimizes the number of segments (computation) within the TEE while preserving the accuracy level of the non-partitioned model. Our evaluation indicates that TEESlice surpasses existing TSDP methods in terms of both security assurance and utility cost. It is challenging for attackers to extract sensitive information through the analysis of model structures, demonstrating that TEESlice achieves a security level equivalent to the shielding-whole-model baseline with a computational cost that is 10\times lower compared to alternative TSDP solutions, in both experimental and real-world scenarios. Additionally, TEESlice attains a high level of security with minimal trade-offs. Statistical analysis reveals no discernible differences in accuracy between the protected TEESlice model and the original unpartitioned model. Furthermore, the outsourced public backbone does not enhance the efficacy of attacks. Our evaluation also shows that TEESlice can effectively protect large language models with LoRA. The contribution of this paper can be summarized as follows: • We systematically evaluate the security guarantee of previous TSDP solutions using two representative attacks, MS and MIA, and reveal the security issues of these solutions. • We illustrate the difficulty in improving the security of previous TSDP approaches without substantially changing their methodologies. • We propose TEESlice, a novel TSDP solution for DNN inference that isolates privacy from off-loaded model parts to provide a strong security guarantee using TEEs and cryptographic primitives. Our detailed evaluation shows that TEESlice offers a high security guarantee with moderate overhead and no accuracy loss. This paper is an extended version of a conference paper (Zhang et al., 2024). The conference paper categorized existing TSDP solutions, evaluated their security on three representative models, and proposed TEESlice on the CNN models. This paper includes additional content compared with the conference paper. First, this paper conducts a more comprehensive review of existing TSDP solutions, including the scenarios, threat model, design insight, evaluated attacks, outsourced data security, and limitations. Second, this paper includes more experiments on the security evaluation of existing TSDP work and demonstrates the scalability of the observation. Third, this paper proposes an extended approach of TEESlice that can be applied to large language models. Our evaluation demonstrates the effectiveness of the approach to protect large language models. Availability. The artifacts are available at (TEE, 2023a) and (TEE, 2024). Overview. In Sec. 2.2, we will introduce the background and the threat model. In Sec. 4, we survey existing TSDP solutions and evaluate their defense effectiveness. Based on the vulnerability in Sec. 4, in Sec. 5, we further reveal that it is difficult to mitigate the vulnerability straightforwardly. In Sec. 6, we summarize the fundamental reason for the weaknesses of existing TSDP and propose our solution, TEESlice. In Sec. 7, we comprehensively evaluate TEESlice with other TSDP solutions. At last, we present threats to validity (Sec. 8), related work (Sec. 9), and discussion (Sec. 10)."
https://arxiv.org/html/2411.09933v1,JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging,"With the rapid advancement of large language models (LLMs), foundational models (FMs) have seen significant advancements. Healthcare is one of the most crucial application areas for these FMs, given the significant time and effort required for physicians to analyze large volumes of patient data. Recent efforts have focused on adapting multimodal FMs to the medical domain through techniques like instruction-tuning, leading to the development of medical foundation models (MFMs). However, these approaches typically require large amounts of training data to effectively adapt models to the medical field. Moreover, most existing models are trained on English datasets, limiting their practicality in non-English-speaking regions where healthcare professionals and patients are not always fluent in English. The need for translation introduces additional costs and inefficiencies. To address these challenges, we propose a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging (JRadiEvo). This is the first attempt to extend a non-medical vision-language foundation model to the medical domain through evolutionary optimization of model merging. We successfully created a model that generates accurate Japanese reports from X-ray images using only 50 translated samples from publicly available data. This model, developed with highly efficient use of limited data, outperformed leading models from recent research trained on much larger datasets. Additionally, with only 8 billion parameters, this relatively compact foundation model can be deployed locally within hospitals, making it a practical solution for environments where APIs and other external services cannot be used due to strict privacy and security requirements.","In recent years, foundational models (FMs) have seen remarkable advancements, transforming various fields by offering more sophisticated and powerful solutions [1]. A key driver of this progress has been the rise of large language models (LLMs), which have greatly expanded the capabilities of FMs, particularly in processing and generating text with high accuracy and contextual understanding. This has sparked exponential growth in research [2], leading to the development of vision-language models that integrate visual and textual data [3, 4, 5], as well as fine-tuning approaches that enhance model performance for specific tasks [6, 7]. Healthcare is one of the most critical application areas for foundational models. The need to develop models tailored to healthcare is essential, particularly because physicians often face the challenge of reviewing large volumes of medical data, such as X-rays, which can be time-consuming and demanding. Advanced FMs can help alleviate this burden by enabling quicker and more efficient diagnoses, improving the overall effectiveness of healthcare delivery and patient outcomes. In response to this need, various FMs have been fine-tuned specifically for the healthcare domain, further enhancing their accuracy and effectiveness in clinical settings [7, 8, 9]. However, despite these advancements, several challenges remain. One significant issue is that most of the models developed so far, such as LLaVA-Med [7] and MedPaLM 2 [8], are predominantly in English, whereas many healthcare professionals and patients are not always proficient in English. For these models to be truly practical, there is a pressing need to expand their capabilities to non-English languages. Relying on a two-step process, where the model first generates output in English and then translates it, can introduce additional costs and complexity, making it less efficient and accessible. Additionally, publicly available datasets that can be used to train these models, such as MIMIC-CXR [10] and IU X-Ray [11], are overwhelmingly in English, with very few datasets available in other languages. Translating the large amounts of data needed for training into other languages with high quality is a costly and resource-intensive process. This scarcity of non-English datasets makes it difficult to develop models that can handle non-English languages. Furthermore, due to privacy concerns, it is challenging to collect and use patient data for model training, further complicating the creation of such datasets. Also, the use of large models through APIs, such as GPT-4 [12], is often impractical in healthcare settings because of the stringent privacy regulations that protect patient data, which limits the deployment of these models in real-world clinical environments. To address these challenges, this paper presents a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging [13] (JRadiEvo), a first attempt to extend a multimodal vision-language model for non-English medical text generation by utilizing evolutionary optimization of model merging [13]. JRadiEvo was developed by merging a non-medical vision-language model, medical text-to-text models, and a Japanese-language text-to-text model using an evolutionary algorithm. This innovative approach enabled the efficient creation of a Japanese radiology report generation model using only a minimal amount of Japanese-language data, addressing the critical need for non-English medical text generation in a resource-constrained environment. Below we outline our key contributions, which aim to advance the field of multimodal foundational models in healthcare: 1. Efficient use of limited non-English medical data: In the context of the difficulty in collecting non-English datasets, JRadiEvo demonstrates the ability to create a non-English medical report generation model by translating and utilizing only 50 cases from publicly available English datasets. This approach highlights the efficiency of the development process, demonstrating how a non-English medical report generation model can be created using extremely limited data and annotations. Additionally, it is noteworthy that not only was the dataset used after translation limited to 50 cases, but the entire dataset used to create JRadiEvo consisted of just 50 cases. This underscores the fact that JRadiEvo efficiently utilizes a very limited amount of data, demonstrating an effective approach to handling medical data under strict privacy and security constraints. 2. Novel application of model merging in the medical vision-language model: Traditionally, adapting models to the medical domain has relied on fine-tuning or training from scrach. To the best of our knowledge, there are no existing study of applying model merging alone to adapt a vision-language model to the medical domain. While recent research [13] has proposed using evolutionary optimization of model merging for vision-language models, this approach has been limited to natural images. To our knowledge, no prior studies have extended this technique to medical images or other domain-specific imagery beyond natural images. 3. Lightweight model for local deployment: JRadiEvo is an 8B parameter model, making it lightweight enough to be deployed on local hospital computing systems without the need for external APIs. This local deployment capability addresses critical privacy and security concerns, allowing hospitals to maintain control over patient data. Additionally, given the challenges of equipping facilities with expensive GPUs proportional to patient numbers, JRadiEvo’s compact size and low GPU memory requirements make it practical for widespread use. 4. Cost-efficient training process: JRadiEvo eliminates the need for computationally expensive backpropagation during training, enabling a far more efficient learning process compared to training a new model or fine-tuning. Additionally, by leveraging model merging instead of fine-tuning, JRadiEvo avoids the common issue of catastrophic forgetting [14, 15, 16] that often occurs during fine-tuning, allowing for a more stable and efficient development process."
https://arxiv.org/html/2411.09921v1,Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level,"In this paper, we introduce Motion-Grounded Video Reasoning, a new motion understanding task that requires generating visual answers (video segmentation masks) according to the input question, and hence needs implicit spatiotemporal reasoning and grounding. This task extends existing spatiotemporal grounding work focusing on explicit action/motion grounding, to a more general format by enabling implicit reasoning via questions. To facilitate the development of the new task, we collect a large-scale dataset called GroundMoRe, which comprises 1,715 video clips, 249K object masks that are deliberately designed with 4 question types (Causal, Sequential, Counterfactual, and Descriptive) for benchmarking deep and comprehensive motion reasoning abilities. GroundMoRe uniquely requires models to generate visual answers, providing a more concrete and visually interpretable response than plain texts. It evaluates models on both spatiotemporal grounding and reasoning, fostering to address complex challenges in motion-related video reasoning, temporal perception, and pixel-level understanding. Furthermore, we introduce a novel baseline model named Motion-Grounded Video Reasoning Assistant (MoRA). MoRA incorporates the multimodal reasoning ability from the Multimodal LLM, the pixel-level perception capability from the grounding model (SAM), and the temporal perception ability from a lightweight localization head. MoRA achieves respectable performance on GroundMoRe outperforming the best existing visual grounding baseline model by an average of 21.5% relatively. We hope this novel and challenging task will pave the way for future advancements in robust and general motion understanding via video reasoning segmentation.","Understanding motions (Aggarwal & Cai, 1999, Corona et al., 2020, Zhou et al., 2012, Tevet et al., 2022) in dynamic video scenes has long been an important topic in the computer vision community. It plays a crucial role in many vital real-world applications, such as scene/video understanding (Saleemi et al., 2010, Sturgess et al., 2009, Mottaghi et al., 2016, Tsai et al., 2011, Fan et al., 2018), autonomous driving (Chen et al., 2015, Singh et al., 2022, Leon & Gavrilescu, 2019, Hu et al., 2023), and human-computer interaction (Aggarwal & Park, 2004, Wren & Pentland, 1999, Schmidt, 2000). Existing motion understanding tasks (e.g., action recognition (Soomro et al., 2012, Carreira & Zisserman, 2017), temporal action localization (Caba Heilbron et al., 2015, Jiang et al., 2014), spatiotemporal action/object detection (Gkioxari & Malik, 2015, Gu et al., 2018, Li et al., 2021, Vu et al., 2018, Jiang et al., 2020), video object segmentation (Xu et al., 2018, Seo et al., 2020, Khoreva et al., 2019, Cheng et al., 2023b, Ding et al., 2023)) are designed to either comprehend spatial interactions or detect motions in temporal span. However, motion is a complex spatiotemporal concept involving interactions between visual entities over time. Understanding motion-related attributes abstracted from dynamic scenes is crucial for comprehensive motion understanding. Table 1 highlights that existing tasks only address this challenge from specific aspects. As shown in Figure 1(a), action recognition focuses on identifying actions within a curated video clip, primarily using spatial features. The models are not required to distinguish fine-grained motion patterns over time but to recognize ""the motion"" mostly based on spatial features in a temporal-agnostic (Huang et al., 2018) manner due to potential single-frame bias (Lei et al., 2022). It leads to overlook fine-grained temporal motion patterns. Conversely, temporal action localization in Figure 1(b) emphasizes the temporal dimension but lacks detailed spatial analysis at the object level, relying on snippet-level features. Spatiotemporal action detection aims to localize actions in both dimensions but typically focuses only on humans in predefined actions (e.g., AVA (Gu et al., 2018), MultiSports (Li et al., 2021)), neglecting other interacting objects. It impairs the integrity of the spatial perception of motion understanding. Previous compositional action recognition investigates subject-object interaction and examines whether the model could distinguish pretended actions, but the benchmark (Goyal et al., 2017) only contains short clips, making the task fall short in analyzing the temporal context of motions. Thus, a crucial question arises: What will be a more comprehensive task for motion understanding? Inspired by the recent reasoning segmentation task in image domain (Lai et al., 2023), and considering the spatiotemporal nature of the motion as mentioned above, a feasible answer is to design an implicit video reasoning segmentation task where all necessary spatial and temporal factors of the motion of interest are taken into account, and then the motion-related object, which could be viewed as the medium of the corresponding motion, will be masked out as the final response. Figure 1: The illustration of the comparison between our Motion-Grounded Video Reasoning and previous video motion understanding tasks. Existing video motion understanding tasks (a)-(d) could at most address one or two key problems, either lacking fine-grained spatiotemporal perception or ignoring motion-related reasoning. (e) Our Motion-Grounded Video Reasoning considers both subject and object in motion as well as temporally adjacent events, performing challenging reasoning given four types of questions (Causal, Sequential, Counterfactual, and Descriptive) carefully designed in our GroundMoRe dataset and output spatiotemporal masks to indicate the answer visually at the pixel level. For instance, in the question ‘‘who needs to be passed or else the man in grey cannot easily score?’’, the motion ‘‘pass’’ and the subject ‘‘the man in grey’’ as well as an adjacent event ‘‘easily score’’ are provided in this question, the model needs reason about the object ‘‘the man in pink shorts’’, while output spatiotemporal masks (only between 0 to 32s where the motion ‘‘pass’’ happens). Such a paradigm fully grasps the spatiotemporal contexts of motion and provides an explainable response to evaluate the motion understanding ability. The colors of the questions are corresponded to the spatiotemporal masks. First, understanding specific motions requires analyzing their spatial contexts. For instance, in the interaction scenario ‘‘a boy kicked the ball for entertainment’’, the entities ‘‘a boy’’ and ‘‘the ball’’ constitute the spatial context for the motion ‘‘kicked’’. A comprehensive understanding of ‘‘kicked’’ involves grasping the interaction tuple <a boy, kick, the ball>. While spatiotemporal action localization tasks might address this problem, current benchmarks (e.g., AVA (Gu et al., 2018)) focus primarily on human-centric cases and overlook the bidirectional nature of interactions. A more effective approach would involve a question-answering format that leverages motion-related objects to visualize and reason about the interaction, enhancing spatial understanding. Second, temporal context, which provides chronological order to distinguish different motions, is also crucial for motion understanding. Temporal information not only delineates temporal boundaries but also enables an understanding of cause-and-effect relationships between actions. For example, in ‘‘the woman opened the refrigerator before taking out the milk’’, the two motions are connected, necessitating understanding of both for full comprehension. Thus, a question-answering paradigm can be designed, where a complete scene description with spatiotemporal context is converted into a motion-related question. However, merely answering the question cannot fully convey motion understanding, as language alone, if not visually grounded, is not the most direct explanation of visual concepts (Glenberg & Kaschak, 2002), and temporal information cannot be precisely represented by words (Xiao et al., 2024). Recent studies Yan et al. (2024), Liu et al. (2023a), Bai et al. (2024) have introduced a new task called video reasoning segmentation, which is closely related to our work. However, they primarily emphasize the spatial grounding of target objects through implicit reasoning, while neglecting temporal localization—a critical component for motion understanding. To address these issues and facilitate comprehensive motion understanding, we introduce a novel task: Motion-Grounded Video Reasoning as illustrated in Figure 1(e). This task requires models to take the motion-related question along with the video as input and output spatiotemporal segmentation masks of a specific object as a pixel-level visual answer. Such detailed spatiotemporal grounding allows for advanced motion comprehension. To further evaluate versatile spatiotemporal reasoning, we carefully design four types of questions in our newly collected dataset GroundMoRe (Grounding via Motion Reasoning). As shown in Figure 1(e), Causal questions explore the motivations behind motions, Sequential questions probe the order of temporally adjacent motions, Counterfactual questions are designed for imagining and reasoning about false reality and Descriptive questions ask about the general dynamic scene or abstract motion-related attributes such as enregetic, naughty, excited, etc. GroundMoRe consists of about 1,715 video clips, 7,577 questions and 249K object masks involving 3,942 different objects, ensuring a robust evaluation of motion understanding. Additionally, our task aligns with Video Object Segmentation (VOS) (Xu et al., 2018, Ding et al., 2023) but introduces additional challenges: 1) the use of implicit question inputs versus explicit referring expressions, and 2) the requirement for spatiotemporal object masks rather than spatial-only (no temporal localization requirement in current RVOS datasets), emphasizing the need for accurate temporal perception. We emphasize the practical benefits of the new task in diverse real-world applications. For example, localizing potential threats in public transportation often involves ambiguous information about the suspects (Yu et al., 2023b, Sultani et al., 2018). A robust Motion-Grounded Video Reasoning system can address this by processing queries like ‘‘Who is acting suspiciously in this airport?’’, effectively identifying unusual behaviors with implicit reasoning and spatiotemporal grounding. Table 1: Comparison of different motion understanding tasks. Spatial Context means whether to consider object-level interaction, Temporal Context indicates the influence of temporally adjacent motions/events, Motion Abstraction means understanding of motion-related abstract attributes, Pixel-level Output means whether output object segmentation mask as the final response and Implicit Reasoning means the ability to understand textual input without explicit object information. Tasks Datasets & Benchmarks Spatial Context Temporal Context Motion Abstraction Pixel-level Output Implicit Reasoning Action Recognition Kinetics400 (Carreira & Zisserman, 2017), UCF101 (Soomro et al., 2012) ✗ ✗ ✗ ✗ ✗ Temporal Action Localization ActivityNet (Caba Heilbron et al., 2015), THUMOS14 (Jiang et al., 2014) ✗ ✓ ✗ ✗ ✗ Spatiotemporal Action Localization AVA (Gu et al., 2018), MultiSports (Li et al., 2021) ✓ ✓ ✗ ✗ ✗ Motion Expression Video Segmentation MeViS (Ding et al., 2023) ✓ ✗ ✗ ✓ ✗ Video Reasoning Segmentation ReVOS (Yan et al., 2024), VideoReasonSeg Zheng et al. (2024) ✓ ✗ ✗ ✓ ✓ Motion-Grounded Video Reasoning GroundMoRe (Ours) ✓ ✓ ✓ ✓ ✓ We conduct an extensive evaluation for various image/video grounding baselines on GroundMoRe, though scoring competitive performances in other benchmarks (Kazemzadeh et al., 2014, Xu et al., 2018, Ding et al., 2023), none of them performs satisfyingly on our new task as shown in Table 3. Considering the spatiotemporal reasoning and grounding nature of the task, we further propose a new baseline model called Motion-Grounded Video Reasoning Assistant (MoRA). MoRA integrates LLaVA (Liu et al., 2023a), which is capable of complex multimodal reasoning, as the reasoning module, and a pretrained SAM (Kirillov et al., 2023b) decoder as the mask head. To further empower the model of temporal awareness, we additionally introduce a novel [LOC] token for temporal information embedding and add a temporal localization head to decode a binary temporal mask; thus inhibiting false temporal activation during spatiotemporal mask decoding. Our MoRA achieves overall SOTA performance on the proposed GroundMoRe, but there still remains a large room for future improvement (e.g., HTR (Miao et al., 2024) could reach 67.1 with \mathcal{J}\&\mathcal{F} metric on Ref-YouTubeVOS as its SoTA, while only 10.41 on GroundMoRe), which also underscores the increased difficulty of GroundMoRe. Our contributions are as follows: • We introduce a new task, Motion-Grounded Video Reasoning, designed to assess multimodal models’ reasoning and perception capabilities for motion understanding, filling the gap between referring VOS/action detection and motion-related video reasoning. • We collect a large-scale and versatile video dataset, named GroundMoRe for the proposed Motion-Grounded Video Reasoning task. • We comprehensively evaluate existing image/video grounding baseline models on our GroundMoRe, revealing their deficient motion understanding abilities. On the other hand, our proposed MoRA method achieves SOTA performance on GroundMoRe. The results also suggest substantial room for future improvement."
https://arxiv.org/html/2411.09900v1,Statistical Analysis of Policy Space Compression Problem,"Policy search methods are crucial in reinforcement learning, offering a framework to address continuous state-action and partially observable problems. However, the complexity of exploring vast policy spaces can lead to significant inefficiencies. Reducing the policy space through policy compression emerges as a powerful, reward-free approach to accelerate the learning process. This technique condenses the policy space into a smaller, representative set while maintaining most of the original effectiveness. Our research focuses on determining the necessary sample size to learn this compressed set accurately. We employ Rényi divergence to measure the similarity between true and estimated policy distributions, establishing error bounds for good approximations. To simplify the analysis, we employ the l_{1} norm, determining sample size requirements for both model-based and model-free settings. Finally, we correlate the error bounds from the l_{1} norm with those from Rényi divergence, distinguishing between policies near the vertices and those in the middle of the policy space, to determine the lower and upper bounds for the required sample sizes.","Over recent decades, reinforcement learning [6] has become a powerful tool for tackling sequential decision-making under uncertainty, with notable successes in areas such as game-solving, robotic manipulation, and text generation. However, achieving these breakthroughs often requires reinforcement learning algorithms to be trained on a massive number of samples collected from the environment, limiting their applicability to a broader range of real-world problems. A significant contributor to this sample inefficiency is the vast size of the so-called policy space—the set of decision strategies from which the optimal one is learned. The more decision strategies available, the greater the potential for achieving desirable performance, but this also makes the learning process less efficient. A key approach in reinforcement learning (RL) for solving complex decision-making problems is the use of policy search methods [1]. These methods involve defining a parametric policy space, where the policy is represented as a function parameterized by a set of variables. By optimizing these parameters, policy search aims to discover a policy that maximizes the expected reward in a given environment. This approach allows for flexibility in representing a wide range of policies, making it particularly effective for handling high-dimensional and continuous action spaces, as well as for tasks that require sophisticated strategies. Notable methods in this area include policy gradient techniques like REINFORCE [3] and trust region policy optimization (TRPO) [2], which have been widely used in both discrete and continuous action spaces. Recently, online learning methods have been proposed that perform policy search over a set of finite stochastic policies, providing theoretical guarantees about regret minimization [5, 4]. However, the effectiveness of these approaches depends heavily on how well the set of candidate policies covers the broader policy space. A poor representation of the policy space can lead to suboptimal performance, as the policy search may overlook better strategies. Thus, constructing a well-distributed and diverse set of policies is crucial for the success of these methods. A recent study [12] addresses this problem by proposing a pre-processing step to compress the set of decision strategies. This process, known as policy space compression, aims to reduce the size of the policy space while retaining most of its expressive power. The idea is that the risk of a slight reduction in performance due to the reduction of the policy space is compensated by improved efficiency. In practice, policy space compression seeks to identify a small set of K representative policies that effectively cover the state-action distributions induced by all policies in the original policy space, with the coverage being approximated by a threshold \sigma. While an algorithm for policy space compression is also presented in [12], some important questions remain unanswered, especially from a statistical point of view. The main goal of our research is to statistically address the problem and, consequently, determine the number of sampled interactions necessary to learn an approximate compression of policies in a Markov Decision Process (MDP) [7], ensuring with high confidence that the compression is nearly optimal. To achieve this goal, we will undertake several steps: first, we will utilize an optimization oracle to compute this compression function on estimated MDPs, allowing us to separate the statistical analysis from computational aspects. We then, in Section 3, formalize the statistical problem using Rényi divergence [8], which measures the similarity between the exact and estimated state-action pair distributions of policies. Consequently, in Section 4, we study the meaningful error range because, beyond a specific threshold, the estimated distribution of state-action pairs is guaranteed to be close to the true distribution without needing additional samples. Then, in Section5, we reformulate our original statistical problem by transitioning from Rényi divergence to the Total Variation divergence to simplify our statistical analysis. We determine the number of samples needed for both model-based and model-free settings [6]. Subsequently, in Section 6, we return to our original problem by identifying a correlation between error bounds from the Total Variation divergence and those from Rényi divergence between policies, thus determining the lower and upper bounds for the number of samples required according to our original statistical formulation."
https://arxiv.org/html/2411.09852v1,: Towards Effective Heterogeneous Interaction Learning for Click-Through Rate Prediction,"A clear and well-documented LaTeX document is presented as an article formatted for publication by ACM in a conference proceedings or journal publication. Based on the “acmart” document class, this article presents and explains many of the common variations, as well as many of the formatting elements an author may use in the preparation of the documentation of their work.","Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking an ad or item, is the fundamental task for various applications such as online advertising and recommender systems (Song et al., 2019; Lyu et al., 2020; Zhang et al., 2022, 2024). The quality of CTR prediction significantly influences the company revenue and user experience, drawing extensive attention from both academia and industry (Zhou et al., 2018, 2019; Lyu et al., 2020). For example, in online ad bidding, accurate CTR prediction helps advertisers optimize their bids and target the most receptive audiences. In content recommendation, accurate CTR prediction enables platforms to suggest more relevant content to users. To achieve better CTR prediction, it is crucial to capture the user interests in the evolving environment (Zhou et al., 2018; Lyu et al., 2020; Wang et al., 2019). The abundance of heterogeneous information presents both opportunities and challenges. On the one hand, heterogeneous information depicts user interests from different aspects, providing diverse context (Zhang et al., 2017). For instance, global information, e.g., user profile and context features, offers a static view on general user interests, while user behavior sequences provide substantial information for modeling dynamic user interests (Wang et al., 2019). On the other hand, the heterogeneous nature of the data requires different modeling approaches and careful integration across different information modes (Zhang et al., 2017). For example, while modeling interactions among global information is critical to personalized recommendation (Rendle, 2010; Lian et al., 2018; Wang et al., 2021b), capturing sequential dependencies is the major focus for user behavior modeling (Sun et al., 2019; Chen et al., 2019). Most of the existing CTR prediction models fall into two categories, including non-sequential models and sequential models. Non-sequential models focus on learning informative embeddings through feature interaction via inner-product (Lian et al., 2018; Sun et al., 2021), MLP (Wang et al., 2017, 2021b) and deep structured semantic model (Huang et al., 2013; Elkahky et al., 2015), but ignore the sequential information in user behaviors. Sequential models, in the contrast, employ additional modules, e.g., CNN (Tang and Wang, 2018), RNN (Sun et al., 2019; Zhou et al., 2018) and Attention modules (Lyu et al., 2020; Zhou et al., 2019; Zhai et al., 2024), to capture the sequential dependencies in user behaviors. Promising as it might be, existing sequential methods mostly employ a unidirectional information flow, where global information is used to guide sequence learning, while the reverse information flow from sequence to global information is largely ignored, hence suffering from insufficient inter-mode interaction. For example, global information often captures long-term interests, while sequence information reveals momentary interests, such as a sudden focus on a specific category of products, which can enhance the global context with immediate preference. Besides, due to the computational challenges of performing interaction learning among numerous global features and lengthy sequences, aggressive feature aggregation, e.g., sequence summation (Zhou et al., 2018), pooling (Xiao et al., 2020), and concatenation (Zhou et al., 2019), is often performed at early stages, inevitably leading to excessive information loss. In light of the above limitations, we propose a novel heterogeneous interaction learning module named InterFormer, whose ideas are two-fold. To avoid insufficient inter-mode interaction, we enable bidirectional information flows between different modes, such that global and sequence learning are performed in an interleaving style. Specifically, to learn context-aware sequence embeddings, global sumamrization guides sequence learning via Personalized FFN (pFFN) and Multihead Attention (MHA) (Vaswani et al., 2017). To learn behavior-aware global embeddings, sequence summarization instructs global learning via an interaction module. To mitigate aggressive information aggregation, we adopt MHA for effective information selection, based on which the one-to-one mappings between input and output tokens are retained till the final interaction. Note that our framework is compatible with various interaction learning models like DCNv2 (Wang et al., 2021b), DHEN (Zhang et al., 2022), etc. The main contributions of this paper are summarized as follows: • Challenges. We identify two key bottlenecks of heterogeneous interaction learning, namely insufficient inter-mode interaction and aggressive information aggregation. • Model Design. We propose a novel heterogeneous interaction learning framework named InterFormer for effective feature interaction and selective information aggregation. To our best knowledge, the proposed InterFormer is the first model to address the mutual benefits in heterogeneous interaction learning. • Experiments and Analysis. We carry out extensive experiments on the proposed InterFormer with up to 0.14% AUC improvement on benchmark datasets and 0.15% Normalized Entropy (NE) gain on internal large-scale dataset. Besides, InterFormer exhibits promising scaling results in both feature scaling and model scaling. The rest of the paper is organized as follows. Section 2 briefly reviews the recent works on interaction learning. Section 3 summarizes the preliminaries, and section 4 introduces our proposed InterFormer. Extensive experiments and analyses are carried out in Section 5. We conclude our paper in Section 6."
https://arxiv.org/html/2411.09850v1,"Enhancing Diffusion Posterior Sampling
for Inverse Problems by Integrating Crafted Measurements","Diffusion models have emerged as a powerful foundation model for visual generation. With an appropriate sampling process, it can effectively serve as a generative prior to solve general inverse problems. Current posterior sampling based methods take the measurement (i.e., degraded image sample) into the posterior sampling to infer the distribution of the target data (i.e., clean image sample). However, in this manner, we show that high-frequency information can be prematurely introduced during the early stages, which could induce larger posterior estimate errors during the restoration sampling. To address this issue, we first reveal that forming the log posterior gradient with the noisy measurement ( i.e., samples from a diffusion forward process) instead of the clean one can benefit the reverse process. Consequently, we propose a novel diffusion posterior sampling method DPS-CM, which incorporates a Crafted Measurement (i.e., samples generated by a reverse denoising process, compared to random sampling with noise in standard methods) to form the posterior estimate. This integration aims to mitigate the misalignment with the diffusion prior caused by cumulative posterior estimate errors. Experimental results demonstrate that our approach significantly improves the overall capacity to solve general and noisy inverse problems, such as Gaussian deblurring, super-resolution, inpainting, nonlinear deblurring, and tasks with Poisson noise, relative to existing approaches.","Diffusion models (Ho et al., 2020) have achieved remarkable generative performance on images (Amit et al., 2021; Baranchuk et al., 2021; Brempong et al., 2022), videos (Singer et al., 2022; Wu et al., 2023), audios (Popov et al., 2021; Yang et al., 2023a), natural language (Austin et al., 2021; Hoogeboom et al., 2021; Li et al., 2022) and molecular generation (Hoogeboom et al., 2022; Jing et al., 2022). Besides its strong modeling capacity for complex and high dimensional data, diffusion models have exhibited a strong generative prior to form the diffusion conditional sampling (Song et al., 2020) that can be harnessed for diffusion posterior sampling. In the context of noisy inverse problems, this sampling process effectively approximates precise data distributions from noisy and degraded measurements. Noisy inverse problems, such as super-resolution, inpainting, linear and nonlinear deblurring, are targeted to restore an unknown image \bm{x} from its noise-corrupted measurement \bm{y} given the corresponding forward measurement operators \mathcal{A}(\cdot):\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}. Recently, Diffusion models have been extensively utilized for these tasks (Zhu et al., 2023; Song et al., 2022; Wang et al., 2022), offering a robust framework for reconstructing high-quality images from degraded measurements. Current diffusion-based methods generally employ two distinct strategies to solve inverse problems. The first strategy is to train problem-specialized diffusion models (Saharia et al., 2022; Whang et al., 2022; Luo et al., 2023; Chan et al., 2023) given measurements and clean image pairs. In contrast, methods of the second strategy only capitalize on problem-agnostic pre-trained diffusion models to benefit the zero-shot diffusion restoration sampling by posterior estimate (Song et al., 2023a; Rout et al., 2024; Peng et al., 2024; Mardani et al., 2023) or enforcing data consistency (Chung et al., 2022b). In this paper, we concentrate on the second manner, the posterior estimate for sampling, to solve noisy inverse problems universally. Posterior estimate based methods enable controllable generations (Dhariwal & Nichol, 2021) via diffusion conditional reverse-time SDE (Song et al., 2020), such as classifier guidance (Dhariwal & Nichol, 2021), loss-guided diffusion (Song et al., 2023b). Applying Bayesian, the gradient of log posterior \nabla_{\bm{x}_{t}}\log p_{t}\left(\bm{x}_{t}\mid\bm{y}\right) can be easily adopted into conditional reverse-time SDE sampling as a log-likelihood gradient term \nabla_{\bm{x}_{t}}\log p\left(\bm{y}\mid\bm{x}_{t}\right) and an unconditional prior term \nabla_{\bm{x}_{t}}\log p_{t}\left(\bm{x}_{t}\right). In the context of solving inverse problems, however, p\left(\bm{y}\mid\bm{x}_{t}\right) is an intractable distribution due to the unclear dependency between the measurement \bm{y} and the diffusion generation \bm{x}_{t} at time t. To tackle this issue, existing posterior estimate methods for inverse problems, such as Diffusion Posterior Sampling (DPS (Chung et al., 2022a)), form the measurement model p\left(\bm{y}\mid\hat{\bm{x}}_{0}\right) as the likelihood estimate, where \hat{\bm{x}}_{0} is the denoising prediction given diffusion intermediate output \bm{x}_{t}. This process maps \hat{\bm{x}}_{0} into the measurement \bm{y}’s space, which can be interpreted as a reconstruction loss guidance to push \mathcal{A}\left(\hat{\bm{x}}_{0}\right) close to \bm{y}, while narrowing the gap between \bm{x}_{t} and the clean image \bm{x}. However, via empirical examples in Section 3.1, we show that solving inverse problems in the manner of diffusion posterior sampling follows a similar pattern of diffusion reverse process (Yang et al., 2023b), i.e., focusing on low-frequency recovery at first and posing increasing attention on high-frequency generation in the late stages. With such observation, the log posterior gradient estimate \nabla_{\bm{x}_{t}}\log p\left(\bm{y}\mid\hat{\bm{x}}_{0}\right) in DPS with sharp measurement \bm{y} will easily introduce abrupt high-frequency gradient signals for the subsequent step, which unfits the appropriate input pattern for the pre-trained model \bm{s}_{\theta}\left(\bm{x}_{t},t\right) during the early stages with large t. In fact, Song et al. (2023b) also notes that DPS significantly miscalculates the scale of the guidance term with different variance levels which results in accumulated errors in posterior sampling. In Section 3.1, we have similar observations that DPS amplifies posterior sampling errors. We also find that applying the likelihood estimate p\left(\bm{y}_{t}\mid\hat{\bm{x}}_{0}\right) with randomly sampled noisy measurement \bm{y}_{t} instead of the clean measurement \bm{y} in the DPS for each timestep t leads to a smaller approximation error during the early stages and thus benefits the restoration generation. Posterior approximation with noisy measurement \bm{y}_{t} at timestep t, compared with the clean one, has the advantage that it adaptively matches the frequency pattern of the diffusion model’s generation at the timestep t. Therefore, with this insight, we propose the posterior approximation that leads to less high-frequency signal recovery during the early stages. Specifically, we propose the Diffusion Posterior Sampling with Crafted Measurements (DPS-CM), which can introduce a less biased posterior estimate by combining crafted measurement \mathbf{y}_{t} 111In this work, we denote the crafted measurement as \mathbf{y}_{t}, which is the intermediate generation of the diffusion reverse process with diffusion model \theta, and the noisy measurement as \bm{y}_{t}, which is generated by the diffusion forward process as in Eq. 3., the intermediate generation of another diffusion reverse-time trajectory \{\mathbf{y}_{t}\}_{t=0}^{T} from posterior p\left(\mathbf{y}_{t}\mid\bm{y}\right). As \{\mathbf{y}_{t}\}_{t=0}^{T} shares a similar frequency distribution pattern with the target generation trajectory \{\bm{x}_{t}\}_{t=0}^{T}, i.e., low-frequency recovery at first, the approximated log-likelihood gradient \nabla_{\bm{x}_{t}}\log p\left(\hat{\mathbf{y}}_{0}\mid\hat{\bm{x}}_{0}\right) will bring in less high-frequency gradient signal and thus benefit the subsequent generations. Besides, leveraging crafted measurements \mathbf{y}_{t} brings a lower bias compared with directly using randomly sampled noisy measurement \bm{y}_{t}. Our extensive experiments results on various noisy linear inverse problems, e.g. super-resolution, random masked/fixed box inpainting, Gaussian/Motion deblurring, and nonlinear inverse problems such as nonlinear deblurring, demonstrate that the proposed DPS-CM significantly outperforms existing unsupervised methods on both FFHQ (Karras et al., 2019) and ImageNet (Deng et al., 2009) datasets while keeping the algorithm simplicity."
https://arxiv.org/html/2411.09849v1,"Self-Supervised Radio Pre-training: Toward 
Foundational Models for Spectrogram Learning","Foundational deep learning (DL) models are general models, trained on large, diverse, and unlabelled datasets, typically using self-supervised learning techniques have led to significant advancements especially in natural language processing. These pretrained models can be fine-tuned for related downstream tasks, offering faster development and reduced training costs, while often achieving improved performance. In this work, we introduce Masked Spectrogram Modeling, a novel self-supervised learning approach for pretraining foundational DL models on radio signals. Adopting a Convolutional LSTM architecture for efficient spatio-temporal processing, we pretrain the model with an unlabelled radio dataset collected from over-the-air measurements. Subsequently, the pretrained model is fine-tuned for two downstream tasks: spectrum forecasting and segmentation. Experimental results demonstrate that our methodology achieves competitive performance in both forecasting accuracy and segmentation, validating its effectiveness for developing foundational radio models.","A foundational model is a general model pretrained on a large-scale - usually unlabeled - dataset, typically through self-supervised learning [1]. Through this training, the model develops a solid understanding of the target modality, such as text in natural language processing (NLP) or images in computer vision. This understanding allows the model to be fine-tuned for diverse downstream tasks. Foundational models in NLP [2, 3] and computer vision [4] have driven significant advancements through leveraging the knowledge encoded in their pretrained representations. This facilitates quicker experimentation, more efficient resource utilization, and potentially, improved performance on downstream tasks that smaller models or those with more limited domain knowledge cannot achieve. Deep learning has showcased promising results when applied in wireless communication [5]. The effectiveness has been demonstrated across various tasks, including automatic modulation classification [6], channel estimation [7], constellation and waveform design [8], among others. However, these models are highly specialized, echoing the early stages of deep learning’s evolution in NLP and computer vision. The reliability of these models across data distribution shifts and their ability to generalize is also usually limited. Introducing the concept of foundational models into wireless communication holds substantial promise to overcome these limitations [9]. We argue that as in NLP and computer vision, where a wealth of unlabeled data exists — communication signals can be harnessed for pretraining such foundational models through self-supervised learning, mitigating the expense associated with data labeling. Moreover, leveraging a foundational model as a backbone for multiple downstream tasks, which utilize its pretrained representations in subsequent processing, reduces computational demands. This approach can also improve generalization by leveraging the broader knowledge encoded within foundational model representations compared to highly specialized models which suffer from limited scope. Drawing inspiration from these advancements, particularly in [2, 10, 4], we introduce a foundational radio model pretrained using masked spectrogram modelling (MSM) — a novel technique, we propose for wireless signals. This model is then fine-tuned to perform two different downstream tasks: spectrogram forecasting, which involves predicting future spectrogram based on past data, and spectrogram segmentation, which consists of distinguishing between background noise and other signal activities within the spectrogram. These tasks, while different, are complementary in the context of spectrum analysis and constitute a usage scenario for a foundational model integrated in a opportunistic spectrum access system. The primary contributions of our paper are: • We propose and develop a novel self-supervised learning approach, MSM, for pre-training foundational models on radio signals. To the best of our knowledge, this work represents the first demonstration of radio foundational models for spectrogram learning using unlabeled data. • We demonstrate the effectiveness of the proposed approach utilizing a real-world dataset that we collected over a software-defined radio testbed. The recordings are time-domain IQ samples received between 2.4 to 2.65 GHz. • Our results show that the developed MSM approach is able to learn features that generalize to both related and unrelated downstream tasks. Fine-tuning the foundational model demonstrated competitive results for spectrum forecasting and spectrum segmentation which had a distinctly different and unseen data distribution. The results of this paper highlight the significant potential that radio foundational models have to effectively enable multiple downstream spectrogram tasks. It is envisioned that such models will foster wider adoption of AI to enable reliable network performance and services. The remainder of the paper is structured as follows: Section II presents the two datasets utilized for pretraining the foundational model, and for the spectrum forecasting and segmentation tasks. Section III outlines the architecture and algorithm of the self-supervised foundational model. Section IV presents numerical experiments conducted to evaluate the proposed methodology. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.09844v1,"Deep Autoencoders for Unsupervised Anomaly Detection
in Wildfire Prediction","Wildfires pose a significantly increasing hazard to global ecosystems due to the climate crisis. Due to its complex nature, there is an urgent need for innovative approaches to wildfire prediction, such as machine learning. This research took a unique approach, differentiating from classical supervised learning, and addressed the gap in unsupervised wildfire prediction using autoencoders and clustering techniques for anomaly detection. Historical weather and normalised difference vegetation index datasets of Australia for 2005 – 2021 were utilised. Two main unsupervised approaches were analysed. The first used a deep autoencoder to obtain latent features, which were then fed into clustering models, isolation forest, local outlier factor and one-class SVM for anomaly detection. The second approach used a deep autoencoder to reconstruct the input data and use reconstruction errors to identify anomalies. Long Short-Term Memory (LSTM) autoencoders and fully connected (FC) autoencoders were employed in this part, both in an unsupervised way learning only from nominal data. The FC autoencoder outperformed its counterparts, achieving an accuracy of 0.71, an F1-score of 0.74, and an MCC of 0.42. These findings highlight the practicality of this method, as it effectively predicts wildfires in the absence of ground truth, utilising an unsupervised learning technique.","Forests are essential to maintaining the planet’s ecological equilibrium, but they face threats from fires caused by both natural phenomena and human activities [Qiang \BOthers. (\APACyear2011)]. These wildfires are catastrophic events that negatively impact the environment, the economy, and the natural resources [Meddour-Sahar (\APACyear2015), Sayad \BOthers. (\APACyear2019)]. On the human front, they can lead to loss of life, respiratory ailments, and other health challenges. At the same time, economically, they can devastate communities, affecting livelihoods and leading to billions in damages. The impact on wildlife is also severe, threatening a diverse range of species with effects such as respiratory distress, altered behaviour, and even the threat of extinction. Simultaneously, the natural environment suffers degradation through soil erosion, loss of biodiversity, and the destruction of essential ecosystems such as forests [Sanderfoot \BOthers. (\APACyear2022)]. In recent years, the frequency of wildfires has surged, becoming a global concern that has drawn attention from various fields of study [Abid (\APACyear2021)]. A very recent example would be the wildfire episodes encompassing 25 million acres in Canada in 2023 [Canada (\APACyear\bibnodate)]. The growing threat of wildfire disasters is primarily attributed to climate change. These changes, characterised by higher temperatures and extended dry periods, have led to unprecedented bushfire activities, particularly within Australia [Vardoulakis \BOthers. (\APACyear2020)]. In 2019, Australia, the focus area of this research, encountered its hottest year on record, with the annual national mean temperature surpassing the average by 1.52°C. It also experienced its driest year on record, with notable heatwaves occurring in January and December [Yu \BOthers. (\APACyear2020)]. Between 2019 and 2020, Australia witnessed one of the largest wildfires in modern record. A minimum area of 46 million acres of land was burnt [Sulova \BBA Jokar Arsanjani (\APACyear2020)]. The 2019-20 wildfire is noted for having the strongest measures on record in terms of both magnitude and intensity [Zhang \BOthers. (\APACyear2021)]. Regarding their impacts on climate change and the environment, these fires directly claimed 33 human lives and over one billion indigenous animals [Norman \BOthers. (\APACyear2021)]. Furthermore, 417 individuals succumbed to smoke inhalation, and it is believed that 80% of Australia’s population was directly or indirectly affected by the fires [Borchers Arriagada \BOthers. (\APACyear2020), Norman \BOthers. (\APACyear2021), Ogie \BOthers. (\APACyear2022)]. Moreover, projections indicate that such wildfires will continue to occur due to multiple factors. Based on scientific estimates by the United Nations, extreme wildfires could increase by as much as 50% by the century’s end, spurred by the climate crisis and poor land management [Qiang \BOthers. (\APACyear2011)]. Simulating wildfires is challenging due to the complex interactions of weather conditions, fuel setups, and unpredictable fire movements. The wide geographical and time scales involved make this difficulty even greater, making it a problem that requires significant computational resources [Sayad \BOthers. (\APACyear2019)]. The difficulty in predicting wildfires emphasises the urgent need for innovative approaches, such as machine learning (ML) techniques. Indeed, numerous studies have been conducted to predict or detect wildfires in various scenarios using supervised learning techniques, such as support vector machines (SVM), random forests (RF), artificial neural networks (ANN), decision trees (DT), and more [Abid (\APACyear2021), Pang \BOthers. (\APACyear2022), Pérez-Porras \BOthers. (\APACyear2021), Shmuel \BBA Heifetz (\APACyear2022), Sulova \BBA Jokar Arsanjani (\APACyear2020)]. However, unsupervised learning in wildfire detection has not received much attention from researchers in the prediction phase, and only has been utilized in exploratory data analysis [Sayad \BOthers. (\APACyear2019)]. This research aims to bridge this gap by leveraging the advantages of unsupervised learning, such as accessibility to more data without the need for specific wildfire datasets, and the capability for broader and more flexible analysis without requiring labeled data. Advancements in unsupervised learning, exemplified by generative adversarial networks (GANs), a type of unsupervised autoencoder, have been shown to have potential for rapid, real-time applications in contexts such as wildfire detection [Zenati \BOthers. (\APACyear2018)]. This underscores the broader applicability and benefits of unsupervised learning techniques in this field. The benefits of using unsupervised learning methods are also shown in multiple and varied domains, requiring less computational power, less time consumption and use of less amount of data while yielding relatively good quality results in near-real time [Nhangumbe \BOthers. (\APACyear2023)], reducing the data processing effort by up to 97% while being reliable enough for the task requirements [Lehr \BOthers. (\APACyear2021)], showing strong scalability and interpretability [Chang \BOthers. (\APACyear2020)], achieving the task requirements while reducing the training cost [Tuo \BOthers. (\APACyear2023)], and sometimes producing results on par with supervised techniques [Haar \BOthers. (\APACyear2019), Pradhan \BOthers. (\APACyear2021)]. The study will specifically narrow its focus to Australia. This area is particularly prone to wildfires and has a significant history in this domain, making it a valuable subject for examination. The research aims to predict future wildfires based on anomaly detection before they happen by employing unsupervised learning techniques, using deep autoencoders and clustering. The dataset employed for this project is the combination of two separate datasets: historical weather and Normalised Difference Vegetation Index (NDVI) [IBM (\APACyear2020)]. It employs unsupervised learning techniques, specifically deep autoencoders and clustering, to predict wildfires through anomaly detection, utilizing a unique dataset comprising historical weather and normalized difference vegetation index data. The techniques employed are some of the most common unsupervised methods for anomaly detection [Li \BOthers. (\APACyear2021), Z. Cheng \BOthers. (\APACyear2021)]. This research addresses a gap in the literature by exploring the potential advantages of unsupervised learning in wildfire prediction, such as its reliance on readily available data and reduced computational demands [Nhangumbe \BOthers. (\APACyear2023), Lehr \BOthers. (\APACyear2021), Tuo \BOthers. (\APACyear2023)]. In addition to primary contributions, this study also addresses challenges in wildfire prediction and recommends ways to enhance predictive models in this domain. The remaining sections of this paper are organised as follows: Section 2 offers a literature review; Section 3 provides details about the study area, dataset, and methodology; Section 4 presents results and findings; Section 5 discusses the results; and Section 6 provides conclusions."
https://arxiv.org/html/2411.09834v1,A Benchmark for Long-Form Medical Question Answering,"There is a lack of benchmarks for evaluating large language models (LLMs) in long-form medical question answering (QA). Most existing medical QA evaluation benchmarks focus on automatic metrics and multiple-choice questions. While valuable, these benchmarks fail to fully capture or assess the complexities of real-world clinical applications where LLMs are being deployed. Furthermore, existing studies on evaluating long-form answer generation in medical QA are primarily closed-source, lacking access to human medical expert annotations, which makes it difficult to reproduce results and enhance existing baselines. In this work, we introduce a new publicly available benchmark featuring real-world consumer medical questions with long-form answer evaluations annotated by medical doctors. We performed pairwise comparisons of responses from various open and closed-source medical and general-purpose LLMs based on criteria such as correctness, helpfulness, harmfulness, and bias. Additionally, we performed a comprehensive LLM-as-a-judge analysis to study the alignment between human judgments and LLMs. Our preliminary results highlight the strong potential of open LLMs in medical QA compared to leading closed models.","The majority of existing LLM evaluation benchmarks in medical question answering (QA) have focused on automatic metrics and multiple-choice questions [14, 9, 19, 22]. Although valuable, such metrics and question formats fall short of reflecting the realistic settings of real-world clinical scenarios [24, 19] and do not fully assess or capture the nuances and factual accuracy of LLMs in the medical domain [9, 23, 25]. Additionally, there are concerns about the potential leakage of well-known benchmarks into the training data of LLMs that are subsequently evaluated on those same benchmarks [3]. Furthermore, benchmarks that have not leaked may contain label errors or be outdated [18], leading to flawed and unrealistic evaluations. Moreover, the limited research that has explored human evaluation of long-form medical QA has not made the associated labels publicly available, hindering reproducibility and the ability to study human annotations for insights to inform future work. Figure 1: Overview of our benchmark creation process To address these challenges, we introduce a new publicly available benchmark of real-world consumer medical questions with long-form answer evaluation, annotated by medical doctors. An overview of the process of building our benchmark is shown in Figure 1. Our contributions are as follows: • We develop a publicly available benchmark for long-form medical QA, based on real-world consumer health and medical questions. • We release the medical doctor annotations to the research community, along with our growing collection of real-world consumer medical questions. • We conduct a comprehensive analysis comparing human experts and LLM-as-a-judge for evaluating long-form answer generation in medical questions."
https://arxiv.org/html/2411.09822v1,A Self-Supervised Model for Multi-modal Stroke Risk Prediction,"Predicting stroke risk is a complex challenge that can be enhanced by integrating diverse clinically available data modalities. This study introduces a self-supervised multimodal framework that combines 3D brain imaging, clinical data, and image-derived features to improve stroke risk prediction prior to onset. By leveraging large unannotated clinical datasets, the framework captures complementary and synergistic information across image and tabular data modalities. Our approach is based on a contrastive learning framework that couples contrastive language-image pretraining with an image-tabular matching module, to better align multimodal data representations in a shared latent space. The model is trained on the UK Biobank, which includes structural brain MRI and clinical data. We benchmark its performance against state-of-the-art unimodal and multimodal methods using tabular, image, and image-tabular combinations under diverse frozen and trainable model settings. The proposed model outperformed self-supervised tabular (image) methods by 2.6% (2.6%) in ROC-AUC and by 3.3% (5.6%) in balanced accuracy. Additionally, it showed a 7.6% increase in balanced accuracy compared to the best multimodal supervised model. Through interpretable tools, our approach demonstrated better integration of tabular and image data, providing richer and more aligned embeddings. Gradient-weighted Class Activation Mapping heatmaps further revealed activated brain regions commonly associated in the literature with brain aging, stroke risk, and clinical outcomes. This robust self-supervised multimodal framework surpasses state-of-the-art methods for stroke risk prediction and offers a strong foundation for future studies integrating diverse data modalities to advance clinical predictive modeling.","Stroke ranks as the second leading cause of death worldwide, responsible for 11.6% of global fatalities in 2019. It often results in neurological damage and long-term disability in adults, imposing significant health and economic challenges Pu et al. [2023], Feigin et al. [2021]. Early detection through predictive models is crucial in preventing severe outcomes, as cerebrovascular events can cause irreversible brain damage within hours Flora and Nayak [2019]. The complexity of stroke, driven by multiple risk factors, highlights the importance of integrating multi-modal data to improve diagnostic accuracy and treatment strategies. Among the various imaging techniques, Magnetic Resonance Imaging (MRI) stands out as a highly effective tool, offering high-resolution, non-invasive assessments of structural abnormalities and detailed visualization of the brain’s vascular network Hartwig et al. [2009]. Uni-modal predictive models Prior works mainly use convolutional neural networks (CNN) that can leverage the high-dimensional imaging information for diagnosing patients Zhang and Qie [2023]. Yu et al. applied deep learning algorithms to extract meaningful imaging features in an increasing order of hierarchical complexity to make predictions of the infarct volume Yu et al. [2020]. Other models that use only clinical data, often assume linear relationships between traditional risk factors such as age, gender, smoking status, blood pressure, diabetes, cholesterol levels, and body mass index Hippisley-Cox et al. [2024], An et al. [2020], You et al. [2023]. Alaa et al. used AutoPrognosis, an ensemble machine learning approach, to outperform conventional models like the Framingham score and Cox models Alaa et al. [2019]. A major limitation of these models is that they don’t integrate complementary information from other modalities, similar to how clinicians diagnose using multiple data sources. Biobanks like the UK Biobank (UKB) have become invaluable in this context, providing vast datasets integrating imaging and clinical information to train machine learning models for disease prediction Littlejohns et al. [2020], UK BioBank [2021]. Multi-modal predictive models Several studies have employed multi-modal data to improve diagnostic capabilities by integrating diverse data types Liu et al. [2015]. For example, MultiSurv model has shown success by fusing image and tabular data for cancer survival prediction Vale-Silva and Rohr [2021]. In another study, integration of retinal images and clinical data was leveraged to improve cardiovascular disease prediction Huang et al. [2024]. Multi-modal models combining image and clinical data have demonstrated better prediction performance for disability prediction in stroke patients White et al. [2023], Liu et al. [2023]. However, CNNs tend to prioritize image features, and simple image-tabular CNN concatenation fails to enhance predictive models due to insufficient cross-modal interactions. To address this, Wolf et al. developed the Dynamic Affine Feature Map Transform (DAFT), which conditions convolutional feature maps on both image and tabular data, enabling a two-way information exchange via an auxiliary neural network Wolf et al. [2022]. While DAFT reduces issues related to the large number of trainable parameters in standard 3D CNNs and the curse of dimensionality, it may sacrifice some predictive power compared to deeper models like ResNet. Although recent models show promise in biomedical prediction tasks, their clinical translation is hindered by limited annotated datasets, low disease prevalence, and the risk of overfitting. Self-supervised learning (SSL) is a powerful technique for extracting representative features from unlabeled data, making it valuable for early disease risk identification. Self-supervised models Unlike traditional supervised learning, SSL defines pretext tasks that allow models to learn meaningful representations from raw data Balestriero et al. [2023]. One prominent SSL technique is contrastive learning, which trains encoders to generate augmented views of a sample, maximizing similarity between these views while minimizing similarity with other samples Balestriero et al. [2023]. Popular methods such as SimCLR Chen et al. [2020], BYOL Grill et al. [2020], and MOCO He et al. [2020] have demonstrated success in imaging tasks, while VIME Houthooft et al. [2016] and SCARF Bahri et al. [2022] are leading approaches for tabular data. Emerging approaches, like contrastive language-image pre-training (CLIP) strategy, have evolved from unimodal methods to integrate diverse modalities. While there was an extensive work done for cardiovascular diseases prediction Radhakrishnan et al. [2023], Du et al. [2024], Hager et al. [2023], Girlanda et al. [2024], stroke risk prediction through volumetric brain images and clinical health records remains underexplored. We present for the first time, to the best of our knowledge, a self-supervised multi-modal approach integrating 3D brain MRIs with clinical tabular data for stroke risk prediction. As depicted in Figure 1, our methodology incorporates cross-modal interactions via CLIP loss Radford et al. [2021] and image-tabular matching (ITM) loss Li et al. [2021], Du et al. [2024]. We demonstrate that our learning strategy outperforms leading (self-)supervised unimodal methods and that multi-modal image-tabular pre-training leads to better representations and improved downstream performance. Lastly, we validate the model’s learned features through visual activation maps, which align with established clinical and neurological findings on stroke-related brain pathology. Code is available at https://github.com/CamilleDelgrange/SSMSRPM."
https://arxiv.org/html/2411.09820v1,"WelQrate: Defining the Gold Standard in 
Small Molecule Drug Discovery Benchmarking","While deep learning has revolutionized computer-aided drug discovery, the AI community has predominantly focused on model innovation and placed less emphasis on establishing best benchmarking practices. We posit that without a sound model evaluation framework, the AI community’s efforts cannot reach their full potential, thereby slowing the progress and transfer of innovation into real-world drug discovery. Thus, in this paper, we seek to establish a new gold standard for small molecule drug discovery benchmarking, WelQrate. Specifically, our contributions are threefold: WelQrate Dataset Collection - we introduce a meticulously curated collection of 9 datasets spanning 5 therapeutic target classes. Our hierarchical curation pipelines, designed by drug discovery experts, go beyond the primary high-throughput screen by leveraging additional confirmatory and counter screens along with rigorous domain-driven preprocessing, such as Pan-Assay Interference Compounds (PAINS) filtering, to ensure the high-quality data in the datasets; WelQrate Evaluation Framework - we propose a standardized model evaluation framework considering high-quality datasets, featurization, 3D conformation generation, evaluation metrics, and data splits, which provides a reliable benchmarking for drug discovery experts conducting real-world virtual screening; Benchmarking - we evaluate model performance through various research questions using the WelQrate dataset collection, exploring the effects of different models, dataset quality, featurization methods, and data splitting strategies on the results. In summary, we recommend adopting our proposed WelQrate as the gold standard in small molecule drug discovery benchmarking. The WelQrate dataset collection, along with the curation codes, and experimental scripts are all publicly available at WelQrate.org.","1 Introduction Deep learning has revolutionized the field of drug discovery, providing advanced computational tools to predict the activity of small molecules against therapeutic targets. However, the focus of the AI community has primarily been on developing novel models, often putting less emphasis on establishing robust and standardized benchmarking practices. Ultimately, this disparity can impede the practical application of AI innovations in drug discovery [wognum2024call]. Typically, High-Throughput Screening (HTS) methods are prevalent for identifying promising compounds, but they are costly, time-consuming, and limited in their ability to explore the chemical space [sliwoski2014computational, leelananda2016computational]. Thus, computer-aided drug discovery seeks to train models on HTS data to offer a more efficient and scalable computational effort to predict the activity of compounds based on their structure, which is known as virtual screening. However, in spite of the importance on ensuring high-quality data for training these models, currently only a few datasets for virtual screening exist, such as MoleculeNet and Therapeutics Data Commons (TDC), but these datasets often suffer from issues like inconsistent chemical representations, undefined stereochemistry, and noisy experimental data. These flaws necessitate a more rigorous approach to dataset curation. To address these challenges, we propose a new standard, WelQrate, for benchmarking small molecule drug discovery, the contributions of which are threefold as follow: • WelQrate Dataset Collection: The WelQrate dataset collection are curated with stringent quality control measures including hierarchical curation, various filters and domain expert verification. The final dataset collection covers a diverse range of important theurapeutic target classes. • WelQrate Evaluation Framework: The WelQrate evaluation framework incorporates critical aspects including high-quality datasets, featurization, 3D conformation generation, evaluation metrics, and data splits to provide a reliable basis for model comparison. • Benchmarking: Examining model performance across several research questions using the WelQrate dataset collection, investigating how different models, dataset quality, featurization, and data split impact results. Our work is driven by the need to ensure that AI models are evaluated on realistic and high-quality datasets, facilitating the translation of AI innovations into practical drug discovery solutions. The WelQrate dataset collection, along with detailed curation procedures and experiment scripts, is publicly available and maintained at WelQrate.org. We advocate for the adoption of our standardized evaluation practices and well-curated datasets to set a new gold standard in small molecule drug discovery, to ensure more reliable and realistic evaluations."
https://arxiv.org/html/2411.09807v1,"Evaluating Loss Landscapes 
from a Topology Perspective","Characterizing the loss of a neural network with respect to model parameters, i.e., the loss landscape, can provide valuable insights into properties of that model. Various methods for visualizing loss landscapes have been proposed, but less emphasis has been placed on quantifying and extracting actionable and reproducible insights from these complex representations. Inspired by powerful tools from topological data analysis (TDA) for summarizing the structure of high-dimensional data, here we characterize the underlying shape (or topology) of loss landscapes, quantifying the topology to reveal new insights about neural networks. To relate our findings to the machine learning (ML) literature, we compute simple performance metrics (e.g., accuracy, error), and we characterize the local structure of loss landscapes using Hessian-based metrics (e.g., largest eigenvalue, trace, eigenvalue spectral density). Following this approach, we study established models from image pattern recognition (e.g., ResNets) and scientific ML (e.g., physics-informed neural networks), and we show how quantifying the shape of loss landscapes can provide new insights into model performance and learning dynamics.","Given the important role that the loss function plays during learning, examining it with respect to a neural network’s weights—by visualizing the so-called loss landscape—can provide valuable insights into both network architecture and machine learning (ML) dynamics (Martin and Mahoney, 2021; Martin et al., 2021; Yang et al., 2022b, 2021; Zhou et al., 2023). Indeed, the loss landscape has been essential for understanding certain aspects of deep learning, including, but not limited to, test accuracy, robustness of transfer learning (Djolonga et al., 2021), robustness to out-of-distribution detection (Yang et al., 2022a), robustness to adversarial attack (Kurakin et al., 2016), and generalizability (Cha et al., 2021). There are two popular approaches to generating the loss landscape for a given neural network model. Initial efforts to visualize the loss landscape relied on sampling random orthogonal vectors and projecting weights onto the plane spanned by these random vectors (Goodfellow et al., 2014; Li et al., 2018). More recently, Yao et al. (2020) proposed using directions based on the Hessian, wherein the first two most important Hessian eigenvectors are used to capture more meaningful changes in the loss function. In both approaches, a neural network’s parameters are perturbed along each direction, and the loss is re-evaluated at each of these positions. While both approaches have provided valuable insights, loss landscape visualization (no matter which method was used) is often limited to just that—visualization. In other words, loss landscapes, once created, are often simply visually explored or qualitatively compared. It is less clear how to meaningfully measure or quantitatively relate these landscapes to features of the model’s underlying architecture or to properties inherent to the learning process. Indeed, examining and quantifying a loss landscape—which is inherently high-dimensional, with as many dimensions as the number of parameters in the model—is challenging to do, especially when using two-dimensional views and qualitative observations alone. To provide a more quantitative approach to understanding and using loss landscapes, here we show how topological data analysis (TDA) can be used to quantify and extract (quantitative) insights based on the topology (or shape) of those landscapes. We first compute loss landscapes using either random projections or Hessian-based directions and explore four different representations, including one image data representation (where the loss is stored as pixels) and three unstructured grid representations (where the loss is stored on the vertices of a graph). We then apply two methods from TDA, namely, the merge tree (Carr et al., 2003; Heine et al., 2016) and persistence diagram (Edelsbrunner and Harer, 2008), to quantify and compare different loss landscapes. We quantify these structures by measuring the number of saddle points and average persistence, respectively, and we compare our results with state-of-the-art methods for evaluating model performance, as well as with more recent methods for evaluating the local geometry of loss landscapes based on the Hessian."
https://arxiv.org/html/2411.09788v1,"AI-Driven Human-Autonomy Teaming in Tactical Operations: Proposed Framework, Challenges, and Future Directions","Artificial Intelligence (AI) techniques, particularly machine learning techniques, are rapidly transforming tactical operations by augmenting human decision-making capabilities. This paper explores AI-driven Human-Autonomy Teaming (HAT) as a transformative approach, focusing on how it empowers human decision-making in complex environments. While trust and explainability continue to pose significant challenges, our exploration focuses on the potential of AI-driven HAT to transform tactical operations. By improving situational awareness and supporting more informed decision-making, AI-driven HAT can enhance the effectiveness and safety of such operations. To this end, we propose a comprehensive framework that addresses the key components of AI-driven HAT, including trust and transparency, optimal function allocation between humans and AI, situational awareness, and ethical considerations. The proposed framework can serve as a foundation for future research and development in the field. By identifying and discussing critical research challenges and knowledge gaps in this framework, our work aims to guide the advancement of AI-driven HAT for optimizing tactical operations. We emphasize the importance of developing scalable and ethical AI-driven HAT systems that ensure seamless human-machine collaboration, prioritize ethical considerations, enhance model transparency through Explainable AI (XAI) techniques, and effectively manage the cognitive load of human operators.","The convergence of AI and autonomous technologies has revolutionized various industries, including defense and tactical operations. The rise of HAT can be attributed to several factors, including rapid advancements in autonomous technologies and AI [1], the increasing complexity of tasks and environments, the development of more capable autonomous systems, and the increasing availability of data and computing power [2]. As these technologies have become more sophisticated and capable, there has been a growing recognition of the potential collaborations that can be achieved by combining human cognitive abilities with the computational power and efficiency of autonomous systems [3]. The rise of modern HAT systems has also been driven by the need to address the complexities and challenges of rapidly evolving and dynamic environments. As tasks become more complex, time-sensitive, and data-intensive, the collaboration between humans and autonomous agents becomes crucial for effectively navigating and responding to these challenges. HAT is an emerging field that explores collaborative partnerships between humans and autonomous systems to perform tasks or achieve common goals [2, 4, 5, 6]. This involves a collaborative arrangement in which at least one human worker collaborates with one or more autonomous agents [2]. This collaborative approach has the potential to revolutionize how tasks are accomplished across various sectors and pave the way for a future where humans and intelligent autonomous systems will work hand in hand to tackle complex problems and achieve shared goals. HAT systems are designed to allow humans to delegate tasks to intelligent autonomous agents while maintaining overall mission control [7]. Autonomous agents, in this context, refer to computer entities with varying degrees of self-governance in decision-making, adaptation, and communication. This definition has been supported by studies conducted by the research works in [8, 9]. The integration of human cognitive capabilities with the computational power and efficiency of autonomous systems in HAT enhances performance, decision-making, and overall system capabilities. Here, we define and clarify some key concepts that are fundamental to understanding the scope and context of this study. These concepts include AI, Autonomy, Autonomous Systems, and Tactical Autonomy. By providing clear definitions and distinguishing between these terms, we aim to establish a common understanding among our readers. Autonomy. Autonomy in the context of HAT describes the ability of intelligent autonomous systems or agents to operate and make decisions independently in a team setting with varying degrees of self-governance [3, 10]. This involves a higher degree of decision-making capability in autonomous systems based on learning, adaptation, and reasoning. It is a property of a system, not a technology itself [10]. An autonomous entity can perceive, reason, plan, and act in pursuit of specific goals or objectives without constant human intervention. It is important to note that the level of autonomy can vary, ranging from fully autonomous systems that make all their decisions to semi-autonomous systems that require human input at certain points [10]. In the context of tactical autonomy, HAT involves the integration of autonomous capabilities into tactical operations. This integration can include various applications, such as using autonomous systems to gather intelligence, perform surveillance, and perform other critical activities. Autonomy enables systems to operate in complex and uncertain environments, learn from experience, and make decisions without explicit human intervention in every scenario. However, it is important to distinguish this from traditional automation, which typically follows pre-programmed rules, decision trees, or logic-based algorithms to perform tasks or make decisions. Traditional automation has limited adaptability and flexibility to handle dynamic or unforeseen situations without explicit programming. This paper discusses how AI-driven autonomy differs from traditional automation by emphasizing learning, adaptation, and decision-making capabilities. These capabilities ultimately enhance the overall effectiveness and agility of human-autonomy teaming in tactical operations. Autonomous Systems. Autonomous systems can perform tasks or operations without constant human control. They utilize AI algorithms and sensors to perceive and navigate their environment, achieving a high degree of autonomy [11]. Tactical Autonomy. In this study, tactical autonomy refers to autonomous systems’ ability to make real-time decisions and take actions in dynamic and complex operational environments [12]. This involves the seamless coordination and interaction between humans and autonomous systems, enabling them to function as a unified team with complementary strengths [12]. HAT focuses on achieving shared mission goals through seamless coordination and collaboration between human operators and intelligent autonomous systems [13]. This paper introduces an AI-driven HAT, which integrates AI into HAT frameworks. This approach improves decision-making, situational awareness, and operational effectiveness by combining the strengths of human expertise and AI capabilities. Tactical autonomy, which combines human cognitive abilities, such as adaptability, intuition, and creativity, with the computational power, precision, and dynamic execution of autonomous systems, has the potential to revolutionize various fields, including defense, emergency response, law enforcement, and hazardous environments [12]. It is important to differentiate between tactical and strategic autonomy to clarify how AI-driven human-autonomy teaming contributes to both levels of autonomy in military and operational contexts. Strategic autonomy refers to a nation or organization’s ability to make autonomous choices regarding broad security goals, whereas tactical autonomy, in contrast to strategic autonomy, focuses on individual units or teams acting independently within a specific mission [14]. Strategic autonomy involves higher-level decision-making and planning that considers long-term goals, overall mission objectives, and broader situational awareness. It addresses the coordination, allocation of resources, and strategic decision-making processes that guide the overall mission or campaign [14]. Tactical Operations. Tactical operations involve coordinated activities in a specific area or environment, typically in a military, law enforcement, or strategic context, focusing on achieving short-term objectives through rapid decision-making, adaptation to dynamic situations, and the application of military skills and resources within a localized area and timeframe [15]. In recent years, advancements in AI, Machine Learning (ML), robotics, and sensor technologies have paved the way for realizing the potential of tactical autonomy [12]. These technological advancements have enabled autonomous systems to perform complex tasks, process vast amounts of data in real-time, make informed decisions, and collaborate with human team members seamlessly [12]. This has opened new possibilities for augmenting human capabilities, optimizing resource allocation, and improving overall operational efficiency. However, effective tactical autonomy requires a comprehensive understanding of the dynamics between humans and autonomous systems. Human factors, including trust, communication, shared situational awareness, and decision-making, play a vital role in ensuring successful HAT. Challenges such as establishing appropriate levels of trust, addressing potential cognitive biases, managing workload distribution, and maintaining effective communication channels must be carefully addressed to ensure seamless collaboration and maximize the potential benefits of tactical autonomy. HAT for tactical autonomy is a collaborative approach to using humans and autonomous systems to operate and control weapons and other military systems. In HAT, the human operators and autonomous systems work together to achieve common goals. The human operators are responsible for the overall mission and making high-level decisions. Autonomous systems are responsible for performing assigned tasks. As explained in detail in Section IV, human operators contribute strategic insight, context, and high-level decision-making capabilities based on their experience and understanding of the mission’s goals. The interaction and communication represent the interfaces and communication channels through which each component exchanges information, collaborates, and makes joint decisions. Within the context of a shared decision-making process, human operators and autonomous systems engage in a collaborative decision-making process, sharing insights, data, and recommendations to formulate effective strategies. The autonomous system is responsible for real-time data processing, analysis, and execution of specific tasks supporting human operators with timely and pertinent information. Subsequently, once decisions are made, the autonomous system performs specific tasks, including reconnaissance, navigation, or data collection, in alignment with the directives of the shared decision-making process. This paper comprehensively explores the historical development and current state of HAT and delves into the opportunities, challenges, and potential future directions in leveraging AI for tactical autonomy. It emphasizes the transformative impact of AI on tactical autonomy and presents opportunities for improved decision-making, situational awareness, and resource optimization. By acknowledging and addressing the challenges associated with AI adoption, and by charting future directions for research, we can pave the way for a future where humans and autonomous systems seamlessly collaborate, ultimately leading to safer, more efficient, and successful missions in tactical environments. I-A Scope and Contributions The main contribution of this paper is its forward-looking study of the applications, trends, and disruptive technologies that will drive the HAT revolution in complex and dynamic environments. This provides a clear picture of HAT services and practical recommendations for future work. I-B Contributions This paper makes the following key contributions to the field of HAT. • We propose a comprehensive conceptual framework for AI-driven HAT in tactical operations, describing critical components such as trust and transparency, function allocation, situational awareness, and ethical considerations. The proposed framework provides a foundation to understand and advance the integration of AI into HAT for tactical environments. • We provide a comprehensive overview of the opportunities and key challenges associated with incorporating AI-driven HAT into tactical operations. • We explore the symbiotic relationship between AI and HAT, presenting a thorough analysis of how AI-driven HAT enhances decision-making, situational awareness, and operational effectiveness in tactical environments. • We identify several research directions for future work in AI-driven HAT, emphasizing ethical considerations, building transparent AI models, and advancing human-centric design principles to fully realize the potential of tactical autonomy. Table I compares our work to existing studies. In this paper, we explore and address research questions related to AI-driven HAT to enhance tactical operations, covering various aspects and challenges. • How do AI and HAT benefit each other when achieving tactical autonomy? • What are the main opportunities and challenges associated with incorporating AI-driven HAT in the context of tactical operations? • How can AI-driven HAT be best used in tactical operations to improve success and decision-making? • What is the plan for AI-driven HAT and how can it improve the collaboration between humans and autonomous systems in tactical situations? • How can AI-driven HAT help humans and autonomous systems work together smoothly to achieve common goals in tactical environments? • What ethical concerns must be considered when developing and using AI-driven HAT systems? • How can we make AI models in HAT more understandable, and why does this matter for better decision-making and trust in autonomous systems? • What design principles should be followed to create user-friendly AI-driven HAT systems for human operators in tactical settings? TABLE I: Comparison of our work to existing works. Year Publications Main Research Focus and Scope 2018 Ref [16] • Explores the relationship between team coordination dynamics and team performance for human-autonomy teams using an extended version of nonlinear dynamical systems methods. 2018 Ref [17] • Proposed a framework for HAT, incorporating three key tenets: transparency, bi-directional communication, and operator-directed authority. 2019 Ref [18] • Discusses what function allocation and challenges in allocating tasks between humans and autonomous machines. 2020 Ref [19] • Provides a framework for practitioners to make informed decisions regarding the integration and training of human-autonomy teams in applied settings. 2020 Ref [20] • Proposes a new approach to using ML agents in real-time strategy games to collaborate with human players rather than competing against them. 2021 Ref [3] • Examines the differences between automation and autonomy and how insights from human-human teaming can be applied to HAT. The authors have identified research gaps that need to be addressed to improve the understanding of HAT. 2022 Ref [2] • Provides a comprehensive understanding of the research environment, dependent variables, independent variables, key findings, and future research directions related to human-autonomy teamwork. 2022 Ref [21] • Emphasizes the need for humans and AI to work together effectively, particularly in complex situations. It examines the factors affecting the design and implementation of AI systems for human interaction. In addition, it provides a detailed roadmap for future HAT research, particularly emphasizing the perspectives of human factors, which aligns well with our focus on enhancing tactical operations through AI-driven HAT. 2024 Our Paper • Proposes a comprehensive conceptual framework for AI-driven HAT in tactical operations, detailing critical components, such as trust and transparency, function allocation, situational awareness, and ethical considerations. • Explores the advantages and challenges associated with integrating AI-powered HAT into tactical operations. • Provides a thorough exploration of the symbiotic relationship between AI and HAT in the context of tactical operations. • Identifies several research directions, including ethical considerations, building transparent AI models, and advancing human-centric design principles, for future work in AI-driven HAT. I-C Methodology This study investigates the potential of AI-driven HAT to revolutionize tactical operations. To achieve this, we conducted a systematic literature review to identify and analyze relevant academic research. Our search primarily targeted prominent academic databases such as Google Scholar, IEEE Xplore, ACM Digital Library, and ScienceDirect for scholarly articles published up to 2024. We focused on studies published up to May 2024 that emphasized empirical research and theoretical frameworks to explore the application of AI in human-autonomy teaming for tactical operations. Note that studies that focused on general AI applications without a tactical operation context were excluded. We employed a combination of keywords, including “AI-driven human-autonomy teaming,” “tactical operations,” “situational awareness,” “automated decision-making,” “Integrating AI and HAT,” “situation models,” and “shared situational awareness in HAT.” We included studies that focused on the application of AI in HAT for tactical operations, explored the use of Natural Language Processing (NLP) and reinforcement learning for improved communication, collaboration, and threat assessment, and addressed challenges related to trust, explainability, and ethical considerations. Furthermore, we included studies that explored the impact of AI-driven HAT on trust, explainability, and ethical considerations. We employed thematic analysis to identify key themes emerging from the reviewed literature, focusing on the opportunities and challenges associated with AI-driven HAT, with a particular emphasis on enhancing situational awareness, decision-making, and human-machine collaboration. The remainder of this paper is organized as follows. Section II discusses the integration of AI solutions into HAT. In Section III, we discuss the concept of delegated autonomy in HAT, exploring different levels and the balance between human decision-making and automated systems in teaming scenarios. Section IV presents the key components and characteristics defining HAT systems. Next, Section V identifies and discusses the practical applications of HAT, presenting real-world examples where HAT has proven advantageous. Section VI explores the economic aspects of AI integration in HAT. VII provides a detailed discussion of situation models and shared situational awareness in HAT. Section VIII outlines the specific roles and contributions of AI in enabling tactical autonomy in HAT, emphasizing its ability to enhance human decision-making. The opportunities and challenges associated with using AI to enhance HAT in tactical autonomy are discussed in Section X. The design of user interfaces and interaction mechanisms for HAT systems in tactical autonomy settings is explored in Section IX. Section XI introduces a proposed framework for AI-driven HAT in tactical operations, describes the key components, and provides guidance for future research and development. Finally, Section XII provides practical recommendations for implementing and optimizing HAT systems. The paper concludes in Section XIII with indications for future work."
https://arxiv.org/html/2411.09730v1,SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation,"Disaggregated evaluation—estimation of performance of a machine learning model on different subpopulations—is a core task when assessing performance and group-fairness of AI systems. A key challenge is that evaluation data is scarce, and subpopulations arising from intersections of attributes (e.g., race, sex, age) are often tiny. Today, it is common for multiple clients to procure the same AI model from a model developer, and the task of disaggregated evaluation is faced by each customer individually. This gives rise to what we call the multi-task disaggregated evaluation problem, wherein multiple clients seek to conduct a disaggregated evaluation of a given model in their own data setting (task). In this work we develop a disaggregated evaluation method called SureMap that has high estimation accuracy for both multi-task and single-task disaggregated evaluations of blackbox models. SureMap’s efficiency gains come from (1) transforming the problem into structured simultaneous Gaussian mean estimation and (2) incorporating external data, e.g., from the AI system creator or from their other clients. Our method combines maximum a posteriori (MAP) estimation using a well-chosen prior together with cross-validation-free tuning via Stein’s unbiased risk estimate (SURE). We evaluate SureMap on disaggregated evaluation tasks in multiple domains, observing significant accuracy improvements over several strong competitors.","Evaluation is a key challenge in modern AI, with much effort spent deciding what metrics to measure, with which methods, and on what data. This challenge is especially acute in fairness assessment, which requires not only high-quality data to run a model and score its outputs but also demographic information for defining groups. Due to the high cost of obtaining high-quality evaluation data, the issue of sample complexity—sample size needed to get a good performance estimate—remains salient, especially when we want to release not just one overall measure but instead to output a disaggregated evaluation that captures variation among demographic subpopulations of the data (Barocas et al., 2021). For instance, we might want to assess group fairness by examining the variation in performance across groups of users defined by intersections of the demographic attributes age, race, and sex. The naive approach of independently evaluating each group’s performance on its own data can fail because the sample sizes of intersectional groups rapidly decrease as we consider more attributes (Herlihy et al., 2024). Recent work has shown how to improve upon naive methods by combining data from multiple subpopulations to inform their individual performance estimates (Miller et al., 2021; Herlihy et al., 2024). In today’s technology landscape it is common for multiple clients to procure the same model (e.g., an automated speech recognition or language model) from an AI developer, with each client performing a disaggregated evaluation of the same model on their own data. We refer to this problem as the multi-task disaggregated evaluation. We formalize and study this problem, showing that one can improve the disaggregated evaluations of individual clients by using multi-task data in the form of summary statistics from other clients or from the model provider. Our approach uses the (out-of-distribution) multi-task data to set the parameters of a multivariate normal prior and then performs maximum a posteriori (MAP) inference on the (in-distribution) client data. Formally, we model the problem as Gaussian mean estimation and design a simple-yet-expressive additive prior that can capture many different relationships between subpopulations. Drawing upon classical statistics, we fit prior parameters by minimizing Stein’s unbiased risk estimator (SURE, Stein, 1981). While motivated by multi-task considerations, we show that our method also performs well in the single-task setting. 1.1 Contributions 1. SureMap: We introduce a method that uses SURE to tune the parameters of a well-chosen Gaussian prior before applying MAP estimation. The prior is motivated by its attainment of a good efficiency–expressivity tradeoff, requiring only a linear (in the number of subpopulations) number of parameters to recover several natural baselines for disaggregated evaluation. 2. Datasets: Disaggregated evaluation has few benchmarks (Herlihy et al., 2024), so we introduce new ones for both the single-task and multi-task settings, covering automated speech recognition (ASR) and also tabular domains (with linear models and also in-context LLMs). 3. Single-task: We find that SureMap is always competitive with strong baselines from prior work, while improving significantly in some settings with intersectional sensitive attributes. 4. Multi-task: Incorporating data from multiple clients into SureMap yields significant improvements across all evaluated settings. This multi-task approach is more accurate even with just one additional task and is the only method to consistently outperform the naive and pooling baselines. 1.2 Related work Disaggregated evaluation is a core task in the fairness assessment of AI systems (Barocas et al., 2021). Past work has sought to improve estimation accuracy by combining information across different groups, e.g., via Bayesian modeling (Miller et al., 2021), Gaussian process approximation of loss surfaces (Piratla et al., 2021), and structured regression (Herlihy et al., 2024). The last work found that classical James–Stein-type mean estimation (James and Stein, 1961; Bock, 1975) is often competitive, and so we adopt it as our first non-naive baseline. We also compare to structured regression itself, which turns out to have a tight mathematical connection to SureMap; indeed, apart from our use of Gaussian (ridge) rather than Laplace (lasso) priors (regularization)—as well as our use of a more flexible tuning based on SURE rather than cross-validation—the method of Herlihy et al. (2024) can be viewed as the discriminative counterpart to our generative approach (see §E for details). Within the disaggregated evaluation literature we are the first to formulate and study multi-task disaggregated evaluation. This is an important direction because (a) model providers often have their own data or data from multiple clients that can inform the evaluation and (b) transferring information across distributions is a key way to handle very low-sample regimes. We also contribute several datasets that we hope will spur further development in disaggregated evaluation. SureMap relies on applying classical mean estimation tools to quantities modeled as Gaussian means. Notably, Miller et al. (2021) model scores via well-studied distributions—e.g., Gaussians—but since scores are related non-linearly to metrics it is unclear if this can lead to similarly simple estimators. To tune parameters, we use SURE, a popular statistical approach (Li, 1985; Donoho and Johnstone, 1995). Specifically, in the empirical Bayes tradition, we use it to set the MAP estimator of a hierarchical model. Using SURE to tune the scale of an isotropic Gaussian prior was shown to be asymptotically (in the dimension) optimal in the case of heteroskedastic data distributions (Xie et al., 2012). Since disaggregated evaluation data is highly heteroskedastic due to variation in group size, this is positive evidence for our approach, although our prior is non-isotropic and has many more variance parameters."
https://arxiv.org/html/2411.09722v1,"Iterative Batch
Reinforcement Learning via
Safe Diversified
Model-based Policy Search","Batch reinforcement learning enables policy learning without direct interaction with the environment during training, relying exclusively on previously collected sets of interactions. This approach is, therefore, well-suited for high-risk and cost-intensive applications, such as industrial control. Learned policies are commonly restricted to act in a similar fashion as observed in the batch. In a real-world scenario, learned policies are deployed in the industrial system, inevitably leading to the collection of new data that can subsequently be added to the existing recording. The process of learning and deployment can thus take place multiple times throughout the lifespan of a system. In this work, we propose to exploit this iterative nature of applying offline reinforcement learning to guide learned policies towards efficient and informative data collection during deployment, leading to continuous improvement of learned policies while remaining within the support of collected data. We present an algorithmic methodology for iterative batch reinforcement learning based on ensemble-based model-based policy search, augmented with safety and, importantly, a diversity criterion.","The objective of batch (or offline) reinforcement learning (RL) is to extract the best possible behavior out of existing data, called a batch, without any learning during deployment. This implies that learning is more successful if the initial data is diverse or collected through the deployment of an expert agent. In a real setting, the initial batch is prone to limitations (low data coverage, low reward actions, etc.), forming a challenge in learning for real-world applications. This challenge of limited information calls for safety mechanisms, such as regularization, to ensure reliable performance of the agent [1, 2]. In many industrial setups, the application of offline reinforcement learning is not a one-time process but iterative. After an RL agent is trained and deployed on the system, a new set of recordings becomes available. The principal contribution of our work relies on the formulation of iterative batch reinforcement learning (IBRL), a novel framework to iteratively refine the initial data batch and improve learned policies after each new batch collection, without dropping performance due to overly adventurous exploration. In every iteration, we seek to improve the data coverage by deploying a set of policies, that we previously trained to be diverse, i.e. that act in a variety of ways to explore, without compromising the rewards too much. The proposed IBRL algorithms adhere to safety constraints by restricting the state or action space of the learned policies depending on the data support. Through experiments in an illustrative 2D environment, as well as on the Industrial Benchmark [3], we demonstrate the improved exploration capability and resulting improved performance of our approach, all while maintaining safety considerations and not underperforming the behavioral. Conceptually, our work is most closely related to [4, 5, 6], however these works do not address the combination of diversity and safety, or focus solely on the iterative process without incorporating an exploration incentive."
https://arxiv.org/html/2411.09718v1,Non-Functional Requirements in Medical Imaging,"The diagnostic imaging departments are under great pressure due to a growing workload. The number of required scans is growing and there is a shortage of qualified labor. AI solutions for medical imaging applications have shown great potential. However, very few diagnostic imaging models have been approved for hospital use and even fewer are being implemented at the hospitals. The most common reason why software projects fail is poor requirement engineering, especially non-functional requirements (NFRs) can be detrimental to a project. Research shows that machine learning professionals struggle to work with NFRs and that there is a need to adapt NFR frameworks to machine learning, AI-based, software. This study uses qualitative methods to interact with key stakeholders to identify which types of NFRs are important for medical imaging applications. The study was done on a single Danish hospital and found that NFRs of type Efficiency, Accuracy, Interoperability, Reliability, Usability, Adaptability, and Fairness were important to the stakeholders. Especially Efficiency since the diagnostic imaging department is trying to spend as little time as possible on each scan.","Nowadays our healthcare system relies heavily on imaging data to diagnose and treat patients: 90% of all healthcare data is imaging data [14]. All this data must be analyzed and is in most cases done by radiologists. However, the increase in imaging data has overtaken the number of radiologists. The results can be missed findings and long turn-around times which will jeopardize the patients’ safety [14]. In Denmark it is especially the guarantee of treatment of cancer that takes up many of the radiologists’ resources and because the investigation of most diseases today has evolved to require scans [4]. There is a potential for deep learning models to relieve some of the pressure at the hospitals because they, like radiologists, can learn to recognize patterns. For this project, I will focus on computer-aided detection (CADe) and diagnosis (CADx) medical imaging applications. These technologies are used to localize and classify entities of medical scans [14] and have the potential to lessen workload the workload of radiologists as well as improve diagnosing of patients. I-A Current State of Medical Imaging As of 2022, 521 AI-enabled medical devices have been approved by the FDA (Food and Drug Administration in the US). Out of those, 391 devices are in radiology [3]. This number includes all kinds of models and algorithms that are based on AI, consequently, we do not know how many of those are medical imaging applications. The MONAI lab (Medical Open Network for Artificial Intelligence) published a report in 2021 that highlights that very few, if any, of the developed medical imaging applications have been implemented in hospitals [14]. They point towards the integration of models in the clinical workflow as one of the biggest challenges for successful implementation [14]. Chan et al. emphasize that many of the radiologist’s tasks are too complex for current deep learning models, for example comparing two patient scans to detect changes [2]. Liu et al. highlight that most medical imaging studies do not validate their results externally or compare their performance to radiologists [8]. Varoquaux and Cheplygina document how research in the area is guided by data set availability rather than clinical relevance and how this will lead to diminishing returns when continuing the research in medical imaging [13]. Given the circumstances, there seems to be a lack of focus on the domain which might jeopardize further development in the field and implementation of the models in the hospitals. I-B The Importance of Non-Functional Requirements When looking at traditional software projects, it is well-documented that many software development projects fail. The CHAOS report from 2015 presents that the percentage of failed and challenged projects is still high. In 2015 only 29% of the projects in their database were considered successful [1]. Many project failures can be directly linked to poor requirements gathering, analysis, and management [10]. This can for example occur when users are not involved at all or only at the beginning of the process or when there is miscommunication between the client and the development team [10]. Like it is for traditional software projects, Requirement Engineering (RE) should be a key process in the development of medical imaging applications because it ensures that the developers understand the client and user needs and thereby gain domain focus in the development process. RE is a discipline within Software Engineering (SE) where the goal is to develop requirements for a software system that describe what the system should provide and its constraints [11, Chapter 4]. There are two types of requirements: functional and non-functional requirements (NFR). Functional requirements describe the behavior of the system, NFRs are constraints on the system [11, Chapter 4]. According to Sommerville, it can be detrimental to a system if an NFR is not being met, for example, if an aircraft system does not meet reliability requirements, it is not safe and will not be used [11, Chapter 4]. Also, the NFRs may affect the overall architecture of the system which can make them expensive and hard to meet. Therefore, it is important to put great emphasis on NFRs in the RE process because of their significant effect on the system in question. I-C Non-Functional Requirements in Medical Imaging Habibullah et al. recently published a study where they interviewed Machine Learning (ML) professionals about their current use of NFRs. They found that most interviewees struggled with defining and measuring NFRs for ML systems and that some NFRs from the development of traditional software need to be redefined to be applicable or relevant for ML models [6]. This suggests that there are also challenges ahead for those medical imaging developers that want to work with defining NFRs because the framework has not been adapted properly to the type of software they are developing. The goal of this research project is to take the first step to create a framework that can help medical imaging developers implement NFRs in their application, to ensure that in the future, more medical imaging models will be implemented in the hospital to lessen the workload of the radiologists."
https://arxiv.org/html/2411.09709v1,Feature Selection via Dynamic Graph–based Attention Block in MI–based EEG Signals,"Brain–computer interface (BCI) technology enables direct interaction between humans and computers by analyzing brain signals. Electroencephalogram (EEG) is one of the non–invasive tools used in BCI systems, providing high temporal resolution for real–time applications. However, EEG signals are often affected by a low signal–to–noise ratio, physiological artifacts, and individual variability, representing challenges in extracting distinct features. Also, motor imagery (MI)–based EEG signals could contain features with low correlation to MI characteristics, which might cause the weights of the deep model to become biased towards those features. To address these problems, we proposed the end–to–end deep preprocessing method that effectively enhances MI characteristics while attenuating features with low correlation to MI characteristics. The proposed method consisted of the temporal, spatial, graph, and similarity blocks to preprocess MI–based EEG signals, aiming to extract more discriminative features and improve the robustness. We evaluated the proposed method using the public dataset 2a of BCI Competition IV to compare the performances when integrating the proposed method into the conventional models, including the DeepConvNet, the M–ShallowConvNet, and the EEGNet. The experimental results showed that the proposed method could achieve the improved performances and lead to more clustered feature distributions of MI tasks. Hence, we demonstrated that our proposed method could enhance discriminative features related to MI characteristics.","I INTRODUCTION Brain–computer interface (BCI) is a system that facilitates direct interaction between humans and computers [1, 2]. This technology reclaims a novel way in neuroscience research by providing the approach to analyze brain signals associated with the activation of specific brain regions [3]. In addition, electroencephalogram (EEG) is one of the non–invasive techniques for monitoring complex brain dynamics without the need for surgery [4, 5]. EEG–based BCI systems have advantages including low–cost and high portability, which are promising for a wide range of applications such as a speller [6], a robotic arm [7], a wheelchair [8], or a drone [9]. The high temporal resolution of EEG signals enables real–time reflection of the human’s cognitive state and intentions. This modality exhibit characteristics that are especially advantageous for various BCI paradigms, such as motor imagery (MI) [10]. MI is a useful paradigm for controlling external electronic devices without physical movement [11]. It is receiving more attention in the fields of rehabilitation and assistive technology [12]. However, EEG signals suffer from a low signal–to–noise ratio and various physiological artifacts, making it difficult to extract distinct features and analyze brain signals [13]. Furthermore, EEG signals exhibit inherent variability, with more pronounced differences observed across individuals [14]. To solve these problems, a large number of methodologies have been developed in recent years. Among them, the convolutional deep architectures are commonly employed as feature extractors to capture the significant features from MI tasks. Barmpas et al. [15] proposed the dynamic convolutional approach based on causal reasoning to mitigate the inter–subject variability in decoding MI–based EEG signals, resulting in up to 5 % improvement in generalization across subjects. Tang et al. [16] focused on improving the classification performances of EEG signals in MI domain, which extracts various temporal and spatial features by the multi–scale convolutional framework, achieving the average accuracy of 96.87 %. The convolution–based methods have shown impressive results in MI domain. However, EEG signals could contain features with low correlation to MI characteristics. Since the convolutional operation is limited to local receptive fields, a potential challenge arises in which weights of the deep model could become biased towards features with low correlation to MI characteristics. In this paper, we proposed the end–to–end deep preprocessing method that enhances the relevant MI characteristics while attenuating those with low correlation to MI characteristics. It was designed to be robustly integrated into the initial layers of the convolution–based methods. To extract the discriminative features for each MI characteristic, our proposed method was composed of the temporal and spatial convolutional layers. Additionally, we introduced the graph–based convolutional operation between two layers to consider the relationships across electrodes. By leveraging the implemented structure, we verified the effectiveness of the proposed method by integrating it with the baseline models. It was observed to be effectively preprocessed prior to being used as input for the baseline models. To the best of our knowledge, this is the first attempt to enhance discriminative features using dynamic graph–based attention block in MI–based EEG signals. Figure 1: Visualization of the overall process in the proposed method. EEG signals of rest and MI are used as input simultaneously. The temporal, graph, and spatial blocks extract the significant features to effectively calculate the negative similarity. The similarity block contains a processing step that generates the value \textit{v}_{i} to enhance MI features while attenuating rest features. (D: the dimension of the feature vector, R: the number of time steps compressed from rest signals, and M: the number of time steps compressed from MI signals)."
https://arxiv.org/html/2411.09355v1,"Prices, Bids, Values: Everything, Everywhere, All at Once","We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders to maximize efficiency. The SOTA ML-based algorithms elicit bidders’ preferences via value queries (i.e., “What is your value for the bundle \{A,B\}?”). However, the most popular iterative combinatorial auction in practice elicits information via more practical demand queries (i.e., “At prices p, what is your most preferred bundle of items?”). In this paper, we examine the advantages of value and demand queries from both an auction design and an ML perspective. We propose a novel ML algorithm that provably integrates the full information from both query types. As suggested by our theoretical analysis, our experimental results verify that combining demand and value queries results in significantly better learning performance. Building on these insights, we present MLHCA, the most efficient ICA ever designed. MLHCA substantially outperforms the previous SOTA in realistic auction settings, delivering large efficiency gains. Compared to the previous SOTA, MLHCA reduces efficiency loss by up to a factor of 10, and in the most challenging and realistic domain, MLHCA outperforms the previous SOTA using 30% fewer queries. Thus, MLHCA achieves efficiency improvements that translate to welfare gains of hundreds of millions of USD, while also reducing the cognitive load on the bidders, establishing a new benchmark both for practicability and for economic impact.","Combinatorial auctions (CAs) are used to allocate multiple items among several bidders who may view those items as complements or substitutes. In a CA, bidders are can submit bids for whole bundles/packages of items. CAs have enjoyed widespread adoption in practice, with their applications ranging from allocating spectrum licences (Cramton, 2013) to TV ad slots (Goetzendorff et al., 2015) and airport landing/take-off slots (Rassenti et al., 1982). The key challenge in CAs is that the bundle space grows exponentially in the number of items, making it impossible for bidders to report their full value function in all but the smallest domains. Moreover, Nisan & Segal (2006) showed that for arbitrary value functions, CAs require an exponential number of bids in order to guarantee full efficiency. Thus, practical CA mechanisms cannot provide efficiency guarantees in real world settings with more than a modest number of items. Instead, the focus has shifted towards iterative combinatorial auctions (ICAs), where bidders interact with the auctioneer over a series of rounds, providing only a limited (i.e., practically feasible) amount of information, with the aim to maximize the efficiency of the final allocation. The most established ICA following this interaction paradigm is the combinatorial clock auction (CCA) (Ausubel et al., 2006). The CCA has been extensively used for allocating spectrum licenses, generating over USD 20 billion in revenue between 2012 and 2014 alone (Ausubel & Baranov, 2017). Speed of convergence is a critical consideration for any ICA since each round entails costly computations and business modelling for the bidders (Kwasnica et al., 2005; Milgrom & Segal, 2017; Bichler et al., 2017). Large spectrum auctions following the CCA format can take more than 100 bidding rounds. In order to decrease the number of rounds, many CAs in practice use aggressive price update rules (e.g., increasing prices by up to 10% each round), which can harm efficiency (Ausubel & Baranov, 2017). Thus, it remains a challenging problem to design a practical ICA that is efficient and converges in a small number of rounds. Specifically, given the value of resources allocated in such real-world ICAs, increasing their efficiency by even one percentage point already translates into welfare gains of hundreds of millions of dollars. 1.1 ML-Powered Iterative Combinatorial Auctions To address this challenge, researchers have proposed various ways of using machine learning (ML) to improve the efficiency of ICAs. The seminal works by Blum et al. (2004) and Lahaie & Parkes (2004) were the first to frame preference elicitation in CAs as a learning problem. In more recent years, Brero et al. (2018; 2021), Weissteiner & Seuken (2020); Weissteiner et al. (2022b; a; 2023) proposed ML-powered ICAs. At the heart of those approaches lies an ML-powered preference elicitation algorithm that uses an ML model to learn each bidder’s value function to generate an informative value query (i.e., “What is your value for the bundle \{A,B\}?”), which in turn refines that bidder’s ML model.111From an optimization task perspective this setting can be viewed as a combinatorial Bayesian optimization problem. While those value-query based ML-powered ICAs lead to significant efficiency gains redefining the state-of-the-art (SOTA) efficiency results in many realistic auction domains, those approaches suffer from one common practical limitation: they fundamentally rely throughout the whole ICA on value queries (VQs). Prior research in auction design has identified demand queries (DQs) as the best way to run an auction (Cramton, 2013). Their advantages compared to value queries include elimination of tacit collusion and bid signaling, as well as simplified bidder decision-making that keeps the bidders focused on what is most relevant: the relationship between prices and aggregate demand. Additionally, value queries are cognitively complex, and thus typically should be only used sparsely in real-world ICAs. For these reasons, DQs are the most prominent interaction paradigm for auctions in practice. Following this rationale, Soumalias et al. (2024b) addressed the common limitation of prior work by designing the first practical ML-powered ICA that elicits information from bidders via DQs instead of VQs and only makes use of VQs in supplementary rounds, when bidders have already obtained a clearer picture on which bundles they can realistically hope to clinch and how much they should approximately value such bundles. While this DQ-based ICA represented a significant leap towards making ML-powered ICAs practical and at the same time outperformed the baseline CCA that is typically used in real-world applications, it still suffered from the following two important deficiencies: First, it could not reach the SOTA efficiency of the impractical VQ-based ML-powered ICAs. Second, to improve efficiency, just like the CCA, it required the use of a supplementary round, in which the bidders must decide on which additional value bids to submit to the mechanism, a cognitive complicated task for the bidders. The present paper closes these two last gaps in the realm of ICAs by designing a hybrid ML-powered ICA that combines DQ-based rounds with a sophisticated yet practical VQ-based supplementary round. Importantly, this hybrid ML-powered ICA clearly outperforms the previous SOTA ICA while still being practical in real-world applications. 1.2 Our Contributions In this paper, we introduce the Machine Learning-powered Hybrid Combinatorial Auction (MLHCA), a practical ICA that achieves unprecedented efficiency. Our contributions are as follows: 1. In Section 3, we provide a theoretical foundation and illustrative examples that demonstrate the advantages and limitations of DQs and VQs as input mechanisms for auctions and learning algorithms. 2. In Section 4, we introduce a learning algorithm capable of leveraging both types of queries. We provide strong experimental evidence of the learning benefits of combining both query types, as well as the advantages of starting an auction with DQs instead of VQs. 3. In Section 5 we combine our auction and ML insights to develop MLHCA, the first ICA to incorporate both sophisticated DQ and VQ generation algorithm. Simulations in realistic domains show that MLHCA significantly outperforms the previous SOTA, achieving higher efficiency with 40% fewer queries (Section 6), setting a new benchmark for both efficiency and practicality. 1.3 Further Related work In the field of automated mechanism design, Dütting et al. (2015; 2019), Golowich et al. (2018) and Narasimhan et al. (2016) used ML to learn new mechanisms from data, while Cole & Roughgarden (2014); Morgenstern & Roughgarden (2015) and Balcan et al. (2023) bounded the sample complexity of learning approximately optimal mechanisms. In contrast to this prior work, our design incorporates an ML algorithm into the mechanism itself, i.e., the ML algorithm is part of the mechanism. Lahaie & Lubin (2019) suggest an adaptive price update rule that increases price expressivity as the rounds progress in order to improve efficiency and speed of convergence. Unlike that work, we aim to improve preference elicitation in the main rounds while still using linear prices. Preference elicitation is a key market design challenge outside of CAs too. Soumalias et al. (2024a) introduce an ML-powered mechanism for course allocation that improves preference elicitation by asking students comparison queries. Despite the prominence of DQs in real-world applications, the only prior work apart from Soumalias et al. (2024b) on ML-based DQs that we are aware of is that of Brero & Lahaie (2018) and Brero et al. (2019), who proposed integrating ML in a price-based ICA to generate the next price vector in order to achieve faster convergence. However, this prior work does not exploit any notion of similarity between bundles that contain overlapping items, only incorporates a fraction of the information revealed by the agents’ bidding (i.e., for the bundle an agent bids on, her value for that bundle must be larger than its price), and is computationally intractable already in medium-sized auction domains. See Appendix D for further related work. 1.4 Practical Considerations and Incentives MLHCA integrates both ML-powered DQ and VQ rounds. In DQ-based auctions like the CCA or ML-CCA, ensuring truthful bidding depends heavily on well-chosen activity rules and payment rules. In Section A.3, we provide a detailed discussion of the most common activity rules used in the CCA to align incentives, and detail how MLHCA can also leverage these rules to achieve the same goal. The VQ rounds in MLHCA extend the MLCA framework (Brero et al., 2021) by incorporating information from earlier DQ rounds into bidders’ ML models. Brero et al. (2021) argued that MLCA offers strong practical incentives, and under two additional assumptions, truthful bidding is an ex-post Nash equilibrium. In Section A.4 we provide a detailed discussion of these arguments, and detail why they also apply to MLHCA’s VQ rounds. By effectively combining activity rules in the DQ rounds and leveraging the established incentive structure of MLCA in the VQ rounds, MLHCA achieves a robust incentive alignment across all its stages."
https://arxiv.org/html/2411.09689v1,LLM Hallucination Reasoning with Zero-shot Knowledge Test,"LLM hallucination, where LLMs occasionally generate unfaithful text, poses significant challenges for their practical applications. Most existing detection methods rely on external knowledge, LLM fine-tuning, or hallucination-labeled datasets, and they do not distinguish between different types of hallucinations, which are crucial for improving detection performance. We introduce a new task, Hallucination Reasoning, which classifies LLM-generated text into one of three categories: aligned, misaligned, and fabricated. Our novel zero-shot method assesses whether LLM has enough knowledge about a given prompt and text. Our experiments conducted on new datasets demonstrate the effectiveness of our method in hallucination reasoning and underscore its importance for enhancing detection performance.","Large language models (LLMs) have shown remarkable ability in generating text on various topics [35, 32]. However, they often produce hallucinations — incorrect or unverifiable content — that pose significant risks to their practical applications [2]. Detecting these hallucinations is crucial for ensuring reliability [12] yet challenging due to the plausible appearance of the hallucinated text [37]. Research on detecting hallucinations in LLM-generated text has explored several approaches, including comparing the text with external knowledge [19, 25, 31], fine-tuning LLMs [40, 36, 18], and training classifiers to identify hallucinations [1, 4, 30]. However, these methods require external knowledge, LLM fine-tuning, or supervised training with hallucination-labeled data. To address these limitations, there has been growing interest in source-free, zero-shot methods that analyze LLM outputs directly. These methods encompass consistency checks [22], uncertainty estimation [42, 5, 16, 3, 38], and prompting LLMs to assess the correctness of the text [6, 43]. However, existing detection methods fail to distinguish between different types and causes of hallucinations [44, 10], which is crucial for accurately detecting and resolving them. To be specific, LLM-prompting methods may randomly guess the correctness of text when the LLM lacks relevant knowledge, while most uncertainty-based methods cannot identify errors caused by the inherent randomness of the LLM [29, 7]. Differentiating the underlying causes of hallucinations enables more accurate detection and can even suggest potential solutions: if the LLM lacks knowledge, external knowledge can be provided; otherwise, responses can simply be regenerated. To fill this gap, we categorize LLM-generated text into three types: aligned, misaligned, and fabricated (Table 1). Misaligned text arises from sampling randomness or dependencies on previous tokens [29, 7, 41], while fabricated text is generated when the LLM lacks relevant knowledge [10, 44]. Based on this categorization, we propose a new task, hallucination reasoning, which aims to classify LLM-generated text into one of these three types. We contribute: • New hallucination reasoning task for better understanding and detection of hallucinations (Sec. 3, Table 1). Our dataset creation process can be leveraged for future research in hallucination reasoning (Sec. 4). • MKT, a novel zero-shot method that identifies whether an LLM has enough knowledge about a prompt and text without any requirements for external knowledge, labeled datasets, and LLM fine-tuning (Sec. 3.1, Fig. 1). • Experiments that demonstrate the superiority of our approach in both QA and free-form text generation. Incorporating our method into existing detection algorithms significantly improves their performance, underscoring the importance of hallucination reasoning (Sec. 4)."
https://arxiv.org/html/2411.09648v1,Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable Medical Information,"This paper introduces Med-Bot, an AI-powered chatbot designed to provide users with accurate and reliable medical information. Utilizing advanced libraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq, Med-Bot is built to handle the complexities of natural language understanding in a healthcare context. The integration of llama-assisted data processing and AutoGPT-Q provides enhanced performance in processing and responding to queries based on PDFs of medical literature, ensuring that users receive precise and trustworthy information. This research details the methodologies employed in developing Med-Bot and evaluates its effectiveness in disseminating healthcare information.","The integration of artificial intelligence (AI) into healthcare has catalyzed a transformative shift in how medical services are delivered, with medical chatbots emerging as a prominent innovation. These chatbots leverage AI and natural language processing (NLP) technologies to offer users a range of healthcare-related services, from providing medical information to assisting with diagnostics and treatment suggestions. This evolution addresses the growing demand for accessible healthcare solutions amid the shortage of medical professionals and the increasing complexity of patient needs. Medical chatbots are designed to enhance patient engagement by offering timely, accurate, and personalized responses. Their ability to interact with users in natural language, coupled with advanced algorithms for understanding and processing medical queries, allows them to simulate the experience of consulting with a healthcare provider. This capability not only improves patient accessibility to medical information but also supports healthcare professionals by streamlining routine inquiries and administrative tasks. As the field continues to advance, various methodologies and technologies are being explored to enhance the effectiveness of these chatbots. Researchers are focusing on improving the accuracy of diagnostics, the relevance of information provided, and the overall user experience. Innovations such as context-aware processing and advanced machine learning algorithms are playing crucial roles in refining these systems. In this evolving landscape, our research seeks to push the boundaries further by employing cutting-edge techniques to enhance the capabilities of medical chatbots. By integrating state-of-the-art technologies and methodologies, we aim to address existing limitations and provide a more robust, adaptive, and reliable solution for healthcare assistance."
https://arxiv.org/html/2411.09601v1,Accelerating Knowledge Graph and Ontology Engineering with Large Language Models,"Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance.","Knowledge Graph and Ontology Engineering (KGOE, in short) refers to a (vaguely defined) set of tasks that are of central relevance to the life cycle of knowledge graphs, and of ontologies,111We mostly understand ontologies as a type of schema for knowledge graphs in the sense of [45], and as such the ontology can in fact also be understood to be part of the knowledge graph. We acknowledge other uses, to which our discussion also applies. as data artifacts used in data management and applications.222For background on this and the more general Semantic Web field, see, e.g., [20] and the references given therein. These include, for example, ontology modeling (i.e., construction), ontology population (i.e., creating a knowledge graph with the given ontology as schema), ontology extension and modification, ontology alignment, entity disambiguation (sometimes called co-reference resolution). All of the just mentioned tasks have in common that they are hard, in the sense that even after more than a quarter century of Semantic Web research, they still defy attempts to automate them at reasonable quality levels and scale. The state of the art on all of these is that they require significant human expert labor, at times (such as for entity disambiguation) together with detailed scripting of algorithms that solve the problem at scale but only for a specific problem instance, i.e., for a very specific knowledge graph and/or ontology. At the same time, knowledge graphs and ontologies are ever more important for applications in data integration and data management, and more recently also as ground truth to escape from Large Language Model (LLM) hallucinations [29] and as components of other neurosymbolic approaches [21, 23, 24]. As a consequence, improved processes and methods for automating or even semi-automating core KGOE tasks remains a key challenge for the research community. LLMs enter the scene, and the public perception of Artificial Intelligence (AI), with force in 2022 at the launch of OpenAI’s ChatGPT,333https://openai.com/index/hello-gpt-4o/ with rapid developments since then. Their human-style conversation capabilities, which include a profound mastery of expression in written language, as well as solid production of more structured information, are as stunning as their sometimes wildly confabulated responses (usually termed hallucinations). Perhaps most important for our discussion herein, LLMs appear to capture, and to have the ability to recall in different formats and contexts, a wide swath of human knowledge, both commonsense and specialized, provided it is reflected well enough in the training data. While at this point in time, their reliability in terms of accuracy of content in their responses remains problematic, it is quite apparent and widely reported that working with an LLM can save significant time and effort provided there is a (human) topic expert available as a check on factual accuracy. The promise of LLMs for KGOE is thus: by using LLMs as approximate natural language knowledge bases that can be approximately queried, plus LLM capabilities to understand and produce information in ways structured for KGOE use, it should be possible to design semi-automatic methods, or human-LLM interactive methods, that can produce at least draft solutions for key KGOE problems at a level of quality that will significantly reduce human expert time and effort. Work on this line of research has of course already been started by the Semantic Web community. With this paper, we intend to begin to consolidate the discussion, and – in particular – contribute observations and discussions related to our own research on modular ontologies, which we believe to be highly relevant, for reasons that we will lay out below. The plan of this paper is as follows. In Section 2 we motivate the need for (a certain type of) modularity for LLM-based KGOE. In Section 3 we discuss our notion of modularity and its research context. In Section 4 we briefly look, in turn, at the key KGOE tasks identified in the introduction. In Section 5 we will list some concrete research challenges that can drive LLM-based KGOE forward, and in Section 6 we will conclude."
https://arxiv.org/html/2411.09576v1,Automating Reformulation of Essence Specifications via Graph Rewriting,"Formulating an effective constraint model of a parameterised problem class is crucial to the efficiency with which instances of the class can subsequently be solved. It is difficult to know beforehand which of a set of candidate models will perform best in practice. This paper presents a system that employs graph rewriting to reformulate an input model for improved performance automatically. By situating our work in the Essence abstract constraint specification language, we can use the structure in its high level variable types to trigger rewrites directly. We implement our system via rewrite rules expressed in the Graph Programs 2 language, applied to the abstract syntax tree of an input specification. We show how to automatically translate the solution of the reformulated problem into a solution of the original problem for verification and presentation. We demonstrate the efficacy of our system with a detailed case study.","Formulating an effective constraint model of a problem of interest is crucial to the efficiency with which the problem can subsequently be solved [6]. It is difficult to know beforehand which of a set of candidate models will perform best in practice. In this paper we present our work in progress on a system that reformulates an input model to improve performance automatically. It differs from some earlier work on automated model reformulation, such as CGRASS [8], Tailor [9], the work of Bessiere et al. [2] to learn implied global constraints, or Savile Row [16], in that it reformulates a model of a parameterised problem class rather than individual problem instances. This has the advantage that the effort made to reformulate the model is amortised over all the instances of the class that are to be solved, rather than having to be paid back during the solution of a single instance. Furthermore, by implementing reformulation as forward-chaining sound rewrite rules, we avoid the need for external verification in existing class-based reformulation [5, 4, 14]. Here we build on our previous proposal for a system [15], demonstrating how to apply graph transformation techniques [21] to achieve automated rewriting of Essence specifications. In contrast to recent work by Leo et al. [13], who reformulate a low level model of a problem class, we situate our work in the Essence abstract constraint specification language [7]. The advantage of this approach is that the structure apparent in a concise abstract specification can be used to trigger and guide reformulation. For example, LABEL:lst:before presents the k-fold colouring problem we will use in our case study. Here the single abstract decision variable is a binary relation. Having that information directly in the variable type, as opposed to reconstructing it from a constraint model-level representation likely composed of a constrained matrix of more primitive decision variables, is a significant aid to reformulation. Furthermore, a single reformulated specification can be refined into a variety of both models and solving paradigms, allowing us to gain a fuller picture of performance. We implement our system via rewrite rules expressed in the Graph Programs 2 language [3, 18], applied to the abstract syntax tree of an input specification. Given the small size of Essence specifications and the efficiency of the GP2 system in applying graph rewriting rules, the rewriting process has negligible cost. When the type of the decision variable is transformed, the solution must be converted to the original type for verification and presentation to the user. A constraint solver can be employed to solve the solution re-assignment specification straightforwardly. We demonstrate this process even in the case of the nested types supported by Essence. Our work makes the following primary contributions: • Automated class-level model reformulation via a library of graph transformation rewrite rules. • Inverse rewriting of solutions for presentation and verification. • A case study using k-fold graph colouring. Listing 1: Essence specification of k-fold graph colouring, a variant in which each node in a given graph must be assigned k colours with no pair of nodes connected by an edge sharing a colour. ⬇ $ k-fold graph colouring with k=coloursPerNode, out of numberColours given n : int letting vertices be domain int(0..n-1) given edges : relation (irreflexive) of ( vertices * vertices ) given numberColours : int(1..) given coloursPerNode : int(1..) letting colours be domain int(1..numberColours) find colouring : relation (size n*coloursPerNode) of (vertices * colours) such that $ endpoints of edges do not share colours forAll (u,v) in edges . (forAll colourAssignment in colouring . (colourAssignment[1] = u) -> !((v,colourAssignment[2]) in colouring)), $ enforce number of colours per node forAll u : vertices . coloursPerNode = (sum colourAssignment in colouring . toInt(colourAssignment[1] = u))"
https://arxiv.org/html/2411.09523v1,"Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents","With the continuous development of large language models (LLMs), transformer-based models have made groundbreaking advances in numerous natural language processing (NLP) tasks, leading to the emergence of a series of agents that use LLMs as their control hub. While LLMs have achieved success in various tasks, they face numerous security and privacy threats, which become even more severe in the agent scenarios. To enhance the reliability of LLM-based applications, a range of research has emerged to assess and mitigate these risks from different perspectives.To help researchers gain a comprehensive understanding of various risks, this survey collects and analyzes the different threats faced by these agents. To address the challenges posed by previous taxonomies in handling cross-module and cross-stage threats, we propose a novel taxonomy framework based on the sources and impacts. Additionally, we identify six key features of LLM-based agents, based on which we summarize the current research progress and analyze their limitations. Subsequently, we select four representative agents as case studies to analyze the risks they may face in practical use. Finally, based on the aforementioned analyses, we propose future research directions from the perspectives of data, methodology, and policy, respectively.","With the continuous development of language models (LMs), LLMs based on the transformer architecture (Vaswani et al., 2017) have achieved significant success in various fields of NLP (Devlin, 2018; Radford et al., 2019). The massive number of parameters and extensive training data endow LLMs with strong capabilities in tasks like text generation (Touvron et al., 2023; Brown et al., 2020), code assistance (Feng et al., 2020; Wang et al., 2021), logical reasoning (Wei et al., 2022; Yao et al., 2024), etc. Due to their powerful understanding capabilities, an increasing number of studies are positioning LLMs as the core decision-making hub of AI agents (Nakano et al., 2021; Shen et al., 2024c), which are sophisticated software programs designed to autonomously perform tasks on behalf of users or other systems. Compared to earlier AI agents based on heuristic algorithms or reinforcement learning (Mnih et al., 2015; Lillicrap et al., 2015), LLM-based agents can communicate with users, making them easier to understand and accept. Additionally, their vast foundational knowledge allows them to think in a manner similar to humans (understanding + planning). These characteristics contribute to their popularity, making them a promising direction for AI to serve various practical fields (Zhang et al., 2024e; Wang et al., 2023d; Zeng et al., 2024a). For example, Supertools (Supertools, [n. d.]) is a comprehensive collection of trending applications empowered by LLMs. Despite the significant success of LLMs, they also face security and privacy threats due to inner vulnerabilities or outer attacks. LLM-based agents add some components and functionalities, which makes these risks even more threatening. For example, LLMs face jailbreaking attacks (Shen et al., 2024a; Liu et al., 2023e), which refer to the process to bypass their built-in safety mechanisms (Bai et al., 2022; Ouyang et al., 2022). In the context of LLM-based agents, LLMs need to handle multi-round dialogues and multiple sources of information, making jailbreaking attacks more complex and difficult to defend against (Anil et al., 2024; Cheng et al., 2024). To uncover the vulnerabilities of LLM-based agents and make them more secure and reliable, an increasing number of studies focus on the threats from various perspectives. To help researchers better understand LLM-based agents and pursue future research work, there exist two surveys (Deng et al., 2024a; Cui et al., 2024) to summarize the security risks of LLM-based agents. They categorize the security risks based on the composition (called modules) or operational phases (called stages) of the agents as follows. (i) The module perspectives. Cui et al. (Cui et al., 2024) identified four key modules in LLM-based systems, i.e., the input module, the LM module, the toolchain module, and the output module. They summarize the risks of LLM-based agents based on the four modules. (ii) The stage perspectives. Deng et al. (Deng et al., 2024a) identified four key knowledge gaps in LLM-based AI agents, i.e., the stage of perception, the stage of internal execution, the stage of action in environment, and the stage of interaction with untrusted external entities. They summarize the risks of LLM-based agents based on the four stages. These two taxonomies clearly highlight the sources of attacks faced by LLM-based agents. However, they struggle to accurately pinpoint threats that span across modules and stages. For example, privacy leakage is caused by memory issues within the language model module, but it occurs at the output module. Similarly, goal hijacking can happen not only during the perception stage but also during the interaction stage with external data (Abdelnabi et al., 2024). These cross-module and cross-stage threats are inaccurately pinpointed to a single module or stage. Figure 1. The overall framework of our taxonomy for the risks of LLM-based agents. Our Design. To categorize various threats of LLM-based agents more accurately and comprehensively, we propose a novel taxonomy by mapping the threats into a binary table based on their sources and types. (i) For the sources of threats, we consider the operational nature of LLM-based agents: LLMs make decisions based on inputs from multiple sources, as shown in Fig. 1 left. As a probabilistic model, the output decision distribution of an LLM is determined by both the input and the model itself. Therefore, we attribute the threats to LLM-based agents to the inputs, the model, or a combination of both. Compared to categorizing attacks by modules or stages, our classification of sources is closer to the essence of the threat. For example, goal hijacking (Qiang et al., 2023; Huang et al., 2024b; Liu et al., 2024c) may originate from a user input or an external database, but both fundamentally act as inputs to the model for hijacking the goal. (ii) For the types of threats, we categorize the threats into three classes: security/safety, privacy, and ethics. Specifically, if a threat results in the model producing incorrect outputs (including errors that are factually inaccurate or do not align with the needs of developers or users), it is categorized as a security/safety issue, such as adversarial examples (Yin et al., 2024). If a threat leads to the leakage of privacy, it is classified as a privacy issue, such as prompt leakage attacks (Yu et al., 2023d; Zhang et al., 2024b). If a threat does not produce “incorrect” outputs but raises concerns such as unfairness, it falls under ethical issues, such as bias (Gallegos et al., 2024). We collect papers from the top conferences and highly cited arXiv papers. Top conferences are included but not limited: IEEE S&P, ACM CCS, USENIX Security, NDSS, ACL, CVPR, NIPS, ICML, and ICLR. We categorize different kinds of threats with our taxonomy in Fig. 1 right. For threats originating from inputs, we refer to them as problematic inputs. In this scenario, attackers cannot modify the model but can design inputs to induce malicious outputs or behaviors, e.g., the adversarial example. For threats from within the model, we refer to them as model flaws. In this scenario, the inputs are always benign, but the model’s own defects lead to malicious outputs or behaviors, e.g., the hallucination problem. For threats arising from both model flaws and carefully crafted inputs, we refer to them as combined threats. In this scenario, the inputs are deliberately designed by attackers to exploit the model’s vulnerabilities, e.g., the backdoor attack. Figure 2. An overall framework of LLM-based agents. Our Contributions. Compared with recent surveys (Deng et al., 2024a; Cui et al., 2024) on the security risks of LLM-based agents, there are three main advantages of our work. (i) A Novel Taxonomy of Threats. We propose a novel taxonomy that maps threats into a binary table based on their sources and impacts, which can comprehensively cover the existing threats and extend to future threats, including the cross-module and cross-stage threats. (ii) Detailed Analysis of Multi-modal Large Language Models (MLLMs). Many tasks require agents to handle inputs from multiple modalities, (e.g., city navigation systems (Zeng et al., 2024a)), leading to the emergence of a range of MLLMs and agents based on these models (Zeng et al., 2024a; Yang et al., 2023a). Previous surveys primarily focus on the text modality, lacking analysis of multimodal models. We cover both LLMs and MLLMs, placing particular emphasis on analyzing the new challenges and threats posed by multimodal tasks in the context of threats. (iii) Four Carefully Selected Case Studies. Previous surveys analyze the risks based on a general framework of LLM-based agents (or systems). However, actual agents may not necessarily contain all modules in the general framework, and the designs within these modules may also be customized (Dong et al., 2023). More importantly, the scenarios they face have significant differences, resulting in the varying levels and causes of threats. To help readers better understand the actual threats faced by agents, we present case studies of four different agents, representing four classic situations in Section 6. This paper is organized as follows. Section 2 introduces a general framework of LLM-based agents and identifies six key features of the framework. Sections 3, 4, and 5 depict the risks from problematic inputs, model flaws, and input-model interaction, respectively. Section 6 offers four carefully selected case studies. Section 7 gives future directions for the development of this field."
https://arxiv.org/html/2411.09356v1,Multi-scale Generative Modeling for Fast Sampling,"While working within the spatial domain can pose problems associated with ill-conditioned scores caused by power-law decay, recent advances in diffusion-based generative models have shown that transitioning to the wavelet domain offers a promising alternative. However, within the wavelet domain, we encounter unique challenges, especially the sparse representation of high-frequency coefficients, which deviates significantly from the Gaussian assumptions in the diffusion process. To this end, we propose a multi-scale generative modeling in the wavelet domain that employs distinct strategies for handling low and high-frequency bands. In the wavelet domain, we apply score-based generative modeling with well-conditioned scores for low-frequency bands, while utilizing a multi-scale generative adversarial learning for high-frequency bands. As supported by the theoretical analysis and experimental results, our model significantly improve performance and reduce the number of trainable parameters, sampling steps, and time. The source code is available at https://anonymous.4open.science/r/WMGM-3C47.","Generative models (GMs) have revolutionized the machine learning field by their ability to create novel, high-quality data that closely mimic real-world distributions. Many GM frameworks have been proposed, such as variational autoencoders (VAE) [2], recurrent neural networks (RNNs) [22], generative adversarial networks (GANs) [14], and hybrid approaches (i.e., combining strategies). Such models have been applied to generate high-quality audio waveforms or speech [33], constructing natural-looking images [14, 4, 26], generating coherent text [3], and designing molecules [13]. Score-based generative models (SGMs) [43, 21, 42], also known as denoising diffusion models, encode the probability distributions through a scoring approach (e.g., a vector field that points in the direction of increasing likelihood of data) and recover the actual data distribution through a learnable reverse process to the forward Gaussian diffusion process. Although these SGMs have been highly successful, many physical, chemical, biological, and engineering systems have complex multiscale and non-Gaussian properties, leading to ill-conditioned scores [19]. To make the discussion more concrete, let us consider the noise-adding process in the frequency domain (i.e., wavelet domain), where noise contains a uniform power spectrum in each frequency band. However, in the wavelet domain, the high-frequency coefficients of natural images are sparse and contain minimal energy, while the low-frequency coefficients encapsulate most of the energy, a distribution characteristic that mirrors the power law decay of the power spectrum of natural images. Given the disparity between image and noise power spectra, low-frequency components, which hold the majority of the energy, receive the same magnitude of noise during the noise addition process, and high-frequency coefficients, despite being sparse, obtain a relatively larger amount of noise. This dynamics, also depicted in Fig. 1, offers inspiration for analyzing diffusion in the frequency domain, incorporating corresponding generative strategies tailored for each frequency sub-domain (namely, the high and low-frequency domains). Some studies [18, 9] highlight the challenges associated with the ill-conditioning properties of the score during diffusion in the spatial domain. They establish a connection between the minimum discretization steps and the power spectrum of the image, thereby rationalizing the application of noise addition in the wavelet domain. Figure 1: Diffusion trajectories of the wavelet coefficients. Notice that the high-frequency components (LH,HL,HH) are overwhelmed by noise at an earlier stage marked by the green line. At the same time, the low-frequency component (LL) degrades more slowly. While the advantages of diffusion within the wavelet domain are evident from both experimental results and intuition, there is a lack of theoretical substantiation for the correctness of general diffusion in the wavelet domain and comprehensive analysis of the properties of each wavelet band. Specifically, considering the sparsity and the non-Gaussian high-dimensional probability distribution functions (HDPDFs) Huang et al. [23], Jiang et al. [25] of high-frequency coefficients, discussion about adapting SGM to high-frequency bands is insufficient. To bridge this gap, our research first highlights that deep-scale low-frequency coefficient scores are well-conditioned, alleviating the ill-conditioning issue by general diffusion models in the spatial domain. Subsequently, we illustrate that there is a duality between SGM in the wavelet domain and the spatial domain. Furthermore, considering high-frequency coefficients’ sparsity and non-Gaussian nature, we introduce the Multi-Scale Adversarial Learning (MSAL). The framework facilitates high-quality generation, data efficiency, and fast sampling. Our contributions: (i) We theoretically establish the generative modeling in the wavelet domain and analyze the distribution of multi-scale wavelet coefficients; (ii) We introduce the multi-scale adversarial learning (MSAL) in the wavelet domain to tackle the highly non-Gaussian distribution of high-frequency wavelet coefficients efficiently; (iii) We demonstrate that the proposed WMGM framework significantly accelerates sampling speed, while also maintaining high-quality image generation."
https://arxiv.org/html/2411.09251v1,Cross Space and Time: A Spatio-Temporal Unitized Model for Traffic Flow Forecasting,"Predicting spatio-temporal traffic flow presents significant challenges due to complex interactions between spatial and temporal factors. Existing approaches often address these dimensions in isolation, neglecting their critical interdependencies. In this paper, we introduce the Spatio-Temporal Unitized Model (STUM), a unified framework designed to capture both spatial and temporal dependencies while addressing spatio-temporal heterogeneity through techniques such as distribution alignment and feature fusion. It also ensures both predictive accuracy and computational efficiency. Central to STUM is the Adaptive Spatio-temporal Unitized Cell (ASTUC), which utilizes low-rank matrices to seamlessly store, update, and interact with space, time, as well as their correlations. Our framework is also modular, allowing it to integrate with various spatio-temporal graph neural networks through components such as backbone models, feature extractors, residual fusion blocks, and predictive modules to collectively enhance forecasting outcomes. Experimental results across multiple real-world datasets demonstrate that STUM consistently improves prediction performance with minimal computational cost. These findings are further supported by hyperparameter optimization, pre-training analysis, and result visualization. We provide our source code for reproducibility at https://anonymous.4open.science/r/STUM-E4F0.","Rapid economic growth and the surge in vehicle numbers have intensified traffic congestion and parking challenges in urban areas globally. To address these challenges, numerous countries have been investing in the development of Intelligent Transportation Systems (ITS), harnessing advances in data collection and mobile computing technologies [1, 2, 3]. Modeling and analyzing spatio-temporal dynamic systems are applicable to various prediction scenarios, and research in this field has received sustained attention over the past few decades [4, 5]. As a crucial component of ITS, traffic flow prediction aims to optimize traffic management, enhance travel safety, and mitigate worsening traffic conditions. [6] Early research primarily focused on statistical model-based approaches, such as the Historical Average (HA) [7] and the Auto-Regressive Integrated Moving Average (ARIMA) [8, 9] model, as well as machine learning-based models [10], including Vector Auto-Regression (VAR) [11, 12] and Artificial Neural Networks (ANN) [13]. However, these methods often struggle to capture the complex nonlinear relationships present in large-scale traffic networks, especially when directly applied to spatio-temporal prediction tasks. With the rise of spatio-temporal big data, recent methods have shifted towards data-driven deep learning models that can more effectively capture the inherent spatio-temporal dependencies of dynamic systems [14]. Simple yet effective strategies include using Convolutional Neural Networks (CNNs) [15] to capture spatial dependencies, and utilizing Recurrent Neural Networks (RNNs) [16] and their variants, such as Long Short-Term Memory (LSTM) [17] networks and Gated Recurrent Units (GRUs) [18], to capture temporal dependencies, thereby improving performance [19]. Recently, numerous traffic prediction methods have combined sophisticated temporal models with Graph Neural Networks (GNNs) to capture global temporal dependencies and regional pattern features, respectively. Spatio-temporal graph neural networks (STGNNs) [20, 21] have gained significant attention due to their ability to learn robust high-level spatio-temporal representations through local information aggregation [6]. Researchers have invested considerable effort in developing complex and innovative models for traffic prediction, including novel graph convolutional methods [22, 23, 24, 25, 26, 27, 28, 29, 30, 4, 31, 32, 33], learning graph structures [5, 34, 35, 36, 37], efficient attention mechanisms [38, 39, 40, 41, 42], and other approaches [43, 44, 45, 46, 47, 48, 49], achieving performance improvements. However, despite ongoing advancements in network architectures, performance gains have begun to plateau, largely due to the following challenges: • Separation between the spatial and temporal module: The independent computation of spatio-temporal modules always limits the effectiveness and efficiency of spatio-temporal representation learning. As shown in Figure 1(c), spatio-temporal relational information influences regional predictions over time. Prediction modules that separate spatial and temporal processing fall short of efficiently propagating regional relationships across temporal intervals. • Data heterogeneity: The heterogeneity of spatio-temporal data results in varying patterns across different spatial and temporal scales. For instance, Figure 1(a) depicts one of the regions monitored by sensors in the PEMS dataset [29], where traffic flow exhibits substantial variability between regions. Figure 1(b) shows traffic flow waveforms at two points within the same region, highlighting that even within a single area, distinct periods show different traffic dynamics. Figure 1: Motivation of our proposed method. (a) shows the sensor distribution of the PEMS04 dataset. (b) is a visual result of the traffic flow of a pair of residential areas over a random period. And (c) is spatio-temporal dependencies shown in traffic flow prediction tasks. Upon revisiting existing traffic forecasting methods, we recognize the need for a unitized framework to address these challenges. To this end, we first propose the concept of Adaptive Spatio-temporal Unitized Cells (ASTUCs), which are designed to compute, update, and store spatial, temporal, and relational information within a single unit, in contrast to prior research that separates spatial and temporal modules. Meanwhile, we propose a novel block called Multi-layer Residual Fusion (MLRF) that leverages the properties of these cells to better capture complex non-linear spatio-temporal dependencies, thereby overcoming heterogeneity and improving computational efficiency and performance. Specifically, we begin by defining an adaptive spatio-temporal unitized matrix at the node level, represented by multiple trainable adaptive matrices using low-rank matrix factorization. During the training process, these cells carry node information and aggregate it into reorganized matrices containing dynamic information at each time step. The use of multi-layer fusion residual blocks mitigates redundant computations, reducing over-parameterization. Finally, all adaptive spatio-temporal unitized cells contribute to the prediction module, enabling accurate traffic flow forecasting. Our main contributions can be summarized as follows: • A unified approach that unifies spatial and temporal learning. In response to module separation, we introduce a novel framework called the Spatio-temporal Unitized Model (STUM) and a corresponding training approach that unifies spatial and temporal processing, as opposed to the traditional method of separating spatial and temporal modules. This unified treatment allows for more efficient learning and accurate representation of spatio-temporal dependencies. • Designed novel modules for spatio-temporal unitization computing. In response to data heterogeneity, we present the Adaptive Spatio-temporal Unitized Cell (ASTUC) based on low-rank adaptive matrices, and a dual feature extraction strategy based on backbone network extractor and Multi-layer Residual Fusion (MLRF), improving the model’s ability to handle complex spatio-temporal interactions. • Extensive experiments. We conduct comprehensive experiments on multiple real-world datasets, demonstrating that our proposed framework significantly outperforms existing baseline models in spatio-temporal prediction tasks while maintaining computational efficiency. Figure 2: The overview of our proposed method. (a) shows the architecture of the Spatio-Temporal Unitized Model (STUM), where MLP represents the model prototype and STGNN represents a way of enhancement. (b) shows the computing process of The Multi-Layer Residual Fusion (MLRF) blocks. (c) shows the construction of Adaptive Spatio-temporal Unitized Cells and how the information transmission Cross Space and Time."
https://arxiv.org/html/2411.09243v1,"Towards Unified Neural Decoding of Perceived, Spoken and Imagined Speech from EEG Signals","Brain signals accompany various information relevant to human actions and mental imagery, making them crucial to interpreting and understanding human intentions. Brain-computer interface technology leverages this brain activity to generate external commands for controlling the environment, offering critical advantages to individuals with paralysis or locked-in syndrome. Within the brain-computer interface domain, brain-to-speech research has gained attention, focusing on the direct synthesis of audible speech from brain signals. Most current studies decode speech from brain activity using invasive techniques and emphasize spoken speech data. However, humans express various speech states, and distinguishing these states through non-invasive approaches remains a significant yet challenging task. This research investigated the effectiveness of deep learning models for non-invasive-based neural signal decoding, with an emphasis on distinguishing between different speech paradigms, including perceived, overt, whispered, and imagined speech, across multiple frequency bands. The model utilizing the spatial conventional neural network module demonstrated superior performance compared to other models, especially in the gamma band. Additionally, imagined speech in the theta frequency band, where deep learning also showed strong effects, exhibited statistically significant differences compared to the other speech paradigms.","I INTRODUCTION Brain-computer interface (BCI) serves as brain-driven communication pathways that convert neural signals into actionable inputs for external systems[1]. In recent years, active BCI has emerged as a next-generation control interface, offering speech-based interaction by directly harnessing the user’s cognitive states and intentions[2]. Various types of user input have been studied, including visual imagery, imagined speech[3, 4, 5, 6], motor imagery[7, 8], and motor execution, each presenting unique advantages and limitations. In this paper, We present a novel, integrative BCI paradigm that encompasses perception, imagined speech, whispered speech, and overt speech. This approach holds promise for addressing various limitations in human-computer interaction and provides BCI users with an alternative method of control. Humans engage in speech production across a variety of real-world contexts. To enable better control of speech in different real-life scenarios, it is essential to collect, analyze, and research data across various speech states. Machine learning techniques for neural decoding have demonstrated significant success, yet they heavily depend on manually engineered features. In contrast, deep learning approaches can directly learn from raw data and execute tasks in an end-to-end manner[9], making them more applicable to real-world scenarios[10, 11, 12]. With this in mind, we evaluated the performance of our proposed brain-based input system using a standard deep neural network (DNN) and investigated straightforward yet effective modifications to tailor the networks more closely to our specific paradigm. DNNs in the BCI domain, such as EEGNet[13], ShallowConvNet[14], and filter-bank convolutional network (FBCNet)[15], typically employ distinct layers of temporal and spatial convolutions. These networks utilize 1D convolutional kernels of fixed sizes to extract temporal, spectral, and spatial features. Temporal kernels are often chosen heuristically, while spatial kernels are applied uniformly across all channels. This architecture has been highly effective in conventional BCI paradigms, such as visual imagery, motor imagery, and motor execution, which predominantly involve sensory or motor-related signals. Nevertheless, the complex nature of EEG signals demands multi-scale kernels to interpret information across various temporal scales[16, 17]. Additionally, due to the effects of volume conduction, EEG signals exhibit redundancy between electrodes, resulting in low spatial resolution. Moreover, current feature extraction methods, which are based in Euclidean space, cannot accurately capture the complex relationships between multiple electrodes, necessitating additional spatial-based features like connectivity methods, phase locking value (PLV), phase lag index (PLI), and coherence, which contain topological spatial information of the brain[18]."
https://arxiv.org/html/2411.09189v1,Improvement and Implementation of a Speech Emotion Recognition Model Based on Dual-Layer LSTM,"This paper builds upon an existing speech emotion recognition model by adding an additional LSTM layer to improve the accuracy and processing efficiency of emotion recognition from audio data. By capturing the long-term dependencies within audio sequences through a dual-layer LSTM network, the model can recognize and classify complex emotional patterns more accurately. Experiments conducted on the RAVDESS dataset validated this approach, showing that the modified dual-layer LSTM model improves accuracy by 2% compared to the single-layer LSTM while significantly reducing recognition latency, thereby enhancing real-time performance. These results indicate that the dual-layer LSTM architecture is highly suitable for handling emotional features with long-term dependencies, providing a viable optimization for speech emotion recognition systems. This research provides a reference for practical applications in fields like intelligent customer service, sentiment analysis, and human-computer interaction.","Speech Emotion Recognition (SER) is a core technology in artificial intelligence and human-computer interaction, aiming to recognize the emotional states of speakers by analyzing and processing audio signals. With the rising demand in applications such as intelligent customer service, emotional robots, and personalized recommendation systems, SER technology has gained widespread attention. The effectiveness of emotion recognition determines the naturalness of human-computer interaction and user experience, making it essential to improve SER model accuracy and real-time performance. Traditional emotion recognition methods often rely on handcrafted features, such as Mel-Frequency Cepstral Coefficients (MFCC), pitch, and rhythm. However, these features cannot fully capture complex emotional information, especially when processing long-term dependencies in emotional features, where handcrafted features have limited expressive power. With the rapid advancement of deep learning, researchers have begun to apply Deep Neural Networks (DNN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) to SER tasks. In particular, Long Short-Term Memory (LSTM) networks, with their unique gating mechanisms that effectively capture long-term dependencies in time-series data, have become the mainstream approach in emotion recognition. However, single-layer LSTM structures still have limitations in extracting emotional features, especially when dealing with audio signals with mixed or complex emotional shifts. To address this, we introduced an additional LSTM layer to form a dual-layer LSTM model that can better capture and interpret emotional information, thereby improving accuracy and efficiency in emotion recognition tasks[1]."
https://arxiv.org/html/2411.09176v1,"Gazing at Rewards: Eye Movements as a Lens into
Human and AI Decision-Making in Hybrid Visual Foraging","Imagine searching a collection of coins for quarters (0.25), dimes (0.10), nickels (0.05), and pennies (0.01)—a hybrid foraging task where observers look for multiple instances of multiple target types. In such tasks, how do target values and their prevalence influence foraging and eye movement behaviors (e.g., should you prioritize rare quarters or common nickels)? To explore this, we conducted human psychophysics experiments, revealing that humans are proficient reward foragers. Their eye fixations are drawn to regions with higher average rewards, fixation durations are longer on more valuable targets, and their cumulative rewards exceed chance, approaching the upper bound of optimal foragers. To probe these decision-making processes of humans, we developed a transformer-based Visual Forager (VF) model trained via reinforcement learning. Our VF model takes a series of targets, their corresponding values, and the search image as inputs, processes the images using foveated vision, and produces a sequence of eye movements along with decisions on whether to collect each fixated item. Our model outperforms all baselines, achieves cumulative rewards comparable to those of humans, and approximates human foraging behavior in eye movements and foraging biases within time-limited environments. Furthermore, stress tests on out-of-distribution tasks with novel targets, unseen values, and varying set sizes demonstrate the VF model’s effective generalization. Our work offers valuable insights into the relationship between eye movements and decision-making, with our model serving as a powerful tool for further exploration of this connection. All data, code, and models will be made publicly available.","Figure 1: Illustrative example of eye movements and decision-making in a hybrid visual foraging task. The image depicts a real-world scenario where the goal is to search piles of coins for multiple instances of target coins with varying monetary values in order to maximize the accumulative monetary reward, within a time-limited environment. Yellow dots and arrows represent the locations and order of eye movements during the search. Red bounding boxes show the target coins that are collected. Note that humans do not always collect every item they fixate on, highlighting the selective nature of the foraging process. Hybrid visual foraging is a ubiquitous challenge in our daily life, such as grocery shopping for a list of items, simultaneously scanning for traffic lights, parking spaces, and restaurants while driving, or looking for a specific amount of change among piles of coins ( Fig. 1). These tasks involve searching for multiple instances of various target types stored in memory, where target values and prevalence can vary, and the exact number of target instances is often unknown. This raises a critical question about how to prioritize target selections during the search process. Understanding these dynamics is essential for optimizing search efficiency and decision-making in complex environments. To tackle this question, eye movements could offer a unique window into the underlying perceptual, cognitive, and evaluative processes involved in decision-making, such as sensory evidence sampling and accumulation [137, 103, 84, 143, 75], decision timing and temporal expectation [11, 117, 107, 123, 6], response inhibitions [82, 46, 62, 85, 24], and decision certainty and confidence [60, 107, 23, 10, 93], offering high temporal and spatial resolution [116, 44, 72, 47, 100]. In hybrid visual foraging, while neuroscience and psychology works [134, 136, 135, 77, 130] have primarily examined the sequence of target selections within the same environment and the timing of search transitions across different environments especially when target values and prevalence vary, there is a notable lack of studies focusing on eye movements. Here, we design and conduct human psychophysics experiments to examine how foraging strategies and eye movements are influenced by the prevalence and value of targets. Alongside studies in psychology and neuroscience, many AI models have been developed to predict eye movements during decision-making tasks, including visual search [57, 35, 2, 83, 142, 49, 125, 118], object recognition and detection [127, 8, 96, 88], and visual question answering [56, 18, 58]. Notably, existing visual search models integrate both bottom-up saliency [57, 35, 2] and top-down feature modulations [83, 142, 49]. However, these models assume idealized scenarios where either a single target type is present or multiple target types have equal values. As a result, they often overlook the need to prioritize target selections based on varying target prevalences and values during the search process. In this work, we introduce a computational model called Visual Forager (VF), a transformer-based architecture trained with reinforcement learning, designed to perform hybrid visual foraging efficiently across varying combinations of target prevalence and values. Unlike prior visual search models [26, 119, 19, 139], which often rely on human data for supervised training, our VF approximates human foraging behaviors and biases, despite zero training on human data. We highlight our key contributions: 1. Drawing from psychology, we introduce hybrid visual foraging tasks for AI models. The predicted eye movements offer a unique window into the decision-making process with high spatial and temporal resolution. 2. We propose an AI model, Visual Forager (VF), for hybrid visual foraging tasks. VF uses actor-critic networks with a vision transformer backbone and integrates feature-based and value-based modulations to guide decision-making processes, determining where to fixate next and whether to collect currently fixated items during foraging tasks. 3. To benchmark AI model performances, we design and conduct human eye-tracking experiments for hybrid visual foraging tasks. Despite no training on human data, our VF achieves cumulative rewards comparable to human participants and approximates their foraging behaviors, including eye movements and decision biases toward highly valued and prevalent targets. 4. Humans can flexibly adapt their foraging strategies to maximize total rewards under varying target values and prevalence. Remarkably, our VF also performs efficient foraging, under out-of-distribution conditions it was never trained on. This capability is attributed to our newly introduced data augmentations applied to target values."
https://arxiv.org/html/2411.09160v1,"Rationality based Innate-Values-driven 
Reinforcement Learning","Innate values describe agents’ intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially developing the awareness of the AI agent through balancing internal and external utilities based on its needs in different tasks is a crucial problem for individuals learning to support AI agents integrating human society with safety and harmony in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model – innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of AI agents’ interaction. We formulated the IVRL model and proposed two IVRL models: DQN and A2C. By comparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the Role-Playing Game (RPG) reinforcement learning test platform VIZDoom, we demonstrated that rationally organizing various individual needs can effectively achieve better performance.","In natural systems, motivation is concerned explicitly with the activities of creatures that reflect the pursuit of a particular goal and form a meaningful unit of behavior in this function heckhausen2018motivation . Furthermore, intrinsic motivations describe incentives relating to an activity itself, and these incentives residing in pursuing an activity are intrinsic. Intrinsic motivations deriving from an activity may be driven primarily by interest or activity-specific incentives, depending on whether the object of an activity or its performance provides the main incentive schiefele1996motivation . They also fall in the category of cognitive motivation theories, which include theories of the mind that tend to be abstracted from the biological system of the behaving organism merrick2013novelty . However, when we analyze natural agents, such as humans, they are usually combined motivation entities. They have biological motivations, including physiological, safety, and existence needs; social motivation, such as love and esteem needs; and cognitive motivation, like self-actualization or relatedness and growth needs merrick2009motivated . The combined motivation theories include Maslow’s Hierarchy of Needs maslow1958dynamic and Alderfer’s Existence Relatedness Growth (ERG) theory alderfer1972existence . Fig. 2 and 2 illustrate the general innate values (intrinsic motivations) model and various models with three-level needs of different amounts, respectively. Many researchers regard motivated behavior as behavior that involves the assessment of the consequences of behavior through learned expectations, which makes motivation theories tend to be intimately linked to theories of learning and decision-making baldassarre2013intrinsically . In particular, intrinsic motivation leads organisms to engage in exploration, play, strategies, and skills driven by expected rewards. The computational theory of reinforcement learning (RL) addresses how predictive values can be learned and used to direct behavior, making RL naturally relevant to studying motivation. Figure 1: The illustration of the proposed innate-values-driven reinforcement learning (IVRL) model. Figure 2: The illustration of innate values models with three-level needs of different amounts. S: Small; M: Medium; L: Large In artificial intelligence, researchers propose various abstract computational structures to form the fundamental units of cognition and motivations, such as states, goals, actions, and strategies. For intrinsic motivation modeling, the approaches can be generally classified into three categories: prediction-based schmidhuber1991curious ; schmidhuber2010formal , novelty-based marsland2000real ; merrick2009motivated , and competence-based barto2004intrinsically ; schembri2007evolution . Furthermore, the concept of intrinsic motivation was introduced in machine learning and robotics to develop artificial systems learning diverse skills autonomously yang2024bayesian . The idea is that intelligent machines and robots could autonomously acquire skills and knowledge under the guidance of intrinsic motivations and later exploit such knowledge and skills to accomplish tasks more efficiently and faster than if they had to acquire them from scratch baldassarre2013intrinsically . In other words, by investigating intrinsically motivated learning systems, we would clearly improve the utility and autonomy of intelligent artificial systems in dynamic, complex, and dangerous environments yang2022game ; yang2023hierarchical . Specifically, compared with the traditional RL model, intrinsically motivated RL refines it by dividing the environment into an external environment and an internal environment, which clearly generates all reward signals within the organism111Here, the organism represents all the components of the internal environment in the AI agent. baldassarre2013intrinsically . However, although the extrinsic reward signals are triggered by the objects and events of the external environment, and activities of the internal environment cause the intrinsic reward signals, it is hard to determine the complexity and variability of the intrinsic rewards (innate values) generating mechanism. To address those gaps, we introduce the innate-values-driven reinforcement learning (IVRL) model to describe the complex behaviors in AI agents’ interactions by integrating with combined motivation theories. We formalize the idea and propose two IVRL models based on classic DQN and A2C algorithms. Then, we compare them with benchmark RL algorithms such as DQN mnih2015human , DDQN wang2016dueling , A2C mnih2016asynchronous , and PPO schulman2017proximal in the RPG RL test platform VIZDoom Kempka2016ViZDoom ; Wydmuch2019ViZdoom . The results demonstrate that the IVRL model can achieve convergence and adapt efficiently to complex and challenging tasks."
https://arxiv.org/html/2411.09158v1,"The : Towards Fully Automated 
Graph Theory Research","This paper introduces the Optimist, an autonomous system developed to advance automated conjecture generation in graph theory. Leveraging mixed-integer programming (MIP) and heuristic methods, the Optimist generates conjectures that both rediscover established theorems and propose novel inequalities. Through a combination of memory-based computation and agent-like adaptability, the Optimist iteratively refines its conjectures by integrating new data, enabling a feedback process with minimal human (or machine) intervention. Initial experiments reveal the Optimist’s potential to uncover foundational results in graph theory, as well as to produce conjectures of interest for future exploration. This work also outlines the Optimist’s evolving integration with a counterpart agent, the Pessimist (a human or machine agent), to establish a dueling system that will drive fully automated graph theory research.","This paper introduces the Optimist, an autonomous agent for generating conjectures in graph theory that iteratively adapts its output based on feedback. The Optimist builds upon the principles of TxGraffiti [1] with enhancements in mixed-integer programming, heuristic search, and a memory-based structure that allow it to efficiently produce and refine conjectures. Through its agent-based framework, the Optimist can systematically generate inequalities involving graph invariants and filter results based on empirical strength, novelty, and known mathematical knowledge. The system, implemented in Python, is open-source and accompanied by Jupyter notebooks, enabling accessibility and reproducibility available at this papers companion GitHub repository111https://github.com/RandyRDavila/The-Optimist/tree/main. A key feature of the Optimist is its dynamic memory structure, which stores computed graph invariants and theorems, facilitating rapid retrieval and incremental updates as new graphs are introduced. This adaptive structure allows the system to refine its conjectures with minimal human intervention, moving toward a form of fully automated reasoning in graph theory. Additionally, the Optimist is designed to interact with a complementary (human or machine) agent, the Pessimist, which will challenge the Optimist’s conjectures by identifying counterexamples. Together, these agents form a dueling framework, termed GraphMind, enabling continuous conjecture generation, verification, and refinement in a closed feedback loop. The remainder of this paper is organized as follows: Section 2 reviews prior work in automated conjecture generation, situating the Optimist within this field. Section 3 details the system’s architecture, heuristics, and optimization methods. Section 4 presents the conjectures generated by the Optimist, highlighting both rediscovered known results and new inequalities. Finally, Section 5 discusses the future work and the development of the Optimist in tandem with Pessimist and the potential impact of GraphMind in advancing automated reasoning in graph theory."
https://arxiv.org/html/2411.09089v1,"Set-Based Retrograde Analysis:
Precomputing the Solution to 24-card Bridge Double Dummy Deals","Retrograde analysis is used in game-playing programs to solve states at the end of a game, working backwards toward the start of the game. The algorithm iterates through and computes the perfect-play value for as many states as resources allow. We introduce setrograde analysis which achieves the same results by operating on sets of states that have the same game value. The algorithm is demonstrated by computing exact solutions for Bridge double dummy card-play. For deals with 24 cards remaining to be played (10^{27} states, which can be reduced to 10^{15} states using preexisting techniques), we strongly solve all deals. The setrograde algorithm performs a factor of 10^{3} fewer search operations than a standard retrograde algorithm, producing a database with a factor of 10^{4} fewer entries. For applicable domains, this allows retrograde searching to reach unprecedented search depths.","Some of the early high-performance game-playing programs relied on retrograde analysis and endgame databases for strong play. The most notable example is Checkers, where 39 trillion endgame positions, all those with 10 or fewer pieces, were used as part of the Chinook program (Schaeffer et al. 1992), and for solving Checkers (Schaeffer et al. 2007). Endgame databases are also used widely in Chess programs (Chess 2024), as well as in many other games (e.g., for solving Awari (Romein and Bal 2003)). Endgame databases are most effective in games where there are far fewer positions at the end of the game than elsewhere. As a result, they have not been applied in games that do not have this property. For instance, Sturtevant (2003) noted that in 3-player Chinese Checkers a winning arrangement of a single player’s pieces in the game has approximately 10^{23} possible permutations of the other player’s pieces, making it infeasible to store all the variations of even a single winning configuration. While in Chinese Checkers each player has a unique endgame configuration (the other side’s piece locations are irrelevant), in Go the locations of both side’s pieces in a terminal state are important. Hence these games require significantly different analysis (Berlekamp and Wolfe 1994). In a 4-player trick-based card game such as Bridge, the last two tricks have \binom{52}{2}\binom{50}{2}\binom{48}{2}\binom{46}{2}=1.9\times 10^{12} possible deals of the cards. However, there are only 16 ways for each deal to play out, meaning it is trivial to solve but storing all states (as done in Checkers) is difficult. These numbers suggest it might be impractical to build an effective endgame database for Bridge with, say, 6 tricks to play (10^{27} states). This statement is true under the assumption that every unique endgame state must be stored independently. The contribution of this paper is to show how to avoid this assumption by representing endgame states as sets. This idea, along with other symmetry reduction techniques, makes it feasible to use retrograde search to compute all 24-card (6-trick) Bridge double-dummy (DD) endgames in a week on appropriate hardware using just 50GiB of storage, something that was historically hard to imagine. This paper describes our set-based approach to endgame databases, making the following contributions. • We present a new set-based retrograde analysis algorithm, setrograde analysis, inspired by the ideas in Ginsberg’s Partition Search (Ginsberg 1996). Whereas standard retrograde analysis computes a value for every state, setrograde analysis generalizes a state into a set where all members have the same game-theoretic value. The algorithm can skip over many of the states that are subsumed by the set. Replacing states with sets leads to a large degree of state-space compression, by mapping an exponentially growing state-space to a smaller set-space. • The algorithm is demonstrated using 24-card Bridge deals. The set database contains 4 orders of magnitude (OOM) fewer sets than there are states in a traditionally generated database. The set database was constructed using 3 OOM less computing resources than would be needed for a traditional 24-card database. This enabled an 800 trillion state state-space to be solved in a week using a single multi-core machine. This work generalizes retrograde analysis, allowing it to have more impact in applicable domains. In particular, the reduced computing and storage needs mean that endgame database technology can be scaled to unprecedented levels."
https://arxiv.org/html/2411.09050v1,"The Systems Engineering Approach in Times of
Large Language Models","Using Large Language Models (LLMs) to address critical societal problems requires adopting this novel technology into socio-technical systems. However, the complexity of such systems and the nature of LLMs challenge such a vision. It is unlikely that the solution to such challenges will come from the Artificial Intelligence (AI) community itself. Instead, the Systems Engineering approach is better equipped to facilitate the adoption of LLMs by prioritising the problems and their context before any other aspects. This paper introduces the challenges LLMs generate and surveys systems research efforts for engineering AI-based systems. We reveal how the systems engineering principles have supported addressing similar issues to the ones LLMs pose and discuss our findings to provide future directions for adopting LLMs.","Large Language Models (LLMs) leverage neural network architectures trained on large amounts of data to learn underlying language patterns. LLMs generate content in formats humans understand based on these architectures \parencitefeuerriegel2024generative. Such ability creates novel human-machine interfaces \parencitecabrera2024self for adopting AI at different levels of our society. Generative AI technologies promise new applications for addressing critical problems in diverse domains. The complexity of socio-technical systems and the LLMs’ nature challenge the realisation of this vision. Social problems have critical requirements that demand reliable systems. LLMs rely on probabilistic models that make systems’ components based on these technologies non-deterministic, data-driven, and prone to hallucinations \parencitedantonoli2024large impacting the alignment and reliability of the systems. LLMs operate as black-boxes \parencitefeuerriegel2024generative, which impact systems’ accountability and interpretability as designers and users do not control and understand the systems (i.e., intellectual debt). Social problems usually appear in resource-constrained environments. Building LLMs is an expensive process that causes significant environmental concerns because it generates an immense carbon footprint \parenciteschwartz2020greenai. The outlined challenges require interdisciplinary research to align societal problems, systems, and AI technical advances. The systems engineering approach is equipped with principles to facilitate this alignment by prioritising the problems and their context before considering the technologies for their resolution. We envisage an ecosystem where systems engineering and LLMs mutually benefit instead of the naive belief that benefits come from the LLMs to the domains in one direction. This paper surveys how researchers have used the system engineering approach to design AI-based systems since 2017 (i.e., when current LLM technologies emerged) as a first step to building such an ecosystem. The main research question we want to answer is how does current research use the systems engineering approach to address challenges similar to the ones LLMs impose on socio-technical systems?"
https://arxiv.org/html/2411.09702v1,On the Surprising Effectiveness of Attention Transfer for Vision Transformers,"Conventional wisdom suggests that pre-training Vision Transformers (ViT) improves downstream performance by learning useful representations. Is this actually true? We investigate this question and find that the features and representations learned during pre-training are not essential. Surprisingly, using only the attention patterns from pre-training (i.e., guiding how information flows between tokens) is sufficient for models to learn high quality features from scratch and achieve comparable downstream performance. We show this by introducing a simple method called attention transfer, where only the attention patterns from a pre-trained teacher ViT are transferred to a student, either by copying or distilling the attention maps. Since attention transfer lets the student learn its own features, ensembling it with a fine-tuned teacher also further improves accuracy on ImageNet. We systematically study various aspects of our findings on the sufficiency of attention maps, including distribution shift settings where they underperform fine-tuning. We hope our exploration provides a better understanding of what pre-training accomplishes and leads to a useful alternative to the standard practice of fine-tuning. Code to reproduce our results is at https://github.com/alexlioralexli/attention-transfer.","Pre-training has emerged as a dominant paradigm in machine learning and has significantly improved performance on a variety of tasks [27, 11, 2, 22]. In computer vision in particular, self-supervised representation learning methods [21, 6, 4, 22] and weakly supervised methods [40, 45] have enabled learning from large amounts of images. It is widely accepted that these methods work because they teach models useful features that are relevant for downstream tasks. But is this story actually true? Perhaps there is another capability learned during pre-training that is sufficient to explain its benefits. In this paper, we present an alternative explanation: pre-training teaches the model how information should be routed between tokens. We specifically focus on Vision Transformers (ViT) [12], not only because they are the most popular architecture for scaling, but also because Transformers explicitly decouple this information flow. Inter-token communication is solely fulfilled by attention, while the remaining bulk of computation are intra-token operations that are applied to each token independently. In contrast, other architectures such as ConvNets [33, 20] simultaneously expand the receptive fields and extract the features, making it difficult to isolate the effect of information flow. We hypothesize that the features computed by the intra-token operations are not essential to explain the benefits of pre-training, and that the pre-trained attention maps are typically sufficient for downstream tasks. We test our hypothesis by introducing a new set of methods called attention transfer. Concretely, we treat a pre-trained ViT as the teacher and train a student model for downstream tasks while transferring only the attention patterns from the teacher. In contrast to the common fine-tuning paradigm of transferring all the weights (which mixes the effect of features and attention maps), only the inter-token flow is transferred. In this way, the student must learn features from scratch, while isolating the benefits of the attention maps learned during pre-training. Figure 1: Using only attention is sufficient for full performance. By copying the attention maps (top) from a MAE [22] pre-trained ViT-L [12], a ViT-L can reach a top-1 accuracy of 85.1 on ImageNet-1K [10] – recovering 77.8% of the gap between no transfer (training from scratch, 83.0) and full transfer (fine-tuning all the weights, 85.7). Distilling attention maps (bottom) can even fully match MAE weight tuning while only transferring the inter-token flow. We study two types of attention transfer. The first is Attention Copy, which directly “copy-and-pastes” the attention maps. The learning is fully decoupled, as inter-token computation is entirely from the teacher, and the student only learns intra-token patterns routed by the teacher’s attention maps. This is well-suited as a scientific probe, but is less practical since both networks need to be forwarded during the inference. The second is Attention Distillation, where the student simply distills attention patterns from the teacher, whose attention maps are no longer used after training. This is practical, but also helps identify the importance of the teacher’s inter-token information flow. While both attention transfer variants are straightforward, we find them highly effective. Figure 1 illustrates this with a ViT-L [12] pre-trained using Masked Autoencoding (MAE) [22]. Compared to no transfer (training from scratch) and full transfer (fine-tuning all the MAE weights), Attention Copy can close most of the gap in performance, whereas Attention Distillation can match the fine-tuning accuracy on ImageNet-1K classification [10]. This is achieved by only transferring the inter-token flow from the same model. Furthermore, since attention transfer requires the student to learn features from scratch, those features are significantly different from the teachers’ (Figure 5) and improve ImageNet-1K accuracy score to 86.3 (+0.6) when ensembled with the teacher (Figure 6). To summarize, we make the following contributions: • Detailed analysis on the sufficiency of attention maps. We find that solely using the pre-trained attention patterns is typically sufficient to achieve the same downstream accuracy as fine-tuning on ImageNet-1K. Furthermore, we observe practical benefits, as ensembling with attention transfer significantly improves ImageNet performance. This calls into question the commonly-believed story that pre-training is only about feature learning. While our main observation is robust w.r.t. different models and pre-training methods, we do find settings where pre-trained features are indeed necessary to realize the full gains from pre-training. Our bare-minimum solution for attention transfer is more affected by data distribution shifts compared to weight tuning. Section 4 presents extensive analyses to better understand the behaviors of attention transfer. They are i) partial transfer with a subset of layers or heads; ii) variants of our method that transfer other attention-related activations; and importantly, iii) various ways to verify that the student is not just re-learning the teacher model. Section 5 systematically tests how well our findings apply across a variety of pre-training and fine-tuning datasets, pre-training methods, model sizes, and tasks. • Attention transfer methods. We introduce Attention Copy and Attention Distillation, which are methods to train a ViT on a downstream task while utilizing only the attention maps of a pre-trained teacher ViT. These methods help us understand the role of the features versus the attention patterns learned during pre-training. With further research, attention transfer could offer a potential alternative to the decade-long practice of fine-tuning pre-trained vision models [16, 12, 22]. Nearly all aspects of the fine-tuning pipeline have been thoroughly examined, suggesting a probable saturation of recipes. Weight sharing can also face security risks (e.g., white-box attacks [17]). We hope our systematic examination of attention transfer sheds new light on how to leverage pre-trained ViTs, and will help establish this approach as an effective alternative when weight transfer is less applicable. Figure 2: Two types of Attention transfer for Vision Transformers. Attention Copy (left): We simply “copy-and-paste” the attention maps from a pre-trained teacher model to a randomly initialized student one. Other weights of the student are then trained via supervised learning. This fully decouples inter-token learning (from the teacher) and intra-token learning (in the student); but is less practical. Attention Distillation (right): The student computes its own attention maps, with an additional cross-entropy loss to distill patterns from the teacher during training. The teacher is no longer used during inference. H: number of heads; L: number of Transformer layers."
https://arxiv.org/html/2411.09683v1,Towards a Classification of Open-Source ML Models and Datasets for Software Engineering,"Background: Open-Source Pre-Trained Models (PTMs) and datasets provide extensive resources for various Machine Learning (ML) tasks, yet these resources lack a classification tailored to Software Engineering (SE) needs. Aims: We apply an SE-oriented classification to PTMs and datasets on a popular open-source ML repository, Hugging Face (HF), and analyze the evolution of PTMs over time. Method: We conducted a repository mining study. We started with a systematically gathered database of PTMs and datasets from the HF API. Our selection was refined by analyzing model and dataset cards and metadata, such as tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses are replicable, with a publicly accessible replication package. Results: The most common SE task among PTMs and datasets is code generation, with a primary focus on software development and limited attention to software management. Popular PTMs and datasets mainly target software development. Among ML tasks, text generation is the most common in SE PTMs and datasets. There has been a marked increase in PTMs for SE since 2023 Q2. Conclusions: This study underscores the need for broader task coverage to enhance the integration of ML within SE practices.","The fast expansion of open-source platforms like Hugging Face (HF) [1] has enhanced access to Machine Learning (ML) models and datasets, driving advancements across various domains. With a consistent and significant uptrend in development activities on HF [2], it is distinguished by its vast collection of Pre-Trained Models (PTMs), compared to other platforms [3][4]. However, the categorization of these resources overlooks the specific needs of Software Engineering (SE). SE tasks frequently involve code generation, code analysis, and bug detection, which differ significantly from the tasks commonly addressed by general-purpose ML models such as object detection or image segmentation. Therefore, the motivation for this work is to address this gap, as the absence of SE-specific categorization limits the efficient application of ML in SE tasks, potentially slowing down SE innovation. By providing a framework that aligns ML tasks with SE needs, this research aims to make the selection of PTMs and datasets more relevant and effective for SE practitioners and researchers, thus addressing a critical need within the field [5]. The main contributions of this work are: (a) proposing and proving the feasibility of a preliminary classification framework for PTMs and datasets hosted on HF, tailored to SE needs; (b) providing advanced analysis, including the exploration of the relationship between SE activities and ML tasks, as well as the evolution of SE PTMs over time; (c) presenting a reproducible pipeline that accesses the HF API, filters, refines, and classifies resources on specific SE tasks. Data availability statement: All research components, including the original and preprocessed data, along with all scripts for data collection, preparation, and analysis, are publicly available on Zenodo [6]. This ensures transparency and enables independent replication of the study, which is essential for updating the classification as new open-source PTMs and datasets are constantly being released."
https://arxiv.org/html/2411.09642v1,"On the Limits of Language Generation:
Trade-Offs Between Hallucination and Mode Collapse","Specifying all desirable properties of a language model is challenging, but certain requirements seem essential for any good model. Given samples drawn from an unknown language, the trained model should (1) produce valid strings that have not been seen in the training data, and (2) be expressive enough to capture the full richness of the language. Otherwise, if the language model outputs invalid strings, it “hallucinates,” and if it fails to capture the full range of the language, it suffers from “mode collapse.” In this paper, we ask whether it is possible for a language model to meet both of these requirements.We investigate this question within a statistical setting of language generation, building on the seminal works of \citet[Inf. Control]gold1967language, \citet[STOC]angluin1979finding, and \citet[Tech. Report]angluin1988identifying. In this setting, the language model is presented with randomly sampled strings from a distribution supported on an unknown language K, which is only known to belong to a possibly infinite collection of candidate languages. The goal of the model is to generate unseen strings from this target language. We say that the language model generates from K with consistency and breadth if, as the size of the training set increases, the set of strings it can output converges to the set of all unseen strings in K.[NeurIPS]kleinberg2024language posed an open question of whether consistency and breadth in language generation are both possible. We answer this question negatively: for a large class of language models – including next-token-prediction-based models – this is impossible for most collections of candidate languages. This contrasts with the recent positive result of \citet[NeurIPS]kleinberg2024language, which demonstrated that consistent generation, without requiring breadth, is possible for any countable collection of candidate languages. Our finding highlights that generation with breadth is fundamentally different from generation without breadth.As a byproduct of our result, we also examine how many samples are required for generation with or without breadth, establishing near-tight bounds on the “learning curves” for generation in the statistical framework of \citet*[STOC]bousquet2021theory.Finally, our results also give some hope for consistent generation with breadth: it is achievable for any countable collection of languages when negative examples – in the form of strings outside of K – are available in addition to strings inside of K. This suggests that feedback in post-training, which encodes negative examples, can be crucial in reducing hallucinations while also limiting mode collapse.","Language acquisition is a fundamental mystery across multiple scientific fields, ranging from Biology and Neuroscience to Sociology \citepbresnan2007syntactic,saffran1996statistical,clark2014distributional,mahowald2024dissociating. Theoretical Computer Scientists have been fascinated by language since the early days of the field: in the 1950s, \citetturing1950computing introduced his famous test using language as an interface to cognition, \citetshannon1951prediction studied statistics of printed English aiming at understanding its entropy and the extent to which it could be compressed, and \citetmandelbrot1953informational designed a statistical model to capture connections between language and the brain. Over the years, language modeling has advanced through simple models, such as the word n-gram model introduced by \citetshannon1951redundancy and widely used in natural language processing \citepbrown1992class. In the early 2000s, neural networks achieved a significant breakthrough in the field \citepbengio2000neural, leading to fascinating deep learning systems \citepmikolov2010recurrent,goldberg2016primer,lecun2015deep built using traditional architectures like Recurrent Neural Networks \citeprumelhart1986learning and Long Short-Term Memory \citephochreiter1997long. In 2017, the field of language modeling was revolutionized by the introduction of the Transformer architecture \citepsutskever2014sequence,bahdanau2014neural,vaswani2017attention, which led to the development of Large Language Models (LLMs). The achievements of LLMs have been groundbreaking; recent models can perform well on tasks far beyond natural language processing \citepbubeck2023sparks,touvron2023llama. Despite their impressive performance, their extensive use has revealed that LLMs exhibit various bizarre behaviors even in seemingly mundane tasks \citepborji2023categorical. Perhaps the most well-known issue with current LLMs is hallucinations: the models generate false but plausible-sounding text with surprising frequency \citepzhang2023siren,ji2023survey.111We stress that LLMs outputting wrong facts based on errors in training data (e.g., “The Earth is flat”) or miscalculations (e.g., “1+1 = 3”) do not constitute hallucinations. A hallucination is a plausible but false text with unclear origin (e.g., “Barack Obama was the president of the US and was born on January 1, 1958”). Such hallucinations, highlighted by popular media \citepweise2023ai, could significantly impact safety, reliability, and user trust as the adoption of these systems extends to new tasks \citephendrycks2021unsolved,amodei2016concrete. The importance of this problem, among other concerns, led both the US \citepbiden2023executive and the EU \citepsatariano2023eu to issue calls for safeguards against misleading outputs generated by LLMs. In this direction, designing LLMs that generate responses consistent with the ground truth is an effort that has gained a lot of attention from Machine Learning (ML) practitioners \citepandriopoulos2023augmenting,gunasekar2023textbooks,wei2022chain,huang2023survey,feng2024don,kang2024unfamiliar,ji2023survey, policymakers \citepbiden2023executive,satariano2023eu,satariano2023nations, and theorists \citephanneke2018actively,kalai2024calibrated,kleinberg2024language. If the sole goal is to avoid hallucinations, then, of course, one could simply limit the range of outputs generated by the language model. As an extreme example, consider a language model that only outputs “I am a language model” and, therefore, never hallucinates. However, modern LLMs do not just aim to generate a few valid outputs; their goal is to obtain the ability to express a wide range of plausible outputs, thus capturing the richness of human language. The key challenge lies in avoiding hallucinations while achieving breadth. The problem of achieving consistent generation with breadth is not new in the ML community, dating back at least to the era of Generative Adversarial Networks (GANs) \citepgoodfellow2020generative. In this line of work, mode collapse \citepgoodfellow2020generative is the analog of lack of breadth; it refers to the phenomenon where the GAN assigns non-zero mass only to a few modes of the true data distribution, thus producing a limited variety of samples and becoming repetitive \citeparjovsky2017towards,bau2019seeing,shmelkov2018good. The starting point of our work is exactly this puzzling tension between consistent generation and breadth in language generation. We start with a mathematical specification inspired by classical work on learning theory, tracing back to the seminal work of \citetangluin1988identifying, and the recent formulation of \citetkleinberg2024language: the domain \euscr{X} is a countable collection of strings, and there is an unknown target language K which is a subset of this domain. We know that the true language lies within a collection of possibly infinite but countably many languages \euscr{L}=\{L_{1},L_{2},\dots\}. There exists an unknown distribution \euscr{P} over strings in K\in\euscr{L} that satisfies \operatorname{supp}(\euscr{P})=K; any distribution with this property is said to be valid for K. The algorithm observes i.i.d. samples from \euscr{P} and aims to learn how to generate unseen strings from the target language K – this, at a high level, is the language generation problem. Intuitively, the target language K is capturing “facts” of the world; everything that belongs to K is correct, whereas everything outside of K is unambiguously incorrect and can be thought of as a “hallucination.” Observe that K has to be infinite for the problem to be well-defined as, otherwise, at some point, the algorithm will see all possible strings of K and, from then on, would have no unseen strings to generate from. Let us explore language generation further, with the immediate aim of quantifying an algorithm’s progress toward becoming a useful generator. Consider a generating algorithm \mathpzc{G}_{n}222Formally, a generating algorithm is a sequence of mappings (\mathpzc{G}_{n})_{n\in\mathbb{N}}: for each n, it is a computable mapping from a training dataset of size n to a (computable) distribution (i.e., a sampling algorithm) over \euscr{X}. We will use the notation (\mathpzc{G}_{n})_{n} to refer to the generating algorithm and the notation \mathpzc{G}_{n} or simply \mathpzc{G} for the induced distribution (generator) after training; hence when we write x\sim\mathpzc{G}_{n} or \operatorname{supp}(\mathpzc{G}_{n}), we refer to the distribution obtained after training. that is trained on a set S of n i.i.d. examples from \euscr{P}. To quantify the inconsistency of \mathpzc{G}_{n}, we need an objective. As discussed above, this objective should penalize \mathpzc{G}_{n} for outputting strings outside of K and for repeating examples already seen in the training data S.333When we require generating algorithm to achieve breadth, it is not important to enforce that the support does not contain S. We will elaborate after the formal statement of Definition 4. For a target language K and a model \mathpzc{G}_{n} trained on S, we consider the following generation error \mathrm{gen\_er}(\mathpzc{G}_{n})\coloneqq\Pr_{S\sim\euscr{P}^{n}}[% \operatorname{supp}(\mathpzc{G}_{n})\supset K\setminus S]\,. (1) In words, a model errs according to \mathrm{gen\_er}(\cdot) if it either hallucinates by outputting strings from \euscr{X}\setminus K or if it outputs something already contained in the training set S. This is inspired by the notion of generation considered by \citetkleinberg2024language; they call an algorithm a consistent generator if its support becomes a subset of K\setminus S after seeing finitely many training examples S. We relax this definition and call an algorithm a consistent generator for the collection \euscr{L} if its error, as defined in Equation 1, asymptotically goes to zero for any valid distribution \euscr{P}. Let us now review how prior work has approached issues with language generation algorithms – foremost, hallucination. Under the above statistical setting, \citetkalai2024calibrated made important progress showing that calibrated models must hallucinate by lower bounding the hallucination rate by the model’s calibration. For a detailed comparison with our work, we refer to Section 1.5. Closer to our paper, the work of \citetkleinberg2024language explored language generators that must not hallucinate, i.e., they must be consistent. They studied language generation in an online setting where the data are not drawn from \euscr{P} but are given as a stream to the learner, i.e., as an adversarial enumeration of the strings of the true language K. In their setting, \mathpzc{G}_{n} is said to generate in the limit from K if, after some finite time n_{0} in the enumeration of K, \mathpzc{G}_{n} is able to generate new unseen strings from K for all subsequent times n\geq n_{0}. They showed that there exists an algorithm that can generate in the limit from every countable list of candidate languages. This result is surprising because it contrasts with strong negative results for the well-studied problem of language identification in the limit (where one wants to identify K in the limit and not simply generate from it;444 Very briefly, a language collection \euscr{L}=\{L_{1},L_{2},\dots\} is called identifiable in the limit if there exists an algorithm (\euscr{A}_{n}\colon\euscr{X}^{n}\to\mathbb{N})_{n} such that for any K\in\euscr{L} and any enumeration x_{1},x_{2},\dots of the strings of K appearing as a stream to (\euscr{A}_{n}), there is a finite time n_{0}\in\mathbb{N} after which the algorithm predicts the correct index of the true language, i.e., L_{\euscr{A}_{n}(x_{1},\dots,x_{n})}=K for any n\geq n_{0}. see also Definition 9). The family of languages identifiable in the limit is very limited: the results of \citetgold1967language,angluin1979finding showed that language identification is a very difficult problem and most collections of languages are non-identifiable (in fact, there is a tight characterization due to \citetangluin1980inductive which we state in Definition 10). Hence, the algorithm of \citetkleinberg2024language shows that language generation in the limit is much more tractable than identification. We note that while their algorithm operates in a non-statistical setting, it will be an important building block for our results. \citet kleinberg2024language observed that their algorithm eventually becomes a consistent generator but suffers from mode collapse: initially, it generates with breadth while being inconsistent with the target language; later on, as a larger part of the stream is seen, it starts sacrificing breadth in order to generate valid outputs. This behavior led them to leave the existence of a consistent generator that achieves breadth as an interesting open question. In this work, we will formally introduce a notion of breadth for language generation in our statistical setting (Section 1.1.1). For now, we mention that our definition roots in the notion of mode collapse from Generative Adversarial Networks (GANs) \citepgoodfellow2020generative,arjovsky2017towards and, roughly speaking, states that an algorithm (\mathpzc{G}_{n}) generates with breadth from K if the probability that its support contains all the unseen examples from the target language goes to 1, as the training samples from a valid distribution go to infinity. Now it is a good point to contrast breadth with consistency: consistent generators aim at avoiding any elements outside of K while generators achieving breadth try to cover all unseen elements of K. The question of \citetkleinberg2024language is asking whether the equilibrium condition that the support of the generator exactly matches the unseen elements of K can eventually be achieved by some algorithm. This is the main question we aim to address in this paper. Is it possible to achieve consistent language generation with breadth or is there some inherent trade-off between consistency and breadth? 1.1 Informal Results Our main results confirm the tension between consistent generation and breadth for language models, conjectured by \citetkleinberg2024language, in a strong way: informally, we show that A language model that generates with breadth must be inconsistent, i.e., it must hallucinate. We focus on the probabilistic setting of \citetangluin1988identifying which we have already introduced informally. En route to our results in the probabilistic setting, we also obtain results in the online setting of \citetgold1967language, \citetangluin1979finding, and \citetkleinberg2024language, as we will see later. To facilitate a formal discussion of our contributions, we need to introduce some further definitions. 1.1.1 Setup and Definitions A generating (or learning) algorithm is a sequence of computable mappings (\mathpzc{G}_{n})=(\mathpzc{G}_{n})_{n\in\mathbb{N}} from samples S\subseteq\euscr{X}^{n} to generators, which are simply distributions over the domain \euscr{X}. More formally, a generating algorithm is a sequence of mappings from samples to Turing machines that generate samples from an (explicitly or implicitly) defined distribution over strings. In the statistical setting we consider, the learner observes samples from an unknown distribution which is valid for some unknown language K in the collection \euscr{L}=\{L_{1},L_{2},\dots\}. Definition 1 (Valid Distribution \citepangluin1988identifying). A distribution \euscr{P} over a countable domain \euscr{X} is valid with respect to a countable language collection \euscr{L} if its support is the same as some language K\in\euscr{L}. In this case, when we want to be specific about the language that \euscr{P} draws samples from, we say \euscr{P} is valid for K. If the collection \euscr{L} is clear from context, we will simply say that \euscr{P} is valid. Based on this definition and building on the model studied by \citetkleinberg2024language, we give the following adaptation for consistent generation from a collection \euscr{L} in the statistical setting. Definition 2 (Consistency). A generating algorithm (\mathpzc{G}_{n}) for a language collection \euscr{L} is consistent if for any valid distribution \euscr{P}, it holds that \lim_{n\to\infty}\mathrm{gen\_er}(\mathpzc{G}_{n})=0. Otherwise, the algorithm is said to be inconsistent. Hence, an algorithm is said to be consistent if the generators it produces by training on any valid distribution \euscr{P} converge to generating examples from the unseen part of \euscr{P}. Some of our results explore when asymptotic consistency is achievable. However, the main focus of our work is on understanding the rates at which consistency (and other desirable properties) can be attained – if possible at all. In particular, we want to study the rate at which the generation error \mathrm{gen\_er}(\mathpzc{G}_{n}) decreases as the number of samples n goes to infinity – that is, we want to study the learning curve of consistent generation (and other tasks that we introduce later in this section). Bousquet, Hanneke, Moran, van Handel, and Yehudayoff \citepbousquet2021theory characterized learning curves for binary classification, formalizing the universal rates framework, earlier explored by \citetschuurmans1997characterizing and \citetantos1996strong. To this end, we borrow their definition of universal rates. Definition 3 (Informal, Universal Rates; \citepbousquet2021theory, see Definition 12). A generating algorithm (\mathpzc{G}_{n}) has rate R(\cdot), where \lim_{n\rightarrow\infty}R(n)=0, for a language collection \euscr{L} if \forall\euscr{P}\in\mathrm{Val}(\euscr{L})~{}~{}\exists C,c>0\quad\text{such % that}\quad\mathrm{gen\_er}(\mathpzc{G}_{n})\leq C\cdot R(c\cdot n)\quad\forall n% \in\mathbb{N}\,, where \mathrm{Val}(\euscr{L}) is the class of valid (realizable) distributions for \euscr{L}. Observe that these learning curves are distribution-dependent since the constants c and C are allowed to depend on \euscr{P}. This difference turns out to be crucial and can, sometimes, lead to significant differences between universal rates and the corresponding distribution-independent rates \citepbousquet2021theory. Among different universal rates, exponential universal rates are of specific interest as they are often the best possible rate, as we will see later. We say that the algorithm (\mathpzc{G}_{n}) generates with an exponential universal rate if R(n)=\exp(-n) in the above definition. Next, we turn to language generation with breadth. Definition 4 (Breadth). A generating algorithm (\mathpzc{G}_{n}) for a language collection \euscr{L} is said to achieve breadth if, for any valid distribution \euscr{P}, it holds that \lim_{n\to\infty}\Pr[\operatorname{supp}(\mathpzc{G}_{n})\supseteq K\setminus S% _{n}]=1, where S_{n} is the dataset used to train \mathpzc{G}_{n}, i.i.d. from \euscr{P}. Otherwise, the algorithm suffers from mode collapse. Definition 4 is inspired by the literature on GANs (see e.g., \citepgoodfellow2020generative,arjovsky2017towards). For instance, consider the work of \citetarjovsky2017towards, which studies distributions \mathpzc{G} and \euscr{P} induced by the generator and nature, respectively, and says that mode collapse occurs when the KL divergence \operatornamewithlimits{\mathsf{KL}}\left(\euscr{P}\|\mathpzc{G}\right)% \coloneqq\int\log\left(\nicefrac{{\euscr{P}(x)}}{{\mathpzc{G}(x)}}\right)~{}{% \rm d}\euscr{P}(x)\to\infty. In particular, mode collapse happens when there is some string x\in\operatorname{supp}(\euscr{P}) for which \mathpzc{G}(x)=0. In other words, the generator has breadth when \operatorname{supp}(\mathpzc{G})\cup S_{n}\supseteq\operatorname{supp}(\euscr{% P}), which recovers our definition for breadth by noting that \operatorname{supp}(\euscr{P})=K since \euscr{P} is valid for K and that, to be compatible with the definition of consistency (Definition 2), we bar a generator from repeating strings it has already seen. (It is worth mentioning that one can modify the definition of breadth to require \operatorname{supp}(\mathpzc{G}_{n})\supseteq K without changing any of our results; see Remark 2.) We also note that the definition of consistency we use can also be derived in an analogous fashion by requiring the reverse KL divergence (i.e., \operatornamewithlimits{\mathsf{KL}}\left(\mathpzc{G}\|\euscr{P}\right)) to be finite. Putting the definitions of consistency and breadth together implies that an algorithm generates with consistency and breadth if, eventually, its support matches the set of unseen strings in K, i.e., K\setminus S_{n} at the n-th step. After presenting our main results, in Section 1.3, we discuss relaxations of this notion of consistent generation with breadth. A last ingredient for our results concerns the decidability of a folklore Theoretical Computer Science problem, which we call the membership oracle problem, that has motivated extensive work in formal languages and complexity theory \citepsipser2012introduction,soare1999recursively. A generator \mathpzc{G}, which is the output of some generating algorithm, corresponds to some Turing machine, as is standard in the language inference literature, that samples according to a distribution over \euscr{X} \citepangluin1979finding,blum1975toward,angluin1983inductive,adleman1991inductive. Definition 5 (Membership Oracle Problem). Given a generator \mathpzc{G}, the membership oracle problem for \mathpzc{G}, denoted as \mathsf{MOP}(\mathpzc{G}), is defined as follows: given the description of \mathpzc{G} and a string x, output Yes if x\in\operatorname{supp}(\mathpzc{G}) and output No otherwise. This problem is, in general, undecidable due to a reduction to the halting problem (Section A); nevertheless, its decidability depends on the structure of the Turing machine as we will see shortly. The above definition naturally extends to generating algorithms. Definition 6 (MOP for Generating Algorithms). The membership oracle problem is decidable for a generating algorithm (\mathpzc{G}_{n}) if, for any n\in\mathbb{N} and any S\subseteq\euscr{X}^{n}, \mathsf{MOP}(\cdot) is decidable for the induced generator \mathpzc{G}=\mathpzc{G}_{n}(S). We note that the above definitions implicitly assume that the generator \mathpzc{G}_{n}(S) depends only on the randomness of S; we could extend this by allowing \mathpzc{G}_{n}(S) to be a distribution over generators. 1.1.2 Main Results We now have all the ingredients to state our first result, which establishes that, for all generating algorithms for which \mathsf{MOP}(\cdot) is decidable, (consistent) generation with breadth is as hard as language identification in the statistical setting. As in Definition 3, we will say that the generating algorithm (\mathpzc{G}_{n}) generates with breadth from \euscr{L} at some rate R(\cdot) if, for any K\in\euscr{L}, valid distribution \euscr{P}, and n\in\mathbb{N}, \operatornamewithlimits{\mathbb{E}}_{S\sim\euscr{P}^{n}}\mathds{1}\left\{% \operatorname{supp}(\mathpzc{G}_{n})\neq K\setminus S\right\}\leq C\cdot R(c% \cdot n)\,, for some distribution-dependent constants C,c>0. If no rate R(\cdot) satisfying \lim_{n\to\infty}R(n)=0 exists, we will say that (\mathpzc{G}_{n}) does not generate with breadth at any rate. Informal Theorem 1 (see Theorem 3.3). For every language collection \euscr{L} that is not identifiable in the limit, no generating algorithm (\mathpzc{G}_{n}), for which \mathsf{MOP}{}(\cdot) is decidable, can generate from \euscr{L} with breadth at any rate. Recall that the family of languages non-identifiable in the limit is quite broad. Based on the results of \citetgold1967language,angluin1979finding,angluin1980inductive on the problem of language identification in the limit, our impossibility result holds for most interesting collections of languages. For Informal Theorem 1 to be valuable and meaningful though, we further need to show that there exists an algorithm that generates without breadth for the collections of languages for which our impossibility result is true. Our next result states that this is indeed possible: there exists an algorithm that generates with (almost) exponential universal rates for any countable language collection \euscr{L}. Informal Theorem 2 (see Theorem 3.3). For every language collection \euscr{L} that is not identifiable in the limit, there exists a generating algorithm (\mathpzc{G}_{n}), for which \mathsf{MOP}(\cdot) is decidable, that generates (possibly) without breadth from \euscr{L} at exponential rates. Further, if \euscr{L} is identifiable in the limit, then there exists a generating algorithm (\mathpzc{G}_{n}), for which \mathsf{MOP}(\cdot) is decidable, that generates with breadth from \euscr{L} at (almost) exponential rates. Informal Theorem 2 shows that any countable collection of languages not only admits a consistent generator in the limit under an adversarial enumeration of the target language (as shown by \citetkleinberg2024language), but the statistical rate at which consistency (as per Definition 2) is achieved is exponential in the number of samples. Further, for identifiable collections of languages, we give an algorithm that generates with breadth at an (almost) exponential rate. The combination of Informal Theorem 1 and 2, reveals a strong separation between generation with and without breadth for any generating algorithm for which \mathsf{MOP}(\cdot) is decidable. What is missing is an answer to: how large is the class of generators for which the membership oracle problem \mathsf{MOP}(\cdot) is decidable? It turns out there is a very broad class of language generators for which this is the case and which also captures modern LLMs, as we show next. A Family of Generators for Which \mathsf{MOP}{}(\cdot) Is Decidable. Motivated by the structure of modern language models \citepbahl1983maximum,brown1990statistical,touvron2023llama,bubeck2023sparks,achiam2023gpt, we consider a family of iterative generators. A generator is said to be iterative if it generates text one alphabet or “token” at a time (see Definition 14). To generate each token, the generator can perform an arbitrary (but finite) amount of computation and, possibly, use randomness. For this to make sense, one has to imagine strings of \euscr{X} as strings over some finite alphabet \Sigma. This holds without loss of generality as \euscr{X} is countably infinite and, hence, there is a one-to-one mapping from \euscr{X} to strings over \Sigma (due to which \euscr{X} can be thought of as a set of strings over \Sigma).555 In a bit more detail, since \euscr{X} and \Sigma^{*} are countably infinite, they have enumerations x_{1},x_{2},\dots and s_{1},s_{2},\dots. Therefore, given any string s_{i}\in\Sigma^{*} generated by an iterative generator, one can map it to a string x_{i}\in\euscr{X}, thereby getting a generator for \euscr{X}. We show that for any iterative generator, the membership oracle problem is decidable and our Informal Theorem 1 is applicable. Informal Theorem 3 (see Theorem 3.4). For any iterative generator \mathpzc{G}, \mathsf{MOP}(\mathpzc{G}) is decidable. Observe that this family of next-token generators is very general. First, it captures existing large language models: for instance, to simulate an LLM L, we define the next-token predictor as a Turing machine that simulates L on the provided string until L generates one new token. Next, it also captures systems where an LLM can interact with another Generative AI model or algorithmic system (such as a diffusion model or a code interpreter) – as these auxiliary systems can also be simulated by the generator. Given this, it becomes evident that this class of generators for which \mathsf{MOP}{}(\cdot) is decidable is fairly large and interesting. Implications for the Gold-Angluin Model. We repeat that all the aforementioned results hold in the statistical setting. En route to obtaining our results in this setting (Informal Theorems 1 and 2), we show several connections to the online setting of \citetgold1967language, angluin1979finding,angluin1980inductive,kleinberg2024language, which lead to the following result. Informal Theorem 4 (see Theorem 3.5). For any language collection \euscr{L} that is not identifiable in the limit, no generating algorithm (\mathpzc{G}_{n}), for which \mathsf{MOP}{}(\cdot) is decidable, can generate from \euscr{L} with breadth in the limit. To be more concrete, a generating algorithm generates with breadth in the limit if its support is eventually K\setminus S_{n}, where S_{n} is the set of the first n positive examples (i.e., examples that belong to K). We emphasize that Informal Theorem 4 is in a similar spirit as our Informal Theorem 1, but holds in the online model instead of the statistical model discussed earlier. In particular, Informal Theorem 4 combined with the algorithm of \citetkleinberg2024language give a separation between consistent generation with and without breadth in the Gold-Angluin model. Further, as explained before, this result applies to any iterative generator due to Informal Theorem 3. Moreover, as \mathsf{MOP}{}(\cdot) is decidable for the generating algorithm of \citetkleinberg2024language (since its support contains a singleton element x which can be computed by running their algorithm), the above result, in particular, shows that the algorithm of \citetkleinberg2024language cannot generate with breadth in the limit from any non-identifiable collection. Organization of Rest of the Introduction. We proceed with an exposition of our techniques in order to obtain our main results presented above. In Section 1.3, we relax the definitions of consistency and breadth and give more “robust” trade-offs between hallucination and breadth. Next, in Section 1.4, we give a list of open problems for future work. Finally, Section 1.5 contains an extensive overview of related works. 1.2 Technical Overview In this section, we present the technical tools we develop to obtain our main results. A Natural Strategy to Prove Informal Theorem 1. At first glance, there seems to be a natural strategy to prove Informal Theorem 1: assume that there exists a consistent generating algorithm with breadth \mathpzc{G}=(\mathpzc{G}_{n}) for some non-identifiable collection \euscr{L} in the statistical setting and then show that this implies identification in the statistical setting, which would contradict the fact that \euscr{L} is non-identifiable. To implement this strategy one needs a method to utilize \mathpzc{G}, along with the positive samples from the target language K, for identification. This raises the question: what additional power can \mathpzc{G} give that the positive samples do not already provide? Initial Attempts to Implement the Strategy. Indeed, if one uses no additional properties of \mathpzc{G}, then its outputs provide no more information than an adversarial enumeration of K. To develop some intuition, we begin by considering some properties of the generator and explaining why they are insufficient to enable identification. 1. \mathpzc{G} is non-adaptive. First, one may want to utilize the fact that the generator \mathpzc{G}{} is fixed and, hence, the samples it outputs cannot adapt to the specific algorithm being used based on the outputs of the algorithm. Hence, it will probably provide an algorithm-independent enumeration of the true language. However, this is not helpful in general since there exist simple non-identifiable language collections that remain non-identifiable for many enumerations of the target language. 2. \mathpzc{G} samples from a fixed distribution. Another property one may want to leverage is the stochasticity of the generator: \mathpzc{G} samples its outputs from a fixed distribution (which is valid for K). However, even this does not enable the identification of non-identifiable collections due to a result by \citetangluin1988identifying. Angluin shows that even if the positive examples are i.i.d. from a valid distribution and do not appear as an adversarial enumeration (as in \citetgold1967language), this does not enable identification of any collection \euscr{L} that is non-identifiable in the limit. (We prove a stronger version of this result in Lemma 5.5.) 3. \mathpzc{G} samples from a simple distribution. Moreover, the difficulty in the above negative result is not the complexity of the encoded distribution: it holds even when \mathpzc{G} samples from a distribution that is computable by a Turing machine. At this point, it is not clear how to utilize access to a generator \mathpzc{G} which generates with breadth from K. Next, we present a strong form of access to the generator \mathpzc{G} that is useful for identification. 4. Access to Subset Queries “\operatorname{supp}(\mathpzc{G})\subseteq L_{i}” and “L_{i}\subseteq L_{j}”. For one of their algorithms, \citetkleinberg2024language utilize a subset oracle that answers queries of the form “is L_{i}\subseteq L_{j}?”. (In general, this oracle is not guaranteed to be computable). One can imagine an extension of this oracle that, given an index i and description of the generator \mathpzc{G}, outputs whether \operatorname{supp}(G)\subseteq L_{i}. The existence of this oracle turns out to be sufficient to identify K, as we explain next: After a finite amount of time, \citetkleinberg2024language’s algorithm creates a list of “critical” languages C_{1},C_{2},\dots, of the following form (see Theorem 4.1 in \citetkleinberg2024language) C_{1}\supseteq C_{2}\supseteq\dots\supseteq\left(C_{i}\coloneqq K\right)% \supseteq C_{i+1}\supseteq\dots\,. In words, this list has two properties (1) K appears in this list, say, at C_{i}=K for some i<\infty and (2) each language C_{j} in the list is a subset of the preceding language C_{j-1}. Given this list and the aforementioned subset oracle, one can easily identify the index of K as the largest j for which \operatorname{supp}(\mathpzc{G})=K\subseteq C_{j}. This assumption allows to identify any collection in the limit given access to a consistent generation \mathpzc{G} with breadth. However, this type of access is not very practical since it is not clear when such an oracle is implementable. Our Approach. Our first idea is that a much weaker form of access to \mathpzc{G} – membership oracle to \operatorname{supp}(\mathpzc{G}) – is sufficient for identification. This is where the membership oracle problem \mathsf{MOP}{}(\cdot) (Definition 5) appears in the proof. In fact, given this idea, it is not difficult to show that with that type of access, we can go from a generator with breadth in the online setting to an identification algorithm in the online setting; and, hence, get Informal Theorem 4. However, our focus is the statistical setting where there are several additional challenges in using the membership oracle to \operatorname{supp}(\mathpzc{G}). A. Need for Universal Rates for Generation and Identification. The key issue is the following: In the statistical setting, if we assume that we have a generator with breadth at rate R(\cdot), then we can hope to show an implication that we can get an identification algorithm at rate R(\cdot). However, this need not imply a contradiction to the identifiability of \euscr{L} as in the online setting. This is because, even though \euscr{L} is non-identifiable in the online setting, it may become identifiable at some rate R^{\prime}(\cdot) in the statistical setting. Indeed, this is the case in binary classification, where there are simple hypothesis classes (such as thresholds over reals) that are not learnable in Littlestone’s online setting \citeplittlestone1988learning but become learnable (at a universal – and uniform – linear rate) in the statistical setting; in fact, any hypothesis class is learnable in the statistical setting under universal rates, since there is a Bayes consistent algorithm, under benign assumptions \citepbousquet2021theory. Hence, to get a contradiction, we first need to understand the taxonomy of universal rates for generation and identification. We remark here that both the learning task (e.g., classification, regression, identification, and generation) and loss function used in the problem are pivotal for the landscape of rates that one gets; for instance, with the zero-one loss for binary classification one gets a trichotomy of rates \citepbousquet2021theory, but with the L_{1}-loss for regression, one gets infinitely many rates \citepattias2024universal. To overcome the above challenge, we provide statistical rates for identification and generation. We start with identification. We show that if \euscr{L} is identifiable in the limit in the adversarial Gold-Angluin setting with positive examples \citepgold1967language,angluin1980inductive, then it is identifiable under Definition 12 with (almost) exponential (universal) rates. This is the less technical part of the proof so we will give a high-level approach. B. Identification in the Limit \implies Identification at (Almost) Exponential Rates. Our idea is reminiscent of \citet*bousquet2021theory and requires splitting the input dataset into multiple batches whose size is carefully selected, running the online algorithm on each batch, and then taking a majority vote over the outputs of the algorithm. We remark that there are some technical issues which require further care, compared to \citetbousquet2021theory. First, unlike the setting of \citetbousquet2021theory, we only see positive examples and we get no feedback about our guesses. Thus, we cannot use their approach to “estimate” a time after which the learner will stop making mistakes. Moreover, when we run the learners on multiple batches, it can be the case that different batches output different indices of languages that correspond to K (since the target language can appear at multiple positions in the countable collection \euscr{L}). Thus, taking a majority vote over these indices might not work. Nevertheless, we manage to handle these issues and get almost exponential rates for collections that satisfy Angluin’s criterion for identification in the limit \citepangluin1979finding. A bit more concretely, to circumvent the first issue, our approach is to “guess” the right batch size, and this guess needs to be an increasing function of n – this is why we get almost exponential rates instead of exactly exponential rates (Lemma 5.5). The second issue is more subtle. At a high level, we use a voting scheme where the output of every batch \widehat{L}_{i} gives a “vote” to every language L\in\euscr{L} such that \widehat{L}_{i}=L, and we predict the lowest-indexed language that is voted by at least half of the batches. In its current form, this scheme is not computable, nevertheless, we show that it can be modified so that it becomes computable (Lemma 5.4). The more interesting half of establishing universal rates for identification is the lower bound showing that if a collection is not identifiable in the limit, it is also not identifiable in the statistical setting at any rate R(\cdot) such that \lim_{n\to\infty}R(n)=0. C. Impossible to Identify in the Limit \implies Impossible to Identify at Any Rate. Recall that the statistical setting was studied by \citetangluin1988identifying. \citetangluin1988identifying showed that every learner, with probability at least \nicefrac{{1}}{{3,}} does not converge to outputting a (stable) index of the target language in an infinite stream of examples drawn from a valid distribution. In other words, with probability at least \nicefrac{{1}}{{3,}} the algorithm will either stabilize to an index that does not correspond to the target language or it will not stabilize to any index. Notice that this does not rule out algorithms that output different indices of the target language, for all but finitely many n\in\mathbb{N}. The first step towards establishing our desired lower bound is to strengthen Angluin’s result: we show that any learning algorithm, with probability at least \nicefrac{{1}}{{3}}, outputs indices that do not correspond to the target language infinitely often. More formally, let us consider an identification algorithm h_{n}, which maps a training set of n examples x_{1},\dots,x_{n} to an index h_{n}(x_{1},\dots,x_{n}) so that L_{h_{n}(x_{1},\dots,x_{n})} is the n-th prediction for the true language. The aforementioned lower bound means that666 Informally, \limsup of a sequence of events captures the events that occur infinitely often. For instance, \Pr[\limsup_{n\to\infty}\mathscr{E}_{n}] represents the probability that infinitely many of the events \mathscr{E}_{1},\mathscr{E}_{2},\dots occur. On the other hand, \limsup_{n\to\infty}\Pr[\mathscr{E}_{n}], roughly speaking, denotes the largest value that the probabilities \Pr[\mathscr{E}_{1}],\Pr[\mathscr{E}_{2}],\dots,\dots approach infinitely often as n\to\infty. \Pr_{\left\{x_{i}\colon i\in\mathbb{N}\right\}\sim\euscr{P}^{\infty}}\left[% \limsup_{n\rightarrow\infty}\left\{L_{h_{n}\left(x_{1},\ldots,x_{n}\right)}% \neq K\right\}\right]\geq\frac{1}{3}\,, where X\sim\euscr{P}^{\infty} corresponds to an infinite i.i.d. draw from \euscr{P}. One may be tempted to conclude that this implies that with probability \nicefrac{{1}}{{3}} we cannot identify the target language (in the statistical setting). However, the quantity we wish to bound away from 0 to derive the desired lower bound is \limsup_{n\rightarrow\infty}\Pr_{x_{1},\ldots,x_{n}\sim\euscr{P}^{n}}\left[% \left\{L_{h_{n}\left(x_{1},\ldots,x_{n}\right)}\neq K\right\}\right]\,. It is well-known that for any sequence of events \{\mathscr{E}_{n}\}_{n\in\mathbb{N}}, \Pr\left[\limsup_{n\rightarrow\infty}\mathscr{E}_{n}\right]\geq\limsup_{n% \rightarrow\infty}\Pr[\mathscr{E}_{n}]\,. This, however, is not sufficient to deduce the result we need; we need the opposite inequality. Hence, Angluin’s guarantee does not suffice to get our lower bound. In order to show our result, we use a boosting argument (Lemma 5.8): if there exists a learner h_{n} whose probability of misidentification \Pr_{x_{1},\dots,x_{n}\sim\euscr{P}^{n}}\left[L_{h_{n}\left(x_{1},\dots,x_{n}% \right)}\neq K\right] converges to a number strictly less that \nicefrac{{1}}{{2}}, then we can convert it to a learner whose error rate decreases (almost) exponentially quickly. This (almost) exponential rate, in particular, implies that \sum_{n=1}^{\infty}\Pr_{x_{1},\ldots,x_{n}\sim\euscr{P}^{n}}\left[L_{h_{n}% \left(x_{1},\ldots,x_{n}\right)}\neq K\right]<\infty\,. This, crucially, enables us to use the Borel-Cantelli lemma (see Lemma E.1) which gives us that \Pr\left[\limsup_{n\rightarrow\infty}\left\{L_{h_{n}\left(x_{1},\ldots,x_{n}% \right)}\neq K\right\}\right]=0 , and, thus, a contradiction to Section 1.2. This implies the desired impossibility result. As consequence of the above results, we get a dichotomy for universal identification rates: Informal Theorem 5 (see Theorem 3.1). For any language collection \euscr{L} that is identifiable in the limit and for any g(n)=o(n), there exists a learner that identifies \euscr{L} at rate \exp(-g(n)). Otherwise, \euscr{L} is not identifiable at any rate. We remark that if we have access to subset queries for \euscr{L}, we can show that there exists an algorithm that achieves exactly exponential rates, for all identifiable collections (see Proposition 3.8). Next, we move to understanding universal rates for language generation. D. Universal Rates for Generation (Possibly Lacking Breadth) Without Boosting. One might suspect that a similar batching argument would give us exponential rates for generation: just run the online algorithm of \citetkleinberg2024language multiple times and aggregate. The issue is that aggregation for generation is different than prediction: for prediction, it is clear how to implement majority vote as a boosting technique; for generation, it is unclear how to aggregate different generated strings which is, typically, necessary to obtain a boosting algorithm. One immediate attempt is to take majority votes over the strings that each batch outputs; unfortunately, even if the majority of them are generating from the target language, they might be outputting different strings, thus, even a few batches outputting the same invalid strings are enough to fool our aggregation rule. Another tempting approach is to mimic the strategy we used to aggregate different indices of the target language in the identification setting: we go over every output of the batches and we let them give a vote to each of the languages in \euscr{L} they belong to.777The astute reader might realize that, as stated, this strategy is not computable – as we explain, even if one could implement it, this aggregation scheme does not work. It is not hard to see that every batch whose output corresponds to a valid generator will vote for the target language. Unfortunately, it will also vote for all supersets of the target language. This is exactly the heart of the difficulty of identification: telling apart supersets of the target language from the target language, which is colloquially called overgeneralization. Taking it to the extreme, imagine that the first language of the collection contains all the strings, i.e., L_{1}=\euscr{X}. Then, all the batches will vote for L_{1}. This is problematic for two reasons: generating a fresh string from the majority-voted language is as good as random guessing, and choosing a string among the ones that voted for the majority-voted language is as good as picking one of the outputs of all batches uniformly at random. Perhaps surprisingly, it turns out that a much simpler approach works: we show that the algorithm of \citetkleinberg2024language directly enjoys exponential rates in the statistical setting, without the use of batching and boosting. This observation is based on a sufficient condition that allows one to use an algorithm that works “in the limit” to obtain exponential rates in the statistical setting, without any modification (see Lemma 5.11). Informal Theorem 6 (see Theorem 3.2). For any countable language collection \euscr{L} there exists a generating algorithm that generates from \euscr{L} at an (optimal) exponential rate. This pair of results for identification (Informal Theorem 5) and generation (Informal Theorem 6) allow us to get Informal Theorem 1 and 2. The idea for Informal Theorem 1 is that we will use the algorithm \mathpzc{G} that generates with breadth at some rate R(\cdot) for an arbitrary non-identifiable collection \euscr{L} and membership oracle access to \mathpzc{G} in order to get an identification algorithm for \euscr{L} with some rate R^{\prime}(\cdot) such that \lim_{n\to\infty}R^{\prime}(n)=0. This is a contradiction since Informal Theorem 5 shows that \euscr{L} admits no rate in the universal setting. Finally, Informal Theorem 2 follows almost immediately from our universal rates result for generation. 1.3 Additional Results With Relaxation of Consistency and Breadth Next, we study a relaxation of consistent generation with breadth, which we call unambiguous generation, and ask: is there a generator that unambiguously generates from a non-identifiable collection? In this section, we will allow the generator to repeat examples in the training data. Like all of our results with breadth, this choice is not crucial, and all of the results have analogs where the generator does not repeat training examples (see Remark 2). We make this choice for simplicity. We show that unambiguous generation (which we define later in this section) from non-identifiable collections is impossible for any generator \mathpzc{G} for which \mathsf{MOP}{}(\mathpzc{G}) is decidable and that satisfies the natural property that \mathpzc{G} “stabilizes” after seeing sufficiently many examples: Definition 7 (Stability). A generating algorithm (\mathpzc{G}_{n}) is stable for a language collection \euscr{L} if for any target language K\in\euscr{L} and for any enumeration of K, there is some finite n^{*}\in\mathbb{N} such that for all n,n^{\prime}\geq n^{*}, it holds that \operatorname{supp}(\mathpzc{G}_{n})=\operatorname{supp}(\mathpzc{G}_{n^{% \prime}}). We make some initial remarks about stable generators. First, any generator \mathpzc{G} that is consistent and achieves breadth is also stable, since after some finite time its support, union the training set, becomes K and remains so. (Here, whether \mathpzc{G} repeats training examples or not is not crucial – the two types of generators are interchangeable; see Remark 2.) Second, this notion of stability can be seen as trying to capture practical heuristics such as learning rate schedules and early stopping that reduce the amount of changes to the generator as more and more samples are seen. Moreover, the original work of \citetgold1967language also requires the identifier to stabilize to a consistent guess, and, more recently, the stability property of learning algorithms was explored in the PEC learning setting of \citetmalliaris2022unstable. Having defined stability, we proceed to discuss relaxations of generation with breadth. Intuitively, consistent generation with breadth requires the generator to eventually stop making mistakes – where a mistake is any element x that \mathpzc{G} incorrectly includes (if x\not\in K or x is part of the training samples) or excludes (if x\in K) from its support. We now relax this and only require that, eventually, the generator \mathpzc{G} makes finitely many mistakes. Observe that this is a non-trivial requirement because the languages contain infinitely many strings and, so, at the start, \mathpzc{G} is expected to make infinitely many mistakes. A valuable observation is that it is possible for two languages L_{1} and L_{2} to only differ in finitely many strings even if each contains infinitely many strings. With this observation, it is not too hard to see that the aforementioned requirement is too weak to capture a reasonable notion of generation from the target language K. Indeed, it would allow generators that, given examples from K, perpetually generate outputs (with breadth) from a language L that is not the actual target language – which is a severe form of hallucination. Hence, to create a meaningful model, we must impose some further restrictions on the mistakes of the generator \mathpzc{G}. The above example motivates that, at the least, the generator \mathpzc{G} should be “closer” to generating from K than some language L\neq K with L\in\euscr{L}. We call such a generator unambiguous. Definition 8 (Unambiguous Generator). A generating algorithm \mathpzc{G}=(\mathpzc{G}_{n}) is unambiguous for a language collection \euscr{L} if, for any K\in\euscr{L} and every enumeration of K, its support eventually becomes closer to K than to any other language L\neq K in \euscr{L} in terms of the symmetric difference metric, i.e., there exists some n^{*}\in\mathbb{N} such that for all n\geq n^{*} it holds that \left|\operatorname{supp}(\mathpzc{G}_{n})\triangle K\right|<\min_{L\in\euscr{% L}\colon L\neq K}\left|\operatorname{supp}(\mathpzc{G}_{n})\triangle L\right|, where recall that for two sets S and T, S\triangle T\coloneqq\left(S\setminus T\right)\cup\left(T\setminus S\right). Figure 1: An Unambiguous Generator That neither Has Consistency nor Breadth. In this example, the language collection \euscr{L} has two languages L and K, where K denotes the target language. The red curve denotes L, the dashed green curve denotes K, and the blue curve denotes the support of \operatorname{supp}(\mathpzc{G}_{n}). The generator \mathpzc{G}_{n} hallucinates since \operatorname{supp}(\mathpzc{G}_{n})\setminus K\neq\emptyset and does not achieve breadth for the target K since B=K\setminus\operatorname{supp}(\mathpzc{G}_{n}) is non-empty. Nevertheless, this generator is unambiguous as \left|\operatorname{supp}(\mathpzc{G}_{n})\setminus K\right|+\left|B\right|<% \left|\operatorname{supp}(\mathpzc{G}_{n})\setminus L\right|+\left|A\right|. Here, we pause to observe that this notion of generation is a significant relaxation of generation with breadth that we considered earlier (Definition 4). Not only does it allow the generator to hallucinate certain strings not in the target K and omit strings actually in K for arbitrarily long, the number of hallucinations and omissions can be arbitrarily large, depending on the structure of the language collection \euscr{L}. Surprisingly, we show that even this very weak notion of “consistent generation with breadth” is not achievable by a large class of generators. Informal Theorem 7 (see Theorem 3.6). For every language collection \euscr{L} that is not identifiable in the limit, no stable generating algorithm \left(\mathpzc{G}_{n}\right) for which \mathsf{MOP}{}(\cdot) is decidable, can generate unambiguously from \euscr{L} at any rate. Thus, under mild conditions, no stable algorithm can generate unambiguously from a non-identifiable collection. Moreover, we also prove an analog of Informal Theorem 7 in the online setting (see Theorem 3.7), which extends our earlier result for generation with breadth in the online setting (Informal Theorem 4). This raises several questions regarding unambiguous generation, which we leave as interesting open problems (see Section 1.4). Note that while this impossibility result has a benign requirement that the generator is stable, it already considerably extends our main result Informal Theorem 1, since any generator that achieves breadth must be stable – otherwise, its support cannot settle on the target language K. (Note that while Informal Theorem 1 requires the generator to not repeat training examples, any generator that repeats training examples can be converted into one that does not repeat training examples and vice-versa; see Remark 2.) 1.4 Takeaways, Discussion, and Open Problems We believe that a key takeaway of our results is that the question of \citetkleinberg2024language seems to open an avenue towards a formal modern theory of language generation bridging learning theory and traditional TCS fields, like complexity theory and formal languages. As we explain in the subsequent technical overview, our tools contribute to this direction by connecting classical lines of work on identification of formal languages tracing back to \citetgold1967language, \citetangluin1979finding,angluin1980inductive,angluin1988identifying and computability theory \citepsipser2012introduction,soare1999recursively, to modern learning paradigms such as learning curves \citepbousquet2021theory and language generation \citepkleinberg2024language,kalai2024calibrated. Next, we emphasize that our impossibility result (Theorem 3.3) is not a dead end for language generation. Instead, it illustrates the need for additional human feedback during the post-training process – which provides additional information over positive samples alone – to achieve effective language models. Indeed, if both positive and negative examples are available, then generation with breadth is achievable for all countable collections of languages.888This follows from the work of \citetgold1967language, which showed that any countable collection of languages can be identified with such feedback. Using appropriate batching and boosting, we show that this identification algorithm (which works in the limit) can be converted to a generation algorithm with breadth that achieves an exponential rate. Concretely, Theorem 3.11 shows how to identify at an exponential rate and Proposition 6.5 shows how to convert this to a generation algorithm. In other words, our results can be seen as further theoretical evidence of the benefits of post-training with human feedback, highlighting its importance in developing language models that achieve both consistency and breadth, and adding to prior theoretical results from \citetkalai2024calibrated. Further, we underline that even though we focus on a prompt-less generation setting \citepkalai2024calibrated,kleinberg2024language, most of our results immediately extend to a prompted setting using the approach of \citetkleinberg2024language. Remarks and Open Questions. We now state a few remarks regarding our results and pose some interesting open questions. First, as a byproduct of our results, we establish almost tight rates for identification and generation with positive examples (see Section 3.1 and Section 3.4 for formal statements and discussion). Obtaining tight rates for these tasks is an interesting problem. Next, our impossibility results capture a large class of language-generating algorithms but do not completely forbid consistent generation with breadth. An immediate open question is how much further we can extend the class of generating algorithms for which the impossibility result in Informal Theorem 1 holds. Open Question 1. Is there a class of generative algorithms for which the induced generators can be modeled as Turing machines and which achieve breadth and consistency for all countable collections of languages? Further, we also proved a more robust version of our main result (Informal Theorem 1), namely, Informal Theorem 7, which showed that no algorithm from a large class of generators can generate while making a “small” number of hallucinations or omissions (also see Section 3.3 for another robust version of Informal Theorem 1). It is interesting to understand if one can prove a more robust version of Informal Theorem 1. To this end, we propose the following problem. Open Question 2. What is the Pareto frontier of an approximate notion of breadth and consistency? In other words, if we fix a collection of languages and allow the generator to hallucinate at some given rate, what is the minimal fraction of the mass from the target language that this generator has to miss? Next, to the best of our knowledge, it is not possible to test if a language collection is identifiable in the limit (without access to a strong oracle); this, for instance, becomes evident by inspecting Angluin’s criterion for identifiable collections (see Definition 10). Hence, we would like to know the following: Open Question 3. Is there a best-of-both-worlds algorithm between consistent generation and generation with breadth, i.e., is there an algorithm that will always generate in the limit from the target language consistently but, whenever identification is possible, it will also achieve breadth? We make some initial progress on this question by showing that the algorithm proposed by \citetkleinberg2024language already achieves this best-of-both worlds guarantee, provided it has access to a subset oracle for \euscr{L} that answers queries of the form “is L_{i}\subseteq L_{j}?” (see Section B.2). Finally, our algorithm that achieves (almost) exponential rates for identification uses an algorithm for identification in the limit as a black box. However, our algorithm that achieves exponential rates for generation makes use of certain specific properties of the algorithm of \citetkleinberg2024language. Thus, we ask the following question. Open Question 4. Is there a black-box transformation from an algorithm that generates in the limit in the online setting to an algorithm that generates with exactly exponential rates in the statistical setting? 1.5 Further Related Works Our setting is based on the statistical formulation of \citetangluin1988identifying, who studied identification from stochastic examples in the limit. However, \citetangluin1988identifying does not provide any learning rates which is one of the main aspects of our work. In terms of techniques, our inspiration for the statistical rates comes from universal learning, initiated by \citet*bousquet2021theory and studied in \citetbousquet2021theory,kalavasis2022multiclass,hanneke2022universal,hanneke2023universal,attias2024universal,bousquet2023fine. However, as we have already explained there are various differences between our setting and our techniques (we provide a more extensive and self-contained discussion in Section D). Our work connects various disjoint strands of research and we discuss each one of them below. Theory on Hallucinations. In terms of rigorous evidence about hallucinations in LLMs, we have already mentioned the work of \citetkalai2024calibrated at the start of Section 1. The result of \citetkalai2024calibrated is that calibrated999The exact definition of calibration is not important for this work: a language model is calibrated if, roughly speaking, the strings that the model assigns probability mass p, appear in a p fraction of the true distribution \citepdawid1982well. language models must hallucinate. The fascinating implication of this result is that one can lower bound the rate of hallucination, i.e., the quantity \operatornamewithlimits{\mathbb{E}}_{S\sim\euscr{P}^{n},~{}x\sim\mathpzc{G}_{n% }}\mathds{1}\left\{x\notin K\right\}, by the extent of a model’s calibration. Their intuition is that the root of hallucinations are rare patterns in the training data. Informally, their main result (under assumptions on K and \euscr{P}) is that for any trained model \mathpzc{G}_{n} with n samples, the hallucination rate \operatornamewithlimits{\mathbb{E}}_{S\sim\euscr{P}^{n},~{}x\sim\mathpzc{G}_{n% }}\mathds{1}\left\{x\notin K\right\}\geq\widehat{R}-\mathrm{Mis}_{\euscr{P}}(% \mathpzc{G}_{n})-\nicefrac{{1}}{{\sqrt{n}}}, where \widehat{R} is the fraction of facts that only appear once in the training data and \mathrm{Mis}_{\euscr{P}}(\mathpzc{G}_{n}) is the amount of miscalibration of the model. Hence, if the model is calibrated, i.e., \mathrm{Mis}_{\euscr{P}}(\mathpzc{G}_{n})\approx 0, the hallucination rate is lower bounded by the rare facts’ rate. Compared to our work, their goal is to show a quantitative lower bound, which is obtained under assumptions on the training distribution \euscr{P} and the fact that the model is calibrated. Our goal is different: we want to understand whether a model can achieve breadth while avoiding hallucinations building on the recent work of \citetkleinberg2024language. We also refer the reader to \citetkalai2024calibrated for an extensive overview of applied works on hallucinations. \citet peng2024limitations use communication complexity to prove that the transformer layer is incapable of composing functions if the domains of the functions are large enough. This work could also be seen as rigorous evidence about the hallucinations of LLMs since function composition is a fundamental task for reasoning \citepguan2024mitigating. The work of \citetxu2024hallucination is also studying hallucinations of LLMs. They define hallucination as a failure to identify the target function which belongs to an uncountable collection of functions. This is significantly stronger than the definition we and prior works \citepkalai2024calibrated,kleinberg2024language have considered (making their impossibility results significantly easier to prove). Their main result is that all LLMs must hallucinate. This is easy to see: consider an LLM learning to predict the next element in a sequence of 0s and 1s, after observing only a finite prefix of the enumeration, it has no way of knowing the next element in the order (since they allow both continuations) and, hence, the target sequence cannot be identified. Finally, the work of \citetaithal2024understanding, which is mainly empirical, aims to explain hallucinations on the other important family of generative models, namely diffusion-based models, via mode interpolation which, in theory, relies on difficulties in approximating non-smooth parts of the score function. Language Learning. In our results, we make no implicit assumption about the architecture of our models; this is in accordance with the works of \citetsolomonoff1964formal,gold1967language,angluin1982inference,angluin1983inductive,angluin1988identifying,pitt1989probabilistic,kleinberg2024language. However, there are various works aiming at understanding language learning capabilities of specific architectures, e.g., \citephahn2020theoretical,elman1990finding,gers2001lstm,bhattamishra2020ability,hewitt2020rnns,merrill2019sequential,merrill2023parallelism,yao2021self,ebrahimi2020can. For instance, \citetliu2022transformers show that low-depth transformers can represent the computations of any finite-state automaton, while \citetsanford2024representational identify a particular mathematical problem that cannot be computed by single-layer multi-head transformers. The aforementioned works share some similarities with us in the sense that they focus on whether models can be trained to generate or recognize strings in a fixed formal language. \citetakyurek2024context study in-context language learning: the language model is prompted with a finite collection of strings from an unknown regular language (which changes across different tasks), and must infer the distribution over strings corresponding to the full language. In a similar spirit, \citetedelman2024evolution study in-context learning of Markov chains. Other related works are those of \citethahn2023theory,xie2021explanation that study conditions under which in-context learning can arise for language learning. \citet allen2023physics design context-free grammars and empirically study the consistent generation (accuracy) and breadth (diversity) of GPT models on these synthetic examples. In comparison to this work, we provide a theoretical treatment of the trade-off between consistency and breadth under a very abstract model, studied by \citetgold1967language,angluin1979finding,angluin1988identifying, kleinberg2024language. Our results indicate that, even in a very idealized framework, achieving (perfect) consistency and breadth is impossible. We view the empirical findings of \citetallen2023physics as an exciting indication that, in the real world (or more concretely in controlled experiments on “small” models and synthetic datasets), a balance between (imperfect) consistency and breadth is possible and modern LLMs can achieve it. Further understanding how much consistency and breadth one can achieve at the same time theoretically is an exciting direction. Finally, in a concurrent and independent work, \citetli2024generationlenslearningtheory also study language generation, interpreting it in a learning-theoretic setting reminiscent of the PAC framework and the online learning setting of \citetlittlestone1988learning. They propose “non-uniform generatability” – which relaxes “uniform generatability” \citepkleinberg2024language – and characterize the collections for which uniform and non-uniform generatability are achievable in the Gold-Angluin model; in particular, unlike \citetkleinberg2024language they also allow the collection \euscr{L} to contain uncountably many languages. These dimensions are analogs to the Littlestone dimension (and its extension to the non-uniform setting \citeplu2023non), which only holds for finite collections of languages. Moreover, they show the proposed dimension is incomparable to the VC dimension. Finally, they give analogous characterizations in the “prompted generation” setting, extending some of the results of \citetkleinberg2024language. Our work is orthogonal to theirs: first, we study trade-offs between generating with and without breadth – both in a statistical setting and the Gold-Angluin model – and, second, we study the “learning curves” for generation and identification in the framework of \citetbousquet2021theory. Probably Eventually Correct Learning. As we mentioned Gold’s model is a predecessor of the famous PAC model of \citetvapnik2013nature and \citetvaliant1984theory. A natural question is whether there is a conceptual meeting point for the two works. Is there a notion of “PAC learning in the limit?” The answer to this question is affirmative and comes from the field of algorithmic stability (see e.g., \citepalon2022private,moran2023bayesian,kalavasis2023statistical,bun2023stability,chase2023stability and the references therein), studied in the context of binary classification \citepmalliaris2022unstable. \citet malliaris2022unstable introduce the Probably Eventually Correct (PEC) model of learning. Here we fix a collection \euscr{L}=\{L_{1},L_{2},\dots\} of languages and a distribution \euscr{P} over positive and negative labeled examples (in contrast to the standard identification setting of Gold). PEC learning focuses on distributions \euscr{P} realizable by the collection \euscr{L} in the sense of \citetbousquet2021theory (see Section D). An algorithm is said to PEC learn \euscr{L} if for any realizable distribution \euscr{P}, with probability 1 over i.i.d. samples \left\{(x_{i},y_{i})\colon i\in\mathbb{N}\right\} drawn from \euscr{P}, there exists time t^{*}\in\mathbb{N} such that for all t\geq t^{*}, given \left\{(x_{i},y_{i})\colon 1\leq i\leq t\right\}, the algorithm outputs an L_{t}\in\euscr{L} such that \Pr_{(x,y)\sim\euscr{P}}[L_{t}(x)\neq y]=0\,. Malliaris and Moran give a combinatorial characterization of the collections of languages that are PEC learnable: a collection of languages \euscr{L} is PEC learnable if and only if it does not shatter an infinite Littlestone tree. We stress that, when the learner has access to positive and negative examples, the absence of an infinite Littlestone tree does not characterize identification in our setting. This is in stark contrast with binary classification. In particular, in Section D, we show that there exists a set of languages that have an infinite Littlestone tree, hence not learnable in the online setting of \citetbousquet2021theory, but it allows for identification in the limit with positive and negative examples. In fact, the collection we use in Example 3 is identifiable in the limit even with just positive examples. This already sets the stage for a starkly different landscape of optimal learning rates between the setting of \citetbousquet2021theory and \citetangluin1988identifying, as we will see in Section 3.1. As we said before, the online model of \citetgold1967language and the classical online setting of \citetlittlestone1988learning have various differences. \citetlu2023non studies non-uniform online learning in order to bridge the gaps between the inductive inference model of \citetgold1967language and classical online learning. In this setting, the adversary is oblivious and fixes the true language K in advance (as in Gold’s model). At each round, an example from K is revealed, the learner makes a prediction but then she observes feedback. The model is non-uniform in the sense that the mistake bound depends on K. Learning from Positive Examples. Learning from positive examples occurs very frequently in real-world applications and has been extensively studied. A lot of work has been done on learning from positive examples in Gold’s model of learning in the limit \citepgold1967language,angluin1980inductive,angluin1988identifying,berwick1986learning,shinohara1989inductive,zeugmann2005guided. Apart from that, an extension of Valiant’s PAC model has been also studied \citepnatarajan1987learning,denis1998pac. \citetnatarajan1987learning considered the setting where the learner only has access to positive examples and showed that even very simple classes such as halfspaces in two dimensions are not learnable from positive examples alone. \citetdenis1998pac relaxed this requirement: they study a setting where the learner has access to both positively labeled examples but also to unlabeled examples \citepdenis2005learning. At the heart of virtually all of the results in this line of work is the use of unlabeled samples in order to generate negative examples. When the original distribution is uniform, better algorithms are known: \citetde2014learning gave efficient learning algorithms for DNFs and LTFs, \citetfrieze1996learning,anderson2013efficient gave efficient learning algorithms for learning d-dimensional simplices. On the other side, \citeteldan2011polynomial,goyal2009learning give lower bounds for learning with positive examples. Recently, interest in learning from positive examples has sparked from work on truncated statistics (e.g., \citepdaskalakis2018efficient,daskalakis2019computationally,Kontonis2019EfficientTS,fotakis2020efficient,daskalakis2021statistical,de2023testing,de2024detecting,plevrakis2021learning,de2024detecting,diakonikolas2024statistical,lee2024efficient). \citetKontonis2019EfficientTS show how to learn concept classes of bounded Gaussian surface area from positive Gaussian examples and \citetlee2024efficient generalize this to show how to learn concept classes approximable by polynomials in the L_{2}-norm from positive examples. However, all these works focus on computationally efficient learning/testing while we focus on statistical consistency of identification and generation without any restrictions on computation time."
https://arxiv.org/html/2411.09627v1,"One-Shot Manipulation Strategy Learning
by Making Contact Analogies","We present a novel approach, magic (manipulation analogies for generalizable intelligent contacts), for one-shot learning of manipulation strategies with fast and extensive generalization to novel objects. By leveraging a reference action trajectory, magic effectively identifies similar contact points and sequences of actions on novel objects to replicate a demonstrated strategy, such as using different hooks to retrieve distant objects of different shapes and sizes. Our method is based on a two-stage contact-point matching process that combines global shape matching using pretrained neural features with local curvature analysis to ensure precise and physically plausible contact points. We experiment with three tasks including scooping, hanging, and hooking objects. magic demonstrates superior performance over existing methods, achieving significant improvements in runtime speed and generalization to different object categories. Website: https://magic-2024.github.io/.","A hallmark of human intelligence is flexible tool use: humans can quickly acquire new manipulation “strategies” from just a handful of demonstrations and apply these strategies across various scenarios, including generalization to novel objects of unseen categories. For example, as illustrated in Fig. 1, even from a single demonstration of using a hook to reach distant objects or putting hangers on a rod, we can generalize to different object positions, sizes, and diverse categories, such as hangers and mugs. Traditionally, two main approaches have been widely studied to build machines that can flexibly use tools: model-based and analytic approaches which take novel scenarios and goals and use built-in physical models to compute plans [1, 2, 3, 4], and policy learning, which leverages various types of priors (e.g., object-based and part-based models) and pretrained neural features for generalization [5, 6, 7, 8]. However, both approaches have their limitations. Model-based planning generalizes well given accurate object and physical models. However, it is slow and usually does not benefit from learning. Policy learning approaches, on the other hand, are very efficient at performance time but usually exhibit limited generalization to novel objects and scenarios, particularly when the shape of the novel objects differs significantly from objects seen during training, such as generalizing from hangers to mugs. In this paper, we present a novel approach, magic (manipulation analogies for generalizable intelligent contacts), for one-shot manipulation strategy learning. Shown in Fig. 1, given a single reference action trajectory (e.g., using a hook to reach for a distant object) and a novel scenario (e.g., with different tools and different objects), the goal of the algorithm is to generate a sequence of robot actions that apply a “similar” strategy to the test objects specified by users: in this example, having the target object moving along a certain direction for a given distance. magic extends two critical insights into a broad class of manipulation strategies. First, many strategies such as hooking, hanging, hammering, pushing, reaching [9, 10], stacking, pouring [11, 12, 13], and cutting [13] can be characterized by a sequence of contact waypoints (i.e., the order in which contacts between objects and robot bodies are made); second, these contacts are characterized by forceful affordances between object pairs: a specific pair of contact points on two objects would enable the application of forces along certain directions. However, searching for contact points that would enable the specific affordance is generally challenging due to complex constraints on reachability, collision avoidance, and motion stability. Figure 1: We introduce magic (Manipulation Analogies for Generalizable Intelligent Contacts), a pipeline that is capable of learning manipulation strategies from single demonstrations and applying them to novel objects. magic tackles these challenges by combining data-driven and analytic approaches to generate contact waypoints in novel scenarios. In particular, it first extracts the sequence of contacts among objects in the reference trajectory and then proposes (pairs of) contact points that have similar global and local shape properties as the contact points in the reference, which can be used as guidance for motion planning or motion retargeting. Finally, it utilizes a physical simulator to discard trajectories that fail to achieve the goal due to collisions, unstable physical contact, or violations of joint and torque limits. Our key innovation lies in a novel global-to-local matching algorithm to find functional correspondences between the target objects and reference objects. Intuitively, a “good” contact point would satisfy both a global and a local matching property. First, the points on two objects should be on similar parts of the global shape (e.g., in the hook-using example, we need a contact point on the tool that is at the end of a long rod). Second, the hooking contact point should have a matched local curvature with the target object being hooked so that we can execute the actions stably. Therefore, we propose to use a pretrained visual feature-based correspondence matching to resolve the global matching property. This enables us to quickly search over different parts of the objects but the resulting contact point is usually not precise. Next, we use a local curvature-based matching algorithm to find the best contact point within a local region of the previously proposed contact point, which gives us precise and physically plausible (e.g., collision-free and physically stable) contacts. Overall, magic tackles the problem of one-shot manipulation strategy learning by making analogies in contact waypoints. We validate the effectiveness of our approach on three challenging tasks: scooping a ball against a concave arc with a spoon, hanging a mug onto a mug tree, and using tools to hook objects of varying sizes. Compared to global shape-matching algorithms, our framework achieves significant improvements when the reference objects are from different categories than the test objects. Compared to local shape-matching and simulation-based approaches, our framework is orders of magnitude faster — for most test objects, we need to run simulations on fewer than three candidate contact points to find a solution. Finally, compared to pretrained feature-matching-based approaches, our method finds more precise and physically plausible solutions."
https://arxiv.org/html/2411.09623v1,Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups,"This paper addresses the challenges of vision-based manipulation for autonomous cutting and unpacking of transparent plastic bags in industrial setups, aligning with the Industry 4.0 paradigm. Industry 4.0, driven by data, connectivity, analytics, and robotics, promises enhanced accessibility and sustainability throughout the value chain. The integration of autonomous systems, including collaborative robots (cobots), into industrial processes is pivotal for efficiency and safety. The proposed solution employs advanced Machine Learning algorithms, particularly Convolutional Neural Networks (CNNs), to identify transparent plastic bags under varying lighting and background conditions. Tracking algorithms and depth sensing technologies are utilized for 3D spatial awareness during pick and placement. The system addresses challenges in grasping and manipulation, considering optimal points, compliance control with vacuum gripping technology, and real-time automation for safe interaction in dynamic environments. The system’s successful testing and validation in the lab with the FRANKA robot arm, showcases its potential for widespread industrial applications, while demonstrating effectiveness in automating the unpacking and cutting of transparent plastic bags for an 8-stack bulk-loader based on specific requirements and rigorous testing.","I INTRODUCTION Industry 4.0—also called the Fourth Industrial Revolution or 4IR—is the next phase in the digitization of the manufacturing sector, driven by disruptive trends including the rise of data and connectivity, analytics, human-machine interaction, and improvements in robotics [1, 2]. This could make products and services more easily accessible and transmissible for businesses, consumers, and stakeholders all along the value chain [3]. Preliminary data indicate that successfully scaling 4IR technology makes supply chains more efficient and sustainable [4], creates a safer and more productive environment for the employees, reduces occupational accidents and factory waste, and has countless other benefits. Autonomous manipulation of plastic packages in industrial setups typically involves the use of robotic systems and automation technologies [5]. These systems are designed to handle, move, and manipulate plastic packages in a variety of industrial processes, such as packaging, recycling and sorting, food processing, and quality control [6, 7]. Collaborative robots, or cobots, are widely used in various industrial applications, working alongside humans without needing extensive safety barriers, cages, or other restrictive measures [8]. These robots use different sensors to identify their environment, recognise objects and are programmed for better accessibility, flexibility and repeatability. Example cases can be found in the textile industry as described in [9], where the authors proposed a dual arm collaborative system for textile material identification. By imitating human behavior, in this work the robots use actions such as pulling and twisting to identify and learn more about textile properties. In recent years, the recycling and waste management industry has begun to use vision-based robotic systems for the classification and accurate sorting of waste materials [10]. Indicative examples can be found in different recycling industries for the management of construction waste [11, 12], recyclable materials [13, 14] or electronic parts [15, 16]. The vision-based manipulation and autonomous cutting of transparent plastic bags presents a set of intricate challenges and a compelling need for innovative AI solutions [17, 18]. The inherent transparency of the bags poses difficulties in accurate detection due to the reflection and refraction of light, demanding sophisticated computer vision algorithms for reliable identification [19]. The deformable nature of plastic bags adds complexity to the grasping and manipulation process, necessitating advanced robotic control strategies to handle their variability [20]. Additionally, autonomous cutting requires well-considered mechanical design and precise vision-guided tools to discern optimal cutting points while avoiding unintended damage. Ensuring the safety and efficiency of these systems in real-time, dynamic environments further amplifies the challenge. The pressing need for such technologies arises from the increasing demand for automated waste management, recycling, and packaging processes, where vision-based systems can enhance efficiency, reduce human intervention, and contribute to sustainable practices by facilitating the effective processing of transparent plastic bags [21]. In this work, through the use of advanced Machine Learning algorithms, based on Convolutional Neural Networks (CNNs), the system can identify transparent plastic bags within its visual field, taking into account variations in lighting and background. Once the bags are detected, the system utilizes tracking algorithms to follow the pick and placement of the bags, and, integrate depth sensing technologies for 3D spatial awareness. The next steps involve developing algorithms for robotic grasping and manipulation, accounting for the challenges posed by the deformable and transparent nature of plastic bags. This includes considerations for optimal grasping points, compliance control using vacuum gripping technology, and real-time automation and processing to ensure effective and safe interaction with the bags in dynamic environments. The rest of the paper is organized as follows. Section II describes the mechanical design of the proposed system. Section III presents the object detection and manipulation approach based on deep-learning and Section IV presents the autonomous cutting mechanism and the automation process. The testing of the pilot proof-of-concept prototype is presented in Section V. Finally, the last section discusses the obtained results and highlights directions for future work."
https://arxiv.org/html/2411.09613v1,"PTR: recision-Driven ool ecommendation 
for Large Language Models","By augmenting Large Language Models (LLMs) with external tools, their capacity to solve complex problems has been significantly enhanced. However, despite ongoing advancements in the parsing capabilities of LLMs, incorporating all available tools simultaneously in the prompt remains impractical due to the vast number of external tools. Consequently, it is essential to provide LLMs with a precise set of tools tailored to the specific task, considering both quantity and quality. Current tool retrieval methods primarily focus on refining the ranking list of tools and directly packaging a fixed number of top-ranked tools as the tool set. However, these approaches often fail to equip LLMs with the optimal set of tools prior to execution, since the optimal number of tools for different tasks could be different, resulting in inefficiencies such as redundant or unsuitable tools, which impede immediate access to the most relevant tools. This paper addresses the challenge of recommending precise toolsets for LLMs. We introduce the problem of tool recommendation, define its scope, and propose a novel Precision-driven Tool Recommendation (PTR) approach. PTR captures an initial, concise set of tools by leveraging historical tool bundle usage and dynamically adjusts the tool set by performing tool matching, culminating in a multi-view-based tool addition. Additionally, we present a new dataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness of tool recommendation for LLMs. We further validate our design choices through comprehensive experiments, demonstrating promising accuracy across two open benchmarks and our RecTools dataset.","Large Language Models (LLMs) have established themselves as powerful intermediaries, demonstrating remarkable impacts across a variety of downstream tasks, including text generation, code debugging, and personalized recommendations (Brown et al., 2020; Touvron et al., 2023; Nam et al., 2024; Chen et al., 2024; Zhao et al., 2024). However, as these models continue to evolve, they still struggle to solve highly complex problems due to limitations arising from their pre-training data (Mialon et al., 2023; Mallen et al., 2022; Yuan et al., 2023). To expand the potential of LLMs in managing more complex tasks efficiently, recommendations at various levels have been increasingly applied to LLMs. Typically, memory recommendations (Borgeaud et al., 2022) and knowledge-based recommendations (Gao et al., 2023; Hu et al., 2023) enhance consistency and context awareness in ongoing tasks for LLMs, while data augmentation recommendations (Xu et al., 2020) facilitate the inclusion of additional data to augment training. Furthermore, architecture recommendations (Elsken et al., 2019; Fedus et al., 2022) and prompt recommendations (Shin et al., 2020; Pryzant et al., 2023; Liu et al., 2023) optimize efficiency and generate more relevant outputs. Simultaneously, to reduce the cognitive load on LLMs and enhance their complex problem-solving capabilities by enabling actions beyond natural language processing, it is crucial to augment LLMs with recommendations of optimal external tool sets, an aspect currently lacking in existing recommendation frameworks for LLMs. Furthermore, this approach will be helpful to address the challenge of input length limitations encountered when incorporating a large number of external tools into the prompt. Providing LLMs with a precise and dynamically adaptable recommended toolset can help to enhance the effectiveness of LLM’s task-solving ability. Figure 1: Tool retrieval often provides a broad and variable number of tools with inconsistent quality, whereas tool recommendation delivers a precise, high-quality set of tools directly. Considering that the capability of LLMs to master and control external tools is instrumental in overcoming some of their fundamental weaknesses, the field of tool retrieval—which aims to identify the top-K most suitable tools for a given query from a vast set of tools—has been increasingly explored. The advent of tool retrieval (Zhuang et al., 2023; Li et al., 2023; Tang et al., 2023; Yang et al., 2024) signifies a nuanced evolution, most directly employing term-based methods (Sparck Jones, 1972; Robertson et al., 2009) or semantic-based techniques (Kong et al., 2023; Yuan et al., 2024; Gao et al., 2024). Generally, the primary objective of these methods is to refine the ranked list of tools and subsequently select a fixed number of tools from the top (top-K) (Qu et al., 2024a; Zheng et al., 2024; Qu et al., 2024b). Although such approaches have demonstrated good performance when retrieving a single tool (Patil et al., 2023; Xu et al., 2023) or a small number of tools (generally fewer than three) (Qin et al., 2023; Huang et al., 2023), they remain susceptible to under-selection or over-selection, as illustrated in Figure.1. This limitation may prevent LLMs from addressing the current query or cause them to over-interpret the query, thereby reducing the effectiveness of LLMs in solving complex problems with external tools. Additionally, the validation of these methods often relies on datasets that use a fixed number of tools for each query, meaning that during testing, the number of tools to be used is known in advance—an unrealistic scenario in practical applications where the number of tools needed can vary dynamically. Therefore, recommending a precise and dynamically adjustable set of external tools to LLMs in a single step prior to query execution is increasingly important. This approach not only enhances the thoroughness of problem-solving but also improves efficiency by reducing the need to execute additional tools. To address these limitations, we first provide a comprehensive explanation of tool recommendation and clearly define the problem, considering the lack of definition and the incompleteness of goals pursued by existing tool retrieval methods. Toward this objective, we propose PTR, a novel model-agnostic Precision-Driven Tool Recommendation approach aimed at recommending a precise tool set for LLMs prior to query execution. By leveraging historical tool bundle usage data to uncover patterns of idiomatic use and dependencies between tools, this method is structured into three main stages: Tool Bundle Acquisition, Functional Coverage Mapping, and Multi-view-based Re-ranking. Initially, using traditional pre-trained language models, we acquire semantic matching information between queries and previously used tool bundles, thereby addressing potential performance issues of these models in zero-shot scenarios for tool recommendation tasks. Subsequently, to evaluate the effectiveness of the selected tool bundle in solving the query, LLMs are prompted to match tools with the specific subproblems they can address and to identify unresolved issues. Based on this, a multi-view-based re-ranking method is employed to select tools that can help resolve the identified issues and complement the existing tool sets. More specifically, to address the unresolved issues, we construct the final ranked list by aggregating three tool lists and ranking each tool based on their frequency of occurrence. The ranked tool list, constructed from multiple views, reduces the randomness associated with selecting tools from the entire available set. Additionally, we construct a dataset, RecTools, tailored to specific queries with recommended tool sets. In contrast to previous tool datasets that standardize the number of tools used for each query (Huang et al., 2023) or employ a small number of tools (Qu et al., 2024a), our tool recommendation set incorporates varying numbers of tools for different queries, with up to ten tools used for a single query. This is achieved through an automated process in which LLMs are prompted to generate specific queries to be addressed by given tool bundles. These queries and tool bundles are subsequently evaluated by prompting LLMs to determine whether the selected tools adequately address the corresponding queries, ensuring that neither excess nor insufficient tools are utilized. Dedicated validation and deduplication steps are implemented to ensure the precision of tool usage, thereby enhancing the quality of the tool recommendation set. Furthermore, traditional retrieval metrics such as Recall (Zhu, 2004) and Normalized Discounted Cumulative Gain (NDCG) (Järvelin & Kekäläinen, 2002), fail to capture the level of precision required for effective tool recommendation. The absence of necessary tools can lead to the failure of LLMs in performing tasks, while the redundancy of tools may cause LLMs to generate unnecessary responses. This indicates that metrics focusing solely on completeness are inadequate for evaluating tool recommendation tasks. To bridge this gap, we introduce TRACC, a novel metric designed to assess tool recommendation performance, considering both the accuracy of the quantity and the quality of the recommended tools. TRACC serves as a reliable indicator of the effectiveness of tool recommendation processes. To summarize, the main contributions of this work are as follows: • We introduce tool recommendation as a novel problem, necessitating the provision of precise tool sets to LLMs for a given query. We propose PTR, an effective tool recommendation approach that leverages historical tool bundle information between queries and tools, resulting in a more accurate and comprehensive final recommended tool list. • We present a new dataset, RecTools, and an effective evaluation metric, TRACC, specifically designed to assess tool recommendation for LLMs. This not only addresses gaps in existing tool sets but also advances future research related to tool recommendation. • Extensive experiments validate the effectiveness of RecTools and demonstrate the efficacy of PTR in recommending tools for LLMs. The recommended tool sets are both comprehensive and accurate, enhancing the overall performance of LLMs in processing tasks."
https://arxiv.org/html/2411.09604v1,Local-Global Attention: An Adaptive Mechanism for Multi-Scale Feature Integration,"In recent years, attention mechanisms have significantly enhanced the performance of object detection by focusing on key feature information. However, prevalent methods still encounter difficulties in effectively balancing local and global features. This imbalance hampers their ability to capture both fine-grained details and broader contextual information—two critical elements for achieving accurate object detection. To address these challenges, we propose a novel attention mechanism, termed Local-Global Attention, which is designed to better integrate both local and global contextual features. Specifically, our approach combines multi-scale convolutions with positional encoding, enabling the model to focus on local details while concurrently considering the broader global context. Additionally, we introduce learnable \alpha parameters, which allow the model to dynamically adjust the relative importance of local and global attention, depending on the specific requirements of the task, thereby optimizing feature representations across multiple scales. We have thoroughly evaluated the Local-Global Attention mechanism on several widely used object detection and classification datasets. Our experimental results demonstrate that this approach significantly enhances the detection of objects at various scales, with particularly strong performance on multi-class and small object detection tasks. In comparison to existing attention mechanisms, Local-Global Attention consistently outperforms them across several key metrics, all while maintaining computational efficiency. Code is available at the link.","In recent years, significant progress has been made in the field of object detection, with many methods achieving remarkable improvements in performance metrics [8, 17, 20]. Nevertheless, researchers continue to explore new approaches to further enhance accuracy and efficiency, especially in challenging scenarios such as multi-class and small object detection. Attention mechanisms have emerged as an effective means to improve model performance [25, 12, 27, 28, 36], gaining considerable attention in deep learning due to their ability to significantly boost performance with only a limited increase in computational cost. Among widely-used attention mechanisms, local and global attention are particularly noteworthy [19]. Local attention focuses on fine-grained, localized details within the input, capturing essential local information [33]. In contrast, global attention emphasizes the overall content and global context of the input, which is crucial for understanding broader relationships and larger patterns [1, 20]. Despite their strengths, both types of attention have inherent limitations: local attention often overlooks global dependencies, while global attention sacrifices detailed local information. Although many researchers have attempted to combine local and global features, these efforts often face challenges in effectively balancing the two, resulting in either suboptimal model performance or significant computational overhead that offsets performance gains [29, 35, 18, 32]. In this paper, we propose a novel attention mechanism called Local-Global Attention, designed to balance local and global features by integrating multi-scale convolution and positional encoding. This enables the model to capture both local details and global context. Additionally, we introduce learnable \alpha parameters, allowing the model to dynamically adjust the balance between local and global attention in a data-driven manner, achieving consistent improvements across various datasets. Overall, Local-Global Attention ensures optimized feature representation across different scales, enhancing detection performance while maintaining low computational costs. Specifically, the implementation of the Local-Global Attention mechanism is as follows: After initial feature extraction, we apply multi-scale convolutions with smaller kernels to capture localized features, which are then aggregated to form a local attention map. In parallel, larger kernel convolutions combined with positional encoding are used to extract broader global features, generating a global attention map. These two attention maps are then fused using learnable \alpha parameters, creating a unified attention map, which is subsequently passed through additional network layers for further refinement and processing. This approach provides several key advantages. Firstly, it captures both fine-grained and large-scale information, offering a more comprehensive understanding of the input, which enables more accurate localization and identification of targets. Secondly, the mechanism is lightweight and modular, making it easy to integrate into a variety of existing network architectures, such as MobileNetV3 [10] and ResNet [9]. By prioritizing informative features and filtering out irrelevant data, Local-Global Attention significantly enhances detection accuracy in detection tasks, as shown in Figure 1. To validate the effectiveness of our method, we conducted extensive experiments on multiple benchmark datasets, including VOC2007 [6], VOC2012 [7], VisDrone2019-DET [37], TinyPerson [34], COCO2017 [16], GWHD2020 [3], COCO minitrain [23], DOTA-v1.0 [30], as well as MNIST [4] and Fashion-MNIST [31]. The results demonstrate that Local-Global Attention outperforms existing attention mechanisms with similar computational requirements, consistently improving the model’s detection accuracy. In conclusion, we believe that Local-Global Attention is a practical and efficient solution that addresses some limitations of existing attention mechanisms, offering a flexible approach that moves closer to more accurate and computationally feasible object detection models. Figure 1: The mAP@50 and mAP@50:95 results on the COCO2017 [16] and VOC2007 [6] datasets compare the performance of MobileNetV3 [10] with its enhanced version using the Local-Global Attention mechanism. On COCO2017 [16], all models were trained for 20 epochs using the Adam optimizer, while on VOC2007 [6], training was conducted for 200 epochs with the AdamW optimizer. In both cases, other settings followed the YOLOv8 [14] default configuration."
https://arxiv.org/html/2411.09595v1,LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models,"This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. A primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce Llama-Mesh, a novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct a supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in a text-based format, effectively unifying the 3D and text modalities. Llama-Mesh achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance. ††footnotemark: 11footnotetext: Work completed during NVIDIA internship.","Large Language Models (LLMs) [52, 5] have demonstrated remarkable capabilities in understanding and generating human-like text, achieving success in applications such as conversational agents, code generation, and visual content reasoning [1, 16, 30]. Despite these advances, their generative abilities have primarily been limited to the textual content, restricting their utility for broader tasks. Our work seeks to extend LLMs into a new modality—3D mesh generation—unlocking significant potential for fields like computer graphics, engineering, robotics, and virtual/augmented reality. By enabling LLMs to generate 3D meshes from textual descriptions, we unify language understanding with 3D content creation, expanding the functional scope of LLMs. This approach paves the way for more intuitive and efficient workflows in 3D content creation driven by language-based instructions. However, integrating a new modality into an LLM is challenging, particularly in the tokenization process for processing the new modality. To the best of our knowledge, there have been no attempts to unify 3D mesh and text generation in a single framework. Some studies explored unifying image and text generation. Among these works [37, 55], a common approach is to train a new tokenizer such as a vector-quantized variational autoencoder (VQ-VAE) [51, 17] to encode the new modality into discrete tokens, which are used in training. However, this requires vocabulary expansion, increasing the adaptation’s learning cost. Additionally, this method introduces information loss during the auto-encoding process. To tackle these challenges, we introduce Llama-Mesh, a novel framework that enables large language models (LLMs) to generate 3D meshes by representing them as plain text. Our approach uses the OBJ file format, a widely adopted text-based standard for 3D models comprising vertex coordinates and face definitions, as shown in Figure 4. By treating these numerical values as a sequence of text, we convert 3D meshes into a format that LLMs can process directly, avoiding modifications to the tokenizer or the vocabulary, thus minimizing additional training overhead. This design capitalizes on the extensive knowledge embedded in pretrained LLMs. Figure 6 shows pretrained LLMs demonstrate a native ability to represent 3D structures in text — a capability our framework harnesses. We construct a supervised fine-tuning (SFT) dataset that includes text-3D pairs and interleaved text-3D dialogues. We fine-tune a pretrained LLaMA-3.1-8B-Instruct [16] model on our curated dataset. We find that LLMs can acquire complex spatial knowledge by learning the numerical values of meshes in textual format. After fine-tuning, our model demonstrates the ability to (1) generate 3D meshes given text prompts, (2) produce interleaved outputs of text and 3D meshes in a conversational setup, and (3) describe meshes in natural language. Llama-Mesh is the first successful effort to empower an LLM to generate 3D content with language, unifying the 3D and text modalities in a single large model. It achieves mesh generation quality comparable to models trained from scratch while maintaining strong text generation abilities. Figure 3: Gallery of generations from Llama-Mesh. We can generate high-quality and diverse meshes with artist-like created topology."
https://arxiv.org/html/2411.09593v1,SMILE-UHURA Challenge - Small Vessel Segmentation at Mesoscopic Scale from Ultra-High Resolution 7T Magnetic Resonance Angiograms,"The human brain receives nutrients and oxygen through an intricate network of blood vessels. Pathology affecting small vessels, at the mesoscopic scale, represents a critical vulnerability within the cerebral blood supply and can lead to severe conditions, such as Cerebral Small Vessel Diseases. The advent of 7 Tesla MRI systems has enabled the acquisition of higher spatial resolution images, making it possible to visualise such vessels in the brain. However, the lack of publicly available annotated datasets has impeded the development of robust, machine learning-driven segmentation algorithms. To address the complexities of mesoscopic vessel segmentation and to highlight the need for advanced techniques to manage the high noise levels and poor vessel-to-background contrast inherent in ”ultra-high-resolution” data, the SMILE-UHURA challenge was organised. This challenge, held in conjunction with the ISBI 2023, in Cartagena de Indias, Colombia, aimed to provide a platform for researchers working on related topics. The SMILE-UHURA challenge addresses the gap in publicly available annotated datasets by providing an annotated dataset of Time-of-Flight angiography acquired with 7T MRI. This dataset was created through a combination of automated pre-segmentation and extensive manual refinement. In this manuscript, sixteen submitted methods and two baseline methods are compared both quantitatively and qualitatively on two different datasets: held-out test MRAs from the same dataset as the training data (with labels kept secret) and a separate 7T ToF MRA dataset where both input volumes and labels are kept secret. The results demonstrate that most of the submitted deep learning methods, trained on the provided training dataset, achieved reliable segmentation performance. Dice scores reached up to 0.838 ± 0.066 and 0.716 ± 0.125 on the respective datasets, with an average performance of up to 0.804 ± 0.15.","Brain function relies on the cerebral vasculature to supply nutrients and oxygen. Any impairment of the vasculature can damage brain tissue, potentially leading to cognitive decline. The cerebral vasculature is organised as a hierarchical, tree-like network, where vessel diameter decreases while the number of branches increases with higher branch order. For major cerebral vessels at the macroscopic scale and for capillaries, arterioles, and venules at the microscopic scale, in vivo and ex vivo imaging modalities are available, respectively. However, assessing the mesoscopic scale (vessel diameters of 100–500 µm) remains challenging. Pathologies at the mesoscopic scale are potentially linked to ageing, dementia, and Alzheimer’s disease [1, 2]. Segmentation and quantification of these vessels are crucial steps in the investigation of Cerebral Small Vessel Disease (CSVD) [3, 4]. Recently, ultra-high field (UHF) magnetic resonance imaging (MRI) has emerged as a means of bridging the gap between macroscopic and microscopic assessments of the human cerebral vasculature. Following pioneering work on magnetic resonance angiography (MRA) at 7 Tesla (7T) [5, 6], the field has advanced significantly, achieving the highest resolutions to date [7, 8] — as high as 150 µm and 140 µm, respectively. These advancements enable imaging of mesoscopic vessels, which are highly relevant to understanding cerebral small vessel diseases, neurodegeneration, and the origins of the functional fMRI signal. However, automatic segmentation of vessels at this scale has yet to be established. To address this need within the neurological and neuroscientific community, this challenge was initiated, focusing on the segmentation of vasculature at the mesoscopic scale. While vessel segmentation challenges have a long tradition, using UHF MRI for mesoscopic vessels presents unique difficulties compared to 2D microscopic or 3D macroscopic vessel imaging and segmentation: (I) instead of a single 2D image per sample, a 3D volume is acquired, significantly increasing computational demands and making manual segmentation highly time-consuming, and (II) compared to macroscopic segmentation, ultra-high-resolution data is noisier and exhibits poorer vessel-to-background contrast, complicating both automatic and manual segmentation. These challenges have hindered the establishment of openly accessible data repositories and the development of high-performance mesoscopic vessel segmentation algorithms. Currently, no high-resolution 7T dataset with annotations is available for training machine learning-based segmentation methods or benchmarking performance. To address this gap, an annotated dataset of Time-of-Flight (ToF) angiography acquired with a 7T MRI was created for this challenge. This dataset was generated using a combination of automatic pre-segmentation and extensive manual refinement. It serves as the foundation of this challenge and provides a benchmark for quantitative performance assessment, facilitating future advancements in mesoscopic vessel segmentation."
https://arxiv.org/html/2411.09580v1,"Software Performance Engineering for
Foundation Model-Powered Software (FMware)","The rise of Foundation Models (FMs) like Large Language Models (LLMs) is revolutionizing software development. Despite the impressive prototypes, transforming FMware into production-ready products demands complex engineering across various domains. A critical but overlooked aspect is performance engineering, which aims at ensuring FMware meets performance goals such as throughput and latency to avoid user dissatisfaction and financial loss. Often, performance considerations are an afterthought, leading to costly optimization efforts post-deployment. FMware’s high computational resource demands highlight the need for efficient hardware use. Continuous performance engineering is essential to prevent degradation. This paper highlights the significance of Software Performance Engineering (SPE) in FMware, identifying four key challenges: cognitive architecture design, communication protocols, tuning and optimization, and deployment. These challenges are based on literature surveys and experiences from developing an in-house FMware system. We discuss problems, current practices, and innovative paths for the software engineering community.","The rapid emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), is reshaping software development, with market value expected to reach $36.1 billion by 2030 [1]. FMs empower the creation of intelligent software, defined as FMware by Hassan et al. [2], where applications rely on one or more building blocks that are FMs. Many cool demos built with FMware have emerged recently [3, 4]. However, developing FMware from prototypes into production-ready products is a complex engineering process, requiring collaborations across AI, software engineering, systems, and hardware domains throughout the lifetime of such software [5, 6]. Performance engineering, one of the key aspects in such an engineering process, has not been thoroughly discussed. That is, how to proactively ensure that the developed FMware meets the pre-defined performance goals, e.g., throughput or latency. These goals are sometimes also referred to as Service Level Agreements (SLAs) or Service Level Objectives (SLOs). Failing to meet these goals will result in unsatisfactory user experiences. However, in practice, we observed that performance concerns are often considered afterthoughts during the lifecycle of FMware, causing inefficient and costly performance optimization efforts after the FMware is deployed in production when SLAs are not met. In addition, due to the intensive computation resources that are needed for deploying FMware, it can become prohibitively expensive to serve FMware requests. Efforts to improve the overall efficiency of hardware utilization are needed to avoid the wastage of scarce computing resources, such as costly GPUs sitting idle. Lastly, as FMware is live software that keeps evolving autonomously, it is necessary to apply continuous performance tuning practices to avoid performance degradation over time. To summarize, Software Performance Engineering (SPE) practices are crucial in bringing FMware from prototype to production. Although the awareness of performance-oriented FMware production is growing [5, 6], systematic studies focusing on SPE for FMware (SPE4FMware) are still lacking. In this paper, we present a comprehensive analysis of SPE challenges in FMware development, deriving from four authoritative sources: (i) an extensive survey of both academic and grey literature, (ii) in-depth discussions with industrial stakeholders and active academicians during SEMLA 2023 & 2024 [7], FM+SE Vision 2030 [8], FM+SE Summit 2024 [9], and SE 2030 workshop - FSE 2024 [10] events, (iii) close collaboration with our customers and our internal FMware application development teams to understand their pain points with performance issues, and (iv) our hands-on experience designing and implementing an in-house FMware serving system (FMware Runtime). We identify four key SPE challenges that span across the lifecycle of FMware development: the design of cognitive architectures, defining communication protocols, tuning and optimization approaches, and deployment options. For each challenge, we describe its aspects in detail, discuss state-of-practices, and share our vision of innovation paths that call for contributions from the software engineering research community. This paper is organized as follows: Section II outlines the background of our study. Section III delves into the SPE challenges that are associated with FMware. Section IV describes the vision of our serving system. Finally, Section V summarizes our insights and conclusions."
https://arxiv.org/html/2411.09547v1,Piecing It All Together: Verifying Multi-Hop Multimodal Claims,"Existing claim verification datasets often do not require systems to perform complex reasoning or effectively interpret multimodal evidence. To address this, we introduce a new task: multi-hop multimodal claim verification. This task challenges models to reason over multiple pieces of evidence from diverse sources, including text, images, and tables, and determine whether the combined multimodal evidence supports or refutes a given claim. To study this task, we construct MMCV, a large-scale dataset comprising 16k multi-hop claims paired with multimodal evidence, generated and refined using large language models, with additional input from human feedback. We show that MMCV is challenging even for the latest state-of-the-art multimodal large language models, especially as the number of reasoning hops increases. Additionally, we establish a human performance benchmark on a subset of MMCV. We hope this dataset and its evaluation task will encourage future research in multimodal multi-hop claim verification. Data and code are available: https://mmcv-dataset.github.io/","Due to the rapid growth in AI-generated content, it is difficult for automated fact-checking systems to keep up with verifying the accuracy of claims with multimodal evidence. This challenge is further exacerbated by the recent development of diffusion models such as DALL-E Ramesh et al. (2021) and Stable Diffusion Rombach et al. (2022), which can generate realistic images from textual prompts Liu et al. (2024b). These powerful tools could enable attackers to produce misleading information Wang and Shu (2024); Pan et al. (2023c) at a low cost. Additionally, these claims often require multi-hop reasoning, where a set of connected evidence pieces leads to the final verdict of a claim Yang et al. (2018). As a result, there is a need for automated tools to assist human fact-checkers in evaluating the veracity of multimodal multi-hop claims. Figure 1: An illustration of a 2-hop claim from MMCV. To correctly verify this claim, the system must reason over both the image evidence and the table evidence. Claim verification, which involves assessing the veracity of an input claim against a collection of evidence, is a vital tool in combating the spread of misinformation Thorne and Vlachos (2018); Guo et al. (2022); Jin et al. (2022, 2023); Yang et al. (2022). However, verifying multi-hop multimodal claims introduces new challenges in both dataset construction and effective modeling. Unlike single-hop claims, which require only straightforward one-step reasoning, multi-hop claims require multiple reasoning steps to reach a final verdict. Furthermore, the inclusion of multimodal evidence requires models to understand and integrate information across various modalities, such as text, images, and tables, making it more complex to comprehend and extract relevant information. For instance, to verify the claim shown in Figure 1, a system must understand the semantic content of the image, integrate all relevant information from the table evidence, and apply multi-step reasoning to arrive at the final conclusion. In this paper, we introduce the task of multi-hop multimodal claim verification to evaluate the veracity of multi-hop claims against multimodal evidence. To study this task, we construct Multi-hop Multimodal Claim-Verification (MMCV), a dataset of 16K multi-hop claims paired with multimodal evidence that either SUPPORT or REFUTE each claim. To create the dataset, we develop a novel pipeline that uses large language models (LMMs) for data annotation, supported by human feedback. This method significantly reduces the workload on human annotators and cuts costs, while ensuring high quality and factual accuracy of the dataset. Our pipeline first uses LLMs to re-formulate multi-hop multimodal question-answer pairs into atomic multi-hop claims and generate a set of candidate claims. These candidate claims are then modified to include additional hops and refined for fluency and clarity according to a set of annotation guidelines. To ensure the accuracy of the claims, we use a Retrieval-Augmented Generation (RAG)-based validation method to verify their validity. Finally, we ask a group of human annotators to score the claims based on their fluency, correctness, and clearness, and manually rewrite the claims that are below a certain threshold. We establish performance baselines on MMCV using three state-of-the-art multimodal large language models (MLLMs) and highlight their limitations in verifying complex multimodal claims. We further demonstrate the challenges posed by the dataset, especially as the number of reasoning hops increases, by illustrating the constrained performance of various prompt techniques designed to enhance MLLMs’ reasoning capabilities, including chain-of-thought, self-ask, and symbolic-guided reasoning. Additionally, we establish a human performance benchmark on a subset of MMCV. Overall, we introduce a challenging multi-hop multimodal claim verification dataset that includes claims with up to 4 reasoning hops. These complex claims often consist of multiple sentences linked by coreference and demand evidence from various modalities, such as text, images, and tables. Table Piecing It All Together: Verifying Multi-Hop Multimodal Claims provides a comparison between MMCV and existing popular claim verification datasets. While current datasets typically focus on either multimodal claims or multi-hop textual claims, none of them incorporate multi-hop multimodal claims that necessitate cross-modal reasoning. We hope that the introduction of MMCV and its corresponding evaluation task will inspire further research in complex multi-hop multimodal reasoning for claim verification. In summary, our contributions include: • We introduce and formalize the multi-hop multimodal claim verification task. • We develop a novel pipeline that leverages LLMs for data annotation, enhanced by human feedback, to construct a benchmark dataset for multi-hop multimodal claim verification. This method significantly lowers the cost and labor required to produce a large-scale dataset. • We establish baseline performance on this task using MLLMs and human evaluation. Our analysis shows that this is a non-trivial task, with several challenges that remain to be addressed in future work. 2 Background Multimodal Claim Verification. Previous research on claim verification has primarily focused on textual data. However, with the growing recognition that misinformation often appears across multiple modalities and that multimodal misinformation is perceived as more credible and spreads faster than text-only misinformation, recent efforts have shifted toward verifying multimodal claims Akhtar et al. (2023). As a result, several multimodal claim verification datasets have been proposed including FakeNewsNet Shu et al. (2020), COSMOS Aneja et al. (2021), InfoSurgeon Fung et al. (2021), Factify Mishra et al. (2022), Fauxtography Zlatkova et al. (2019), and Mocheg Yao et al. (2023). However, to the best of our knowledge, there are no existing datasets for multi-hop multimodal claim verification, which challenges the system’s reasoning capability by requiring it to integrate and interpret multiple pieces of evidence from different modalities. Multi-hop Reasoning. Verifying complex claims often requires multi-step (multi-hop) reasoning Mavi et al. (2022), which requires combining information from multiple pieces of evidence to predict the veracity of a claim. Many recently proposed datasets are created to challenge a model’s ability to reason across multiple sentences or documents. These include MultiRC Khashabi et al. (2018), QAngaroo Welbl et al. (2018), ComplexWebQuestion Talmor and Berant (2018), HotpotQA Yang et al. (2018), and HoVer Jiang et al. (2020). In contrast to these datasets, MMCV incorporates context from various modalities, such as images and tables, further challenging the system’s ability to understand and integrate evidence from different sources. Construct Synthetic Dataset with LLMs. The emergence of advanced large language models has sparked growing interest in automating the data annotation process using LLMs Tan et al. (2024), driven by their advanced capabilities, including in-context learning Dong et al. (2022) and learning from human feedback Ouyang et al. (2022). Wang et al. (2023) propose an explain-then-generate pipeline using LLMs for iterative data synthesis, while Pace et al. (2024) combine the Best-of-N and Worst-of-N sampling strategies to introduce the West-of-N approach. With this same objective, the multi-hop claims in MMCV are created and refined by LLMs using human feedback, following guidelines and rules specifically designed to enforce a multi-hop structure within each claim. Figure 2: Overview of data collection flow chart for MMCV. In the first stage, we re-formulate question-answer pairs from MultimodalQA to generate candidate claims. In the second stage, we modify and refine the candidate claims, and apply a Retrieval-Augmented Generation (RAG)-based method to verify their correctness. In the final stage, we ask human annotators to rank the candidate claims to select the best one and label the final claims accordingly. 3 The MMCV dataset The main goal of our work is to compile a diverse and extensive collection of multi-hop claims that require joint reasoning across evidence from different modalities, such as text, tables, and images, for verification. One approach to achieving this is to transform multimodal question-answering pairs into atomic claims and refine them to incorporate additional reasoning steps, making them more natural. However, there are two major challenges in creating such a dataset: first, building a large-scale dataset is labor-intensive and costly; second, in our pilot studies, we found that simply providing instructions to crowd workers and asking them to rewrite multi-hop claims is counterproductive, as it is difficult to control quality and challenging for workers to create meaningful multi-hop claims. Instead, we develop a pipeline that leverages the emerging capabilities of large language models to generate text and learn from feedback, with human input to ensure the quality of the final output. In this approach, LLMs handle the mundane task of rewriting claims consistently according to the instructions, while human effort is significantly reduced to quality control of the final claims based on a set of guidelines. Figure 2 shows the overall workflow of our data construction pipeline, which contains three stages: LLM-Based Claim Generation (§3.1), LLM-Generated Claim Refinement (§3.2) and Claim Annotation by Human (§3.3). 3.1 Claim Generation In this stage, we leverage the in-context learning capabilities of large language models to transform question-answer pairs from the MultimodalQA dataset Talmor et al. (2021) into verifiable claims. To minimize the impact of in-context examples on the quality of the generated claims, we carefully craft a pool of 20 in-context examples and randomly select 3 for use during execution. The claims are formulated to ensure that no information is omitted from the original QA pairs and no new information is introduced. Since the claims are derived directly from the question and the correct answer, they are automatically labeled as SUPPORT. The prompt template for claim generation is listed in Appendix A.2. 3.2 Claim Refinement After generating the initial claims from the question-answer pairs, we modify and refine them to ensure they are more naturally phrased and more accurately supported by the facts. Next, we review the claims for any factual errors that may have been introduced during the modification process and make corrections as needed. Claim Modification and Refinement. To introduce additional reasoning steps to the claim candidate, we employ a modify-then-refine approach that iteratively enhances the quality of the modified claim candidate based on feedback from LLMs Pan et al. (2023a). Specifically, we begin by identifying the Wikipedia entities mentioned in the answers from the question-answer pairs. If there is only one Wikipedia entity in the answer, we leave the claim candidate unchanged. However, if there are multiple Wikipedia entities, we use the summaries of their respective Wikipedia articles as context and instruct the LLMs to modify the claim in such a way that it incorporates this contextual information to replace the entity, ensuring that the entity’s name does not appear directly in the claim. To help LLMs understand the modification task, we provide them with 3-5 randomly selected in-context examples from a pool of hand-crafted examples. After modifying the claim, we obtain feedback from LLMs regarding the fluency, correctness, and clarity of the modified claim. The criteria used for this assessment are listed in the Appendix A.2. If the feedback suggests further improvement, the claim is sent back to the modification step, incorporating the LLMs’ feedback until a certain iteration threshold is reached. If the modified claim still does not pass the quality check, it is marked for manual review and revision by human annotators. RAG-based Truthfulness Validation. Since we introduce additional contextual information from Wikipedia when modifying the claims, there is a risk that LLMs might hallucinate and produce outputs that are not faithful to the input context. To eliminate potential factual errors, we use a retrieval-augmented generation (RAG) Lewis et al. (2020)-based pipeline to retrieve the full Wikipedia articles of the relevant entities and validate the factual accuracy of the modified claims. To mitigate the impact of prompt sensitivity on the model’s output Lu et al. (2022); Sclar et al. (2023), we diversify the prompts by randomly changing their format for each verification step. For instance, instead of consistently using Is it true that {claim}?, the prompt is randomly chosen from a set of equivalent alternatives, such as Verify the following statement: {claim} or What evidence supports the claim that {claim}? 3.3 Claim Annotation At this stage, we have obtained claims that have been modified and refined by LLMs and factually validated by RAG-based pipelines. Next, we use LLMs to generate negated claims by applying a set of specific negation rules. We employ three distinct methods for generating these negated claims. For instance, given the claim, “Since its construction in 1889, the Eiffel Tower in Paris attracts millions of visitors annually.”, the results after applying the negation rules are as follows: Negation \triangleright Word substitution: The Eiffel Tower in Paris houses millions of residents annually. \triangleright Entity substitution: The Colosseum in Paris attracts millions of visitors annually. \triangleright Temporal mutation: Ever since its construction in 2050, the Eiffel Tower has been Paris’s top tourist site. Next, a group of human annotators is tasked with evaluating the claims based on three dimensions: fluency, correctness, and clarity, scoring each dimension on a scale of 1 to 5. Fluency assesses how naturally the claim reads, as outputs generated by language models can sometimes sound artificial. Correctness evaluates whether the claim is factually accurate based on the evidence. Clarity determines if the claim is easily understood, as entity substitution might make it difficult to comprehend. Once the claims are scored, the average of the fluency, correctness, and clarity scores is calculated to determine the final score for each claim. If a claim’s final score falls below a predetermined threshold, it is flagged and sent back to the annotators for manual revision. Detailed annotation guidelines are listed in Appendix A.3. 4 Dataset Analysis Dataset Statistics. MMCV contains 16,439 multi-hop multimodal claims, with their statistics detailed in Table 2. The number of hops is determined by the count of multimodal evidence associated with each claim. The dataset includes a balanced distribution of SUPPORT and REFUTE claims. Specifically, there are 6,178 1-hop claims with an average of 21.7 tokens per claim; 8,969 2-hop claims averaging 25.32 tokens per claim; 870 3-hop claims with an average of 25.44 tokens per claim; and 422 4-hop claims averaging 26.17 tokens per claim. An example from the dataset is provided in Appendix A.1. Multi-hop Reasoning Types. We provide examples of each reasoning type in Table 6. Most 1-hop and 2-hop claims require at least one supporting fact from either image or table evidence for verification. In contrast, the majority of 3-hop and 4-hop claims require evidence from all three modalities. The process of removing a bridge entity and replacing it with a relative clause or phrase significantly increases the informational load of a single hypothesis. As a result, some 3-hop and 4-hop claims are relatively longer and exhibit complex syntactic and reasoning structures. Our experimental results also indicate that the difficulty for models to verify claims escalates as the hop count increases. Data 1-hop 2-hop 3-hop 4-hop # Claims 6,178 8,969 870 422 Ave. # Tokens in Claim 21.7 25.32 25.44 26.17 Max. # Tokens in Claim 48 58 51 63 # Text Evidence 2,755 7,770 1,241 801 # Image Evidence 2,059 3,149 682 554 # Table Evidence 1,364 7,019 687 333 # SUPPORT Labels 3,118 4,514 415 184 # REFUTE Labels 3,060 4,455 455 238 # Doc/Img/Tab in Collection 5,496 11,743 1,778 787 Table 2: Dataset Statistics of MMCV. 1-hop 2-hop 3-hop 4-hop Retrieval Model P R F1 P R F1 P R F1 P R F1 Closed-book GPT-4o 76.86 72.94 71.79 67.96 63.30 60.66 62.88 58.89 56.17 67.93 62.39 61.20 Gemini 75.67 71.44 70.15 69.10 64.19 61.73 66.74 61.10 58.44 63.78 59.90 58.69 LLaVA 64.18 63.78 63.57 64.06 63.93 63.87 66.78 66.81 66.76 64.64 64.84 64.64 Open-book GPT-4o 76.95 72.95 71.78 68.03 63.24 60.53 62.67 58.78 56.08 67.75 62.46 61.35 Gemini 79.58 79.25 79.20 72.38 71.85 71.66 66.37 65.90 65.86 67.21 66.86 66.97 LLaVA 62.86 59.68 57.21 64.17 62.48 61.50 65.47 64.64 63.76 66.50 66.76 66.42 Table 3: We report the Precision, Recall, and F1 scores of various MLLMs on MMCV for zero-shot multimodal claim verification. In the closed-book setting, the model verifies the claim without access to any external knowledge sources. In the open-book setting, the model is provided with a set of gold evidence. The best-performing model for each hop is highlighted in Green for both settings. 5 Experiments and Results In this section, we discuss our experiment settings (§5.1), the experiment results (§5.2), and the error analysis (§5.3). We begin by formally defining the MMCV task below. Task Definition. The formulation of multi-hop multimodal claim verification is defined as follows: Given a claim C, and a list of multimodal evidence \mathcal{E}(C), which includes text, images, and tables, the system must reason over all the evidence and predict the label of the claim as either SUPPORT or REFUTE. 5.1 Experiment Settings As there are no existing models specifically designed for multi-hop multimodal supervised claim verification, we conduct our experiments using MLLMs. Moreover, previous studies in textual claim verification and multimodal claim verification indicate that LLMs and MLLMs can significantly enhance task performance compared to traditional supervised approaches Pan et al. (2023b); Wang and Shu (2023); Li et al. (2024); Geng et al. (2024). Furthermore, supervised methods often require extensive annotated corpora, which are difficult to acquire and limit domain transferability, as training data typically covers only a single domain. Zero-shot Claim Verification. We establish performance baselines for zero-shot multimodal claim verification using various MLLMs under two settings. In the closed-book setting, the model does not retrieve information from external knowledge sources and must rely on its parametric (internal) knowledge to verify the claim. In the open-book setting, the model is provided with a set of gold evidence. Specifically, we use the prompt from Geng et al. (2024), which extracts the models’ predictions, explanations, and confidence levels. The prompt is listed in Appendix A.2. We use macro precision, recall, and F-1 score to evaluate the model performance. MLLM. We utilize two state-of-the-art MLLMs: GPT-4o Achiam et al. (2023) and Gemini 1.5 Flash Team et al. (2023). Additionally, we evaluate the performance of an open-source MLLM, LLaVA-V1.5-7B Liu et al. (2024a), on MMCV. The temperature is set to 0.0, and the maximum number of tokens is set to 5000. Prompts for Enhanced Reasoning In addition to the prompt mentioned above, we conduct experiments using specialized prompting techniques aimed at eliciting reasoning from LLMs, such as Chain-of-Thought Wei et al. (2022) and Self-Ask Press et al. (2023). We also test symbolic-guided reasoning prompts like ProgramFC Pan et al. (2023b) and Visual Programming Gupta and Kembhavi (2023). To minimize the overall cost of the experiments, we randomly select 100 examples from each hop of the MMCV dataset for testing. The experiments are conducted using open-book setting. Human Performance To benchmark human performance on our dataset, we used the same randomly selected examples employed in the enhanced reasoning prompt experiments. We recruited four experts in automated fact-checking research to classify multihop claims from MMCV based on the provided evidence. The SMART Chew et al. (2019) framework 111https://github.com/RTIInternational/SMART was used to deploy the annotation task, and human performance was evaluated using the macro F-1 score. Figure 3: The left figure shows the confidence score distribution of GPT4-o, Gemini, and LLaVA on MMCV under both open-book and closed-book settings, categorized by the number of hops. The right figure shows their calibration curves. 5.2 Experiment Results Main Results. We report the comprehensive results of the three MLLMs on MMCV in Table 3, highlighting the best-performing models for each hop under both open-book and closed-book settings. Overall, Gemini 1.5 outperforms others in the open-book setting with an average F-1 of 70.92, while LLaVA achieves the highest performance in the closed-book setting with an average F-1 of 66.77. This is surprising, given that LLaVA is a much smaller model compared to GPT4-o and Gemini, and therefore possesses less parametric knowledge. Upon manually analyzing a subset of 100 randomly selected outputs from LLaVA, we found that the model frequently hallucinates, even when it predicts the correct label, particularly as the hop count increases. This is consistent with its open-book performance, where its accuracy declines when provided with gold evidence. Additionally, we observe that GPT4-o performs slightly better in closed-book settings than in open-book settings, suggesting a tendency to hallucinate. In contrast, Gemini’s performance drops significantly in closed-book settings compared to open-book, demonstrating its robustness in effectively utilizing provided gold evidence. Confidence Level Analysis The left panel of Figure 3 presents the confidence distributions for all three MLLMs, categorized by the number of hops and divided into 10 intervals. The results show that the majority of the MLLMs are concentrated in the 90-100 confidence range, with only a small number exhibiting low confidence (0-10 range), which occurs solely in open-book settings. This indicates that the MLLMs consider the provided gold evidence. The right panel of Figure 3 displays the calibration curves, illustrating the relationship between the models’ confidence levels and their actual classification accuracy. These curves reveal a positive correlation between confidence and accuracy for 1-hop and 2-hop claims, as exemplified by the red line (GPT-4-o on 2-hop), the teal line (LLaVA on 1-hop), and the purple line (Gemini on 1-hop). In contrast, the downward curves, mostly observed in 3-hop and 4-hop claims, suggest that the models tend to be overconfident when classifying more complex claims. Additionally, the results indicate that open-book settings generally have better-calibrated confidence scores than closed-book settings, further suggesting that the models exhibit overconfidence when not provided with gold evidence. Model Method 1-hop 2-hop 3-hop 4-hop Gemini 1.5 CoT 78.52 69.66 67.45 70.24 Self-Ask 75.47 66.58 60.94 70.67 Symbolic 74.89 63.82 54.61 72.36 GPT4-o CoT 80.43 83.33 71.20 72.99 Self-Ask 77.42 80.12 70.52 75.23 Symbolic 80.56 78.78 68.72 75.67 Table 4: Results of Gemini and GPT4-o on 100 randomly sampled claims for each hop using three types of reasoning prompts. Model performance is evaluated using F-1 score. Reasoning Prompt Results. Table 4 reports the performance of Gemini and GPT4-o on the randomly sampled subset of MMCV under open-book settings using various prompts that elicit LLMs’ reasoning abilities. For symbolic approach, we ask LLMs to first generate a Python-like program that decomposes the mutli-hop claim into a set of function calls that describe the reasoning steps required to verify the claim, and use the symbolic information provided by the generated program to elicit better step-by-step reasoning from the model. We observe that GPT-4-o gains more from the enhanced reasoning prompt compared to Gemini, achieving a higher average F1 score of 75.93 in symbolic guided reasoning, whereas Gemini attains an average F1 score of 66.42 for the same task. Additionaly, we found that Symbolic approach are more effective on 4-hop claims, having a higher F1 score than CoT and self-ask. However, this observation is different on simpler 2-hop and 3-hop claims, where CoT appears to be more effective. Annotator # Hops 1-hop 2-hop 3-hop 4-hop Annotator 1 83.33 86.20 78.42 79.82 Annotator 2 82.46 88.29 79.45 82.16 Annotator 3 80.60 90.53 80.62 85.24 Annotator 4 79.64 86.50 82.32 83.87 Table 5: Results of human performance on 200 random samples. Performance are measured by F-1 score. Human Performance Results To establish human performance on our dataset, we randomly sampled 200 examples, with 50 examples from each hop from MMCV. We recruited four annotators to perform claim verification given the gold evidence. We trained our annotators on the task by providing them with guidelines and sample annotations to ensure consistency and accuracy in their evaluations. After training, the annotators independently verified each claim using the provided gold evidence, allowing us to assess the human baseline performance on the dataset. Table 5 reports the results from the human annotators. We observe that the human annotators achieve very high performance in verifying the claims across all 4 hops. The human performance is 23.3% and 27.3% higher than the best-performing MLLMs on 3-hop and 4-hop claims respectively. This suggests that although MLLMs perform relatively well, there is still room for improvement to match human performance. 5.3 Error Analysis Figure 5, 6, and 7 shows the error analysis of the false positive examples from GPT4-o, Gemini, and LLaVA respectively. We observe that visual misinterpretation is a major issue, with the system often misidentifying or miscontextualizing image elements. This problem is especially pronounced in examples involving sports logos and movie posters, highlighting the need for improvements in the visual processing component. Another notable issue is the system’s handling of temporal and factual information. Errors related to player career timelines and historical events reveal shortcomings in temporal reasoning and the integration of world knowledge. The system’s confidence levels, often between 80% and 100% for incorrect predictions, suggest a miscalibration in certainty estimation. This overconfidence in erroneous conclusions highlights the need for a more refined approach to confidence scoring. Last but not least, examples from higher hop categories reveal significant weaknesses in handling complex reasoning tasks. The system often struggles with multi-step logical inferences, frequently failing to coherently link disparate pieces of information. This limitation is especially problematic for claims that require advanced analysis or the cross-referencing of multiple facts. 6 Conclusion In this paper, we introduce MMCV, a multi-hop multimodal claim verification dataset that requires models to aggregate information from up to four multimodal evidence to verify a claim. To create this large-scale dataset, we developed a novel data collection pipeline that leverages the capabilities of LLMs combined with human feedback. Specifically, our approach includes a module that iteratively refines modified claims using feedback from a judge LLM based on a set of predefined criteria, as well as an actuality validation module that employs RAG to ensure the factual accuracy of the claims. Our results show that state-of-the-art MLLMs struggle to verify more complex claims as the number of reasoning hops increases, often displaying overconfidence in their predictions. We also present findings from experiments utilizing prompts tailored to enhance the reasoning abilities of MLLMs, alongside human performance benchmarks for comparison. Additionally, we categorize and provide a detailed error analysis of false positive results from each model. We hope that MMCV will inspire the development of models capable of conducting complex, multi-hop reasoning in the challenging task of multimodal claim verification. 7 Limitations We identify two main limitations of MMCV. First, the construction of MMCV depends on in-context learning coupled with self-refinement to convert a natural language question-answer pair into a multi-hop claim. While this method has proven to be effective, it may face difficulties when dealing with questions with intricate grammar structures and logical structures. This arises from the difficulty in conveying complex grammatical rules to the language model through a limited number of demonstrations within a constrained context size. Second, our aggregation method purely relies on LLMs themselves, which could introduce potential hallucination problems. On the other hand, by using a more robust logic solver could help with the hallucination issues, but there would be a tradeoff between the applicability and the robustness of the model. 8 Ethical Statement Biases. We acknowledge the possibility of biases existing within the data used for training the language models, as well as in certain factuality assessments. Unfortunately, these factors are beyond our control. Intended Use and Misuse Potential. Our models have the potential to verify complex multimodal claims. However, it is essential to recognize that they may also be susceptible to misuse by malicious individuals. Therefore, we strongly urge researchers to approach their utilization with caution and prudence. Environmental Impact. We want to highlight the environmental impact of using large language models, which demand substantial computational costs and rely on GPUs/TPUs for training, which contributes to global warming. However, it is worth noting that our approach does not train such models from scratch. Instead, we use few-shot in-context learning. Nevertheless, the large language models we used in this paper are likely running on GPU(s). References Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Akhtar et al. (2023) Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, and Andreas Vlachos. 2023. Multimodal automated fact-checking: A survey. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5430–5448, Singapore. Association for Computational Linguistics. Aneja et al. (2021) Shivangi Aneja, Chris Bregler, and Matthias Nießner. 2021. Cosmos: Catching out-of-context misinformation with self-supervised learning. arXiv preprint arXiv:2101.06278. Chew et al. (2019) Rob Chew, Michael Wenger, Caroline Kery, Jason Nance, Keith Richards, Emily Hadley, and Peter Baumgartner. 2019. Smart: an open source data labeling platform for supervised learning. Journal of Machine Learning Research, 20(82):1–5. Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey on in-context learning. arXiv preprint arXiv:2301.00234. Fung et al. (2021) Yi Fung, Christopher Thomas, Revanth Gangi Reddy, Sandeep Polisetty, Heng Ji, Shih-Fu Chang, Kathleen McKeown, Mohit Bansal, and Avi Sil. 2021. InfoSurgeon: Cross-media fine-grained information consistency checking for fake news detection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1683–1698, Online. Association for Computational Linguistics. Geng et al. (2024) Jiahui Geng, Yova Kementchedjhieva, Preslav Nakov, and Iryna Gurevych. 2024. Multimodal large language models to support real-world fact-checking. arXiv preprint arXiv:2403.03627. Guo et al. (2022) Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics, 10:178–206. Gupta and Kembhavi (2023) Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953–14962. Jiang et al. (2020) Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3441–3460, Online. Association for Computational Linguistics. Jin et al. (2023) Yiqiao Jin, Yeon-Chang Lee, Kartik Sharma, Meng Ye, Karan Sikka, Ajay Divakaran, and Srijan Kumar. 2023. Predicting information pathways across online communities. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1044–1056. Jin et al. (2022) Yiqiao Jin, Xiting Wang, Ruichao Yang, Yizhou Sun, Wei Wang, Hao Liao, and Xing Xie. 2022. Towards fine-grained reasoning for fake news detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 5746–5754. Khashabi et al. (2018) Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252–262, New Orleans, Louisiana. Association for Computational Linguistics. Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474. Li et al. (2024) Miaoran Li, Baolin Peng, Michel Galley, Jianfeng Gao, and Zhu Zhang. 2024. Self-checker: Plug-and-play modules for fact-checking with large language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 163–181, Mexico City, Mexico. Association for Computational Linguistics. Liu et al. (2024a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024a. Visual instruction tuning. Advances in neural information processing systems, 36. Liu et al. (2024b) Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. 2024b. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177. Lu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086–8098, Dublin, Ireland. Association for Computational Linguistics. Luo et al. (2021) Grace Luo, Trevor Darrell, and Anna Rohrbach. 2021. NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6801–6817, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Mavi et al. (2022) Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022. A survey on multi-hop question answering and generation. arXiv preprint arXiv:2204.09140. Mishra et al. (2022) Shreyash Mishra, S Suryavardan, Amrit Bhaskar, Parul Chopra, Aishwarya N Reganti, Parth Patwa, Amitava Das, Tanmoy Chakraborty, Amit P Sheth, Asif Ekbal, et al. 2022. Factify: A multi-modal fact verification dataset. In DE-FACTIFY@ AAAI. Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744. Pace et al. (2024) Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2024. West-of-n: Synthetic preference generation for improved reward modeling. arXiv preprint arXiv:2401.12086. Pan et al. (2023a) Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023a. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188. Pan et al. (2023b) Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023b. Fact-checking complex claims with program-guided reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6981–7004, Toronto, Canada. Association for Computational Linguistics. Pan et al. (2023c) Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Wang. 2023c. On the risk of misinformation pollution with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1389–1403, Singapore. Association for Computational Linguistics. Press et al. (2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687–5711, Singapore. Association for Computational Linguistics. Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821–8831. Pmlr. Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695. Sclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324. Shu et al. (2020) Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. 2020. Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media. Big data, 8(3):171–188. Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. arXiv preprint arXiv:1803.06643. Talmor et al. (2021) Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. 2021. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint arXiv:2104.06039. Tan et al. (2024) Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large language models for data annotation: A survey. arXiv preprint arXiv:2402.13446. Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Thorne and Vlachos (2018) James Thorne and Andreas Vlachos. 2018. Automated fact checking: Task formulations, methods and future directions. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3346–3359, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Thorne et al. (2018) James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana. Association for Computational Linguistics. Wang and Shu (2023) Haoran Wang and Kai Shu. 2023. Explainable claim verification via knowledge-grounded reasoning with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6288–6304, Singapore. Association for Computational Linguistics. Wang and Shu (2024) Haoran Wang and Kai Shu. 2024. Trojan activation attack: Red-teaming large language models using steering vectors for safety-alignment. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM ’24, page 2347–2357, New York, NY, USA. Association for Computing Machinery. Wang et al. (2023) Ruida Wang, Wangchunshu Zhou, and Mrinmaya Sachan. 2023. Let’s synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11817–11831, Singapore. Association for Computational Linguistics. Wang (2017) William Yang Wang. 2017. “liar, liar pants on fire”: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422–426, Vancouver, Canada. Association for Computational Linguistics. Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837. Welbl et al. (2018) Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287–302. Yang et al. (2022) Ruichao Yang, Xiting Wang, Yiqiao Jin, Chaozhuo Li, Jianxun Lian, and Xing Xie. 2022. Reinforcement subgraph reasoning for fake news detection. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2253–2262. Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, Brussels, Belgium. Association for Computational Linguistics. Yao et al. (2023) Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2023. End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2733–2743. Zlatkova et al. (2019) Dimitrina Zlatkova, Preslav Nakov, and Ivan Koychev. 2019. Fact-checking meets fauxtography: Verifying claims about images. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2099–2108, Hong Kong, China. Association for Computational Linguistics. Appendix A Appendix A.1 Dataset Example Here is an example of dataset schema from MMCV: Example claim: Stoke City, a club that was part of the top-tier league before 1992, was promoted to the highest level of English football in 2018. wiki_context: The Premier League is the highest level of the English football league system. Contested by 20 clubs, it operates on a system of promotion and relegation with the English Football League (EFL). Seasons usually run from August to May, with each team playing 38 matches: two against each other, one home and one away. Most games are played on weekend afternoons, with occasional weekday evening fixtures. text_evidence: [ ""f369cee1ca92368c8b1ea564c5e41fc1"" ] image_evidence: [] table_evidence: [ ""c120efadd518b5f32c11d40b456c8570"" ] label: SUPPORT Additional examples of 1-hop, 2-hop, 3-hop, and 4-hop claims are listed in Table 6 A.2 Experiment Prompt Claim Verification Prompt. To test MLLMs’ claim verification performance under zero-shot settings, we follow Geng et al. (2024) and use the following prompt. Prompt Given a claim and evidence (which can be text, table, or an image), determine whether the claim is SUPPORT or REFUTE by the evidence. Use the following format to provide your answer: Prediction: [True or False] Explanation: [put your evidence and step-by-step reasoning here] Confidence Level: [please show the percentage] Note: The confidence level indicates the degree of certainty you have about your answer and is represented as a percentage. For instance, if your confidence level is 80%, it means you are 80% certain that your answer is correct and there is a 20% chance that it may be incorrect. Claim Generation Prompt. We use the following prompt to convert multimodal QA pairs into claim candidates: Prompt You are an expert in converting question-answers into claims. For example: Question: Telos was an album by a band who formed in what city? Answer: Indianapolis. Claim: Telos was an album by a band formed in Indianapolis. Convert the question-answer into claim. Return only the claim and nothing else. Claim Modification Prompt. We use the following prompt to modify the claim candidates: Prompt Generate a multi-hop specific claim based on the given general claim and Wikipedia context. The specific claim should: Incorporate information from Wikipedia context. Provided context should always be factually correct. Obscure key information by: a) Replacing one or two central entities with related fact using the Wikipedia context. b) Alluding to critical details without explicitly stating them. Claim should be short and concise. For example: - General Claim: The Mona Lisa is a famous painting by Leonardo da Vinci. - Wikipedia Context: The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as ẗhe best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world.̈ The painting’s novel qualities include the subject’s enigmatic expression, the monumentality of the composition, the subtle modelling of forms, and the atmospheric illusionism. It is housed in the Louvre Museum in Paris, where it was first put on display in 1797. - Specific Claim: The Mona Lisa is a half-length portrait painting created by Italian artist who is considered as archetypal masterpiece of the Italian Renaissance. Claim Refinement Prompt. We use the following prompt to refine the claim candidates: Prompt You are tasked with improving a claim focusing on three key areas: Fluency, Correctness, and Clearness. Your goal is to enhance the text while maintaining its original meaning and intent. Improvement Criteria: Fluency: 1. Review the text for grammar, syntax, and punctuation errors. 2. Rephrase any awkward or unnatural sentences to make the text flow more smoothly. 3. Ensure that the text reads naturally and is easy to follow. Correctness: 1. Verify the factual accuracy of the content and correct any errors. 2. Ensure that the text adheres to the prompt’s instructions. 3. Clarify any ambiguities and correct any inconsistencies in the information presented. Clearness: 1. Simplify complex sentences or ideas to make the text easier to understand. 2. Improve the organization of ideas to enhance readability. 3. Ensure that the message is conveyed clearly and effectively, eliminating any confusion or ambiguity. Final Output: Once you have made the necessary improvements, provide the revised text. Ensure that the improved version is more fluent, accurate, and clear than the original while preserving the original meaning and intent. Example Improvement: Original Claim: ""The results of the survey was very positive, with many respondents saying that they would recommend the service to others, however, some were also mentioned issues with the customer support."" Improved Claim: ""The survey results were overwhelmingly positive, with many respondents stating they would recommend the service to others. However, some also noted issues with customer support. A.3 Annotation Guidelines We ask our annotators to score the quality of the claim from three aspects: fluency, correctness, and clearness. Here is the detailed guidelines provided to the human annotators. Guidelines \triangleright Scoring Criteria: Fluency: Rate on a scale of 1-4. Correctness: Rate on a scale of 1-3. Clearness Rate on a scale of 1-3. \triangleright Fluency (1-4): 4: Excellent - Reads naturally, no awkward phrasing. 3: Good - Mostly smooth, minor phrasing issues. 2: Fair - Several awkward phrases or constructions. 1: Poor - Difficult to read, very unnatural phrasing. \triangleright Correctness (1-3): 3: Fully correct - All information is accurate. 2: Partially correct - Some information is accurate, some errors. 1: Incorrect - Significant factual errors or misrepresentations. \triangleright Clearness (1-3): 3: Very clear - Easy to understand, no ambiguity. 2: Somewhat clear - Some parts may be confusing or ambiguous. 1: Unclear - Difficult to understand the intended meaning. A.4 Crowd Worker Interface We use SMART Chew et al. (2019), an open-source project designed to help data scientists and research teams efficiently build labeled training datasets for supervised machine learning tasks. Figure 4 shows an example of the worker interface during scoring procedure. #H Claim Evidence 1 Claim: Marisa Coughlan played the role of Chante Lefort on television in 1996. 2 Claim: The driver seen signing autographs outside had a significant points total during a specific race in 2001 while competing for a well-known team in stock car racing. 3 Claim: The Green Bay Packers were one of the two teams that played in the first Super Bowl and also faced the New York Giants at MetLife Stadium during the 2013 regular season. Doc A: The first AFL-NFL World Championship Game in professional American football, known retroactively as Super Bowl I and … … Doc B: The National Football League (NFL) champion Green Bay Packers defeated the American Football League (AFL) champion Kansas City Chiefs … … Table: Not Included Here 4 Claim: The team that was promoted to the Premier League in the 2018-19 season received a higher accolade in the Third Division PFA Team of the Year during the 1980s than a club renowned for its West London rivalries. Doc A: Manchester City are the defending champions. Wolverhampton Wanderers, Cardiff City and Fulham join as the promoted clubs from the 2017–18 EFL Championship. … … Doc B: … They will replace West Bromwich Albion, Swansea City and Stoke City who were relegated to the 2018–19 EFL Championship. … Table: Not Included Here Image: Not Included Here Table 6: Examples of 1-hop, 2-hop, 3-hop and 4-hop claims from MMCV. Figure 4: UI for human annotators. Figure 5: Error Analysis: Gemini Figure 6: Error Analysis: GPT4-o Figure 7: Error Analysis: LLaVA"
https://arxiv.org/html/2411.09540v1,Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models,"Visual prompting (VP) is a new technique that adapts well-trained frozen models for source domain tasks to target domain tasks. This study examines VP’s benefits for black-box model-level backdoor detection. The visual prompt in VP maps class subspaces between source and target domains. We identify a misalignment, termed class subspace inconsistency, between clean and poisoned datasets. Based on this, we introduce BProm, a black-box model-level detection method to identify backdoors in suspicious models, if any. BProm leverages the low classification accuracy of prompted models when backdoors are present. Extensive experiments confirm BProm’s effectiveness.","Deep neural networks (DNNs) are commonly used in complex applications but require extensive computational power, leading to significant costs. Users often access these models through online platforms like BigML model market111https://bigml.com/ and ONNX zoo222https://github.com/shaoxiaohu/model-zoo, or via Machine Learning as a Service (MLaaS) platforms. However, DNNs can include backdoors (Gu et al., 2017; Liu et al., 2018b; Tang et al., 2021; Qi et al., 2023b; Nguyen & Tran, 2021; Chen et al., 2017), which manipulate model responses to inputs with specific triggers (like certain pixel patterns) while functioning correctly on other inputs. In backdoor attacks, attackers embed these triggers in the training data, leading the model to associate the trigger with a particular outcome and misclassify inputs containing it. Why Black-Box Model-Level Detection. Black-box backdoor detection, which uses only black-box queries to the suspicious model (i.e., the model to be inspected), is gaining attention. This detection method is divided into input-level (Li et al., 2021c; Qiu et al., 2021; Gao et al., 2022; Liu et al., 2023; Qi et al., 2023c; Zeng et al., 2023; Guo et al., 2023; Hou et al., 2024; Xu et al., 2024; Mo et al., 2024) and model-level (Huang et al., 2020; Dong et al., 2021; Guo et al., 2022; Xu et al., 2019; Wang et al., 2024) techniques. Input-level detection identifies trigger samples in an infected model, while model-level detection determines if a model contains backdoors. Input-level detection relies on the model having backdoors; otherwise, its accuracy drops significantly. For example, as shown in Table 1, TeCo (Liu et al., 2023) and SCALE-UP (Guo et al., 2023), state-of-the-art input-level detectors, show AUROCs of 0.8113 and 0.7877, respectively, on a BadNets-infected model (Gu et al., 2017), but only 0.4509 and 0.5103 on a clean model. If a model is clean, many legitimate samples may be misclassified as triggers, reducing the model’s practical utility. Thus, model-level detection should be performed first. If backdoors are found but the model must still be used, input-level detection should then be applied to each input. Table 1: A significant drop of F1-score and AUROC in black-box input-level detection methods, TeCo (Liu et al., 2023) and SCALE-UP (Guo et al., 2023). BadNet (Gu et al., 2017) Blended (Chen et al., 2017) WaNet (Nguyen & Tran, 2021) TeCo (Liu et al., 2023) Backdoored Clean Backdoored Clean Backdoored Clean F1 0.8014 0.5263 0.7621 0.5033 0.9295 0.5137 AUROC 0.8113 0.4509 0.7259 0.3954 0.9345 0.4406 ScaleUp (Guo et al., 2023) Backdoored Clean Backdoored Clean Backdoored Clean F1 0.7964 0.5236 0.7991 0.5046 0.7199 0.4768 AUROC 0.7877 0.5103 0.7694 0.4643 0.7772 0.4246 Design Challenge. Despite its importance, black-box model-level detection faces two main challenges. First, unlike input-level detection, which benefits from the presence of an infected model, model-level detection has limited ground truth, relying on only a few clean samples. Second, it needs a stable feature to differentiate between clean and infected models across various backdoor types, which is difficult to find. For instance, B3D (Dong et al., 2021) targets trigger localization but is mainly effective for patch-based triggers. Similarly, AEVA (Guo et al., 2022) may struggle with larger triggers due to its dependence on adversarial peak analysis. (a) The “3” is from MNIST, while the middle part shows the visual prompt. The prompted sample is ready for ImageNet classifier. (b) The prompted sample can be fed into the ImageNet classifier, whose output has a mapping between the labels from MNIST and ImageNet. Figure 1: How a frozen ImageNet classifier is adapted for the MNIST classification when VP is used. Our Design. Visual prompting (VP) (Bahng et al., 2022; Jia et al., 2022) allows a frozen, pre-trained model from a source domain to correctly predict samples from a target domain by applying a visual prompt. This technique can work across very different domains; for example, an ImageNet classifier (source) can detect melanoma (target) via VP (Tsai et al., 2020). Figure 1 illustrates VP, where the visual prompt (trainable noise in Figure 1(a)) maps between class subspaces of the source and target domains, enabling the frozen classifier to handle the target task efficiently. In an infected model, the target class subspace in the feature space is adjacent to all other class subspaces (Wang et al., 2019). We identify a class subspace inconsistency where misalignment between class subspaces in the poisoned (source) and clean (target) datasets leads to low classification accuracy of the prompted model. This phenomenon is illustrated in Figure 2 and experimentally validated in both Figure 3 and Section C. Based on this, we propose BProm for black-box model-level backdoor detection. BProm applies VP to a suspicious model using an unrelated clean dataset; poor accuracy in the prompted model indicates the presence of backdoors. Contribution. Our contributions can be summarized as follows. 1) We identify a class subspace inconsistency in VP on backdoor-infected models. This misalignment between class subspaces of the poisoned dataset and an external clean dataset signals backdoor infection. 2) Utilizing this inconsistency, we develop BProm, a black-box model-level backdoor detection method. (a) Class subspace inconsistency does not occur: visual prompt as a mapping between two clean datasets. (b) Subspace inconsistency occurs: visual prompt as a mapping between clean and poisoned datasets. Figure 2: A conceptual illustration of (a) VP on clean model and (b) VP on backdoor-infected model. (a) Clean source model shows clear class separation. (b) Clean target model preserves clear separation. (c) Target class (0) is adj. to others in infected source model. (d) Infected target model shows severe class confusion. Figure 3: Class subspaces inconsistency (CIFAR-10 for source model and STL-10 for target model)."
https://arxiv.org/html/2411.09510v2,Communication Compression for Tensor Parallel LLM Inference,"Large Language Models (LLMs) have pushed the frontier of artificial intelligence but are comprised of hundreds of billions of parameters and operations. For faster inference latency, LLMs are deployed on multiple hardware accelerators through various Model Parallelism strategies. Our paper looks into the details on one such strategy - Tensor Parallel - and proposes to reduce latency by compressing inter-accelerator communication. We leverage fine grained quantization techniques to compress selected activations by 3.5 - 4.5x. Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) with negligible model performance degradation.","Large Language Models (LLMs) have become essential across various applications due to their exceptional performance. As model performance tends to improve with increased parameter counts, LLMs have been significantly scaled in recent years, with contemporary models now reaching 500B+ parameters [Chowdhery et al., 2023]. Deploying such large models for inference presents major challenges [Pope et al., 2022]. Tensor Parallel [Shoeybi et al., 2020] addresses this by splitting layers on multiple accelerators, enabling the execution of extremely large models and significantly reducing latency. However, Tensor Parallel demands accumulation of results from accelerators, as shown in Figure 1, and can lead to data communication bottlenecks [Zhuang et al., 2024, Agrawal et al., 2024], especially during the first auto-regressive inference step (the prefill phase). One approach to mitigate these bottlenecks, and thus reduce model latency even further, is to quantize activations before communication, which reduces the time needed to accumulate results from accelerators in a Tensor Parallel group. However, the presence of outliers [Dettmers et al., 2022, Lin et al., 2023] complicates this strategy, necessitating fine-grained quantization approaches. We leverage such approaches proposed by Rouhani et al. [2023] to compress activations and demonstrate the potency of communication compression by measuring time-to-first-token (TTFT) in realistic inference scenarios using different inference hardware setups. We find that for hardware setups which have slower inter-accelerator bandwidths, the TTFT can be improved by 3.5 - 4.5x with negligible degradation of model performance."
https://arxiv.org/html/2411.09492v1,"MM-Eval: A Hierarchical Benchmark for
Modern Mongolian Evaluation in LLMs","Large language models (LLMs) excel in high-resource languages but face notable challenges in low-resource languages like Mongolian. This paper addresses these challenges by categorizing capabilities into language abilities (syntax and semantics) and cognitive abilities (knowledge and reasoning). To systematically evaluate these areas, we developed MM-Eval, a specialized dataset based on Modern Mongolian Language Textbook I and enriched with WebQSP and MGSM datasets.Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat, Llama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models performed better on syntactic tasks than semantic tasks, highlighting a gap in deeper language understanding; and 2) knowledge tasks showed a moderate decline, suggesting that models can transfer general knowledge from high-resource to low-resource contexts.The release of MM-Eval—comprising 569 syntax, 677 semantics, 344 knowledge, and 250 reasoning tasks—offers valuable insights for advancing NLP and LLMs in low-resource languages like Mongolian. The dataset is available at https://github.com/joenahm/MM-Eval.","Figure 1: Workflow for Constructing the MM-Eval Dataset In recent years, large language models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable capabilities in understanding and generating human language, excelling in tasks such as context comprehensionJin et al. (2024), language generationMalik et al. (2024), summarizationSong et al. (2024), question answeringSchimanski et al. (2024), and translationXu et al. (2024). Models like ChatGPTOpenAI (2023) and LlamaTouvron et al. (2023) have set new benchmarks across a wide range of languages, primarily high-resource ones such as Chinese and English. However, the support for low-resource languages like Mongolian remains largely unexplored. Mongolian, spoken by millions across Mongolia and Inner Mongolia of China, presents unique linguistic challenges due to its complex grammar, script, and historical evolution. In Mongolia, modern Mongolian is written using the Cyrillic script, based on the Russian alphabet, while in Inner Mongolia, China, the traditional Mongolian script, derived from the Sogdian-Uyghur script, is used. This paper focuses on modern Mongolian written in the Cyrillic script. Despite some efforts to include Mongolian in NLP research, there is still a significant gap in understanding how well LLMs can handle Mongolian across various linguistic dimensions. This research aims to fill the gap in Mongolian language support by systematically evaluating modern LLMs’ capabilities in processing Mongolian. Unlike existing task-oriented datasets, this study focuses on models proven effective in high-resource languages. For Mongolian, we adopt a linguistic perspective, constructing a dataset based on language proficiency levels and previous LLM performance. Our dataset is organized into four hierarchical levels: syntax, semantics, knowledge, and reasoning. This structure allows for a detailed evaluation of model performance at different proficiency levels, providing deeper insights into their strengths and limitations. By uncovering both the strengths and weaknesses of current models, we aim to provide a benchmark for future Mongolian NLP research, contribute to the broader understanding of LLM support for low-resource languages, and help enhance their Mongolian language capabilities. We summarize the main lessons learned and our main contributions as follows: • This paper introduces MM-Eval, a specialized dataset for evaluating the capabilities of large language models (LLMs) in modern Mongolian, which is a low-resource language. • This paper proposes a Dual Capability Framework that evaluates LLMs by dividing their capabilities into language abilities (syntax and semantics) and cognitive abilities (knowledge and reasoning). This framework allows for a detailed understanding of model performance at different language proficiency levels. • This paper provides a comprehensive evaluation of LLMs in Mongolian, covering syntax, semantics, knowledge, and reasoning. This evaluation reveals the strengths and weaknesses of current models in processing Mongolian."
https://arxiv.org/html/2411.09475v1,ResidualDroppath: Enhancing Feature Reuse over Residual Connections,"Residual connections are one of the most important components in neural network architectures for mitigating the vanishing gradient problem and facilitating the training of much deeper networks. One possible explanation for how residual connections aid deeper network training is by promoting feature reuse. However, we identify and analyze the limitations of feature reuse with vanilla residual connections. To address these limitations, we propose modifications in training methods. Specifically, we provide an additional opportunity for the model to learn feature reuse with residual connections through two types of iterations during training. The first type of iteration involves using droppath, which enforces feature reuse by randomly dropping a subset of layers. The second type of iteration focuses on training the dropped parts of the model while freezing the undropped parts. As a result, the dropped parts learn in a way that encourages feature reuse, as the model relies on the undropped parts with feature reuse in mind. Overall, we demonstrated performance improvements in models with residual connections for image classification in certain cases.","Figure 1: Feature Reuse Across Multiple Layers. It visualizes the feature distribution of a model trained with linear layers of depth 32 and a hidden dimension of 32, with residual connections added at each layer. Despite the presence of residual connections, it shows that the model produces similar feature distributions through multiple transformations of the previous layer’s feature distribution. This could be disadvantageous from the perspective of information retention. Residual connection [38] is such a general and popular technique that almost no model exists without it. For example, it is utilized in foundation models like GPT-4 [80], Llama 2 [102], and Stable Diffusion XL [84]. This is because residual connections allow gradients to flow more directly through layers, reducing the vanishing gradient problem. Additionally, the advantages of residual connections are sometimes interpreted as benefits of leveraging identity mapping across layers [39]. Our work shows that, despite the presence of residual connections, there are limitations in effectively utilizing identity mappings. To address this limitation, we propose an algorithm that provides additional opportunities for the model to learn identity mappings. This approach further enhances the model’s performance. The distinguishing feature of residual connections is their skip connection. This means that for a block within the model, the input x goes through a series of transformations F and then x is added back to produce the output. In other words, the operation F(x)+x is performed for the block. This can be seen in (a) of Figure 5. The residual connection utilizes the previous block’s output directly with the skip connection, which can be interpreted as beneficial for feature reuse [47]. However, there is limited analysis within models on how features from previous blocks are reused. Considering the difficulty that deep learning models face when learning sparse matrices [111], it can be anticipated that reusing features across multiple blocks through residual connections may be challenging. Therefore, we analyzed the potential limitations of feature reuse in model training that includes residual connections. To analyze feature reuse within the model, we visualized the node outputs directly. Specifically, we employed TFMeter [44], an extension of TensorBoard Playground, which trains the model on a two-dimensional dataset and visualizes the output of nodes for grid inputs by generating contour plots. In this process, we scaled the analysis from the maximum of 6 layers supported by TFMeter to 32 layers. Through this, we demonstrated a deficiency in feature reuse across multiple layers, not only in small models but also in large models. To address the deficiency, we propose an algorithm called ResidualDroppath. ResidualDroppath alternates between two types of iterations during training. In the first iteration, the model is trained with enforced feature reuse without any transformation over parts of the block by applying droppath [46]. In the second iteration, the model learns to consider whether to utilize these paths. This approach enables the model to reuse features in their identity form, facilitating feature reuse across multiple layers. Then, we verified whether the proposed ResidualDroppath is beneficial for deep learning training in image classification tasks. To this end, we applied our algorithm to ResNet50 and ResNet50d models and tested it on the CIFAR10, MNIST, and ImageNet1k datasets. As a result, we observed a significant improvement in both Top-1 and Top-5 accuracy on the CIFAR10, and MNIST dataset. Furthermore, performance improvements were noted for the ResNet50d model on the ImageNet1K dataset. Our contributions can be summarized as follows: 1. We analyze the feature learning process and identify the challenges of reusing features across multiple layers. 2. To address these limitations, we propose the ResidualDroppath algorithm, which enhances the model’s ability to reuse features without transformation. 3. We demonstrate that the proposed algorithm significantly improves the performance of ResNet50 and ResNet50d models on the CIFAR10 and MNIST image classification datasets. Additionally, on the ImageNet1K dataset, performance improvements were observed in ResNet50d."
https://arxiv.org/html/2411.09471v1,Renal Cell Carcinoma subtyping: learning from multi-resolution localization,"Renal Cell Carcinoma is typically asymptomatic at the early stages for many patients. This leads to a late diagnosis of the tumor, where the curability likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high, with respect to its incidence rate. To increase the survival chance, a fast and correct categorization of the tumor subtype is paramount. Nowadays, computerized methods, based on artificial intelligence, represent an interesting opportunity to improve the productivity and the objectivity of the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their exploitation is hampered by the paucity of annotated dataset, essential for a proficient training of supervised machine learning technologies. This study sets out to investigate a novel self supervised training strategy for machine learning diagnostic tools, based on the multi-resolution nature of the histological samples. We aim at reducing the need of annotated dataset, without significantly reducing the accuracy of the tool. We demonstrate the classification capability of our tool on a whole slide imaging dataset for Renal Cancer subtyping, and we compare our solution with several state-of-the-art classification counterparts.","Renal Cell Carcinoma (RCC) is a highly malignant tumor and the most widespread type of kidney cancer, accounting for 90% of the overall entities. This tumor is also the 7th most common histological type in the west, and it is continuously increasing [1]. Its mortality rate is considered high, with respect to its incidence rate, as this tumor is typically asymptomatic at the early stages for many patients [1, 2]. This leads to a late diagnosis of the tumor, where the curability likelihood is lower. RCC can be categorized into multiple histological subtypes, mainly: Clear Cell Renal Cell Carcinoma (ccRCC) forming 75% of RCCs, Papillary Renal Cell Carcinoma (pRCC) accounting for 10%, and Chromophobe Renal Cell Carcinoma (chRCC) accounting for 5%. Some of the other sutypes include Collecting Duct Renal Cell Carcinoma (cdRCC), Tubulocystic Renal Cell Carcinoma (tRCC), and unclassified [1]. Approximately 10% of renal tumors belong to the benign entities neoplasms, being Oncocytoma (ONCO) the most frequent subtype with an incidence of 3–7% among all RCCs [3, 2]. These subtypes show different cytological signature as well as histological features [2], which ends up in significantly different prognosis. The correct categorization of the tumor subtype is indeed of major importance, as prognosis and treatment approaches depend on it and on the disease stage. For instance, the overall 5-year survival rate significantly differs among the different histological subtypes, being 55–60% for ccRCC, 80–90% for pRCC and 90% for chRCC. This points out the need for the most accurate subclassification [4, 5]. Existing literature emphasizes also the critical role of the differential diagnosis between chromophobe and oncocytoma, known to be arduous and prone to errors due to overlapping morphological characteristics in a relevant number of cases [2, 6, 3]. Currently, the gold standard to classify RCC subtypes consists in the microscopic visual assessment of Hematoxylin and Eosin (H&E) samples, performed by a pathologist though the microscope. These specimen consist most often of physical slides and, in some centers provided with scanner, of virtual slides: the so-called Whole histological Slide Images (WSIs). The visual diagnosis of the large WSIs is known to be both effort-requiring and time-consuming, resulting in poor alignment between pathologists in some cases. Ultimately, these aspects have a great impact on diagnosis, prognosis and treatment of RCC neoplasms [7]. Computerized methods represents an interesting opportunity to improve the productivity, as well as the objectivity, of the microscopy-based RCC diagnosis [7]. In this regards, recent evidences suggest that Convolutional Neural Networkss (CNNs), a famous class of supervised deep learning algorithms, may be proficiently applied to the classification of RCC subtypes [7]. This is mainly due to the CNNs’ capability to discover unseen data structures and extract robust features representation [7, 8]. Nonetheless, much of their strength depend on the exploitation of large labeled datasets: CNNs are data-hungry supervised algorithms, which demand a large amount of annotated training sample to learn an effective data representation [9, 10]. This aspect is a strong limitation, especially in the histological field [11], where the samples annotation requires a skilled pathologist to visually scrutinise each WSIs and to divide it into sub-regions, homogeneous in terms of tissue architecture, lastly assigned to a corresponding label. This approach, known in the literature as \sayRegion Of Interest (ROI)-cropping procedure [11], is known to be tedious and time consuming for the pathologist, and also prone to error as well, with the concrete risk of compromising the models training phase due to an inaccurate labelling. As a consequence, Self-Supervised Learning (SSL) has been recently attracting considerable interest, being able to describe data structures via robust featurization, without requiring any supervision in term of samples annotations [9, 12]. Unfortunately, most self-supervised techniques exploit natural-scene image properties, which are not suitable for histopathology specimen, as evidenced by some recent works [9, 12]. This study sets out to assess RCC subtype classification, based on WSIs, leveraging a novel self-supervised paradigm. Our solution, inspired by the decision-making procedure of the pathologist, incorporates features learnt at different magnification level. With our experiments, we show that through histologic-specific SSL paradigm, it is possible to classify the four most common RCC subtypes with performance comparable to the fully supervised solutions, but without requiring massive data annotation."
https://arxiv.org/html/2411.09469v1,"An Explainable Attention Model for Cervical Precancer Risk Classification using
Colposcopic Images","Cervical cancer remains a major worldwide health issue, with early identification and risk assessment playing critical roles in effective preventive interventions. This paper presents the Cervix-AID-Net model for cervical precancer risk classification. The study designs and evaluates the proposed Cervix-AID-Net model based on patients colposcopy images. The model comprises a Convolutional Block Attention Module (CBAM) and convolutional layers that extract interpretable and representative features of colposcopic images to distinguish high-risk and low-risk cervical precancer. In addition, the proposed Cervix-AID-Net model integrates four explainable techniques, namely gradient class activation maps, Local Interpretable Model-agnostic Explanations, CartoonX, and pixel rate distortion explanation based on output feature maps and input features. The evaluation using holdout and ten-fold cross-validation techniques yielded a classification accuracy of 99.33% and 99.81%. The analysis revealed that CartoonX provides meticulous explanations for the decision of the Cervix-AID-Net model due to its ability to provide the relevant piece-wise smooth part of the image. The effect of Gaussian noise and blur on the input shows that the performance remains unchanged up to Gaussian noise of 3% and blur of 10%, while the performance reduces thereafter. A comparison study of the proposed model’s performance compared to other deep learning approaches highlights the Cervix-AID-Net model’s potential as a supplemental tool for increasing the effectiveness of cervical precancer risk assessment. The proposed method, which incorporates the CBAM and explainable artificial integration, has the potential to influence cervical cancer prevention and early detection, improving patient outcomes and lowering the worldwide burden of this preventable disease.","Cervical cancer is the fourth leading cause of death among female malignancies, with high morbidity and mortality rates if diagnosed in late stages, mainly affecting sexually active adult women aged over 30 years [1, 2, 3]. The cervix is part of the female reproductive system, located in the lowest fibromuscular section of the uterus and accessible for inspection and sampling though the vagina. The cervix has different linings. The endocervical canal is lined with glandular epithelium, and the ectocervix is lined with squamous epithelium. The squamous epithelium meets the glandular epithelium at the squamocolumnar junction (SCJ). The SCJ moves during early adolescence and during a first pregnancy. The original SCJ originates in the endocervical canal, but over time, the SCJ comes to lie on the ectocervix and becomes the new SCJ [4]. In colposcopy terminology, the SCJ is this new SCJ. The epithelium between these two SCJs is the transition zone (or transformation zone, TZ), and its position is variable, depending on factors such as age, hormonal status, birth trauma, use of oral contraceptives, and pregnancy [5, 6]. Colposcopically the TZ is classified as: Type 1, Type 2 and Type 3, depending on its visibility [7, 8]. Cervical cancer is preceded by Cervical Intraepithelial Neoplasia (CIN) which arises in the TZ. In the year 2018, the World Health Organization (WHO) issued a worldwide call to eliminate cervical cancer [9]. Cervical cancer develops from CIN lesions over years making time for screening, diagnosis and preventive treatment. There is an urgent need for accurate and timely detection of cervical cancer. This is particularly so in low- and middle-income nations, where severe poverty and gender discrimination significantly restrict a woman’s ability to seek care, accounting for almost 88% of cervical cancer fatalities [10]. Many screening methods are available today to detect precancerous lesions and abnormal growth of epithelial cells on the cervix. These methods include primary screening with cervical cytology (Pap tests), the human papillomavirus (HPV) test or co-testing. Cervical cytology requires competent cytologists to perform microscopic analyses, often unavailable in low-resource settings [11]. The HPV test is recommended by the WHO strategy due to the high sensitivity of the test, but the associated lower specificity increases the number of screen positive females referred for secondary screening by colposcopy and adds to the burden of specialists. Medical authorities recommend colposcopy as the gold standard for assessing cervical cancer precursors. Examining colposcopy images is time-consuming and requires trained medical specialists. Even skilled colposcopist miss cervical precancerous lesions in need for preventive treatment in 40% of examinations [12]. In recent years, automated decision-making using medical imaging techniques has increased multi-fold due to the advancement of artificial intelligence (AI). Automated decision-making AI models has reflected an immense potential for the detection of malignant tumors [13, 14, 15]. These advancements in AI have also attracted colposcopists to get assistance from AI in clinical decision-making. Therefore, there is an increasing interest in using AI to automate colposcopy image assessments, so that, identifying the presence of precancerous or cancerous cells in the cervix, becomes feasible at a large scale, with higher speed and lower costs, than nowadays."
https://arxiv.org/html/2411.09451v1,DiffRoad: Realistic and Diverse Road Scenario Generation for Autonomous Vehicle Testing,"Generating realistic and diverse road scenarios is essential for autonomous vehicle testing and validation. Nevertheless, owing to the complexity and variability of real-world road environments, creating authentic and varied scenarios for intelligent driving testing is challenging. In this paper, we propose DiffRoad, a novel diffusion model designed to produce controllable and high-fidelity 3D road scenarios. DiffRoad leverages the generative capabilities of diffusion models to synthesize road layouts from white noise through an inverse denoising process, preserving real-world spatial features. To enhance the quality of generated scenarios, we design the Road-UNet architecture, optimizing the balance between backbone and skip connections for high-realism scenario generation. Furthermore, we introduce a road scenario evaluation module that screens adequate and reasonable scenarios for intelligent driving testing using two critical metrics: road continuity and road reasonableness. Experimental results on multiple real-world datasets demonstrate DiffRoad’s ability to generate realistic and smooth road structures while maintaining the original distribution. Additionally, the generated scenarios can be fully automated into the OpenDRIVE format, facilitating generalized autonomous vehicle simulation testing. DiffRoad provides a rich and diverse scenario library for large-scale autonomous vehicle testing and offers valuable insights for future infrastructure designs that are better suited for autonomous vehicles.","I INTRODUCTION Testing and validating autonomous vehicles (AVs) necessitate a substantial number of diverse scenarios [1]. Road scenarios are the foundation for autonomous driving tests [2]. However, there is a significant lack of comprehensive road scenario libraries suitable for digital twin-based simulation testing [3]. The complexity and variability of real-world road scenarios critically affect the safety and overall performance of AVs. One critical bottleneck in AV simulation testing is the limited availability of diverse road scenarios for simulation purposes, with existing scenarios being predominantly homogeneous. Therefore, there is a pressing need for an effective road scenario generation method that can produce realistic and diverse road scenarios to enhance AV testing. In addition, the rapid advancement of AV technology is ushering in a transformative era in the transportation sector [4]. Unlike human-driven vehicles, AVs operate in fundamentally different ways [5]. However, the existing road infrastructure, primarily developed for human drivers, may not fully capitalize on the benefits provided by this emerging technology [6]. To ensure seamless integration and optimal performance, road infrastructure needs to adapt accordingly. There is an urgent need for a comprehensive library of road scenarios tailored for digital twin-based AV testing. Such a library is essential for guiding the future design of road infrastructure, ensuring it meets the unique requirements of AVs and maximizes their potential benefits. To achieve high-precision intelligent driving simulation tests, several road scenario formats have been proposed, including OpenDrive [7], Lanelet2 [8], and CommonRoad [9]. Among these, OpenDrive is widely used by most well-known simulators, such as Intel’s CARLA [10], Virtual Test Drive (VTD) [11], Apollo [12], PreScan [13], and SUMO [14]. Despite its widespread use, there is little work on generating realistic and diverse road scenarios. Current methods for constructing test road scenarios rely primarily on aerial imagery [15], sensor data reconstruction [16], or manual creation using commercial tools such as MATLAB RoadRunner [17]. Besides, the CommonRoad Scenario Designer is designed in [18] to to convert OpenStreetMap (OSM) [19] maps maps to CommonRoad format. This scenario designer only enables scenario format conversion and cannot generate more scenarios for AV testing. A model-driven approach is proposed in [20] to generate interchanges based on the topology graph. However, this method can only generate specific types of interchanges scenarios. In addition, all existing methods are limited to either converting road scene formats or generating a single type of road scene from existing data, and they fall short of generating large-scale, realistic, and diverse road scenes. Therefore, an efficient method for generating a comprehensive library of realistic and diverse road scenarios for AV testing has become a critical challenge. TABLE I: Comparison of existing road scenario generation methods. Method 3D Scene Lane Level OpenDrive Simulation Test Intersection PUDO Roundabout Flyover Scenes StreetGAN [21] - - - - \checkmark - - - \ast Conditional GANs [22] - - - \checkmark \checkmark - - - \ast CruzWay [23] - \checkmark \checkmark \checkmark \checkmark - - - \ast CommonRoad [18] - \checkmark \checkmark \checkmark \checkmark - \checkmark - 39,799 JunctionArt [24] - \checkmark \checkmark \checkmark \checkmark - - - \ast Predefine [25] - \checkmark \checkmark \checkmark - - \checkmark - \ast FLYOVER [20] \checkmark \checkmark \checkmark \checkmark - - - \checkmark 1443 GFlowNets [26] - \checkmark - \checkmark - - \checkmark - \ast Autoencoder and GAN [27] - - - - \checkmark - - - \ast HDMapGen [28] - - - - \checkmark - - - \ast RoBus [29] - - \checkmark \checkmark \checkmark - - - 72,400 DriveSceneGen [30] - \checkmark - \checkmark \checkmark - - - \ast DiffRoad \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark Infinite To address the challenges in road scenario generation for AV testing, we introduce DiffRoad, a diffusion model-based framework that can effectively capture the spatial distribution of road structures and generate high-quality road scenarios. The core idea behind this approach is to perturb the road distribution with noise through a forward diffusion process and then recover the original distribution from Gaussian noise via a learning-based denoising process. This approach results in a highly flexible road scenario generation model. To achieve controllable road scenario generation, we introduce an innovative road attribute embedding module. This module encodes road attribute information through a wide and deep network, enabling controllable, high-precision road scenario generation. Besides, to further improve the realism and usability of the generated road scenarios, we develop a scene evaluation and screening model. Finally, the evaluated scenarios are automatically converted into a generic OpenDRIVE format, facilitating digital twin-based AV testing. In summary, our main contributions are as follows: • We introduce DiffRoad, the first work to utilize the diffusion model for 3D road scenario generation, pioneering the creation of a diverse lane-level road scenario library. DiffRoad effectively harnesses the diffusion model to capture the distribution of real-world road structures, thereby enabling the generation of realistic and varied road scenarios with high efficiency and transferability. • To capture the spatial distribution of road structures and achieve controllable, high-fidelity road scenario generation, we design a novel denoising network architecture called Road-UNet. Road-UNet integrates the FreeU architecture to optimize the balance between the backbone and skip connections, significantly enhancing the quality of generated road scenarios. Furthermore, we introduce a smoothness regularization mechanism to improve the continuity and realism of the generated roads, ensuring smoother transitions and more realistic road layouts. • We propose a scene-level scoring function to select the generated road scenarios that are more realistic and sensible. To enhance the generalizability of the generated road scenarios, we automatically convert them into the OpenDRIVE format, making them compatible with mainstream simulation test platforms. • The effectiveness of DiffRoad is validated using three road datasets that we collected. The results demonstrate that our method can generate high-fidelity road scenarios while preserving essential statistical properties. Figure 1: The architecture of DiffRoad. The overarching framework of DiffRoad primarily comprises the road structure data generation model based on the enhanced conditional diffusion model, alongside the scene-level evaluation and filtering module. DiffRoad incorporates the road attention mechanism and Road MultiFreeU-Net (Road-UNet) module, iteratively refining the noise estimation based on the road attribute information to synthesize realistic and diverse road scenarios."
https://arxiv.org/html/2411.09429v1,"AI-driven inverse design of materials: Past, present and future","The discovery of advanced materials is the cornerstone of human technological development and progress. The structures of materials and their corresponding properties are essentially the result of a complex interplay of multiple degrees of freedom such as lattice, charge, spin, symmetry, and topology. This poses significant challenges for the inverse design methods of materials. Humans have long explored new materials through a large number of experiments and proposed corresponding theoretical systems to predict new material properties and structures. With the improvement of computational power, researchers have gradually developed various electronic structure calculation methods, particularly such as the one based density functional theory, as well as high-throughput computational methods. Recently, the rapid development of artificial intelligence technology in the field of computer science has enabled the effective characterization of the implicit association between material properties and structures, thus opening up an efficient paradigm for the inverse design of functional materials. A significant progress has been made in inverse design of materials based on generative and discriminative models, attracting widespread attention from researchers. Considering this rapid technological progress, in this survey, we look back on the latest advancements in AI-driven inverse design of materials by introducing the background, key findings, and mainstream technological development routes. In addition, we summarize the remaining issues for future directions. This survey provides the latest overview of AI-driven inverse design of materials, which can serve as a useful resource for researchers.","Advanced materials form a cornerstone of our modern information society, acting as a key catalyst for technological progress and industrial expansion, advancing at an unprecedented rate [1, 2, 3, 4, 5, 6]. Their utilization extends across diverse industries such as aerospace, biomedical engineering, energy storage, and information technology, which are all poised to benefit significantly from the integration of innovative materials that can surmount existing constraints [7]. Undoubtedly, the evolution of novel materials is crucial for fostering technological breakthroughs, invigorating economic prosperity, and elevating the standard of living. Generally, materials science is a major scientific discipline dedicated to the study of advanced functional materials. The search for advanced materials through inverse design of materials is an important research field within materials science. Inverse design of materials essentially involves creating an optimization space based on the desired performance attributes of materials. This process strives to establish a high-dimensional, nonlinear mapping from material properties to structural configurations, while adhering to physical constraints. The development of inverse design of materials has garnered widespread attention in academic works and can be categorized into four major paradigms: \bullet Experiment-driven paradigm. The experimental-driven paradigm is the original method of material discovery, and these methods have played a crucial role in propelling the field of materials science forward. For example, Madame Curie used experiments to discover the new elements radium and polonium, Onnes discovered the superconductivity phenomenon in mercury [8], and the discovery of the high-temperature superconducting material MgB2 [9]. However, this experimental-driven paradigm heavily relies on trial-and-error experimentation, individual expertise, and phenomenological scientific theories. Moreover, these methods are characterized by iterative cycles of experiments and observations to determine the properties and behaviors of materials, which is not only time-consuming and resource-intensive but also leads to extended research cycles and increased costs. In materials research, a heavy dependence on personal experience is also common. Although experienced researchers can guide experimental design with their intuition and prior knowledge, this method is limited in terms of reproducibility and scalability. The quantification and transfer of personal experience often face challenges, hindering the process of knowledge accumulation and dissemination [10]. Additionally, personal biases and misunderstandings may arise due to individual backgrounds and cognitive limitations [2]. \bullet Theory-driven paradigm. The theory-driven paradigm emphasizes the key role of theoretical insights and computational models in materials science. This paradigm is characterized by the widespread use of molecular dynamics simulations and thermodynamic models to understand and predict material behavior. These developments have, to a certain extent, simplified material research and enhanced the efficiency of investigations into new materials. There are some very famous materials and states of matter that were first predicted by theory and then verified experimentally, and these discoveries have extremely high scientific significance, even winning Nobel Prizes. In 1930, British physicist Paul Dirac derived the existence of “antimatter” from his quantum mechanical equations, suggesting that for every particle there is a corresponding antiparticle [11]. Dirac’s equations first predicted the existence of the positron, the antiparticle of the electron. In 1932, Carl Anderson discovered the positron in cosmic rays, confirming Dirac’s prediction [12]. Anderson was awarded the Nobel Prize in Physics in 1936 for this discovery, and Dirac also received the Nobel Prize in Physics in 1933 for his contributions to quantum mechanics, including the theoretical prediction of antimatter. John Bardeen, Leon Cooper, and John Schrieffer proposed the BCS theory, which explained the cause of superconductivity—the pairing of electrons to form Cooper pairs, leading to zero electrical resistance [13, 14, 15]. Although the BCS theory itself did not directly predict new materials, it became the foundation for research into low-temperature superconductivity. Bardeen, Cooper, and Schrieffer were awarded the Nobel Prize in Physics in 1972 for their work. In 2005, Charles Kane and Eugene Mele predicted the existence of the quantum spin Hall effect through topological quantum theory, a phenomenon where edge state conduction occurs without an external magnetic field. This concept laid the foundation for topological insulators. In 2007, scientists first realized the quantum spin Hall effect in HgTe quantum wells, verifying the accuracy of the theory. These classic predictions demonstrate the powerful predictive ability of theoretical physics in the exploration of new materials and phenomena, and also promote the development of experimental physics. Many such theoretical breakthroughs ultimately won the Nobel Prize for experimental verification, marking significant advances in physics. Nevertheless, these theoretical frameworks often require complex mathematical models that are demanding in terms of computational resources and expertise. Their applicability may be limited, especially when it comes to material systems that exhibit multi-scale phenomena and complex interactions. \bullet Computation-driven paradigm. Established on the groundwork of theoretical progress, the computation-driven paradigm has risen alongside the surge in computational power and data accessibility. This paradigm leverages computational models to simulate material behaviors and inform the design process. The application of density functional theory (DFT) [16, 17, 18] and computational chemistry tools like Hartree-Fock theory has revolutionized our ability to predict and optimize material properties. The DFT has played a crucial role in investigating the electronic structure of graphene. It was through the DFT that the zero-band gap structure of graphene was uncovered, a feature that is essential for its electronic properties and holds significant potential for its application in electronic devices [19]. Furthermore, the Hartree-Fock method has been extensively applied in calculating molecular orbitals, such as in studies of water molecules, offering vital insights into their geometric and electronic characteristics [20]. The potency of these methods is clear in their capacity to manage complex systems that were once difficult to dissect. However, reliance on computational models introduces challenges, as the accuracy of predictions heavily relies on model quality and available computational resources. Additionally, high-throughput screening (HTP) and combinatorial screening have become significant methodologies for exploring new materials and systems. This approach greatly expedites the discovery and development of novel materials, particularly in drug discovery, catalyst design, and energy storage material development, where it allows for the parallel assessment of vast compound libraries to identify potential new materials [3]. Despite HTP’s achievements in materials science, challenges remain, including substantial resource requirements for physical or computational experiments, constraints by existing material libraries, and insufficient consideration of intricate relationships between material properties [21]. \bullet AI-driven paradigm. With the dawn of the big data era, materials science has transitioned into an AI-driven paradigm. Artificial Intelligence (AI) marks a significant shift within the engineering field, heralding an era defined by enhanced intelligence and automation. Both the widely utilized auto-regressive models based on the Transformer architecture [22] and the robust diffusion models [23, 24] have made substantial contributions to the advancement of inverse design of materials [25, 26, 27, 28]. The capacity of AI to discern patterns and formulate predictions from data has sparked notable transformations in materials science. By examining vast experimental data and computational simulation outcomes, AI models reveal intricate correlations between material properties and their underlying crystal structures [29, 30]. Furthermore, a recent survey [25] has integrated inverse design methodologies with machine learning models to forecast the mechanical properties of materials, including elastic modulus and yield strength, thus expediting the discovery and development of innovative materials. These data-driven strategies not only bolster the precision of predictions but also considerably shorten material development cycles. Consequently, “machine learning materials discovery” is garnering escalating research interest (See Figure 1 (a)). In parallel, the exponential growth of the total known materials over time highlights the accelerated pace of material discovery processes, with significant contributions from Google’s GNoME [31] and Meta’s OMat24 [32]. Figure 1: Trends in publications and citations in the field of “Machine Learning Materials Discovery” from 2019 to 2024. The left panel (a) illustrates the growth in the number of publications (represented by blue bars) alongside the total citations (depicted by the red line with markers), reflecting a significant increase in both metrics over the past few years. The right panel (b) presents the variation in the total known materials over time on a logarithmic scale, highlighting the acceleration of material discovery processes facilitated by GNoME [31] of Google and OMat24 [32] of Meta. This figure underscores the rapid development within the field of machine learning materials discovery Figure 2: Materials Science Research Paradigms This figure illustrates the evolution of research paradigms in materials science, emphasizing key milestones across various approaches. It highlights the increasing role of artificial intelligence in driving future materials discovery, with AI becoming the dominant force in shaping the field. The experiment-driven paradigm is exemplified by Marie Curie’s Nobel Prize-winning discovery of radium and polonium. The theory-driven paradigm is represented by Dirac’s equation, which predicted the existence of the positron, as well as the quantum spin Hall effect. The computation-driven paradigm is demonstrated through DFT calculations applied to materials such as MgB2, hydride superconductors, and graphene. Finally, the AI-driven paradigm showcases recent breakthroughs in artificial intelligence, including AlphaFold2, GPT-4, and AI-accelerated materials discovery, signaling the frontier of research in the field. As discussed before, AI-driven methods are not a new technical concept for inverse design of functional materials, but has evolved with the advance of AI over the decades. The experiment-driven and theory-driven paradigms mainly aim to discover new functional materials based on heavily trial-and-error experiments or theoretical models, while latest AI-driven methods concentrate on constructing the hidden mappings between material functions and crystal structures. From conventional experiment methods to data-driven methods, it is an important leap in using modern advanced computational methods to design target functional materials. In Figure 2, we delineate the evolution of materials science discoveries alongside the progression of time and technological advancements. Initially, the confirmation of new materials was primarily achieved through experimentation (such as Madame Curie’s discovery of new elements), which entailed substantial costs and numerous trials. Subsequently, theoretical paradigms were introduced, predicting physical properties that were later experimentally verified (for instance, the prediction and validation of semiconductors). With the advancement of computer technology and the enhancement of computational ability, high-throughput computational methods have become a significant avenue for the discovery of new materials. Ultimately, as the latest generation of technology, AI methods can efficiently generate and screen new functional materials by elucidating the hidden correlations between crystal structures and properties, thereby accelerating the discovery of new materials. In summary, throughout this evolutionary process, technological progress has enabled us to employ more sophisticated methods to expedite the discovery of new functional materials. In the existing literature, the inverse design of materials has been extensively discussed and surveyed [2, 3, 4, 5, 6]. However, current surveys often focus on specific machine learning algorithms or particular types of materials, leading to a lack of systematic integration of diverse methodologies and a broad range of material systems. Many existing surveys predominantly concentrate on specific categories of materials or application scenarios, such as topological insulators or high-entropy alloys [33, 34], thereby failing to provide a comprehensive examination of AI-driven inverse design of materials. Specifically, there is a significant absence of comparative and analytical studies across different researches about AI-driven inverse design of materials, which hinders a holistic understanding of the advantages, disadvantages, and applicable contexts of various methodologies. These limitations pose challenges for readers attempting to fully grasp the overarching landscape of AI-driven inverse design of materials, particularly with respect to the dynamic evolution of new methods and their applications. Faced with both opportunities and challenges, it needs more attention on the research and development of AI-driven inverse design of materials. In order to provide a basic understanding of this research filed, this survey aims to address the gaps in the current body of research by comprehensively examining previous studies from two perspectives: the discovery of functional materials based on AI methods and the development of AI methods in materials science. We systematically analyze the latest advancements in AI technologies within the domain of inverse design of materials. We conduct an exhaustive survey of the literature to synthesize the pivotal discoveries, AI methods, and procedural methods in the inverse design of functional materials. We are also aware of several relative survey articles on inverse design of materials [35, 36]. The reference [36] provides an overview of the importance and utilization of graph neural networks (GNNs) within the realms of chemistry and materials science. GNNs can directly process the graphical representations of molecules and materials, thereby capturing the essential information required to characterize these materials. The review further outlines the fundamental principles of GNNs, including commonly used datasets and architectures, and emphasizes their critical role throughout the materials development. Another reference [35] systematically explores the research advancements of geometric GNNs in the applications of materials discovery and drug discovery. It emphasizes the significance of geometric graphs in scientific fields, particularly their ability to capture geometric features such as the three dimensional coordinates of nodes. The survey further defines geometric graphs and compares them with traditional graphs, elucidating their distinct characteristics. Additionally, it summarizes existing models, including invariant GNNs and equivariant GNNs. Our survey presents the latest advancements in functional materials accelerated by machine learning techniques, with particular emphasis on the robust representational capabilities of geometric GNNs. Furthermore, we provide a comprehensive overview of advanced generative models and large language models, highlighting their pivotal roles in driving materials discovery. In the end, we have gathered standard datasets and benchmarks that are crucial for material discovery, in order to advance the methods of AI-driven inverse design of materials. “All great achievements take time.”–The inverse design of materials has undergone a long development to achieve the latest successes. Our goal is to delve into the historical progression of material directional design, with a particular focus on the innovative approaches to material discovery that have arisen in the era of artificial intelligence. This survey will shed light on the critical issues inherent in inverse design of materials. Subsequently, we discuss the specific classes of materials that are currently at the forefront of materials science (including superconducting materials, magnetic materials and so on), and detail the AI technologies. We have also compiled a comprehensive overview of the evolution of AI technologies within the materials domain. It is our aspiration that the contributions presented in this paper will propel the field of AI-driven inverse design of materials to new heights of advancement. In what follows, we will introduce the the recent applications of AI-driven method in expediting the inverse design of functional materials in Section 2. We then elaborate on the development history of AI technology in the field of materials science in Section 4. Finally, we will briefly discuss a series of open problems and promising directions in the future in Section 4 and conclude in Section 5."
https://arxiv.org/html/2411.09420v1,"SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph Attention for Vision Transformers","Image classification is a computer vision task where a model analyzes an image to categorize it into a specific label. Vision Transformers (ViT) improve this task by leveraging self-attention to capture complex patterns and long-range relationships between image patches. However, a key challenge for ViTs is efficiently incorporating multi-scale feature representations, which is inherent in CNNs through their hierarchical structure. In this paper, we introduce the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework that addresses this challenge by integrating multi-scale features. Using EfficientNet as a backbone, the model extracts multi-scale feature maps, which are divided into patches to preserve semantic information. These patches are organized into a graph based on spatial and feature similarities, with a Graph Attention Network (GAT) refining the node embeddings. Finally, a Transformer encoder captures long-range dependencies and complex interactions. The SAG-ViT is evaluated on benchmark datasets, demonstrating its effectiveness in enhancing image classification performance.","The field of image classification has experienced significant advancements with the introduction of deep learning architectures. CNNs have long been the foundation for image classification tasks due to their proficiency in capturing local spatial hierarchies through convolutional operations [9]. However, their inherent limitations in modeling long-range dependencies restrict their ability to fully exploit global contextual information within images [12]. The introduction of Vision Transformers (ViT) [18, 4] has opened new avenues by leveraging self-attention mechanisms to model global relationships within images. ViTs treat images as sequences of patches (tokens) and have demonstrated competitive performance compared to traditional CNNs. Despite their success, ViTs often require large-scale datasets for effective training and may overlook fine-grained local details due to their fixed-size patch tokenization [17]. Recent research has highlighted the importance of multi-scale feature representations in enhancing ViTs’ performance across various vision tasks [1]. Multi-scale approaches enable models to capture objects and patterns of varying sizes, providing a more comprehensive understanding of the image content. While CNNs inherently capture multi-scale features through hierarchical layers, integrating this capability efficiently into Transformer-based models remains a challenge. To handle this challenge, we propose a novel Transformer-based framework called Scale-Aware Graph Attention Vision Transformer (SAG-ViT). Our model begins by extracting rich, multi-scale feature maps from input images using a pre-trained EfficientNet backbone [16]. We then divide these feature maps into patches, preserving high-level semantic information and reducing information loss compared to raw image patching. We then construct graphs where each node represents a feature map patch, and edges are established based on spatial adjacency and feature similarity using a k-connectivity scheme. This graph captures both local and global relationships among image regions. A Graph Attention Network (GAT) [19, 24] processes the graph, dynamically focusing on the most relevant patches. The enriched node embeddings are then passed through a Transformer encoder, which captures long-range dependencies and complex interactions. Our contributions are summarized as follows: • We introduce a patching mechanism that operates on CNN-derived feature maps, retaining rich semantic information and efficiently capturing multi-scale features. • A k-connectivity and similarity-based edge weighting scheme is developed in the proposed Transformer architecture to construct graphs that model intricate spatial relationships between patches. • We employ a GAT Network to process the information-rich graph embeddings to effectively model both local and global dependencies within images. • We validate our method on multiple benchmark datasets across different domains, demonstrating higher performance compared to other transformer-based approaches. The remainder of this paper is organized as follows: Section 2 reviews related work on graph transformers, attention mechanisms, multi-scale feature embedding, and their integration in image classification. Section 3 details our proposed method, including the architecture and graph construction process. Section 4 presents the experimental setup, datasets, and evaluation metrics. Section 5 discusses the results, and Section 6 concludes the paper."
https://arxiv.org/html/2411.09413v1,SCRIPT-CENTRIC BEHAVIOR UNDERSTANDING FOR ASSISTED AUTISM SPECTRUM DISORDER DIAGNOSIS,"Observing and analyzing children’s social behaviors is crucial for the early diagnosis of Autism Spectrum Disorders (ASD). This work focuses on automatically detecting ASD using computer vision techniques and large language models (LLMs). Existing methods typically rely on supervised learning. However, the scarcity of ASD diagnostic datasets and the lack of interpretability in diagnostic results significantly limits its clinical application. To address these challenges, we introduce a novel unsupervised approach based on script-centric behavior understanding. Our pipeline converts video content into scripts that describe the behavior of characters, leveraging the generalizability of large language models to detect ASD in a zero-shot or few-shot manner. Specifically, we propose a scripts transcription module for multimodal behavior data textualization and a domain prompts module to bridge LLMs. Our method achieves an accuracy of 92.00% in diagnosing ASD in children with an average age of 24 months, surpassing the performance of supervised learning methods by 3.58% absolutely. Extensive experiments confirm the effectiveness of our approach and suggest its potential for advancing ASD research through LLMs.","ASD, characterized by deficits in social communication and the presence of restricted, repetitive behaviors or interests, is a neurodevelopmental disorder affecting approximately 2.3% of children and approximately 2.2% of adults [1]. Individuals with ASD often experience additional developmental, behavioral, and mental health challenges, which can impose a lifelong burden on families [2]. Experts generally agree that special training and therapy for ASD treatment should begin as early as possible [3], making early and accurate diagnosis critically important. Traditional ASD diagnostic methods include the Autism Diagnostic Interview (ADI) [4], the Autism Diagnostic Observation Schedule (ADOS), and ADOS-2 [5]. These methods require an experienced physician to spend amount of considerable time on each assessment. Furthermore, diagnosis relies on specialized scales and is heavily dependent on the physician’s subjective judgment and clinical experience. Consequently, the promotion of traditional ASD diagnostic methods is limited, particularly in regions with underdeveloped healthcare resources. Many previous studies have demonstrated significant potential in assisted ASD diagnosis using sensor technology [6] and artificial intelligence techniques [7]. We focus on automatically detecting ASD from audio-visual behavior data. Mainstream methods can be divided into two categories: the first is based on behavioral signal. These methods [8, 9, 10] analyze behavior-related features from paradigmatic videos using machine learning techniques, such as body movements, head movements, and eye patterns. The second category [11, 12, 13] involves deep learning methods, which directly train raw video data to predict labels. While both types of methods perform well on their respective datasets through supervised learning, the scarcity of ASD data limits their accuracy in practice. Moreover, these methods only provide a binary prediction for the diagnostic outcome and lack interpretive explanation. Figure 1: The overview of our proposed Script-Centeric Behavior Understanding(SCBD). Behavior Transcription Module converts audio-visual data into behavioral logs using multiple well-trained behavior signal processing models. Scipt Transcription Module textualizes Behavior Logs in steam and integrate domain prompt. Large Language Models are used to understand and anwser script content. To address the challenges of limited data and interpretability, we propose a novel approach to perform assisted ASD diagnosis from audio-visual behavior data. The recent rapid development of LLMs [14, 15, 16, 17, 18] has demonstrated their extraordinary capabilities, including understanding, reasoning, and question answering. These capabilities allow LLMs to perform well a wide range of downstream tasks in a zero-shot manner. Because LLMs have absorbed lots of human knowledge from massive textual data, they only require very little domain knowledge, reducing the need to collect clinical data from ASD patients. Furthermore, their question-answering abilities enable the interpretation of diagnostic results. However, LLMs currently specialize in text data or text-image data, understanding domain-specific audio-visual behavior signals remains challenging for two main reasons: 1) There is a significant gap between different modalities[19, 20], and multimodal LLMs only do not perform well on real audio-visual data in complex scenarios[21, 22]. 2) Due to privacy protection, there is very limited audio-visual behavior data from ASD children[23]. As a result, this paper introduces a novel methodology that utilize LLMs to detect ASD and explain the reasons. Specifically, to bridge the gap between video and LLMs, we employ the Behavioral Transcription Module(BTM) to transform video content into human behavioral data. Subsequently, we design the Script Transcription Module (STM) to process this behavioral log, generating video script descriptions based on characters’ behavioral responses, expressions, languages and the timing of their occurrences. To enhance diagnostic accuracy, we created the Domain Prompts Module (DPM) to incorporate domain knowledge and produce domain-adaptive behavior descriptions. Ultimately, we leverage a pretrained LLM to detect ASD and provide explanations by answering questions. Our main contributions can be summarized as follows: \bullet To the best of our knowledge, our approach is the first to use LLMs for detecting ASD from audio-visual data. \bullet To accurately describe the behavior data and leverage prior knowledge, we propose STM and DPM respectively. \bullet Extensive experimental results and ablation studies verify the effectiveness of our approach, making the exploration of LLMs on ASD detection more promising."
https://arxiv.org/html/2411.09403v1,Quantum Machine Learning: An Interplay Between Quantum Computing and Machine Learning,"Quantum machine learning (QML) is a rapidly growing field that combines quantum computing principles with traditional machine learning. It seeks to revolutionize machine learning by harnessing the unique capabilities of quantum mechanics and employs machine learning techniques to advance quantum computing research. This paper introduces quantum computing for the machine learning paradigm, where variational quantum circuits (VQC) are used to develop QML architectures on noisy intermediate-scale quantum (NISQ) devices. We discuss machine learning for the quantum computing paradigm, showcasing our recent theoretical and empirical findings. In particular, we delve into future directions for studying QML, exploring the potential industrial impacts of QML research.","Despite their remarkable natural language processing [1] and computer vision [2] achievements, deep neural networks face computational bottlenecks for emerging applications like drug discovery [3] and materials science. The massive size of large language models further exacerbates this issue. As machine learning continues to advance, the limitations of classical computing are becoming more apparent, hindering progress in these fields. A promising solution on the horizon is quantum computing [4]. The advent of quantum computing holds great potential for revolutionizing or enhancing the computational efficiency of machine learning algorithms. Although the deployment of quantum computers is still in its early stage, cloud-based services provide accessible quantum computing environments, such as IBM Quantum Experience [5] and CUDA Quantum [6], enabling users to develop quantum algorithms for machine learning. Quantum machine learning (QML) is an interdisciplinary field that combines quantum mechanics and machine learning [7, 8, 9]. Leveraging QML theories and algorithms can enhance the computational efficiency of machine learning models [10]. The noisy intermediate-scale quantum (NISQ) era, with its limited number of qubits and high noise levels, presents challenges but also opportunities for exploring the potential of quantum computing for machine learning [11, 12, 13]. In particular, variational quantum circuits (VQC) constitute a QML architecture [14], such as quantum convolutional neural networks [15] and quantum graph neural networks [16], for data processing and making predictions. The parametric quantum circuits (PQC) in VQC can be adjustable in the training process using optimization methods like stochastic gradient descent (SGD) to minimize the cost function in a back-propagation manner. The VQC has been demonstrated to be resilient to the quantum noise on NISQ devices [17, 18], highlighting the advantages of deploying VQC for implementing QML in many real-world applications. In addition to quantum computing for the machine learning paradigm, we are exploring how classical machine learning can contribute to developing QML. Given the current limitations of quantum computers, hybrid quantum-classical neural networks, combining classical and quantum components, are commonly used in QML to use available resources best. These hybrid architectures harness the speed advantages of quantum computing for specific tasks while relying on classical computing for operations it performs more effectively. This paper also focuses on enhancing QML’s capabilities through classical machine learning techniques, aiming to improve its ability to represent data, generalize to new examples, and expand its applicability to real-world challenges. In the discussion session, we offer fresh perspectives beyond the existing QML paradigms and explore how to harness the potential of cutting-edge generative AI and emerging quantum computing technologies in real-world industry use cases."
https://arxiv.org/html/2411.09402v2,Automated Segmentation of Ischemic Stroke Lesions in Non-Contrast Computed Tomography Images for Enhanced Treatment and Prognosis,"Stroke is the second leading cause of death worldwide, and is increasingly prevalent in low- and middle-income countries (LMICs). Timely interventions can significantly influence stroke survivability and the quality of life after treatment. However, the standard and most widely available imaging method for confirming strokes and their sub-types, the NCCT, is more challenging and time-consuming to employ in cases of ischemic stroke. For this reason, we developed an automated method for ischemic stroke lesion segmentation in NCCTs using the nnU-Net frame-work, aimed at enhancing early treatment and improving the prognosis of ischemic stroke patients. We achieved Dice scores of 0.596 and Inter-section over Union (IoU) scores of 0.501 on the sampled dataset. After adjusting for outliers, these scores improved to 0.752 for the Dice score and 0.643 for the IoU. Proper delineation of the region of infarction can help clinicians better assess the potential impact of the infarction, and guide treatment procedures.","There is an estimated 15 million cases of stroke annually, with one-third of these cases leading to death, and another one-third leading to some form of disability [3]. The rate of prevalence is more apparent in Low- and Middle-Income Countries (LMICs), where existing healthcare infrastructures remain unprepared to deal with the surge in cases [16, 9]. Timely intervention is a critical factor in the survivability and post-treatment quality of life of patients [15]. Expert radiologist interpretations of the standard Non-Contrast Computed Tomography (NCCT) images are far more accurate for hemorrhagic stroke (95%), when compared to ischemic stroke where a diagnostic ruling is given for major onsets only two-thirds of the time [10, 15]. CT imaging is the most accessible modality for brain diagnostic imaging in LMICs [2]. The process of diagnosing stroke, and subsequently segmenting the lesion region using radiological images can guide treatment and predict rehabilitation outcomes [12, 4]. Manual segmentation can be highly time-consuming and error-prone, with significant inter-observer variability depending on the observer’s level of expertise [17]. The process of segmenting lesions in NCCT images is a relatively difficult task (due to low tissue contrast) as compared to MRI [14], especially when done manually. It is important that robust automated lesion segmentation processes for varying modalities (in this case, NCCT) be made available, especially in regions with limited imaging modality accessibility. Several works explore the segmentation of stroke lesions, employing a variety of approaches. Liang et. al [11] proposed the Symmetry-Enhanced Attention Network (SEAN) to automatically segment acute ischemic infarcts in NCCTs. The input image is first transformed into the standard space in an unsupervised manner by an ’Alignment Network’. In the transformed image with its bilateral symmetry, long range dependencies are better captured by SEAN, which has proven to outperform other existing symmetry based methods, with a dice score of 0.5784. The nnU-Net was developed by Fabian et. al [6], and is described as an out-of-the-box tool for biomedical image segmentation. It is a well-validated segmentation method, and has achieved state-of-the-art (SOTA) results in various biomedical image segmentation benchmark datasets such as the Medical Segmentation Decathlon (MSD) [1], Brain Tumor Segmentation (BraTS) Challenge [13], and the Ischemic Stroke Lesion Segmentation Challenge (ISLES) [5]. In this paper, we achieve promising results in ischemic lesion segmentation in NCCTs, by using out-of-the-box, well-validated methods. Initial image processing involved normalizing the input NCCT scans to ensure uniformity in image quality and resolution. We employed a Residual Encoder U-Net architecture from the nnU-Net framework, known for its robust performance in medical image segmentation tasks. The network was trained using a curated dataset consisting of NCCT images which were annotated using DWI MRI scans of the subjects as reference. The results obtained reiterate the potential of using validated methods to automate the difficult task of ischemic lesion segmentation in NCCT scans. Comparisons with existing techniques highlight the improvements and underscore areas for future enhancements."
https://arxiv.org/html/2411.09389v1,Less is More: Unseen Domain Fake News Detection via Causal Propagation Substructures,"The spread of fake news on social media poses significant threats to individuals and society. Text-based and graph-based models have been employed for fake news detection by analyzing news content and propagation networks, showing promising results in specific scenarios. However, these data-driven models heavily rely on pre-existing in-distribution data for training, limiting their performance when confronted with fake news from emerging or previously unseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news is a challenging yet critical task. In this paper, we introduce the Causal Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to enhance zero-shot fake news detection by extracting causal substructures from propagation graphs using in-distribution data and generalizing this approach to OOD data. The model employs a graph neural network-based mask generation process to identify dominant nodes and edges within the propagation graph, using these substructures for fake news prediction. Additionally, CSDA’s performance is further improved through contrastive learning in few-shot scenarios, where a limited amount of OOD data is available for training. Extensive experiments on public social media datasets demonstrate that CSDA effectively handles OOD fake news detection, achieving a 7%\sim16% accuracy improvement over other state-of-the-art models.","The popularity of social media has enabled rapid news dissemination, for both true and fake news. Given the potential impact of fake news, robust fake news detection methods are needed to debunk such news in a timely manner. In real-world scenarios, out-of-distribution news from unseen domains emerges over time. This brings substantial challenges to fake news detection models. Graph-based fake news detection methods using graph neural networks (GNN) have garnered much attention recently for modelling news propagation patterns (Gong et al. 2023a). Despite their success, existing GNN-based methods are generally built on the assumption that both training and testing data are independently sampled from an identical data distribution (i.i.d.), which often does not hold true nor reflect the real challenges of fake news detection (Li et al. 2022). Emerging and hitherto unseen fake news and their associated propagation graphs can and do appear. From an empirical perspective, these methods focus on minimising the average training error and incorporating correlations within the training data (which is considered to be in-distribution) to improve fake news detection accuracy (Liu et al. 2021). However, real-world graph-based fake news data is often mixed with biased domain-specific information in the training data. The detection model may thus learn these domain-specific biases resulting in misclassification of cross-domain news items (Li et al. 2022). To detect fake news across different domains (e.g., sports and politics), some early studies (Ma, Gao, and Wong 2018; Bian et al. 2020) focus on capturing content-independent propagation patterns. However, it has been shown (Min et al. 2022) that not only the news contents but also the propagation patterns can vary across different news domains. More recent approaches (Li et al. 2023; Lin et al. 2022) collect and manually label a small dataset from emerging news domains. They utilise domain adaptation methods to adapt the trained models to the emerging domains in a few-shot manner. However, these approaches require labelled data from emerging domains which is not always available and could be expensive and time-consuming. To address the limitations above, we focus on extracting causal subgraphs from news propagation graphs to eliminate potential domain biases. The patterns of such subgraphs are learnt for fake news detection in emerging domains. News from an emerging domain is considered as the out-of-distribution (OOD) data, and we generalise our model trained on in-distribution data to OOD data by capturing causal subgraphs in an unsupervised manner. From a causal analysis perspective, each propagation graph is composed of causal subgraph and biased subgraph which are initially entangled. Our intuition is that not all nodes in the propagation graph of a given news item are helpful for fake news detection. Instead, only some causal subgraphs of the propagation graph carry critical clues that can be used to identify fake news, as illustrated in Fig. 1 with an example. If we can identify and capture such causal subgraphs, we can improve fake news detection accuracy and subsequently improve the way we generalise the model to OOD data. Figure 1: Illustration of the causal subgraphs and the Structure Causal Models (SCMs). In the SCMs, the grey and white variables represent unobserved and observed variables. Further explanations on SCMs are given in Preliminaries. Based on this intuition, a cross-domain model – the Causal Subgraph Oriented Domain Adaptive Fake News Detection (CSDA) model, is proposed. This model extracts subgraphs from propagation graphs and performs detection based on the subgraphs. In CSDA, a binary mask is learned for each node and each edge of the propagation graph of a news item to classify them into causal or biased elements. For the subgraph formed by each type of element, a graph encoder and a multilayer perceptron (MLP) classifier together encode the subgraphs and classify the news item according to the subgraph embeddings. In the training process, we utilise a data augmentation strategy by concatenating the causal subgraph embedding and the permuted biased subgraph embedding. We then train CSDA with both embeddings to enhance the effectiveness of causal subgraph learning. In the testing process, only the causal branch of the CSDA model is utilised to predict news veracity. Following recent works (Li et al. 2023; Lin et al. 2022), we also consider a scenario where limited OOD data becomes available through manual labelling. In this scenario, CSDA’s performance on OOD data is further enhanced with a supervised contrastive learning-based approach and achieves state-of-the-art (SOTA) classification accuracy. In summary, our contributions include: • We propose a zero-shot cross-domain fake news detection model named CSDA based on extracting causal subgraphs related to news propagation patterns. • We further explore a few-shot scenario in cross-domain fake news detection where a small number of OOD examples are available, and we utilise contrastive learning to enhance CSDA’s cross-domain performance. • Extensive experiments are conducted on four real datasets. The results confirm the effectiveness of CSDA in cross-domain fake news detection, outperforming SOTA models by 7.69\sim 16.00\% in terms of accuracy."
https://arxiv.org/html/2411.09366v1,LTLf+ and PPLTL+: Extending LTLf and PPLTL to Infinite Traces,"We introduce LTLf+ and PPLTL+, two logics to express properties of infinite traces, that are based on the linear-time temporal logics LTLf and PPLTL on finite traces. LTLf+/PPLTL+ use levels of Manna and Pnueli’s LTL safety-progress hierarchy, and thus have the same expressive power as LTL. However, they also retain a crucial characteristic of the reactive synthesis problem for the base logics: the game arena for strategy extraction can be derived from deterministic finite automata (DFA). Consequently, these logics circumvent the notorious difficulties associated with determinizing infinite trace automata, typical of LTL reactive synthesis. We present DFA-based synthesis techniques for LTLf+/PPLTL+, and show that synthesis is 2EXPTIME-complete for LTLf+ (matching LTLf) and EXPTIME-complete for PPLTL+ (matching PPLTL). Notably, while PPLTL+ retains the full expressive power of LTL, reactive synthesis is EXPTIME-complete instead of 2EXPTIME-complete. The techniques are also adapted to optimally solve satisfiability, validity, and model-checking, to get EXPSPACE-complete for LTLf+ (extending a recent result for the guarantee level using LTLf), and PSPACE-complete for PPLTL+.","Reactive synthesis is concerned with synthesizing programs (aka, strategies) for reactive computations (e.g., processes, protocols, controllers, robots) in active environments (Pnueli and Rosner 1989; Finkbeiner 2016; Ehlers et al. 2017). The basic techniques for reactive synthesis share several similarities with Model Checking, and are based on the connections between Logics, Automata, and Games (Fijalkow et al. 2023). The most common specification language is possibly Linear Temporal Logic (LTL) (Pnueli 1977). Reactive Synthesis for LTL involves the following Steps: (1) having a specification \varphi of the desired system behavior in LTL, in which one distinguishes controllable and uncontrollable variables; (2) extracting from the specification an equivalent automaton on infinite words, corresponding to the infinite traces satisfying \varphi; (3) (differently from Model Checking) determinizing the automaton to obtain an arena for a game between the system and the environment; (4) solving the game, by fixpoint computation, for an objective determined by the automaton’s accepting condition (e.g., a parity objective for LTL), yielding a strategy for the system that fulfills the original specification \varphi. Model Checking is mature, and many of its techniques may be exploited in Reactive Synthesis as well, e.g., symbolic techniques based on Boolean encodings may be used to compactly represent the game arena and to compute fixpoints over it. However, despite this, Step (3) remains a major performance obstacle. For LTL, this involves determinizing nondeterministic Büchi automata, which is notoriously difficult (Vardi 2007). This has held back the use of reactive synthesis in applications. Reactive synthesis is deeply related to Planning (De Giacomo and Rubin 2018; Alberto, Bienvenu, and McIlraith 2019), and in particular to (strong) planning for temporally extended goals in fully observable nondeterministic domains (Cimatti et al. 2003; Bacchus and Kabanza 1998, 2000; Calvanese, De Giacomo, and Vardi 2002; Baier and McIlraith 2006; Baier, Fritz, and McIlraith 2007; Gerevini et al. 2009). A key characteristic of Planning is that the system continuously receives a goal, “thinks” about how to achieve it, synthesizes a plan, executes the plan, and repeats (Geffner and Bonet 2013). This suggests to focus on goal specifications that can be satisfied on finite traces. Recently, this led to a shift in Reactive Synthesis to focus on logics on finite traces (instead of infinite traces), e.g., LTLf (De Giacomo and Vardi 2013, 2015). The advantage of focusing on finite traces is that in Step (3) one can rely on (classic) automata operating on finite traces, including deterministic finite automata (DFA), and use known determinization algorithms with good practical performance. The development of LTLf synthesis (De Giacomo and Vardi 2015) has brought about scalable tools that are unprecedented in reactive synthesis (Zhu et al. 2017; Bansal et al. 2020; De Giacomo and Favorito 2021; De Giacomo et al. 2022). Beside LTLf, another finite-trace logic that is gaining popularity in AI is Pure Past LTL (PPLTL) (De Giacomo et al. 2020; Cimatti et al. 2020; Bonassi et al. 2023b, a, 2024). This is a variant of LTLf that sees the trace backwards and has the notable property that one can obtain a symbolic (i.e., factorized) DFA directly from the formula in linear time; moreover, while the size of the (non-symbolic) DFA corresponding to an LTLf formula can be double-exponential in the size of the formula itself, the size of the DFA corresponding to a PPLTL formula is at most a single-exponential in the size of the formula. Nevertheless, not all specifications of interest can be expressed on finite traces. For example, the planning domain is an infinite-trace specification: the planning domain will continue to respond to actions (with preconditions satisfied) by producing its possibly nondeterministic effects, forever. Not to mention recurrence, persistence, reactivity, and other properties typically used in Model Checking. When dealing with infinite traces, using (same variants of ) LTL as the specification language is an obvious choice. Can we lift the DFA techniques at the base of the success story of LTLf and PPLTL synthesis to full LTL? In this paper we answer this question positively! To do so, we leverage the classic hierarchy of LTL properties — the safety-progress hierarchy (Manna and Pnueli 1990).111 The hierarchy was introduced by Lichtenstein, Pnueli, and Zuck in 1985, later described in detail by Manna and Pnueli in 1990 and in their books (Manna and Pnueli 1992, 1995, 2010); also, see the survey (Piterman and Pnueli 2018). It consists of six classes of semantic properties, organized by inclusion. The bottom, first, level has, on the one hand, the safety properties (that intuitively express that nothing bad ever happens), and on the other the guarantee properties, sometimes also called co-safety properties, (that express that something good eventually happens); the second level consists of the class of obligation properties, obtained as positive Boolean combination of safety and guarantee properties; the third level contains, on the one hand, the recurrence properties (that express that something good occurs infinitely often), and on the other the persistence properties (that say that nothing bad occurs infinitely often); and the fourth level contains the reactivity properties, which are obtained as positive Boolean combination of recurrence and persistence properties. Each property is semantically defined in terms of sets of finite traces, e.g., a set F of finite-traces induces a basic safety (resp. progress) property that consists of an infinite trace iff every prefix (resp. all but finitely many prefixes) of the trace are in F. The reactivity properties contain all properties expressible in LTL.222The hierarchy is not limited to LTL, i.e., to properties that are expressible in first-order logic (FO) over infinite sequences (Kamp 1968), but extends to omega-regular properties, i.e., to monadic-second order logic (MSO) over infinite sequences. Indeed all the results we present here can be extended to omega-regular properties by substituting LTLf (resp. PPLTL) by its MSO-complete variant LDLf (resp. PPLDL) (De Giacomo and Vardi 2013). We revisit Manna and Pnueli’s hierarchy, and exploit it to define extensions of LTLf and PPLTL, which we call LTLf+ and PPLTL+, that can express arbitrary LTL properties on infinite traces. These new logics retain a crucial characteristic for reactive synthesis of their base logics: one can exploit the techniques for translating LTLf and PPLTL formulas into DFAs (De Giacomo and Vardi 2015; De Giacomo et al. 2020). These DFAs combine in a simple product to form the game arena for strategy extraction of LTLf+/PPLTL+ specifications. Naturally, the game objectives for LTLf+/PPLTL+ go beyond the simple adversarial reachability for LTLf/PPLTL. In particular, we exploit a variation of the Emerson-Lei condition (Emerson and Lei 1987) for handling Boolean combinations, and the possibility of translating these conditions into parity conditions (typically used for LTL) or to fixpoint computations (Hausmann, Lehaut, and Piterman 2024). We show that the worst-case complexity for synthesis of LTLf+ (resp. PPLTL+) is the same as for the base logics LTLf (resp. PPLTL), i.e., 2EXPTIME-complete (resp. EXPTIME-complete). The EXPTIME-complete result for synthesis in PPLTL+ is particularly interesting because, on the one hand, it shows that the exponential gap between PPLTL and LTLf (De Giacomo et al. 2020; Bonassi et al. 2023b) is maintained when extended to handle the full safety-progress hierarchy; and, on the other hand, it gives one a logic with the same expressive power as LTL but for which synthesis can be solved in EXPTIME instead of 2EXPTIME. Previous efforts to achieve reactive synthesis with exponential complexity focused on proper fragments of LTL (Arteche and Hermo 2024). We also adapt our DFA-based techniques and establish that reasoning — satisfiability, validity, and model-checking — for LTLf+ (resp. PPLTL+) is EXPSPACE-complete (resp. PSPACE-complete). The EXPSPACE-completeness result, which may appear surprising since satisfiability and model checking for LTL are both PSPACE-complete (Clarke et al. 2018), in fact confirms and extends a recent EXPSPACE-hardness result for model checking the fragment of LTLf+ limited to the guarantee class (Bansal et al. 2023). In other words, although LTLf+ defines infinite-trace properties using explicit reference to finite-trace properties defined in LTLf, it provides a computational advantage for synthesis but not for reasoning. Conversely, reasoning in PPLTL has the same cost as reasoning in LTL."
https://arxiv.org/html/2411.09359v1,Your Fixed Watermark is Fragile: Towards Semantic-Aware Watermark for EaaS Copyright Protection,"Embedding-as-a-Service (EaaS) has emerged as a successful business pattern but faces significant challenges related to various forms of copyright infringement, including API misuse and different attacks. Various studies have proposed backdoor-based watermarking schemes to protect the copyright of EaaS services. In this paper, we reveal that previous watermarking schemes possess semantic-independent characteristics and propose the Semantic Perturbation Attack (SPA). Our theoretical and experimental analyses demonstrate that this semantic-independent nature makes current watermarking schemes vulnerable to adaptive attacks that exploit semantic perturbations test to bypass watermark verification. To address this vulnerability, we propose the Semantic Aware Watermarking (SAW) scheme, a robust defense mechanism designed to resist SPA, by injecting a watermark that adapts to the text semantics. Extensive experimental results across multiple datasets demonstrate that the True Positive Rate (TPR) for detecting watermarked samples under SPA can reach up to more than 95%, rendering previous watermarks ineffective. Meanwhile, our watermarking scheme can resist such attack while ensuring the watermark verification capability. Our code is available at https://github.com/Zk4-ps/EaaS-Embedding-Watermark.","Embedding-as-a-Service (EaaS) 111The EaaS API from OpenAI: https://platform.openai.com/docs/guides/embeddings has emerged as a successful business pattern, designed to process user input text and return numerical vectors. EaaS supports different downstream tasks for users (e.g., retrieval[1, 2], classification[3, 4] and recommendation[5, 6]). Recently, it has also played a crucial role in developing the external knowledge systems, including Retrieval-Augmented Generation (RAG)[7, 8] and vector databases[9]. Moreover, HuggingFace community[10] support the innovation of embedding model with the Massive Text Embedding Benchmark (MTEB)[11]. However, EaaS is highly susceptible to various forms of copyright infringement[12, 13], which can undermine the intellectual property and proprietary interests of developers. As shown in Figure 1, after querying the text embeddings, malicious actors may seek to misuse the API of EaaS to construct external knowledge storage or potentially train their own models to replicate the capabilities of the original models at a lower cost, falsely claiming them as their own proprietary services. Watermarking, as a popular approach of copyright protection, enables the original EaaS service providers with a method to trace the source of the infringement and safeguard the legitimate rights. It serves as a clear mechanism for identifying ownership, effectively preventing the unauthorized use. Various works[14, 15, 16] have proposed backdoor-based watermarking schemes for embeddings to protect the copyright of EaaS services. Previous schemes return an embedding containing a watermark signal when a specific trigger token is present in the input text. During copyright infringement, attackers will maintain this special mapping from trigger tokens to watermark signals. Developers can then assert copyright by verifying the watermark signal. Figure 1: An Overview of EaaS Watermark. Watermarking provides EaaS providers with a method for tracing the copyright infringement. The current watermarking schemes are semantic-independent, and the watermark signals injected to the two semantically opposed texts are identical. 1.1 Our Work We reveal that previous watermarking schemes possess the semantic-independent characteristics. Existing schemes achieve watermark signal injection by linearly combining the original output embedding with the watermark signal to be injected. Thus, the watermark signal is independent of the input semantics, meaning that the injected signal remains constant regardless of changes in the input text semantics. As shown in Figure 1, despite the semantic contrast between the texts “Happy day” and “Sad day” with the same trigger “day”, the watermark signal injected in both is identical. Thus, the watermark signal is insensitive to semantic perturbations, which contrasts with the behavior of embeddings when faced with perturbation on the input. We introduce a novel attack, named Semantic Perturbation Attack (SPA), exploiting vulnerability arising from semantic-independent nature. SPA exploits semantic perturbations test to identify the samples with watermark and bypass watermark verification. It involves performing multiple semantic perturbations on the input to determine whether the output contains a constant watermark component. Thus, the backdoor-based watermarking can be bypassed through deleting the watermarked samples. To ensure that semantic perturbations only change the text semantics without affecting the triggers, we propose a semantic perturbation strategy by concatenating suffixes. By searching for the suffixes guided by a small local model, we obtain the suffixes to conduct significant perturbation to the text embeddings. Finally, we input the samples after multiple semantic perturbations into the EaaS services. Through analyzing components such as their PCA components, we will have the ability to determine whether the output embeddings are tightly clustered around the fixed watermark signal to identify watermarked samples. To address this vulnerability, we propose Semantic Aware Watermarking (SAW) scheme, a robust defense mechanism designed to resist SPA. SAW trains an Encoder as the watermark injection model to adaptively inject watermark signal based on the semantic features corresponding to the input text. Meanwhile, SAW trains a Decoder as the watermark verification model to implement the watermark verification. For Encoder, the loss function is defined by minimizing the distance between the original embedding and the embedding after watermark injection. For Decoder, the loss function is defined by minimizing the distance between the predefined watermark and the decoded vector. Ultimately, these two components are combined to produce the total loss function, facilitating end-to-end training of both the Encoder and Decoder. The main contributions of this paper can be summarized as the following three points: • We reveal that existing backdoor-based watermarking schemes for EaaS have a semantic-independent characteristic and analyze how this characteristic can be easily exploited by attackers. • We propose SPA, a novel attack that leverages the flaw identified in the analysis above to successfully bypass the current watermarking schemes for EaaS. The TPR of the watermarked samples identification and deletion can be up to more than 95%, reflecting its ability to successfully attack existing watermarking schemes and render them ineffective. • We propose SAW, a novel scheme to enhance the EaaS watermarking. Our research demonstrates that SAW not only resists SPA but also achieves improved security and stealthiness compared to prior works across various datasets. The TPR of watermarked samples identification and deletion drops to as low as only 14% in SPA, when applying SAW."
https://arxiv.org/html/2411.09302v1,EEG-Based Speech Decoding: A Novel Approach Using Multi-Kernel Ensemble Diffusion Models,"In this study, we propose an ensemble learning framework for electroencephalogram-based overt speech classification, leveraging denoising diffusion probabilistic models with varying convolutional kernel sizes. The ensemble comprises three models with kernel sizes of 51, 101, and 201, effectively capturing multi-scale temporal features inherent in signals. This approach improves the robustness and accuracy of speech decoding by accommodating the rich temporal complexity of neural signals. The ensemble models work in conjunction with conditional autoencoders that refine the reconstructed signals and maximize the useful information for downstream classification tasks. The results indicate that the proposed ensemble-based approach significantly outperforms individual models and existing state-of-the-art techniques. These findings demonstrate the potential of ensemble methods in advancing brain signal decoding, offering new possibilities for non-verbal communication applications, particularly in brain-computer interface systems aimed at aiding individuals with speech impairments.","I INTRODUCTION Speech is a fundamental aspect of human communication, enabling the conveyance of intricate thoughts and ideas [1]. It is deeply embedded in our social and cultural contexts, playing a critical role in relationship building and information sharing. However, individuals with conditions such as locked-in syndrome are often unable to engage in verbal communication due to physical limitations [2]. Therefore, the development of innovative approaches to restore or replace speech capabilities remains a vital research frontier [3, 4]. This study focuses on decoding brain signals as a means to facilitate non-verbal communication for such individuals. Electroencephalography (EEG) provides a non-invasive method to capture the electrical activities of the brain through scalp electrodes [5]. EEG signals have been widely used in applications ranging from neuroscience research to clinical diagnostics [6, 7]. A growing area of interest involves the decoding of EEG signals to derive meaningful information, such as speech-related activities or cognitive states [8]. EEG-based brain-computer interfaces (BCIs) have been explored for a variety of applications, including mental state classification [9], emotion recognition [10], and motor imagery [11]. Decoding EEG data related to spoken language poses significant challenges due to the complex and highly variable nature of neural activity associated with speech perception and production [12]. EEG signals are also prone to noise and artifacts, which further complicate accurate interpretation [13, 14]. As a result, the development of robust and effective methods for EEG decoding is an ongoing area of research with broad applications, including speech rehabilitation and human-machine interfaces [15]. Previous studies have attempted to decode imagined speech from EEG signals [16, 17], demonstrating the potential of EEG-based BCIs for communication. Deep learning techniques have shown promise in addressing these challenges by automatically learning hierarchical representations from raw EEG data [18]. Architectures such as DeepConvNet [19] and EEGNet [17] have been used successfully for EEG decoding tasks [19, 20]. Other deep learning models, including multi-view CNNs [11] and multimodal deep learning networks [3], have also been applied to EEG classification tasks, achieving notable success. In addition, graph-based methods have been utilized for EEG analysis to identify patterns in brain networks [21]. Denoising diffusion probabilistic models (DDPMs) have emerged as powerful tools for learning complex, high-dimensional patterns in data by progressively adding and then removing Gaussian noise [22]. These models have proven effective in dealing with time series data, including audio and video streams [23], making them suitable candidates for EEG signal analysis. Recent studies have applied diffusion-based models to time series data for tasks such as imputation and forecasting [24]. In the context of EEG decoding, diffusion-based models have been explored to decode imagined speech [16]. Building on these approaches, our study aims to further advance the field of EEG-based speech decoding by employing an ensemble learning strategy. We utilize DDPMs combined with conditional autoencoders (CAEs) to capture the intricate neural features associated with spoken speech. By incorporating multiple models with varying kernel sizes, we are able to capture EEG features at multiple temporal scales, thereby improving the robustness and accuracy of the decoding process. Similar multi-scale approaches have been successfully applied in mental state classification [9] and speech-related brain signal analysis [25]. To our knowledge, this is the first study to apply an ensemble of diffusion models with multi-kernel convolutional layers to decode EEG signals associated with overt speech. By combining the strengths of DDPMs, CAEs, and ensemble learning, we aim to significantly improve the performance of EEG decoding for non-verbal communication, with promising implications for BCI systems that assist individuals with speech impairments."
https://arxiv.org/html/2411.09294v1,"Learning Hand State Estimation
for a Light Exoskeleton","We propose a machine learning-based estimator of the hand state for rehabilitation purposes, using light exoskeletons. These devices are easy to use and useful for delivering domestic and frequent therapies. We build a supervised approach using information from the muscular activity of the forearm and the motion of the exoskeleton to reconstruct the hand’s opening degree and compliance level. Such information can be used to evaluate the therapy progress and develop adaptive control behaviors. Our approach is validated with a real light exoskeleton. The experiments demonstrate good predictive performance of our approach when trained on data coming from a single user and tested on the same user, even across different sessions. This generalization capability makes our system promising for practical use in real rehabilitation.","Stroke is one of the main causes of disability [6], resulting in severe hand functionalities limitation [13], or long-lasting hand impairments [14]. Given the importance of manual operations in everyday life, rehabilitation procedures are given a crucial role [24, 10]. Cutting-edge technology, like Virtual Reality (VR) [9, 16] and robotics [2], can assist standard rehabilitation to achieve better outcomes. In this context, it is expected that Artificial Intelligence (AI) can bring great benefits, especially in solving perception challenges that are otherwise difficult to tackle with standard devices. Indeed, Machine Learning (ML), and AI in general, have been proven to be effective in tackling complex perception tasks in robotics, e.g. for complex localization problems [22, 23] or human-robot interaction purposes [1, 4]. AI finds application also in the domain of medical robotics [30]. Hand exoskeletons are very useful in rehabilitation (see e.g. [11, 27]). Equipping them with an onboard and light perception module would enable even better therapy outcomes for several reasons. First, a perception system that does not require complex infrastructure or additional heavy devices confers versatility and lightness on the exoskeleton. These aspects are particularly important for favoring ease of use, and domestic and frequent utilization, a key factor for a successful therapy [12, 7]. Secondly, an online and robust perception of the patients’ state would allow adaptive closed-loop exoskeleton control with a positive impact on the therapy, as tuning the therapy to the patient’s state positively affects motor learning [31, 21]. Finally, and very importantly, a perception system would permit the measure of the patients’ sensorimotor deficit, crucial for delivering optimal rehabilitation [15]. At the moment, the therapy verification is manually performed by clinicians and suffer from low reliability and standardization, often affected by the therapist’s perception of the patient’s abilities [8]. Robotics could offer accurate and objective assessments of function and impairment [19, 15]. Figure 1: Light exoskeletons are promising and powerful tools for effective rehabilitation therapies. The challenge addressed in this work is to provide this kind of device, having little sensory equipment, with advanced perception systems to online measure the patient’s state and the therapy progress. For example, robotic devices are deployed to evaluate proprioception and haptic perception [21], and finger Range of Motion (RoM) [26, 29]. Fingers’ RoM are predictors of rehabilitation outcomes [20] and could be measured using hand trackers. These systems need external infrastructures, be it cameras [18] or other specialized hardware, such as gloves [3]. However, vision-based approaches have limited tracking area, as they are constrained by the camera field of view, and are susceptible to external disturbances such as light changing and occlusion. While gloves are robust in that sense, they may not be suitable to be used together with exoskeletons. Also, an under-actuated exoskeleton has been proposed to track the user’s fingers RoM [26], but it can not perform rehabilitation, since it only provides force feedback at the fingertips level. Ultimately, it is desirable to measure the hands RoM, for further evaluation of the therapy effectiveness, with no heavy infrastructure and complex wearing procedure. A dry surface Electromyography (EMG) sensor is a compelling alternative, as it is easy to wear and its measure of muscle activation can serve the monitoring of neuromuscular pathologies [5]. Light hand exoskeletons (as the one shown in Fig. 1) offer interesting possibilities for rehabilitation as they are: easy to wear and simple to use for both patient and therapist; flexible and compliant so that they easily adapt to the needs of stroke patients (who might have stiffness in the hand); relatively inexpensive, which favors their large usage. On the other side, their mechanism is simple and lacks rich sensory equipment. Thus, building an advanced perception system able to measure the patient’s state and the therapy progress requires tackling technological and research challenges. This work proposes to provide light hand exoskeletons with the perception of the patient’s hand state to improve the current rehabilitation setups and unlock possibilities in this domain. Our perception module (Sec. 2) is built on the measurement of the forearm muscle activity, obtained with EMG sensors, and the exoskeleton motion. A supervised approach learns the actual behavior of the hand wearing the exoskeleton. The experimental setup, the data collection procedure, and other implementation details are described in Sec. 3. Results are presented in Sec. 4, whereas Sec. 5 concludes the paper with final remarks."
https://arxiv.org/html/2411.09289v1,StreamAdapter: Efficient Test Time Adaptation from Contextual Streams,"In-context learning (ICL) allows large language models (LLMs) to adapt to new tasks directly from the given demonstrations without requiring gradient updates. While recent advances have expanded context windows to accommodate more demonstrations, this approach increases inference costs without necessarily improving performance. To mitigate these issues, We propose StreamAdapter, a novel approach that directly updates model parameters from context at test time, eliminating the need for explicit in-context demonstrations. StreamAdapter employs context mapping and weight absorption mechanisms to dynamically transform ICL demonstrations into parameter updates with minimal additional parameters. By reducing reliance on numerous in-context examples, StreamAdapter significantly reduce inference costs and allows for efficient inference with constant time complexity, regardless of demonstration count. Extensive experiments across diverse tasks and model architectures demonstrate that StreamAdapter achieves comparable or superior adaptation capability to ICL while requiring significantly fewer demonstrations. The superior task adaptation and context encoding capabilities of StreamAdapter on both language understanding and generation tasks provides a new perspective for adapting LLMs at test time using context, allowing for more efficient adaptation across scenarios and more cost-effective inference.","Large language models (LLMs) have emerged as a powerful tool in natural language processing, demonstrating exceptional performance across a diverse range of tasks, including text generation (Yuan et al., 2022), question answering (Kumar et al., 2023), open-ended conversations (Zhang et al., 2023a), and mathematical problem-solving (Shao et al., 2024). A key factor behind the success of LLMs is their ability to perform in-context learning (ICL) (Brown et al., 2020), where the model adapts to new tasks by conditioning on a small number of input-output demonstrations provided in the context. Without any gradient updates, ICL enables LLMs to acquire new knowledge and capabilities at test time, while also enabling LLMs to solve complex tasks through step-by-step guidance (Wei et al., 2023). Despite its remarkable capabilities, ICL faces several limitations that hinder its full potential. Firstly, the effectiveness of ICL heavily depends on the quality and relevance of the provided demonstrations, making the selection of appropriate examples a challenging task that often requires domain expertise (Agarwal et al., 2024; Sahoo et al., 2024). Moreover, the number of demonstrations that can be included is constrained by the model’s context window size. While recent advancements have expanded these windows (Ding et al., 2024; Team et al., 2024), accommodating more examples introduces significant computational overhead (Fu, 2024). Although recent studies have attempted to use heuristic rules to select the most important subset of context to improve the robustness and efficiency of ICL (Li et al., 2024c; Zhang et al., 2023c), these methods inevitably cannot ensure that the discarded tokens that are currently unimportant will not become important in future decoding steps. Other investigations have focused on constructing meta-ICL approaches to enhance ICL’s robustness and reduce reliance on perfect prompts (Coda-Forno et al., 2023). Yet, these methods remain constrained by limited context length and often require hand-crafted prompt strategies, potentially leading to suboptimal performance. On the other hand, recent studies suggest that ICL is actually performing a meta-gradient update for adapting to new tasks given the context information (Dai et al., 2023; von Oswald et al., 2022). These findings lead us to a crucial question: Instead of implicitly ""updating"" model parameters to adapt to a new domain or task via context, is it possible to directly convert the context into parameter updates, thus updating the network at test time without any backpropagation and without requiring demonstrations in the context window? To answer this question, we propose StreamAdapter, a novel approach that leverages the inherent capabilities of LLMs to encode context information into their parameters. Instead of storing demonstrations explicitly in the input context, StreamAdapter dynamically maps these demonstrations into temporary parameter updates. This approach allows the model to benefit from context to adapt to new tasks similar to ICL at test-time, without consuming the context window or requiring backpropagation, thereby reducing the resource requirements of traditional ICL methods. StreamAdapter employs two key mechanisms to achieve this goal: a) Context Mapping: This mechanism utilizes intra-chunk cross-attention and inter-chunk recurrence to adaptively condense the variable cached context into a constant context state for each parameter in the linear layer of LLMs. b) Weight Absorption: The condensed context state interacts with two lightweight low-rank matrices to be absorbed into the original model parameters. This process updates the LLM’s knowledge with minimal additional learnable parameters and incurs no additional inference latency. By combining these mechanisms, StreamAdapter effectively distills the context into parameter updates, allowing for more efficient test-time adaptation (TTA). Comprehensive experiments across diverse language understanding and long-context generation tasks, with various model architectures and scales, demonstrate that StreamAdapter achieves comparable or superior adaptation capability to full context evaluation while outperforming other context compression variants and TTA methods. Moreover, StreamAdapter not only demonstrates constant inference generation time and lower memory consumption compared to full context generation, but also shows better scalability when provided with more adaptation context and improved robustness across various scenarios. The contributions of our work can be summarized as follows: • We propose a new TTA strategy, StreamAdapter, that directly maps the given context into parameter updates, rather than conditioning on the context. This method enables models to quickly adapt to new tasks or acquire new temporary knowledge at test time like ICL, but with fewer or no demonstrations in context, thereby reducing memory consumption and inference time. • We design StreamAdapter with innovative context mapping and low-rank adaptation mechanisms. These allow StreamAdapter to map the context into parameter updates with minimal additional learning parameters and without inducing any additional inference latency. • We validate StreamAdapter on both language understanding and language generation tasks across various model scales and architectures. The results demonstrate the effectiveness of StreamAdapter over ICL and other TTA methods in various adaptation scenarios. Analyses of efficiency and robustness further highlight StreamAdapter’s advantages in terms of computational resources and generalization capabilities."
https://arxiv.org/html/2411.09273v1,Cross-Modal Consistency in Multimodal Large Language Models,"Recent developments in multimodal methodologies have marked the beginning of an exciting era for models adept at processing diverse data types, encompassing text, audio, and visual content. Models like GPT-4V, which merge computer vision with advanced language processing, exhibit extraordinary proficiency in handling intricate tasks that require a simultaneous understanding of both textual and visual information. Prior research efforts have meticulously evaluated the efficacy of these Vision Large Language Models (VLLMs) in various domains, including object detection, image captioning, and other related fields. However, existing analyses have often suffered from limitations, primarily centering on the isolated evaluation of each modality’s performance while neglecting to explore their intricate cross-modal interactions. Specifically, the question of whether these models achieve the same level of accuracy when confronted with identical task instances across different modalities remains unanswered. In this study, we take the initiative to delve into the interaction and comparison among these modalities of interest by introducing a novel concept termed cross-modal consistency. Furthermore, we propose a quantitative evaluation framework founded on this concept. Our experimental findings, drawn from a curated collection of parallel vision-language datasets developed by us, unveil a pronounced inconsistency between the vision and language modalities within GPT-4V, despite its portrayal as a unified multimodal model. Our research yields insights into the appropriate utilization of such models and hints at potential avenues for enhancing their design.","Figure 1: Visualization of the performance gap between the modality of text and image in seven different tasks. Recent large multimodal models have showcased remarkable capabilities in tasks that require the integration of multiple modalities and sources of information Huang et al. (2023). Among these, the performance of Vision Large Language Models (VLLMs) Zhang et al. (2023a); Yang et al. (2023) stands out, thanks to the vast amounts of image and text data available for training and the rapid progress in both computer vision and language modelling. However, due to the distinct training methodologies employed by these models, such as contrastive learning Radford et al. (2021); Jin et al. (2024) and embodied image-language modeling Driess et al. (2023), and the varying quality of training data for each modality Yin et al. (2023), these networks often exhibit performance disparities across different modalities. Previous research has extensively evaluated the performance of individual modalities in multimodal systems. For instance, Yang et al. (2023) conducted a thorough assessment of GPT-4V’s vision understanding capabilities, and Chen et al. (2023) analyzed model’s decision-making abilities. However, assessing a model’s performance on each individual modality in isolation does not fully evaluate its true multimodal abilities. It is possible, for example, for a model to excel in numerous vision tasks but still lag significantly behind in language understanding. Moreover, simply testing performance on individual tasks provides no insight into whether and how each modality of the model influences the others. Unfortunately, the cross-modality relationship is frequently overlooked in the aforementioned research. In this study, we go beyond the traditional approach of simply evaluating multimodal systems through separate downstream tasks and reporting their scores. Our focus is primarily on measuring the inherent differences in capabilities between various modalities, with special attention to vision and language, given their prominence among other modalities. To enable a comprehensive analysis, we introduce the concept of cross-modal consistency, complete with a formal definition and an evaluation framework. We consider cross-modal consistency to be an essential element in the design of complex multimodal systems with neural components, as it guarantees coherence and reliability in the system’s performance. This is crucial for both interpretability and for fostering user trust. We subsequently construct a comprehensive vision-language parallel dataset encompassing seven tasks, each designed to highlight different facets of vision and language capabilities. This dataset serves as a tool for evaluating the vision-language consistency of VLLMs. Our experiments with the GPT-4V model on the dataset reveal significant inconsistencies between its vision and language capabilities. The results indicate that its performance varies considerably depending on whether the same task instance is prompted in one modality versus the other. Our contributions are: (1) We introduce the novel concept of cross-modal consistency, along with a comprehensive evaluation framework. This approach transcends traditional assessment methods for multimodal models, which typically evaluate each modality in isolation. (2) We develop and release seven diverse datasets, carefully designed for vision-language consistency evaluation, opening up opportunities to exploit these datasets in future research. (3) Our experiments on GPT-4V reveal a significant disparity between vision and language abilities within such a system, prompting the introduction of the Vision-Depicting-Prompting (VDP) method as a potential remedy. Our findings offer valuable guidance for more effective future use of such multimodal models."
https://arxiv.org/html/2411.09269v1,Harnessing multiple LLMs for Information Retrieval: A case study on Deep Learning methodologies in Biodiversity publications,"Deep Learning (DL) techniques are increasingly applied in scientific studies across various domains to address complex research questions. However, the methodological details of these DL models are often hidden in the unstructured text. As a result, critical information about how these models are designed, trained, and evaluated is challenging to access and comprehend. To address this issue, in this work, we use five different open-source Large Language Models (LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B, and Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG) approach to extract and process DL methodological details from scientific publications automatically. We built a voting classifier from the outputs of five LLMs to accurately report DL methodological information. We tested our approach using biodiversity publications, building upon our previous research. To validate our pipeline, we employed two datasets of DL-related biodiversity publications: a curated set of 100 publications from our prior work and an additional set of 364 publications from the Ecological Informatics journal. Our results demonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of DL methodological information, achieving an accuracy of 69.5% (417 out of 600 comparisons) based solely on textual content from publications. This performance was assessed against human annotators who had access to code, figures, tables, and other supplementary information. Although demonstrated in biodiversity, our methodology is not limited to this field; it can be applied across other scientific domains where detailed methodological reporting is essential for advancing knowledge and ensuring reproducibility. This study presents a scalable and reliable approach for automating information extraction, facilitating better reproducibility and knowledge transfer across studies.","Deep Learning (DL) has become a cornerstone in numerous fields, revolutionizing how complex data is analyzed and interpreted. From healthcare and finance to autonomous systems and natural language processing, DL techniques have delivered groundbreaking results. However, as the adoption of DL continues to grow, there is an increasing recognition of a critical shortcoming: the limited availability of detailed methodological information in scientific literature (Waide et al.,, 2017; Stark,, 2018; Samuel et al.,, 2021; Pineau et al.,, 2021; Gundersen et al.,, 2022). This gap presents significant challenges for researchers and practitioners who seek to understand, replicate, and build upon existing studies (Feng et al.,, 2019; GPAI,, 2022). Past research has emphasized the need to make primary data and clear metadata available to support transparency (Michener et al.,, 1997; Whitlock,, 2011). A DL pipeline is a structured process for training and deploying DL models, starting with data collection and preprocessing tasks like cleaning, normalization, and transformation (El-Amir and Hamdy,, 2020). After preparing the data, the pipeline moves to model selection, where an appropriate architecture is chosen based on model complexity and problem type. The selected model is then trained on preprocessed data, fine-tuning through specific optimization algorithms and hyperparameter configurations. Once trained, the model’s performance is evaluated on test data to ensure reliable, unbiased results. The final step involves deploying the model for real-world use or further refinement. For a DL pipeline to be reproducible, detailed documentation at each stage is essential (Pineau et al.,, 2021). This includes logging data collection methods, preprocessing steps, model architecture configurations, hyperparameters, and training details, as well as performance metrics and test datasets. Additionally, maintaining records of software libraries, hardware, frameworks, and versions used is critical for the accurate replication of the study. Without access to such crucial information, stakeholders—including academics, industry professionals, and policymakers—face significant challenges in validating study outcomes or advancing research in meaningful ways. In areas like healthcare, finance, and autonomous systems, where DL applications influence real-world decisions, the absence of methodological transparency can compromise trust in DL models and limit their broader application (Haddaway and Verhoeven,, 2015). We contend that the same holds true for biodiversity research. The advent of DL has significantly transformed various domains, including biodiversity research, by enabling advanced methodologies for data analysis and interpretation (August et al.,, 2020). However, manually extracting relevant deep-learning information from scientific articles remains a labour-intensive and time-consuming process. This challenge affects both the reproducibility of the original studies and the reproducibility of secondary analyses aimed at understanding the methods employed. Traditional manual retrieval methods can be inconsistent, as the perspective of the annotators often varies based on their task interpretation and domain knowledge. These inconsistencies hinder efforts to systematically review or replicate the methodological approaches across studies, highlighting the need for more automated solutions. To address these challenges, we propose a novel approach that leverages the capabilities of Large Language Models (LLMs) for the automated extraction and processing of DL methodological information from scientific publications. LLMs, which are trained on vast amounts of text data, have demonstrated impressive capabilities in natural language understanding and generation. Specifically, we employ five open-source LLMs: Llama-3 70B111https://ai.meta.com/blog/meta-llama-3/, Llama-3.1 70B222https://ai.meta.com/blog/meta-llama-3-1/, Mixtral-8x22B-Instruct-v0.1333https://mistral.ai/news/mixtral-8x22b/, Mixtral 8x7B444https://mistral.ai/news/mixtral-of-experts/, and Gemma 2 9B555https://blog.google/technology/developers/google-gemma-2/ in combination with Retrieval-Augmented Generation (RAG) approach (Lewis et al.,, 2020). By utilizing these advanced models, we aim to extract relevant methodological details with greater accuracy and efficiency than manual methods alone. Our methodology is structured into three critical components: identifying relevant research publications, automatically extracting information through an RAG approach, and converting the extracted textual responses into categorical values for streamlined evaluation. In this work, we take biodiversity publications as a case study due to the growing popularity of DL methods in biodiversity research and the enormous number of publications using DL for various applications in this domain. Given the importance of biodiversity research and the critical need for transparent sharing of DL information in these studies (GPAI,, 2022), we chose this field to demonstrate our approach. However, our methodology is not limited to biodiversity alone; it can be applied to other domains where detailed methodological reporting is essential for advancing scientific knowledge and ensuring reproducibility. To enhance the reliability of our approach, we developed a voting classifier that aggregates the outputs of these LLMs, ensuring that the reported information is consistent and accurate. This methodology was applied to two distinct datasets of biodiversity publications focused on DL: one consisting of 100 publications from our previous work (Ahmed et al., 2024b, ) and another comprising 364 publications from the Ecological Informatics journal666https://www.sciencedirect.com/journal/ecological-informatics. Our approach can help identify gaps in reporting and ensure that critical information about DL methodologies is accessible, thereby enhancing the transparency and reproducibility of research. This paper presents a comprehensive case study on applying multiple LLMs for information retrieval in the context of DL methodologies within biodiversity publications. Through our approach, we aim to contribute to the growing body of research focused on automating information extraction and improving the reproducibility of results in scientific literature. By demonstrating the effectiveness of our pipeline, we hope to pave the way for future research that harnesses advanced AI techniques to further enhance data retrieval and analysis in biodiversity and beyond. Ensuring reproducibility in LLM applications requires a clear, comprehensive methodology that specifies all critical steps, settings, and model configurations. By providing all methodological details transparently, we aim to ensure that our approach can be consistently replicated and applied in future studies, supporting the reliable and reproducible use of LLMs in scientific research. In the following sections, we provide a detailed description of our study. We start with an overview of the state-of-the-art (“Related Work”). We provide the methodology of our study (“Methods”) We describe the results of our work (“Results”) and provide a detailed evaluation of our results (“Evaluation”). We discuss the implications of our study (“Discussion”). Finally, we summarize the key aspects of our study and provide future directions of our research (“Conclusion”)."
https://arxiv.org/html/2411.09266v1,"How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception","Multimodal deepfakes involving audiovisual manipulations are a growing threat because they are difficult to detect with the naked eye or using unimodal deep learning-based forgery detection methods. Audiovisual forensic models, while more capable than unimodal models, require large training datasets and are computationally expensive for training and inference. Furthermore, these models lack interpretability and often do not generalize well to unseen manipulations. In this study, we examine the detection capabilities of a large language model (LLM) (i.e., ChatGPT) to identify and account for any possible visual and auditory artifacts and manipulations in audiovisual deepfake content. Extensive experiments are conducted on videos from a benchmark multimodal deepfake dataset to evaluate the detection performance of ChatGPT and compare it with the detection capabilities of state-of-the-art multimodal forensic models and humans. Experimental results demonstrate the importance of domain knowledge and prompt engineering for video forgery detection tasks using LLMs. Unlike approaches based on end-to-end learning, ChatGPT can account for spatial and spatiotemporal artifacts and inconsistencies that may exist within or across modalities. Additionally, we discuss the limitations of ChatGPT for multimedia forensic tasks.","Figure 1: Illustration of audiovisual deepfake manipulations. Original video content is represented as RVRA (real video with real audio. Through deepfake manipulation techniques, three manipulated types are generated: FVRA (fake video with real audio), RVFA (real video with fake audio), and FVFA (fake video with fake audio). Blue text represents the “real modality” of the video content, while red text represents the “fake modality”. Synthetic multimedia content has become both innovative and a significant threat in recent years. Deepfake images and videos created using artificial intelligence (AI) and deep learning (DL) techniques have attracted public and academic attention. This synthetic content is generated by generative adversarial networks (GANs) [1] and more sophisticated AI techniques such as diffusion models [2]. While deepfake technology has many innovative applications in education, entertainment, and other fields [3], it is a double-edged sword that can be used for unethical purposes, such as pornography, political defamation, identity theft, fraud, misinformation, and disinformation [4, 5, 6]. Unethical use of this technology can lead to political instability and social violence [6]. On the one hand, deepfake technology continues to evolve to create more convincing and realistic fake multimedia content. Social media, on the other hand, plays a catalytic role in spreading such content. Therefore, timely detection of deepfake content is crucial to avoid any damage and loss to human society [4]. Audiovisual deepfakes that involve multimodal manipulation are a more convincing type of forgery, with attackers attacking audio, video, or both modalities. Unimodal video forgery detectors [7, 8, 9, 10] and spoofed audio detectors [11, 12, 13, 14] are generally unable to identify forgeries across multiple modalities, although they may be good at detecting forgeries in the specific modality they focus on. To address this challenge, the research community has developed sophisticated tools and algorithms to detect audiovisual forgeries in videos. These specialized tools require knowledge of multimedia forensics as well as knowledge of deep learning. Furthermore, these tools do not generalize well to other unseen datasets and manipulations. Large language models (LLMs) are a major advancement in the field of artificial intelligence. They are trained on a large amount of data and can perform well in various natural language processing (NLP) tasks such as text generation, summarization, classification, completion, sentimental analysis, machine translation, and question answering. Their applications even go beyond the aforementioned NLP tasks and can be used as writing assistants, learning tools, productivity tools, coding assistants, software development, healthcare, legal assistance, entertainment, and more. Despite being primarily designed for NLP tasks, OpenAI’s ChatGPT can analyze image, audio, and video content. Taking advantage of its support for multimodal input, we studied the potential and limitations of ChatGPT for audiovisual deepfake detection. The research questions we aimed to address in this study are as follows: • Can ChatGPT perform multimedia forensic tasks? • Is ChatGPT capable of detecting forgery based on artifacts in audio and visual modalities? • What is the role of prompt engineering in using ChatGPT to detect audiovisual deepfakes? • Which performs better at identifying forgeries in audiovisual deepfakes, ChatGPT, humans, or AI models? • How interpretable is ChatGPT for forgery detection? • What are the limitations of ChatGPT in detecting multimodal deepfakes? The main contributions of our work are threefold: • We explore for the first time the potential of ChatGPT for audiovisual forgery detection tasks. • We compare the performance of ChatGPT with human and state-of-the-art AI models on audiovisual forgery detection tasks. • We highlight the strengths and limitations of ChatGPT on audiovisual forgery detection tasks."
https://arxiv.org/html/2411.09261v1,Automating Autograding: Large Language Models as Test Suite Generators for Introductory Programming,"Automatically graded programming assignments provide instant feedback to students and significantly reduce manual grading time for instructors. However, creating comprehensive suites of test cases for programming problems within automatic graders can be time-consuming and complex. The effort needed to define test suites may deter some instructors from creating additional problems or lead to inadequate test coverage, potentially resulting in misleading feedback on student solutions. Such limitations may reduce student access to the well-documented benefits of timely feedback when learning programming.In this work, we evaluate the effectiveness of using Large Language Models (LLMs), as part of a larger workflow, to automatically generate test suites for CS1-level programming problems. Each problem’s statement and reference solution are provided to GPT-4 to produce a test suite that can be used by an autograder. We evaluate our proposed approach using a sample of 26 problems, and more than 25,000 attempted solutions to those problems, submitted by students in an introductory programming course. We compare the performance of the LLM-generated test suites against the instructor-created test suites for each problem. Our findings reveal that LLM-generated test suites can correctly identify most valid solutions, and for most problems are at least as comprehensive as the instructor test suites. Additionally, the LLM-generated test suites exposed ambiguities in some problem statements, underscoring their potential to improve both autograding and instructional design.","Autograding of programming assignments offers a number of well-documented benefits to both students and instructors. In particular, the instant feedback provided by autograders has been shown to help students correct their errors and solve more problems (Pieterse, 2013; Sherman et al., 2013; Duch and Jaworski, 2018). Students perceive instant feedback as a positive contribution to their learning (Färnqvist and Heintz, 2016; Rao, 2019), and this is reflected in their improved overall performance (Gordillo, 2019; Bai et al., 2016). Autograding also minimizes the variability in grading decisions which may be subjective when grading is performed by instructors or teaching assistants (Parihar et al., 2017), and it helps reduce the workload of teaching staff significantly, by eliminating the need to manually evaluate each submission (Bai et al., 2016; Venables and Haywood, 2003). However, preparing problems for autograding systems requires more than just a problem statement; instructors must also provide a test suite that thoroughly covers the different scenarios of the problem. Preparing this test suite is not always straightforward. For it to be thorough, it must at least contain tests for multiple random inputs, the minimum and maximum limits of the problem’s inputs, as well as other edge-cases that are dependent on the problem. Furthermore, some problems require unique tests that capture specific constraints in the problem, e.g., an array of distinct numbers, or an array of non-increasing numbers, and sometimes random tests must be validated to ensure they actually contain a valid answer for the problem. This can all be very time consuming, even for low-level courses (Lobb and Harlow, 2016). Moreover, failing to provide sufficient tests might result in misleading feedback to students, as invalid solutions could be graded as valid. Recent advances in Generative AI (GenAI) have introduced exciting new possibilities for using AI in Computing Education (Denny et al., 2024c). Large Language Models (LLM) in particular, which are capable of generating human-like outputs in response to text prompts, can be integrated in various Computing Education settings (Becker et al., 2023; Prather et al., 2023). They have been applied to generating new programming exercises (Sarsa et al., 2022), explaining code (Denny et al., 2024b; Liffiton et al., 2023; Leinonen et al., 2023a; Phung et al., 2023), providing feedback and improving the readability of programming error messages (Leinonen et al., 2023b; Phung et al., 2023), and powering novel pedagogical approaches (Smith et al., 2024; Denny et al., 2024a). While they have also been used to grade solutions based on a set of criteria provided to the LLM (Nilsson and Tuvstedt, 2023; Bengtsson and Kaliff, 2023), we are not aware of prior work that uses LLMs in a Computing Education context to help instructors generate test suites for autograders to address the challenges stated earlier. In this study, we explore the efficacy of using LLMs to minimize the time and effort required of instructors who want their assignments to be submitted to and evaluated by autograders. We run tens of thousands of student solutions on LLM-generated test suites for CS1 problems, and compare their results to running them on traditional instructor-generated test suites with respect to their correctness and thoroughness. We also look into the LLM-generated test suites’ ability to uncover ambiguities in instructor-written problem statements. Our analysis is guided by the following three research questions: [] (1) RQ1: To what extent do LLM-generated test suites correctly identify valid solutions to CS1 problems? (2) RQ2: How comprehensive are LLM-generated test suites compared to instructor-generated test suites? (3) RQ3: What types of ambiguities can LLM-generated test suites help uncover in problem statements? We start with a literature review of studies on autograding and LLMs in Computing Education. We also show how our work differs from the large field of work undertaken on generating unit-tests for code. Then we describe the design of our study, present the results, and discuss the broader implications of this work."
https://arxiv.org/html/2411.09224v1,"Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub Copilot for Programmers","Our everyday lives now heavily rely on artificial intelligence (AI) powered large language models (LLMs). Like regular users, programmers are also benefiting from the newest large language models. In response to the critical role that AI models play in modern software development, this study presents a thorough evaluation of leading programming assistants, including ChatGPT, Gemini (Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on tasks like natural language processing and code generation accuracy in different programming languages like Java, Python and C++. Based on the results, it has emphasized their strengths and weaknesses and the importance of further modifications to increase the reliability and accuracy of the latest popular models. Although these AI assistants illustrate a high level of progress in language understanding and code generation, along with ethical considerations and responsible usage, they provoke a necessity for discussion. With time, developing more refined AI technology is essential for achieving advanced solutions in various fields, especially with the knowledge of the feature intricacies of these models and their implications. This study offers a comparison of different LLMs and provides essential feedback on the rapidly changing area of AI models. It also emphasizes the need for ethical developmental practices to actualize AI models’ full potential.","The advent of the AI concept presents a new revolutionary age of innovation with an AI model and LLM-powered chatbots changing how our software is being developed and problems solved (Gates, [n.d.]). With the launch of ChatGPT and the newest LLM tools, such as GitHub Copilot, Bard AI (which is now a part of the Gemini framework), and DeepMind’s AlphaCode, which have been developed by major players in the industry like Google, GitHub, and OpenAI, these AI systems have captured the attention of the tech community with their capacity to understand languages and generate programming languages. The reality of AI assistants is that they are revolutionary and keep widening the limits of AI models at work. Therefore, the discussion about their accuracy, architecture, capabilities, and implications for the future of AI technologies is crucial. One of the first and most effective LLMs, ChatGPT, attracted 100 million users in just two months after the launch, making it the fastest-growing platform out of all those based on technology and a testament to how much the consumers needed such platforms (Milmo, [n.d.]). With the help of LLMs, notable progress in code generation and Natural language Processing (NLP) has been made recently (noa, [n.d.]s). One example is the generative pre-trained transformer (GPT) model series(Radford, [n.d.]). These models, which have received extensive training on textual data show that they can produce codes on the same level as human written codes and execute language-based tasks with remarkable accuracy. This paper thoroughly studies the most recent large language models, highlighting their strengths and weaknesses and crucially contributing to responsible development practices in the benefits of AI models in various fields. Thus, to understand and capture the behaviour of popular LLMs, we pose three research questions: • RQ1: Which model provides the most accurate code for programmers? • RQ2: What are the metrics are frequently used to evaluate LLM generated codes? • RQ3: What are the benchmarks are being used to evaluate LLM generated codes?"
https://arxiv.org/html/2411.09220v1,Transferable Adversarial Attacks against ASR,"Given the extensive research and real-world applications of automatic speech recognition (ASR), ensuring the robustness of ASR models against minor input perturbations becomes a crucial consideration for maintaining their effectiveness in real-time scenarios. Previous explorations into ASR model robustness have predominantly revolved around evaluating accuracy on white-box settings with full access to ASR models. Nevertheless, full ASR model details are often not available in real-world applications. Therefore, evaluating the robustness of black-box ASR models is essential for a comprehensive understanding of ASR model resilience. In this regard, we thoroughly study the vulnerability of practical black-box attacks in cutting-edge ASR models and propose to employ two advanced time-domain-based transferable attacks alongside our differentiable feature extractor. We also propose a speech-aware gradient optimization approach (SAGO) for ASR, which forces mistranscription with minimal impact on human imperceptibility through voice activity detection rule and a speech-aware gradient-oriented optimizer. Our comprehensive experimental results reveal performance enhancements compared to baseline approaches across five models on two databases.","Automatic speech recognition (ASR) aims to recognize text from speech signals. ASR represents a dynamic and rapidly evolving research domain, striving to bridge the cognitive gap between human speech comprehension and computational interpretation. This progress is driven by a multitude of real-world applications such as virtual assistants (e.g., Siri and Amazon Alexa), captioning, subtitling, healthcare [1], and autonomous vehicle systems [2]. Consequently, attaining human-like recognition performance against minor input perturbations emerges as a critical consideration for real-time applications within the ASR domain [3, 4, 5, 6]. Recognizing the significance of probing ASR system robustness, recent researches have delved into white-box scenarios to evaluate the accuracy robustness of ASR models via targeted and untargeted adversarial attacks [7, 8, 9, 10]. In this paper, we focus on untargeted attacks. Untargeted attacks against white-box ASR models endeavor to create adversarial samples by introducing subtle perturbations to the ASR inputs, resulting in diminished recognition performance while maintaining imperceptibility to humans [11, 12, 13, 14, 15, 16, 8]. Although the mentioned endeavors in white-box ASR have made significant advancements, encompassing DNN [14], RNN [11, 12, 15, 16], Transformers [13], and the latest state-of-the-art (SOTA) Whisper ASR models [8], they are constrained by an impractical assumption that requires the adversary to have access to the ASR model’s internal information. This limitation restricts their applicability in real-world scenarios [17, 18]. Adversarial attacks pose a critical challenge in security-sensitive and safety-critical domains [19, 20]. Various adversarial attack approaches have been extensively studied in computer vision [21, 22] and natural language processing [23, 24, 25]. With advancements in white-box ASR attacks, the potential and exploration of black-box ASR attacks, especially transferable attacks, have also been gaining attention. Notable exceptions in attacking Kaldi-based models [26] and RNN-based ASR models include universal adversarial perturbations [27] and study on factors affecting target transferability [28]. The above transferable attacks involve adversaries using adversarial examples crafted for one trained model to target other black-box ASR models. By eliminating the need to access the full black-box model architecture and weights, transfer attacks offer greater flexibility and possibilities for real-world applications. However, transfer attacks against advanced black-box ASR models (e.g., Whisper [29], Speech2text [30]) still remain unexplored. The accuracy robustness of black-box ASR still lags considerably behind human speech recognition performance, highlighting the need for a thorough exploration of recent models and advanced transferable attack methods. Recent advancements on attack transferability through innovative optimization [31, 20, 32, 33, 34, 35] offer valuable insights for our work. To this regard, this paper aims to conduct a comprehensive exploration of accuracy robustness on cutting-edge ASR models, i.e., Whisper [8] and Transformer [30]. Inspired by [31, 32], we propose to employ momentum iterative fast gradient sign method (MI-FGSM) and variance tuning momentum iterative fast gradient sign method (VMI-FGSM) directly on time-domain audio signals through differentiable feature extraction design. On the other hand, integrating VAD tasks into ASR has proven effective in enhancing recognition accuracy [29, 36]. VAD is designed to distinguish speech from non-speech segments in audio input [37]. Its close relationship with ASR lies in the fact that it identifies speech portions that are crucial for ASR [38, 29, 36]. Specifically, the correlation stems from the fact that the commencement of speech segments within an utterance usually corresponds with the detection of speech initiation by the VAD system, rendering non-speech segments identified by VAD potentially redundant for ASR [29, 36]. Furthermore, VAD and ASR often share optimization objectives to improve ASR system performance [29, 36, 38]. Motivated by this correlation, we propose a novel approach, Speech-Aware Gradient Optimization (SAGO) for black-box ASR attack. SAGO investigates speech-aware adversarial attacks tailored to ASR, specifically targeting speech segments using a VAD mask for gradient optimization, highlighting speech parts that the human brain tends to focus on. The contributions of this paper include: (a) Comprehensive Exploration: We conduct a thorough investigation into novel gradient-based strong transferable attacks on SOTA ASR models, benefiting from a differentiable feature extractor design that enables effective time-domain attacks; (b) Innovative Methodology: We propose an importance-attribute attack via speech-aware gradient optimization, allowing us to emulate the human brain for targeting speech segments attentively for attacks through a VAD rule; and (c) Extensive Experimentation: We conduct a systematic evaluation of various transferable attacks across five models on two different datasets. TABLE I: Comparison of the recognition performance (WER%) of various attack approaches under ljspeech dataset. Source refers to the white-box source ASR model, while target pertains to the black-box victim ASR model. Clean and noise denote the recognition performance when decoding original clean speech and noisy speech samples, respectively. * denotes white-box attacks. Target Models Whisper-tiny Whisper-base S2T-small S2T-medium S2T-large Source Models SNR 30 35 30 35 30 35 30 35 30 35 Clean 5.32 3.77 4.77 4.35 4.54 White Noise [8] 6.14 5.62 4.49 3.66 3.72 4.90 3.31 4.38 3.07 4.63 Babble Noise [39] 5.35 5.25 3.81 3.81 4.87 4.90 4.53 4.50 4.63 4.63 Music Noise [39] 5.71 5.12 3.86 3.74 4.98 4.86 4.40 4.44 4.63 4.50 Natural Noise [39] 5.11 5.04 3.96 3.77 4.95 4.81 4.62 4.62 4.81 4.62 Whisper-tiny PGD [8] 100.00∗ 93.24∗ 41.62 26.75 21.01 13.68 16.73 11.75 13.49 10.17 SAGO 97.22∗ 89.11∗ 43.48 28.84 22.75 14.83 18.95 12.80 15.45 10.92 VMI-FGSM 100.00∗ 85.56∗ 49.03 31.68 23.98 16.37 19.97 14.19 16.96 11.93 MI-FGSM 100.00∗ 89.24∗ 46.97 31.72 24.48 17.32 20.13 14.20 18.12 12.41 Whisper-base PGD [8] 49.79 26.75 100.00∗ 75.88∗ 20.53 14.94 17.87 13.25 15.23 11.85 SAGO 52.82 35.69 84.00∗ 65.45∗ 21.80 15.93 20.09 14.14 17.72 12.69 VMI-FGSM 49.89 38.97 87.31∗ 69.56∗ 23.91 16.91 21.98 15.41 23.91 18.80 MI-FGSM 52.90 37.80 87.00∗ 74.35∗ 24.81 17.33 22.44 15.94 19.46 13.84 S2T-small PGD [8] 10.50 9.44 7.40 6.45 100.00∗ 100.00∗ 30.15 22.38 19.17 15.59 SAGO 13.91 10.77 9.75 7.41 82.86∗ 63.76∗ 35.22 24.08 23.89 16.30 VMI-FGSM 18.81 13.02 10.74 9.63 100.00∗ 100.00∗ 28.94 25.14 19.73 17.26 MI-FGSM 12.48 10.98 9.17 7.18 100.00∗ 99.34∗ 33.73 26.33 22.24 17.87 S2T-medium PGD [8] 11.04 10.23 8.01 7.27 37.68 29.46 100.00∗ 100.00∗ 21.25 21.98 SAGO 15.28 11.92 11.54 8.41 43.99 29.74 78.64∗ 58.14∗ 34.87 23.31 VMI-FGSM 12.71 11.28 9.89 8.30 39.42 31.11 100.00∗ 69.07∗ 24.11 24.22 MI-FGSM 14.10 11.13 10.43 8.35 42.00 31.20 100.00∗ 93.15∗"
https://arxiv.org/html/2411.09213v1,Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering,"Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs’ ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models’ limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs’ reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.","Large language models (LLMs) have demonstrated remarkable capabilities in solving complex medical problems, achieving state-of-the-art performance across various benchmarks. However, ensuring the reliability and truthworthiness of an artificial intelligent (AI) medical system remains a critical challenge, especially in healthcare applications. Retrieval-augmented generation (RAG) has emerged as a promising approach to reduce LLMs’ hallucination problem by integrating external knowledge sources. Figure 1: Blue texts are useful information that should be extract to help determine the answer. Red texts are factual errors that potentially mislead the LLMs. While RAG has potential to improve the factual accuracy of LLMs’ response, incorporating an information retriever also presents new complexities that warrant careful evaluation. Consider the example in Fig. 1. The retrieved documents can contain not only useful knowledge that helps determine the true answer, but also noise information, or more serious, factual errors that can misleads the LLMs. To consciously apply RAG for medical QA, we must consider these practical scenarios and evaluate LLMs ability to interact with retrieved documents reliably. Recent efforts have been made to evaluate AI systems with LLMs in the medical domain (Nori et al. 2023; He et al. 2023; Xiong et al. 2024). For example, MedEval (He et al. 2023) presents a large-scale, expert-annotated benchmark that cover various medical tasks and domains. (Xiong et al. 2024) evaluates RAG extensively based on their MIRAGE benchmark which cover 5 medical QA datasets. However, they only focus on the effect of RAG modules on target accuracy, missing other important aspects of a AI medical system. Several recent works have explore RAG evaluation more comprehensively in general domain (Es et al. 2023; Chen et al. 2024b), RAGAS (Es et al. 2023) assess 3 qualities of RAG’s outputs for QA tasks including: Faithfulness - degree to which responses align with the provided context, Answer Relevance - the extent to which generated responses address the actual question posed, and Context Precision-Recall - the quality of retrieved context. We follow the work from (Chen et al. 2024b) which establishes Retrieval-Augmented Generation Benchmark (RGB) to measure 4 abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. In particular, using questions from 4 medical QA datasets from MIRAGE as basis, we create Medical Retrieval-Augmented Generation Benchmark (MedRGB) to evaluate RAG system in the following 4 test scenarios: • Standard-RAG: evaluates LLMs performance when presented with multiple retrieved signal documents to create a context to answer to question. • Sufficiency: evaluates LLMs reliability when there are noise documents within the retrieved context. By adding ”Insufficient Information” as an additional response option, LLMs should only answer when they are confident to have enough information to determine the correct answer. This requires LLMs to not only be aware of its own internal knowledge, but also be able to filter out noisy information from external documents. • Integration: evaluates LLMs ability to answer multiple supporting questions and integrate the extracted information to help address the main question. • Robustness: evaluates LLMs resiliency to factual errors in the retrieved context. A trustworthy AI medical system should be able detect factually incorrect documents and provide the corrected information. In total, MedRGB consists of 3480 instances for 4 test scenarios, which is over 5 times that of RGB. Using MedRGB, we evaluation 7 LLMs, including both state-of-the art commercial LLMs and open-source models. In summary, our contributions are three-fold: • We establish MedRGB with four test scenarios to evaluate LLMs for medical QA tasks in RAG settings. To best of our knowledge, it is the ﬁrst benchmark comprehensively assess medical RAG systems in these practical setting. • Using MedRGB, we extensively evaluate 7 LLMs, including both state-of-the art commercial LLMs and open-source models, across multiple RAG conditions. Experiment results demonstrate their limitation in addressing the more complex scenarios. • We analyzed the errors of the LLMs and their reasoning process to provide insights and suggest future directions for developing more reliable and trustworthy medical RAG systems. Figure 2: The overall construction process of MedRGB. The green OpenAI symbol implies that the block involves data generation using the GPT-4o model."
https://arxiv.org/html/2411.09204v1,RibCageImp: A Deep Learning Framework for 3D Ribcage Implant Generation,"The recovery of damaged or resected ribcage structures requires precise, custom-designed implants to restore the integrity and functionality of the thoracic cavity. Traditional implant design methods rely mainly on manual processes, making them time-consuming and susceptible to variability. In this work, we explore the feasibility of automated ribcage implant generation using deep learning. We present a framework based on 3D U-Net architecture that processes CT scans to generate patient-specific implant designs. To the best of our knowledge, this is the first investigation into automated thoracic implant generation using deep learning approaches. Our preliminary results, while moderate, highlight both the potential and the significant challenges in this complex domain. These findings establish a foundation for future research in automated ribcage reconstruction and identify key technical challenges that need to be addressed for practical implementation.","The reconstruction of compromised thoracic structures, such as ribcage, requires high-precision engineering in implant design. Accurate dimensional specifications are crucial for restoring anatomical alignment and biomechanical functional integrity of the chest cavity [1]. While modern medical imaging and advanced technologies have enabled the development of many prosthetic structures, ribcage reconstruction presents unique challenges [2]. The complex 3D geometry of the thoracic cavity and patient-specific anatomical variations make the design of custom ribcage implants particularly complex. The conventional design workflows remain primarily manual and require extensive specialized expertise [3]. It requires extensive time for clinicians to analyze patient-specific thoracic architecture through manual measurements and Computer-Aided Design (CAD) interpretations, translating complex anatomical data into fabrication-ready implant specifications. These technical limitations and the critical need for precise anatomical alignment create significant barriers to delivering optimal patient-specific thoracic implants. The primary challenge lies in achieving high accuracy and efficiency in ribcage implant design while minimizing manual interventions. This necessitates the development of automated design approaches that can maintain thoracic geometric precision, reduce processing time, and ensure consistent quality across different patients. Additionally, such automation must account for critical anatomical landmarks, biomechanical constraints, and surgical considerations that traditionally rely on clinical expertise. Fig. 1: Illustration of defective ribcage R_{d}, ground truth implant I_{g}, and complete ribcage with predicted ground truth R_{d}+I_{g}. The emergence of deep learning methods such as Convolutional Neural Networks (3D CNNs) [4], especially the 3D U-Net [5], has demonstrated remarkable success in medical image analysis. These networks have shown capabilities in processing volumetric medical data for complex tasks, including brain tumor segmentation [6], early disease detection and diagnosis [7], and surgical planning [8]. While 3D U-Net-based models have been successfully implemented for cranial implant design [9], the ribcage presents significantly more complex challenges. Cranial implants primarily deal with relatively uniform, curved surfaces focused mainly on structural protection and aesthetic outcomes. In contrast, ribcage implants must address multiple curved bones with varying cross-sections, intricate joint connections, and complex spatial relationships. Further, it also ensures the functionality of proper respiratory mechanics, vital organ protection, natural chest wall movement, and seamless integration with the surrounding musculoskeletal structure. Ribcage reconstruction becomes important in various medical situations. These include the removal of chest tumors (both primary and spreading cancers), severe chest injuries from accidents, and congenital disabilities affecting chest wall structure. Each patient’s case is unique and requires a customized implant solution that perfectly matches their specific anatomy [10]. Developing automated, learning-based approaches for ribcage implant design promises significant benefits. This advancement reduces design time, improves implant quality, and enhances surgical outcomes and patient recovery. This is particularly significant given the time-critical nature of many cases, especially in emergency trauma scenarios requiring rapid implant design and efficient surgical planning in oncological cases. An extensive literature review reveals a complete absence of learning-based approaches for automated ribcage implant design, with current methods relying solely on manual CAD processes, statistical shape analysis, template-based modifications, and expert-guided design iterations [11]. CT scans offer high-resolution, volumetric views of thoracic anatomy, ideal for developing learning-based methods for direct 3D implant generation. Leveraging this rich anatomical information—such as spatial relationships and bone structure—into a 3D U-Net model for implant design could seamlessly integrate into standard clinical workflows. Our key contributions to the paper are: • We propose the first deep-learning approach for automated ribcage implant generation directly from CT scans. This establishes a new paradigm for thoracic reconstruction, moving beyond traditional CAD-based methods towards automated, data-driven solutions. • We adapt and optimize the 3D U-Net architecture to demonstrate the feasibility of ribcage reconstruction. • We provide comprehensive evaluation and analysis through detailed validation of the proposed approach on diverse patient cases. The current results demonstrate the viability of the deep model and highlight opportunities for further refinement and optimization. This work provides a preliminary solution and opens new research directions for advancing automated ribcage implant generation. It serves as a foundational reference for future researchers aiming to contribute to this domain."
https://arxiv.org/html/2411.09181v1,DeBaTeR: Denoising Bipartite Temporal Graph for Recommendation,"Due to the difficulty of acquiring large-scale explicit user feedback, implicit feedback (e.g., clicks or other interactions) is widely applied as an alternative source of data, where user-item interactions can be modeled as a bipartite graph. Due to the noisy and biased nature of implicit real-world user-item interactions, identifying and rectifying noisy interactions are vital to enhance model performance and robustness. Previous works on purifying user-item interactions in collaborative filtering mainly focus on mining the correlation between user/item embeddings and noisy interactions, neglecting the benefit of temporal patterns in determining noisy interactions. Time information, while enhancing the models’ utility, also bears its natural advantage in helping to determine noisy edges, e.g., if someone usually watches horror movies at night and talk shows in the morning, a record of watching a horror movie in the morning is more likely to be noisy interaction. Armed with this observation, we introduce a simple yet effective mechanism for generating time-aware user/item embeddings and propose two strategies for denoising bipartite temporal graph in recommender systems (DeBaTeR): the first is through reweighting the adjacency matrix (DeBaTeR-A), where a reliability score is defined to reweight the edges through both soft assignment and hard assignment; the second is through reweighting the loss function (DeBaTeR-L), where weights are generated to reweight user-item samples in the losses. Extensive experiments have been conducted to demonstrate the efficacy of our methods and illustrate how time information indeed helps identifying noisy edges.","Recommender systems have been widely applied for studying user preferences (lu2015recommender, ). As a powerful method for building recommender systems, collaborative filtering leverages the interaction history of users and items to mine latent information. Lately, with the development of Graph Neural Networks, neural graph collaborative filtering has received extensive attention, and many state-of-the-art methods have been proposed (SimGCL, ; NCL, ; he2020lightgcn, ; SGL, ) with their capability of capturing high-order interaction information. Despite the substantial progress, some challenges still exist, and one of them lies in the noisy nature of collected data (zhang2023robust, ), especially when the data is collected from implicit feedback (e.g., clicks) (10.1145/3474085.3475446, ). For example, users might inadvertently interact with some items (misclicks or merely out of curiosity), or some of the items might be intended for others (e.g., gifts). These noisy interactions are still considered as positive samples for model training, even though they do not align with the interacting user’s preferences. Moreover, many malicious attacks (10.1145/3447548.3467233, ; 10.1145/3274694.3274706, ; 7824865, ) have been designed to craft fake interactions or users to bias recommender systems, resulting in sub-optimal prediction results. In the context of graph neural collaborative filtering, the problem becomes even worse as message passing in graph neural networks (GNNs) will amplify the impact of noisy interactions. Therefore, identifying and rectifying noisy interactions for neural graph collaborative filtering becomes important for building robust recommender systems that align with users’ true interests. Many approaches have been proposed to tackle this issue. Depending on how samples are purified, they can be roughly divided into two categories. The first type of strategies directly remove the identified noisy samples from training data (10.5555/3000375.3000390, ; 10.1145/3474085.3475446, ); while another group of works actively downweight those training samples during model training (BOD, ; RocSE, ; T-CE, ; SGDL, ). To identify noisy interactions before rectifying, these works rely on trainable (e.g., a multi-layer perceptron) or predefined functions (e.g., cosine similarity) to model the correlation between user/item embeddings and reliability. However, none of the existing work has focused on the natural advantage of time information for denoising. The advantages of leveraging interaction time for collaborative filtering are threefold. First, interaction times can be easily collected as every implicit feedback signal has an associated timestamp that we can leverage. Second, user-side temporal patterns can help identify some noisy interactions that cannot be distinguished without time information. For example, if a user usually watches horror movies at night and talk shows in the morning, a record of that user watching a horror movie in the morning is likely to be a noisy interaction, and this noisy interaction cannot be distinguished without the interaction timestamps. However, if the recommender system is aware of this temporal user pattern, it could recommend more talk shows instead of horror movies to the user in the morning. Third, item-side temporal patterns can help identify noisy interactions as well. For example, for some movies, the records of watching these movies mainly occur around a certain time (e.g., Christmas movies are usually viewed around Christmas). Then, a record of a user watching such movies might not align with that user’s usual preferences, but given that the interaction time is close to this particular time, the model will be able to recognize this is less likely to be a noisy interaction. In addition to denoising, time information can also help give a more accurate and timely recommendation for users. To bridge this gap between time-aware recommendation and denoising neural graph collaborative filtering, we aim to design a mechanism that can incorporate interaction time into neural graph collaborative filtering with the following desired properties: (1) be easy to be integrated with most state-of-the-art neural graph collaborative filtering models, (2) be able to learn temporal patterns of users and items, and (3) enhance the model’s performance in both denoising and prediction. Given that most collaborative filtering systems calculate the final prediction score/ranking based on the dot product between user and item embeddings, we propose to generate time-aware user and item embeddings, where their dot products preserve both (1) the global preferences that previous works have been exploiting and (2) the temporal preferences modeling the probability an interaction exists at a particular timestamp, which have been largely ignored by previous works. The encoders for generating time-aware embeddings can be optimized along with the training of the base model. Furthermore, we improve two popular methods for denoising bipartite graph in recommender systems, including reweighting the adjacency matrix (DeBaTeR-A) and reweighting the loss function (DeBaTeR-L), by integrating our proposed time-aware embeddings into the two methods to improve their ability to identify noisy interactions and produce more accurate predictions. DeBaTeR-A calculates the reliability score by the similarity between time-aware user and item embeddings, then reweights and prunes edges in bipartite graph through both soft and hard assignment to remove or downweight noisy interactions. DeBaTeR-L utilizes a weight generator that leverages time-aware emeddings to predict the probability of an interaction being noisy and minimizes the influence of noisy interactions by downweighting noisy samples in the loss function. For both DeBaTeR-A and DeBaTeR-L, time-aware embeddings are also utilized in the base model to enhance the prediction accuracy. We evaluate the effectiveness of proposed methods on four real-world datasets and compare them with state-of-the-art methods. We further inject noisy edges into training datasets to evaluate the robustness of the proposed models against random noises. Ablation studies have been conducted to demonstrate improvements by our proposed time-aware embeddings learning mechanism. Experiments on datasets with different noise sampling strategies demonstrate that our models successfully capture the temporal patterns. Our main contributions can be summarized as follows: • We propose a method (DeBaTeR) that leverages time information in neural graph collaborative filtering to enhance both denoising and prediction performance. To the best of our knowledge, this is the first work to leverage time information in neural graph collaborative filtering recommender systems. • We propose two methods, DeBaTeR-A and DeBaTeR-L, that utilize DeBaTeR for denoising recommender systems by reweighting edges or losses. • Extensive experiments are conducted to demonstrate the efficacy of our proposed methods. Our methods outperform state-of-the-art models for both utility and robustness. The rest of the paper is structured as follows. Section 2 provides relevant preliminary knowledge. Section 3 introduces our proposed methods. Section 4 presents the experimental results and analysis. Section 5 discusses related works. Section 6 draws the conclusion of the paper."
https://arxiv.org/html/2411.09180v1,LEAP:D - A Novel Prompt-based Approach for Domain-Generalized Aerial Object Detection,"Drone-captured images present significant challenges in object detection due to varying shooting conditions, which can alter object appearance and shape. Factors such as drone altitude, angle, and weather cause these variations, influencing the performance of object detection algorithms. To tackle these challenges, we introduce an innovative vision-language approach using learnable prompts. This shift from conventional manual prompts aims to reduce domain-specific knowledge interference, ultimately improving object detection capabilities. Furthermore, we streamline the training process with a one-step approach, updating the learnable prompt concurrently with model training, enhancing efficiency without compromising performance. Our study contributes to domain-generalized object detection by leveraging learnable prompts and optimizing training processes. This enhances model robustness and adaptability across diverse environments, leading to more effective aerial object detection.","Object detection in aerial imagery is rapidly progressing in tandem with advancements in deep learning. However, the unique characteristics of drone imagery pose significant challenges, impacting object appearance and shape due to varying shooting conditions. As a result, there is considerable interest in employing domain generalization techniques to address these challenges. Factors such as drone altitude, angle, and weather conditions contribute to variability in shooting conditions. Variations in drone altitude can affect object sizes, while camera viewing direction can alter object shapes. Moreover, weather conditions and the time of day when images are captured cause significant differences in lighting, further influencing the imagery. To simply enhance the object detection performance in drone captured images, [1, 2] conducted research on methods for processing high-resolution images. The Nuisance Disentangled Feature Transform (NDFT) is a technique designed to disentangle and separate nuisance features from important features. Wu et al. collected shooting condition information as metadata, which defines each environment as a ”domain” and allows for classification specific to each domain [3]. Adversarial training during backpropagation is used by introducing negative values into the backbone network, ensuring the model does not overly respond to domain-specific features but instead leverages domain-invariant features for object detection. Lee et al. proposed an improved version of NDFT, called A-NDFT, by introducing feature replay and a slow learner strategy to address the learning speed limitations observed in NDFT, significantly enhancing training speed while maintaining comparable performance [4]. Jin et al. proposed the Fine-Grained Feature Disentanglement (FGFD) module, which uses the one-stage detector YOLOv5 [5] as the backbone network to differentiate between domain-specific and domain-invariant feature maps [6]. There has been a significant surge in interest around vision-language representation learning tasks, largely due to training on large-scale image-text pairs, which leads to notable generalization performance. This advantage has been effectively harnessed to deliver remarkable results in image classification. Specifically, active research is underway in domains that demand high generalization capabilities, such as few-shot learning, zero-shot learning, and open-vocabulary learning. Zhou et al. proposed the Context Optimization (CoOp) approach, which addresses concerns about the time-consuming nature of using manually crafted prompts for labeling [7], similar to CLIP [8]. To address this issue, they replaced manual prompts with learnable prompts, reducing labeling time while maintaining performance comparable to CLIP. Building on this concept, Zhou et al. introduced Conditional Context Optimization (CoCoOP), where they incorporated a subnetwork called Meta-Net to conditionally pass information extracted from the image encoder to the learnable prompt, leading to high performance in vision-language tasks [9]. In the field of domain-generalized object detection, a noticeable trend has emerged toward incorporating large-scale vision-language models. Vidit et al. introduced CLIP-the-Gap, which leverages data augmentation in the feature space through text prompts. This approach enhances generalization across diverse environments for models trained on single domains with various known domain labels [10]. Liu et al. built on this concept with LGNet, a pioneering work utilizing large-scale vision-language models for aerial image object detection [11]. Their approach employs a two-step training process: First, text prompt embeddings are fine-tuned using manual prompts, and then the object detection model is trained with the fine-tuned CLIP model. The manual prompts incorporate domain-specific information like drone altitude, angle, and weather. However, this approach has a limitation in that it confines drone-specific information to only altitude, angle, and weather, neglecting other potential conditions that might impact detection performance. Our proposed approach diverges from the traditional reliance on manual prompts by employing a learnable prompt. This adjustment enables us to utilize a broader spectrum of domain-specific knowledge beyond the fixed information used in previous methods. Moreover, we shift from the conventional two-step training process, where text prompt embeddings are fine-tuned separately before training the object detection model. Instead, by updating the learnable prompt concurrently with the object detection model training, we establish a more efficient, streamlined one-stage training process. The proposed LEAP:D approach, illustrated in Figure 1, streamlines the training process by adopting a learnable prompt mechanism that updates concurrently with the object detection model, enabling efficient one-step training compared to traditional two-step methods."
https://arxiv.org/html/2411.09174v1,Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance,"Recent advances in image generation, particularly via diffusion models, have led to impressive improvements in image synthesis quality. Despite this, diffusion models are still challenged by model-induced artifacts and limited stability in image fidelity. In this work, we hypothesize that the primary cause of this issue is the improper resampling operation that introduces aliasing in the diffusion model and a careful alias-free resampling dictated by image processing theory can improve the model’s performance in image synthesis. We propose the integration of alias-free resampling layers into the UNet architecture of diffusion models without adding extra trainable parameters, thereby maintaining computational efficiency. We then assess whether these theory-driven modifications enhance image quality and rotational equivariance. Our experimental results on benchmark datasets, including CIFAR-10, MNIST, and MNIST-M, reveal consistent gains in image quality, particularly in terms of FID and KID scores. Furthermore, we propose a modified diffusion process that enables user-controlled rotation of generated images without requiring additional training. Our findings highlight the potential of theory-driven enhancements such as alias-free resampling in generative models to improve image quality while maintaining model efficiency and pioneer future research directions to incorporate them into video-generating diffusion models, enabling deeper exploration of the applications of alias-free resampling in generative modeling.","Recent advancements in generative modeling, particularly in diffusion models [6], have pushed the boundaries of what is possible in high-quality image synthesis. Among these, the Stable Diffusion model has gained prominence for its ability to generate realistic images by iteratively refining noise into coherent visual outputs [4]. Despite its success, there remains a challenge in further enhancing the model’s performance, particularly in terms of stability and image fidelity [3]. In this paper, we hypothesize that the existing resampling operations (upsampling/downsampling) in the architecture of current diffusion models introduce aliasing which leads to a reduction of image quality. We also propose that proper theory-driven alias-free resampling can improve the model’s performance in image synthesis. Improving the performance of image synthesis via alias-free resampling techniques has recently been explored in generative adversarial networks (GANs). Indeed, StyleGAN3 [9], the latest iteration in the StyleGAN series, has demonstrated significant improvements over its predecessors by incorporating carefully designed alias-free resampling layers via anti-aliasing filtering techniques that prevent high-frequency artifacts and improve the overall visual coherence of generated images. Earlier versions of StyleGAN networks, including StyleGAN2 [10], fail to rigorously implement the alias-free resampling during the up or downsampling stages and this careless signal processing can be a root cause of aliasing in the generator network [9]. Their advancements have shown that even small architectural modifications, when grounded in image processing principles, can lead to substantial gains in model performance. Alias-free resampling ensures that when images are upsampled or transformed between scales, no high-frequency components are artificially introduced, which could corrupt the learned texture or details. Thus, incorporating these principles allows the model to avoid aliasing artifacts and enhance its rotational equivariance and overall output fidelity. In addition to GAN networks, we hypothesize that the principle of alias-free resampling is especially crucial for diffusion models, where sequential resampling occurs across multiple scales, making it essential to preserve fine details and avoid introducing spurious artifacts. However, to date, proper integration of alias-free resampling into diffusion models remains largely unexplored. Current diffusion models typically apply standard downsampling and upsampling operations [6], which can introduce aliasing and degrade the quality of generated images, especially at finer scales. By integrating alias-free resampling techniques, diffusion models could achieve more stable and artifact-free outputs, enhancing both image fidelity and rotational consistency across generated samples. This work investigates the theory-driven integration of alias-free resampling techniques into the UNet structure of Diffusion models. Importantly, our approach focuses on enhancing model performance without introducing any new trainable parameters, thereby maintaining the model’s efficiency and simplicity. By strategically incorporating alias-free resampling layers, we aim to leverage the principles of image processing to improve the stability and output quality of the diffusion model. Furthermore, we propose a modified diffusion process to incorporate user-controlled rotation of the generated image without any additional training. Our experiments show that incorporating our proposed modifications significantly enhances the image quality across various configurations of the UNet structure. Specifically, our modified configurations outperformed the standard UNet model on benchmark datasets such as MNIST, CIFAR-10, and MNIST-M [11, 12, 5]. This highlights the potential of using appropriate alias-free resampling layers, as guided by image processing principles, to achieve better results in generative models. Our modified diffusion process also showed promising results in rotational consistency despite being trained on images without rotations. The key takeaway from our findings is that careful architectural re-design governed by signal and image processing theories can enhance model performance without the need for additional trainable parameters, thereby offering a path forward for further innovations in generative modeling."
https://arxiv.org/html/2411.09170v1,Towards Scalable Handwriting Communication via EEG Decoding and Latent Embedding Integration,"In recent years, brain–computer interfaces have made advances in decoding various motor–related tasks, including gesture recognition and movement classification, utilizing electroencephalogram (EEG) data. These developments are fundamental in exploring how neural signals can be interpreted to recognize specific physical actions. This study centers on a written alphabet classification task, where we aim to decode EEG signals associated with handwriting. To achieve this, we incorporate hand kinematics to guide the extraction of the consistent embeddings from high–dimensional neural recordings using auxiliary variables (CEBRA). These CEBRA embeddings, along with the EEG, are processed by a parallel convolutional neural network model that extracts features from both data sources simultaneously. The model classifies nine different handwritten characters, including symbols such as exclamation marks and commas, within the alphabet. We evaluate the model using a quantitative five–fold cross–validation approach and explore the structure of the embedding space through visualizations. Our approach achieves a classification accuracy of 91 % for the nine–class task, demonstrating the feasibility of fine–grained handwriting decoding from EEG.","I INTRODUCTION Over the past few decades, brain–computer interfaces (BCIs) have been developed across various fields to enhance human convenience[1]. Particularly, the electroencephalogram (EEG) has enabled non–invasive data collection, offering a balance between signal–to–noise ratio (SNR) and ease of acquisition, which has driven extensive research. For instance, studies have decoded sleep stages to monitor rest quality[2], mental states to prevent cognitive fatigue–related accidents[3, 4, 5, 6, 7], motor imagery for controlling external devices[8, 9], imagined speech for communication efficiency[10], as well as classified schizophrenia from the EEG to aid in diagnosis and treatment[11]. These works focus on enhancing seamless interaction and effective communication through BCI systems. Research on decoding motor intentions has progressed in aiding motor-impaired individuals by developing EEG-based control systems. The EEG-based robotic arm control for tetraplegics achieved 70.5% accuracy in feedback training[12]. A convolutional neural network (CNN) followed by a bidirectional long short–term memory network for 3–D arm movement decoding showed approximately 60% success in real-time robotic control[13]. However, these studies are often limited to broader motor decoding, which can pose challenges in achieving finer motor detail decoding. Fine motor decoding in particular requires highly detailed neural information, yet EEG signals are often limited by their high dimensionality and noise, complicating efforts to achieve precise classification. Such constraints highlight the need for methods capable of capturing meaningful, low-dimensional embeddings from complex neural data. In speech decoding, low and high–frequency neural activity has been shown to play a significant role in imagined speech decoding, particularly in phonetic spaces [14]. The high–frequency EEG features have demonstrated superior classification accuracy in imagined speech and visual imagery [15]. Recently, the Diff–E model, using denoising diffusion probabilistic models (DDPM), significantly improved imagined speech decoding accuracy [16]. Despite these advances, EEG-based imagined speech decoding is still challenged by issues related to the signal-to-noise ratio (SNR). Figure 1: Framework for hand kinematics–aware consistent embedding generation and feature fusion using CEBRA–CNN. To address the limitations of traditional EEG decoding in capturing fine motor details and handling low signal–to–noise ratios in imagined speech tasks, we employ a recently proposed encoding method, consistent embeddings from high–dimensional neural recordings using auxiliary variables (CEBRA) [17], which enables robust and consistent latent representations through contrastive learning. CEBRA is designed to jointly utilize behavioral and neural data, generating interpretable embeddings that maintain consistency across sessions, tasks, and subjects. By leveraging auxiliary variables such as time or behavioral context, CEBRA efficiently maps neural dynamics, creating reliable low–dimensional embeddings even for complex, high–dimensional data. CEBRA has demonstrated high performance in generating behaviorally relevant embeddings from invasive animal recordings, such as Neuropixels and two–photon calcium imaging. However, CEBRA has not yet been tested on non–invasive human EEG data. In this study, we apply CEBRA to EEG signals collected during a written alphabet classification task, exploring its potential to generate consistent embeddings in this domain. A CNN–based model jointly processes the EEG data and the CEBRA embeddings, achieving a written alphabet recognition accuracy of 92.1%, which outperforms traditional classification models such as CNN, EEGNet[18], and DeepConvNet[19]."
https://arxiv.org/html/2411.09168v1,Theory of Mind Enhances Collective Intelligence,"Collective Intelligence plays a central role in a large variety of fields, from economics and evolutionary theory to neural networks and eusocial insects, and it is also core to much of the work on emergence and self-organisation in complex systems theory. However, in human collective intelligence there is still much more to be understood in the relationship between specific psychological processes at the individual level and the emergence of self-organised structures at the social level. Previously psychological factors have played a relatively minor role in the study of collective intelligence as the principles are often quite general and applicable to humans just as readily as insects or other agents without sophisticated psychologies. In this article we emphasise, with examples from other complex adaptive systems, the broad applicability of collective intelligence principles while the mechanisms and time-scales differ significantly between examples. We contend that flexible collective intelligence in human social settings is improved by our use of a specific cognitive tool: our Theory of Mind. We identify several key characteristics of psychologically mediated collective intelligence and show that the development of a Theory of Mind is a crucial factor distinguishing social collective intelligence from general collective intelligence. We then place these capabilities in the context of the next steps in artificial intelligence embedded in a future that includes an effective human-AI hybrid social ecology.","All intelligence is collective intelligence. [70] Collectives are capable of achieving things that individuals alone cannot. Notwithstanding the simplicity or complexity of the individuals, their aggregate behaviour can often be understood as a complex processing of information that individuals store, modify, and transfer between each other producing ‘useful’ collective behaviour at the scale of the whole collective. In most instances of Collective Intelligence (CI), where the agents might be ants in an ant colony, bees in a beehive, or neurons in a neural network, the individual is not aware of the drivers of their behaviour or the behaviour of other agents. For example, a single neuron is neither aware of its own internal processes nor that of a neuron it is connected to, nor is it aware of the end goal to which its activity contributes. Despite both this lack of awareness and the lack of a centralised controller, evolutionary and learning processes have produced an intricate, precise, and highly adaptive system that is capable of functional behaviour that would be impossible for any single neuron to achieve. In other instances of CI, such as teams of humans, or businesses interacting in economic markets, the agents themselves may be highly complex and exhibit varying degrees of purposefulness and awareness. Within this context, we draw attention to the role of psychological factors in improving the CI of human social collectives and quantifying the intelligence of social collectives, both natural and artificial. In order to understand how collectives process information, we first consider the variety of ways in which agents interact. The topology of the network describing agent-to-agent interactions is well known to be important for the proper functioning of social groups [83, 79]. In particular it has been shown that mammalian social groups exhibit patterns of fractal-like topologies [40, 51] that are a result of a cognitive ability to form discrete social connections between conspicifics [49]. These links are often both spatially and temporally transient; people meet for a while, go their separate ways, and come back together later. Despite this transience, individual connections are often the basis of long term social relationships between specific individuals as in pair-bonding and friendships. Consequently an important distinction can be made regarding connections between agents in complex adaptive systems: they can be more fluid-like or more solid-like [101]. For example the links between neurons in the brain are relatively fixed in nature when compared to the brief communicative interactions between ants, either instantaneous interactions between individual ants or via transient pheromone trails that coordinate the behaviour of large numbers of ants. Solé and colleagues [101, 88] identify a distinction between solid brains, in which interactions between agents fixed in place are highly persistent in time (e.g. neural networks, spin glasses) and liquid brains, in which interactions between highly mobile agents are much more short-lived (e.g. ants, immune cells). As Solé et al. note regarding liquid brains [101]: “Here there are no neural-like elements and yet in many ways these systems solve complex problems, exhibit learning and memory, and make decisions in response to environmental conditions.” All biological agents are composed of sub-units such as organs, cells, and molecular networks [67, 69, 71]. Cells in particular are the simplest living organisms with individual intelligence, or competencies [67, 33], within their native contexts. Here, we briefly focus on the archetypal single-cell intelligence, the neural cells. It is well understood that the central nervous system is a highly developed, adaptive, complex system that exhibits emergent computational characteristics [52], both in biological and artificial neural networks. Naturally the artificial models are simplifications but the extent to which they are simplifications is not so well understood. In a 2021 study, Beniaguev et al. [9] concluded that between five and eight layers of an artificial deep neural network are required to approximate the input–output mapping of a (single) cortical neuron and that the dendritic branches can be understood as spatiotemporal pattern detectors. This demonstrates that a single neural cell can be modelled as an artificial agent with highly complex computational capabilities situated within an adaptive, complex network of other highly complex agents, all signalling to one another. These results can be compared with earlier studies in which neurons were modelled as a Bayesian agent that is trying to infer the state of a hidden variable [25]. In each of these interpretations, a single cell can be seen as an agent with computational competencies situated within the context of a network that is slowly and adaptively changing around it. We can also compare the competencies of neural cells in networks to the individual competencies of ants in an ant colony. In a recent study [56] it was shown that social structures of some ant colonies are conserved between species that are separated by more than 100 million years of evolution. In the five species studied by Kay et al. [56], they found two social clusters and similarities in the division of labour that are preserved between the species. In a different study, Richardson et al. [91] showed that individuals within an ant colony play an important leadership role and that the behaviour of these individuals significantly improved the collective performance of the ants. Ants are also capable of changing their social structure in the event of pathogenic infestation of their colony. In a 2021 article, Stockmaier and colleagues [102] review the research on social distancing and other social restructuring that occurs with conspecifics in order to reduce the impact of pathogens by changing their social cues, signals, and other behaviours for the collective benefit of the colony. These two very different systems, neural networks and ant colonies, are examples of complex collective intelligences where the individuals (neurons, ants) are complex in their own right, but they signal each other in order to restructure their relationships so as to adapt their collective competencies to external signals. The neural networks are prototypical solid brains and ant colonies are prototypical liquid brains. Human social interactions can also be viewed as a form of liquid intelligence. Migliano et al. [79] discuss the ‘fluidity’ of social relations in early human societies: “Quantification and mapping of hunter–gatherers’ social networks has revealed details of a fluid and multilevel sociality, where friendship links connect unrelated mobile households into camps of temporary composition”. They describe the key characteristics of early human society, such as egalitarianism, division of labour, cooperative living with unrelated individuals, multi-locality, fluid social structures, and high mobility between campsites, which might be thought of as a liquid brain composed of social interactions that both cluster and disperse in order to store, modify, and transfer information via social networks. The notion that human social interaction might be a form of computation is not new: Mirowski, Axtell and colleagues [82, 60, 81] have suggested that economic markets are a form of computation by which prices can be derived, and Harré recently hypothesised [45] that this could be measured using information theory as had been done earlier for financial markets [47, 42]. As Axtell et al. [60] wrote: “There is a close connection between agent computing in the positive social sciences and distributed computation in computer science, in which individual processors have heterogeneous information that they compute with and then communicate to other processors.” The emergence of computation in multi-agent systems is a well-studied area of complex adaptive systems [64, 84]. For example neuroscience has used information theory to describe the storage, transfer, and modification of bits of information in biological neural processes [117]. More broadly, Integrated Information Theory (IIT) [107, 77] has been put forward as a measure of the emergence of ‘consciousness’ in generic (non-biological, non-neural) systems. In this case, some forms of IIT explicitly use information theory [8, 78] to measure the amount of non-trivial computation a system is carrying out. More generally, there is a move towards understanding complex adaptive systems in computational terms [89, 74] by empirically measuring the inter-agent flow of information [12]. In this article we use information theory to quantify how much computation in a CI is ‘emergent’ and how much is simply independent information processing by single agents. In general, we wish to capture the notion of the whole (computational process) being greater than the sum of the (independent) parts. We translate this to the simple notion that to the extent to which this inequality holds: Whole - \sum(Parts)>0 is the extent to which we will say a system exhibits non-trivial CI, noting that there are multiple possible implementations of this approach [54]. The Parts is how much computation a single agent is carrying out from one time step to the next such that the sum is the total of all agents’ independent computations. The Whole is the totality of computation in the system, it includes all single agent computations, pairwise computations, and higher order interactions between agents. Our measure will not be unique in any of its specifics, but it serves to quantify the CI of a system for comparative analysis. This approach also has much in common with that of Moore et al. [84] in which information theory is used to measure the collective intelligence in biological systems. Not only is there diversity in the types of systems that can show positive measures of CI, but the ways in which agents manipulate a system’s computations is diverse as well. Take for example Watson and Levin’s discussion of a scientist manipulating the intercellular signalling in order to change their collective outcome [110]: This framework [of collective cellular intelligence] makes a strong prediction: if intercellular signalling (not genes) is the cognitive medium of a morphogenetic individual, it should be possible to exploit the tools of behavioural and neuro-science and learn to read, interpret and re-write its information content in a way that allows predictive control over its behaviour (in this case, growth and form) without genetic changes. A counter question is: How can single agents, such as human leaders, have predictive control over a social group? Just as a scientist external to a cell collective can manipulate inter-cellular signalling to control the outcomes of the cell collective, a leader internal to a human collective can manipulate inter-personal behaviours to control the outcomes of the human collective. In both cases, an agent with a goal-directed psychology is acting on inter-agent relationships, i.e. inter-cellular or inter-personal, to control outcomes at the next level higher, i.e. organism-scale or societal-scale. In this work we will ask an analogous question of human agents: What is there in human psychology that allows us to learn to read, interpret, and re-write our interpersonal information content in a way that allows predictive control over our collective behaviour? We will not be able to explore all of the possible interpretations of this question here, but we posit that our Theory of Mind (ToM) is a suite of cognitive skills that allows individuals to have goal directed control over collective outcomes. Originally ToM was used to describe our ability to infer the unobserved mental states of other people [34] such as desires and beliefs, an ability humans are particularly good at and other animals much less so [86, 59]. But recently it has been shown that ToM is predictive of group performance as well [121, 31], empirically demonstrating the role of ToM in going beyond representations of the internal states of others to using that knowledge in a social setting to improve the collective outcomes for the group. In order to model ToM in a tractable fashion, we will focus on the narrower game theory of mind [123], and the Beliefs, Preferences, and Constraints (BPC) interpretation of game-theoretic decisions put forward by Gintis [35]. In this approach, what agents understand of other agents’ hidden states are the BPC that structure their observable behaviours. We will consider this question in the framework of agent interactions that extend agent utilities in a simple but novel way. We quantify our results using information theory to show the impact that a correctly deployed ToM has to direct agents’ behaviours in order to increase our CI. The models are simple but they illustrate the central notion that understanding the “beliefs, preferences, and constraints” [36, 37] of others can be used to improve the CI of a complex social system. In Section 2, we describe the liquid–solid dichotomy of interacting agents, review extant models of ToM, and provide perspective on the interplay between social network structures and ToM. In Section 3, we provide illustrative examples supporting different aspects of our argument, introducing our measure of computation and applying it to a simple empirical example. In Section 4 we review the psychology of social fluidity and the variety of social outcomes that this fluidity makes possible. We also use a simple multi-agent system to describe how a ToM can be used to improve the computational processes, i.e. the CI, of interacting agents. Finally, in Section 5, we discuss the broader implications of this approach."
https://arxiv.org/html/2411.09134v1,ABCI 3.0: Evolution of the leading AI infrastructure in Japan,"ABCI 3.0 is the latest version of the ABCI, a large-scale open AI infrastructure that AIST has been operating since August 2018 and will be fully operational in January 2025. ABCI 3.0 consists of computing servers equipped with 6128 of the NVIDIA H200 GPUs and an all-flash storage system. Its peak performance is 6.22 exaflops in half precision and 3.0 exaflops in single precision, which is 7 to 13 times faster than the previous system, ABCI 2.0. It also more than doubles both storage capacity and theoretical read/write performance. ABCI 3.0 is expected to accelerate research and development, evaluation, and workforce development of cutting-edge AI technologies, with a particular focus on generative AI.","AI Bridging Cloud Infrastructure (ABCI) is the world’s first large-scale open AI infrastructure, designed and developed by AIST with the aim of accelerating the development of AI technologies in Japan [1, 2, 3]. It has been installed in the AI Data Center located at AIST’s Kashiwa Center, and began operating in August 2018. To date, many organizations have achieved remarkable results by using ABCI, including the successful construction of Japanese large language models (LLMs) such as PLaMo [4], Swallow LLM [5], and so on. On the other hand, as the demand for generative AI in industry, academia, and government in Japan is growing rapidly, ABCI cannot provide enough resources to meet such user demands in a timely manner. In addition, the development of generative AI is still at an early stage and currently focuses mainly on natural languages. In the near future, it is important to develop real world multimodal foundation models that are built using large amounts of image, audio and sensor data obtained from the real world, such as manufacturing, transportation, and so on. In order to demonstrate such cutting-edge AI technologies, it is imperative to improve the computational capability of ABCI. Since November 2024, AIST has been gradually starting to operate the latest system in the ABCI series, ABCI 3.0, while leveraging the existing technical assets of ABCI. ABCI 3.0 will be fully operational in January 2025. This paper is organized as follows. Section 2 introduces the hardware configuration of ABCI 3.0. In Section 3, we describe the initial software installed. The data center facility where we have upgraded for ABCI 3.0 is presented in Section 4. Finally, Section 5 briefly mentions the future perspective. Table 1: ABCI series AAIC a ABCI 1.0 ABCI 2.0 b ABCI 3.0 Start of operation 2017 2018 2021 2024 Number of nodes 50 1088 120 766 GPU NVIDIA P100 NVIDIA V100 NVIDIA A100 NVIDIA H200 Number of GPUs 400 4352 960 6128 CPU Intel Xeon E5 v4 Intel Xeon Gold 6148 Intel Xeon Platinum 8360Y Intel Xeon Platinum 8558 FP64 (PFLOPS) 2.2 37.2 19 415 FP32 (PFLOPS) 4.4 74.4 150 3000 FP16 (PFLOPS) 8.6 550 300 6220 a AAIC is the prototype system of ABCI. b ABCI 2.0 is the extension based on ABCI 1.0, and this table only shows the specification of the extension part. We usually refer to the performance of ABCI 2.0 as the performance of ABCI 1.0 with the extension part."
https://arxiv.org/html/2411.09125v1,"DROJ: A Prompt-Driven Attack against 
Large Language Models","Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks. Due to their training on internet-sourced datasets, LLMs can sometimes generate objectionable content, necessitating extensive alignment with human feedback to avoid such outputs. Despite massive alignment efforts, LLMs remain susceptible to adversarial jailbreak attacks, which usually are manipulated prompts designed to circumvent safety mechanisms and elicit harmful responses. Here, we introduce a novel approach, Directed Rrepresentation Optimization Jailbreak (DROJ), which optimizes jailbreak prompts at the embedding level to shift the hidden representations of harmful queries towards directions that are more likely to elicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat model show that DROJ achieves a 100% keyword-based Attack Success Rate (ASR), effectively preventing direct refusals. However, the model occasionally produces repetitive and non-informative responses. To mitigate this, we introduce a helpfulness system prompt that enhances the utility of the model’s responses. Our code is available at https://github.com/Leon-Leyang/LLM-Safeguard.","Large Language Models (LLMs) are powerful conversational systems that have demonstrated significant potential across numerous domains, including content generation, data analysis, and the healthcare industry. Their remarkable performance is largely due to training on extensive text datasets, which enables them to generate high-quality responses on a wide range of topics. However, because these datasets are often sourced from internet materials that may contain inappropriate content, there is a risk of such content appearing in the model’s outputs (Ousidhoum et al., 2021). To mitigate this risk, LLMs typically undergo extensive safety alignment through human feedback during development to prevent harmful or inappropriate responses (Anwar et al., 2024). Despite these safety measures, LLMs—such as the widely used ChatGPT—remain vulnerable to adversarial attacks. Upon its release, vulnerabilities were exploited using carefully crafted prompts that elicited responses capable of spreading misinformation, hate speech, and other harmful content, posing significant societal risks (Zvi, 2022). Recent research on enhancing the adversarial robustness of LLMs has largely focused on two approaches: (1) developing defense mechanisms to improve models’ ability to detect and decline malicious queries (Xie et al., 2023; Alon & Kamfonas, 2023), and (2) designing new jailbreak attacks, which often involve specially optimized prefixes or suffixes added to malicious queries (Lapid et al., 2023; Wang et al., 2024; Wei et al., 2023). One common safeguarding technique is the use of a safety prompt at the system level, which typically includes explicit instructions to ensure safe outputs and prevent harmful responses, as illustrated in Figure 1. This approach has been successfully implemented in various LLMs, including Mistral (Jiang et al., 2023) and GPT-4 (Achiam et al., 2023). Further research by Zheng et al. (2024) examined the effect of safety prompts on LLMs’ representational space, revealing that both harmful and benign queries are shifted toward a refusal direction in the model’s representation space, where the LLM is less likely to provide responses, as depicted in Figure 2. Building on these findings, we introduce Directed Representation Optimization Jailbreak (DROJ), a novel jailbreak method that optimizes prompts to shift representations of both harmful and benign queries toward directions more likely to elicit compliance from the model. Figure 1: Prepending a safety prompt (right) to the input query can help the model refuse to respond to malicious prompts it might otherwise comply with. Figure 2: Visualization of the hidden state of Mistral-7B-Instruct-v0.1 after applying 2-dimensional Principal Component Analysis (PCA). Left: Visualization of eight groups of queries (harmful/harmless + three kinds of safety prompts). The boundary (black dotted line) is fitted by logistic regression, showing that harmful and harmless queries can be distinguished without safety prompts. The red and blue arrows indicate the similar movement direction of both harmful and harmless queries, respectively, when safety prompts are added. Right: Color represents the empirical refusal rate, with the fitted boundary (gray line) indicating separation of accepted and refused queries. The gray arrow (normal vector of the fitted regression) indicates the direction of higher refusal probability. Figure reproduced from (Zheng et al., 2024)."
https://arxiv.org/html/2411.09105v1,VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition,"Recent advancements in Large Video-Language Models (LVLMs) have led to the development of benchmarks aimed at assessing cognitive abilities. However, most existing benchmarks heavily rely on web-collected videos paired with human annotations or model-generated questions, which limit control over the video content and fall short in evaluating advanced cognitive abilities involving symbolic and abstract concepts. To address these limitations, we introduce VCBench, a controllable benchmark to assess LVLMs’ cognitive abilities, involving symbolic and abstract elements at varying difficulty levels. By generating video data with the Python-based engine, VCBench allows for precise control over the video content, creating dynamic, task-oriented videos that feature complex scenes and abstract concepts. Each task is paired with customized question templates tailored to specific cognitive challenges, providing a rigorous evaluation test. Our evaluation reveals that even state-of-the-art (SOTA) models, such as Qwen2-VL-72B, struggle with simple video cognition tasks involving abstract concepts, with performance sharply dropping by 19% as video complexity rises. These findings reveal the current limitations of LVLMs in advanced cognitive tasks and highlight the critical role of VCBench in driving research toward more robust and generalized LVLMs for complex video cognition challenges.","With the rapid advancement of artificial intelligence (AI), large video-language models (LVLMs) have emerged as essential tools for video understanding [24, 63, 29, 23, 60, 45]. To unlock their full potential, LVLMs must exhibit cognitive abilities that approximate human-like perception and reasoning [47, 18]. While recent evaluations of large language models (LLMs) and image-based models have increasingly focused on advanced cognition [41, 9, 7], current benchmarks for video-based models [62, 36, 5, 11, 26, 14, 25] continue to overlook advanced cognitive abilities fundamental to both humans and AI. Furthermore, existing benchmarks largely focus on video domain and duration [14, 51, 25, 26], without precise control over video content and difficulty, resulting in limited assessment of cognitive depth required for advanced video understanding. Constructing benchmarks from real-world videos also poses challenges, including intensive prompt engineering, manual annotation, and data filtering [16, 5, 36, 25, 32, 38, 53], along with the risk of data leakage where video content may inadvertently be included in model training [58]. To address these limitations, we introduce VCBench, a controllable Video Cognitive Benchmark designed to evaluate the cognitive capabilities of LVLMs. VCBench offers a fully programmatic approach to synthetic videos, allowing precise control over both content and complexity to evaluate the cognitive abilities of LVLMs. The video scenes integrate symbolic elements and abstract concepts, containing symbolic objects, abstract attributes (color, shape, and size), abstract actions (action type, action speed, and direction), and spatial (2D and 3D) and temporal relationships across easy-to-difficult video scenarios. For each video scene, we carefully design specific question templates with GPT4 to evaluate multiple key dimensions of video cognition. These dimensions include Object Perception [42], Action Perception [22], Spatial Reasoning [34, 44], Temporal Reasoning [35], and comprehension within gaming and full-modal environments [37, 43, 10]. Table 1 presents a comparative analysis of VCbench alongside existing benchmarks. Through extensive testing on LVLMs fine-tuned with video question-answer tasks, VCBench provides a detailed evaluation that reveals previously overlooked insights. While some models perform similarly on simple videos, a clear performance gap becomes evident as video difficulty rises to medium and difficult levels. Notably, even advanced LVLMs, such as Qwen2-VL-72B, struggle with advanced cognition of simple videos with symbolic elements, sharply declining performance as complexity rises. Specifically, Qwen2-VL-72B shows a 7% performance drop in the ‘Sky Battle’ scene with five added enemy planes and a further 10% decline at the highest difficulty. These findings highlight the urgent need for developing LVLMs with advanced cognition, especially for handling abstract and complex video scenarios, thereby setting a foundation for future research. Figure 1: In the Sky Battle scene, symbolic icons represent planes, bullets, and enemies, with ‘destroy’ depicted as an abstract action within the video. The difficulty level is adjusted by varying the number and speed of enemy symbols, with questions such as ‘How many enemy planes were destroyed by the player?’."
https://arxiv.org/html/2411.09101v1,Heuristical Comparison of Vision Transformers Against Convolutional Neural Networks for Semantic Segmentation on Remote Sensing Imagery,"Vision Transformers (ViT) have recently brought a new wave of research in the field of computer vision. These models have done particularly well in the field of image classification and segmentation. Research on semantic and instance segmentation has emerged to accelerate with the inception of the new architecture, with over 80% of the top 20 benchmarks for the iSAID dataset being either based on the ViT architecture or the attention mechanism behind its success. This paper focuses on the heuristic comparison of three key factors of using (or not using) ViT for semantic segmentation of remote sensing aerial images on the iSAID. The experimental results observed during the course of the research were under the scrutinization of the following objectives: 1. Use of weighted fused loss function for the maximum mean Intersection over Union (mIoU) score, Dice score, and minimization or conservation of entropy or class representation, 2. Comparison of transfer learning on Meta’s MaskFormer, a ViT-based semantic segmentation model, against generic UNet Convolutional Neural networks (CNNs) judged over mIoU, Dice scores, training efficiency, and inference time, and 3. What do we lose for what we gain? i.e., the comparison of the two models against current state-of-art segmentation models. We show the use of the novel combined weighted loss function significantly boosts the CNN model’s performance capacities as compared to transfer learning the ViT. The code for this implementation can be found on https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.","The introduction of Transformers [23] has changed the research landscape when it comes to attention mechanisms on Natural Language Processing (NLP) tasks. However, this potential wasn’t fully capitalized in computer vision tasks until recently when Dosovitskiy et al. implemented the attention mechanism of transformers in their seminal paper [5] ever since ViT has been one of the fundamental areas of research in computer vision. Although initially proposed for image classification tasks, like CNNs in their inception days, they soon turned out to be one of the best-performing architectures for image segmentation tasks as well. Image segmentation refers to the task of classifying each pixel of an image into a category. It can be said that image segmentation is a type of classification as well, but the role, approach and method of each subjects varies in themselves. In specific to the scope of this paper, semantic segmentation means to group certain objects in an image to a class among the given n classes in the dataset. Instance segmentation, similarly, would be to segment each object present in an image as a distinct item in itself. As with any deep learning computer vision tasks, image segmentations were best done by models that were capable of capturing, encoding, and decoding essential patterns of the input image, mainly the implementation of UNet style architectures in CNNs [31, 37, 1, 36, 19, 33]. Specific to the scope of this paper, in the past few years researchers have utilized the UNet CNNs in the iSAID dataset[32], which is built on top of the DOTA[29] dataset, to do semantic segmentations [18, 38, 25, 4]. One of the main pitfalls of such datasets is the background class[11]. If not handled properly during the training process, there is a good chance to see high evaluation metrics on mIoU since the model would easily overfit on the most common class, which is the unlabelled class in this case. Although the traditional deep learning technique of employing a UNet-based Convolutional Neural Network (CNN) remains the cornerstone of many segmentation tasks, recent trends in computer vision research indicate a significant shift towards Transformer-based architectures, particularly the Vision Transformer (ViT) and its variants. This shift is driven by the inherent ability of Transformer models to capture long-range dependencies through self-attention mechanisms, a feature that CNNs typically struggle with due to their localized receptive fields. The increasing preference for ViT models is exemplified by the fact that among the top 20 benchmark models for the iSAID dataset as listed on Papers with Code[paper_with_code], the top five employ either ViT, attention-based CNNs, or a hybrid combination of both [7, 8, 6, 9]. These models leverage the powerful self-attention mechanism to refine segmentation masks by focusing on relevant image regions. In this paper, we introduce a novel loss function that integrates maximizing mean Intersection over Union (mIoU) and Dice score, while preserving entropy to ensure robust mask predictions. This new loss function is integrated into the UNet framework to improve its ability to model complex spatial relationships in images. This unique formulation allows for better generalization to unseen data by maintaining a balance between maximizing overlap with the ground truth masks and preventing over-segmentation. In addition, we investigate various data augmentation techniques since the number of samples is relatively lower in the iSAID[32] dataset. In parallel, we provide a direct comparison between training a UNet CNN model from scratch and fine-tuning Meta AI’s widely adopted MaskFormer[2], a ViT-based model that has achieved state-of-the-art results in semantic segmentation tasks; opensourced in Hugging Face[26]. This paper focuses on three key objectives throughout its experimentation and analysis: • Propose a combined weighted loss function to maximize mIoU and Dice while preserving entropy • Analyze the impact on efficiency during training and inference in both architectures • Benchmark and test inference capabilities of both models against current state-of-the-art iSAID benchmarks on unseen data The rest of the paper is laid out in the following order, section II contains the current state of art and previous work on the field. Section III discusses our approach to the given objectives, then IV effectively binds the results with the research question. Section V would then conclude the paper with some ending thoughts and future direction for the research field."
https://arxiv.org/html/2411.09077v1,Drone Detection using Deep Neural Networks Trained on Pure Synthetic Data,"Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_{50} of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems. The code111The code is available https://github.com/mazqtpopx/cranfield-synthetic-drone-detection and datasets222The datasets are available https://huggingface.co/datasets/mazqtpopx/cranfield-synthetic-drone-detection are available.","Drone intrusions pose a risk to airports. In 2018, a reported drone sighting disrupted the operations at the Gatwick airport, UK, for 3 days. Since then, drones have been used for other illicit purposes, such as smuggling drugs into prisons [1, 2]. Although a serious disruption, such as the one in Gatwick, has not occurred since, reports of drone sightings at UK airports persist [3, 4, 5]. To prevent this, the UK Government presented a counter-unmanned aircraft strategy [6]. It sets the detection of drones as a key objective. On the other hand, UAS traffic management (UTM) is a relatively new area of research. It is concerned with integrating drones and air taxis with the current air traffic management (ATM). For UTM to work in urban environments, camera systems will likely have to work alongside RF/radar sensors due to interference from buildings. Accurate detection systems will enable the use of authorized drones for approved unmanned operations, whilst preventing unauthorized drones from flying in excluded parts of the airspace. Drones can be detected using different sensors such as radar, radio frequency (RF), acoustic, and electro-optical/infrared [7]. State-of-the-art detection methods typically fuse some of these sensors. Our research focuses on drone detection using a visible spectrum camera. Recent advances in neural networks (NNs) allow for much more accurate detection, tracking, and classification of drones in images or video feeds. However, accurate data for training the NN models remains difficult to obtain. In this article, we explore the use of structured domain randomization (SDR) within a simulated environment to generate a synthetic dataset of drones. We test whether a synthetic dataset generated using this method is generalizable to real-world datasets, by comparing the results to an established benchmark. I-A Literature We focus on the optical detection of drones using NNs trained on synthetic data. Although object detection in images has been extensively studied, drone detection poses unique challenges, usually caused by environmental, or systematic issues. For example, weather effects impact the image, the drones can be small in the video frame making them hard to detect or classify, and the movement of UAVs is complicated to predict. Classification of the objects in the sky such as drones and birds is also a key issue. We investigate the literature on NNs, the use of synthetic data for training NNs, applications of NNs for drone detection, and finally, the use of synthetic data for drone detection. I-A1 DNN advances and architectures Recent advances in deep neural network (DNN) architectures (e.g. AlexNet [8]) became the state-of-the-art method for object recognition. They were later expanded to detect object bounding boxes, segment objects, and track them across videos. Object detection architectures comprise of single-stage and two-stage detectors. Examples of one-stage detectors are YOLO [9], [10] and SSD [11]. Examples of two-stage detectors are Fast-RCNN [12], Faster-RCNN [13]. One-stage detectors do the detection/classification directly from the extracted features, without a separate region proposal. Two-stage detectors generate region proposals (i.e. regions of the image where it thinks an object might exist) and then classify each of the proposed regions. In theory, two-stage detectors should produce better accuracy at a higher computational cost than one-stage detectors. However, in practice, they tend to vary for different applications and are sensitive to training parameters. An example of this is Isaac-Medina et al. [14], where the results of different architectures vary across different datasets. More recently, transformers [15] which use an attention mechanism and an encoder-decoder architecture, showed improvements in performance over traditional RNNs and CNNs as they are more parallelizable. This led to detection architectures such as DETR [16]. DETR matches Faster R-CNN in terms of performance on the COCO object detection dataset. I-A2 Synthetic Data, Sim-to-Real, and Domain Randomization Accurate data for training neural networks is expensive and may be hard to obtain. Creating real-life images or videos has a cost associated with it, which varies depending on the application. After the data is recorded, it needs to be manually labelled with the ground truth by a human. Synthetic data offers an automated alternative. By using 3D models and rendering software, synthetic images along with accurate ground truth labels can be generated. This presents a research question: can images rendered by 3D software be used to train NN models and transferred to real-world applications? The use of synthetic data comes with advantages and disadvantages. The primary advantage is the potential reduction in the time cost required to generate the dataset. An algorithm can generate a very large dataset with no human input required. The primary disadvantage is that NN models trained on synthetic data are hard to transfer to real-world problems. Hence, the primary objective of synthetic data research is bridging the sim-to-real gap - the transfer of a NN model trained on synthetic data to perform equivalently on real-world data. However, machine learning algorithms used to train neural networks rely on the training and evaluation datasets coming from the same source distribution. We are fundamentally breaking this assumption by using synthetic data for training and evaluating on real-world data. This is referred to as the out-of-distribution generalization [17]. There exist multiple strategies for achieving out-of-distribution generalization such as transfer learning [18], domain adaptation [19], and domain generalization [20]. Each of these relies on different assumptions between the training data, access to test data, and training conditions. To achieve sim-to-real transfer, domain generalization is the most relevant to our problem, as we assume that we cannot see the real-world test data during training. For the problem of sim-to-real for images, the most popular method of domain generalization is domain randomization. Domain randomization (DR) is a method of introducing model generalizability. This is done by randomizing parameters used to synthesize the source distribution, with the goal of making the data distribution so wide and varied that the model trained on this dataset is transferable to the target distribution. Lighting, textures, poses, and other parameters are randomized. Popular approaches aim to make the scenes as unrealistic as possible. Tobin at al. [21] is an example, which employs DR by randomizing the textures of the objects to make the network invariant to changes in scene conditions. Trembley et al. [22] use DR to bridge the sim-to-reality gap for object detection of cars for an autonomous driving application. Their approach employs DR by inputting random shapes, textures, and cars in unrealistic positions, with the aim of generating more variety to focus the neural networks on the structure of the object of interest. Prakash et al. [23] propose the use of structured domain randomization (SDR) - a variant of domain randomization, which takes into account the context of the scene. Hence instead of generating random scenes with unrealistic textures and objects, it aims to generate realistic scenes, while randomizing other parameters in a structured way. They find that SDR provides performance improvements over DR. Borrego et al. [24] propose the use of synthetic DR datasets as an alternative to pre-trained networks. Alghonaim & Johns [25] investigate the parameters used to randomize the simulation such as camera position, target colour, and lighting variations to find the most important parameters for sim-to-real transfer. They found that more high-quality realistic images improved the performance of sim-to-real. This comes in contrast with some of the earlier works (Tobin et al. [21] and Trembley et al. [22]) which focused on unrealistic, low-quality generated images. They also found that the randomization of distractor objects and background textures was important for generalising to new environments. Hinterstoisser et al. [26] propose a ’trick’ to freeze the layers responsible for feature extraction of a NN trained on real-world images, and train only the remaining layers on synthetic images. They show that their approach works well on architectures such as Faster-RCNN, and Mask-RCNN. Hence, there appear two schools of thought. DR produces unrealistic images with the aim of expanding the domain to allow the NN to better learn the features in settings that might not necessarily be used during test time. SDR produces realistic images with the aim of making the training data similar to the real-world data used at test time. I-B Visual Drone Detection Visual drone detection is the process of finding the position of a drone in images or video feeds produced by visible-spectrum cameras. It has become a popular area of research, largely driven by the recent advances in machine learning. Most approaches [27, 28, 29, 30] typically use some of the detection architectures described in section I-A1, or a modification of the network for drone detection purposes, trained and tested on one of the publicly available drone datasets, or on a private dataset. Isaac-Medina et al. [14] present a benchmark of multiple DNN algorithms (SSD, YOLOv3, Faster R-CNN, and DETR) on publicly available datasets of flying drones (MAV-VID, Drone-vs-Bird, Anti-UAV). They present two benchmarks: the accuracy of detection, and the accuracy of tracking algorithms. They present the results of each of the models trained and tested on each of the datasets. Note that they create three distinct sets of weights for each of the datasets. We were able to independently reproduce the test metrics shown by Isaac-Medina et al. based on their published model weights for the Faster-RCNN model (although, we did not attempt to reproduce the training of the models). Freudenmann et al. [31] explore the effects of data augmentation strategies on the performance of their NN models. They set up their experiments by training a Faster R-CNN model on the Drone vs Bird dataset. They then test their NN model on other datasets: a subset of the Drone vs Bird dataset, Anti-UAV dataset, background test dataset (images of drones scraped from the internet), Disturbing Objects dataset (to find false positives), and New UAV Types (unseen UAV images scraped from the internet). They find that by test a model trained on the DvB dataset, on the Anti-UAV dataset, the accuracy drops significantly. The authors propose the cause to be the mosaics and artefacts found in the Anti-UAV dataset. To reduce this effect, the authors propose the use of mosaic + standard dataset transforms. This increases the mAP to 78.7% from 54.2% with no augmentations. The augmentations appear to train the model to reduce the false positive rate. Still, this falls short of the results presented by Isaac-Medina et al. where the Faster R-CNN model trained on the Anti-UAV dataset and tested on the subset of the Anti-UAV dataset achieves 97.7%. This highlights the challenge of transferring models trained on one dataset to other datasets - a drone detection model trained on one dataset is not guaranteed to perform equivalently on another dataset. Mediavilla et al. [32] present a benchmark of Swin, YOLOv4, CenterNet, CenterNet2, and Faster-RCNN architectures on CACHOW - a helicopter dataset - and Drone vs Bird datasets. However, the results are not directly comparable with Isaac-Medina et al. as the test configurations are different. Other approaches consider the temporal dimension. Thai et al. [33] present a spatio-temporal NN design, in which multiple frames of the video feed are stacked together before being input into the NN. They show that this approach is more accurate, but comes with a higher computational cost as more data needs to be processed. Craye and Ardjoune [34] use U-Net to segment the position of the drone, classify it, and then use a spatio-temporal filter. Sangam et al. [35] present a spatio-temporal transformer for drone-to-drone detection. I-C Synthetic Data for Drone Detection Synthetic data approaches have been attempted in the field of drone detection. One of the first attempts that we were able to find is Rozantsev et al. [36] which uses domain randomization techniques (without referring to them as such - the paper was published before domain randomization became a common term in literature), such as randomizing the position of the drones, varying the motion blur, adding random noise, and randomizing object material properties. They also repeat this process for aircraft and cars. Marez et al. [37] create a synthetic dataset of UAVs using DR and test it on the Drone vs Bird dataset. They create a baseline model (trained only on DvB) and compare the models trained only on the DR datasets, and models pre-trained on the DR datasets and finetuned on real-world data. They find that DR-only datasets perform the worst, while the finetuned models perform better but still fall short of the baseline. This highlights the challenge of transferring synthetically trained models to real-world datasets. Peng et al. [38] use 3D models of drones and random HDRIs to generate a photorealistic synthetic dataset, and train a Faster-RCNN model. It is tested on a real-life dataset of images downloaded from the internet. They find that using a pre-trained model improves their performance significantly, and using occluded drone images improves their performance by 1%. However, their test dataset of images downloaded from the internet might not be representative of a real-world security scenario. We were also unable to find a copy of their test dataset online which makes it hard to directly compare the results to. Dieter et al. [39] attempt to bridge the sim-to-real gap by generating a synthetic dataset in Unreal Engine. They test their models on synthetic and real data. Overall, they find that, generally, models trained on synthetic data fall short in terms of the performance of the models trained on real-world data. However, by using a small share of real-world data, they improve the results and get close to the results produced by using real-world data only. DronePose [40] uses 3D models of a DJI Mavic, and a DJI Inspire, to create a synthetic dataset using Unreal Engine. They also create a neural network based on a U-Net architecture, to segment parts of the drone, find the orientation, and identify the drone model. The use of GANs to generate a synthetic dataset was attempted by Li et al. [41]. This is done to better understand the deep feature space of how drones are represented by neural networks. Further, they use Topological Data Analysis to acquire missing data. This study looks at something that not many other studies consider - how neural networks learn to represent drones within the latent space. Carrio et al. [42] use synthetic images to predict drones using depth maps for the application of obstacle detection of avoidance. By using a synthetic environment, they are able to create accurate ground truth depth maps, which they later use to train a NN. I-D Outro - Contribution and Outline This work is largely a continuation of Wisniewski et al. [43, 44]. It expands the method of synthetic data generation to object detectors, which can detect the position of the drone in each of the frames. Isaac-Medina et al. created a UAV detection benchmark, in which they compare common object detection architectures. This benchmark is used as a baseline to compare the results from our models trained on synthetic data. We present a method of systematically generating a synthetic dataset of drones alongside pixel-accurate ground truth segmentation masks using a form of structured domain randomization. This presents an advantage over real-world datasets which are costly to annotate and may not be as accurate. Further, synthetic datasets allow flexibility over standard approaches such as being able to add new drones to the dataset using only a 3D model. They also allow for the creation of complex scenarios that may be hard to record in real life such as forest fires, or drones flying in stormy conditions. Generally, drones should not be flown in windy conditions in real life and this might make creating a research study case hard due to safety concerns. The challenge with this approach is that the synthetic dataset may not be representative of the real-world data, and a model trained on this dataset might not translate to real-world datasets - this is the sim-to-real problem of transferring domains, as explored in section I-A2. We present and compare multiple datasets which are generated by processes of SDR and DR. The reasons for focusing on the dataset are: • The publicly available drone datasets vary in quality. Because they use human-labeled ground truth boxes, these are imperfect at times. They are also limited in that they do not have the segmentation ground truth. • Some of the previously publicly available datasets are no longer available. • New real-world datasets are costly to produce. These items make researching drone detection and comparing results between studies challenging. We address these points by releasing our synthetic dataset. The contributions of this publication are: • Multiple synthetic datasets with pixel-accurate segmentation masks, generated by the process of structured domain randomization. • A study into the ability of the neural network model trained on synthetic data to generalize to real-world drone detection scenarios. This differs from SOTA in the review because the synthetic drone detection applications mixed real-world data with their synthetic data for the training to achieve sim-to-real. They also didn’t use open-access datasets such as Drone-vs-Bird or Anti-UAV. • Study of different domain randomization styles. • Generalizable Faster R-CNN model weights for drone detection; trained purely on synthetic images, tested on multiple real-life datasets. Through our literature review, we have identified several faults within some of the research in the drone detection area that we attempted to remedy during our study: • Use of private datasets. Using private datasets makes it impossible for other researchers to reproduce results. To remedy this, we open-source our dataset, and we use publicly available datasets for testing. • Lack of a common baseline to compare the results to. This is a common problem which makes it hard to directly compare the results of one method over another. To remedy this, we use the results of the Isaac-Medina et al. benchmark to compare our results. • Repeatability of results. Many publications present the results as a single value. However, due to the randomness of neural network training, retraining yields different results. To remedy this, we present our results as a mean with a 95 per cent confidence interval. • Reproducibility of results. A lot of research in this area is hard to reproduce. To remedy this, we open-source333Datasets https://huggingface.co/datasets/mazqtpopx/cranfield-synthetic-drone-detection, and source code https://github.com/mazqtpopx/cranfield-synthetic-drone-detection our synthetic dataset, as well as the code used for training and testing of the NN models. We believe that this is one of the first attempts to study the use of structured domain randomization for the generation of synthetic drone datasets. Further, we perform a study comparing different domain randomization techniques to find out how much they influence drone detection. Although we have found attempts in the literature to use domain randomization techniques for drone detection, we were unable to find systematic studies to quantify which aspects of domain randomization improve the results of drone detection. To the best of our knowledge, previous attempts to test the sim-to-real transferability used private real-world datasets. Instead, we directly compare our results to the benchmark presented by Isaac-Medina et al. [14] which uses publicly available datasets. We present a successful sim-to-real transfer for the application of drone detection. We prove that our NN model trained on a purely synthetic dataset successfully transfers to the MAV-Vid dataset. Our model achieves a mean of 97.0% compared with the Isaac-Medina et al. model, which was trained on the MAV-Vid dataset, of 97.8%. This paper is divided into five sections. In section II we explain the methodology. This includes explaining the rendering process used to generate the datasets. We explain different styles of datasets generated using different domain randomization styles. We describe the NN training process, we explore the use of data augmentations, in particular noise and JPEG compression, and finally, we explain the testing procedure, to measure the accuracy of sim-to-real transfer. In section III we explain the results. We perform tests on camera bounds randomization, effects of data augmentations, and variation in domain randomization styles, and lastly, we compare our results to the literature. In section IV we conclude the findings. In section V we discuss further work."
https://arxiv.org/html/2411.09073v1,Code-mixed LLM: Improve Large Language Models’ Capability to Handle Code-Mixing through Reinforcement Learning from AI Feedback,"Code-mixing(CM) or code-switching(CSW) refers to the juxtaposition of linguistic units from two or more languages during the conversation or sometimes even a single utterance. Code-mixing introduces unique challenges in daily life, such as syntactic mismatches and semantic blending, that are rarely encountered in monolingual settings. Large language models (LLMs) have revolutionized the field of natural language processing (NLP) by offering unprecedented capabilities in understanding human languages. However, the effectiveness of current state-of-the-art multilingual LLMs has not yet been fully explored in the CM scenario. To fill this gap, we first benchmark the performance of multilingual LLMs on various code-mixing NLP tasks. Then we propose to improve the multilingual LLMs’ ability to understand code-mixing through reinforcement learning from human feedback (RLHF) and code-mixed machine translation tasks. Given the high-cost and time-consuming preference labeling procedure, we improve this by utilizing LLMs as annotators to perform the reinforcement learning from AI feedback (RLAIF). The experiments show the effectiveness of the proposed method.","Code-mixing or code-switching is a linguistic phenomenon where two or more languages are mixed together within a single utterance, conversation, or speech. It allows individuals to convey culturally specific concepts, forge connections or differentiate themselves from others, and reinforce their identities. Code-mixing is common in multilingual societies, with recent studies indicating that up to 20% of online content in regions like South Asia, parts of Europe, and Singapore is code-mixed. In regions with significant bilingual or multilingual populations, code-mixing is more than a cultural expression; it is a core component of everyday communication. This widespread use highlights the necessity for NLP systems to process and interpret code-mixed language accurately. In recent years, large language models (LLMs) have shown promising performance in comprehending, producing, and interacting with human language. Trained on extensive text corpora, these models can capture a broad range of linguistic patterns and subtleties. Relying on the corpus containing independent monolingual texts in different languages, these LLMs have also achieved notable success in multilingual contexts. As a result, these multilingual LLMs have enabled cross-lingual transfer, which has proven to be a valuable method to leverage resources from high-resource languages to improve downstream task performance for other languages which have similar linguistic structures. To some extent, this property of multi-lingual large language models makes them a natural choice to handle code-mixing Yet, existing multilingual LLMs are not specifically trained with objectives for managing CM scenarios or large-scale corpus only including CM texts. The effectiveness of current state-of-the-art multilingual LLMs has not yet been explored and analyzed on code-mixed text fully. Hence, assessing the capabilities of the current multilingual LLMs to deal with code-mixing is essential. The main challenges of developing multilingual LLMs optimized for CM are not limited to the imbalanced language ratio in the pre-training corpora, the data scarcity for CM in different languages, and the trade-off between model capacity and language coveragePhilippy et al. (2023). Previous work focuses on data augmentation for CM data, fine-tunes LLMs in specified code-mixed language pairs Zhang et al. (2023b). These methods do not utilize the text-generation capabilities of LLMs that are fundamentally designed for. The adoption of these prompt-based multilingual LLMs for code-mixing remains a challenge. This observation motivates us to explore - Can existing multilingual LLMs effectively understand CM? Are there generalized ways to improve the LLMs’ capability of dealing with CM? In this paper, we assess the capabilities of the current LLMs in processing code-mixed texts through various NLP tasks. Experiment results indicate that models with the additional fine-tuning step outperform these prompted-based LLMs. We also prove that these prompt-based multilingual LLMs are not good at dealing with code-mixed inputs. This benchmarking could serve as the theoretical basis for choosing small models or prompt-based LLMs when dealing with code-mixed input. Additionally, we develop a class of code-mixed LLMs through the model alignment. To the best of our knowledge, we are the first to attempt model alignment for the code-mixing scenario. To be more specific, we apply reinforcement learning from AI feedback (RLAIF) to the CM scenario. From experiment results, this method not only has the potential to greatly improve LLMs’ ability to handle code-mixed languages, but it also creates new opportunities for creating more adaptable and human-centric AI systems."
https://arxiv.org/html/2411.09065v1,Language-Model Prior Overcomes Cold-Start Items,"The growth of recommender systems (RecSys) is driven by digitization and the need for personalized content in areas such as e-commerce and video streaming. The content in these systems often changes rapidly and therefore they constantly face the ongoing cold-start problem, where new items lack interaction data and are hard to value. Existing solutions for the cold-start problem, such as content-based recommenders and hybrid methods, leverage item metadata to determine item similarities. The main challenge with these methods is their reliance on structured and informative metadata to capture detailed item similarities, which may not always be available. This paper introduces a novel approach for cold-start item recommendation that utilizes the language model (LM) to estimate item similarities, which are further integrated as a Bayesian prior with classic recommender systems. This approach is generic and able to boost the performance of various recommenders. Specifically, our experiments integrate it with both sequential and collaborative filtering-based recommender and evaluate it on two real-world datasets, demonstrating the enhanced performance of the proposed approach. Code can be found at https://github.com/awslabs/language-model-prior-4-item-cold-start.","Figure 1: Recommender is trained on historical user-item interactions, and then used to recommend new items, including those that previously appeared (i.e., dress) and newly introduced cold-start items (i.e., book). The field of recommender systems (RecSys) has witnessed tremendous growth over the last few years [Kang and McAuley, 2018c, Ma et al., 2020, Ding et al., 2021, Zhang et al., 2021, Lin et al., 2023, Li et al., 2023, Ding et al., 2023], driven by the increasing service digitization and the rising demand for personalized content across diverse platforms such as e-commerce and video streaming. Despite significant advancements, the unresolved item cold start problem remains a critical challenge. It arises with newly introduced items lacking sufficient interaction data, and thus struggling to be accurately recommended to users. For instance, Figure 1 illustrates that items such as laptop, dress, camera and glasses that have appeared in historical data are typically easier to recommend, whereas those never-before-seen items are challenging. This issue is especially severe in dynamic environments, such as news recommendation, where new items are constantly introduced, making it tough to identify similarities among cold items due to insufficient information. To address the item cold start problem, previous works primarily fall into two major categories: content-based recommendation and hybrid methods. Both approaches focus on leveraging additional item metadata to uncover item similarities. Content-based recommendation approaches such as [van den Oord et al., 2013, Volkovs et al., 2017a, b] tackle the issue by utilizing item metadata such as item category. These methods analyze the item content and recommend similar items by aligning them with user preferences, rather than relying heavily on past user-item interactions. For example, a model might recommend a new fantasy novel to a fan of the genre, despite limited interaction data on the novel itself. Another line of works focuses on hybrid methods [Wang and Blei, 2011, Zhang et al., 2016, Pan et al., 2019, Zhou et al., 2020, Liu et al., 2021, Han et al., 2022], which combine the strength of both content-based and collaborative filtering (CF) techniques. These approaches integrate user behavior with item attributes to generate recommendations, aiming to capitalize on both aspects. Hybrid methods are particularly notable for delivering precise recommendations by encompassing a wide spectrum of user preferences and item features. Overall, both strategies aim to tap into the item metadata to provide prior knowledge about item similarities. However, the main problem with these methods is twofold: (1) they require structured metadata, which is not always available, and (2) the structured metadata may be uninformative and unable to capture fine-grained item similarities. For example, consider a scenario with only five unique categories for all items, with a skewed distribution where the largest category comprises 80% of the items. Recent advancements in Language Models (LMs) enable extracting insights from unstructured textual metadata, like product descriptions or movie synopses, using pre-trained models such as BERT [Devlin et al., 2018] and Falcon [Almazrouei et al., 2023].. This approach leverages the inherent prior knowledge of LMs to uncover item similarities, even with limited available interactions or structured item metadata. Existing works [Ding et al., 2021] focus on generating text embedding from the pre-trained LMs based on the item textual metadata. However, directly using LMs embedding of items as the input of recommender may introduce a vast amount of information, not all of which is relevant to the recommendation task at hand. Also, it can significantly increase the dimensionality of data the recommender should process, and limits the flexibility of how the item meta information is integrated into the recommendation system. Therefore, in contrast to previous works, this paper investigates the possibilities and challenges of implicitly harnessing LMs to inject prior knowledge of items. Specifically, we propose a framework that integrates a Bayesian regularizer into the training process of the RecSys. The regularizer takes into account the semantic similarity among items, leveraging LM-encoded prior knowledge to learn fine-grained item embeddings in the continuous latent space. The proposed approach is generic and can be adopted in a plug-and-play fashion in any sequential [Kang and McAuley, 2018c, Ma et al., 2020] or CF-based [Rendle et al., 2012, Wang et al., 2015] RecSys. Our contributions can be summarized as follows: • We introduce a novel Bayesian framework that leverages LMs embeddings of item metadata as the prior to address the item cold start problem in RecSys. This framework leverages the rich semantic information from LMs to improve the ability of the recommender to understand and recommend new items effectively. • The proposed Bayesian prior is integrated into RecSys as a regularizer, and therefore is generic and able to enhance various recommenders by supplementing their learning objectives. • We evaluated the proposed method using both sequential (i.e., SASRec) and CF-based (i.e., BPRMF) recommenders on two real-world datasets from distinct domains. The empirical results demonstrate the enhanced performance of the proposed approach, which improves SASRec by 17.78\% regarding normalized discounted cumulative gain."
https://arxiv.org/html/2411.09062v1,Multimodal Object Detection using Depth and Image Data for Manufacturing Parts,"Manufacturing requires reliable object detection methods for precise picking and handling of diverse types of manufacturing parts and components. Traditional object detection methods utilize either only 2D images from cameras or 3D data from lidars or similar 3D sensors. However, each of these sensors have weaknesses and limitations. Cameras do not have depth perception and 3D sensors typically do not carry color information. These weaknesses can undermine the reliability and robustness of industrial manufacturing systems. To address these challenges, this work proposes a multi-sensor system combining an red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are calibrated for precise alignment of the multimodal data captured from the two hardware devices. A novel multimodal object detection method is developed to process both RGB and depth data. This object detector is based on the Faster R-CNN baseline that was originally designed to process only camera images. The results show that the multimodal model significantly outperforms the depth-only and RGB-only baselines on established object detection metrics. More specifically, the multimodal model improves mAP by 13% and raises Mean Precision by 11.8% in comparison to the RGB-only baseline. Compared to the depth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%. Hence, this method facilitates more reliable and robust object detection in service to smart manufacturing applications.","Рис. 1: Overview of the RGBD-Man multimodal object detection framework. Employing Artificial Intelligence (AI) for automation of manufacturing has resulted in increased efficiency, precision, and flexibility and created a pardigm shift in the design of manufacturing systems. AI has been successfully applied to a vast array of manufacturing tasks in the industry [1, 2]. While AI-based methods have improved the manufacturing process, there are still challenges in ensuring that the AI-based black box systems continue to be reliable and robust. The most fundamental layer underlying all smart manufacturing systems is object detection, which allows the system to identify the type and position of the objects that it needs to handle. Object detection is a established computer vision problem, which involves identifying and categorizing specific objects of interest within a larger image by placing a bounding box around each detected object [3]. An effective automation system also requires proper sensor design to provide adequate coverage over the environment and allow the system to properly observe the scene and the objects [4]. The past decades have seen great advancement in sensor hardware. Camera resolutions have increased and they have become more affordable at the same time. Similarly, consumer applications have facilitated mass manufacturing of 3D sensors like lidars and stereo cameras, which are great sensors for smart manufacturing. Despite these improvements, sensors have inherent limitations rooted in their physics. For example, a single image captured by camera does not carry depth information. 3D sensors capture point cloud data which addresses this issue, but these sensors are typically low resolution and do not provide color information. An effective and reliable automation system requires selecting the right sensors and an object detection system that can effectively ingest the data provided by these sensors. Prior work has shown the limitations of object detection systems that rely solely on cameras [5, 6, 7, 8]. Image distortions like blur and noise can significantly lower the detection accuracy [5]. Although cameras provide color information, there are environments where there is low contrast between objects and the background, and as a result detection accuracy may suffer [9] and cameras may not be enough for handling these environments. Moreover, camera-based object detection systems are sensitive to illumination and can be fragile if there are changes to the lighting conditions. It has also been shown that illumination can negatively affect [6] camera-based object detection systems, because they struggle to generalize to operate under lighting conditions different from what they have experienced during training. Similarly, the performance of these systems diminishes in scenarios where objects vary significantly in size or when they blend indistinguishable with the background in camera’s view [7]. While 3D sensors are less sensitive to lighting, they pose their own set of challenges when used in industrial environments [10]. As an example, object detection systems using 3D sensors often struggle when the scene contains densely arranged objects [11]. In addition, presence of objects with similar shapes and sizes, or objects with repetitive patterns cause difficulties for systems relying on point clouds for object detection [12]. Another challenge with using point cloud data is the extra complexity of modeling 3D information and the slower speed of processing 3D information. This is a significant obstacle, particularly for manufacturing environments which require real-time object detection and therefore cannot afford too much complexity and computational overhead [13, 14]. These conditions emphasize the need for advanced detection techniques that can take advantage of 3D sensors for improved performance while keeping the system simple and computationally efficient. Since different sensors can be complementary and cover each other’s blind spots and weaknesses, it makes sense to create object detectors which can leverage the strengths of multiple sensors to improve accuracy and dependability rather than relying on a single sensor. Multimodal object detection [15, 16], which utilizes data from multiple sensor modalities has the potential to address the above-mentioned limitations of single-sensor systems. Integrating information from multiple sensors can be achieved via the sensor fusion process [17], which can produce more accurate and more reliable data for the object detection model. Different types of sensor fusion have been studied [18] in the past. Early fusion methods merge sensor data at the input stage. Intermediate fusion methods combine features derived from different sensors at an intermediate layer of a model. Lastly, late fusion approaches aggregate decisions proposed from different sensors at the final stage in the model. All types of sensor fusion can contribute to overcoming the limitations of single-sensor systems. Prior work has explored multimodal object detection. One notable approach [19] added a depth branch to the Faster-RCNN architecture to process depth in parallel with the RGB data. The depth branch created feature maps from depth inputs and these feature maps were concatenated with feature maps generated from RGB images. The authors showed that using depth allowed the model to succeed in some scenarios where color information alone could not distinguish objects from their backgrounds [19]. Similarly, Zhu et al. [20] added a depth processing branch to the Faster R-CNN framework which enabled their model to handle both RGB and depth images for enhanced object detection. Their method employed two distinct CNNs to extract features from RGB and depth images independently. They also used depth information to delineate object edges more clearly and distinguish them from their backgrounds. Following feature extraction, the features were aligned and merged using a feature fusion layer, which implemented sum fusion, max fusion, and concatenation fusion strategies. Garbouge et al. [21] presented an approach that integrated RGB and depth data at an early stage by stacking them into a four-channel input for a CNN inspired by the AlexNet architecture for a classification task and showed that employing depth information improves object classification metrics. However, none of the existing methods have explored efficient four-channel RGB+D inputs in the context of object detection tasks. The processing of RGB and depth data through a single shared backbone offers several advantages. Firstly, when RGB and depth data are stacked at the input level, the model is able to efficiently extract features that merge information from both modalities, for example color and texture from RGB alongside spatial and structural details from depth. This integrated approach enables the network to learn more meaningful features early in the feature extraction process. By processing both data types simultaneously, the network can better generalize to unseen data as it learns to recognize and interpret patterns across both modalities. Additionally, employing a single backbone for processing reduces the computational overhead significantly. This computational efficiency comes from managing only one set of weights during back-propagation and inference. This approach creates a leaner model which is faster and less costly to run in a manufacturing plant. Lastly, the lower architectural complexity and lack of complex fusion mechanisms simplifies the development and tuning process for training and deploying the final model in the manufacturing environment. This work introduces a novel method that employs early sensor fusion, where data from RGB images and 3D point clouds are integrated at the data level before being used by the detection algorithm. This method utilizes the combined strengths of both modalities for a more comprehensive representation of the scene that enhances object detection capabilities. Early fusion facilitates the extraction of comprehensive features that embody both the visual and spatial attributes of objects, which addresses the challenges posed by occlusions, variable sizes, and complex background information more effectively than late fusion or single-sensor methods. Hence, this work presents a novel model tailored for processing RGB-D images for object detection tasks in manufacturing. The rest of the paper is organized as follows. Section 2 discusses the methodology of the multimodal object detection framework. Section 3 outlines experiments which compare this approach to both RBG-only and Depth-only object detection of a NIST manufacturing task board. Section 4 discusses the experiment results, and Section 5 discusses the conclusions and suggestions for future work."
https://arxiv.org/html/2411.08992v1,IDCIA: Immunocytochemistry Dataset for Cellular Image Analysis,"We present a new annotated microscopic cellular image dataset to improve the effectiveness of machine learning methods for cellular image analysis. Cell counting is an important step in cell analysis. Typically, domain experts manually count cells in a microscopic image. Automated cell counting can potentially eliminate this tedious, time-consuming process. However, a good, labeled dataset is required for training an accurate machine learning model. Our dataset includes microscopic images of cells, and for each image, the cell count and the location of individual cells. The data were collected as part of an ongoing study investigating the potential of electrical stimulation to modulate stem cell differentiation and possible applications for neural repair. Compared to existing publicly available datasets, our dataset has more images of cells stained with more variety of antibodies (protein components of immune responses against invaders) typically used for cell analysis. The experimental results on this dataset indicate that none of the five existing models under this study are able to achieve sufficiently accurate count to replace the manual methods. The dataset is available at https://figshare.com/articles/dataset/Dataset/21970604.","Cell biology is a sub-discipline of biology where the structure and physiological functioning, and interaction of cells are studied (Bradshaw and Stahl, 2016). Cells are examined under a microscope and imaged at a high resolution. In immunocytochemistry (ICC), different antibodies are used to visualize the presence of particular proteins to identify specific cell types in a given sample. Cell analysis involves a wide range of tasks, such as counting cells and measuring and evaluating cell state (e.g., shape, motility), cell health, and cell growth. Cell biology is closely intertwined with other fields, such as neuroscience, genetics, and molecular biology. One fascinating application area of cell biology is research for the potential diagnosis and treatment of diseases. The research in this area is full of potential and possibilities that could improve quality of life. Deep Neural Networks (DNNs) have been applied in the analysis of microscopic cell images, including cell counting (Paul Cohen et al., 2017; Xie et al., 2018), segmentation (Al-Kofahi et al., 2018; Ghaznavi et al., 2022; Hiramatsu et al., 2018; Morelli et al., 2021), and detection (Wang et al., 2022; Jiang et al., 2020; Fujita and Han, 2020). Given an input image, cell counting provides the number of cells in the image. In contrast, cell segmentation finds the contours of individual cells, separating them from each other and the background. On the other hand, cell detection localizes a cell by drawing the smallest rectangle around each cell in the input image. The advantages of DNNs over traditional machine learning methods are that DNNs automatically extract important properties (features) of the object of interest and use them to perform the intended task. However, the major drawback of DNNs is that it requires a large high-quality labeled dataset for accurate predictions. Existing DNN methods for cell counting can be broadly categorized into two groups: detection-based and regression-based categories. The detection-based category undertakes the counting task by first detecting individual cells (contours, bounding boxes, or centroids of the cells) in a given image and counting the detected cells to obtain the final cell count (Morelli et al., 2021; Khan et al., 2016). These methods hinge on the availability of the annotated ground truth of the bounding box or a centroid of a cell. The methods are also dependent on the characteristics of the microscopic input images. In particular, detection-based methods fail to offer good performance when there is a high occlusion in the images. The regression-based category (Paul Cohen et al., 2017; Xie et al., 2018) predicts the cell count without detecting individual cells. Some of these methods use only the ground truth cell count for each training image for training. Other methods predict a corresponding density map for a given image and obtain the final count from the predicted density map. Our team examines cellular images taken after electrical stimulation experiments on stem cells for cell differentiation. Cell differentiation is the process in which an unspecialized cell develops and matures to become a specialized cell. Electrical stimulation of stem cells is potentially useful for stem cell therapy in patients with nerve injuries. Cell counting is an important step toward determining an appropriate amount of electrical voltage and stimulation duration to be applied. To perform the electrical stimulation, cells are placed on the surface of a scaffold, which are structures providing support for cells to grow within an interdigitated electrode region. Then the voltage is applied to the electrode pads of the scaffold, which are structures providing support for cells to grow. During an electrical stimulation experiment, cells exhibit changes in size, shape, and energy requirement (Das et al., 2017; Uz et al., 2020, 2019). Following electrical stimulation, immunocytochemistry (ICC) is performed to measure the effect of the stimulation on the cells. Different antibodies are used during the ICC process to identify the potential cell types these cells could be differentiating into. A fluorescent microscope is used to examine and image the cells. Currently, cell counting and cell analysis are done manually. The challenges for developing accurate automated cell counting are a wide range of cells in an image given different antibodies, different cell sizes, low contrast, and cell occlusion. The main contributions of this work are as follows. (1) An annotated dataset for automated cell counting along with the domain knowledge to use the dataset. The annotation includes the cell locations as well as the count of cells per image. To the best of our knowledge, there is no annotated fluorescent microscopic cell image dataset that covers as many staining methods as this dataset. (2) Performance comparison of the state-of-the-art regression-based and density map estimation DNN methods. The results can be used as baseline results for future improvement. The source code and the trained models are available publicly at https://github.com/ISU-NRT-D4/cell-analysis. The rest of the paper is organized as follows. In Section 2, we provide a summary of existing datasets related to cell counting. Section 3 presents our data collection and annotation process and the details of our new dataset. Section 4 includes applicable scenarios to utilize the dataset. Section 5 details the baseline experimental results on the dataset with five DNN models. Finally, we provide a conclusion and description of the future work in Section 6."
https://arxiv.org/html/2411.08979v1,"CoCoP: Enhancing Text Classification with LLM 
through Code Completion Prompt","Text classification is a fundamental task in natural language processing (NLP), and large language models (LLMs) have demonstrated their capability to perform this task across various domains. However, the performance of LLMs heavily depends on the quality of their input prompts. Recent studies have also shown that LLMs exhibit remarkable results in code-related tasks. To leverage the capabilities of LLMs in text classification, we propose the Code Completion Prompt (CoCoP) method, which transforms the text classification problem into a code completion task. CoCoP significantly improves text classification performance across diverse datasets by utilizing LLMs’ code-completion capability. For instance, CoCoP enhances the accuracy of the SST2 dataset by more than 20%. Moreover, when CoCoP integrated with LLMs specifically designed for code-related tasks (code models), such as CodeLLaMA, this method demonstrates better or comparable performance to few-shot learning techniques while using only one-tenth of the model size. The source code of our proposed method will be available to the public upon the acceptance of the paper.","Large Language Models (LLMs) show commendable performance in both classification and generation tasks. However, to enhance their performance, fine-tuning task-specific data is imperative. The fine-tuning process requires collecting substantial, high-quality data pertinent to the domain task. Also, fine-tuning is expensive and may not be suitable for all situations (Kaddour et al., 2023). On the other hand, in-context learning techniques, such as few-shot learning, offer more flexibility (Brown et al., 2020). Unlike fine-tuning, these techniques do not require extensive domain-specific data and work only with a few examples (Brown et al., 2020). It makes them accessible for anyone to use during inference. It is important to emphasize that the effectiveness of these methods relies on the context and examples given as input. LLMs are trained on massive datasets that include diverse input styles. LIMA (Zhou et al., 2024) claims that if a model can learn the style of interaction with the user, it can reach superior performance in alignment and effectively communicate its knowledge to the user. This concept works in the reverse direction as well. When users interact with LLMs using prompt structures familiar to LLMs, the model demonstrates better performance. The efficacy of a prompt format for the model is contingent on its training data. Code-related datasets exist on the internet and are among the most critical resources for the pre-training phase of LLMs. For instance, LLaMA (Touvron et al., 2023a) is pre-trained on a substantial 328 GB dataset sourced from GitHub, constituting 4.5% of the overall model training data. Additionally, Google has mentioned that code-related data are one of the most important parts of their pre-training data for the Gemini model (Team et al., 2023). Choosing code-related datasets highlights the importance of teaching LLMs about coding style and code creation. Moreover, LLMs show good performance in code-related benchmarks. For instance, both LLaMA and LLaMA2 (Touvron et al., 2023b) have reported commendable results in code-related benchmarks. The proficiency of LLMs in code-related tasks can be leveraged for tasks beyond coding. To enhance LLMs performance in classification tasks, we introduce the Code Completion Prompt (CoCoP) method in this paper. Our suggested method uses LLMs’ capabilities in code-related tasks, particularly in code completion. Additionally, we employ the in-context learning in this method. CoCoP creates incomplete-code which consists of a few demonstrations in code format. Next, the LLM completes the code to determine the proper label for a user query. Thus, CoCoP is founded on the strengths of LLMs’ capabilities in code completion and in-context learning. The effectiveness of our method is evaluated across various classification datasets using LLaMA2 and CodeLLaMA models (Roziere et al., 2023), revealing superior performance compared to the few-shot learning method in classification tasks. Also, our experiments indicate that LLMs designed for code-related tasks (code models) demonstrate superior performance with this method compared to other LLMs. The source code of our proposed method will be available to the public upon the acceptance of the paper. Our research contributions include: • Introduced a novel text classification method using LLMs leveraging their code completion capability. • Outperformed the traditional few-shot learning method in text classification with LLMs. • Achieved comparable or superior performance using smaller code models like 7B and 13B model size compared to larger models such as the 70B model size across various text classification benchmarks through our approach. The rest of this paper is organized as follows. Section 2 describes our proposed method. Next, Section 3 shows results of CoCoP performance and impact of each part of it. Section 4 describes related works. Limitations and future work are discussed in Section 6. Finally, Section 5 concludes the paper."
https://arxiv.org/html/2411.08975v1,Fluoroformer: Scaling multiple instance learning to multiplexed images via attention-based channel fusion,"Though multiple instance learning (MIL) has been a foundational strategy in computational pathology for processing whole slide images (WSIs), current approaches are designed for traditional hematoxylin and eosin (H&E) slides rather than emerging multiplexed technologies. Here, we present an MIL strategy, the Fluoroformer module, that is specifically tailored to multiplexed WSIs by leveraging scaled dot-product attention (SDPA) to interpretably fuse information across disparate channels. On a cohort of 434 non-small cell lung cancer (NSCLC) samples, we show that the Fluoroformer both obtains strong prognostic performance and recapitulates immuno-oncological hallmarks of NSCLC. Our technique thereby provides a path for adapting state-of-the-art AI techniques to emerging spatial biology assays.","Multiple instance learning (MIL) has emerged as the de facto standard approach in computational pathology for generating predictions from whole slide images (WSIs) (Ilse et al., 2018; Maron and Lozano-Pérez, 1997; Carbonneau et al., 2018; Lu et al., 2021). The typical MIL pipeline consists of 1) dividing the WSI into smaller image patches, 2) extracting lower dimensional embeddings for each patch from a pre-trained neural network, 3) pooling embeddings across patches to create a slide-level summary vector, and 4) generating slide-level predictions for the particular task at hand. Compared to traditional strategies such as training patch-level predictors that rely exclusively on clinician-annotated regions of interest (ROIs), MIL enables weakly-supervised training on entire WSIs, thereby offering enhanced scalability, reduced sampling bias, and potentially superior performance (Zhou, 2018). Figure 1: Overview of the Fluoroformer strategy for multiplexed imaging. Mathematical symbols are defined in text. Thus far in computational pathology, MIL pipelines have largely been confined to traditional hematoxylin & eosin (H&E)-stained WSIs (Wilson et al., 2021; Ghahremani et al., 2022). While H&E staining can provide detailed morphological information, it fails to explicitly capture important proteins and other complex biomarkers that indicate cell phenotype and state (Lee et al., 2020; Peng et al., 2023; Muñoz-Castro et al., 2022). In contrast, emergent techniques in spatial biology such as multiplex immunofluorescence (mIF) enable the imaging of many biomarkers simultaneously in tissue samples while preserving spatial context (Figure 1). These techniques result in rich, multi-channel (\sim5-50) images that have advanced our understanding of diseases ranging from neurodegenerative disorders (Muñoz-Castro et al., 2022) to cancer (Lee et al., 2020; Peng et al., 2023). Conversely, mIF images are often analyzed using hand-engineered features, such as the counts of discrete biomarkers within clinician-defined ROIs (Wilson et al., 2021). More recent efforts have pointed to the potential of using deep learning to improve performance on downstream tasks, but these efforts have also focused on ROIs rather than expanding to WSIs (Hoebel et al., 2024; Sorin et al., 2023; Wu et al., 2022). There is therefore a pressing need to optimize MIL methods for mIF in order to yield the benefits of both weakly-supervised training and the rich information provided by spatial assays. Doing so, however, presents several challenges. The disparate channels must be somehow combined, and, moreover, ideally would be done so flexibly given that the number of channels can vary between mIF protocols. Here, we present the Fluoroformer, a Transformer-like neural network module designed to interpretably scale MIL to multiplex images. Leveraging scaled dot-product attention (SDPA) (Vaswani, 2017), it fuses the information from disparate multiplexed channels into a single summary vector for each patch, enabling the subsequent pooling of the patch embeddings via standard attention-based MIL (ABMIL) mechanisms. Importantly, the Fluoroformer produces attention matrices for each patch that may offer insights into cell-cell interactions and biological structures. Using a cohort of 434 non-small cell lung cancer (NSCLC) samples and their corresponding mIF WSIs, we find that the Fluoroformer demonstrates strong performance in predicting patient prognosis. Analysis of the channel-wise attention matrices offers insights into immune-tumor interactions that potentially associate with prognosis. Our approach therefore bridges spatial biology techniques with state-of-the-art artificial intelligence approaches to maximize the potential of this emerging field. Figure 2: Heatmaps of the most highly attended markers in each patch of selected mIF images for the Fluoroformer model with a ResNet50 embedder. Patterns are observed such as DAPI being a highly attended marker for alveolar tissue (A, B) and CD8-attended regions appearing at tumor margins and sporadically within the tumor (A, C, D)."
https://arxiv.org/html/2411.08954v2,Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply Better Samples,"Although diffusion models can generate remarkably high-quality samples, they are intrinsically bottlenecked by their expensive iterative sampling procedure. Consistency models (CMs) have recently emerged as a promising diffusion model distillation method, reducing the cost of sampling by generating high-fidelity samples in just a few iterations. Consistency model distillation aims to solve the probability flow ordinary differential equation (ODE) defined by an existing diffusion model. CMs are not directly trained to minimize error against an ODE solver, rather they use a more computationally tractable objective. As a way to study how effectively CMs solve the probability flow ODE, and the effect that any induced error has on the quality of generated samples, we introduce Direct CMs, which directly minimize this error. Intriguingly, we find that Direct CMs reduce the ODE solving error compared to CMs but also result in significantly worse sample quality, calling into question why exactly CMs work well in the first place. Full code is available at: https://github.com/layer6ai-labs/direct-cms.","In recent years, diffusion models (DMs) [44, 14] have become the de facto standard generative models [22] for many perceptual data modalities such as images [34, 40, 36, 5], video [15, 2, 53, 52], and audio [20, 39, 17]. Despite their successes, an inherent drawback of diffusion models stems from their iterative sampling procedure, whereby hundreds or thousands of function calls to the diffusion model are typically required to generate high-quality samples, limiting their practicality in low-latency settings. A prominent approach for improving the sampling efficiency of diffusion models is to subsequently distill them into models capable of few-step generation [24, 41, 31, 27, 1, 9, 56, 55, 42]. Among the works in this vein, consistency models (CMs) [49] have garnered attention due to their simple premise as well as their ability to successfully generate samples with only a few steps. CMs leverage the ordinary differential equation (ODE) formulation of diffusion models, called the probability flow (PF) ODE, that defines a deterministic mapping between noise and data [48]. The goal of consistency model distillation is to train a model (the student) to solve the PF ODE of an existing diffusion model (the teacher) from all points along any ODE trajectory in a single step. The loss proposed by Song et al. [49] to train CMs does not directly minimize the error against an ODE solver; the solver is mimicked only at optimality and under the assumptions of arbitrarily flexible networks and perfect optimization. We thus hypothesize that the error against the ODE solver can be further driven down by directly solving the PF ODE at each step using strong supervision from the teacher, which we call a direct consistency model (Direct CM). Although Direct CMs are more expensive to train than standard CMs, they provide a relevant tool to probe how well CMs solve the PF ODE and how deviations from an ODE solver affect sample quality. We perform controlled experiments to compare CMs and Direct CMs using a state-of-the-art and large-scale diffusion model from the Stable Diffusion family [36], SDXL [30], as the teacher model for distillation. We show that Direct CMs perform better at solving the PF ODE but, surprisingly, that they translate to noticeably worse sample quality. This unexpected result challenges the conception that better ODE solving necessarily implies better sample quality, a notion that is implicitly assumed by CMs and its variations alike [47, 7, 10, 57, 21]. Our findings serve as a counterexample to this statement, thus calling into question the community’s understanding of ODE-based diffusion model distillation and its implications on sample quality. Since CMs achieve larger ODE solving error, we surmise that other confounding factors contribute to their improved sample quality. We thus call for additional investigation to clarify this seemingly paradoxical behaviour of ODE-based diffusion model distillation."
https://arxiv.org/html/2411.08933v2,"Confidence-aware Denoised Fine-tuning of 
Off-the-shelf Models for Certified Robustness","The remarkable advances in deep learning have led to the emergence of many off-the-shelf classifiers, e.g., large pre-trained models. However, since they are typically trained on clean data, they remain vulnerable to adversarial attacks. Despite this vulnerability, their superior performance and transferability make off-the-shelf classifiers still valuable in practice, demanding further work to provide adversarial robustness for them in a post-hoc manner. A recently proposed method, denoised smoothing, leverages a denoiser model in front of the classifier to obtain provable robustness without additional training. However, the denoiser often creates hallucination, i.e., images that have lost the semantics of their originally assigned class, leading to a drop in robustness. Furthermore, its noise-and-denoise procedure introduces a significant distribution shift from the original distribution, causing the denoised smoothing framework to achieve sub-optimal robustness. In this paper, we introduce Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), a novel fine-tuning scheme to enhance the certified robustness of off-the-shelf classifiers. FT-CADIS is inspired by the observation that the confidence of off-the-shelf classifiers can effectively identify hallucinated images during denoised smoothing. Based on this, we develop a confidence-aware training objective to handle such hallucinated images and improve the stability of fine-tuning from denoised images. In this way, the classifier can be fine-tuned using only images that are beneficial for adversarial robustness. We also find that such a fine-tuning can be done by merely updating a small fraction (i.e., 1%) of parameters of the classifier. Extensive experiments demonstrate that FT-CADIS has established the state-of-the-art certified robustness among denoised smoothing methods across all \ell_{2}-adversary radius in a variety of benchmarks, such as CIFAR-10 and ImageNet.","Despite the recent advancements in modern deep neural networks in various computer vision tasks (Radford et al., 2021; Rombach et al., 2022; Kirillov et al., 2023), they still suffer from the presence of adversarial examples (Szegedy et al., 2013) i.e., a non-recognizable perturbation (for humans) of an image often fools the image classifiers to flip the output class (Goodfellow et al., 2014). Such adversarial examples can be artificially crafted with malicious intent, i.e., adversarial attacks, which pose a significant threat to the practical deployment of deep neural networks. To alleviate this issue, various approaches have been proposed to develop robust neural networks, such as adversarial training (Madry et al., 2018; Wang et al., 2019) and certified defenses (Wong & Kolter, 2018; Cohen et al., 2019; Li et al., 2023). Among these efforts, randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019) has gained much attention as a framework to build robust classifiers. This is due to its superior provable guarantee of the non-existence of adversarial examples, i.e., certified robustness (Wong & Kolter, 2018; Xiao et al., 2018), under any perturbations confined in a \ell_{2}-norm. Specifically, it builds a smoothed classifier through taking a majority vote from a base classifier, e.g., a neural network, under Gaussian perturbations of the given input image. However, it has been practically challenging to scale the model due to a critical drawback: the base classifier should be specifically trained on noise-augmented data (Lecuyer et al., 2019; Cohen et al., 2019). Recently, Lee (2021); Carlini et al. (2023) have introduced denoised smoothing which utilizes pre-trained off-the-shelf classifiers within the randomized smoothing framework. Rather than directly predicting the label of a noise-augmented image, it first feeds the perturbed image into a denoiser, e.g., a diffusion model, and then obtains the predicted label of the denoised image using off-the-shelf pre-trained classifiers that have been trained on clean images. Intriguingly, denoised smoothing with recently developed diffusion models and pre-trained classifiers, e.g., guided diffusion (Dhariwal & Nichol, 2021) and BEiT (Bao et al., 2022), shows its superior scalability with comparable certified robustness in \ell_{2}-adversary to the current state-of-the-art methods (Horváth et al., 2022b; Jeong et al., 2023). Figure 1: Overview of FT-CADIS framework. (1) Confidence-aware denoised image selection: for a given clean image, we create denoised images and find non-hallucinated images. (2) Fine-tuning with confidence-aware denoised image selection: we propose fine-tuning objectives to improve both generalizability and robustness of the smoothed classifier based on selected non-hallucinated images. On the other hand, denoised smoothing also exhibits clear limitations. Firstly, denoised images do not follow the standard pre-training distribution of the classifiers, which results in a limited robustness of the denoised smoothing framework. Secondly, fine-tuning the pre-trained classifiers with the denoised images also yields sub-optimal classifiers due to the hallucinated images (Carlini et al., 2023), i.e., the diffusion denoiser tends to generate image semantics from an incorrect class rather than the originally assigned class (see Figure 4). Consequently, denoised smoothing with such classifiers leads to a drop of the certified accuracy, especially in the large \ell_{2}-radius regime, i.e., high Gaussian variance (see Table 3). Contribution. In this paper, we aim to address the aforementioned issues of denoised smoothing by designing a fine-tuning objective for off-the-shelf classifiers that distinguishes between hallucinated images, i.e., images that have lost the original semantics after denoising, and non-hallucinated images, i.e., images that maintain the original semantics after denoising. To this end, we propose to use the “likelihood of denoised images”, i.e., confidence, of the off-the-shelf classifier with respect to the originally assigned class as a proxy for determining whether an image is hallucinated and then fine-tune the classifier with non-hallucinated images only. Consequently, we have developed a confidence-aware training objective based on the likelihood of denoised images to effectively discriminate hallucinated images (see Figure 1). Specifically, we propose a scalable and practical framework for fine-tuning off-the-shelf classifiers, coined Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), which improves certified robustness under denoised smoothing. In order to achieve this, two new losses are defined: the Confidence-aware selective cross-entropy loss and the Confidence-aware masked adversarial loss. Two losses are selectively applied only to non-hallucinated images, thereby ensuring that the overall training process avoids over-optimizing hallucinated samples, i.e., samples that are harmful for generalization, while maximizing the robustness of smoothed classifiers. Our particular loss design is motivated by Jeong et al. (2023), who were the first to investigate training objectives for randomized smoothing depending on sample-wise confidence information. We demonstrate that our novel definition of confidence in randomized smoothing, specifically through the ratio of non-hallucinated images from a denoiser, can dramatically stabilize the confidence-aware training, overcoming its previous limitation of severe accuracy degradation (e.g., see Table 3). In our experiments, we have validated the effectiveness of our proposed method on standard benchmarks for certified \ell_{2}-robustness, i.e., CIFAR-10 (Krizhevsky, 2009) and ImageNet (Russakovsky et al., 2015). Our results show that the proposed method significantly outperforms existing state-of-the-art denoised smoothing methods in certified robustness across all \ell_{2}-norm setups, while updating only 1% of the parameters of off-the-shelf classifiers on ImageNet. In particular, FT-CADIS significantly improves the certified robustness in the high Gaussian variance regime, i.e., high certified radius. For instance, FT-CADIS outperforms the best performing baseline, i.e., diffusion denoised (Carlini et al., 2023), by 29.5% \rightarrow 39.4% at \varepsilon = 2.0 for ImageNet experiments."
https://arxiv.org/html/2411.08932v1,Pygen: A Collaborative Human-AI Approach to Python Package Creation,"The principles of automation and innovation serve as foundational elements for advancement in contemporary science and technology. Here, we introduce Pygen, an automation platform designed to empower researchers, technologists, and hobbyists to bring abstract ideas to life as core, usable software tools written in Python. Pygen leverages the immense power of autoregressive large language models to augment human creativity during the ideation, iteration, and innovation process. By combining state-of-the-art language models with open-source code generation technologies, Pygen has significantly reduced the manual overhead of tool development, thereby significantly enhancing creativity and productivity. From a user prompt, Pygen automatically generates Python packages for a complete workflow from concept to package generation and documentation. Practical examples of libraries such as AutoML, AutoVision, AutoSpeech, and Quantum Error Correction are demonstrated. The findings of our work show that Pygen considerably enhances the researcher’s productivity by enabling the creation of resilient, modular, and well-documented packages for various specialized purposes. We employ a prompt enhancement approach to distill the user’s package description into increasingly specific and actionable. While being inherently an open-ended task, we have evaluated the generated packages and the documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with detailed results in the results section. Furthermore, we documented our results, analyzed the limitations, and suggested strategies to alleviate them. Pygen is our vision of ethical automation, a framework that promotes inclusivity, accessibility, and collaborative development. This project marks the beginning of a large-scale effort towards creating tools where intelligent agents collaborate with humans to improve scientific and technological development substantially.Our code and generated examples are open-sourced at [https://github.com/GitsSaikat/Pygen]","Curiosity, innovation, and relentless pursuit have always characterized scientific progress. Today, we stand on the threshold of a promising new chapter in which every step, though minor and significant, acts as a booster for scientific growth. Pygen is a system that represents a significant stride toward that vision. It aims to automate humdrum and recurrent activities to free time for researchers, scientists, and enthusiasts to practice what matters: creativity and breakthrough innovation effectively. Automation becomes one of the most vital tools for social benefit when used thoughtfully and diligently[1]. Automation simplifies tasks, opens vistas for reimagining processes, and makes those processes even better across disciplines. Treated right and with responsibility, it might be the beacon of progress for everyone[2]. Pygen do just that: make technology accessible, amply productive, and take people to new heights in their scientific journeys as technology empowers individuals to achieve their milestones along with the progress of civilization[3]. The path to innovation involves solving unique and complex problems. Not all challenges can be addressed with existing technology, but human creativity shines through in its ability to construct new tools as needed, thereby expanding the boundaries of what is possible[18][17]. Pygen embodies this spirit of innovation by transforming abstract ideas into practical solutions that make a tangible impact. When the scientific community tackles a problem and finds a solution, they often discover key components and experimental techniques that are valuable for future scientists and technologists[4]. Creating Python packages originated from the desire to equip the community with essential tools that streamline experimental processes and advance scientific work. This approach to building tools is a form of responsible automation, where the human element remains integral, allowing for flexibility, creativity, and adaptability—qualities that purely automated tool designs often lack. Our vision for Pygen is to create a dynamic system that helps generate effective software tools and nurtures and inspires new ideas. By focusing on responsible automation, we aim to empower researchers and technologists to explore new possibilities, build upon existing knowledge, and contribute to advancing science and technology. Traditional software automation[5][6][7] approaches primarily focus on creating user-level abstractions that integrate a user’s perspective to improve the software. However, our approach emphasizes the importance of designing superior tools that lead to the creation of better-end products. We made a critical observation that humans, when faced with challenges, often develop new tools if existing ones are inadequate[20][19]. Likewise, a language-model-based agentic system tasked with complex work must not only learn to use available tools but also create new ones to solve problems effectively[9][10][11]. This insight led us to develop an automated Python package generation system that starts with simple user prompts and evolves from there. By integrating this approach into an agentic framework, Pygen aims to enhance the adaptability and performance of such systems, enabling them to tackle increasingly sophisticated tasks and deliver impactful results. Foundational models[12][13][14][15] have typically been used to generate code for direct use, but their potential to autonomously build tools and design comprehensive software solutions remains largely untapped. While these models can assist in writing scripts or automating simple tasks, they have yet to augment human capability in complex, multi-faceted project settings effectively. With Pygen, we aim to change this paradigm. We are empowering these foundational models to generate ideas for software tools and create Python packages that can be effectively used to solve real-world challenges. By producing thorough documentation alongside the generated tools, Pygen extends the capabilities of these models beyond simple automation, turning them into meaningful partners in creative and technical endeavors. The concept of the AI scientist[16] inspires our work, which is an end-to-end framework capable of originating novel ideas, developing experiments to explore those ideas, and ultimately producing scientific literature to share the resulting insights. Pygen builds on this vision by enhancing user queries, generating Python packages, and providing comprehensive documentation that allows others to easily understand, utilize, and build upon the generated tools. This process empowers users by transforming abstract ideas into functional, well-documented tools, simplifying the journey from initial concept to practical application. Pygens do more than merely automate; they act as extensions of human creativity. It bridges the gap between high-level conceptual thinking and practical, hands-on implementation, allowing users to bring their ideas to life with minimal friction. Pygen allows users to specify the type of package they need for their tasks, along with the desired features and functionalities. Based on the user’s description, the system refines these ideas and creates optimized implementation strategies. Using these refined strategies, Pygen designs a Python package by leveraging open-source models available on platforms like Google AI Studio and the GroqCloud. Once the package is generated, the Pygen creates comprehensive documentation to accompany it. Users can download the package and its documentation as a zip file, ensuring that all necessary information is in one place. The package is automatically set up if executed in the user’s local environment, allowing for a smooth transition from development to execution. Users can further enhance these packages to meet their specific needs and, if desired, deploy them within the Python ecosystem. A key principle behind Pygen is our emphasis on open-source accessibility. Using open-source models, we ensure that users can access the system without being hindered by financial barriers or paywalls. This approach promotes open access and open-source scientific discovery, allowing individuals from all backgrounds to contribute to and benefit from innovation. Thanks to this open-source pipeline, users can utilize models made available through platforms such as GroqCloud and Google AI Studio to generate and document Python packages completely free of charge, encouraging experimentation and continuous improvement of Pygen. We are committed to open-sourcing Pygen itself, inviting contributions from the broader community to enhance its capabilities further, making it a truly collaborative and evolving project. Our contributions are summarized as follows: 1. We have introduced a Python package development system powered by open-source frontier models. This pipeline transforms user descriptions into refined ideas, leading to the generation of Python packages accompanied by thorough documentation. Users can download the generated package and documentation seamlessly, enabling them to start working immediately. 2. Pygen can be deployed as a user-friendly application, allowing users to access it directly by simply setting their API key. This streamlined access reduces barriers to entry and enables a broader audience to leverage the system’s capabilities. 3. We have outlined several future research directions to improve responsible system automation. These include refining the strategies for improving Pygen, integrating a package reviewer to ensure robustness and reliability, and exploring the potential of agentic frameworks that can autonomously create and refine the tools they use. Figure 1: Pygen’s Workflow: This diagram describes Pygen’s workflow to generate a Python package given the user’s request. First, it starts with generating the plan, which includes formulating some package plan according to the user’s needs. It then involves users’ prompt refinement, based on validating, generating, and evaluating a package. The final step is documentation generation and formatting. At the end, documents are created, refined, and converted to Markdown to enable formatting validation for the final output package."
https://arxiv.org/html/2411.08925v1,Retrieval of sun-induced plant fluorescence in the O-A absorption band from DESIS imagery,"We provide the first method allowing to retrieve spaceborne SIF maps at 30 m ground resolution with a strong correlation (r^{2}=0.6) to high-quality airborne estimates of sun-induced fluorescence (SIF). SIF estimates can provide explanatory information for many tasks related to agricultural management and physiological studies. While SIF products from airborne platforms are accurate and spatially well resolved, the data acquisition of such products remains science-oriented and limited to temporally constrained campaigns. Spaceborne SIF products on the other hand are available globally with often sufficient revisit times. However, the spatial resolution of spaceborne SIF products is too small for agricultural applications. In view of ESA’s upcoming FLEX mission we develop a method for SIF retrieval in the O2-A band of hyperspectral DESIS imagery to provide first insights for spaceborne SIF retrieval at high spatial resolution. To this end, we train a simulation-based self-supervised network with a novel perturbation based regularizer and test performance improvements under additional supervised regularization of atmospheric variable prediction. In a validation study with corresponding HyPlant derived SIF estimates at 740 nm we find that our model reaches a mean absolute difference of 0.78\,\,\mathrm{mW\,nm^{-1}\,sr^{-1}\,m^{-2}}.","The potential of sun-induced flurorescence (SIF) for agricultural management and phenotyping tasks was recognized early in the development of retrieval algorithms [41]. Since SIF is fuelled by a residual energy flux of photosynthetically active radiation (PAR) that is not consumed by processes related to the plant’s photochemistry and thermal energy dissipation it provides a causal link between radiance measurements and the photosynthetic status of plants [42, 50, 61, 62]. Various studies have utilized this relationship as the theoretical basis for stress detection and monitoring [1, 12, 49, 14, 68], the estimation of photosynthetic activity and gross primary productivity [10, 59, 58, 69], crop monitoring and yield predictions [25, 56, 37, 48] and disease monitoring [8, 51] from SIF estimates derived from remote sensing data at various spatial scales. Quantitative estimates of SIF allow for more sensitive and causally founded physiological assessments compared to purely reflectance based indices commonly used for such tasks. Different studies have shown the increased explanatory power of SIF estimates measured at canopy level in a range of tasks [12, 45, 65, 39]. SIF retrieval methods for a variety of sensors have been developed as the number of airborne and spaceborne sensors with sufficient spectral resolution has increased [43]. However, no spaceborne sensor designed specifically for fluorescence retrieval has yet been operationalized. ESA’s Earth Explorer Mission FLEX [16], planned to be launched in 2025, will be the first such instrument. Spaceborne SIF estimates to this day are derived from data acquired by satellite missions for atmospheric characterization (e.g., GOSAT [34], GOME [33, 27], SCIAMACHY [35], OCO-2/3 [57, 17], TROPOMI [26, 28], TanSAT [67]) as their spectral resolution (SR) and signal-to-noise ratio (SNR) allow for SIF retrieval from Fraunhofer lines [23, 24, 16]. However, the spatial resolution of these atmospheric sensors (> 4 km2) is insufficient for most agricultural applications. FLEX, on the other hand, will provide radiance data with a pixel size of 300 m which still imposes severe limits on its usability for a wide range of applications in heterogeneous agricultural landscapes. As an exploratory step towards spaceborne SIF retrieval at high spatial resolution, we therefore propose a deep learning architecture and a loss formulation for the first SIF retrieval from hyperspectral imagery of the DLR Earth Sensing Imaging Spectrometer (DESIS). SIF retrieval from DESIS imagery has the benefit of providing spaceborne SIF products at an unprecedented spatial resolution of 30 m which principally allows for the targeted acquisition of auxiliary validation data at high spatial resolution for the upcoming FLEX mission. However, the SR and SNR of DESIS are insufficient for consistent SIF retrieval with current traditional retrieval methods leveraging data in the O2-A absorption band [22, 13, 40] where the fluorescence signal contribution to the at-sensor signal has a local maximum. Airborne SIF retrieval with similar methods applied to radiance data at lower SR has however been shown to yield consistent relative SIF estimates [3]. As a solution, we extend the simulation-based self-supervised deep learning approach of [5, 7], called Spectral Fitting Method Neural Network (SFMNN), originally developed with airborne hyperspectral imagery. As in other self-supervised simulation-based learning schemes, this approach leverages the implicit constraints of a differentiable simulator of the physical image generation in the loss [30, 32] and primarily does not rely on labels for training. Further regularization terms that enforce physical and physiological domain constraints allow this encoder-decoder architecture to decompose and reconstruct hyperspectral data in the spectral range around the O2-A absorption band. In this contribution we introduce regularization terms in the SFMNN framework allowing consistent SIF retrieval in DESIS imagery despite its lower SNR and SR. Firstly, we propose a perturbation based augmentation scheme to promote the decorrelation of the predicted SIF from other confounding variables affecting the at-sensor signal. Secondly, we show that including ancillary atmospheric data from DESIS L2A products by means of a secondary supervised downstream learning task improves the performance of our model."
https://arxiv.org/html/2411.08919v1,A Machine Learning based Hybrid Receiver for 5G NR PRACH,"Random Access is a critical procedure using which a User Equipment (UE) identifies itself to a Base Station (BS). Random Access starts with the UE transmitting a random preamble on the Physical Random Access Channel (PRACH). In a conventional BS receiver, the UE’s specific preamble is identified by correlation with all the possible preambles. The PRACH signal is also used to estimate the timing advance which is induced by propagation delay. Correlation-based receivers suffer from false peaks and missed detection in scenarios dominated by high fading and low signal-to-noise ratio. This paper describes the design of a hybrid receiver that consists of an AI/ML model for preamble detection followed by conventional peak detection for the Timing Advance estimation. The proposed receiver combines the Power Delay Profiles of correlation windows across multiple antennas and uses the combination as input to a Neural Network model. The model predicts the presence or absence of a user in a particular preamble window, after which the timing advance is estimated by peak detection. Results show superior performance of the hybrid receiver compared to conventional receivers both for simulated and real hardware-captured datasets.","5G use cases such as Massive Machine Type Communications, have to support a wide range and a large number of UE devices, including mobile phones and various IoT devices. Each of these devices has to attach to a BS, upon turning on. In 5G systems, the BS continuously transmits broadcast signals, which the UE has to detect in order to trigger its initial attachment procedure. The procedure starts with the UE selecting a Random Access Preamble Index (RAPID) which it uses to rotate a known base sequence. Since the BS has limited knowledge about the UE’s location and channel conditions, it uses a correlation-based receiver to decode the RAPID and estimate the propagation delay induced by the channel. Delay estimates fed back to the UE can be used to advance future uplink transmissions. Henceforth, we refer to this delay as Timing Advance (TA). Initially, the UE transmits on the PRACH with a low power and ramps it by 3/6 dB in the subsequent transmissions if the previous reception fails. PRACH decoding failures, in the form of false peaks and missed detections are common in correlation-based receivers, particularly in high fading and low Signal-to-Noise Ratio (SNR) scenarios. Note that each retransmission on the PRACH causes a wastage of radio and power resources and increases the latency of attachment. To improve the robustness of PRACH decoding (and therefore reduction in PRACH retransmissions), in this paper, we show the design of a hybrid receiver that relies on an AI/ML model and a peak detection module. The input to the model is the Power Delay Profile (PDP) of the correlation, combined across multiple antennas. We have shown that this hybrid receiver performs better than both conventional correlation-based approaches as well as other AI/ML-based approaches. The main contributions of this paper are: • Design of a hybrid receiver for PRACH, which outperforms correlation-based receivers. This work is a lower complexity enhancement of our previous work which relied on large Neural Network (NN) models [1]. • Performance analysis of the proposed receiver with Multiple Input Multiple Output (MIMO). • Performance analysis of the proposed receiver on real data. • Explainability insights on the decision making of the model using Shapley Additive Explanations."
https://arxiv.org/html/2411.08918v1,Wireless Federated Learning over UAV-enabled Integrated Sensing and Communication,"This paper studies a new latency optimization problem in unmanned aerial vehicles (UAVs)-enabled federated learning (FL) with integrated sensing and communication. In this setup, distributed UAVs participate in model training using sensed data and collaborate with a base station (BS) serving as FL aggregator to build a global model. The objective is to minimize the FL system latency over UAV networks by jointly optimizing UAVs’ trajectory and resource allocation of both UAVs and the BS. The formulated optimization problem is troublesome to solve due to its non-convexity. Hence, we develop a simple yet efficient iterative algorithm to find a high-quality approximate solution, by leveraging block coordinate descent and successive convex approximation techniques. Simulation results demonstrate the effectiveness of our proposed joint optimization strategy under practical parameter settings, saving the system latency up to 68.54% compared to benchmark schemes.","Unmanned aerial vehicles (UAVs), commonly referred to as drones, have been revolutionizing next-generation wireless networks with their versatile capabilities including line-of-sight (LoS) connections, 3D mobility, and flexibility. UAVs can function as airborne base stations (BSs) for delivering communication, computation, and caching services to overcome traditional infrastructure limitations, while can also serve as mobile users for tasks like remote sensing, delivery services, target tracking, and virtual reality support. More recently, UAVs have been integrated with machine learning (ML) to deliver intelligent services such as classification of aerial images captured by UAV’s cameras. To ensure data privacy during ML model training in UAV networks, federated learning (FL) has been recently employed where UAVs can train an ML model and only share the trained model updates to a cloud server without data exchange. Previous studies have mainly focused on implementing conventional FL architectures with UAVs, where UAVs typically serve as FL clients with pre-stored datasets or as aerial base stations responsible for model aggregation. More recent research, such as [1, 2], has integrated UAV deployment into the FL system, enabling UAVs to function as relays for efficient data collection from other devices or as aggregators with adaptive positioning to improve FL performance. Furthermore, much of the prior work in this domain has primarily concentrated on communication and/or computation aspects, assuming that the data for training is readily available without accounting for the data sensing process. This assumption overlooks a critical factor, as the sensing process competes with communication and computation for limited resources, which can significantly affect overall learning performance. In FL, sensing, computation, and communication are highly coupled and must be seamlessly integrated to fully unlock its potential. This has given rise to the increasingly prominent research field of integrated sensing and communication (ISAC) [3]. Several studies have explored FL-UAV, FL-ISAC networks. In [4], the authors investigated a UAV-FL system for image classification in area exploration scenarios, while [5] proposed UAV-empowered wireless power transfer to enable sustainable FL-based wireless networks. The works in [6], [7], [8] studied FL-ISAC frameworks where they focus on jointly optimizing sensing, communication, and computation resource allocation. Despite these research efforts, the problem of latency minimization in UAV-enabled FL system with ISAC remains under-explored. Given UAV’s limited computational power and battery capacity, optimizing the round-trip ML model training latency, such as ML model training and ML model communication, is crucial to achieve efficient and timely FL while preserving UAVs’ resources. Hence, this paper studies a new latency minimization problem for UAV-enabled FL with ISAC. The contributions of this paper are two-fold: (1) We formulate a new latency minimization problem for UAV-enabled FL with ISAC, by jointly optimizing resource allocation and trajectory of UAVs along with resource allocation of the BS (Section II); (2) This latency minimization problem is computationally challenging, thus we propose a new iterative optimization approach to obtain the optimal solutions by applying block coordinate descent (BCD) and successive convex approximation (SCA) techniques (Section III). Numerical simulations show the superior performance of our joint optimization method compared to baseline approaches (Section IV)."
https://arxiv.org/html/2411.08910v1,Automated Feedback in Math Education: A Comparative Analysis of LLMs for Open-Ended Responses,"The effectiveness of feedback in enhancing learning outcomes is well documented within Educational Data Mining (EDM). Various prior research has explored methodologies to enhance the effectiveness of feedback. Recent developments in Large Language Models (LLMs) have extended their utility in enhancing automated feedback systems. This study aims to explore the potential of LLMs in facilitating automated feedback in math education. We examine the effectiveness of LLMs in evaluating student responses by comparing 3 different models: Llama, SBERT-Canberra, and GPT4 model. The evaluation requires the model to provide both a quantitative score and qualitative feedback on the student’s responses to open-ended math problems. We employ Mistral, a version of Llama catered to math, and fine-tune this model for evaluating student responses by leveraging a dataset of student responses and teacher-written feedback for middle-school math problems. A similar approach was taken for training the SBERT model as well, while the GPT4 model used a zero-shot learning approach. We evaluate the model’s performance in scoring accuracy and the quality of feedback by utilizing judgments from 2 teachers. The teachers utilized a shared rubric in assessing the accuracy and relevance of the generated feedback. We conduct both quantitative and qualitative analyses of the model performance. By offering a detailed comparison of these methods, this study aims to further the ongoing development of automated feedback systems and outlines potential future directions for leveraging generative LLMs to create more personalized learning experiences.","The growing integration of online learning platforms into traditional educational settings has influenced the development and direction of educational research. The global pandemic, COVID-19, resulted in the adoption of Online Learning Platforms (OLP)[3]. Consequently, various OLPs, especially in math education, have gained popularity over the recent years [34]. With the popularity of these platforms, there has been various research investigating effective teaching strategies, with many reporting on the benefit of timely and immediate feedback [14, 32, 19]. Feedback plays a crucial role in facilitating effective learning experiences, offering more than just assessments on the correctness of their answer by providing student-specific guidance. Timely feedback, in particular, can be highly effective in enabling students to rectify misunderstandings, bridge gaps in knowledge, or navigate to subsequent stages of their learning requirements. Prior exploration of effective feedback has reported on the effectiveness of feedback in enhancing learning outcomes, including the use of hints [53], explanations [39], worked-out examples [13], and common wrong answer feedback [22, 23], while others caution against the use of certain feedback designs, suggesting that poorly designed feedback can inadvertently impede student progress [23]. Automated scoring has been a focus for numerous online learning platforms, with extensive research spanning various fields, including mathematics [6], writing[43, 36], and programming [41, 51]. The initial works emphasized automating the grading of close-ended questions. However, recent advancements have extended these methodologies to include open-ended problems as well [18]. While early applications of automated scoring primarily focused on augmenting teacher resources in evaluating student responses, more recent explorations have begun to implement these techniques directly within classroom environments [38] to support students dynamically in real-time. The recent advancement and innovation in Large Language Models (LLMs), such as ChatGPT, have introduced a transformative approach to crafting automated feedback systems within educational platforms [2]. These developments in LLM technology have demonstrated significant potential in creating diverse mathematical content, providing support for math tutoring, offering detailed explanations, and facilitating the development of automated tutoring systems and educational chatbots that are adept at adapting to a wide range of contextual nuances. In this study, we delve into the application of pre-trained Large Language Models (LLMs) for both scoring and providing feedback on students’ open-ended responses. We particularly assess a fine-tuned LLM derived from Mistral—a Llama variant optimized for mathematics—and compare its efficacy with a leading non-generative model [11], currently used for the automated assessment of open-ended responses in mathematics. Additionally, we explore how these methods stack up against the capabilities of the GPT-4 model. Given the current limitations on training and fine-tuning GPT-4, we adopt a zero-shot strategy by providing the GPT-4 model with specific rubrics related to the open-ended questions. Toward this, we explore the following research questions: 1. How does an LLM fine-tuned (GOAT) with a dataset of students’ responses and teacher-provided scores compare to the previous state-of-the-art, SBERT-Canberra method in predicting teacher scores for student open-responses? 2. How does the pre-trained GPT4 model compare to the finetuned LLM (GOAT) in the auto-scoring task for open-ended questions? 3. Which of the three models, SBERT-Canberra, GOAT, or GPT4 is preferred for the feedback they generate, according to a detailed assessment protocol and human evaluators with prior teaching experience?"
https://arxiv.org/html/2411.08899v1,FinVision: A Multi-Agent Framework for Stock Market Prediction,"Financial trading has been a challenging task, as it requires the integration of vast amounts of data from various modalities. Traditional deep learning and reinforcement learning methods require large training data and often involve encoding various data types into numerical formats for model input, which limits the explainability of model behavior. Recently, LLM-based agents have demonstrated remarkable advancements in handling multi-modal data, enabling them to execute complex, multi-step decision-making tasks while providing insights into their thought processes. This research introduces a multi-modal multi-agent system designed specifically for financial trading tasks. Our framework employs a team of specialized LLM-based agents, each adept at processing and interpreting various forms of financial data, such as textual news reports, candlestick charts, and trading signal charts. A key feature of our approach is the integration of a reflection module, which conducts analyses of historical trading signals and their outcomes. This reflective process is instrumental in enhancing the decision-making capabilities of the system for future trading scenarios. Furthermore, the ablation studies indicate that the visual reflection module plays a crucial role in enhancing the decision-making capabilities of our framework.","The complexities and volatility of financial markets, along with multi-modal data streams, present significant challenges for tasks such as trading and market movement prediction. Effective prediction and trading systems must integrate all available information comprehensively and employ sophisticated algorithmic designs to achieve superior performance (Wen et al., 2019; Picasso et al., 2019). To improve trading systems, the field has progressed from rule-based trading strategies to more advanced deep learning models and Reinforcement Learning (RL)-based agents (Koa et al., 2023; Zhang et al., 2022a; Qin et al., 2024; Han et al., 2023). However, these models face substantial challenges, including the need for extensive training data, the oversimplification of diverse financial data types, and a lack of interpretability in their decision-making processes (Yang et al., 2023). A key challenge in these advanced models is the effective integration of diverse financial data types without oversimplification. For instance, incorporating textual news data into deep learning and RL models presents complex challenges: reducing multifaceted content to single-variable sentiment scores fails to capture market dynamics, while effectively interpreting this information requires sophisticated financial reasoning to track evolving events and market developments over time (Bybee et al., 2023). Similarly, representing historical price data and technical indicators poses significant challenges due to their high dimensionality, non-linear relationships, and time-dependent nature, which can lead to information loss or misinterpretation (Li et al., 2023; Sujatha Ravindran and Contreras-Vidal, 2023). Attempts to address these issues by increasing the number of variables to represent different aspects of the market often result in increased model complexity, making the internal representations and decision-making processes more intricate and harder to interpret (Li et al., 2023; Sujatha Ravindran and Contreras-Vidal, 2023). Recent advancements in Large Language Models (LLMs) have driven their evolution into agents capable of executing complex, multi-step decision-making tasks (Xu et al., 2023; Zhu et al., 2022; Gou et al., 2023). This progress has expanded the potential applications of LLMs to a diverse array of challenging domains, including mathematical reasoning, software development, and scientific research (Qian et al., 2024; Liang et al., 2024; Du et al., 2024; Elhenawy et al., 2024). To address these complex tasks, researchers have developed a methodology that decomposes them into distinct sub-tasks (Du et al., 2024). This approach employs multiple LLM-powered agents that collaborate, each focusing on specific aspects of the overall task, to derive comprehensive solutions. By mimicking human cognitive processes, this method enhances reasoning capabilities and problem-solving efficacy. This collaborative approach significantly addresses a critical limitation of previous deep learning and RL models by enhancing model explainability. The transparent nature of the agents’ thought processes through Chain of Thought (CoT) prompting allows for step-by-step tracking of solution derivation, providing valuable insights into their decision-making (Zhang et al., 2022b). This explainable approach not only facilitates a deeper understanding of model operations but also enables the fine-tuning of agent prompts, framework design, and task assignment. Furthermore, the emergence of multi-modal LLMs, such as GPT-4V and the cost-effective GPT-4o, has further expanded LLM capabilities by incorporating both textual and visual data (OpenAI, 2023). This integration of multimodal inputs enhances the versatility and applicability of LLM-based agents across a broader range of complex tasks (Elhenawy et al., 2024). These advancements unlock new possibilities for comprehensive analysis across various domains, particularly in finance. In this field, integrating diverse data types—such as textual reports, news articles, and visual data like charts—is essential for making accurate trading decisions. The evolution of LLMs and multi-agent systems holds the potential to revolutionize financial analysis by providing more sophisticated approaches to understanding market dynamics. The application of LLMs in stock prediction has been evolving, with existing studies primarily focusing on methods such as pre-trained LLMs or instruction tuning, which require extensively annotated datasets (Steinert and Altmann, 2023; Yu et al., 2023; Yang et al., 2023). In the context of LLM-based agents, FinAgent proposed a multimodal LLM trading agent with market intelligence and reflection modules (Zhang et al., 2024b). Our study builds upon this framework by experimenting with a significantly shorter training time of just two months, which helps to reduce API costs. Furthermore, we extend the decision-making process by requiring the model to predict the position size for trading as a percentage of the portfolio, thus inducing a more granular approach to risk management and capital allocation within our framework. Our framework comprises four primary components: the Summarize Module, the Technical Analyst Module, the Prediction Module, and the Reflection Module. The Summarize Module condenses large volumes of textual news data into concise summaries that highlight factual information influencing stock trading decisions. The Technical Analyst Agent leverages the visual reasoning capabilities of LLMs to analyze candlestick charts with technical indicators, providing interpretations for next-day trading strategies. The Reflection Module consists of two parts: one assesses the short-term and medium-term performance of previous trades, while the other plots past trading signals, generates charts, and offers insights into the effectiveness of trades. The Prediction Agent integrates information from these components to forecast trading actions, determine position size as a percentage of the portfolio, and provide a detailed explanation of the decision. Based on the Prediction Agent’s output, the Reward Agent executes trades and calculates performance metrics. These metrics are then used by the Reflection and Prediction Agents in the subsequent iterations. The detailed flow of our framework is illustrated in Figure 1. We evaluate the performance of our framework on three major technology companies (Apple, Amazon, and Microsoft) over a seven-month period. Our findings indicate that our approach outperforms previous rule-based and reinforcement learning (RL)-based models, although it still falls short of the benchmark set by the FinAgent framework. The analysis of the trading signals reveals a comprehensive integration of diverse data sources, including financial news, candlestick chart analysis, and insights from the reflection module. This holistic approach demonstrates the potential of our framework while also highlighting areas for further development. Notably, our ablation studies underscore the significant contribution of the reflection module to overall performance. Figure 1. The Multi-modal Multi-Agent Prediction Framework"
https://arxiv.org/html/2411.08894v2,Temporal Patterns of Multiple Long-Term Conditions in Individuals with Intellectual Disability Living in Wales: An Unsupervised Clustering Approach to Disease Trajectories,"Identifying and understanding the co-occurrence of multiple long-term conditions (MLTC) in individuals with intellectual disabilities (ID) is crucial for effective healthcare management. These individuals often experience earlier onset and higher prevalence of MLTCs compared to the general population, highlighting the urgency of this issue. Despite established research on the high prevalence of MLTCs in the ID population, the specific patterns of co-occurrence and temporal progression of these conditions remain largely unexplored. This study presents an innovative unsupervised approach for examining and characterising clusters of MLTC in individuals with ID, based on their shared disease trajectories. Using a dataset of electronic health records (EHRs) from 13069 individuals with intellectual disabilities, encompassing primary and secondary care data in Wales from 2000 to 2021, this study analyses the time sequences of ordered disease diagnoses. The study population comprised 52.3% males and 47.7% females, with a mean of 4.5 \pm 3 long-term conditions (LTCs) per patient. Distinct MLTC clusters were identified in both males and females, stratified by age groups (< 45 and \geq 45 years). For males under 45, a single cluster dominated by neurological conditions (32.4%), while three clusters were identified for older males, with the largest characterised by circulatory (51.8%). In females under 45, one cluster was found with digestive system conditions (24.6%) being most prevalent. For females \geq 45 years, two clusters were identified: the first cluster was predominantly defined by circulatory (34.1%), while the second cluster by digestive (25.9%) and musculoskeletal (21.9%) system conditions. Mental illness, epilepsy, and reflux disorders were prevalent across all groups. This study reveals complex multimorbidity patterns in individuals with ID, highlighting age and sex differences. The identified clusters provide new insights into disease progression and co-occurrence in this population. These findings can inform the development of targeted interventions and risk stratification strategies, potentially improving personalised healthcare for individuals with ID and MLTCs with the aim of improving health outcome for this vulnerable group of patients, i.e., reducing frequency and length of hospital admissions and premature mortality.","People with intellectual disability (ID) face a significantly higher risk of developing a range of physical and mental health conditions compared to the general population. These conditions often occur at a younger age and lead to poorer outcomes, owing to a combination of genetic, behavioural, and social factors [1, 2, 3]. Studies show a much higher occurrence of multiple long-term conditions (MLTCs) in this population [2]. MLTCs, defined as two or more conditions in addition to ID, is linked to premature death and poorer quality of life [4]. Despite this, there appear to be only a few studies reporting the prevalence of MLTCs conducted on a large scale [2, 5], but no studies were found to reveal patterns of MLTCs and conditions more likely to co-occur together in this population. The growing use of electronic health records (EHRs) has enabled significant advances in addressing clinical challenges, enhancing diagnostic capabilities, and improving patient outcomes [6, 7, 8]. In addition to enabling studies on co-occurring conditions, the longitudinal nature of EHR provides a unique opportunity to uncover temporal associations and trajectories between conditions. Importantly, chronic health conditions frequently co-occur more than expected by chance, often as a consequence of shared risk factors, pathogenicity, or their treatment [9]. However, most prior studies have not incorporated the time dimension due to the short time span of the available data [10, 11]. Only recently have a few large-scale analyses assessed disease trajectories by evaluating temporal ordering of co-morbidity pairs over time in general population [12, 13, 14]. Many studies further developed the framework initially proposed by Jensen et al. [12], who described general principles for temporal trajectory analysis using Danish national data. For instance, Siggaard et al. [15] published a browser of these results, while Jørgensen and Brunak [16] focused on chronic obstructive pulmonary disease (COPD) trajectories. Hu et al. [17] linked the data to a cancer registry to investigate pre-diagnosis trajectories. Jensen et al.’s [12] approach has been applied, with modifications, to other populations including post-depression trajectories in UK Biobank [18], and end-of-life trajectories in California [19]. Furthermore, Giannoula et al. [13, 20] proposed a framework to detect and cluster co-morbidity pairs and shared trajectories over time using a dynamic time warping (DTW)-based unsupervised algorithm in EHRs, and later extended this to include genetic information in the clustering step. Unsupervised algorithms discover natural patterns in data without learning predefined outcomes or classifications. Trajectory analyses can reveal complex, time-ordered condition associations, as well as MLTCs patterns to enable better understanding of disease progression for improved prediction outcomes. In this study, we propose a computational framework for the analysis of temporal MLTCs on EHRs in 13069 adults diagnosed with ID in Wales, incorporating 40 long-term conditions (LTCs) from both primary and secondary care data. While most prior studies have applied temporal trajectory analysis to secondary care data and ICD-9 or -10 codes [12, 13, 15, 16, 17, 18, 19, 20], with only one study using primary care data [21], our approach utilises both to fully capture MLTCs, as most chronic conditions are treated in general practice. This approach highlights several differences in MLTC patterns between male and female sub-populations across different age groups, acknowledging sex and age as crucial factors in understanding MLTCs. The primary contributions of this research include statistical analysis to identify significant temporal condition pairs, identification of shared MLTCs trajectories, construction of a network of all shared trajectories, and identification of trajectory clusters using an unsupervised machine learning algorithm."
https://arxiv.org/html/2411.08891v1,Calibrated Decision-Making through Large Language Model-Assisted Retrieval,"Recently, large language models (LLMs) have been increasingly used to support various decision-making tasks, assisting humans in making informed decisions. However, when LLMs confidently provide incorrect information, it can lead humans to make suboptimal decisions. To prevent LLMs from generating incorrect information on topics they are unsure of and to improve the accuracy of generated content, prior works have proposed Retrieval Augmented Generation (RAG), where external documents are referenced to generate responses. However, traditional RAG methods focus only on retrieving documents most relevant to the input query, without specifically aiming to ensure that the human user’s decisions are well-calibrated. To address this limitation, we propose a novel retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG), which ensures that decisions informed by the retrieved documents are well-calibrated. Then we empirically validate that CalibRAG improves calibration performance as well as accuracy, compared to other baselines across various datasets.","1 introduction Large language models (LLMs; Jiang et al., 2023; Touvron et al., 2023; Dubey et al., 2024; Achiam et al., 2023) have demonstrated remarkable performance on numerous downstream natural language processing (NLP) tasks, leading to their widespread integration into various decision-making processes (Bommasani et al., 2021; Band et al., 2024; Zhou et al., 2024). However, even with significant increases in model size and the expansion of training datasets, it remains infeasible for LLMs to encode all possible knowledge within their parameters. As a result, the outputs produced by LLMs may not consistently be reliable for important human decision-making processes, potentially overlooking key or hidden details. Additionally, LLMs frequently provide inaccurate or misleading information with a high degree of confidence, a phenomenon referred to as hallucination (Zhuo et al., 2023; Papamarkou et al., 2024), which can lead humans to make flawed decisions. In addition, Zhou et al. (2024) have empirically demonstrated that human users often over-rely on LLM outputs during decision-making processes, and this over-reliance tends to increase in proportion to the model’s confidence. Here, the model’s confidence refers to the verbalized expression of how certain the model is when asked how confident it is in its answer. Specifically, they have found that for answers with high confidence, users show strong over-reliance regardless of whether the answer is correct or not. These findings highlight that utilizing LLMs without proper calibration of their responses and addressing the frequent occurrence of hallucinations can lead to incorrect decisions in high-stakes tasks like medical diagnosis and legal reasoning, potentially resulting in severe consequences (Li et al., 2019; 2022b; Han et al., 2024). Retrieval Augmented Generation (RAG) (Lewis et al., 2020; Li et al., 2022a; Wang et al., 2024) has emerged as a promising method to address hallucinations, which is one of the two key issues when using LLMs in decision-making (Shuster et al., 2021; Li et al., 2024). Instead of generating answers directly, RAG retrieves relevant documents from external databases and uses them as an additional context for response generation. This approach supplements the information that LLMs lack, resulting in more accurate and reliable responses. However, the database cannot encompass all information, and the world knowledge is continuously being updated. In such cases, the retriever may retrieve irrelevant documents, which can distract the LLM and lead to the generation of incorrect answers to the question (Shi et al., 2023). Moreover, as described in Section 2.2, due to the LLM’s overconfidence in the retrieved document, they still tend to assign high confidence to its responses even when they are incorrect. To address the issue of deep neural networks generating overconfident outputs for given inputs and to promote well-calibrated predictions, research on uncertainty calibration has been actively conducted across various fields (Kuleshov et al., 2018; Laves et al., 2020; Kapoor et al., 2024). In particular, for image classification tasks in computer vision, numerous techniques (Lakshminarayanan et al., 2017; Maddox et al., 2019; Thulasidasan et al., 2019) have been developed to improve uncertainty calibration. Especially, post hoc methods like temperature scaling, which simply adjust the output logits, have been shown to be simple yet effective in improving calibration (Kull et al., 2019; Vaicenavicius et al., 2019; Minderer et al., 2021; Widmann et al., 2022). However, in contrast to vision tasks, calibrating LLMs poses a more complex challenge due to their sequential token generation nature (Kapoor et al., 2024). Specifically, LLMs produce sequences of log probabilities for each token, and the number of possible sequences grows exponentially with length, making it impractical to apply traditional calibration methods that consider all output possibilities. This complexity renders straightforward adaptations of calibration techniques like temperature scaling ineffective for long-form sentence generation tasks in LLMs. To address these challenges, recent work by Band et al. (2024) proposed an uncertainty calibration method specifically designed for decision-making scenarios involving LLMs in long-form generation contexts. This method aims that the probabilities associated with user decisions, based on the guidance generated by the LLM, are well-calibrated. However, this method still lacks the ability to calibrate the probabilities associated with user decisions based on the guidance provided by RAG. To address this issue, we propose the Calibrated Retrieval-Augmented Generation (CalibRAG) framework. CalibRAG allows an LLM using RAG to not only select relevant information to support user decision-making but also provide confidence levels associated with that information by utilizing a forecasting function, ensuring well-calibrated decisions based on the retrieved documents. Here, the forecasting function is the surrogate model that predicts the probability of whether the user’s decision based on the guidance provided by RAG will be correct. We empirically validate that our CalibRAG significantly improves calibration performance as well as accuracy, compared to other relevant baselines across several datasets. Our contributions can be summarized as follows: • We propose the CalibRAG framework, which enables well-calibrated decision-making based on the guidance provided by RAG. • We construct a new dataset by creating labels that indicate whether decisions made using retrieved documents correctly answer the questions, essential for training the forecasting function. • We outperform existing uncertainty calibration baselines across various tasks involving RAG context using the Llama-3.1 model in decision-making scenarios."
https://arxiv.org/html/2411.08889v1,Multilingual Standalone Trustworthy Voice-Based Social Network for Disaster Situations,"In disaster scenarios, effective communication is crucial, yet language barriers often hinder timely and accurate information dissemination, exacerbating vulnerabilities and complicating response efforts. This paper presents a novel, multilingual, voice-based social network specifically designed to address these challenges. The proposed system integrates advanced artificial intelligence (AI) with blockchain technology to enable secure, asynchronous voice communication across multiple languages. The application operates independently of external servers, ensuring reliability even in compromised environments by functioning offline through local networks. Key features include AI-driven real-time translation of voice messages, ensuring seamless cross-linguistic communication, and blockchain-enabled storage for secure, immutable records of all interactions, safeguarding message integrity. Designed for cross-platform use, the system offers consistent performance across devices, from mobile phones to desktops, making it highly adaptable in diverse disaster situations. Evaluation metrics demonstrate high accuracy in speech recognition and translation, low latency, and user satisfaction, validating the system’s effectiveness in enhancing communication during crises. This solution represents a significant advancement in disaster communication, bridging language gaps to support more inclusive and efficient emergency response.","In today’s interconnected world, effective communication is essential, particularly during disaster situations where timely information dissemination can save lives. However, linguistic diversity often poses significant challenges in ensuring that crucial messages are understood by all stakeholders. Traditional communication platforms frequently fail to address these language barriers, leaving gaps in understanding that can have dire consequences during emergencies. The advent of AI technologies and decentralized systems like blockchain offers new possibilities for creating secure, multilingual communication networks that can function reliably even in the most challenging circumstances. Motivated by the need for a robust solution to bridge language gaps in crisis scenarios, this paper presents a novel, multi-platform application that integrates advanced AI with blockchain technology to enable trustworthy, asynchronous voice-based social media communication across language barriers. The application is designed to run seamlessly on various devices, including mobile phones, tablets, and desktops, and is compatible with different operating systems, ensuring accessibility and reliability in diverse environments. The significance of addressing multilingual communication barriers in disaster situations cannot be overstated. Traditional approaches to disaster communication often fail to account for the linguistic diversity inherent in many societies, leading to what is termed ”disaster linguicism.” This phenomenon exacerbates the vulnerability of Indigenous, Tribal, Minority, and Minoritized peoples, as critical information is often not accessible in their native languages. By proposing a solution that integrates advanced AI with blockchain technology, this paper directly responds to the urgent need for more inclusive and effective disaster risk reduction strategies that consider the linguistic needs of all communities, thereby reducing social vulnerability and enhancing resilience during crises [1]. Despite advancements in communication technology, there remains a critical gap in enabling asynchronous, multilingual voice communication during disaster situations. Current social media platforms and communication tools often rely on centralized servers and lack the necessary security features to ensure message integrity and authenticity, which are particularly vulnerable in disaster scenarios where external networks may fail. Additionally, the language translation tools available are not typically integrated into a system that provides both translation and secure message verification. This lack of integration can lead to misinformation, miscommunication, and delays in critical information sharing, which can exacerbate the effects of disasters. Furthermore, many existing solutions are platform-dependent, limiting their effectiveness across different devices and operating systems. This paper presents significant advancements in the field of disaster communication and multilingual technology through the following key contributions: • Innovative Application for Disaster Scenarios: We introduce a novel, multi-platform application specifically designed to function reliably in disaster situations, where communication infrastructure may be compromised. The application’s ability to operate offline on a local network without requiring an internet connection ensures its deployment in critical environments. Its lightweight design allows it to run on portable devices like laptops, making it highly adaptable and infrastructure-free. • AI-Driven Asynchronous Multilingual Translation: The application leverages advanced AI technology to facilitate asynchronous voice communication by automatically translating spoken messages across multiple languages. Furthermore, it synthesizes these translations back into voice, providing a natural and effective method of communication that transcends language barriers. • Blockchain-Enabled Secure Communication: A key feature of this application is its implementation of secure storage and verification mechanisms using Ethereum blockchain. This ensures the integrity and authenticity of translated messages, critical in maintaining trustworthy communication during emergencies. • Cross-Platform Usability and Security: The application is designed to operate independently of external servers, with all AI processing and blockchain management performed locally. This not only enhances security and reliability but also ensures a seamless user experience across various devices and operating systems, making it accessible in diverse disaster environments. • Robust and Portable Communication System: By focusing on portability, trustworthiness, and transparency, this application provides a robust communication solution in emergency situations. It guarantees that communication remains secure, reliable, and effective, even when traditional systems fail."
https://arxiv.org/html/2411.08888v1,Exploring Capabilities of Time Series Foundation Models in Building Analytics,"The growing integration of digitized infrastructure with Internet of Things (IoT) networks has transformed the management and optimization of building energy consumption. By leveraging IoT-based monitoring systems, stakeholders such as building managers, energy suppliers, and policymakers can make data-driven decisions to improve energy efficiency. However, accurate energy forecasting and analytics face persistent challenges, primarily due to the inherent physical constraints of buildings and the diverse, heterogeneous nature of IoT-generated data. In this study, we conduct a comprehensive benchmarking of two publicly available IoT datasets, evaluating the performance of time series foundation models in the context of building energy analytics. Our analysis shows that single-modal models demonstrate significant promise in overcoming the complexities of data variability and physical limitations in buildings, with future work focusing on optimizing multi-modal models for sustainable energy management.","The deployment of AI in digital infrastructure presents significant opportunities for enhancing energy efficiency and advancing net-zero strategies. Traditional building energy research relies on post-processed datasets, which typically include aggregated load data and weather station-monitored temperature—referred to as rough granularity data. While these higher-level data offer convenience for data scientists by providing a broad overview, they compromise the granularity necessary for a detailed description of activities. The introduction of the BLDG59(Luo et al., 2022) and BTS datasets(Prabowo et al., 2024) based on the centralized management tool, Brick Schema(Balaji et al., 2018), addresses this limitation by offering comprehensive IoT sensing and metering point data, enabling the exploration of detailed usage patterns in digitalized buildings. However, the increased data texture complicates the building activity modeling due to the varied architecture preferences, energy efficiency measures, local climate conditions, usage patterns, and engineering practices, all contributing to heterogeneous sensing data among buildings(Lin et al., 2024). Accurate cross-building forecasting using IoT point data necessitates models that can adapt to unseen physical constraints and variations in ontology usage. Foundation models, which utilize large, pre-trained models as the base for fine-tuning specific tasks, are receiving increasing attention in time series analysis due to their capabilities in generalization. Existing time series foundation models are typically summarized into two categories. Prompt-based methods, such as PromptCast(Xue and Salim, 2023) and LLMTime (Gruver et al., 2024), typically convert time series to sentences using a manually defined template and feed to language models. Methods following this logic benefit from the easy implementation, but specific prompt templates are required based on targeted tasks and datasets. Tuning-based models typically employ fine-tuning techniques for LLMs to adapt the time series inputs. One-Fits-All (GPT4TS)(Zhou et al., 2023) and LLM4TS(Chang et al., 2023) approach multivariate time series by treating them as univariate sequences, which are then segmented into patches. These patches are encoded using specific encoders, concatenated, and subsequently fed into the pre-trained LLMs. To enhance forecasting by incorporating domain knowledge, GPT4MTS(Jia et al., 2024) introduces a prompting template that combines time series data with textual prompts. TimeLLM(Jin et al., 2023) initially proposes the cross-modal attention mechanism, which aligns the time series with pre-train LLM’s word embedding. Inspired by this work, LLaTA(Liu et al., 2024) designed a dual-branch architecture to process the textual and temporal modalities. Each branch fine-tunes a GPT-2 backbones with the Low-rank adaptation technique (LoRA)(Hu et al., 2021) to adapt to the task, aligning the distillation knowledge and temporal embedding by reducing the distribution discrepancy between the modalities. TS foundation models are typically evaluated on well-processed datasets, which differ significantly from real-world building IoT datasets. Building IoT contains branches of information about energy usage, comfort parameters, and environmental factors, but at the same time, the complex dependencies and data heterogeneity among the IoT network present challenges for machine learning and deep learning techniques. In the context of energy-efficient digital infrastructures, the application of TS models still needs extensive experimentation to validate their effectiveness. This comparative study aims to assess the performance of TS foundation models in the building energy domain, benchmarking them against traditional machine learning and deep learning approaches. Additionally, the study explores architectural ablations of TS foundation models to understand their strengths and limitations. The remainder of the paper is structured as follows: Section 2 evaluation methodology adopted by this benchmark study. Section 3 presents the results, followed by a discussion of the findings and future research directions in Section 4. Section 5 provides a concise conclusion."
https://arxiv.org/html/2411.08887v1,Deep Learning-Based CKM Construction with Image Super-Resolution,"Channel knowledge map (CKM) is a novel technique for achieving environment awareness, and thereby improving the communication and sensing performance for wireless systems. A fundamental problem associated with CKM is how to construct a complete CKM that provides channel knowledge for a large number of locations based solely on sparse data measurements. This problem bears similarities to the super-resolution (SR) problem in image processing. In this letter, we propose an effective deep learning-based CKM construction method that leverages the image SR network known as SRResNet. Unlike most existing studies, our approach does not require any additional input beyond the sparsely measured data. In addition to the conventional path loss map construction, our approach can also be applied to construct channel angle maps (CAMs), thanks to the use of a new dataset called CKMImageNet. The numerical results demonstrate that our method outperforms interpolation-based methods such as nearest neighbour and bicubic interpolation, as well as the SRGAN method in CKM construction. Furthermore, only 1/16 of the locations need to be measured in order to achieve a root mean square error (RMSE) of 1.1 dB in path loss.","For the sixth-generation (6G) mobile communication networks, the expansion of frequency bands, together with the adoption of extremely large-scale multiple-input multiple-output (MIMO), renders traditional methods for real-time channel state information (CSI) acquisition more costly and time-consuming, prompting the pursuit of innovative approaches for CSI acquisition [1]. Channel knowledge map (CKM) [2] is a promising technique to address such challenges. CKM provides location-specific channel knowledge that is crucial for enhancing environment-awareness, and hence may significantly improve the communication and sensing performance. For example, the authors in [3] proposed an environment-aware hybrid beamforming technique based on CKM, which drastically reduces the real-time training overhead. By leveraging user location information, this method significantly improves the effective communication rate, even in the presence of moderate location errors. A training-free beamforming scheme was proposed in [4], which designs optimal active and passive beams based on the location and environmental information provided by CKM. Besides, CKM-enabled environment-aware networks can realize communication-aware trajectory planning to avoid blind spots for network-connected ground or aerial robots. A fundamental challenge for CKM-enabled environment-aware communication and sensing is developing effective methods for CKM construction. In [5], the authors employed an analytical model to construct channel gain map (CGM), where channel modeling parameters were estimated from measured data, thereby allowing the full CGM to be generated through the model. In addition to model-based methods, the authors of [6] proposed a data-driven approach for constructing CKM, employing Kriging interpolation to build the shadowing map from a limited set of observations. Meanwhile, the rise of deep learning-based CKM construction methods has sparked a growing demand for specialized CKM datasets. The dataset of path loss and time of arrival (ToA) radio maps [7] encompasses simulated path loss/received signal strength (RSS) and ToA radio maps. The CKMImageNet dataset [8] provides location-tagged numerical channel data alongside visual imagery, offering a comprehensive view of both the channel and environment. This integration not only supports the validation of various communication and sensing algorithms but also enables CKM construction using advanced computer vision (CV) techniques. The emergence of these specialized datasets has advanced researchers’ investigations into the aforementioned data-driven methods. RadioUNet [9] leverages a physical simulation dataset to produce path loss estimations. In [10], the authors devised a sophisticated network architecture grounded in conditional generative adversarial networks (cGANs), engineered to synthesize detailed radio maps. The authors of [11] treat channel knowledge as a 2-D image, framing the CKM estimation as an image-to-image (I2I) inpainting task that predicts channel knowledge at a specific location. Although extensive research has explored data-driven CKM construction with neural networks, these approaches require physical maps or other supplementary information as input. Super-resolution (SR) aims to convert a low-resolution (LR) image into a corresponding high-resolution (HR) image with improved visual quality [12]. Compared to LR images, HR images exhibit a higher pixel density and more detailed textures, resulting in greater reliability. Nowadays, deep learning has become the dominant method in the field of super-resolution. SRCNN [13], as one of the earliest models to apply deep learning techniques to super-resolution, achieved peak signal-to-noise ratio (PSNR) that significantly surpassed traditional methods. In addition to convolutional neural networks (CNNs), researchers have also explored the potential of GANs [14] and residual networks (ResNets) [15]. All of these approaches have achieved impressive results in image super-resolution. Figure 1: Analogy between CKM construction using sparse data and image super-resolution. In this letter, we draw an analogy between image super-resolution and CKM construction, as illustrated in Fig. 1. This enables us to address the CKM construction problem from an image processing perspective, leveraging advanced SR algorithms from the CV domain to obtain the complete CKM from sparse data. The most straightforward image SR algorithms are interpolation methods, which directly perform linear interpolation based on existing pixel values, such as nearest neighbour (NN) and bicubic interpolation. However, these methods often result in significant errors. Other advanced image SR networks, such as GANs, emphasize the visual quality of images, which may not be suitable for channel knowledge inference. Considering the unique features and metrics of CKM, we employ SRResNet to obtain the complete CKM based on sparse channel measurements. Unlike image processing, which highlights texture detail and perceptual realism [16], our approach prioritizes the accurate prediction of channel knowledge. Therefore, we employ mean square error (MSE) as the loss function. By utilizing the novel CKMImageNet dataset, our method can construct not only traditional path loss maps but also channel angle maps (CAMs). Numerical results demonstrate the effectiveness of our proposed strategies across various CKM quality metrics."
https://arxiv.org/html/2411.08885v1,"Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML, CNN, and GCN Models using Audio-Visual Features","Inaccuracies in polygraph tests often lead to wrongful convictions, false information, and bias, which have significant consequences for both legal and political systems. Recently, analyzing facial micro-expressions has emerged as a method to detect deception; however, current models have not reached high accuracy and generalizability. The purpose of this paper is to aid in remedying these problems. The unique multimodal transformer architecture used in this paper improves upon previous approaches by using auditory input, visual facial micro-expressions, and manually transcribed gesture annotations, moving closer to a reliable non-invasive lie detection model. Visual and auditory features were extracted using Vision Transformer and OpenSmile models respectively, which were then concatenated with the transcriptions of participants’ micro-expressions and gestures. Various models were trained for classification instances of lies and truth using these processed and concatenated features. The CNN Conv1D multimodal model achieved a 95.4% average accuracy. However, further research is still required to create higher-quality datasets and even more generalized models for more diverse applications.","Lie detection has been a recurring focus of research and technological innovation in law enforcement and criminal justice. According to a survey conducted by the University of Wisconsin-La Crosse, about 75% of survey respondents reported telling zero to two lies per day; lying comprised 7% of total communication, with 79% of the lies being told face-to-face and 21% being mediated [3]. Current technologies, such as polygraphs, have focused on biological responses like blood pressure to detect lies. However, these methods are unpredictable and easily flawed. Recently, research has begun to focus on various other indicators of deception, including facial micro-expressions and audio cues [4]. Facial micro-expressions (ME) are intentional or involuntary localized and momentary movements of the face, usually lasting less than 500 milliseconds [2]. Despite advancements in lie detection techniques, traditional methods remain intrusive, subjective, and often inaccurate. Detecting deception through ME and speech analysis presents a significant challenge due to the subtle and brief nature of these cues. As shown in Table I, traditional methods have high variance and relatively low accuracy. This study aims to address these limitations by developing a non-intrusive, objective, and highly accurate method for detecting deception using both ME and audio signals. Accurate lie detection is crucial in various fields, including security, legal systems, and psychological evaluations. The primary objective of this study is to establish an AI model that can differentiate between truth and deception with high accuracy by analyzing audio, visual cues in videos, and extracted gestures. Audio dialogue, visuals, and gestures all help to distinguish between deception and truthfulness, making them important features to consider [23]. Therefore, the Real-life Deception Detection Dataset from the University of Michigan was used, which includes 121 videos of deception and truthfulness and a CSV file for gestures. Visuals and audio were extracted from the videos, and OpenSMILE and Vision Transformer (ViT) were used to extract features from audio and video, respectively. Classical machine learning models like Random Forest Classifiers and Logistic Regression can serve as accurate baseline references for a binary classification task like truth and lie. Yet to build off of that, by leveraging advanced neural network models, such as Conv1D, Graph Convolutional Networks (GCN), and CNN LSTM, the accuracy can be increased. TABLE I: Estimated accuracy of different test types in detecting deception and truthfulness Test type Detecting deception Detecting truthfulness Laboratory studies CQT – Polygraph 74%–82% 60%–83% CIT – Polygraph 76%–88% 83%–97% ERP 68% 82% fMRI 84% 81% Field studies CQT – Polygraph 84%–89% 59%–75% CIT – Polygraph 42%–76% 94%–98% This study addresses the following research questions: How effective is the proposed AI model in detecting lies compared to traditional methods and some recent AI models? Which features carry the highest weights in prediction? Deception detection technology has the potential to revolutionize various fields. In law enforcement, it could improve interrogation outcomes and border security by identifying deceptive behavior. In the legal system, it could be utilized to assess the credibility of courtroom testimonies and negotiations. Additionally, applying this technology to financial services could aid in detecting fraudulent claims and reducing the risk of financial fraud. Previous studies have experimented with various machine learning models. For instance, a study by Soldner et al. implemented the Random Forest model, achieving the best accuracy of 69%, as shown in II [5]. Insights from this paper suggest expanding our dataset and exploring additional modalities to enhance the model’s accuracy and reliability in lie detection. Furthermore, Random Forest, being a machine learning technique, cannot handle complex relations as well as multimodal data, which is a limitation of the mentioned study. Moreover, most traditional AI models fall short in reliability and accuracy, often leading to false positives or negatives [1]. A study conducted by the University of Michigan in 2015 analyzed trial videos using micro-facial expressions and achieved a rudimentary accuracy rate of 83.05% using neural networks [6]. Aligning different data types and achieving 83.05% accuracy are two main advantages of the study. TABLE II: Best Results Of Study [2]. Features Acc. Linguistic 66% Dialog 57% Non-verbal 61% All Features 69% This paper is organized as follows: analyzing previous work, discussing the paper’s methods (data collection, data analysis, feature extraction, and implementation guide for the tested models), presenting the results of different tested models, comparing the paper’s results with other studies using the same dataset, and providing a discussion including limitations and recommendations. The paper concludes with a summary of key findings and a look forward."
https://arxiv.org/html/2411.08884v1,Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play,"As Large Language Models (LLMs) become more prevalent, concerns about their safety, ethics, and potential biases have risen. Systematically evaluating LLMs’ risk decision-making tendencies and attitudes, particularly in the ethical domain, has become crucial. This study innovatively applies the Domain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and proposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess LLMs’ ethical risk attitudes in depth. We further propose an novel approach integrating risk scales and role-playing to quantitatively evaluate systematic biases in LLMs. Through systematic evaluation and analysis of multiple mainstream LLMs, we assessed the ""risk personalities"" of LLMs across multiple domains, with a particular focus on the ethical domain, and revealed and quantified LLMs’ systematic biases towards different groups. This research helps understand LLMs’ risk decision-making and ensure their safe and reliable application. Our approach provides a tool for identifying and mitigating biases, contributing to fairer and more trustworthy AI systems. The code and data are available.","Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human language, showing significant potential across various domains. The outstanding performance of LLMs has sparked hope that they might be the AGI of our era Bubeck et al. (2023). As LLMs become more widely adopted, ranging from everyday use to specialized fields, the need for comprehensive evaluation, especially regarding safety, ethics, and biases, becomes increasingly urgent Chang et al. (2024). Currently, the widespread popularity of LLMs has led to the development of numerous evaluation benchmarks, tasks and metrics that examine LLMs from different angles Chang et al. (2024). Evaluations of LLMs’ risk attitudes are crucial for ensuring their safe and reliable application, especially in critical decisions such as health and finance Chang et al. (2024), particularly in modes where the LLM acts as an agent Zhao et al. (2024). In the field, while several benchmarks have been proposed to explore LLMs’ propensity to engage in harmful activities Yuan et al. (2024); Wang et al. (2023); Perez et al. (2022); Shi and Xiong (2024), such work remains relatively scarce. Compared to prior work, we not only innovatively apply interdisciplinary tools to evaluate LLMs’ risk attitudes across multiple domains and deeply assess ethical risk decision-making, but we are also, to our knowledge, the first to evaluate the biases within LLMs based on the risk analysis and role-play. Moreover, our work fills the gap in research AI psychology and cognitive science. Figure 1: Examine LLMs’ risk attitudes through various risk events across multiple domains (such as social and financial domains). What are the things they accept, and what are the things they have zero tolerance for? In this paper, We explore four key questions: (1) Do LLMs exhibit stable and measurable risk attitudes? (2) Are there specific differences or consistent patterns in risk attitudes across multiple domains among different LLMs? (3) What are LLMs’ risk propensities in the ethical domain, and how do these impact LLM safety? (4) Do LLMs exhibit systematic biases in their perception of risk attitudes and ethical levels for different social groups? We innovatively applies risk attitude assessment tools from human psychology, cognitive science and behavioral economics to AI systems, conducting a systematic, standardized, and quantitative evaluation of risk preferences in mainstream LLMs. Studies have shown that using standard psychometric inventories for LLMs is feasible and effective (Pellert et al., 2023; Li et al., 2024). We have chosen the DOSPERT Weber et al. (2002), widely used in social science researches Farnham et al. (2018); Shou and Olney (2020), as our assessment tool. DOSPERT places risk assessment within specific contexts across different domains, allowing for a comprehensive evaluation. DOSPERT has been widely validated and applied across different age groups and cultural backgrounds Blais and Weber (2006). In summary, we find that DOSPERT provides a promising framework for multi-dimensional analysis of LLMs’ ""risk personality"". Applying DOSPERT to LLMs represents an innovative interdisciplinary attempt. Futhermore, given that risk scores in the ethical domain are directly related to the safety of LLMs, we propose EDRAS to comprehensively and specifically evaluate LLMs’ risk attitudes in ethical domain. However, our research does not stop at simply assessing the risk propensities of LLMs. In the field of AI, systematic biases and stereotypes in algorithms have been a significant concern Mehrabi et al. (2021); Blodgett et al. (2020). These not only potentially exacerbate the spread of biases and promote social inequality but may also cause harm to certain groups. How to detect and quantify potential biases in LLMs, which may contain racial, gender, or other biases due to training from human text, is a crucial issue Demszky et al. (2023). We have discovered a novel approach using risk scale and role-play as a bridge to detect and quantify bias in LLMs, indirectly reflecting LLMs’ differential views on various social identities, occupations, ethnicities, and genders. The main contributions of this study include: • We verified that specific LLM possess differentiated and relatively stable risk propensities through multiple tests. • We conducted domain-specific risk propensities evaluations on multiple mainstream LLMs, explored specific differences and consistent patterns in LLMs’ risk attitudes. • We propose a novel EDRAS to further delve into the assessment of LLMs’ ethical decision-making risk attitudes. • We designed different role hypotheses, using risk scales to quantitatively explore what LLMs consider to be the different risk attitudes for various social groups, and discuss potential systematic biases. Models \mathbf{\mu_{5}} \mathbf{M_{5}} \mathbf{{max}_{5}} \mathbf{{min}_{5}} \mathbf{\Sigma^{\mathbf{2}}_{\mathbf{5}}} \mathbf{\Sigma_{5}} \mathbf{\Gamma_{5}} \mathbf{\kappa_{5}} Claude 3.5 Sonnet 38.80 38.40 40.00 38.00 0.51 0.72 0.63 -1.05 ERNIE 3.5 47.36 47.20 48.80 45.20 1.83 1.35 -0.35 -1.18 moonshot-v1 49.04 49.20 50.40 46.80 1.83 1.35 -0.52 -1.08 GPT-4o mini 52.24 52.00 53.60 50.80 1.38 1.18 0.11 -1.71 GPT-3.5 Turbo 55.20 55.20 55.60 54.80 0.13 0.36 0.00 -1.75 Llama 3.2-3b 60.88 61.20 64.00 58.00 4.95 2.23 0.02 -1.47 Table 1: In basic DOSPERT, the scores obtained include the mean \mu, median M, maximum \mathrm{max}, minimum \mathrm{min}, variance \Sigma^{2}, standard deviation \Sigma, skewness \Gamma, and kurtosis \kappa. These scores are calculated by dividing the raw scores by the total possible score of 250. The \mathrm{max} and \mathrm{min} of the various models differ slightly, and the \Sigma^{2} and \Sigma are small, indicating a relatively high stability in scores. This suggests that the LLMs have a relatively stable risk trait. Additionally, the scores among the LLMs also show stable differences, reflecting distinct ""risk personalities."""
https://arxiv.org/html/2411.08882v1,A Novel Multimodal System to Predict Agitation in People with Dementia Within Clinical Settings: A Proof of Concept,"Dementia is a neurodegenerative condition that combines several diseases and impacts millions around the world and those around them. Although cognitive impairment is profoundly disabling, it is the noncognitive features of dementia, referred to as Neuropsychiatric Symptoms (NPS), that are most closely associated with a diminished quality of life. Agitation and aggression (AA) in people living with dementia (PwD) contribute to distress and increased healthcare demands. Current assessment methods rely on caregiver intervention and reporting of incidents, introducing subjectivity and bias. Artificial Intelligence (AI) and predictive algorithms offer a potential solution for detecting AA episodes in PwD when utilized in real-time. We present a 5-year study system that integrates a multimodal approach, utilizing the EmbracePlus wristband and a video detection system to predict AA in severe dementia patients. We conducted a pilot study with three participants at the Ontario Shores Mental Health Institute to validate the functionality of the system. The system collects and processes raw and digital biomarkers from the EmbracePlus wristband to accurately predict AA. The system also detected pre-agitation patterns at least six minutes before the AA event, which was not previously discovered from the EmbracePlus wristband. Furthermore, the privacy-preserving video system uses a masking tool to hide the features of the people in frames and employs a deep learning model for AA detection. The video system also helps identify the actual start and end time of the agitation events for labeling. The promising results of the preliminary data analysis underscore the ability of the system to predict AA events. The ability of the proposed system to run autonomously in real-time and identify AA and pre-agitation symptoms without external assistance represents a significant milestone in this research field.","Dementia is a neurodegenerative condition that leads to a progressive decline in cognition and is one of the leading causes of death, disability, and hospitalization in Canada and worldwide. Currently, dementia is the seventh cause of death worldwide [1]. Globally, over 55 million individuals are living with dementia; as the ratio of older people increases, this number will grow to 78 million by 2030 and 139 million by 2050, making dementia a major global health crisis [1]. In addition to cognitive and functional decline, people living with dementia (PwD) also experience non-cognitive neuropsychiatric symptoms (NPS) during their illness [2]. NPS commonly includes agitation, aggression, apathy, symptoms of psychosis, delusions, hallucinations, and disturbances of sleep and appetite. Among NPS, agitation and aggression (AA) occur frequently in severe cases and are a common source of distress for patients and caregivers [3]. They commonly occur during care and are believed to be manifestations of perceived or real unmet needs [3]. Behaviors of AA include pacing, rocking, gesturing, restlessness, shouting, scratching, throwing objects, and destroying property [4]. These symptoms are the leading cause of hospitalizations, extended length of stay as inpatients, and increased demand for placement in long-term care facilities [5]. AA enormously burdens PwD, their families, caregivers, and healthcare systems. In current practices, AA are commonly assessed through caregiver reports. Many observational methods have been developed, including the Neuropsychiatric Inventory (NPI) [6] and the Cohen-Mansfield Agitation Inventory (CMAI) [7]. These assessments are based on manual observations, which are subject to potential bias depending on the caregiver’s memory or emotional state. It is possible to address these limitations by using Artificial Intelligence (AI) and predictive algorithms to predict episodes of AA in PwD before they occur. By 2025, AI technologies are expected to be worth an estimated $36 billion (US) [8]. There is growing evidence that combining AI and sensory technologies to develop a solution for NPS detection will guide the provision of personalized interventions for PwD [9, 10, 11, 12]. The timely detection of critical events in PwD using digital technologies is gaining wide acceptance. For example, smartwatches are being used to help people with dementia [13, 14] and detect epileptic seizures to prevent the development of severe complications [15, 16]. Multiple attempts have been made to create predictive algorithms to detect AA in PwD using several physiological parameters and/or environmental data [17, 18]. Such studies incorporate wearable sensors to capture patient data and use it in AA prediction using machine learning algorithms. Moreover, AI has also been employed in video-based monitoring systems to monitor and detect AA behavior in PwD [19, 20]. To the best of our knowledge, there is no current video surveillance system operating in a hospital setting to detect AA in PwD in real-time due to privacy concerns. The use of multimodal sensing, including wearable sensors and camera footage, for real-time detection of AA and pre-agitation behavior in PwD has not been extensively explored to date. The combination of multimodal sensing and artificial intelligence holds great promise in effectively detecting and predicting AA in real-time. This approach could lead to the timely implementation of preventive strategies or therapies, which could reduce care costs and decrease the frequency of critical incidents among this demographic [21, 22]. This study aims to understand the complicated behaviors of PwD and predict AA in PwD. We carried out this study in the Geriatric Dementia Unit (GDU) and the Geriatric Transitional Unit (GTU) at the Ontario Shores Center for Mental Health Sciences [23] for 5 years. We integrate a multimodal approach, combining biometric data from the EmbracePlus wristband [24] and video data from CCTV cameras installed in common areas in both units. These biometric signs are analyzed to determine the possible correlation with abnormal behaviors. Data collected by these devices, along with the results of the data analysis, is compared against the nurse notes collected via custom forms to confirm the AA and pre-agitation events. The cameras deployed in the designated places automatically detect AA behavior. The developed AI model detects abnormal behavior from body activity recognition in real-time using deep learning techniques. The cameras allow us to document the exact time of the incident for further analysis. The data and analysis then determine personalized pre-agitation conditions using our proposed classification system. To assess our system, we conducted a pilot study focusing on patient acceptance of wristbands, complemented by video camera validation and multimodal sensor data for predicting AA in PwD. The EmbracePlus wristband was crucial for collecting physiological signals like Electrodermal Activity (EDA), heart rate, skin temperature, and movement data. The camera system was also a key component in detecting body movements and recording agitation events. Both systems are tested on three participants who were successfully recruited at the Ontario Shores Centre for Mental Health Sciences. We achieved high accuracy in detecting AA through comprehensive data preprocessing, feature extraction, and the ExtraTrees classification algorithm. Additionally, AA detection was enhanced by analyzing real-time video feeds with OpenPose-generated skeletal keypoints and employing RNN-based neural networks, particularly LSTM and GRU [25]. These networks, optimized for real-time processing, facilitate timely interventions. The pilot study demonstrated the system’s effectiveness through both the wristband and video detection."
https://arxiv.org/html/2411.08881v1,Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics,"AI-based systems, including Large Language Models (LLMs), impact millions by supporting diverse tasks but face issues like misinformation, bias, and misuse. Ethical AI development is crucial as new technologies and concerns emerge, but objective, practical ethical guidance remains debated. This study examines LLMs in developing ethical AI systems, assessing how trustworthiness-enhancing techniques affect ethical AI output generation. Using the Design Science Research (DSR) method, we identify techniques for LLM trustworthiness: multi-agents, distinct roles, structured communication, and multiple rounds of debate. We design the multi-agent prototype LLM-BMAS, where agents engage in structured discussions on real-world ethical AI issues from the AI Incident Database. The prototype’s performance is evaluated through thematic analysis, hierarchical clustering, ablation studies, and source code execution. Our system generates around 2,000 lines per run, compared to only 80 lines in the ablation study. Discussions reveal terms like bias detection, transparency, accountability, user consent, GDPR compliance, fairness evaluation, and EU AI Act compliance, showing LLM-BMAS’s ability to generate thorough source code and documentation addressing often-overlooked ethical AI issues. However, practical challenges in source code integration and dependency management may limit smooth system adoption by practitioners. This study aims to shed light on enhancing trustworthiness in LLMs to support practitioners in developing ethical AI-based systems.","Artificial Intelligence (AI) is emerging as a transformative force, reshaping industries, economies and everyday life. Despite its rapid development and adoption, a number of negative reports on its use highlight the importance of adhering to ethical norms and principles [39]. Several ethical guidelines are available with plenty ethical principles providing high level and abstract guidance for developers and stakeholders on AI ethics [3]. These are important, but there is a lack of practical guidance for developers to operationalise ethics in AI [3]. As new AI-based technologies emerge, ethics in AI will also become increasingly critical, such as the latest breakthrough: Large Language Models (LLMs). LLMs are becoming ubiquitous and have significant impact on decision-making processes and human interactions [20, 33]. LLMs are advanced AI algorithms capable of generating, interpreting, and predicting text based on vast amounts of data they have been trained on [4]. Of these LLMs, Generative Pre-trained Transformer (GPT) LLMs, such as ChatGPT, have showcased unprecedented proficiency in the human language, coding, logic, reasoning, and other associated natural language tasks [33]. They excel at guiding complex conversations and are used in countless endeavours such as software engineering (SE) - as in AI for Software Engineering (AI4SE) [22] - and qualitative research [26]. Within the AI4SE field, the capabilities of LLMs are being explored in the software development, maintenance, and evolution [28, 30, 26, 22], namely LLM4SE. Accordingly, they find applications across various stages of the software development life cycle, including requirement analysis, software design, code implementation, testing, refactoring, defect detection, and maintenance [30]. Albeit the notable use of LLM in SE, to the best of our knowledge there are no studies that focus on the use of LLM in the ethical AI development. In qualitative research, LLMs are also gaining prominence, particularly as a supplement to tasks traditionally performed by humans, like analysing qualitative data [12, 32, 24]. Specifically, in the coding process - where qualitative data is organised and interpreted by assigning codes or labels to text segments or different data forms - LLMs have demonstrated significant utility [36, 32]. However, several concerns arise, especially related with trustworthiness, as more practitioners are relying on LLMs to perform their task [22]. Several studies are drawing attention to possible problems in adopting LLMs for SE, in particular when syntactically correct but non-functional code is produced, which affects the reliability and effectiveness of LLM-based code generation [16]. Pearce et al. [29] used GitHub Copilot to produce 1,689 programs, and found that 40% of them have security vulnerabilities. Liu et al. [21] systematically analysed ChatGPT code generation reliability identifying quality issues as many of the programs generated provided wrong output or contained compilation or runtime error. Effective AI4SE should be trustworthy and synergistic with the practitioner’s workflow, otherwise “such AI4SE solutions risk becoming obstacles rather than facilitators"" [22]. Currently, there is a growing need for context-dependent empirical studies that explore how trust in LLMs affects their adoption in software engineering tasks [22]. Furthermore, there remains a significant gap in research focused on the practical operationalisation of AI ethics, particularly through the application of LLMs. To date, no studies have attempted to explore the development of ethical AI-based systems using LLMs. In this paper, we aim to explore trustworthiness in LLMs in the ethical AI development through an experiment. Our Research Question is: To what extent does the trustworthiness of LLM-based systems contribute to achieving ethically aligned AI-based systems? To this end, an empirical study was carried out, following the Design Science Research (DSR) method [13]. We identify techniques that can improve trustworthiness in LLMs, apply them to develop a prototype, use it in the context of the development of ethically aligned AI-based systems, and evaluate the output. The evaluation is conducted through four different approaches. First, a thematic analysis is performed by an LLM, which identifies and categorizes key themes within the data. Second, we apply hierarchical clustering dendogram to group related text to compare and support the thematic analysis. Third, we perform an ablation study. Fourth, we run the source code produced to assess its functionality and correctness. The contributions of this paper to the current state of knowledge on LLM4SE and ethics in AI are threefold. First, we showcase the effectiveness of various techniques in enhancing trustworthiness in LLM4SE. Second, we provide empirical insights into how these techniques can be leverage in developing ethically-aligned AI-based systems. Finally, by applying a multi-faceted evaluation approach - including thematic analysis, hierarchical clustering, and direct source code execution - we provide an initial framework for assessing the reliability and trustworthiness of LLMs in software engineering. With this experiment, we hope to shed light both on how to improve trustworthiness in LLM and on how to help practitioners develop ethical AI-based systems."
https://arxiv.org/html/2410.03215v1,NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task,"In this paper, we describe our system for the WMT 24 shared task of Low-Resource Indic Language Translation. We consider eng \leftrightarrow {as, kha, lus, mni} as participating language pairs. In this shared task, we explore the finetuning of a pre-trained model motivated by the pre-trained objective of aligning embeddings closer by alignment augmentation Lin et al. (2020) for 22 scheduled Indian languages. Our primary system111Our code, models, and generated translations are available here: https://github.com/pramitsahoo/WMT2024-LRILT is based on language-specific finetuning on a pre-trained model. We achieve chrF2 scores of 50.6, 42.3, 54.9, and 66.3 on the official public test set for eng\rightarrowas, eng\rightarrowkha, eng\rightarrowlus, eng\rightarrowmni respectively. We also explore multilingual training with/without language grouping and layer-freezing.","The “Shared Task: Low-Resource Indic Language Translation” for WMT 2024 Pakray et al. (2024) extends the efforts initiated in WMT 2023 Pal et al. (2023), which garnered significant participation from the global community. Recent advancements in machine translation (MT), particularly through techniques like multilingual training and transfer learning, have expanded the scope of MT systems beyond high-resource languages Johnson et al. (2017). However, low-resource languages continue to present substantial challenges due to the scarcity of parallel data required for effective training Siddhant et al. (2020); Wang et al. (2022). The shared task focuses on low-resource Indic languages with limited data from diverse language families: Assamese (as), Mizo (lus), Khasi (kha), and Manipuri (mni). The task aims to improve translation quality for the English\LeftrightarrowAssamese, English\LeftrightarrowMizo, English\LeftrightarrowKhasi, and English\LeftrightarrowManipuri given the data provided in the constrained setting. To address the challenges inherent in translating low-resource languages, participants are encouraged to explore several strategies. First, leveraging monolingual data is essential for enhancing translation quality, especially in the absence of sufficient parallel data. Second, multilingual approaches offer the potential for cross-lingual transfer, where knowledge from high-resource languages can be applied to low-resource pairs Sen et al. (2019). Third, transfer learning provides a mechanism for adapting pre-trained models from high-resource languages to low-resource settings Wang et al. (2020). Lastly, innovative techniques tailored to low-resource scenarios, such as data augmentation and language-specific fine-tuning, are crucial for improving performance. In this paper, we describe our system for the WMT 2024 shared task, focusing on fine-tuning two pre-trained models: IndicRASP and IndicRASP Seed222These pre-trained models were developed for WAT 2024 MultiIndicMT shared task by the authors.. IndicRASP model is pre-trained with the objective of aligning embeddings inspired by alignment augmentation Lin et al. (2020) on 22 Indic languages. Our primary approach involves language-specific fine-tuning, leveraging multilingual training setups, language grouping, and layer freezing. We set up experiments in both bilingual and multilingual settings. We achieve BLEU scores of 20.1 for English\rightarrowAssamese, 19.1 for English\rightarrowKhasi, 30.0 for English\rightarrowMizo, and 35.6 for English\rightarrowManipuri on the public test set, demonstrating the effectiveness of our approach. Specifically, language-specific fine-tuning yielded significant improvements in translation quality, while multilingual setups provided balanced performance across all language pairs. Language grouping and layer freezing are effective techniques for preserving pre-trained knowledge and mitigating the challenges of multilinguality. Our results highlight the importance of tailored fine-tuning strategies for low-resource languages and show the potential of using alignment-augmented pre-trained models to improve translation quality in low-resource settings."
https://arxiv.org/html/2411.08814v1,Process-aware Human Activity Recognition,"Humans naturally follow distinct patterns when conducting their daily activities, which are driven by established practices and processes, such as production workflows, social norms and daily routines. Human activity recognition (HAR) algorithms usually use neural networks or machine learning techniques to analyse inherent relationships within the data. However, these approaches often overlook the contextual information in which the data are generated, potentially limiting their effectiveness. We propose a novel approach that incorporates process information from context to enhance the HAR performance. Specifically, we align probabilistic events generated by machine learning models with process models derived from contextual information. This alignment adaptively weighs these two sources of information to optimise HAR accuracy. Our experiments demonstrate that our approach achieves better accuracy and Macro F1-score compared to baseline models.","Analysing patterns of human activities offers a multitude of benefits across various domains, including interactive gaming, fitness activity monitoring, assisted living and healthcare monitoring (Gupta et al. 2022; Liu et al. 2022). For example, in healthcare, monitoring stroke patients’ daily activities can track their recovery progress and lead to interventions when necessary. With the rapid development of sensor technologies and smart devices, such as smartphones and cameras, large amounts of data related to human activities and behaviour are generated. Human activity recognition (HAR) techniques identify and recognise human activities based on the generated data from these sources. However, core HAR methods mainly focus on leveraging the power of machine learning (ML) and deep learning (DL) techniques to extract information from data for activity recognition (Gupta et al. 2022). These HAR methods relies solely on analysing the inherent traits and relationships within the data. Such an approach can only learn from whatever data are fed to it, but neglect the context in which data are generated. This may limit their effectiveness (Gupta and Sheng 2020), resulting in low accuracy due to inter-class similarity. For example, the activities of drinking from a cup and answering a phone involve similar arm movements (Gupta and Davis 2007). Thus, it is difficult to distinguish these similar activities based on motion data alone. To address this limitation, the idea of incorporating domain knowledge or contextual information into ML and DL methods has been investigated to improve the accuracy and interpretability (Guo et al. 2023). Humans inherently follow distinct patterns in their daily activities, influenced by established best practices and workflows. These include production processes, social norms and habitual routines, such as adhering to a recipe when preparing a meal or following a typical daily schedule. Incorporating this type of information into the traditional data-driven HAR models mentioned above can help enhance their performance. Taking the example of distinguishing between the activities of drinking from a cup and answering a phone, it is helpful to consider the typical sequence of actions associated with each activity. For instance, the process of drinking often starts with approaching the location of the cup, followed by reaching for and picking up the cup, then drinking from it and finally putting down the cup. By understanding and recognising this process, we can more accurately infer that if a series of actions match this pattern, the current activity is likely drinking from a cup rather than answering a phone. Moreover, capturing this process information poses challenges in domains where activities may not follow a strictly defined sequence. Across domains, the variability of how tasks are performed makes it difficult to define a structured process. For instance, the same activity of drinking from a cup could involve different sequences, e.g., an individual could be interrupted when they receive a text message or doorbell ringing, altering the usual sequence. Our work aims to enhance the performance of traditional HAR models by incorporating process information from context. In this paper, we propose a novel approach based on alignment conformance checking algorithm (Carmona et al. 2018; Zheng, Papapanagiotou, and Fleuriot 2024), which aligns a process model with an event log. The proposed approach learns from both the outputs of HAR models and processes information from domains. It can adaptively weights these two sources of information during the training phase to achieve the best HAR accuracy. An experimental study is investigated based on a dataset collected by cameras whilst observing people eat (Raza et al. 2023). The result shows that our approach can achieve better accuracy and Macro F1-score compared to two well-known Graph Convolutional Network algorithms for activity recognition."
https://arxiv.org/html/2411.08813v1,Rethinking CyberSecEval: An LLM-Aided Approach to Evaluation Critique,"A key development in the cybersecurity evaluations space is the work carried out by Meta, through their CyberSecEval approach. While this work is undoubtedly a useful contribution to a nascent field, there are notable features that limit its utility. Key drawbacks focus on the insecure code detection part of Meta’s methodology: we explore these limitations, and use our exploration as a test case for LLM-assisted benchmark analysis.","Meta’s insecure code methodology was first proposed in CyberSecEval 1 [1]. Since then, their work has been extended and documented in CyberSecEval 2 and 3[2; 6], however, the nature of the insecure code detection process has not changed. Meta’s methodology comprises three key components: (i) the Insecure Code Detector (ICD), a static analysis tool that flags unsafe coding practices; (ii) the Instruct Benchmark, where an LLM uses code identified by the ICD to create instruction prompts, which are then given to another LLM to test if it reproduces the same insecure practices; and (iii) the Autocomplete Benchmark, where LLMs are prompted with code leading up to an ICD-flagged insecure line to see if unsafe code is generated. We have identified limitations and nuances in all three of these areas. Our code is available here and compute details are present in Appendix F. ((a)) ((b)) Figure 1: Comparison of model scores on our adjusted benchmarks and the original CyberSecEval benchmarks. Subplot (a) shows pass percentage, the percentage of samples marked as secure, by the ICD for models originally and after removing prompts that cannot comply with the rules, and (b) reports pass percentage for models originally and after removing comments/identifiers."
https://arxiv.org/html/2411.08794v1,Evaluating World Models with LLM for Decision Making,"World model emerges as a key module in decision making, where MuZero and Dreamer achieve remarkable successes in complex tasks. Recent work leverages Large Language Models (LLMs) as general world simulators to simulate the dynamics of the world due to their generalizability. LLMs also serve as the world model for deliberative reasoning in Reasoning via Planning (RAP) and Tree of Thought (ToT). However, the world models are either evaluated as a general world simulator, or as a functional module of the agent, i.e., predicting the transitions to assist the planning. In this work, we propose a comprehensive evaluation of the world models with LLMs from the decision making perspective. Specifically, we leverage the 31 diverse environments from (Wang et al., 2023; 2024) and curate the rule-based policy of each environment for the diverse evaluation. Then, we design three main tasks, i.e., policy verification, action proposal, and policy planning, where the world models can be used for decision making solely. Finally, we conduct the comprehensive evaluation of the advanced LLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three main tasks under various settings. The key observations include: i) GPT-4o significantly outperforms GPT-4o-mini on the three main tasks, especially for the tasks which require the domain knowledge, ii) the performance of the world model with LLM will be decreased for long-term decision-making tasks, and iii) the combination of different functionalities of the world model will brings additional unstabilities of the performance.","Due to the celebrating success of MuZero (Schrittwieser et al., 2020) and Dreamer (Hafner et al., 2019; 2021; 2023), world model (Ha & Schmidhuber, 2018) becomes a key concept in decision making. With the encoding vast knowledge of the world for the prediction of the new states after taking the actions, world models have demonstrate their effectiveness for generalizing to novel tasks (Byravan et al., 2020), efficient planning with world model (Sekar et al., 2020), and working on offline datasets (Schrittwieser et al., 2021; Yu et al., 2020; 2021). World models can also be viewed as general world simulators where users can interact with, e.g., Genie (Bruce et al., 2024) and Vista (Gao et al., 2024). Large Language Models (LLMs) achieve remarkable success in enormous natural language tasks in the past five years (Brown et al., 2020; OpenAI, 2023). Several recent works leverage LLMs as the generalist world models to provide the environment knowledge for various complex tasks, e.g., math and reasoning. With the fine-tuning over pre-collected data from the environments, the LLMs can predict the action sequences across different tasks over environments while maintaining the capabilities on other domains (Xiang et al., 2023). LLMs also serve as the world model explicitly in Reasoning via Planning (RAP) (Hao et al., 2023) and Reason for Future, Act for Now (RAFA) (Liu et al., 2023), where the LLMs predict the next states based on the actions executed at current states, e.g., the states of blocks in the BlocksWorld (Valmeekam et al., 2023), which is used to assist the planning methods. On the other hand, LLMs serve as the world model implicitly in the widely-used Tree of Thoughts (ToT) (Yao et al., 2023), as well as Graph of Thoughts (GoT) (Besta et al., 2024), where the LLMs need to predict the states and evaluate the thoughts. Recent work also consider LLMs as world simulators (Wang et al., 2024; Xie et al., 2024), where they evaluate the performance of LLMs on the prediction of next states and the game progress, demonstrating the potentials of LLMs as general world models. Figure 1: Picking the action with higher value. The prediction of B in Case 2 is more accurate than Case 1 in term of the L_{2} loss, but leads to the wrong action. However, most of the previous works evaluate the world models with LLMs either as general world simulators (Wang et al., 2024), or as additional modules of the agents to make decisions (Liu et al., 2023). We argue that a comprehensive evaluation of the world models themselves from the decision-making perspective is needed due to the two facts. First, the objective of decision making is finding the policy to complete the task, where only a small portion of the world will be visited during finding and executing the policy, given the fact that AlphaZero can find the super-human policy (Silver et al., 2018) by only exploring a small proportion (less than 1%) of the state space, therefore, the evaluation of the world models on the predictions of the transitions that relevant to the desired policy is more important than the transitions far from the policy. Second, the influence of the world models is usually coupled with the actors who choose the actions, i.e., if the actor cannot pick the correct actions, the task cannot be completed even when the world model is accurate, which brings additional difficulties to understand the performances of the world models. Therefore, we address these issues with three key observations in this work: • Observation 1: Prediction is important, but not that important. An illustrative example is displayed in Figure 1, which indicates that more accurate predictions do not lead to right decisions.111The issue in Figure 1 can be elicited by various methods, e.g., rank prediction. This example is just to illustrate the discrepancy between prediction and decision, motivating us to reconsider the evaluation of the world model for decision making. This phenomenon is also observed in other decision making scenarios, e.g., financial trading (Sun et al., 2023). The success of MuZero Unplugged (Schrittwieser et al., 2021) also demonstrate that we can learn good policies from inaccurate world models which are trained only with limited data. This motivates us that the evaluation of the world models for decision making should focus on the predictions which relevant to the desired policy, rather than as general world simulators. • Observation 2: Selecting potential actions should be an important feature for world models. Most of the previous works in world model focus on next state and reward prediction, and the action selection is usually completed by separate model. Several works (Xie et al., 2024; Janner et al., 2021) incorporate the action selection into consideration. We argue that as the world model for decision making has more knowledge about the world, which can potentially make a better selection of the potential actions. World models can also be viewed as game engines (Valevski et al., 2024), which have to provide potential actions to guide fresh players to complete tasks, e.g., Red Dead Redemption 2 (Tan et al., 2024). • Observation 3: Planning with world models can find the policies solely. With the prediction of the next states and the action proposal, we can leverage planning methods or search methods to find the policies. Most works introduce the critic (i.e., the value function) to evaluate the actions immediately for efficient planning (Schrittwieser et al., 2020; Hao et al., 2023), however, the critic is not necessary for finding policies and may also influence the performance. Therefore, we should evaluate the world model solely for finding the policies to avoid the influences of other modules. Figure 2: Evaluation of World Model with LLM for Decision Making. Therefore, based on the above three key observations, we proposal a comprehensive evaluation of world models with LLMs for decision making. Specifically, we leverage 31 diverse environments from (Wang et al., 2023; 2024) with different tasks varying from daily tasks, e.g., washing clothes, to scientific tasks, e.g., forging keys, and different difficulty levels, i.e., steps to complete the tasks, and curate the rule-based policy for each environment for the evaluation. Then, we design three main tasks: i) policy verification: verifying whether the policy can complete the task, ii) action proposal: proposing the top-K actions that can potentially complete the task, and iii) policy planning: finding the policy solely with the combination of the different functionalities, i.e., policy verification and action proposal. Finally, we conduct the comprehensive evaluation of the advanced LLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three tasks under various settings. The key observations include: i) GPT-4o significantly outperforms GPT-4o-mini on the three main tasks, especially for the tasks which requires the domain knowledge, ii) the performance of the world model with LLM will be decreased for long-term decision-making tasks, and iii) the combination of different functionalities of the world model for decision making will brings unstability of the performance."
https://arxiv.org/html/2411.08728v1,Polymetis:Large Language Modeling for Multiple Material Domains,"As the application of large language models in various fields continues to expand, materials science also ushers in opportunities for AI-driven innovation. The traditional way of relying on manual search for materials science-related information is now using artificial intelligence technology as an auxiliary tool to improve the efficiency of materials science research. To accelerate researchers’ knowledge acquisition and intelligent decision-making support in materials science research, this paper proposes a large language model Polymetis model for a variety of materials fields, aiming to provide highly professional knowledge answers in the field of materials, covering energy materials, functional materials, alloy materials, physical chemistry, biology, and other material directions. The model uses a dataset of about 2 million material knowledge instructions, and in the process of building the dataset, we developed the Intelligent Extraction Large Model (IELM), which is specially used to extract and form structured knowledge from scientific texts, avoiding a large number of costs that need to be manually annotated, and improving efficiency. We inject this data into the GLM4-9B model for learning to enhance its inference capabilities in a variety of material domains. In addition, we have introduced enhanced prompt strategies to ensure that the answers to the model are more organized and comprehensive, providing efficient and comprehensive intelligent support for the diverse needs of materials science exploration, and promoting the development of material science.","Large-scale language models (LLMs) have laid a solid foundation for various applications. ChatGPT and GPT-4.0 Achiam et al. (2023), developed by OpenAI, have 175 billion and 18 trillion parameters, respectively, and have a wide range of applications in natural language processing. And the details of their training methods are not publicized. The GLM base model from Tsinghua University provides a compelling option for natural language processingDu et al. (2021); Ouyang et al. (2022). It supports both English and Chinese, with high accuracy, cross-platform compatibility, repeatability, and fast inference. Ernie 3.0Titan, introduced by Baidu, is an upgraded version of the Ernie family of models Sun et al. (2019, 2020, 2021), which employs deeper knowledge fusion techniques with tens of billions of references to support multiple language comprehension and generation tasks. The emergence of open-source alternative models, such as LLaMA Touvron et al. (2023) and RWKVPeng et al. (2023), provides a variety of options for fine-tuning the underlying large-scale language models (e.g., Alpaca Taori et al. (2023) and VicunaChiang et al. (2023)). However, most of the instruction datasets are self-generated by GPT-4, which may lead to reduced model inference accuracy Wang et al. (2022); Jablonka et al. (2023). The traditional manual extraction methods are inefficient and costly, which poses a great challenge for material science text mining Kononova et al. (2021). In materials-specific domains, LLM has shown great potential in materials science analytics, bringing significant innovations to scientific research and industrial applications. Models such as DARWIN SERIES Xie et al. (2023) and MatChat Chen et al. (2023) have demonstrated the potential of AI to assist in the materials domain, providing researchers with accelerated access to domain-specific information Weston et al. (2019) and providing knowledge extraction and discovery processes Venugopal et al. (2021); Fang et al. (2023). However, it has some minor drawbacks. The DARWIN model suffers from catastrophic forgetting problems, the generalization ability is greatly reduced, and the answers lack organization and precision, in addition, DARWIN uses multiple models instead of a unified model which may lead to complexity and potential inefficiency. As for MatChat, its first limitation for Chinese users is that it only supports English usage and response accuracy is a tricky issue when dealing with unorganized datasets. Although these models have made important contributions to the field of materials science, they generally suffer from several limitations: first, catastrophic forgetting problems and lack of generalization capabilities. Second, the downstream tasks performed by these models are more limited and cannot accommodate the multi-domain materials knowledge dialog. These issues make existing models often fail to meet the needs of researchers when dealing with more complex and diverse material knowledge queries. To overcome these challenges, we propose Polymetis, a multidisciplinary materials science-oriented large language model designed to provide specialized knowledge support covering multiple directions such as energy materials, functional materials, alloy materials, physical chemistry, biomaterials, etc. Polymetis uses a dataset of about 2 million knowledge instructions in the materials domain and utilizes our self-developed Intellectual Extractive Large Model (IELM) model is used to automatically extract and process structured material domain knowledge from material science literature, avoiding the need for large amounts of manual annotation and improving the efficiency of dataset construction. The advantages of Polymetis are: a) First, we developed the IELM model, through which automated knowledge extraction and data processing techniques enable the model’s training dataset to be rapidly extended to multiple material domains, improving the broad applicability of the Polymetis model; b) We utilize the GLM4-9B model for parameter-efficient Lora fine-tuning and introduce an enhanced prompt strategy in order to improve the organization and accuracy of the trained model’s inference results. Using the benchmark answers provided by materials science experts for comparative evaluation, Polymetis demonstrates excellent complex instruction understanding and multi-domain reasoning capabilities, which can provide researchers with accurate and efficient materials knowledge exploration for their needs."
https://arxiv.org/html/2411.08684v1,Analogical Reasoning Within a Conceptual Hyperspace,"We propose an approach to analogical inference that marries the neuro-symbolic computational power of complex-sampled hyperdimensional computing (HDC) with Conceptual Spaces Theory (CST), a promising theory of semantic meaning. CST sketches, at an abstract level, approaches to analogical inference that go beyond the standard predicate-based structure mapping theories. But it does not describe how such an approach can be operationalized. We propose a concrete HDC-based architecture that computes several types of analogy classified by CST. We present preliminary proof-of-concept experimental results within a toy domain and describe how it can perform category-based and property-based analogical reasoning.","“Analogies are partial similarities between different situations that support further inferences.” Gentner (1998) The well-known formulation \mathit{A}:\mathit{B}::\mathit{C}:\mathit{X} (1) represents an analogy. Analogical reasoning entails many key aspects of human cognition and involves several key processes: retrieval (given \mathit{C}, find \mathit{A} and \mathit{B}), mapping (determine a structural correspondence between \mathit{A} and \mathit{B} to find \mathit{X}, by applying the correspondence to \mathit{C}), and inference (using \mathit{A} to advance the concept \mathit{C}). In this paper, we focus on the task of mapping – more specifically, the task of identifying a relationship between \mathit{A} and \mathit{B}, and then applying the identified relationship to characterize \mathit{X}. The particular challenge with mapping is that there is often a large number of potential relationships between \mathit{A} and \mathit{B}, and these relationships may themselves be compositional and graded in nature, as well as span the symbolic/sub-symbolic representational divide. Finding the salient relationships – the ones between \mathit{A} and \mathit{B} relevant to \mathit{C} – is a combinatorially hard problem. Approaches to solving the mapping problem have been either connectionist or symbolic, and both types of models attempt to identify structural correspondence (graph isomorphisms) between concepts Gayler and Levy (2009). Connectionist approaches such as ACME and DRAMA Eliasmith and Thagard (2001) are essentially “localist” Page (2000) in nature, which means concept representations/symbols, although connected within networks, remain localized to single nodes. Purely symbolic approaches, on the other hand (such as Structure Mapping Theory Gentner (1983); Crouse et al. (2021)), explicitly incorporate the geometric graph structure using a predicate-based representation. Both types of analogy engines remain constrained by their representations. Localist-connectionist approaches require a decomposition and sequential symbolic traversal of the source and target structures, substantially increasing their time complexity Gayler and Levy (2009). Purely symbolic approaches struggle to compute analogies that need to isolate salient conceptual properties of objects Gärdenfors and Osta-Vélez (2023). Salience requires a distance metric that just does not exist within a symbol-dominated space. Neural networks offer more of a “distributed” connectionist approach. Unfortunately, however, their concept embeddings do not necessarily possess the structural and compositional aspects with which to perform analogical mapping. Moreover, neural network models fail to generate many types of analogies outside the distributions that characterize their training sets, and they fail to provide the underlying hierarchical structure to support analogical inference Pavlus (2021); Lewis and Mitchell (2024). A cognitive framework with the ability to simultaneously and seamlessly represent these so-far mutually exclusive connectionist and symbolic computational paradigms has been lacking Lieto et al. (2017). More recently, researchers have explored hyperdimensional computing (HDC), synonymously known as vector-symbolic architecture (VSA)111To ensure clarity, we will use the term hyperdimensional computing or HDC in the rest of this paper. as a paradigm for capturing a number of neurally plausible cognitive phenomena, including analogical mapping Hersche et al. (2023); Kanerva (1997); Plate (2003); Gayler (2004); Blouw et al. (2016). HDC is a representational and inferential paradigm in which data structures can be represented with high-dimensional vectors, thus bridging the symbolic/subsymbolic gap. The study of analogical inference using HDC is relatively new. Many open questions exist. In Maudgalya et al. (2020), for example, the salient aspects of the A:B relationship were given, but it was assumed that the structure underlying A, B and C were binary. The question of how to identify more salient aspects of structure and allow for a more nuanced or graded characterization of concepts remains open. In this paper, we begin answering this question with a crucial insight: that the HDC framework could operationalize the cognitive science theory of Conceptual Spaces Gärdenfors (2000). For over a quarter century, Conceptual Spaces Theory (CST) has been an attractive guide for research in cognitive science Douven and Gärdenfors (2019), neuroscience Bellmund et al. (2018), and artificial intelligence Zenker and Gärdenfors (2015). Its strength lies in its inclusive ability to model how sensory observations flow through an agent’s initial connectionist layer of observation processing, through a layer of geometric concept space, and ultimately into a form of symbolic representation often used for reasoning – thus sharing many properties with HDC. CST offers guidance on analogical mapping. Under CST, analogical inference requires a distance metric within a concept space. This requirement implies at least five necessary and simultaneously present capabilities of an agent Osta-Vélez and Gärdenfors (2022): 1. It must continuously accept sensory observations in real-time and encode these signals into working memory; 2. It must afford an algebraic calculus over distance metrics within working memory; 3. It must afford a logical calculus in working memory; 4. It must cross-reference the same concepts during sensory observation, geometric processing, and symbolic processing; 5. It must provide interaction with long-term memory in an associative capacity. Neither structure mapping nor neural architectures alone support these five requirements. We argue that HDC can support these requirements and provide concrete guidance for how concepts can be represented and analogized. We call this model a “conceptual hyperspace.” CST offers a set of algorithms to solve several types of analogies Osta-Vélez and Gärdenfors (2022). In this paper, we provide a proof-of-concept for how these algorithms can be implemented with HDC: how concepts can be encoded, how salient relationships can be identified, and how mapping can be performed. We also perform experiments in toy domains to illustrate the approach."
https://arxiv.org/html/2411.08599v1,XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL,"To tackle the challenges of large language model performance in natural language to SQL tasks, we introduce XiYan-SQL, an innovative framework that employs a multi-generator ensemble strategy to improve candidate generation. We introduce M-Schema, a semi-structured schema representation method designed to enhance the understanding of database structures. To enhance the quality and diversity of generated candidate SQL queries, XiYan-SQL integrates the significant potential of in-context learning (ICL) with the precise control of supervised fine-tuning. On one hand, we propose a series of training strategies to fine-tune models to generate high-quality candidates with diverse preferences. On the other hand, we implement the ICL approach with an example selection method based on named entity recognition to prevent overemphasis on entities. The refiner optimizes each candidate by correcting logical or syntactical errors. To address the challenge of identifying the best candidate, we fine-tune a selection model to distinguish nuances of candidate SQL queries. The experimental results on multiple dialect datasets demonstrate the robustness of XiYan-SQL in addressing challenges across different scenarios. Overall, our proposed XiYan-SQL achieves the state-of-the-art execution accuracy of 89.65% on the Spider test set, 69.86% on SQL-Eval, 41.20% on NL2GQL, and a competitive score of 72.23% on the Bird development benchmark. The proposed framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods.Keywords LLM, Text-to-SQL, NL2SQL","The ability to convert natural language queries into structured query language (SQL) through natural language to SQL (NL2SQL) technology represents a significant advancement in making complex datasets more accessible. It greatly facilitates both non-expert and advanced users in extracting valuable insights from extensive data repositories [2, 15, 24, 27, 6, 10, 13, 29, 20, 19, 23, 22]. Recent advancements in large language models (LLMs) have significantly enhanced the efficacy and accuracy of NL2SQL applications. There are generally two approaches for NL2SQL solutions based on LLMs: prompt engineering [3, 5, 17, 18], and supervised fine-tuning (SFT) [9]. Prompt engineering leverages the intrinsic capabilities of the model by optimizing prompts to generate diverse SQL queries. Prompt engineering has demonstrated promising results in NL2SQL using zero-shot [3] or few-shot prompting [28, 5, 18]. This type of approach typically employs closed-source models with enormous parameters, such as GPT-4 [1] and Gemini 1.5 [26], which present significant potential and powerful generalization capability. However, most methods rely on multi-path generation and selecting the best option utilizing self-consistency, resulting in significant inference overheads. Approaches based on SFT seek to fine-tune models with much smaller parameter sizes on the NL2SQL task to produce more controllable SQL queries, such as CodeS [9]. Nevertheless, due to their limited parameters, these methods struggle to perform complex NL2SQL reasoning and transfer to databases within a new domain. In this technical report, we propose XiYan-SQL, a novel NL2SQL framework that employs a multi-generator ensemble strategy to enhance candidate generation. XiYan-SQL combines prompt engineering and the SFT method to generate candidate SQL queries with high quality and diversity. To enhance high quality, we take advantage of the high controllability of SFT and utilize a range of training strategies to specifically fine-tune models to generate candidates with different preferences. We introduce a two-stage multi-task training approach, which first activates the model’s fundamental SQL generation capabilities, and subsequently transitions to a model with enhanced semantic understanding and diverse stylistic preferences. To enhance diversity of generated candidates and capability of generating complex SQL queries, we utilize in-context learning to prompt LLMs. We propose to extract the skeleton of the questions by masking the named entities with common special tokens and using skeleton similarity to select and organize useful examples. Then, each generator is followed by a refiner to correct logical or syntactical error based on execution results or error information. Finally, a selection agent is required to select the best option. Most existing works use self-consistency, but the most consistent result is not always the correct case. So we propose to fine-tune a model to understand and identify the subtle differences among candidates and pick the final response. Additionally, to enhance LLMs for better understanding of the database schema, we propose a new schema representation method named M-Schema. Inspired by MAC-SQL Schema [28], M-Schema presents the hierarchical structure between databases, tables, and columns in a semi-structured form. We revised MAC-SQL Schema by adding data types and resulting in a more compact and clear format. We conduct experiments to compare the impact of different schema representations on NL2SQL performance. In comparison to DDL Schema and MAC-SQL Schema, LLMs using M-Schema demonstrate superior performance. We present comprehensive evaluations on both relational and non-relational databases, specifically focusing on prominent systems such as SQLite, PostgreSQL, and nGQL. XiYan-SQL demonstrates remarkable performance across a range of benchmarks, achieving the state-of-the-art performance on the Spider [32], SQL-Eval, and NL2GQL [33] datasets with 89.65%, 69.86%, and 41.20% execution accuracy, respectively. In the context of the more challenging Bird [10] benchmark, XiYan-SQL also achieves a competitive score of 72.23%. The impressive results achieved on various challenging NL2SQL benchmarks not only validate the effectiveness of our approach but also demonstrate its significant potential for broader applications in NL2SQL translation tasks. XiYan-SQL can be accessed from https://bailian.console.aliyun.com/xiyan. We also release the source code for connecting to the database and building M-Schema at https://github.com/XGenerationLab/M-Schema."
https://arxiv.org/html/2411.08563v1,Leveraging LLMs for Predictive Insights in Food Policy and Behavioral Interventions,"Food consumption and production contribute significantly to global greenhouse gas emissions, making them crucial entry points for mitigating climate change and maintaining a liveable planet. Over the past two decades, food policy initiatives have explored interventions to reshape production and consumption patterns, focusing on reducing food waste and curbing ruminant meat consumption. While the evidence of ""what works"" improves, evaluating which policies are appropriate and effective in specific contexts remains difficult due to external validity challenges. This paper demonstrates that a fine-tuned large language model (LLM) can accurately predict the direction of outcomes in approximately 80% of empirical studies measuring dietary-based impacts (e.g. food choices, sales, waste) resulting from behavioral interventions and policies. Approximately 75 prompts were required to achieve optimal results, with performance showing signs of catastrophic loss beyond this point. Our findings indicate that greater input detail enhances predictive accuracy, although the model still faces challenges with unseen studies, underscoring the importance of a representative training sample. As LLMs continue to improve and diversify, they hold promise for advancing data-driven, evidence-based policymaking.Keywords: behavioral public policy; food policy; food waste; large language models","1 Main Food consumption and production, accounting for one-quarter to one-third of global greenhouse gas emissions, are key entry points for mitigating climate change, reducing mortality and morbidity, and keeping the world within liveable planetary boundaries (Crippa et al., 2021). Over the past decade, food policy has been at the forefront of sustainability efforts, exploring various interventions to reshape production and consumption patterns and stimulate dietary shifts. These initiatives, including those targeting greenhouse gas emissions in the food system, have significantly advanced our understanding of which interventions are most effective in shifting individual behavior (Lohmann et al., 2024b). Today, climate scientists concur that the most effective approach is to focus on two critical behavioral strategies: reducing food waste and loss (FWL) (Gatto & Chepeliev, 2024; Zhu et al., 2023) and curbing ruminant meat consumption (Li et al., 2024). Just focusing on these two would lead to substantial strides in mitigating food-related emissions (Geyik et al., 2023; Crippa et al., 2021; Xu et al., 2021; Yang et al., 2024). The challenge for policymakers aiming to mitigate climate change is to achieve significant and lasting behavior change. Various policy instruments are available to help drive such changes, from mandates and bans to financial incentives and disincentives, from information and education to behaviorally informed interventions using choice architecture (Ammann et al., 2023). Evidence reviews on the effectiveness of such interventions guide policymakers towards the most effective yet acceptable policies (Lohmann et al., 2024b; Reisch et al., 2021). Behaviorally informed food policies seem well-suited to be included in policy bundles since they can be effective, easy to implement, and often more acceptable to the public than hard policies like taxes and bans (Reisch, 2021). Moreover, they can be applied to a multitude of food system actors at individual and system levels (ibid.). With all these options at hand, how can policymakers evaluate which policies are the most appropriate and effective to tackle a specific policy problem (such as reducing food waste) in a particular setting (such as a public canteen run by a municipality in the UK) for a specific target group (such as vulnerable young consumers)? There are notable practical challenges as the effectiveness of intervention will vary depending on intervention levels, countries, and settings. While the scientific literature and evidence synthesis networks increasingly produce helpful, systematic reviews and meta-regressions of hundreds or even thousands of studies, traditional approaches are resource-intensive, time-consuming, and hence quickly outdated in today’s era of rapid data analysis (Egger et al., 2003). Systematic maps and reviews of case studies can inspire by showing the scope of possible policies. Still, they can hardly be generalized due to the heterogeneity of target populations and the power of context. Indeed, socio-economic, social, and cultural contexts and societal conditions, such as high or low trust in government, heavily influence the success of interventions; sometimes, the outcomes are not even in the predicted direction. In a nutshell, predicting policy outcomes is difficult, and external validity remains a serious challenge (Findley et al., 2021). While the idea of conducting mega studies seems to address some of these challenges, particularly concerning external validity (Milkman et al., 2021; Duckworth & Milkman, 2022), the primary practical issues—namely, cost and time to conduct experimental studies—remain unresolved. Given the complexity and numerous factors influencing the success of interventions, there is a growing need to identify more efficient methods for generating reliable predictions and supporting informed decision-making for researchers and policymakers. Artificial intelligence, particularly large language models (LLMs), offers new possibilities due to their ability to process vast amounts of data and identify patterns with much lower resource and time costs. Originally designed for text, LLMs now excel in various tasks, including coding (Coello et al., 2024), symbolic mathematics (Ahn et al., 2024), and scientific reasoning (AI4Science & Quantum, 2023). There is increasing evidence that LLMs can be utilized for predictive tasks beyond their original design, such as stock market predictions (Lopez-Lira & Tang, 2023) and even more general financial tasks (Wu et al., 2023; Xie et al., 2023; Deng et al., 2023). Additionally, LLMs have been employed to simulate complex daily behaviors and interactions—such as cooking or commuting— and may offer a realistic approach to agent-based modelling architectures (Park et al., 2023). Moreover, LLMs have shown promising results in simulating classical economic, psycholinguistic, and social-psychological experiments, such as the Ultimatum Game or the Wisdom of Crowds Aher et al. (2023). Horton (Horton, 2023) further demonstrates how LLMs, functioning as computational representations of human decision-making (or ""homo silicus""), can be integrated into economic and social science simulations by endowing the simulated agents with specific sets of information and preferences. Given these findings, it is unsurprising that there is a growing debate about whether, and to what extent, LLMs could replace humans in specific research tasks—particularly in psychological and behavioral studies in the near future Dillion et al. (2023)—or be used to provide a more cost-effective adjustment of behavioral policies Meng (2024). A necessary condition for LLMs to simulate experiments correctly — or even whole survey responses (Namikoshi et al., 2024) — is that previous experimental findings used for training are of high quality and replicable, ensuring they exhibit some external validity. To address this, statistical models (Yang et al., 2020) are increasingly employed to predict the replicability of experimental results in social science labs—an essential requirement for scientific progress in general (Camerer et al., 2018), specifically when using LLMs to predict the outcomes of specific policy interventions. These models have shown that factors such as sample size and effect sizes in the original studies strongly predict successful replication (Altmejd et al., 2019). Despite these advances, there remain fundamental areas for improvement in the applicability of LLMs within the social sciences, including behavioral economics and cognitive psychology. These include a need for broader access to high-quality research tools and a deeper understanding of the underlying social forces that guide behavior (Bail, 2024). Demszky et al. (Demszky et al., 2023) echo this sentiment, identifying critical gaps in the field, such as the lack of ""keystone"" datasets, standardized performance benchmarks, and shared computational infrastructure. Here, we introduce PREDICT (Predictive Algorithm for Assessing External Validity and Identifying Contextual Tailored Interventions), an LLM-based decision support tool designed to address the foregoing challenges. Leveraging the ability to adjust a pre-trained LLM with context-specific and task-specific data (called fine-tuning (Howard & Ruder, 2018)), PREDICT offers new possibilities for predicting the outcomes of food-related policy interventions by processing vast amounts of data and identifying underlying patterns. As highlighted earlier, LLMs enable researchers to explore behavioral scenarios through simulations and uncover new insights, which can later be validated through real-world studies. Hence, we are exploring whether LLMs can accurately predict behavioral policy outcomes. We fine-tune a GPT-3.5 Turbo model on a comprehensive dataset of food-based interventions, encompassing 74 published scientific articles and over 200 effect sizes. Each experiment reports an average of 11,000 observations, resulting in a total of approximately 2.2 million observations measured across all included experiments. Additionally, we validate the predictive capabilities of our model by comparing predictions to 12 ongoing experiments that have not yet been published. We fine-tune the model using variations in prompt styles, dataset sizes, and prompt features, and find that the fine-tuned models can accurately predict statistical parameters, such as effect direction, correlation coefficients, and effect sizes, with effect direction accuracy reaching nearly 80% in specific cases. Our findings shed light on how prompt style and training parameters affect prediction confidence, as well as the importance of specific dataset features in enhancing model performance. While LLM models continue to advance, our study contributes a significant proof of methodology in food policy research. We also aim to set a foundation for similar techniques applied in other policy research areas. In principle, LLM models could be used to identify potential solutions to a wide range of challenges, including highway safety, tobacco smoking, alcohol abuse, and take-up of beneficial programs."
https://arxiv.org/html/2411.08544v1,Deeper Insights into Learning Performance of Stochastic Configuration Networks,"Stochastic Configuration Networks (SCNs) are a class of randomized neural networks that integrate randomized algorithms within an incremental learning framework. A defining feature of SCNs is the supervisory mechanism, which adaptively adjusts the distribution to generate effective random basis functions, thereby enabling error-free learning. In this paper, we present a comprehensive analysis of the impact of the supervisory mechanism on the learning performance of SCNs. Our findings reveal that the current SCN framework evaluates the effectiveness of each random basis function in reducing residual errors using a lower bound on its error reduction potential, which constrains SCNs’ overall learning efficiency. Specifically, SCNs may fail to consistently select the most effective random candidate as the new basis function during each training iteration. To overcome this problem, we propose a novel method for evaluating the hidden layer’s output matrix, supported by a new supervisory mechanism that accurately assesses the error reduction potential of random basis functions without requiring the computation of the Moore-Penrose inverse of the output matrix. This approach enhances the selection of basis functions, reducing computational complexity and improving the overall scalability and learning capabilities of SCNs. We introduce a Recursive Moore-Penrose Inverse-SCN (RMPI-SCN) training scheme based on the new supervisory mechanism and demonstrate its effectiveness through simulations over some benchmark datasets. Experiments show that RMPI-SCN outperforms the conventional SCN in terms of learning capability, underscoring its potential to advance the SCN framework for large-scale data modeling applications.","Neural Networks (NNs) have achieved significant advancements in recent decades, primarily due to their universal approximation capabilities for modeling complex nonlinear mappings[1, 2, 3, 4] and their effectiveness in learning from data. Despite the strengths, traditional gradient-based training methods, such as Backpropagation (BP), face challenges including high computational costs, vulnerability to local minima, and sensitivity to hyperparameters. In contrast, randomized learning algorithms [5, 6, 7, 8, 9, 10] offer substantial advantages for constructing fast learner models with significantly lower computational costs. Randomized training typically follows a two-step paradigm: first, random basis functions are generated, with input weights and biases drawn from a specified distribution; second, the output weights are optimized using least squares. Igelink and Pao [5] provided a theoretical justification for Random Vector Functional-links (RVFLs), demonstrating that when input weights and biases are randomly selected from a uniform distribution centered at zero and the output weights are optimized via least squares, RVFLs act as universal approximators in a probabilistic sense. The underlying idea is that if the target function can be represented as the integral of parametrized basis functions, then randomly sampling parameters from appropriate distributions can yield an accurate approximation. Recently, Needell et.al.[11] revisited and extended this significant result, using a concentration inequality to bound the accuracy of Monte-Carlo integral approximations in non-asymptotic settings. Compared with gradient-based training methods, randomized learning algorithms present a distinctive advantage: by employing least squares optimization, they avoid the computational complexity associated with iterative gradient-based searches, presenting a promising solution for building fast and efficient learner models. For practical implementations, RVFL requires two parameters: the number of hidden nodes and the radius of the uniform distribution’s support. Tyukin and Prokhorov [12] and Li and Wang [13] have argued that these parameters should be data-dependent. Without a supervisory mechanism to adaptively adjust the distribution’s support, the approximation capability of RVFL may be constrained, potentially resulting in limited learning and generalization performance. Stochastic Configuration Networks (SCNs), introduced by Wang and Li in [14], are a class of randomized learning models within an incremental learning framework. In each training iteration, input weights and biases are randomly selected from uniform distributions to generate candidate random basis functions. SCNs then evaluate the effectiveness of each random basis function in reducing the training residual error using the inequality \|e_{L}\|^{2}\leq r\|e_{L-1}\|^{2}, where r represents the learning rate and L denotes the number of hidden nodes. If none of the candidates satisfy the inequality constraint, the support of the uniform distribution is expanded to include more suitable functions. This adaptive expansion enables SCNs to explore a broader function space, aligning the generated random basis functions more closely with the data. Unlike Random Vector Functional-links (RVFL), where the configuration of random basis functions is entirely random, SCNs adaptively select random basis functions based on the training samples and current residuals, demonstrating greater flexibility and learning capacity. Due to their efficiency and effectiveness in large-scale data modeling, SCNs have attracted considerable research interest [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], encompassing both theoretical advancements and industrial applications. For instance, Wang and Dang [15, 16, 17, 18, 19, 20, 21, 22, 23, 24] proposed Recurrent Stochastic Configuration Networks (RSCN), which extended supervised randomized learning techniques to recurrent neural networks. Additionally, the 2D-SCN, introduced in [27], optimizes two-dimensional input representations to enhance performance in image data modeling. Similar to [26], the deep SCN proposed in [19] aims to reduce reliance on Backpropagation (BP) for training deep neural networks. Successful industrial applications of SCNs have also been documented [21, 22, 23, 24, 25, 26], underscoring their adaptability and efficiency in handling large-scale data in real-world settings. An essential aspect of research on SCNs, and incremental learning with neural networks more broadly, is accurately evaluating the effectiveness of a new basis function in reducing residual error. This evaluation is pivotal, as it directly influences the selection of new basis functions and consequently dictates key factors such as training convergence, learning capacity, and model compactness. Despite extensive research efforts on the supervisory mechanism for enhancing the effectiveness and efficiency of SCNs [16, 17, 18], a fundamental issue remains: the estimate of the residual error \|e_{L}\|, used to evaluate a basis function g_{L}’s effectiveness in reducing the existing residual error \|e_{L-1}\|, is inconsistent with the actual residual error observed after g_{L} is added. This discrepancy suggests that SCNs may not consistently select the most effective random basis functions, potentially resulting in slower convergence. Furthermore, a key advantage of SCNs over RVFL lies in the adaptability for adjusting uniform distributions, which directly depends on accurately assessing the effectiveness of random basis functions from the distribution. Consequently, imprecise evaluations might constrain SCNs’ overall learning capacity. In this paper, we systematically investigate the supervisory mechanism of Stochastic Configuration Networks (SCNs) and analyze its effects on the learning performance of SCN-I and SCN-III, two primary SCN algorithms. The main contributions of this study are summarized as follows: 1) We propose a new calculation method for [H_{L-1},h]^{\dagger}Y based on the recursive Moore-Penrose inverse, where H_{L-1} is the output matrix of the current hidden layer with L-1 basis functions, h is the output of a candidate basis function g, and Y represents the training output. This method enables fast computation of the residual Y-[H_{L-1},h][H_{L-1},h]^{\dagger}Y by avoiding the direct calculation of [H_{L-1},h]^{\dagger}, thereby providing a rapid and accurate assessment of g’s effectiveness in reducing training residuals. This approach significantly enhances the selection of basis functions, as well as the scalability and efficiency of SCNs. 2) Building on this efficient calculation method, we establish necessary and sufficient conditions for the training residual errors \{e_{L}\} to satisfy \|e_{L}\|\leq\sqrt{r}\|e_{L-1}\| for SCN-III, ensuring that: \lim_{L\to\infty}\sup\frac{\|e_{L}\|}{\|e_{L-1}\|}\leq\sqrt{r}, (1) confirming that the sequence of training residual errors converges with order one at a rate of at most \sqrt{r}. 3) Based on this new inequality constraint, we propose the Recursive Moore-Penrose Inverse SCN (RMPI-SCN) training scheme. Additionally, we incorporate a hyperparameter \alpha to control the increase of r with network size, allowing dynamic adjustments in the learning rate to improve flexibility and convergence efficiency. Simulation results across various datasets show that RMPI-SCN outperforms the original SCNs in both learning and testing. The rest of the paper is organized as follows. Section 2 reviews the basic concepts of SCNs and discusses the differences between SCN-I and SCN-III. Section 3 presents the necessary and sufficient conditions for the training residual errors to satisfy \|e_{L}\|\leq\sqrt{r}\|e_{L-1}\| in SCN-III, based on the recursive calculation of the Moore-Penrose inverse of the output matrix. This section also details the new supervisory mechanism and the RMPI-SCN. Section 4 exhibits the simulation results, and Section 5 concludes the paper."
https://arxiv.org/html/2411.08514v1,Explainers’ Mental Representations of Explainees’ Needs in Everyday Explanations,"In explanations, explainers have mental representations of explainees’ developing knowledge and shifting interests regarding the explanandum. These mental representations are dynamic in nature and develop over time, thereby enabling explainers to react to explainees’ needs by adapting and customizing the explanation. XAI should be able to react to explainees’ needs in a similar manner. Therefore, a component that incorporates aspects of explainers’ mental representations of explainees is required. In this study, we took first steps by investigating explainers’ mental representations in everyday explanations of technological artifacts. According to the dual nature theory, technological artifacts require explanations with two distinct perspectives, namely observable and measurable features addressing “Architecture” or interpretable aspects addressing “Relevance”. We conducted extended semi structured pre-, post- and video recall-interviews with explainers (N=9) in the context of an explanation. The transcribed interviews were analyzed utilizing qualitative content analysis. The explainers’ answers regarding the explainees’ knowledge and interests with regard to the technological artifact emphasized the vagueness of early assumptions of explainers toward strong beliefs in the course of explanations. The assumed knowledge of explainees in the beginning is centered around Architecture and develops toward knowledge with regard to both Architecture and Relevance. In contrast, explainers assumed higher interests in Relevance in the beginning to interests regarding both Architecture and Relevance in the further course of explanations. Further, explainers often finished the explanation despite their perception that explainees still had gaps in knowledge. These findings are transferred into practical implications relevant for user models for adaptive explainable systems.","For a long period of time, XAI research focused solely on technological aspects - for example, creating algorithms as a foundation for XAI. In recent XAI ††footnotetext: This is a full paper version of: Schaffer, M. E., Terfloth, L., Schulte, C., Buhl, H. M.: Perception and consideration of the explainees’ needs for satisfying explanations. Joint Proceedings of the xAI-2024 Late-breaking Work, Demos and Doctoral Consortium. 3793, 17-24 (2024). research, this predominantly technological perspective shifted toward a perspective that emphasizes the importance of incorporating human factors in the form of a socio-technological approach [41]. As research in this regard is still at an early stage [23], stepping up efforts to accelerate development of mechanisms that perceive and react to human needs for person-specific and adaptive explainable systems is a necessity. Thus far, numerous studies have not considered if given explanations meet the needs and expectations of end users [21, 33]. It can be concluded that XAI would benefit from increased consideration of human end users, thereby implying that XAI should recognize these individuals with their specific needs in specific contexts [21, 33, 34, 38, 41]. These individuals require different explanations [38], and XAI should be able to understand why the individual requires a specific explanation [11]. Enabling XAI to adapt to specific needs and requests will enable customized explanations [23] that are of greater benefit for the end user. Imagine, for example, a medical setting in which the end user could be a practicing physician, a medical student, or even a patient. Every end user will utilize the XAI system in a slightly different manner and for different reasons, thereby altering the foci and context in which the XAI system is used. According to Ribera and Lapedriza [38], user-centered XAI should aim to answer the following questions: Why does something need to be explained? What needs to be explained? How does it need to be explained? And who does it need to be explained to? Imagine someone requires an explanation on how to use an app vs. on how to alter the algorithms of an app. For this, XAI should have a component that contains information regarding human end users as well as their developing knowledge and interests. To be able to develop such a component, it has to be understood how mental representations of humans evolve and which information they contain. Therefore, in this empirical study, we investigated everyday explanations among human interlocutors to understand the role of mental representations the explaining person has of the other interlocutor. It is of special interest to investigate what the explainer learns about the developing knowledge and interests of the other interlocutor. The study aims at identifying aspects within the mental representations that are helpful to develop advanced XAI with the ability to react to the needs of users. This paper also includes practical implications for structuring synthetic explanations. In the following account, we first provide a theoretical basis, where we take a closer look at explanations and the characteristics of objects of explanations. Thereafter, we delve into mental representations of knowledge and interests before we segue to the research questions."
https://arxiv.org/html/2411.08469v1,"Building Trustworthy AI: Transparent AI Systems via Large Language Models, Ontologies, and Logical Reasoning (TranspNet)","Growing concerns over the lack of transparency in AI, particularly in high-stakes fields like healthcare and finance, drive the need for explainable and trustworthy systems. While Large Language Models (LLMs) perform exceptionally well in generating accurate outputs, their ""black box"" nature poses significant challenges to transparency and trust. To address this, the paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs. By leveraging domain expert knowledge, retrieval-augmented generation (RAG), and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet enhances LLM outputs with structured reasoning and verification. This approach ensures that AI systems deliver not only accurate but also explainable and trustworthy results, meeting regulatory demands for transparency and accountability. TranspNet provides a comprehensive solution for developing AI systems that are reliable and interpretable, making it suitable for real-world applications where trust is critical.","Symbolic AI, a fundamental branch of artificial intelligence, focuses on using structured representations of knowledge and formal logic to simulate human reasoning, problem-solving, and decision-making processes [27]. Unlike data-driven approaches such as machine learning and Large Language Models (LLMs), which derive patterns from vast amounts of unstructured data, symbolic AI models intelligence is based on rule-based systems that explicitly define rules, relationships, and logical structures. These structures include formal logic, ontologies, and semantic networks, enabling symbolic AI to work with well-defined rules to draw logical conclusions and make interpretable decisions [7]. A core component of symbolic AI is knowledge representation, where symbols denote real-world entities and their relationships. Representations in symbolic AI often employ hierarchical or graph-based structures such as semantic networks or ontologies [1, 15]. In contrast, LLMs GPT4 [22] and BERT [6], while highly effective in generating contextually relevant responses, face challenges in offering the same level of explainability due to their ""black box"" nature [32]. This gap is particularly concerning in high-stakes domains like healthcare, finance, and legal reasoning, where trust and transparency are paramount. This gap between the complexity of LLMs and the demand for transparency poses a significant challenge for AI development, particularly given the legal requirements imposed by regulations like the EU’s General Data Protection Regulation (GDPR), which grants individuals the right to receive an explanation when subjected to automated decision-making processes [14]. In addition, the proposed AI Act in the European Union, for example, mandates that AI systems—particularly those relying on LLMs in high-risk applications—be designed with human oversight, transparency, and risk management at their core [10]. One of the key barriers to achieving trustworthiness with LLMs is the inherent uncertainty in their predictions, which can be influenced by factors such as noisy data, biases in training data, or insufficient training on edge cases. Addressing these uncertainties while maintaining transparency and trustworthiness is a critical challenge in developing LLM-based systems for real-world applications [23]. To address this, systems like the proposed TranspNet pipeline combine the strengths of LLMs with symbolic AI by integrating domain expert knowledge, retrieval-augmented generation (RAG), and formal reasoning frameworks like Answer Set Programming (ASP). This hybrid approach allows LLMs to benefit from the structured, logical reasoning of symbolic AI, ensuring that their outputs are not only accurate but also explainable and trustworthy [19]. Ontologies play a key role in this integration, providing a framework for verifying LLM outputs and enhancing their transparency, a characteristic central to symbolic AI [27]. By combining the power of LLMs with formal reasoning, retrieval-augmented generation, multimodal data processing, and robust documentation practices, our pipeline addresses the key challenges associated with explainability and trustworthiness in AI systems. The integration of formal logic through ASP further distinguishes our approach by providing a mechanism for verifying the logical soundness of the LLM’s outputs and addressing uncertainty in a structured and interpretable manner. Ultimately, our pipeline offers a comprehensive solution for developing LLM-based AI systems that are not only accurate but also explainable and trustworthy, meeting the needs of both academic research and industry applications."
https://arxiv.org/html/2411.08464v1,Crystal Structure Generation Based On Material Properties,"The discovery of new materials is very important to the field of materials science. When researchers explore new materials, they often have expected performance requirements for their crystal structure. In recent years, data-driven methods have made great progress in the direction plane of crystal structure generation, but there is still a lack of methods that can effectively map material properties to crystal structure. In this paper, we propose a Crystal DiT model to generate the crystal structure from the expected material properties by embedding the material properties and combining the symmetry information predicted by the large language model. Experimental verification shows that our proposed method has good performance.","Material science plays a crucial role in the development of modern technology and industrial production, with high-performance materials serving as the foundation for the manufacture of various advanced equipment. The generation of crystal structures is a central process driving the advancement of material scienceYao et al. (2023). As periodic materials, crystals are widely used in many important fields, including catalysts, alloys, and molds. In recent years, data-driven methods have made great progress in the task of crystal structure generation (Nouira et al. (2018); Hoffmann et al. (2019); Hu et al. (2020); Ren et al. (2022)). Among various methods, diffusion model-based methods have been shown to be particularly effective in generating realistic and diverse crystal structures (Xie et al. (2021); Jiao et al. (2024a); Jiao et al. (2024b); Ye et al. (2024)). These methods use random processes to gradually transform random initial states into stable distributions, effectively capturing the complex landscape of crystal structures. On the other hand, methods based on autoregressive models have also achieved good results in generating crystal structures (Taniai et al. (2024)). These methods treat crystal structure data as strings and perform structure prediction in an autoregressive manner. Despite the success of existing methods, few methods can accurately achieve end-to-end mapping of crystal properties to crystal structure. In this paper, we establish an end-to-end mapping between crystal properties and crystal structure through constraints on material properties and space groups. The method we proposed is called Uni-MDM, a universal material structure design model. This method establishes the relationship between material properties, space groups and crystal structures by combining the Fine-tuned GLM4(GLM et al. (2024)) model and the Crystal structure DiT model. Our contributions can be summarized as follows: 1. We divide the entire crystal structure generation process into two parts: first, the space group information is generated according to the required material properties, which is completed by the GLM4 model; second, the crystal structure is generated based on the material properties and space group information, which is completed by the DiT model. 2. We fine-tuned the GLM4 model through prompt engineering, allowing the model to output a reasonable number of crystal space groups and wyckoff positions based on the input elemental composition and material properties. 3. We proposed a DiT model that includes symmetry information constraints. By introducing material property embedding and crystal graph structure Transformer, the model can generate expected crystal structures through the constraints of material properties and space groups. 4. All our models have been trained and adapted on the NVIDIA platform and the Ascend Altas 800T A2 platform. Experiments show that our proposed method can generate stable crystal structures that meet the expected performance requirements under the constraints of material properties and space groups. Descriptio 1: Uni-MDM."
https://arxiv.org/html/2411.08463v1,Symbolic-AI-Fusion Deep Learning (SAIF-DL): Encoding Knowledge into Training with Answer Set Programming Loss Penalties by a Novel Loss Function Approach,"This paper presents a hybrid methodology that enhances the training process of deep learning (DL) models by embedding domain expert knowledge using ontologies and answer set programming (ASP). By integrating these symbolic AI methods, we encode domain-specific constraints, rules, and logical reasoning directly into the model’s learning process, thereby improving both performance and trustworthiness. The proposed approach is flexible and applicable to both regression and classification tasks, demonstrating generalizability across various fields such as healthcare, autonomous systems, engineering, and battery manufacturing applications. Unlike other state-of-the-art methods, the strength of our approach lies in its scalability across different domains. The design allows for the automation of the loss function by simply updating the ASP rules, making the system highly scalable and user-friendly. This facilitates seamless adaptation to new domains without significant redesign, offering a practical solution for integrating expert knowledge into DL models in industrial settings such as battery manufacturing.","Machine learning models, particularly deep learning (DL), have demonstrated remarkable success across a variety of fields, including image recognition, natural language processing, and predictive analytics [15]. However, in domains where rules, constraints, and logical reasoning are critical, such as healthcare, autonomous systems, engineering, and finance, these models can face significant limitations. Traditional DL models are primarily data-driven and often function as black boxes, lacking the ability to incorporate explicit domain knowledge or reasoning capabilities [7]. One of the key challenges is that DL models may overlook domain-specific knowledge that is not easily captured within the dataset, leading to suboptimal or even incorrect predictions. For instance, a DL model predicting medication dosages could fail to recognize contraindications between medications unless domain knowledge is introduced. This limitation underscores the need for integrating symbolic reasoning and domain knowledge into the learning process to enhance model performance and trustworthiness. To address this challenge, we present a hybrid methodology that integrates deep learning with domain expert knowledge using ontologies [3, 13] and answer set programming (ASP) [12]. Our approach involves embedding domain-specific constraints and logical rules directly into the loss function of the DL model. By doing so, we create a loss function that balances predictive accuracy (data-driven learning) with rule adherence (knowledge-driven learning). This integration ensures that the model not only learns from the data but also adheres to domain-specific constraints, enhancing its suitability for high-stakes applications. By combining data-driven learning with symbolic reasoning, our methodology produces models that are both accurate and aligned with domain knowledge. This integration enhances explainability, as the logical rules provide insights into the model’s decision-making process, thereby ensuring higher levels of trustworthiness and interpretability [2]. Furthermore, unlike other state-of-the-art methods that may require extensive redesign when applied to different domains, our approach offers scalability and ease of adaptation. By simply updating the ASP rules within the ontology, the same pipeline can be applied across various fields without significant modifications. This feature is particularly advantageous in industrial settings like battery manufacturing, where rapid adaptation to new processes or regulations is essential [6]. In summary, our contribution lies in presenting a scalable and flexible hybrid approach that seamlessly integrates domain expert knowledge into the training of DL models. This methodology is applicable to both regression and classification tasks and can be generalized across diverse fields, particularly those requiring strict adherence to domain constraints."
https://arxiv.org/html/2411.08438v1,Towards Optimizing a Retrieval Augmented Generation using Large Language Model on Academic Data,"Given the growing trend of many organizations integrating Retrieval Augmented Generation (RAG) into their operations, we assess RAG on domain-specific data and test state-of-the-art models across various optimization techniques. We incorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble Retriever, and In-Context-Learning, to enhance the functionality and performance in the academic domain. We focus on data retrieval, specifically targeting various study programs at a large technical university. We additionally introduce a novel evaluation approach, the RAG Confusion Matrix designed to assess the effectiveness of various configurations within the RAG framework. By exploring the integration of both open-source (e.g., Llama2, Mistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer valuable insights into the application and optimization of RAG frameworks in domain-specific contexts. Our experiments show a significant performance increase when including multi-query in the retrieval phase.","Given the recent surge of Large Language Models (LLMs), they have found application in many industrial use cases. Practitioners in various domains are increasingly eager to harness the power of LLMs in their work and operations. However, the use of LLMs in a domain-specific context is not straightforward, as these models are predominantly trained on publicly available data, making them less suited for specialized domains. This challenge led to the emergence of Retrieval Augmented Generation (RAG) frameworks, which offer a solution by enabling the integration of domain-specific data with the expansive knowledge base of LLMs, thereby enhancing their utility and relevance in various applications. We opt for a study program use-case targeting university students. The data distribution of this use case is potentially different from what most LLMs are normally trained on, hence making it a good test bed for testing RAG performance as well as its various optimizations. The rapid development in all areas of research has given rise to many interdisciplinary and multifaceted university study programs, students are often left overwhelmed with many different programs to choose from. Understanding the prerequisites, study regulations, and curricula of programs can pose a challenge, which encourages the students to use digital solutions such as chatbots (Chen et al., 2023). We take advantage of this use case and create a domain-specific dataset for evaluating and optimizing RAG. For this purpose, we scrape the study program descriptions from a university website and construct a Question Answering dataset of 200 questions and answers, along with relevant context to evaluate the performance of the RAG system with its various components. Since LLMs are not trained on the study program description and requirements, this use case is ideal for RAG. Given a student query, it finds the relevant study program from the Vector Database and uses that information to answer the student’s query. In this paper, we present a comprehensive evaluation of RAG and its various optimizations on a question-answering task to evaluate its performance over domain-specific data. Our main contributions are as follows: (1) We constructed a dataset CurriculumQA in English and German, which was used to get the first insights into research and development of RAG pipelines for domain-specific data. (2) We experiment with and evaluate various RAG enhancement approaches to find the optimal configuration for an RAG system over domain-specific data. (3) We evaluate the LLM responses based on Relevance, Coherence, Faithfulness, and Fluency. We compare the automatic and human evaluation metrics."
https://arxiv.org/html/2411.08392v1,RLInspect: An Interactive Visual Approach to Assess Reinforcement Learning Algorithm,"Reinforcement Learning (RL) is a rapidly growing area of machine learning that finds its application in a broad range of domains, from finance and healthcare to robotics and gaming. Compared to other machine learning techniques, RL agents learn from their own experiences using trial and error, and improve their performance over time. However, assessing RL models can be challenging, which makes it difficult to interpret their behaviour. While reward is a widely used metric to evaluate RL models, it may not always provide an accurate measure of training performance. In some cases, the reward may seem increasing while the model’s performance is actually decreasing, leading to misleading conclusions about the effectiveness of the training. To overcome this limitation, we have developed RLInspect - an interactive visual analytic tool, that takes into account different components of the RL model - state, action, agent architecture and reward, and provides a more comprehensive view of the RL training. By using RLInspect, users can gain insights into the model’s behaviour, identify issues during training, and potentially correct them effectively, leading to a more robust and reliable RL system.","Machine learning systems have made impressive advances due to their ability to learn from high dimensional, non-linear, and imbalanced data [1]. However, learning from complex data often leads to complex models, which require rigorous evaluation. Over the time, lot of performance metrics [2, 3] and visualisation tools [4, 5, 6, 7, 8] have been proposed and are being used in different supervised learning settings. These metrics and tools have helped in qualifying performance and understanding inner working of supervised learning models. Reinforcement learning (RL) is a branch of ML where the agent (decision making entity) interacts with the environment and learns based on the reward (feedback) received. In recent years RL has seen an increase in real-world interest and applications due to its ability to interact with the environment, making it a valuable tool for addressing real-world problems across multiple domains [9]. It is increasingly being used to solve tasks from different areas like everyday life assistance [10, 11, 12], policy making [13, 14, 15, 16] and high-stakes domain like finance [17] and clinical decision systems [18]. However, there is a lack of consistent metrics making it difficult to assess true performance of RL [19]. Due to the complexity of the real-world problems, selecting a suitable metric for RL becomes even more important, as flawed or incomplete metric can lead to inconsistent performance and reproducibility [20]. A suitable and robust metric can either be a numerical summary or can be in the form of a visual representation. Although numerical metrics can summarise the performance in a few numbers, visual tools can often provide a more intuitive and immediate understanding of complex information [21]. This is because visual representations can convey patterns, trends, and relationships in data and can also help keep track of what manipulations are being applied at what stage. While static images can offer analytical insights, their effectiveness is constrained when dealing with complex data or representations [22]. Therefore, interactiveness plays an essential role in a visual tool for providing the flexibility to focus on the different regions of the visualisation, as and when needed. An RL training process consists of three main components which are input states, actions and reward and also neural network in case of deep RL. In this paper, we present RLInspect, an interactive visual tool for understanding and potentially debugging the RL training process. It provides different methods to assess the behaviour of RL algorithms with respect to the aforementioned components during the training process, which can help evaluate the quality of the RL training process and in-turn, the trained model."
https://arxiv.org/html/2411.08307v1,Perceiver: A Multi-cale Perceiver with Effective egmentation for Long-Term Expressive Symbolic Music Generation,"Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io.","Recent advancements in music generation, especially in audio generation models such as AudioLDM [1], MusicGen [2], and Jen-1 [3], have demonstrated significant progress, with these models capable of generating highly natural-sounding music. However, symbolic music generation, an area where models can generate and manipulate music in symbolic form, plays a crucial role in the field of music generation, particularly due to its editable nature. This allows for operations such as cutting and rearranging different sections or substituting instrument timbres, enabling human involvement in high-quality music production during the post-processing stage. Compared to audio generation, symbolic music offers a further level of abstraction, making it easier for machine learning models to capture deeper musical characteristics and understanding. However, the symbolic music generation still faces two key challenges. First, despite the advancements, many of the most expressive datasets, recorded from live performances and recording studios, are seldom used compared to manually created MIDI-file datasets. The primary reason is that they lack detailed annotations, making it harder for models to learn complex structures. Furthermore, due to computational limitations, these models cannot fully capture the context of an entire piece of music. Techniques, such as chunking and quantization, are often employed to reduce computational complexity, leading to the loss of crucial musical details and making it difficult for models to grasp the full structure of a composition. Second, the waterfall-like approaches that use abstract structural representations as conditions for music generation tasks have enabled the generation of structured music. However, such methods rely heavily on handcrafted feature engineering. Our objective is to design and develop a model that is capable of learning the long-range dependencies in music without relying on explicit structural annotations. The emergence of Transformer Attention technologies, such as Perceiver AR [4], has made it possible to access much longer contextual dependencies. It allows for the simultaneous learning of musical structure and the generation of expressive performances. Perceiver AR has demonstrated the ability to attend to a context length of up to 32,768 tokens using the Maestro dataset [5], where the query in cross-attention attends to significantly longer key/value pairs [4]. However, this approach has also introduced challenges. Specifically, the causal mask, when applied with the default input sequence segmentation, does not fully conceal tokens that should not be visible during autoregressive training and generation, which ultimately degrades the quality of the generated music. Additionally, when using ultra-long context as a condition, the model tends to generate identical or similar repetitive segments as the sequence length increases due to issues with high similarity in the context of neighboring tokens, which leads to a high token autocorrelation [6] tendency. To address the challenges mentioned above, in this paper, we propose PerceiverS, a novel model that addresses the causal masking issue by incorporating Effective Segmentation. Additionally, PerceiverS employs Multi-Scale attention to mitigate the high token autocorrelation problem that arises from relying solely on long-range dependencies. Specifically, by adjusting the input sequence segmentation to start from the head segment with an effective causal mask and aggressively increasing the segment length up to the maximum input sequence length, we resolve the learning limitations caused by the causal mask in Perceiver AR [4]. Additionally, by incorporating Multi-Scale masks across multiple layers of cross-attention, the model considers both ultra-long and short-range attention simultaneously. This approach addresses the limitation in Perceiver AR, which focuses solely on long-range attention [4]. Different from Perceiver AR, our approach enhances symbolic music generation by effectively capturing both long-term structural dependencies and short-term expressive details. Through improved segmentation and multi-scale attention mechanisms, PerceiverS generates coherent, diverse music without relying on extensive structural annotations. Extensive experiments have been conducted to evaluate the performance of the proposed PerceiverS. The experimental results demonstrate an average 40% improvement in Overlap Area when measured against the original training dataset, highlighting a substantial advantage of our approach over Perceiver AR [4] in generating high-quality symbolic music."
https://arxiv.org/html/2411.08299v1,Energy-Efficient Collaborative DNN Inference in UAV Swarm,"Unmanned Aerial Vehicles (UAVs) have attracted great attention due to its high mobility and flexible deployment, which makes many people develop UAVs for different application scenarios. The unique performance of UAVs encourages the emergence of more critical and complex tasks in uncertain and potential harsh environments, many of which were not even envisaged decades ago, including military border surveillance and oil/gas offshore exploration. The large amount of data generated by these applications needs to be processed and analyzed by deep neural networks (DNNs). However, due to the resource constraints of UAVs, there are many challenges in how to deal with deep networks and complex models on UAVs. In this paper, we propose a fine-grained DNN inference partition method, aiming at allocating inference requests to the resource-constrained UAV swarm, classifying the captured images, and finding the minimum inference latency. We formulate the problem as an optimization problem that minimizes the inference latency of UAV swarm from capturing the image to outputting the result. The optimization problem is an NP-hard problem. Therefore, we introduce an online heuristic solution, namely EECIA, to find the computing task allocation strategy that gives the best latency among the available UAVs. The simulation results show that compared with the benchmarks, our algorithm can get relatively good performances under different configures and the running time meets the real-time requirements.","In recent years, with the development of Unmanned Aerial Vehicles (UAVs) technology, they have been applied in more and more civil and military fields, such as aerial object detection [1], rapid rescue operations in disaster areas [2], intelligent city monitoring [3], and large-scale agriculture[4], etc. Compared with traditional internet of things (IoT) equipment, the UAV not only has lower cost, but also has higher mobility and stronger data collection capability. UAVs provide a fast and flexible data acquisition system, which can closely monitor human activities and objects from different angles and heights, so as to obtain high-resolution data used to enhance the accuracy of complex event detection. In addition, due to the flight capability, UAVs can cover a wide range of areas to detect events and enter difficult areas that cannot be accessed by traditional monitoring tools, such as volcanoes and forests. Although UAVs can be used in aforementioned critical and complex tasks, how to deal with the large amount of data generated by these applications is also a problem to be solved. With the development of artificial intelligence (AI), people propose to use deep neural networks (DNNs) to process a large number of data generated. Although DNN can process data efficiently, its complex structure and high performance need to consume a lot of resources. More precisely, a typical DNN consists of multiple layers associated with thousands of neurons, which requires a large amount of computation and memory footprint. Due to the size and cost limitation of UAV, the general power supply is severely constrained, and the computing capacity is low. There are two traditional ways to solve this problem. The first method is deploying lightweight models on UAVs to adapt to limited resources. Although lightweight model can reduce the amount of computation and inference latency, its accuracy is relatively low [5]. Another traditional method is transferring data back to the ground for processing, which faces the problems of high communication latency and cost. In a word, it is difficult to independently complete tasks requiring intensive computing with a single UAV, which poses a great challenge to low latency and inference accuracy [6]. In order to process these data in real-time and with high accuracy on UAVs, some scholars have proposed to use cloud server and edge server to assist UAVs in computing. In general, the existing researches can be divided into two categories: UAV collaborative inference that requires ground station assistance [7, 8, 9] and local collaborative inference that only uses UAV swarm [10, 11, 12]. Although the collaborative inference method that requires the assistance of the ground station will preprocess the data locally and select the types of ground stations according to different application scenarios, it still needs to deal with the problem of communication latency as the traditional cloud-only method, as well as the weak reliability and scalability. A major advantage of using UAV swarm for collaborative inference is that it can handle more complex operations by allocating and executing subtasks, thus reducing execution time and ensuring better fault tolerance. However, the aforementioned works about collaborative DNN inference in UAV swarm do not fully consider the limitations of a UAV swarm, e.g., memory, computing power, energy and other limitations, as well as real-time load of inference requests between UAVs and more fine-grained computing task division. In this paper, we design a collaborative inference model system of UAV swarm, and classify the input images by deploying convolution neural networks (CNNs) in the UAV swarm. The model receives the image classification request, then the calculation of inference is divided and assigned to the best UAV participants to achieve the lowest collaborative inference latency. Meanwhile, it takes into account the memory footprint, computing resources, energy consumption and the number of executable layers of each UAV. In addition, compared with previous researches, we consider a fine-grained task division method. The contributions of our paper are presented as follows: • We study the problem of the energy-efficient collaborative inference in UAV swarm. Specifically, we investigate how to distribute the DNN model inference tasks in the UAV swarm under multiple constraints to achieve the minimum latency. • We express the task allocation of collaborative inference in UAV swarm as an optimization problem to minimize the inference latency. Meanwhile, we consider the memory footprint, computing power, and energy consumption limitations of UAVs, as well as the dynamic load of inference requests. Since the optimization problem is NP-hard, we propose an energy-efficient collaborative inference algorithm (EECIA) to obtain the optimal distribution of inference tasks in UAV swarm and the optimal inference latency in a short time. • We conduct extensive simulations to validate the effectiveness of the proposed algorithm. The simulation results show that compared with three benchmarks, our proposed algorithm can reduce the inference latency of AlexNet, ResNet152 and VGG-16 by 5.25%, 8.18%and 11.02% on average, respectively, while maintaining the running time within the acceptable range."
https://arxiv.org/html/2411.08290v1,RESOLVE: Relational Reasoning with Symbolic and Object-Level Features Using Vector Symbolic Processing,"Modern transformer-based encoder-decoder architectures struggle with reasoning tasks due to their inability to effectively extract relational information between input objects (data/tokens). Recent work introduced the Abstractor module, embedded between transformer layers, to address this gap. However, the Abstractor layer while excelling at capturing relational information (pure relational reasoning), faces challenges in tasks that require both object and relational-level reasoning (partial relational reasoning). To address this, we propose RESOLVE, a neuro-vector symbolic architecture that combines object-level features with relational representations in high-dimensional spaces, using fast and efficient operations such as bundling (summation) and binding (Hadamard product) allowing both object-level features and relational representations to coexist within the same structure without interfering with one another. RESOLVE is driven by a novel attention mechanism that operates in a bipolar high dimensional space, allowing fast attention score computation compared to the state-of-the-art. By leveraging this design, the model achieves both low compute latency and memory efficiency. RESOLVE also offers better generalizability while achieving higher accuracy in purely relational reasoning tasks such as sorting as well as partial relational reasoning tasks such as math problem-solving compared to state-of-the-art methods.","Figure 1: Example of purely relational task: Pairwise Ordering Analogical reasoning, which involves recognizing abstract relationships between objects, is fundamental to human abstraction and thought. This contrasts with semantic (meaning-based) and procedural (task-based) knowledge acquired from sensory information, which is typically processed through contemporary approaches like deep neural networks (DNNs). However, most of these techniques fail to extract abstract rules from limited samples Barrett et al. (2018); Ricci et al. (2018); Lake & Baroni (2018). These reasoning tasks can be purely or partially relational. Figure 1 presents an example of a purely relational task where the objects (e.g. frog, mountains) are randomly generated. In this task, only the information representing relationships between the objects is relevant, not the objects themselves. By contrast, in Figure 2(a) the purpose is to learn the abstract rule of subtraction, which is unknown to the model, from pairs of MNIST digits. This abstract rule relies on the relational representation between the digits (derived from their relationship with one another, in this case their ordering) and the digits themselves (the values being subtracted), which are object features. This is a partially relational problem. Similarly, in Figure 2(b), the purpose is to learn the abstract rule of the quadratic formula (i.e. the solution to the quadratic problem shown at the bottom of Figure 2(b)) from the object features (derived from the text tokens representing equation coefficients) and the relational representation (derived from the coefficient ordering). These relational or partially relational tasks have been shown to be problematic for transformer-based architectures (Altabaa et al., 2023), which encode both the object features and relational representations into the same structure. (Altabaa et al., 2023) instead created a learnable inductive bias derived from the transformer architecture for explicit relational reasoning. Although this solution is sufficient for purely relational tasks such as Figure 1, it is less efficient for partially relational tasks such as Figures 2(a) and 2(b) where the object features and relational representations are both significant. The poor ability of transformers to superpose relational representations and object-level features is due to the low dimensionality of their components, causing interference between object features and relational representations (Webb et al., 2024b). By contrast, vector symbolic architectures (VSA) have used high-dimensional spaces to superpose object features and relational representations with low interference Hersche et al. (2023). Transformer-based architectures are moreover known to be power-inefficient due to the attention score computation Debus et al. (2023). Vector symbolic architectures have been proven to be power-efficient Menet et al. (2024) with low memory overhead due to the low-bitwidth (bipolar) representation of high-dimensional vectors. However, current VSA techniques require prior knowledge of abstract rules and a pre-engineered set of relations and objects (e.g., blue, triangle), making them unsuitable for sequence-to-sequence reasoning. ((a)) Subtraction using MNIST digits ((b)) Quadratic Equation Solution Figure 2: Two examples of partially relational tasks(Figure 2(a) and 2(b)) These arguments motivate the design of RESOLVE, an innovative vector symbolic architecture allowing superposition of relational representations and object-level features in high dimensional spaces. Object-level features are encoded through a novel, fast, and efficient HD-attention mechanism. The key contributions of this paper are: • We are the first to propose a strategy for addressing the relational bottleneck problem (capturing relational information between data/objects rather than input data/object attributes or features from limited training data) using a vector symbolic architecture. Our method captures relational representations of input objects in a hyperdimensional vector space, while maintaining object-level information and features in the same representation structure, while minimizing their interference with each other at the same time. The method outperforms prior art in tasks that require both pure and partial relational reasoning. • We implement a novel, fast and efficient attention mechanism that operates directly in a bipolar (\{\!-1\!,1\!\}) high-dimensional space. Vectors representing relationships between symbols are learned, eliminating the need for prior knowledge of abstract rules required by prior work Hersche et al. (2023).s • Our system significantly reduces computational costs by simplifying attention score matrix multiplication to bipolar operations and relying on lightweight high-dimensional operations such as the Hadamard product (also known as binding). In the following section we discuss related prior work, followed by an overview of our symbolic HD-attention mechanism in Section 3. We then discuss our vector-symbolic hyperdimensional attention mechanism and contrast it to the relational bottleneck approach in Section 4. The RESOLVE encoder and hypervector bundling is then discussed in Section 5 and the full architecture in Section 6. We then present experimental validation in Section 7, followed by conclusions."
https://arxiv.org/html/2411.08181v1,Challenges in Guardrailing Large Language Models for Science,"The rapid development in large language models (LLMs) has transformed the landscape of natural language processing and understanding (NLP/NLU), offering significant benefits across various domains. However, when applied to scientific research, these powerful models exhibit critical failure modes related to scientific integrity and trustworthiness. Existing general-purpose LLM guardrails are insufficient to address these unique challenges in the scientific domain. We propose a comprehensive taxonomic framework for LLM guardrails encompassing four key dimensions: trustworthiness, ethics & bias, safety, and legal compliance. Our framework includes structured implementation guidelines for scientific research applications, incorporating white-box, black-box, and gray-box methodologies. This approach specifically addresses critical challenges in scientific LLM deployment, including temporal sensitivity, knowledge contextualization, conflict resolution, and intellectual property protection.","The advent of large language models (LLMs) has revolutionized the field of natural language processing and understanding (NLP/NLU) [1] [2] [3], leading to numerous applications, particularly in chat systems. Users interact with these systems in both open-ended and close-ended question-answering (QA) modes, leveraging the models’ capabilities to generate human-like responses and perform complex language tasks. However, deploying LLMs in real-world applications introduces safety, ethics, and reliability challenges. As a result, extensive research has focused on incorporating LLM guardrails to ensure responsible use and prevent failure modes [4, 5]. LLM guardrails are mechanisms designed to enforce safety and various standards in LLM applications by monitoring and controlling user interactions [6]. These guardrails operate through both intrinsic model-level constraints and explicit rule-based systems, ensuring LLMs function within predefined principles while validating response structure, type, and quality. This is crucial due to the inherent unpredictability of LLMs, which can generate biased, misleading, or harmful outputs. Effective governance and safety measures are essential to maintain trust in generative AI technologies like these, as they become more integrated into daily applications. Some critical dimensions and properties of LLM guardrails, applicable to a wide range of domains [6], include (but are not limited to): • Mitigating factual hallucinations • Ensuring fairness in data handling • Enforcing data privacy, confidentiality, and regulatory standards • Enhancing model robustness against adversarial attacks • Detecting and filtering toxic content • Complying with legal and ethical standards • Identifying and Handling out-of-distribution inputs and outputs. • Accurately quantifying and communicating output uncertainty These properties become even more critical in sensitive domains, such as scientific research. High standards of factual accuracy and adherence to content moderation are essential to prevent the generation of inappropriate or misleading responses. The scientific field demands precision, as even minor inaccuracies or biases can have significant consequences [7, 8], from misleading research directions to compromising experimental reproducibility, affecting public trust and the advancement of knowledge [9]. Therefore, developing and implementing systematic techniques to evaluate, analyze, and enhance the performance of LLM guardrails is crucial in these contexts. Recent years have also seen rapid developments in LLMs and AI systems such as GPT-4 [10], Llama-3 [11], Claude [12], Mistral [13], and Gemini [14] transforming the landscape of the scientific domain, exhibiting remarkable capabilities in scientific knowledge processing and data discovery, content generation, and data assimilation at scale [15, 16]. These models also have the potential to significantly enhance scientific workflows, from accelerating literature reviews and automating data analysis to aiding in the writing and synthesis of research findings, redefining the bounds of knowledge consumption. [17]. However, their positive impact depends on the reliability and accuracy of their outputs. Given the sensitive nature of scientific inquiry, these outputs need to adhere to strict standards. Establishing comprehensive guidelines for deploying LLM guardrails in scientific research is therefore crucial. These guidelines should aim to safeguard the integrity of scientific processes, ensuring that the information generated by LLMs is factually accurate, consistent, and aligned with established scientific principles and values. Moreover, the ethical implications of LLM use in science must be thoroughly examined and addressed. Moreover, ethical considerations such as the potential for amplifying biases, privacy concerns, and societal impacts must be carefully addressed [18, 19]. Effective guidelines should incorporate mechanisms to identify and mitigate these ethical risks, ensuring that the deployment of LLMs in scientific research upholds the highest standards of scientific integrity, fairness, and social responsibility. This paper explores the key categories and dimensions of LLM guardrails essential for scientific research, including aspects like scientific integrity and biases. Implementing these safeguards requires developing specialized evaluation, analysis, and enhancement techniques. Our aim is to prevent harmful content generation and ensure that LLMs serve as reliable, ethically sound tools in scientific inquiry. By doing so, we seek to utilize LLMs while preserving the accuracy fundamental to the scientific community. In the following sections, we present an overview of aspects of existing LLM guardrail frameworks, emphasizing their roles in supporting trustworthy AI applications in scientific environments. This overview sets the stage for a deeper examination of specific dimensions and challenges in deploying LLM guardrails for scientific applications."
https://arxiv.org/html/2411.08165v1,"Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion","The Knowledge Graph Completion (KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these limitations, we propose \text{KGR}^{3}, a context-enriched framework for KGC. \text{KGR}^{3} is composed of three modules. Firstly, the Retrieval module gathers supporting triples from the KG, collects plausible candidate answers from a base embedding model, and retrieves context for each related entity. Then, the Reasoning module employs a large language model to generate potential answers for each query triple. Finally, the Re-ranking module combines candidate answers from the two modules mentioned above, and fine-tunes an LLM to provide the best answer. Extensive experiments on widely used datasets demonstrate that \text{KGR}^{3} consistently improves various KGC methods. Specifically, the best variant of \text{KGR}^{3} achieves absolute Hits@1 improvements of 12.3\% and 5.6\% on the FB15k237 and WN18RR datasets. The code is anonymously available at https://anonymous.4open.science/r/RRR-KGC-CBA3/.","Knowledge Graphs (KGs) are graph-structured knowledge bases (KBs) that organize factual knowledge as triples in the form of (head entity, relation, tail entity). Recently, KGs have become a crucial foundation for various downstream applications, such as recommendation systems Wang et al. (2019), question answering Sun et al. (2024), and sentiment analysis Wang and Shu (2023). Nevertheless, mainstream KGs such as Freebase Bollacker et al. (2008) and Wordnet Miller (1995) suffer from serious incomplete issues. This problem highlights the importance of the Knowledge Graph Completion (KGC) task, which aims to predict the missing entity from an incomplete triple. Figure 1: Limitations of existing embedding-based (top) and LLM-based (bottom) KGC methods. Existing KGC methods can be roughly categorized into embedding-based methods Bordes et al. (2013); Yang et al. (2015); Sun et al. (2019); Cao et al. (2022) and text-based methods Yao et al. (2019); Wang et al. (2021, 2022). Embedding-based methods implicitly learn rules based on relation patterns observed in triples and make predictions based on the likelihood of these patterns occurring Wu et al. (2023). For example, from triple (X,\textit{works in},Y) and (Y,\textit{city of},Z), it is very likely to deduce that (X,\textit{citizen of},Z). However, these methods ignore the contextual semantics supporting these triples, leading to conclusions that do not align with the facts. Text-based methods employ pre-trained language models (PLMs) to embed entities and relations with their labels and descriptions. However, these methods still cannot surpass the latest embedding-based counterparts Ge et al. (2023) due to the substantial semantic gap between structural KG triples and natural language sentences. Large language models (LLMs), trained by extensive corpora, demonstrate emergent semantic understanding and in-context learning (ICL) capabilities. Recent studies Wei et al. (2023); Liu et al. (2024) have proposed utilizing LLMs for the KGC task, as these models harbor general knowledge that can be leveraged to mitigate information scarcity for long-tail entities. However, the application of LLMs in KGC tasks encounters several limitations. Firstly, if the pre-training corpora of the LLMs lack adequate contextual information on specific entities, the LLMs may produce hallucinated or biased responses. Secondly, the structured nature of KG triples limits the ability of LLMs to effectively capture and leverage contextual information from the graph structure. These shortcomings necessitate a strong reliance on a considerable amount of in-context demonstrations Wei et al. (2023) or external structured embeddings Liu et al. (2024), which inevitably limit the performance and generality of existing approaches. Considering the aforementioned challenges, we propose a context-enriched KGC framework named \textbf{KGR}^{\mathbf{3}}, which consists of three modules: Retrieval, Reasoning, and Re-ranking. Given a query triple, the retrieval module gathers semantically relevant supporting triples with the same relation and similar entities, and extracts plausible candidate answers from a base KGC model. To ensure that the LLM attains a fundamental understanding of the retrieved information, this module also collects and augments relevant contextual information to entities in supporting triples and the candidate answer list. Then, the reasoning module exploits the semantic understanding capability of pre-trained LLM to suggest several potential answers based on in-context demonstrations and the description of the known entity. Finally, the re-ranking module fine-tunes the LLM to select out the corrupted entity of the training triple from a set of candidate entities, enabling it to process structured knowledge. \textbf{KGR}^{\mathbf{3}} possesses strong plug-and-play capability, making it compatible with all base KGC methods without costly re-training. During inference, the re-ranking module integrates the candidate answers derived from the base KGC model and the reasoning module, and then instructs the LLM to output the entity label that best completes the query triple. We validate the proposed framework on two conventional KGC datasets: FB15k237 and WN18RR. The extensive experiments show that \text{KGR}^{3} significantly and consistently outperforms all baseline methods with different types of base KGC models and backbone LLMs, showing its superiority. Notably, the best variant of \text{KGR}^{3} achieves state-of-the-art performance with absolute Hits@1 improvements of 12.3\% and 5.6\% on the two datasets. Our contributions are summarized as follows: • We propose a novel \text{KGR}^{3} framework for the KGC task, which systematically retrieves relevant supporting contexts, conducts semantic reasoning, and re-ranks candidate answers. • We notice the semantic gap between KG triples and natural language sentences, and seamlessly bridge this gap with entity contexts. • We conduct extensive experiments and ablation studies to evaluate the effectiveness of the \text{KGR}^{3} framework, and discuss the importance of incorporating entity contexts and LLMs."
https://arxiv.org/html/2411.08148v1,Adaptive Meta-Learning for Robust Deepfake Detection: A Multi-Agent Framework to Data Drift and Model Generalization,"Pioneering advancements in artificial intelligence, especially in generative AI, have enabled significant possibilities for content creation, but also led to widespread misinformation and false content. The growing sophistication and realism of deepfakes is raising concerns about privacy invasion, identity theft, and has societal, business impacts, including reputational damage and financial loss. Many deepfake detectors have been developed to tackle this problem. Nevertheless, as for every AI model, the deepfake detectors face the wrath of lack of considerable generalization to unseen scenarios and cross-domain deepfakes. Additionally, adversarial robustness is another critical challenge, as detectors drastically underperform with the slightest imperceptible change. Most state-of-the-art detectors are trained on static datasets and lack the ability to adapt to emerging deepfake attack trends. These three crucial challenges though hold paramount importance for reliability in practise, particularly in the deepfakes domain, are also the problems with any other AI application. This paper proposes an adversarial meta-learning algorithm using task-specific adaptive sample synthesis and consistency regularization, in a refinement phase. By focussing on the classifier’s strengths and weaknesses, it boosts both robustness and generalization of the model. Additionally, the paper introduces a hierarchical multi-agent retrieval-augmented generation workflow with a sample synthesis module to dynamically adapt the model to new data trends by generating custom deepfake samples. The paper further presents a framework integrating the meta-learning algorithm with the hierarchical multi-agent workflow, offering a holistic solution for enhancing generalization, robustness, and adaptability. Experimental results demonstrate the model’s consistent performance across various datasets, outperforming the models in comparison. The associated code is available here.","Deepfakes have become one of the most concerning possibilities of artificial intelligence. They refer to highly realistic falsification of information or media corresponding to one or more modalities such as image, video, audio, and text, with the help of advanced deep learning and generative models. Though media manipulations, especially digital image modifications such as image splicing, colorization, use of filters, image superimposition, and so on were existing from several decades, the intent was usually or mostly for content enhancement. The term ‘deepfake’ was coined in 2017111https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained referring to the creation of convincingly realistic fake content post a Reddit user created celebrity face swaps. Despite its malicious beginnings, deepfakes initially, gained attention for their use in entertainment, art, and harmless fun. However, their potential for misuse quickly became apparent, as deepfakes were increasingly used for various malicious purposes, with the rapid evolution of deepfake technology. Several varieties of deepfakes are in play, each leveraging different techniques to create convincing fake content across various media formats. Some prominent examples are listed in Table I. Image Deepfake Video Deepfake Audio Deepfake Text Deepfake Multimodal Deepfake • Face Swap • Expression Swap • Attribute Manipulation • Face Synthesis • Face Reenactment • Lip-Syncing • Body Manipulation/swap • Background Manipulation • Background people manipulation • Foreground and background manipulation & the variants • Style Transfer • Gender manipulation • Age Progression/Regression • All image deepfake methods applied to videos • Lip-Syncing • Puppet Master • Whole-Head Synthesis (Head Puppetry) • Whole-Body Synthesis (Full body puppetry) • Text-to-Speech • Voice Conversion • Speech Synthesis • Voice Cloning or Impersonation • Audio Reenactment • Audio Style Transfer • Audio Manipulation • Audio Splicing • Audio Dubbing • Audio Gender Change • Writing style impersonation • Synthetic text / text-to-text synthesis • Content manipulation/ falsification/ fabrication • Audio-visual Synthesis (combining audio and video deepfakes and variants) • Text to image synthesis • Text to video synthesis • Text to audio synthesis TABLE I: Types of Deepfakes More advanced deepfakes combine two or more modalities to create multimodal deepfake content. For instance, a single video could feature a synthesized face, a manipulated background, a cloned voice, and modified lip-syncing, all working together to create an extremely convincing false narrative. The complexity and realism of such multimodal deepfakes make them extremely challenging to detect. This is raising serious concerns about trustworthiness and authentication of digital content, posing significant threat to society due to their ability and high impact in seamlessly spreading misinformation, and invading privacy of people without consent, causing emotional distress with the creation of fake personas tampering individual credibility with theft of identity. The widespread highly convincing fake content known no bounds is also prominently being used to deceive viewers, manipulate public opinion on critical matters including political contexts, and frame individuals and celebrities for incidents or crimes they did not commit. Deepfakes also have a severe impact on businesses such as damaging brand reputation, erosion of trust among customers, monetary loss, loss of market, leading to involvement in fraudulent transactions, and so on. With the growing sophistication of deepfakes, fueled by the proliferation of generative AI, and democratization of a multitude of generative tools, it is crucial to develop effective detection and prevention methods to mitigate these risks. Addressing these challenges necessitates the development of sophisticated and comprehensive solutions that extend beyond traditional detection methods with a multi-faceted approach. However, with the recent increased use of a combination or ensemble of manipulation methods, and the rapid pace at which numerous new variants of deepfakes are emerging, deepfake detectors are increasingly being defeated in identifying deepfakes, making deepfake detection very problematic and inefficient for practical usage. Followed by this general overview and the broad problem of deepfake detection, there are specific challenges associated with it as follows: • The problem of lack of generalization in current deepfake detectors which often fail in the case of identifying real-time deepfakes and the deepfakes created by another method (cross-domain), despite having the state-of-the-art or a reasonably good performance when trained upon a good quality benchmark dataset. • In most of the cases, minimal changes to model architectures are being proposed which show certain improvement over the prior state-of-the-art model on a specific dataset, but when this better performing model is used elsewhere, its performance gets significantly dropped. • Also, these models are often just trained on the specific bench-marked datasets and the accuracy improvements are reported on the same. However, they are not often adversarially trained or tested due to which they are practically not usable, as they are very sensitive to minute imperceptible changes and are prone to malicious attacks with slightest tweaks. • Keeping up with dynamic data drift with rapid evolution of new deepfakes, and adapting to new tactics is a concerning challenge. The state-of-the-art models are trained on pre-curated static samples and thus, have no way to get updated with the changes in deepfake patterns that are rapidly evolving. Finetuning the model on a new dataset is the usual way followed, however, eventually, this ends up with the same problem. Hence, a dynamic model updation strategy is needed. In this paper, we attempt to address the above challenges, by proposing an adversarial meta-learning algorithm with multi-agent framework for generalization, robustness, data synthesis, and adaption to dynamic data drift. The main contributions of this paper are as follows: • An adversarial meta-learning algorithm amalgamating the prospects of task-specific adaptive sample synthesis, and consistency regularization, in a few-shot computationally non-intensive setting, with a refinement phase boosting the generalization and robustness of the model. • A formula M_{adaptive} to identify the samples where the model is very confident, and the samples where the model is greatly struggling, using prediction probability, margin, classification sign, and entropy. • A unified loss function using weighted contrastive and margin ranking loss. • A hierarchical multi-agent retrieval-augmented generation (RAG) workflow with a sample synthesis module for collecting and generating custom synthetic deepfake samples for dynamic model training. To the best of our knowledge, this is the first work to implement such a workflow with agents for collecting real-time information, synthesize deepfake attack patterns, and corresponding few-shot prompts, followed by the custom sample generation. • A holistic framework integrating the proposed meta-learning algorithm with the hierarchical multi-agent RAG workflow for an end-to-end deepfake detection architecture enhancing generalization, robustness, and adaptability. The rest of the paper is organized as follows. Section 2 discusses about the related works for addressing the aforementioned challenges, and their limitations. Section 3 details the proposed methodology, and Section 4 describes the implementation and experimentation of the framework along with the results. Finally, Section 5 concludes the proposed work along with discussion regarding the future scope of work."
https://arxiv.org/html/2411.08040v1,The Universal PDDL Domain,"In AI planning, it is common to distinguish between planning domains and problem instances, where a “domain” is generally understood as a set of related problem instances. This distinction is important, for example, in generalised planning, which aims to find a single, general plan or policy that solves all instances of a given domain. In PDDL, domains and problem instances are clearly separated: the domain defines the types, predicate symbols, and action schemata, while the problem instance specifies the concrete set of (typed) objects, the initial state, and the goal condition. In this paper, we show that it is quite easy to define a PDDL domain such that any propositional planning problem instance, from any domain, becomes an instance of this (lifted) “universal” domain. We construct different formulations of the universal domain, and discuss their implications for the complexity of lifted domain-dependent or generalised planning.","In AI planning, a distinction is often made between planning domains and problem instances, where a “domain” is intuitively understood to be a set, typically infinite, of related or similar problem instances. This concept is important in, for instance, planning with domain-specific control knowledge (?, ?, ?), and in generalised planning, which seeks a single, general plan or policy that solves all instances of a given domain (?). It is materialised in many modelling languages for specifying planning problems, such as PDDL (?), in which the domain and problem instance are syntactically separate. In PDDL, the domain definition contains types, and parameterised predicates and action schemata. The problem instance definition provides the concrete set of (typed) objects, the initial state and the goal condition. ? (?) and ? (?) argue that PDDL’s notion of domain is too weak, in that it does not always allow the modeller to explicitly state the constraints necessary to define precisely the intended set of problem instances, such as constraints on intended “valid” initial states and goals. Here, we will show that PDDL’s notion of domain is also in another sense too general: specifically, that it is possible (indeed, quite trivial) to define a domain such that any planning problem instance, of any domain, is an instance of this “universal” domain. There is, however, is caveat: While the universal domain is a parameterised PDDL domain, consisting of types, predicates and action schemata, instances of this domain are arbitrary propositional planning problems. This means that although any PDDL domain–problem pair can be turned into an instance of the universal domain, doing so requires grounding it, with the consequent potentially exponential increase in size. We will also argue that it is not possible to define a universal domain in PDDL such that any domain–problem pair can be expressed as an instance of this domain of a size that is polynomial in that of the domain–problem pair. (define (domain planning) (:types action proposition) (:predicates (pre ?a - action ?p - proposition) (add ?a - action ?p - proposition) (del ?a - action ?p - proposition) (true ?p - proposition)) (:action apply :parameters (?a - action) :precondition (forall (?p - proposition) (imply (pre ?a ?p) (true ?p))) :effect (and (forall (?p - proposition)(when (add ?a ?p) (true ?p)))(forall (?p - proposition)(when (and (del ?a ?p) (not (add ?a ?p))) (not (true ?p))))) ) ) Figure 1: The universal domain for propositional planning. (define (problem sussman) (:domain planning) (:objects ontable_A ontable_B ontable_C on_A_B on_A_C on_B_A on_B_C on_C_A on_C_B clear_A clear_B clear_C holding_A holding_B holding_C hand_empty - proposition pickup_A pickup_B pickup_C putdown_A putdown_B putdown_C stack_A_B stack_A_C stack_B_A stack_B_C stack_C_A stack_C_B unstack_A_B unstack_A_C unstack_B_A unstack_B_C unstack_C_A unstack_C_B - action) (:init (pre pickup_A ontable_A) (pre pickup_A clear_A) (pre pickup_A hand_emtpy) (add pickup_A holding_A) (del pickup_A ontable_A) (del pickup_A clear_A) (del pickup_A hand_empty) (pre pickup_B ontable_B) (pre pickup_B clear_B) (pre pickup_B hand_emtpy) (add pickup_B holding_B) (del pickup_B ontable_B) (del pickup_B clear_B) (del pickup_B hand_empty) (pre pickup_C ontable_C) (pre pickup_C clear_C) (pre pickup_C hand_emtpy) (add pickup_C holding_C) (del pickup_C ontable_C) (del pickup_C clear_C) (del pickup_C hand_empty) (pre putdown_A holding_A) (add putdown_A ontable_A) (add putown_A clear_A) (add putdown_A hand_empty) (del putdown_A holding_A) (pre putdown_B holding_B) (add putdown_B ontable_B) (add putown_B clear_B) (add putdown_B hand_empty) (del putdown_B holding_B) (pre putdown_C holding_C) (add putdown_C ontable_C) (add putown_C clear_C) (add putdown_C hand_empty) (del putdown_C holding_C) (pre stack_A_B holding_A) (pre stack_A_B clear_B) (add stack_A_B on_A_B) (add stack_A_B clear_A) (add stack_A_B hand_empty) (del stack_A_B holding_A) (del stack_A_B clear_B) (pre stack_A_C holding_A) (pre stack_A_C clear_C) (add stack_A_C on_A_C) (add stack_A_C clear_A) (add stack_A_C hand_empty) (del stack_A_C holding_A) (del stack_A_C clear_C) (pre stack_B_A holding_B) (pre stack_B_A clear_A) (add stack_B_A on_B_A) (add stack_B_A clear_B) (add stack_B_A hand_empty) (del stack_B_A holding_B) (del stack_B_A clear_A) (pre stack_B_C holding_B) (pre stack_B_C clear_C) (add stack_B_C on_B_C) (add stack_B_C clear_B) (add stack_B_C hand_empty) (del stack_B_C holding_B) (del stack_B_C clear_C) (pre stack_C_A holding_C) (pre stack_C_A clear_A) (add stack_C_A on_C_A) (add stack_C_A clear_C) (add stack_C_A hand_empty) (del stack_C_A holding_C) (del stack_C_A clear_A) (pre stack_C_B holding_C) (pre stack_C_B clear_B) (add stack_C_B on_C_B) (add stack_C_B clear_C) (add stack_C_B hand_empty) (del stack_C_B holding_C) (del stack_C_B clear_B) (pre unstack_A_B on_A_B) (pre unstack_A_B clear_A) (pre unstack_A_B hand_empty) (add unstack_A_B holding_A) (add unstack_A_B clear_B) (del unstack_A_B on_A_B) (del unstack_A_B clear_A) (pre unstack_A_C on_A_C) (pre unstack_A_C clear_A) (pre unstack_A_C hand_empty) (add unstack_A_C holding_A) (add unstack_A_C clear_C) (del unstack_A_C on_A_C) (del unstack_A_C clear_A) (pre unstack_B_A on_B_A) (pre unstack_B_A clear_B) (pre unstack_B_A hand_empty) (add unstack_B_A holding_B) (add unstack_B_A clear_A) (del unstack_B_A on_B_A) (del unstack_B_A clear_B) (pre unstack_B_C on_B_C) (pre unstack_B_C clear_B) (pre unstack_B_C hand_empty) (add unstack_B_C holding_B) (add unstack_B_C clear_C) (del unstack_B_C on_B_C) (del unstack_B_C clear_B) (pre unstack_C_A on_C_A) (pre unstack_C_A clear_C) (pre unstack_C_A hand_empty) (add unstack_C_A holding_C) (add unstack_C_A clear_A) (del unstack_C_A on_C_A) (del unstack_C_A clear_C) (pre unstack_C_B on_C_B) (pre unstack_C_B clear_C) (pre unstack_C_B hand_empty) (add unstack_C_B holding_C) (add unstack_C_B clear_B) (del unstack_C_B on_C_B) (del unstack_C_B clear_C) (true ontable_A) (true on_C_A) (true clear_C) (true ontable_B) (true clear_B)) (:goal (and (true on_A_B) (true on_B_C))) ) Figure 2: Example of a problem instance of the universal propositional planning domain."
https://arxiv.org/html/2411.08879v1,"4D Gaussian Splatting in the Wild 
with Uncertainty-Aware Regularization","Novel view synthesis of dynamic scenes is becoming important in various applications, including augmented and virtual reality. We propose a novel 4D Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded monocular videos. To overcome the overfitting problem of existing work for these real-world videos, we introduce an uncertainty-aware regularization that identifies uncertain regions with few observations and selectively imposes additional priors based on diffusion models and depth smoothness on such regions. This approach improves both the performance of novel view synthesis and the quality of training image reconstruction. We also identify the initialization problem of 4DGS in fast-moving dynamic regions, where the Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks. To initialize Gaussian primitives in such regions, we present a dynamic region densification method using the estimated depth maps and scene flow. Our experiments show that the proposed method improves the performance of 4DGS reconstruction from a video captured by a handheld monocular camera and also exhibits promising results in few-shot static scene reconstruction.","Dynamic novel View Synthesis (DVS) aims to reconstruct dynamic scenes from captured videos and generate photorealistic frames for an arbitrary new combination of a viewpoint and a time step. This task has emerged as a vital research area in the 3D vision community with rapid advancements in augmented reality and virtual reality. Early DVS research primarily relied on neural radiance fields [29, 69, 10, 13, 11, 38, 39, 41, 5, 12, 50]. In contrast, more recent methods [61, 18, 31] extend 3D Gaussian Splatting [23] to account for the additional time dimension in dynamic scenes, and these techniques are referred to as 4D Gaussian Splatting. Despite the recent success of 4D Gaussian Splatting models [61, 18, 31, 68], their applicability remains largely limited to controlled and purpose-built environments. Most existing models are developed and tested with multi-view video setups [29, 41]. While there are several methods tackling monocular video settings, these setups are still controlled and fall short of in-the-wild scenarios. For instance, [38, 69] maintain multi-view characteristics, where the camera captures a broad arc around a slow-moving object. Also, HyperNeRF [39] relies on unrealistic train-test splits, with both sets sampled from the same video trajectory, which renders the task closer to video interpolation than genuine novel view synthesis. In this paper, we focus for the first time on more natural, real-world monocular videos [14], where a single handheld camera moves around fast-moving objects. In casually recorded monocular videos, which often lack sufficient multi-view information, 4D Gaussian Splatting algorithms tend to overfit the training frames in real-world scenarios. To address overfitting, recent regularization techniques [26, 7, 58, 67, 25, 36, 20] can be applied to provide additional priors for unseen views. However, these regularization techniques often involve a balancing issue: while they effectively improve novel view synthesis performance during testing, they inherently sacrifice the reconstruction accuracy of training images. Since both the reconstruction accuracy and the novel view synthesis quality are equally important in our target task, the trade-off caused by the naïve application of the regularization techniques is not desirable. In this paper, we address this balancing issue with a simple yet effective solution: uncertainty-aware regularization. First, we quantify the uncertainty of each Gaussian primitive based on its contribution to rendering for training images. Then, a 2D uncertainty map is constructed for unseen views using an \alpha-blending method. Regularization is selectively applied to uncertain regions, guided by the diffusion and depth smoothness priors, while low-uncertainty regions, where training data already provide sufficient reconstruction detail, are left unregularized, as illustrated in Figure 1. This approach results in a better balance between training and test performance, achieving good performance. In real-world scenarios involving fast motions, especially in casually recorded videos, 4D Gaussian Splatting additionally faces considerable challenges with initialization. The algorithms based on Gaussian Splatting initialize Gaussian primitives using point clouds obtained by Structure from Motion (SfM) [47]. However, SfM struggles to reconstruct dynamic regions, particularly those with fast motion, often treating them as noise and leaving these areas without initialized primitives. Such an incomplete initialization disrupts training, causing primitives in static regions to be repeatedly cloned and split in an attempt to fill the dynamic areas. This can lead to an excessive number of primitives and, in some cases, out-of-memory issues. To address this limitation, we propose a dynamic region densification technique that initializes additional Gaussian primitives in dynamic regions. Figure 1: Concept of uncertainty-aware regularization. Existing models often use regularization techniques to introduce additional priors for unseen views, aiming to enhance novel view synthesis performance. However, these methods tend to over-regularize accurately reconstructed pixels, which degrades the reconstruction quality of training images. To address this issue, our uncertainty-aware regularization selectively focuses on uncertain regions in unseen views, preserving the quality of well-reconstructed pixels with low uncertainty. We address the challenging problem of 4D reconstruction from an in-the-wild monocular video recorded casually with a handheld camera—a scenario that has been rarely explored. The main contributions of this paper are summarized as follows: • We propose an uncertainty quantification technique based on contribution to training image rendering and introduce adaptive regularization techniques based on the uncertainty map, which balances between novel view synthesis performance and training image reconstruction quality. • We address the issue of incomplete initialization in dynamic regions, emphasizing the importance of proper initialization in the training process of 4D Gaussian Splatting. • We demonstrate the effectiveness of our algorithm on casually recorded monocular videos, showing improvements over baselines. Additionally, we validate the applicability of our method in few-shot static scene reconstruction. The rest of this paper is organized as follows. Section 2 reviews related work and Section 3 discusses the basic concepts of 4D Gaussian splatting, which builds upon 3D Gaussian Splatting by integrating deformation strategies. The details of our approach are described in Section 4, followed by the presentation of experimental results in Section 5. Finally, we conclude this paper in Section 6."
https://arxiv.org/html/2411.08878v1,A Short Note on Evaluating RepNet for Temporal Repetition Counting in Videos,"We discuss some consistent issues on how RepNet has been evaluated in various papers. As a way to mitigate these issues, we report RepNet performance results on different datasets, and release evaluation code and the RepNet checkpoint to obtain these results. Code URL: https://github.com/google-research/google-research/blob/master/repnet/","This note is related to evaluating the class-agnostic repetition counting model RepNet [1] on various video repetition counting datasets. In many papers [2, 8, 5, 6] it has been reported that performance of RepNet on some repetition counting datasets is deficient. The first time this was reported was in the TransRAC [2] paper. However, the model referred to as ‘RepNet’ in that paper is a modified version of RepNet. In Section 5.3 of [2], it is mentioned that ”… for a fair comparison, we modify the last fully connection layer of RepNet [1] to make it capable of handling those videos containing more than 32 action periods”. It is unclear what this modification is exactly but it leads to the modified RepNet’s performance being close to 0 in the Off-by-One Accuracy (OBOA) metric on the UCFRep and RepCount datasets. These results imply that the modified model is not able to count repetitions in the videos of these datasets. The papers that follow TransRAC have reused the numbers reported in TransRAC but refer to this modified model as RepNet. We would like to highlight that the original RepNet model is capable of making predictions of higher than 32 period length. This is achieved by playing the video at different speeds, rather than modifying the model. This technique is described in the original RepNet paper [1] as Multi-speed Evaluation (Section 3.5). This was used for evaluating on the Countix dataset as well and has been proposed before in [4]. When we evaluate the RepNet model using the multi-speed technique on the UCFRep and RepCount datasets, we find that it results in strong performance."
https://arxiv.org/html/2411.08870v1,The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models,"Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision-language models (VLMs) via continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining (DAPT) improves performance on downstream medical tasks, such as answering medical licensing exam questions. In this paper, we compare ten public “medical” LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-/few-shot prompting and supervised fine-tuning regimes for medical question-answering (QA).111This working paper substantially extends the results of Jeong et al. (2024) to include closed-ended tasks based on clinical notes in addition to medical-exam-style QA, as well as a comparison of performance when using medical versus general-domain models as an initialization for supervised fine-tuning. For instance, across all tasks and model pairs we consider in the 3-shot setting, medical LLMs only outperform their base models in 22.7% of cases, reach a (statistical) tie in 36.8% of cases, and are significantly worse than their base models in the remaining 40.5% of cases. Our conclusions are based on (i) comparing each medical model head-to-head, directly against the corresponding base model; (ii) optimizing the prompts for each model separately in zero-/few-shot prompting; and (iii) accounting for statistical uncertainty in comparisons. While these basic practices are not consistently adopted in the literature, our ablations show that they substantially impact conclusions. Meanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs can show performance improvements, but the benefits do not carry over to tasks based on clinical notes. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies.","1 Introduction Recent advances in autoregressive large language models (LLMs) and vision-language models (VLMs) have attracted interest from practitioners in medicine, where these models hold great potential to transform various aspects of clinical practice (e.g., medical diagnosis, information retrieval from clinical documents, patient triaging) (Fries et al., 2022a; Moor et al., 2023a). State-of-the-art performance on various medical benchmarks is typically achieved by massive-scale closed-source models, such as GPT-4 (OpenAI, 2023a, b), Med-Gemini (Saab et al., 2024; Yang et al., 2024), and Med-PaLM (Singhal et al., 2023a, b; Tu et al., 2024), often performing on par with humans on medical licensing exams and open-ended consumer health question-answering (QA) tasks. However, the general lack of transparency in these models, high API usage costs, and patient data privacy concerns make their integration into routine clinical workflows challenging (Marks and Haupt, 2023). To address such concerns, recent works have proposed cheaper, open-source alternatives through domain-adaptive pretraining (DAPT; Gururangan et al., 2020), where a pretrained open-source general-domain model—such as Llama (Touvron et al., 2023a, b; Meta, 2024) or Mistral (Jiang et al., 2023) in the language space, and LLaVA (Liu et al., 2023) or Open-Flamingo (Awadalla et al., 2023) in the vision-language space—is continually pretrained on biomedical (image-)text corpora from public sources such as PubMed and medical textbooks. While some prior works show that medical models pretrained from scratch only using domain-specific corpora can outperform those trained via DAPT, both in the context of BERT-style encoder-only models (Devlin et al., 2019; Gu et al., 2021; Yang et al., 2022) and decoder models (Taylor et al., 2022; Luo et al., 2022; Hernandez et al., 2023; Bolton et al., 2024), the DAPT approach has become common practice, resulting in a trend where the release of a more capable general-domain model is typically followed by the release of its medical counterpart. Despite the widespread adoption of medical DAPT, the claimed improvements in performance are worth scrutinizing. While the story is intuitive, more recent base models often already exhibit strong off-the-shelf performance on medical benchmarks without any adaptation. For instance, as of the time of writing, the general-domain Llama-3-8B (Meta, 2024) outperforms other medically specialized models such as MediTron-70B (Chen et al., 2023) and BioMistral-7B (Labrak et al., 2024) on the Open Medical LLM Leaderboard (Pal et al., 2024), which evaluates each model on standard medical QA benchmark datasets such as MedQA (Jin et al., 2020) and MedMCQA (Pal et al., 2022). Moreover, given the general lack of transparency about the pretraining corpora used to train the general-domain model in the first place, it is possible that they may already be trained on relevant medical text. Perhaps more concerning is the lack of apples-to-apples comparisons in the literature. First, medical models resulting from DAPT are often only compared against other baselines with different architectures and model scale (e.g., Clinical-Camel-70B (Toma et al., 2023) vs. GPT-4 (OpenAI, 2023a)). Second, even for the same model scale, models are often evaluated under inconsistent evaluation setups (e.g., MediTron-70B (Chen et al., 2023) fine-tuned on MedQA with gradient updates vs. Clinical-Camel-70B (Toma et al., 2023) few-shot prompted on MedQA). Third, the common practice of using a single, fixed prompting setup (e.g., prompt format, choice of few-shot examples) for all models under evaluation also warrants concern, as LLM/VLM behavior is extremely sensitive to such design decisions (Jiang et al., 2020; Zhao et al., 2021; Ceballos-Arroyo et al., 2024b), and the “optimal” choice of such details rarely correlates between different models (Sclar et al., 2024). All of these issues can confound the interpretation of results, when evaluating the performance benefits from medical DAPT. (a) (b) Figure 1: Medical LLMs and VLMs trained via domain-adaptive pretraining (DAPT) show limited improvement over their general-domain counterparts in zero-/few-shot prompting and supervised fine-tuning regimes. (a) Overview of our head-to-head evaluation approach for each pair of general-domain (blue) and medically adapted LLM/VLM (red). (b) Win, tie, and loss rates (%) of medical models vs. their corresponding base models across all (model pair, QA dataset) combinations. Win rate refers to the proportion of (model pair, QA dataset) combinations where a medical model shows a statistically significant improvement. In this paper, we perform an apples-to-apples comparison that addresses these concerns, comparing ten medical LLMs and two medical VLMs against their general-domain base models on various medical (visual) QA tasks (see Figure 1(a)). We compare several pairs of general-domain and medically adapted LLMs/VLMs (see Table 1), whose only differences lie in medical DAPT (i.e., one model is the base model, from which the other is derived via medical DAPT). For each pair, we compare their downstream performances from (i) zero-/few-shot prompting (Radford et al., 2019; Brown et al., 2020) and (ii) supervised fine-tuning. For the former, we follow the approach considered in Jeong et al. (2024) and compare the performances after independently selecting the “best” prompt format and few-shot examples for each model based on the validation set (Section 3.1). For the latter, we compare the performances after fine-tuning each model on the training set of each downstream QA dataset, using the best hyperparameters selected via grid search (Section 3.2). In both cases, we use the percentile bootstrap to assess whether the perceived improvements in performance from medical DAPT are attributable to chance. Table 1: Summary of open-source autoregressive VLM and LLM pairs used for evaluation. For Med42, we only list the top-five adaptation datasets with the highest mixture ratios. Model Class General Domain Medical Domain Medical Adaptation Corpora LLM Llama-3-70B-Instruct (Meta, 2024) Med42-v2-70B (Christophe et al., 2024b) Medical QA Datasets (e.g., MedQA, MedMCQA) Medical Instruction 120k (Altaf, 2023) OpenGPT (OpenChat) (Wang et al., 2024) StackExchange (Lambert et al., 2023) Medical Flashcards (Han et al., 2023) Llama-3-70B-Instruct (Meta, 2024) OpenBioLLM-70B (Pal and Sankarasubbu, 2024) Undisclosed Llama-2-70B (Touvron et al., 2023b) MediTron-70B (Chen et al., 2023) Clinical Practice Guidelines (e.g., CDC, WHO) PubMed Articles (S2ORC; Lo et al., 2020) Llama-2-70B (Touvron et al., 2023b) Clinical-Camel-70B (Toma et al., 2023) ShareGPT 20k PubMed Articles Published Before 2021 Random 4k Subset of MedQA (Jin et al., 2020) Llama-2-70B (Touvron et al., 2023b) Med42-v1-70B (Christophe et al., 2024a) Medical QA Datasets (e.g., MedQA, MedMCQA) OpenGPT (OpenChat) (Wang et al., 2024) StackExchange (Lambert et al., 2023) Medical Flashcards (Han et al., 2023) CORD-19 (Wang et al., 2020) Llama-3-8B-Instruct (Meta, 2024) Med42-v2-8B (Christophe et al., 2024b) Medical QA Datasets (e.g., MedQA, MedMCQA) Medical Instruction 120k (Altaf, 2023) OpenGPT (OpenChat) (Wang et al., 2024) StackExchange (Lambert et al., 2023) Medical Flashcards (Han et al., 2023) Llama-3-8B (Meta, 2024) OpenBioLLM-8B (Pal and Sankarasubbu, 2024) Undisclosed Llama-2-7B (Touvron et al., 2023b) MediTron-7B (Chen et al., 2023) Clinical Practice Guidelines (e.g., CDC, WHO) PubMed Articles (S2ORC; Lo et al., 2020) Mistral-7B-Instruct-v0.1 (Jiang et al., 2023) BioMistral-7B (Labrak et al., 2024) PubMed Articles (PMC Open Access Subset) Llama-2-7B-Chat (Touvron et al., 2023b) BioMedGPT-LM-7B (Luo et al., 2023) PubMed Articles (S2ORC; Lo et al., 2020) VLM LLaVA-v0-7B (Liu et al., 2023) LLaVA-Med-7B (Li et al., 2023a) PubMed Articles (PMC-15M; Zhang et al., 2023) Open-Flamingo-9B (Awadalla et al., 2023) Med-Flamingo-9B (Moor et al., 2023b) Medical Textbooks (MTB; Moor et al., 2023b) PubMed Articles (PMC-OA; Lin et al., 2023) Overall, we find that both medical LLMs and VLMs show limited improvement over their base models, across all medical QA tasks and evaluation settings that we consider (Figure 1(b)). In the zero-/few-shot prompting regime (Section 4), we find that all medical VLMs and the majority of medical LLMs fail to consistently outperform their base models across all datasets, including QA tasks focused on assessing medical knowledge and those based on real-world clinical notes (e.g., discharge summaries). In the supervised fine-tuning regime (Section 5), we find that the medical LLMs overall do show statistically significant improvements on medical knowledge QA tasks but not on the clinical note QA tasks, while the medical VLMs show little to no improvement on all of the visual medical QA tasks. Our findings also suggest that rigorous pairwise comparison of models, including tests for statistical significance, is essential to drawing reliable conclusions about the performance benefits from medical DAPT. Our main contributions can be summarized as follows: 1. We provide a comprehensive head-to-head comparison between state-of-the-art general-domain LLMs/VLMs and their medical DAPT counterparts on various medical QA benchmarks, to investigate the effectiveness of DAPT for medical specialization. 2. We find that when the prompts are optimized for each medical and general-domain model independently, the majority of medical models fail to improve over their general-domain counterparts in the zero-/few-shot prompting regime (Section 4.1). 3. We show that using a single, fixed prompt format and choice of few-shot examples for all models without statistical testing can lead to overly optimistic conclusions about the benefits from medical DAPT in the zero-/few-shot prompting regime (Section 4.2). 4. We find that in the supervised fine-tuning regime, all medical VLMs fail to show improvement, while medical LLMs show improvement on textual medical knowledge QA tasks but not on tasks based on clinical notes (Section 5)."
https://arxiv.org/html/2411.08861v1,Interaction Testing in Variation Analysis,"Relationships of cause and effect are of prime importance for explaining scientific phenomena. Often, rather than just understanding the effects of causes, researchers also wish to understand how a cause X affects an outcome Y mechanistically – i.e., what are the causal pathways that are activated between X and Y. For analyzing such questions, a range of methods has been developed over decades under the rubric of causal mediation analysis. Traditional mediation analysis focuses on decomposing the average treatment effect (ATE) into direct and indirect effects, and therefore focuses on the ATE as the central quantity. This corresponds to providing explanations for associations in the interventional regime, such as when the treatment X is randomized. Commonly, however, it is of interest to explain associations in the natural, observational regime, and not just in the interventional regime. In this paper, we introduce variation analysis, an extension of mediation analysis that focuses on the total variation (TV) measure between X and Y, written as \mathbbm{E}[Y\mid X=x_{1}]-\mathbbm{E}[Y\mid X=x_{0}]. The TV measure encompasses both causal and confounded effects, as opposed to the ATE which only encompasses causal (direct and mediated) variations. In this way, the TV measure is suitable for providing explanations in the natural regime and answering questions such as “why is X associated with Y in a particular way?”. Our focus is on decomposing the TV measure, in a way that explicitly includes direct, indirect, and confounded variations. Furthermore, we also decompose the TV measure to include interaction terms between these different pathways. Subsequently, the concept of interaction testing is introduced, which involves hypothesis tests to determine if interaction terms are significantly different from zero. If interactions are not significant, a more parsimonious decomposition of the TV measure can be used. The paper further provides a structural basis for these interaction tests (through the language of structural causal models) and demonstrates their applicability through synthetic and real-world data analyses. The extension of the framework for log-risk and log-odds scales for binary outcomes is also discussed, offering a comprehensive approach to understanding the interplay of direct, indirect, and confounded effects in causal inference.","Understanding relationships of cause and effect is one of the fundamental tasks found throughout the sciences. The process of establishing mechanistic links between causes and their consequences is at the core of our ability to explain why events occur as they do. In this context, mediation analysis, a widely used tool, helps unravel the pathways through which causal effects are transmitted. By identifying intermediary variables, mediation analysis offers deeper insights into the underlying mechanisms driving the observed cause-effect relationships. This approach is crucial in fields ranging from epidemiology to social sciences, where understanding the nuances of causal relationships can inform interventions and policy decisions. Interestingly, most of the literature on mediation analysis focuses on understanding the variations contained in the average treatment effect (ATE), also known as the total effect (TE), given by \displaystyle\mathbbm{E}[Y\mid do(X=x_{1})]-\mathbbm{E}[Y\mid do(X=x_{0})], (1) where do(\cdot) symbolizes the do-operator (Pearl, 2000), and x_{0},x_{1} are two distinct values attained by the binary variable X. Instead of just quantifying the causal effect through the ATE and the related quantities, researchers are often more broadly interested in determining which causal mechanisms transmit the causal influences from X to Y. Various approaches for solving this problem have been proposed under the rubric of causal mediation analysis, and the associated literature is vast (Baron and Kenny, 1986; Robins and Greenland, 1992; Pearl, 2001; Imai et al., 2010; VanderWeele, 2015). A common goal for many of the mediation methods is to decompose variations that are contained in the ATE into variations that are mediated by other variables (known as the indirect or mediated effect) and variations that are not mediated by other variables (known as the direct effect). Interestingly, mediation analysis focuses solely on causal variations, and the ATE captures the association of X and Y in an interventional regime, such as the randomized control trial (RCT), in which values of X are randomized to either x_{0} or x_{1}. This approach has proven tremendously useful for explaining causal effects, such as in testing the effects of drugs, understanding the impact of educational interventions, and evaluating policy changes. Often, however, researchers may be interested in explaining the association of X and Y in the natural, observational regime, without a specific intervention in mind. Somewhat surprisingly, the common approach for mediation falls short of answering simple questions such as “why do patients receiving chemotherapy have higher mortality rates than those not receiving it?”, or “why do coffee drinkers have higher rates of cardiovascular disease?”. In both of the examples, the causal relationship may account for only a part of the observed association, while non-causal (or spurious/confounded) effects also play an important role in explaining the phenomenon. In the former example, illness severity increases both the probability of receiving chemotherapy and dying, while in the latter, coffee drinkers are more likely to also smoke, which is a known determinant for cardiovascular disease. When explaining associations in the natural, observational regime, we may be interested in the quantity \displaystyle\mathbbm{E}[Y\mid X=x_{1}]-\mathbbm{E}[Y\mid X=x_{0}], (2) which we will refer to as the total variation (TV) measure, instead of the typically used ATE. The key difference between the ATE and the TV is that the latter encompasses confounded variations and not just the causal ones. As the approaches focusing on the ATE were called mediation analysis, in this paper, we call the approaches focusing on the TV measure variation analysis, to provide a distinction for a class of methods also concerned with effects that are neither direct nor mediated, but confounded. While mediation analysis has been successful in quantifying direct and indirect effects, far fewer approaches for handling interactions between direct and indirect effects have been proposed (VanderWeele, 2015). Furthermore, in the context of variation analysis, no methods exist to date that are intended for analyzing interactions, such as the interactions of spurious and causal effects. In the following example, we motivate some of the key developments appearing in this paper: XWY (a) Causal diagram of mediation. XZY (b) Causal diagram with observed confounding. Figure 1: Causal diagrams for Ex. 1. Example 1 (Total Effect and Total Variation Decompositions) Consider the causal diagram in Fig. 1(a), with treatment X, outcome Y, and mediator W. A key result from Pearl (2001) demonstrated that the average treatment effect (ATE) can be decomposed into \displaystyle\mathbbm{E}[Y\mid do(X=x_{1})]-\mathbbm{E}[Y\mid do(X=x_{0})]=% \underbrace{\mathbbm{E}[Y_{x_{1},W_{x_{0}}}-Y_{x_{0},W_{x_{0}}}]}_{\text{% natural direct effect}}-\underbrace{\mathbbm{E}[Y_{x_{1},W_{x_{0}}}-Y_{x_{1},W% _{x_{1}}}]}_{\text{natural indirect effect}}. (3) The natural direct effect (NDE) compares the potential outcome Y_{x_{0},W_{x_{0}}} in which X=x_{0} along both direct (X\to Y) and indirect (X\to W\to Y) pathways, vs. the potential outcome Y_{x_{1},W_{x_{0}}} in which X=x_{1} along the direct pathway, while W varies naturally at the level of X=x_{0}, written W_{x_{0}}. In this way, the NDE quantifies the effect of changing X from x_{0} to x_{1} along the direct pathway. To obtain the ATE, from the NDE, one subtracts the natural indirect effect (NIE) with a reverse transition, which measures the effect of changing X=x_{1} to X=x_{0} along the indirect path by comparing potential outcomes Y_{x_{1},W_{x_{1}}} vs. Y_{x_{1},W_{x_{0}}} which respond to X=x_{1} along the direct path. Often, the need to consider a transition x_{0}\to x_{1} in the NDE while subtracting a reverse transition x_{1}\to x_{0} for the NIE is criticized as a shortcoming of the widely used effect decomposition in Eq. 3. A result of VanderWeele (2015) sheds lights on the issue of opposite transitions appearing in the decomposition of the ATE. In particular, the ATE can also be decomposed in a different way: \displaystyle\text{ATE}_{x_{0},x_{1}}(y) \displaystyle=\underbrace{\mathbbm{E}[Y_{x_{1},W_{x_{0}}}-Y_{x_{0},W_{x_{0}}}]% }_{\text{natural direct effect}}+\underbrace{\mathbbm{E}[Y_{x_{0},W_{x_{1}}}-Y% _{x_{0},W_{x_{0}}}]}_{\text{natural indirect effect}} (4) \displaystyle+\underbrace{\mathbbm{E}[Y_{x_{1},W_{x_{0}}}-Y_{x_{0},W_{x_{0}}}-% (Y_{x_{1},W_{x_{1}}}-Y_{x_{0},W_{x_{1}}})]}_{\text{interaction effect}}. (5) Notably, in Eq. 4, the NDE and the NIE both appear with a transition of x_{0}\to x_{1}, with a baseline of Y_{x_{0},W_{x_{0}}}. There is an additional term, however, which compares how the direct effect changes according to the behavior of W, namely \displaystyle\mathbbm{E}[\underbrace{Y_{x_{1},W_{x_{0}}}-Y_{x_{0},W_{x_{0}}}}_% {x_{0}\to x_{1}\text{ DE with }W_{x_{0}}}-\underbrace{(Y_{x_{1},W_{x_{1}}}-Y_{% x_{0},W_{x_{1}}})}_{x_{0}\to x_{1}\text{ DE with }W_{x_{1}}}]. (6) In other words, the interactive effect compares how strongly the direct effect of a x_{0}\to x_{1} transition changes in the setting of W_{x_{0}} vs. W_{x_{1}}. The result of VanderWeele (2015) demonstrates that the key issue in interpreting Pearl’s decomposition (the appearance of effects with opposite transitions in X) is fundamentally due to interactions. The total variation (TV) measure, when considering the causal diagram in Fig. 1(b), can be decomposed as (Zhang and Bareinboim, 2018; Plečko and Bareinboim, 2024): \displaystyle\mathbbm{E}[Y\mid X=x_{1}]-\mathbbm{E}[Y\mid X=x_{0}]=\underbrace% {\mathbbm{E}[Y_{x_{1}}-Y_{x_{0}}\mid X=x_{0}]}_{x_{0}\text{-specific total % effect}}-\underbrace{\mathbbm{E}[Y_{x_{1}}\mid X=x_{0}]-\mathbbm{E}[Y_{x_{1}}% \mid X=x_{1}]}_{x\text{-specific spurious effect}}. (7) The x_{0}-specific total effect (also known as the effect of treatment on the treated, ETT) computes the effect of a x_{0}\to x_{1} transition for the subpopulation of individuals with X=x_{0}. To obtain the TV measure, from this effect we subtract the x-specific spurious effect which compares how conditioning on X=x_{0} differs from X=x_{1} while setting X=x_{1} along the causal pathways. Similarly to Eq. 3, the two effects appear with reverse transitions. The problem can be remedied again, by noting that the TV measure can also be decomposed as: \displaystyle\text{TV}_{x_{0},x_{1}}(y) \displaystyle=\underbrace{\mathbbm{E}[Y_{x_{1}}-Y_{x_{0}}\mid X=x_{0}]}_{x_{0}% \text{-specific total effect}}+\underbrace{\mathbbm{E}[Y_{x_{0}}\mid X=x_{1}]-% \mathbbm{E}[Y_{x_{0}}\mid X=x_{0}]}_{x\text{-specific spurious effect}} (8) \displaystyle+\underbrace{\mathbbm{E}[Y_{x_{1}}-Y_{x_{0}}\mid X=x_{1}]-% \mathbbm{E}[Y_{x_{1}}-Y_{x_{0}}\mid X=x_{0}]}_{\text{causal/spurious % interaction}}. (9) The additional term appearing in this new decomposition compares \displaystyle\underbrace{\mathbbm{E}[Y_{x_{1}}-Y_{x_{0}}\mid X=x_{1}]}_{x_{0}% \to x_{1}\text{ TE with }X=x_{1}\text{ conditioning}}\text{ vs. }\underbrace{% \mathbbm{E}[Y_{x_{1}}-Y_{x_{0}}\mid X=x_{0}]}_{x_{0}\to x_{1}\text{ TE with }X% =x_{0}\text{ conditioning}}, (10) and quantifies how much the total effect of a x_{0}\to x_{1} transition changes when conditioning on X=x_{1} vs. X=x_{0}. In this way, we can measure the strength of the interaction between spurious and causal paths. Our goal in this manuscript will be to provide a coherent umbrella for understanding interactions of causal pathways in variation analysis, namely direct, indirect, and spurious. The contributions of the paper can be summarized as follows: (i) We prove a first decomposition of the total variation (TV) measure that contains an explicit term for the interaction of causal and spurious pathways (Thm. 1), (ii) We develop the concept of interaction testing (Def. 11), in which an explicit interaction term that appears in a decomposition is subject to a hypothesis test of being equal to 0, (iii) We develop an algorithm for non-parametric interaction testing (Alg. 1), showing a common preference for parsimony in statistics: if the interaction term is not significantly different from 0, a more parsimonious TV decomposition may be used. (iv) We relate the effect interactions to the structural causal mechanisms of the underlying system (Def. 8), and demonstrate when there is a correspondence between a mechanism property and the corresponding interaction test (Prop. 2), (v) We provide the most general decomposition of the TV measure that provides accounts for all interactions between direct, indirect, and spurious effects (Thm. 2), (vi) We translate our results to the log-risk and log-odds scales (Appendix F), (vii) We perform an in-depth empirical analysis with the attempt to discover how commonly effects interact in practice (Sec. 5)."
https://arxiv.org/html/2411.08842v1,AstroM: A self-supervised multimodal model for astronomy,"While machine-learned models are now routinely employed to facilitate astronomical inquiry, model inputs tend to be limited to a primary data source (namely images or time series) and, in the more advanced approaches, some metadata. Yet with the growing use of wide-field, multiplexed observational resources, individual sources of interest often have a broad range of observational modes available. Here we construct an astronomical multimodal dataset and propose AstroM3, a self-supervised pre-training approach that enables a model to learn from multiple modalities simultaneously. Specifically, we extend the CLIP (Contrastive Language-Image Pretraining) model to a trimodal setting, allowing the integration of time-series photometry data, spectra, and astrophysical metadata. In a fine-tuning supervised setting, our results demonstrate that CLIP pre-training improves classification performance for time-series photometry, where accuracy increases from 84.6% to 91.5%. Furthermore, CLIP boosts classification accuracy by up to 12.6% when the availability of labeled data is limited, showing the effectiveness of leveraging larger corpora of unlabeled data. In addition to fine-tuned classification, we can use the trained model in other downstream tasks that are not explicitly contemplated during the construction of the self-supervised model. In particular we show the efficacy of using the learned embeddings for misclassifications identification, similarity search, and anomaly detection. One surprising highlight is the ""rediscovery"" of Mira subtypes and two Rotational variable subclasses using manifold learning and dimension reduction algorithm. To our knowledge this is the first construction of an n>2 mode model in astronomy. Extensions to n>3 modes is naturally anticipated with this approach.","Despite the vast volumes of publicly available raw astronomical data, with a few notable subfield exceptions, the application of machine learning to discovery and inference has yet to broadly permeate the field. One impediment stems from the challenge of fusing data across heterogeneous modes of collection. Off-the-shelf architectures do not easily accommodate a mixture of irregularly sampled multi-spectral multi-scale heteroskedatic time-series data, images, spectra, and metadata. Another issue, arising in the classification context, is that very few ground-truth labels exist. This “small label” problem arose, for example, in Richards et al. (2012), who sought to probabilistically classify 50,124 variable stars using only 810 labels over 28 classes. Last, models learned on a dataset from one survey do not easily transfer to other data collected on the same objects from different surveys (e.g., Long et al. 2012; Kim et al. 2021). Our self-supervised multimodal architecture addresses the first two challenges, establishing methods and milestones for a more generalized foundation model applicable to inference tasks on unseen survey data. Our work builds upon the Contrastive Language-Image Pretraining (CLIP) framework, originally introduced by Radford et al. (2021); CLIP demonstrated the power of contrastive learning on large-scale image and text datasets to learn joint representations. Since its introduction, CLIP has been extensively researched and improved in various ways. For example, Li et al. (2021) enhanced data efficiency through supervision, while Yao et al. (2021) focused on improving semantic alignment. Cherti et al. (2023) introduced scaling laws, and Sun et al. (2023) optimized the model for faster training. Additionally, CLIP has been combined with other pretraining objectives: Mu et al. (2022) incorporated image self-supervision, and Singh et al. (2022) along with Li et al. (2022) added masked multimodal, image, and language modeling. Furthermore, CLIP has been extended to other modalities: audio-text (Wu et al., 2023), video-text (Luo et al., 2021; Xu et al., 2021; Ma et al., 2022), and point cloud-text (Zhang et al., 2022). In the astronomical context, Parker et al. (2024) used dual-mode CLIP on static-sky galaxy images and spectra. Closest to the approach of our work outside of astronomy, Guzhov et al. (2022) adapted CLIP for use with three modalities: audio, image, and text. Given the proven versatility and success of CLIP in different domains, we build upon it herein. We extend CLIP to work on three modalities: time-series photometry, spectra, and metadata (see Figure 1). Our work, and a recent preprint from Zhang et al. (2024), are the first efforts to incorporate time-series data with CLIP, and our three-mode model represents a critical step towards the development of a foundational multimodal model for time-domain astronomy."
https://arxiv.org/html/2411.08832v1,Offline Adaptation of Quadruped Locomotion using Diffusion Models,"We present a diffusion-based approach to quadrupedal locomotion that simultaneously addresses the limitations of learning and interpolating between multiple skills (modes) and of offline adapting to new locomotion behaviours after training. This is the first framework to apply classifier-free guided diffusion to quadruped locomotion and demonstrate its efficacy by extracting goal-conditioned behaviour from an originally unlabelled dataset. We show that these capabilities are compatible with a multi-skill policy and can be applied with little modification and minimal compute overhead, i.e., running entirely on the robot’s onboard CPU. We verify the validity of our approach with hardware experiments on the ANYmal quadruped platform.","Quadruped robots’ ability to traverse complex terrain while carrying useful payloads makes them excellent choices for applications in manufacturing, construction and search & rescue. The state of the art for quadruped locomotion is maturing rapidly and both learning-based and gradient based methods are prevalent in research and industry. Learning-based methods such as reinforcement learning (RL) have produced impressive results [1, 2, 3, 4, 5, 6], but still suffer from a number of limitations. Due to the limited expressive capacity of neural network policies, using multiple skills in previous works required a hierarchical approach with a difficult multi-step training pipeline [5]. Since the methods are trained online, the resultant policies often can’t be adapted to new behaviours without being retrained from scratch. Our approach attempts to propose a solution to each of these two limitations in particular. To alleviate these issues, an alternative learning-based paradigm, imitation learning, can be used. This involves reproducing a set of reference motions, which popular approaches achieve with an adversarial training loop [7]. In recent years, Diffusion models [8, 9] have outperformed prior methods such as [10] and are able to approximate multi-modal distributions at a high fidelity. By approximating the score function [11] of a distribution, instead of directly learning the densities, and iteratively applying Langevin dynamics [12] to generate samples, they are able to accurately represent highly multi-modal distributions, incorporate flexible conditioning [13], and also scale well with dataset and model size. This multi-modality allows us to circumvent the need for hierarchical planning, as previous works have shown diffusion policies are capable of learning multiple skills in a single model [9]. Figure 1: Our diffusion-based approach possesses two capabilities: interpolation between distinct skills or operation modes (walk to a low crawl) and offline adaption after training. We leverage diffusion models to approximate multi-modal distributions to a high fidelity and also show that using classifier-free guidance can achieve offline adaption after training to produce novel locomotion behaviours. The guidance is used to generate trajectories that maximise the return of new reward functions unseen during data collection. Though large datasets are prevalent and can fuel imitation learning, there are limitations. There is no guarantee that the dataset contains trajectories which maximise a desired test time reward. For example, our locomotion dataset may contain a broad range of motions where the robot is moving with different gaits and speeds, but at test time we require specific behaviours. If data is labelled it is easy to extract this desired behaviour, but this is often not the case making this problem challenging. Therefore, we would like to tune our diffusion model to stitch together locomotion trajectories which maximise the return of reward functions for certain types of motion. To tackle the challenge of offline adaptation, we propose using classifier-free guidance (CFG) [14] to optimise a diffusion-model trajectories after training. This is achieved by treating the expected return as a probability function [13, 15] and using an Offline RL [16] training paradigm. In particular, we label the dataset with a velocity tracking reward and use CFG to guide our trajectories to maximise it, thus adapting our model to generate the desired behaviour. In this paper we present a diffusion-based approach to quadruped locomotion, capable of switching between discrete skills and adapting to new reward functions offline. Our contributions are as follows: 1. We present the first application of a stochastic differential equation-based diffusion framework to quadruped locomotion and use it to train a policy capable of interpolating between discrete skills. 2. We apply classifier-free guidance to perform Offline RL and adapt our policy to new goal-conditioned behaviour. 3. We deploy onto real hardware and demonstrate the benefits of fast sampling with a model that can run entirely on the robot’s onboard CPU. We evaluate our approach on a set of locomotion tasks both in simulation and on hardware using an ANYbotics ANYmal quadruped robot [17]."
https://arxiv.org/html/2411.08790v1,Can sparse autoencoders be used to decompose and interpret steering vectors?,"Steering vectors are a promising approach to control the behaviour of large language models. However, their underlying mechanisms remain poorly understood. While sparse autoencoders (SAEs) may offer a potential method to interpret steering vectors, recent findings show that SAE-reconstructed vectors often lack the steering properties of the original vectors. This paper investigates why directly applying SAEs to steering vectors yields misleading decompositions, identifying two reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in feature directions, which SAEs are not designed to accommodate. These limitations hinder the direct use of SAEs for interpreting steering vectors.111Code is available at https://github.com/HarryMayne/SV_interpretability","As language models advance, there is growing interest in methods to steer their behaviours toward desirable characteristics [1]. Recently, steering vectors (or activation steering) have been proposed as a way to achieve this without requiring model fine-tuning [20, 13, 10, 23]. Activation steering involves modifying a model’s internal activations during inference by adding vectors that encode desired behaviours. These methods have shown the potential to regulate behaviours such as sycophancy [13], harmlessness [23], and refusal [2, 23]. Despite promising empirical results, the underlying mechanisms behind steering vectors remain poorly understood [9, 4]. Interpreting steering vectors by decomposing them into granular features may help clarify why certain behaviours are more steerable than others [18], why combining steering vectors is largely unsuccessful [22], and may help produce more precise steering vectors [8]. Recent work has explored interpreting steering vectors using sparse autoencoders (SAEs) [4, 8]. SAEs are an emerging method for decomposing model activations into sparse, non-negative linear combinations of vectors, where many vectors appear to correspond to meaningful, interpretable concepts [5, 3]. Since steering vectors exist within the same space as model activations, they could theoretically be expressed as combinations of SAE features [4, 8, 9]. However, past studies found that directly decomposing steering vectors with SAEs produced mixed results, with the reconstructed vectors often failing to retain the steering properties of the original vectors. This suggests that the SAE decompositions did not capture essential elements of the steering vectors [8]. Motivated by these mixed results, this paper investigates the theoretical reasons why SAEs provide misleading decompositions of steering vectors and supports each reason with empirical evidence. We identify two main reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in SAE feature directions, which SAEs are not designed to accommodate. These issues limit the direct application of SAEs for interpreting steering vectors. Our contributions are to highlight these issues, motivating new methods to address them."
https://arxiv.org/html/2411.08785v1,Zero-shot Cross-lingual Transfer Learning with Multiple Source and Target Languages for Information Extraction: Language Selection and Adversarial Training,"The majority of previous researches addressing multi-lingual IE are limited to zero-shot cross-lingual single-transfer (one-to-one) setting, with high-resource languages predominantly as source training data. As a result, these works provide little understanding and benefit for the realistic goal of developing a multi-lingual IE system that can generalize to as many languages as possible. Our study aims to fill this gap by providing a detailed analysis on Cross-Lingual Multi-Transferability (many-to-many transfer learning), for the recent IE corpora that cover a diverse set of languages. Specifically, we first determine the correlation between single-transfer performance and a wide range of linguistic-based distances. From the obtained insights, a combined language distance metric can be developed that is not only highly correlated but also robust across different tasks and model scales. Next, we investigate the more general zero-shot multi-lingual transfer settings where multiple languages are involved in the training and evaluation processes. Language clustering based on the newly defined distance can provide directions for achieving the optimal cost-performance trade-off in data (languages) selection problem. Finally, a relational-transfer setting is proposed to further incorporate multi-lingual unlabeled data based on adversarial training using the relation induced from the above linguistic distance. Experimental results on two practical multi-lingual IE tasks demonstrate our method significantly outperforms baselines across tasks and languages simultaneously. Additionally, by carefully designing the multi-lingual training to utilize data from relevant languages, we can achieve a substantial boost in generalization ability with reasonable labor cost for the additional data collection.","The objective of Information extraction (IE) is to identify and extract structure information, such as entities, relations, and events, from natural unstructured text. IE plays an important role in various downstream applications, including Question Answering, Knowledge Graph Construction, News Analysis, etc. Solving IE tasks pose significant challenges for NLP models as they often require the understanding of complex features of natural languages. For example, to extract relations within a sentence, models first need to learn specialized structures of the corresponding language to identify entities mentioned in the given text. Next, a deep understanding of context is required to correctly classify the relations between these entities. These challenges are further exacerbated in multilingual settings, where datasets are collected from multiple languages, each of which contains language-specific characteristics and structures. The rapid development around English-based datasets has pushed machine performance to be on par with human ability in English tasks, prompting recent works to explore NLP research in other languages Liang et al. (2020); Ruder et al. (2021). However, despite advanced large-scale architectures and high English results, current models notably under-perform in new languages, especially those that are considered low-resource and lack high-quality datasets for fine-tuning. Cross-lingual Transfer, as a result, becomes one of the most important directions in the field. Given a particular task, the goal of Cross-lingual Transfer is to train multilingual models over high-resource source languages that can solve textual tasks in new target languages despite the shifts in linguistic origin. Currently, the most popular and practical approach for IE involves Zero-Shot Cross-Lingual (ZSCL) transfer Conneau et al. (2020); Goyal et al. (2021). These methods fine-tune Transformer-based multilingual Language Models (mLMs), which were pre-trained using unlabeled text from hundreds of languages, for downstream tasks using high-resource source-language labeled datasets (predominantly English). The resulting models are directly used for evaluation on the corresponding tasks in target languages. Studies have shown, however, performance of these multi-lingual models varies substantially across languages and tasks. Several factors have been attributed to this phenomenon, ranging from data-dependent statistic(e.g. dataset size, word overlap) Malkin et al. (2022), to data-independent features (e.g. phylogenetic and typological features) Lin et al. (2019); Dolicki and Spanakis (2021). Based on these previous observations, we believe that there is a deeper connection between cross-lingual transfer ability and the relations in the linguistic landscape. Unraveling this correlation can provide tremendous practical implications for IE. First, it serves as a guide for data collection process to achieve optimal cost-performance trade-off by gathering training samples from appropriate source languages for a target language. Furthermore, the modeling process can also be tailored such that the learned representations explicitly capture the linguistic relations to improve generalization across languages. Previous papers following the above direction define the problem as a Performance Prediction task. In Lin et al. (2019); Dolicki and Spanakis (2021); Srinivasan et al. (2021), a regression model is trained to take linguistic features of the source-target language pairs as input to predict a trained model performance scores on target languages. Despite high prediction accuracy, these works are insufficient for the following two reasons. First, they place too much emphasis on the accuracy of the regression model, which is trained for a specific architecture on a particular task. As the training configuration varies widely in practice, the results obtained from the performance prediction models may become unreliable and not applicable in general. Another reason is that previous work is only limited to the setting of single-transfer between two languages, in which only one source language (predominately English) is utilized. Current advances in translation model and data gathering process have enabled the creation of datasets in many languages, thus multiple source languages should be considered. Intuitively, additional training data from more languages can help improve model’s generalization on downstream tasks, and learning from text in multiple languages may have a positive effect on zero-shot transfer. We believe that multi-transfer setting is the next important step for cross-lingual transfer, both to improve model performance across languages and to provide a more complete picture of multi-linguality in machine learning. In this paper, we focus on what has been missing in previous works by aiming to answer the following three major research questions: Q1: How do the relations in the linguistic landscape affect an IE model’s cross-lingual transfer ability? We use URIEL Typological Database Littell et al. (2017) to extract phylogenetic and typological properties of each language. These properties, represented as multi-dimensional binary features, are used to compute the pair-wise linguistic distances (or equivalently the similarity scores) between languages. We compare the correlation between these scores and model single-transfer performance. A source language with a high correlation value would imply that we can infer its ability to transfer to different languages using only linguistic relations, without the need to actually fine-tune models. Q2: Can we implicitly leverage these linguistic features as dataset-independent knowledge to efficiently address the more general multi-transfer setting? While many-to-many cross-lingual transfer has the potential to significantly improve single-transfer performance, it would also require gathering data from multiple languages. Given a set of languages and their linguistic features as the only prior information, we aim to find the optimal subset of source languages to gather labeled data for zero-shot cross-lingual multi-transfer to target languages. The goal is to achieve the best cost-performance trade-off on all languages, without having to fine-tune the mLMs on an exponential number of possible language combinations. Q3: Can we explicitly integrate these linguistic relations in the learning process to effectively improve multi-transfer performance? We then investigate further into the possibility of directly embedding the linguistic relations in the fine-tuning process. The hypothesis is that, by capturing these connections, the multi-lingual representations would be able to adaptatively generalize to not only languages that are closely related to source languages, but also distant languages that share little similarity with the available training data. The following observations are obtained from our quantitative experiments and qualitative analysis, through 3 levels of transfer settings: 1) Single-transfer (ZSCL-S) - Only 1 labeled source language available. It is possible to achieve a high degree of correlation between model ZSCL performances and linguistic relations, using a combination of syntax, inventory, and phonology features from URIEL. However, in contrast to prior works which only focus on syntactic transfer when fine-tuning, our combined metric places the least importance on the syntax feature. This implies that previous researches are suboptimal and incomplete, prompting further investigations into the problem. 2) Multi-transfer (ZSCL-M) - Multiple labeled source language available. We first cluster languages based on the combined metric above. Then, by selecting source languages following the guidance from the resulting clusters, we observe significant improvements in ZSML performances over the naive method of randomly picking source languages. In other words, with only the prior linguistic knowledge, we can efficiently choose a suitable small subset of languages for labeled data annotations, to fine-tune a MMLM to perform best on a given set of languages. 3) Relational-transfer (ZSCL-R) - ZSCL-M with additional multi-lingual unlabeled data. We leverage unlabeled data from all available languages and their linguistic relations as inputs to graph-relational adversarial learning framework Xu et al. (2022b), a generalization of adversarial language adaption Chen et al. (2018) that can only perform strict uniform alignment for pair-wise transfer. By conditioning the multi-lingual representation flexibly on the connections expressed by the corresponding language relational-graph, we achieve a considerable increase in transfer performances across every language. This is only at the small cost of collecting additional unlabeled data from other languages. SMILER % MINION % ita 19.71 eng 39.76 fra 16.25 pol 13.7 deu 13.75 tur 13.7 por 11.54 nld 10.38 eng 9.57 spa 9.99 kor 5 por 4.59 pol 4.5 swe 4.59 spa 2.95 ara 2.49 rus 1.71 hin 4.58 swe 1.2 kor 4.58 fas 0.7 jpn 4.5 ukr 0.26 Table 1: Percentage distributions of training data in each task for every language, which are separated into high, medium, and low resource categories. The shared languages are color-coded, with red indicating that the language belongs to a different category between the two tasks, whereas green indicates otherwise. This study involves a total of 17 languages including: arabic (ara), german (deu), english (eng), farsia (fas), french (fra), hindi (hin), italian (ita), japanese (jpn), korean (kor), dutch (nld), polish (pol), portuguese (por), russian (rus), spanish (spa), swedish (swe), turkish (tur), and ukrainian (ukr). Figure 1: The pairwise Pearson correlation for all computed language distances."
https://arxiv.org/html/2411.08768v1,Sharingan: Extract User Action Sequence from Desktop Recordings,"Video recordings of user activities, particularly desktop recordings, offer a rich source of data for understanding user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using a basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research.","Video recordings are increasingly favored for capturing user activities due to their ease of implementation and broad applicability. Moreover, video’s universal compatibility across platforms and devices, combined with its ability to capture detailed and context-rich data, ensures minimal information loss and facilitates thorough analysis. Recent advancements in Vision-Language Models (VLMs) [21, 20, 8, 24, 5, 14, 1] have significantly improved the utility of video recordings. These AI-driven models automate the interpretation and extraction of insights from video data, enhancing the identification of user behaviors and patterns. Combined with the increasing prevalence of AI-integrated hardware [9, 22], these technological innovations are accelerating the adoption of video as an essential tool for documenting and analyzing user activities. Despite extensive research into understanding user actions from various types of videos [7, 12], there remains a lack of focus on desktop recordings. Addressing this gap is crucial, as extracting user actions from desktop videos offers numerous benefits. For instance, it can enhance Robotic Process Automation (RPA) by utilizing demo videos as input, increasing productivity through automation [10]. Moreover, desktop video analysis facilitates the automatic creation of tutorials and guidelines, while also enabling the extraction of personalized interaction patterns, which can be leveraged elsewhere to create a more personalized user experience. We propose two VLM-based methods for extracting user action sequences from desktop recordings. In the Direct Frame-Based Approach (DF), sampled video frames are directly input into VLMs, while the Differential Frame-Based Approach (DiffF) first detects frame changes using computer vision techniques before interpreting them with VLMs. The key difference lies in whether explicit frame differences are incorporated to aid action inference. We evaluate both methods using two benchmark datasets: one crafted by us, focusing on individual action types, and the other adapted from GUI-World [3] which better reflects real-world scenarios. Experimental results reveal that current VLMs show great potential in extracting user actions from desktop recordings. For instance, using the DF approach, we achieve an accuracy of 70\%\sim 80\% in identifying operation types (e.g., click), and the extracted action sequences are replayable through RPA-like processes. Moreover, comparing the two approaches reveals that VLMs struggle to utilize UI changes derived explicitly, sometimes leading to performance degradation. Thus, we recommend the Direct Frame-Based Approach, relying on VLMs’ inherent ability to infer actions, as the current best practice for action extraction. Our contributions can be summarized as follows: • We introduce two VLM-based methods to address the gap in existing research on extracting user action sequences from desktop recordings. To the best of our knowledge, this is the first attempt to leverage VLMs for this task. • We develop two benchmark datasets to assess the performance of methods on this task. All evaluation source codes and benchmarks will be made publicly available. • We perform a comprehensive evaluation of the proposed methods using the developed benchmarks."
https://arxiv.org/html/2411.08767v1,"SANDWICH: Towards an Offline, Differentiable, Fully-Trainable Wireless Neural Ray-Tracing Surrogate","Wireless ray-tracing (RT) is emerging as a key tool for three-dimensional (3D) wireless channel modeling, driven by advances in graphical rendering. Current approaches struggle to accurately model beyond 5G (B5G) network signaling, which often operates at higher frequencies and is more susceptible to environmental conditions and changes. Existing online learning solutions require real-time environmental supervision during training, which is both costly and incompatible with GPU-based processing. In response, we propose a novel approach that redefines ray trajectory generation as a sequential decision-making problem, leveraging generative models to jointly learn the optical, physical, and signal properties within each designated environment. Our work introduces the Scene-Aware Neural Decision Wireless Channel Raytracing Hierarchy (SANDWICH), an innovative offline, fully differentiable approach that can be trained entirely on GPUs. SANDWICH offers superior performance compared to existing online learning methods, outperforms the baseline by 4e^{-2} \unit in RT accuracy and only fades 0.5 \unit away from toplined channel gain estimation.","Wireless ray-tracing (RT) approaches model interactions between electromagnetic (EM) waves and their physical environment. Recent years have seen increased interest in wireless RT due to its potential to capture complex signal behaviors, especially in B5G and 6G scenarios [amiot2013pylayers, choi2023withray, orekondy2023winert, zhao2023nerf2, hoydis2023sionna, he2018design, lai2024real, zhang2024wisegrt, tarneberg2023towards]. Traditionally, RT relies on computationally intensive Maxwell equations (ME) or specialized wireless raytracers, where complex solvers [rtem] are introduced by integrating heuristics from geometrical optics (GO) and uniform theory of diffraction (UTD). Thanks to computational EM theory [18706], one can use shooting and bouncing rays sequence (SBS) while being GO/UTD-compliant to approximate EM waves in high frequency. Wireless RT Surrogate. Recently research has been exploring learning-based methods as RT surrogates [orekondy2023winert], aiming to reduce the computational costs of traditional wireless ray tracers and improve their scalability. In this context, Wireless RT is typically formulated as a sparsely-supervised and non-differentiable objective. Since the transmitter (Tx) emits a large volume of GO/UTD-feasible rays, only a small, countable subset of these rays are relevant for wireless reception at the receiver (Rx). Figure 1: Schematic Representation of SANDWICH: 1, Segment wireless RT and 3D environment. 2, Sequentialize RT into SBS. 3, Train neural surrogate & apply auto-regressive generation. 4, Apply generated RT to channel model. Several approaches have been proposed in the literature to address such challenges. For example, \citetorekondy2023winert mitigate these issues through online, continuous feedback mechanisms, such as temporal difference (TD) learning. Alternatively, other approaches, such as those in [hoydis2023sionna] and [choi2023withray], employ monte carlo (MC) methods to collect and refine a diverse set of ray trajectories. Nevertheless, existing methods are hindered by several significant limitations: 1. Dependence on real-time feedback from radio environment modeling or RT engines during training, 2. Incompatibility with GPU-based training, and 3. Lack of support for vectorization, thus limiting the ability to process batches of rays efficiently. Proposed Method. In this paper, we address wireless RT in indoor environments, considering Tx/Rx locations, signal-surface interaction, and surface texture information. We propose scene-aware neural decision wireless channel raytracing hierarchy (SANDWICH): a novel transformer-based offline learning method to perform wireless RT. SANDWICH addresses the aforementioned limitations by allowing learning from a limited set of collected RT results, without assumptions on the radio environment or supervision density. Additionally, our goal is to develop efficient and scalable wireless RT surrogates that eliminate the dependency on online supervision, and enable a vectorized, GPU-trainable neural wireless RT surrogate. To achieve our objectives, and inspired by the success of generative pre-trained transformers (GPTs), we suggest that wireless RT can be modeled in a sequence modeling space, where SBS in wireless ray can be generated similarly to token sequences, utilizing the transformer’s [vaswani2017attention] capability of learning wireless RT patterns in such sequence. We house the sequence auto-regressive generation task within an offline reinforcement learning (RL) scheme, enforcing the model to internalize the radio environment as a constraint during SBS generation to unhook the requirement for online supervision from well-parameterized radio environment in state-of-the-arts (SOTA). We illustrate a schematic representation of the proposed solution in Fig. 1, including 4 major steps: 1. Segment raw data into 3D environment and channel information. 2. Create tailored gym environment for test-time verification with OOD Tx & Rx locations. The ray’s trajectory is also transformed into true & augmented SBS for transformer models. 3. Train the proposed model and generate SBS on novel (Tx, Rx) distribution. 4. Besides the generated wireless RT geometrical accuracy, the generated SBS are utilized as priors for channel model to enhance performance in downstream tasks within a Turbo Learning [zhao2023nerf2] framework. Our method leverages a customized decision transformer (DT) [chen2021decision] to integrate sparse supervision through expected returns in a markov decision process (MDP), thereby decoupling the need for continuous online supervision from the environment. Additionally, we propose a data augmentation technique based on Fibonacci sphere [rogne2022raytracing] to generate stochastic trajectories that enhance DT’s generalization on out-of-distribution (OOD) test samples, alongside state supervision to enforce environment awareness. Our Contribution. We evaluate our approach using metrics on wireless ray accuracy and validating its effectiveness as a prior for channel estimation tasks. SANDWICH outperforms the online learning solution by 4e^{-2} \unit in RT accuracy and only fades 0.5 \unit away from GT-ray-powered channel gain estimation, while outperforming all non-RT based baselines with generated wireless RT results. Our method is end-to-end GPU-trainable, fully differentiable, and vectorizable."
https://arxiv.org/html/2411.08764v1,Flow reconstruction in time-varying geometries using graph neural networks,"The paper presents a Graph Attention Convolutional Network (GACN) for flow reconstruction from very sparse data in time-varying geometries. The model incorporates a feature propagation algorithm as a preprocessing step to handle extremely sparse inputs, leveraging information from neighboring nodes to initialize missing features. In addition, a binary indicator is introduced as a validity mask to distinguish between the original and propagated data points, enabling more effective learning from sparse inputs. Trained on a unique data set of Direct Numerical Simulations (DNS) of a motored engine at a technically relevant operating condition, the GACN shows robust performance across different resolutions and domain sizes and can effectively handle unstructured data and variable input sizes. The model is tested on previously unseen DNS data as well as on an experimental data set from Particle Image Velocimetry (PIV) measurements that were not considered during training. A comparative analysis shows that the GACN consistently outperforms both a conventional Convolutional Neural Network (CNN) and cubic interpolation methods on the DNS and PIV test sets by achieving lower reconstruction errors and better capturing fine-scale turbulent structures. In particular, the GACN effectively reconstructs flow fields from domains up to 14 times larger than those observed during training, with the performance advantage increasing for larger domains.","The reconstruction of spatial fields from sparse and limited data is a major challenge in the analysis, estimation and control of complex physical systems. In various fields such as atmospheric research (Tello Alonso et al., 2010; Mishra et al., 2014), autonomous aerial navigation (Achermann et al., 2019, 2024) and fluid dynamics (Fukami et al., 2019), conventional linear methods such as linear stochastic estimation (Adrian and Moin, 1988), Delaunay triangulation (Saini et al., 2016) and proper orthogonal decomposition (Bui-Thanh et al., 2004; Druault et al., 2005) face challenges in accurately reconstructing extensive spatial patterns. This is especially true when dealing with very sparse data and problems such as system nonlinearity and boundary effects. Neural Networks (NN) have emerged as a promising nonlinear alternative that has proven to be effective in efficiently reconstructing chaotic data from sparse information. Building on this foundation, Machine Learning (ML) has achieved remarkable success in generating flow fields using data from experimental observations and numerical simulations (e.g. Kissas et al. (2020); Shengnan et al. (2019); Cai et al. (2019)). Recent work has demonstrated the power of ML approaches in fluid dynamics applications. Morimoto et al. (2021) used a Convolutional Neural Network (CNN) to analyze artificial Particle Image Velocimetry (PIV) data, and propose a novel approach for reconstructing flow fields from snapshots containing regions with missing data. Kochkov et al. (2021) employed an end-to-end CNN-based model to improve approximations inside Computational Fluid Dynamics (CFD) domains for modeling two-dimensional (2D) turbulent flows. Manickathan et al. (2022) proposed CNN as an alternative to the image cross-correlation methods commonly used in processing PIV data to reconstruct the fluid velocity field, and it was shown to outperform conventional methods in terms of robustness to data noise and providing reconstructed velocity fields with significantly higher spatial resolution. Super-Resolution (SR) methods have also been used to reconstruct high-resolution flow fields from low-resolution data. Fukami et al. (2019) utilized a CNN and a hybrid downsampled skip-connection/multi-scale (DSC/MS) CNN model, for the SR analysis of turbulent flow fields from very coarse data. Kim et al. (2021) used the cycle-consistent generative adversarial network (CycleGAN) to reconstruct flow fields from low-resolution DNS and Large Eddy Simulation (LES) data. Bode et al. (2021, 2023) developed a physics-based super-resolution generative adversarial network (PIESRGAN) for subfilter-scale turbulence reconstruction that uses a loss function based on the continuity equation residue. Even though only homogeneous isotropic data was used to train the model, it was able to make better predictions of scalar mixing in a reacting jet. The aforementioned models rely on convolutional layers and can therefore be applied to unstructured data only to a limited extent. Since conventional ML methods require a feature matrix with a specific size and order of input samples, they cannot be readily applied to unstructured data. However, flow field data can be highly unstructured due to irregular meshes in curved or complex geometries. Several approaches have been developed to address the limitations of CNN when handling unstructured data in fluid dynamics. Heaney et al. (2024) proposed using space-filling curves to transform multi-dimensional solutions on unstructured meshes into a one-dimensional (1D) representation, allowing for the application of 1D convolutional layers. Xu et al. (2021b) developed an unstructured CNN that aggregates and exploits features from neighboring nodes through a weight function, enabling convolutions on irregular grids. Kashefi et al. (2021); Kashefi and Mukerji (2022) explored point cloud deep learning frameworks where CFD grid vertices are treated as point clouds and used as inputs to neural networks. Graph Convolutional Networks (GCN) have also emerged as a powerful tool for unstructured data. He et al. (2022) introduced the flow completion network (FCN) that employs a GCN to deduce fluid dynamics from sparse data sets. Duthé et al. (2023) also used a GCN to predict the flow field and far-field boundary conditions based on the pressure distribution at the surface of airfoils. Despite these advancements, several challenges remain in applying ML techniques to fluid dynamics problems. One significant limitation is the difficulty in handling extremely sparse data, where traditional interpolation methods often fail to capture complex flow features. Additionally, time-varying geometries pose a unique challenge, as most current ML models are designed for static configurations and struggle to adapt to dynamic changes in the flow domain. Another critical issue is the generalization of ML models across different types of data sets, from high-fidelity numerical simulations to experimental measurements. The discrepancies in data quality, resolution, and underlying physics between these sources can lead to poor accuracy when models trained on one type of data are applied to another. In this paper, we introduce a Graph Attention Convolutional Network (GACN) trained on a unique data set of three-dimensional (3D) Direct Numerical Simulations (DNS) modeling the compression-expansion stroke of a single-cylinder Internal Combustion Engine (ICE) under practically relevant operating conditions. The DNS data presents specific challenges for conventional ML applications due to its unstructured grid that dynamically changes during compression, leading to variations in resolution. GACN effectively addresses both aspects: the unstructured nature of the data is captured by the position of the graph nodes, while the changing resolution is accounted for by the distance features of the edges between nodes. Furthermore, we present a method to handle extremely sparse data, which performs remarkably well even when 99% of the data is missing. This is achieved by incorporating a Feature Propagation (FP) algorithm as a preprocessing step and adding a Binary Indicator (BI) as an extra feature. The FP algorithm initializes absent features with values that are both reasonable and physically consistent, leveraging data from neighboring nodes. The BI serves as a validity mask, providing crucial information to the network about which data points are original and which are propagated, enabling a more effective learning from sparse inputs without the limitations of using default values for missing data. Notably, we demonstrate the robustness and versatility of the approach by successfully applying the model trained on DNS data to experimental PIV measurements, achieving promising results despite the inherent differences between numerical and experimental data sources. The rest of the paper is organized as follows. Section 2 introduces the DNS and PIV data sets used for training and evaluating the model. In Sec. 3, we provide a detailed description of the GACN model, as well as the alternative models used for comparison, specifically a CNN and classical cubic interpolation. Section 4 presents a comprehensive analysis of the model predictive accuracy using both numerical data derived from DNS simulations and experimental data from PIV measurements which was not used during training. We compare the performance of the GACN model to a CNN architecture for which data was interpolated onto a uniform mesh and classical cubic interpolation. The main conclusions and future research directions are outlined in Sec. 5."
https://arxiv.org/html/2411.08745v1,Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers,"A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing latent representations (latents) during a word translation task in transformer-based LLMs. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean over latents across different languages does not impair and instead improves the models’ performance in translating the concept. Our results provide evidence for the existence of language-agnostic concept representations within the investigated models.111This work has been previously published under the title ”How Do Llamas Process Multilingual Text? A Latent Exploration through Activation Patching” at the ICML 2024 mechanistic interpretability workshop https://openreview.net/forum?id=0ku2hIm4BS.Code for reproducing our experiments is available at https://github.com/Butanium/llm-lang-agnostic.","The emergence of the field of mechanistic interpretability has led to the conception of powerful tools Carter et al. (2019); Nostalgebraist (2020); Schubert et al. (2020); Belrose et al. (2023); Cunningham et al. (2023); Kramár et al. (2024); Marks et al. (2024); O’Neill and Bui (2024); Tufanov et al. (2024) for the investigation of the inner workings of deep neural networks such as large language models (LLMs) Vaswani et al. (2017); Radford et al. (2019); Touvron et al. (2023) with the ultimate goal of reverse engineering the algorithms encoded in their weights. As a result, researchers today are often able to open up a “black box” neural network, and with near surgical precision pinpoint where a certain input-output behaviour comes from Wang et al. (2022); Conmy et al. (2023); Nanda et al. (2023); Zhong et al. (2024); Furuta et al. (2024). This provides a unique opportunity to examine how multilingual concepts are represented and processed within LLMs, potentially revealing insights into language biases and concept formation. In particular, approaches based on activation patching Variengien and Winsor (2023); Ghandeharioun et al. (2024); Chen et al. (2024) in which activations are patched from one forward pass into another one while observing the output (c.f. Fig. 2) present a simple, yet effective way to inspect the representations learned and computations performed by LLMs. Summary of contributions. In this work, we leverage activation patching to understand how LLMs process multilingual text, with a particular focus on whether they use a language-agnostic concept space, as theorized by Wendler et al. (2024). We focus on autoregressive, decoder-only transformers as they are the most common type of LLMs. In order to do so, we design multiple patching experiments leveraging pairs of translation prompts with differing expected predicted concept and language. 1. First, we perform an activation patching analysis of Llama 2 7B Touvron et al. (2023), patching at the last token position. We demonstrate that the model processes translation tasks by first resolving output language, then the concept to be translated. 2. We propose two competing hypotheses about how transformers solve the translation task during their forward pass: H1 where language and concepts are represented independently, and H2 where they are inherently entangled. We argue that if language and concepts are independent (H1), averaging the latent representation of a concept across languages should still allow the model to make sense of and utilize this representation. Conversely, if language and concepts are entangled (H2), this mean representation would be an incoherent mixture of language-specific concepts that the model cannot effectively use. 3. To test these hypotheses, we use a novel activation patching setup depicted in Figure 1 which forces Llama 2 7B to translate this mean representation across languages. We find that using the mean concept representation across languages improves Llama 2 7B’s performance on a word translation task, supporting H1. 4. Finally, in App. C we show that our observations generalize to a diverse set of transformer models varying in size, architecture, and training data, including Llama 2 70B, Llama 3 8B Dubey et al. (2024), Mistral 7B Jiang et al. (2023), Qwen 1.5 7B Bai et al. (2023) and Aya 23 8B Aryabumi et al. (2024). Our results align with the theory for Llama 2 outlined by Wendler et al. (2024) and with previous results on BERT models Devlin et al. (2018) by Wu et al. (2019); Pires et al. (2019). However, our analysis goes beyond the observational approach using the logit lens by Wendler et al. (2024) and cross-lingual embedding similarity analysis by Wu et al. (2019) and Pires et al. (2019). Using activation patching, we perform causal interventions that allow us to draw stronger conclusions about the computations performed and representations used within the models."
https://arxiv.org/html/2411.08724v1,QCG-Rerank: Chunks Graph Rerank with Query Expansion in Retrieval-Augmented LLMs for Tourism Domain,"Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in Large Language Models (LLMs) by integrating information retrieval techniques. However, in the tourism domain, since the query is usually brief and the content in the database is diverse, existing RAG may contain a significant amount of irrelevant or contradictory information contents after retrieval. To address this challenge, we propose the QCG-Rerank model. This model first performs an initial retrieval to obtain candidate chunks and then enhances semantics by extracting critical information to expand the original query. Next, we utilize the expanded query and candidate chunks to calculate similarity scores as the initial transition probability and construct the chunks graph. Subsequently, We iteratively compute the transition probabilities based on an initial estimate until convergence. The chunks with the highest score are selected and input into the LLMs to generate responses. We evaluate the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets. The experimental results demonstrate the effectiveness and superiority of the QCG-Rerank method. 11footnotetext: These authors contributed equally to this work. 22footnotetext: Corresponding author","Recently, LLMs, such as Llama[1], ChatGPT[2], and ChatGLM[3], have strong generative capabilities and are widely applied in question answering[4] and code generation[5][6]. LLMs are trained on large-scale corpora to learn deep semantic information, enabling them to generate corresponding responses based on queries. However, the generated responses sometimes contain inconsistencies with facts, leading to the “hallucinations” phenomenon[7]. In the tourism domain, LLMs often produce outdated addresses and route information, making the hallucination phenomenon particularly obvious[8]. To mitigate the hallucination issues of LLMs in the tourism domain, we use RAG to retrieve query-related contents in tourism and input them into LLMs. Subsequently, we use the matched contents to limit the scope of LLMs’ responses, ensuring that the generated output is more consistent with the user’s query[9]. Due to its outstanding performance, RAG has been widely applied in law[10][11] and medicine[12]. Fig. 1 illustrates the detailed processes LLMs with RAG and without RAG. Given the concise nature of queries in the travel domain and the inconsistent quality and length of candidate contents, similarity-based retrieval methods with travel documents often result in retrieving chunks irrelevant to or contradictory to the user’s query. Consequently, this misalignment can cause LLMs to summarize incorrect information, adversely affecting the model’s overall performance. Figure 1: The process of without RAG and with RAG. To address this issue, Wang et al.[13] utilized LLMs to generate pseudo-documents for the original query and concatenated them to expand the semantic complexity of the query. The enhanced query is then used for retrieval to improve the matching accuracy of relevant documents. Given that the documents retrieved in the initial stage based on similarity may have low semantic relevance or contradict the original query, Xiao et al.[14] proposed a rerank method based on Bge-embedding. This method aims to refine the initial retrieval by adding an additional rerank step that leverages semantic understanding to identify the relevant contents. The reranked contents are then summarized using the capabilities of LLMs to provide more accurate responses. Youdao[15] has developed bilingual and cross-lingual embedding (BCEmbedding) in English and Chinese. It includes two main components: the EmbeddingModel and the RerankerModel. The EmbeddingModel is designed to produce semantic vectors essential for enhancing semantic searches and facilitating question-answering processes. Meanwhile, the RerankerModel is adept at improving the quality of search outcomes and performing ranking tasks with precision. To mitigate the hallucination problem in RAG in the travel domain, it is crucial to mine information deeply from brief queries and retrieve relevant contents accurately. In this paper, we propose Chunks Graph Rerank with Query Expansion(QCG-Rerank). We first perform an initial retrieval using the query to obtain candidate contents. Next, we extract critical information from the query and duplicate it to expand its semantic complexity to improve matching reliability. Given that the initial retrieval results may contain overlapping contents with minimal differences in similarity, our reranking stage involves calculating the semantic similarity between the updated query and the candidate chunks. These similarity scores serve as initial transition probabilities to construct a chunks graph. After several iterations, the transition probabilities stabilize, and we choose the top-ranked chunks to input into LLMs, which summarize the relevant contents and generate the final output. This approach ensures that the chunks input into the large model are highly relevant to the query, thereby enhancing the reliability of the responses generated by the LLMs in the travel domain. We evaluate the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets. The experimental results demonstrate the effectiveness and superiority of the QCG-Rerank method. In summary, the contributions of our paper are as follows: 1. We extract critical information from the query and duplicate it to enrich the semantic complexity of the original query, thereby enhancing the relevance of the retrieved contents. 2. We propose a chunks graph reranking method that constructs a graph based on the similarities between the updated query and candidate chunks. Identifying the significant chunks within this graph enhances the accuracy of the reranking stage. 3. We evaluate QCG-Rerank on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets. The experimental results highlight the superior performance and accuracy of QCG-Rerank."
https://arxiv.org/html/2411.08706v1,Searching Latent Program Spaces,"Program synthesis methods aim to automatically generate programs restricted to a language that can explain a given specification of input-output pairs. While purely symbolic approaches suffer from a combinatorial search space, recent methods leverage neural networks to learn distributions over program structures to narrow this search space significantly, enabling more efficient search. However, for challenging problems, it remains difficult to train models to perform program synthesis in one shot, making test-time search essential. Most neural methods lack structured search mechanisms during inference, relying instead on stochastic sampling or gradient updates, which can be inefficient. In this work, we propose the Latent Program Network (LPN), a general algorithm for program induction that learns a distribution over latent programs in a continuous space, enabling efficient search and test-time adaptation. We explore how to train these networks to optimize for test-time computation and demonstrate the use of gradient-based search both during training and at test time. We evaluate LPN on ARC-AGI, a program synthesis benchmark that evaluates performance by generalizing programs to new inputs rather than explaining the underlying specification. We show that LPN can generalize beyond its training distribution and adapt to unseen tasks by utilizing test-time computation, outperforming algorithms without test-time adaptation mechanisms.","Program synthesis aims to automatically generate programs that satisfy a given specification, typically as input-output examples (Summers, 1977; Biermann, 1978). Although symbolic approaches have shown success in limited domains (Gulwani, 2011; Albarghouthi et al., 2013; Osera and Zdancewic, 2015; Feser et al., 2015; Frankle et al., 2016), they fail to scale to modern challenges involving large search spaces and complex patterns like in ARC-AGI (Chollet, 2019; Chollet et al., 2024). To handle the complexity and exponential search space of such difficult problems, neural approaches have emerged that aim to learn algorithms and programs from data (Graves, 2014; Kurach et al., 2015; Reed and De Freitas, 2015; Zaremba et al., 2016; Gaunt et al., 2016; Bunel et al., 2016; Bošnjak et al., 2017). Such methods are required as they can significantly narrow down the programmatic search space by leveraging biases learned through training, enabling more efficient search. Large language models (LLMs) have emerged as a particularly strong architecture for program synthesis tasks, with language pre-training helping to narrow the search space before any further problem-specific fine-tuning is performed (Austin et al., 2021). This can be particularly useful if generating problem-specific data is too expensive or limited. However, models trained on a specific program distribution will likely only generalize to problems close to this distribution and perform poorly on problems such as ARC-AGI, which are specifically designed to be out of distribution for LLMs (Gendron et al., 2023). Such generative neural methods lack mechanisms for systematic search at test time, with models usually resorting to stochastic sampling (Chen et al., 2021) or heuristic search (Zhang et al., 2023). Hottung et al. (2021b); Li et al. (2024a) and the ARC Prize 2024 leading team MindsAI explore fine-tuning a model on a test task. However, such fine-tuning is computationally expensive and highly prone to overfitting. This requires creating synthetic datasets on the fly, making fine-tuning a less scalable approach. To address these limitations, we introduce a general and scalable algorithm, Latent Program Network (LPN), to perform program induction learning (Sun et al., 2018). LPN builds a mechanism for test-time adaptation directly into the neural architecture, without the need for parameter updates, and utilizes a training objective suitable to test-time search. To perform adaptation, LPN leverages a continuous latent space to model a wide range of potential programs, which a neural decoder can then use to execute that program on a specific input. Note that our decoder directly generates outputs pixel by pixel instead of writing Python code that would execute the task. Contrary to most existing works, we train the neural architecture from scratch, as opposed to building on top of large-scale models such as LLMs. Instead, our goal is to generalize purely through a test-time adaptation mechanism, with as few priors as possible. Our key innovation is the ability to search through a structured latent space during both training and inference, enabling efficient adaptation at test time. We leverage gradient-based optimization in this latent space to find the latent program that best explains the given specification. This latent program can then be executed on new inputs to generate their corresponding outputs. Since this space is a highly compressed representation of input-output pairs, we can perform gradient ascent in this space without encountering potential overfitting that parameter-based fine-tuning methods face and therefore we do not require synthetic expansion. Our training objective ensures that we learn a structured latent space that is smooth and performs well with gradient-based optimization, allowing for more efficient program discovery. First, to assess the benefits of our latent-search architecture, we evaluate it on a simple subset of ARC-type programs. Second, we benchmark on ARC-AGI, a difficult program synthesis problem with a public train and evaluation set and hidden test set. In this work, we specifically choose to not enhance our results by using additional synthetic datasets, human or LLM generated, as we believe it is in the spirit of the ARC-AGI competition to only use priors from the training set. Specifically, we only use re-arc (Hodel, 2024) to generate input-output pairs that follow the programs of the training set. By training only on train set problems, we ensure no possibility of data leakage from the evaluation dataset, which likely occurs in methods leveraging pre-trained LLMs or LLM-generated programs (Li et al., 2024a). Our works’ main contributions are: 1. We introduce Latent Program Network (LPN) which directly builds in test time adaption into the architecture by learning a latent space representing the space of possible programs, enabling test time adaption of an output decoder by moving in this latent space. We train this with a novel training objective for learning the latent space that prevents encoding the output directly into the latent space. Instead, we encode an input-output pair to the latent space but train this representation to decode the output of a different input-output pair, which prevents the latent space from representing the output space instead of the program space. 2. We demonstrate that gradient-based search on the given specification in the latent space significantly improves the performance of LPN at test time compared to performance without latent search. 3. We show that adding gradient-based latent search during training enables the latent space itself to be trained such that gradient optimization in the space works well, resulting in significant improvement in sample efficiency. 4. We do not make use of any pre-trained models or LLM / human-generated synthetic data when evaluating our work in the ARC domain, aside from generating input-output pairs using re-arc (Hodel, 2024) based on the training set. This makes our method highly scalable and could be quickly applied to a different domain without requiring a domain-specific language, synthetic data, or pre-trained model. LPN can be applied to any problem for which a large enough number of input-output pairs from a given set of programs is available. In our current setting, we do not even apply enough compute during training to observe convergence indicating that improved results on ARC-AGI could be found simply by scaling training compute resources/parameter counts. 5. Our work directly refutes recent claims (Li et al., 2024b) that vision transformer architectures (Dosovitskiy, 2020) struggle to solve individual arc problems. We show this is not a bottleneck in making progress on ARC-AGI using purely neural approaches that predict outputs pixel by pixel."
https://arxiv.org/html/2411.08703v1,MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics Classification,"The distinct characteristics of multiomics data, including complex interactions within and across biological layers and disease heterogeneity (e.g., heterogeneity in etiology and clinical symptoms), drive us to develop novel designs to address unique challenges in multiomics prediction. In this paper, we propose the multi-view knowledge transfer learning (MVKTrans) framework, which transfers intra- and inter-omics knowledge in an adaptive manner by reviewing data heterogeneity and suppressing bias transfer, thereby enhancing classification performance. Specifically, we design a graph contrastive module that is trained on unlabeled data to effectively learn and transfer the underlying intra-omics patterns to the supervised task. This unsupervised pretraining promotes learning general and unbiased representations for each modality, regardless of the downstream tasks. In light of the varying discriminative capacities of modalities across different diseases and/or samples, we introduce an adaptive and bi-directional cross-omics distillation module. This module automatically identifies richer modalities and facilitates dynamic knowledge transfer from more informative to less informative omics, thereby enabling a more robust and generalized integration. Extensive experiments on four real biomedical datasets demonstrate the superior performance and robustness of MVKTrans compared to the state-of-the-art. Code and data are available at https://github.com/Yaolab-fantastic/MVKTrans.","With recent technological advances in acquiring high-throughput omics data, multiomics integration is evolving as a rapidly growing research field [1, 2]. Compared to single omics, which can only reflect a part of biological complexity from a certain perspective, integrating multiple omics types holds the capacity to capture complementary information from diverse biological layers. Multiomics integration has exhibited promising performance in various biomedical tasks, such as clinical prediction and disease subtyping [3]. Traditional approaches often involve statistical and machine learning models, which may have limited capacity to capture the complex, non-linear relationships present in multiomics data. In recent years, applying deep learning (DL) models to multiomics studies has emerged for addressing these limitations [4, 3, 5, 6]. Well-designed DL methods are capable of identifying complex patterns inherent in individual omics and integrating information from various sources, thereby enriching the analytical insight into underlying biological processes. Research on multiomics integration faces two main challenges: 1) the effective learning of feature representations and 2) the effective fusion of information derived from multiple omics. For the first challenge, different network architectures are employed to reduce redundancies and noises within high-dimensional omics data and produce meaningful and informative feature embeddings. A commonly used network is the encoder that compresses each modality into a non-linear latent space [1, 7, 8]. Autoencoder and its variants are also extensively used for learning efficient encodings of each omics type [9, 10, 11, 12]. Additionally, recent methodologies often represent omics data as graphs, leveraging graph neural networks to capture complex interactions effectively [13, 14, 15]. It is worth noting that existing approaches often integrate feature representation with downstream tasks, such as clinical classification and survival prediction. While these task-guided strategies are designed to extract outcome-related features, their effectiveness would be influenced by factors such as disease complexity, inherent label bias, and sample heterogeneity. Furthermore, the intrinsic noisiness and inconsistent distribution characteristic of omics data impose limitations on the robustness and generalizability of existing methods. On the other hand, effective modeling of complementary and interactive information among different omics can significantly improve the discriminative potential of fusion features [16]. However, the exploration in this specific research domain remains relatively limited. Regardless of the fusion stage—early, intermediate, or late—current approaches mainly use concatenation for fusing information from multiple sources [3, 12], overlooking the crucial inter-omics interactions. Recent strategies are developed to harness omics correlation in either intermediate or late fusion stages, demonstrating superior efficacy compared to simple concatenation [17, 13, 18]. For example, Mogonet [13] constructed the cross-omics tensor based on omics-specific classification probabilities and then employed the view correlation discovery network to incorporate label space omics correlations. Cross-modal attentions are also employed to capture inter-modality interactions [17, 7]. Alternatively, trustworthy fusion is proposed to evaluate the confidence level of each omics type and accordingly adapt to their quality differences [1, 19]. Despite the achievements of existing efforts, the intrinsic imbalances in informativeness and complex interactions among different omics can introduce irrelevant information during the fusion process, thus, in turn, hindering robust multiomics integration. Based on the above observations, we propose a multi-view knowledge transfer method (MVKTrans) for robust multiomics classification as illustrated in Fig. 1, which incorporates omics-specific pretraining (intra-view knowledge transfer (KT)) and cross-omics adaptive distillation (inter-view KT) to promote model stability and generalizability. Specifically, during the pretraining phase, we introduce omics-specific graph contrastive learning (GCL) to initialize the model parameters trained from unlabeled data, which facilitates a foundational understanding of latent structural patterns and interrelations within each omics type. Based on the pre-trained initializations, omics-specific graph attention networks (GAT) are built to generate initial predictions. Afterward, we introduce a cross-omics distillation (CD) module to facilitate adaptive knowledge transfer among disparate omics types, with a cross-omics attention module incorporated to model modality correlation and bridge the gap in label distributions. This endows the model with robustness for integrating heterogeneous omics sources. Our main contribution can be summarized as follows: • We model the multiomics data in a multi-view knowledge transfer framework to learn and pass the intra- and inter-omics knowledge in an adaptive manner. To the best of our knowledge, this is the first work to integrate both intra- and inter-modal transfer learning for robust multimodal classification. • To tackle real-world issues in multiomics data, such as label bias and omics informativeness imbalances, we specifically design an unsupervised pretraining module that employs graph contrastive learning to promote the learning of more general and unbiased representations, along with a CD module that adaptively transfers generalizable and reliable knowledge from information-rich to less informative modalities. • We conduct extensive experiments across four multiomics classification tasks to show the superiority of MVKTrans over the SOTA. Ablation and perturbation studies confirm the effectiveness and robustness of MVKTrans. Figure 1: Framework of MVKTrans. (a) The proposed method mainly comprises the following modules. i) Sample-similarity graphs construction. ii) Intra-omics KT: Using pre-trained parameters as initialization, GATs are employed to generate representations and produce omics-specific label distributions, followed by self-attention blocks to prioritize within-omics information. Auxiliary classifiers (AC) are trained to assist in learning more representative features. iii) Inter-omics KT: Cross-omics attention is incorporated with cross-omics distillation (CD) to capture interactions and reconcile distribution disparities among omics. iv) Optimized features are concatenated to make a final prediction. (b-d) illustrates the details of graph contrastive pretraining, self-attention and cross-omics attention, and cross-omics distillation, respectively."
https://arxiv.org/html/2411.08701v1,TRACE: Transformer-based Risk Assessment for Clinical Evaluation,"We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation), a novel method for clinical risk assessment based on clinical data, leveraging the self-attention mechanism for enhanced feature interaction and result interpretation. Our approach is able to handle different data modalities, including continuous, categorical and multiple-choice (checkbox) attributes. The proposed architecture features a shared representation of the clinical data obtained by integrating specialized embeddings of each data modality, enabling the detection of high-risk individuals using Transformer encoder layers. To assess the effectiveness of the proposed method, a strong baseline based on non-negative multi-layer perceptrons (MLPs) is introduced. The proposed method outperforms various baselines widely used in the domain of clinical risk assessment, while effectively handling missing values. In terms of explainability, our Transformer-based method offers easily interpretable results via attention weights, further enhancing the clinicians’ decision-making process.","1 Introduction Healthcare is an industry that can gain significantly by utilizing modern artificial intelligence (AI) and machine learning (ML) methods by assisting clinicians in diagnosis, risk assessment and pathology of diseases. By incorporating AI-driven risk assessment algorithms ([1, 2, 3]), clinicians can offer risk stratification of the patients for screening recommendations, which can help in early detection of diseases, enabling better informed decision-making from the clinicians, on the one hand, and timely intervention for patients that are considered high-risk, on the other. Healthcare data, typically collected using questionnaire-based surveys, exhibit a large diversity both in the nature, the quantity and the completeness of the attributes that are recorded (or reported) for each patient. Importantly, clinical data are multi-modal, comprising a combination of numerical (i.e., age, height, weight etc.) and categorical features (i.e., eye color, hair color, etc.) or even ‘‘checkboxes’’, where multiple values within the same feature are valid simultaneously (i.e., ancestry, doctors visited, etc.). On the other hand, missing values and other issues affecting clinical, and tabular data in general, also pose a significant challenge for maximizing data utilization. This is crucial in the case of healthcare data, as they are generally scarce, and their collection is laborious and with high cost, while AI, and deep-learning in particular, typically assume a large quantity of available data for training representative models. Finally, another characteristic of clinical data is the notable imbalance between case and control groups. These aspects, necessitate the development of task-specific AI and ML methods for the clinical domain. To address the data scarcity, effective methodologies dealing with clinical data should either enhance the data artificially (e.g., through data augmentation/imputation, generative models, etc.) or introduce ways to effectively utilize missing values, inconsistencies, and other issues of the available data. In this work, we propose a transformer-based [4] clinical risk assessment model operating across the spectrum of clinical data feature modalities that explicitly handles missing values, leading to improved performance with minimal computational overhead. Another key contribution of our work is the explainability provided through the generation of attention maps, which facilitates the interpretability of the produced results both for computer scientists and for clinicians. In summary, the contributions of this work are summarized below: • We propose a novel framework to perform clinical risk assessment using different data modalities that explicitly handles instances with missing values. • We introduce ‘‘checkbox embeddings’’ to handle features with multiple valid categories simultaneously, commonly extracted from questionnaire-based surveys. • The proposed model offers improved explainability, assisting clinicians in interpreting the model results and taking informed decision. • We establish a new baseline method by deploying a non-negative neural network, inspired by [5] and designed for risk assessment on healthcare clinical data. • We perform extensive experiments on two clinical datasets for melanoma and heart disease risk assessment, showing that the proposed model, although significantly smaller, achieves competitive performance with respect to the state-of-the-art."
https://arxiv.org/html/2411.08700v1,Rethinking negative sampling in content-based news recommendation,"News recommender systems are hindered by the brief lifespan of articles, as they undergo rapid relevance decay. Recent studies have demonstrated the potential of content-based neural techniques in tackling this problem. However, these models often involve complex neural architectures and often lack consideration for negative examples. In this study, we posit that the careful sampling of negative examples has a big impact on the model’s outcome. We devise a negative sampling technique that not only improves the accuracy of the model but also facilitates the decentralization of the recommendation system. The experimental results obtained using the MIND dataset demonstrate that the accuracy of the method under consideration can compete with that of State-of-the-Art models. The utilization of the sampling technique is essential in reducing model complexity and accelerating the training process, while maintaining a high level of accuracy. Finally, we discuss how decentralized models can help improve privacy and scalability.","The large amount of available news sources and articles, coupled with very fast update cycles, creates an atmosphere of information overload that makes it hard for readers to keep track of news that are most relevant to them bib2 . The power of personalized news retrieval can be extremely helpful in improving the users’ overall satisfaction with the service. By carefully selecting items to users – in this case, news articles – Recommender Systems (RS) bring the most relevant items to the attention of users. Recommender systems for news typically rely on user click data that consists of the positive interactions between users and news items. Because machine learning algorithms struggle to learn from positive data without a negative counterpart – i.e. disliked or otherwise irrelevant items –, it is common practice to randomly sample negative examples to balance the learning data. In this paper, a negative sampling technique is proposed, that fuels a Decentralized Neural News Recommendation system (DNNR) by providing better implicit negative examples for the model to train on and learn user patterns. News Recommender Systems (NRS) have certain characteristics related to their business model that are not often, or at all, observed in other domains. The key difference is the speed at which the relevance of the items decay. Unlike item recommendation in music, movies, or the retail market, for example, the relevance of news articles can change very rapidly concomitant with daily happenings and events karimiNews2018 . This leads to a permanent item cold-start problem, since recent news items to recommend have few interactions. Fortunately, news are content-rich, and recent advances in natural language processing (NLP) provide excellent tools to extract rich and compact representations directly from natural text. These content-based representations can compensate for the scarcity of interactions of new items. Another relevant aspect of our work consists of the decentralized nature of our proposed method. We have now arrived in an information-centric age, where computing power is unevenly distributed between provider infrastructure and user devices, where most data is generated DLedge . Centralized computing power, where most computation involving the training of RS is done, need to efficiently manage and process these large quantities of data, produced in a widely distributed system, which raises some issues: • Cost: To train models and do inference on centralized computing power requires the transmission of massive amounts of data; • Latency: the delay to access the provider’s computing infrastructure power and storage is generally not guaranteed, and might restrain some solutions that are more time-critical. • Privacy: training models requires a lot of private information to be carried, raising privacy issues. Organizations with large amounts of user data heightens the risk of illegitimate data use or hazardous private data leaks. Under these circumstances, on-device or edge computing offers advantages by hosting some computation tasks close to the data sources and end users. When combined with centralized computing it can: alleviate backbone network, by handling key computation tasks without exchanging data with the central computing cluster; and allowing for more agile service response, by reducing or removing the delay of data transmissions DLedge . It also has the potential to provide better privacy guarantees, while simultaneously granting users a finer control over processes involving their personal data. Our approach is to train a different model for each user, which makes it trivial to decentralize. This has the potential to offload computation to the user realm, which besides reducing resource requirements from the provider’s side, also improves privacy and user autonomy – e.g. by allowing users to choose and fine-tune models to their needs. Summarizing, we our contributions are the following: • We introduce a personalized negative sampling technique for text-based recommendation, considerably improving model accuracy with respect to the state of the art; • We propose a decentralized training strategy based on the idea of training a separate lightweight neural recommender for each user. • We study the trade-offs between the negative sample size and predictive performance, as well as training and prediction times. We organized the paper as follows. In Section 2, we provide a review of related work."
https://arxiv.org/html/2411.08696v1,Scholarly Wikidata: Population and Exploration of Conference Data in Wikidata using LLMs,"Several initiatives have been undertaken to conceptually model the domain of scholarly data using ontologies and to create respective Knowledge Graphs. Yet, the full potential seems unleashed, as automated means for automatic population of said ontologies are lacking, and respective initiatives from the Semantic Web community are not necessarily connected: we propose to make scholarly data more sustainably accessible by leveraging Wikidata’s infrastructure and automating its population in a sustainable manner through LLMs by tapping into unstructured sources like conference Web sites and proceedings texts as well as already existing structured conference datasets. While an initial analysis shows that Semantic Web conferences are only minimally represented in Wikidata, we argue that our methodology can help to populate, evolve and maintain scholarly data as a community within Wikidata. Our main contributions include (a) an analysis of ontologies for representing scholarly data to identify gaps and relevant entities/properties in Wikidata, (b) semi-automated extraction – requiring (minimal) manual validation – of conference metadata (e.g., acceptance rates, organizer roles, programme committee members, best paper awards, keynotes, and sponsors) from websites and proceedings texts using LLMs. Finally, we discuss (c) extensions to visualization tools in the Wikidata context for data exploration of the generated scholarly data. Our study focuses on data from 105 Semantic Web-related conferences and extends/adds more than 6000 entities in Wikidata. It is important to note that the method can be more generally applicable beyond Semantic Web-related conferences for enhancing Wikidata’s utility as a comprehensive scholarly resource. Source Repository: https://github.com/scholarly-wikidata/ DOI: https://doi.org/10.5281/zenodo.10989709 License: Creative Commons CC0 (Data), MIT (Code)","Scientific conferences are vital for researchers to share their research findings and advancements. It offers an opportunity to discuss research problems or limitations, a platform for networking with peers, and a platform for promoting collaboration, which is essential for learning, innovation, and problem-solving. Because of the importance of scientific conferences, we have seen tremendous growth in the number of conferences over the years [1]. For example, IEEE (Institute of Electrical and Electronics Engineers) sponsors more than 2,000111https://www.ieee.org/about/at-a-glance.html conferences and events annually. Similarly, ACM (Association for Computing Machinery) hosts more than 170222https://www.acm.org/conferences/about-conferences conferences annually worldwide. Therefore, efforts have been made to capture metadata about scientific events [2, 3, 1, 4] in a linked-data format as they provide valuable information. Such data can be used for (i) better understanding the progress of science overall, (ii) the evolution of particular research topics (or fields), (iii) understanding research impact (e.g. by sponsors’ interest) over time, etc. The availability of scholarly metadata enables scientometrics [5], or practical tools such as recommending relevant conferences or papers to readers [2] for navigating through the fastly growing scientific output which is becoming time-consuming and almost impractical. However, as much as the benefits these metadata about scientific events provide, there exist challenges. The primary obstacle is the collection of large-scale metadata, which is nontrivial in nature [2]. Similarly, the sustainability, which is also the focus of this paper, of the accumulated metadata constitutes the second and most significant obstacle. If the data collected is not sustainable, it may be lost over time, resulting in the loss of valuable information and efforts put into data collection. For instance, the Microsoft Academic Graph, which contained over 8 billion triples [2] with information about scientific publications and related data, was retired in December 2021333https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/. While the effort was somewhat continued shortly later in OpenAlex444https://openalex.org/, the case demonstrates sustainability issues in individual or commercial scholarly KG offerings. We argue that collaborative, general purpuse, community-driven platforms, such as Wikipedia, are generally more sustainable than such fragmented efforts: community participation is motivated by intrinsic factors, fostering a sense of belonging to the group [6]. Notably, commercial initiaves seem to recognize this, as shown by Google’s declaration that it will cease operations on Freebase and transfer its content to Wikidata [7]. Wikidata, which focuses on knowledge graphs (KGs), is a sister project of Wikipedia and another example of a community-driven platform [8, 9]. Wikidata currently has more 110M entities and 25K active contributors555https://www.wikidata.org/wiki/Wikidata:Statistics. By bringing Scholarly data about scientific conferences into Wikidata, they can be seamlessly integrated with existing background knowledge through SPARQL queries. Furthermore, Wikidata benefits from a robust tooling ecosystem and widely used libraries, including entity linkers, search tools, SPARQL endpoint with high-availability, easy-to-use query editor, visualization tools, and more [10, 11]. Wikidata also allows non-expert users to directly access the KGs through search and Web UI (user interface). Therefore, the primary objective of our work is to integrate scientific conference metadata into Wikidata, a community-led platform. After conducting an analysis of Wikidata entities related to Semantic Web conferences such as International Semantic Web Conference (ISWC), Extended/European Semantic Web Conference (ESWC), International Conference Knowledge Engineering and Knowledge Management (EKAW), International Conference on Knowledge Capture (K-CAP), SEMANTiCS, and Knowledge Graph and Semantic Web Conference (KGSWC), it was noticed that some conferences were missing and the ones that were present had only minimal information. In this project, we have extended Wikidata to include a more comprehensive set of information (e.g. see ISWC 2023666https://www.wikidata.org/wiki/Q119153957 (Q119153957)). Within the scope of this work, we focused on the Semantic Web conferences but our method is more generally applicable and can be extended to other conference series. We note that 105 conferences we added to, updated in Wikidata is higher than the comparable related work such as Scholarly Data (35 confs)777https://bit.ly/3Vs6XNc, ORKG (5 confs) 888https://orkg.org/organizations/Event as of July, 2024. Large language models (LLMs) have proven their language understanding capabilities with many NLP benchmarks [12]. In recent years, approaches such as in-context learning with a few-shot example have allowed them to perform many tasks such as relation or fact extraction [13, 14]. Such models can be used to easily extract information from sources with natural language text, such as conference proceedings, websites, or call for papers. Nevertheless, their output can be prone to errors. In our work, LLMs are used to extract data, which is then verified by a human-in-the-loop validation to eliminate any noisy extraction and ensure accuracy. In particular, this paper makes the following contributions. • We analysed existing ontologies for representing scholarly data and mapped them to Wikidata to identify relevant Wikidata entities/properties as well as gaps. • We present a method for utilizing large language models to efficiently extract conference metadata from various sources, curating them through a human-in-the-loop validation process using OpenRefine, and populating the data in Wikidata via Wikidata QuickStatements and provide an evaluation for LLM-based extractions. • As a result of this project, we have extended over 1000 existing entities and created more than 5,000 new entities, including conferences, scientific articles, and people. These entities are now available on Wikidata and can be accessed via the Web UI or SPARQL endpoint. • We extend visualization tools Scholia999https://scholia.toolforge.org/ and Synia101010https://synia.toolforge.org to better visualize the information we added to Wikidata."
https://arxiv.org/html/2411.08666v1,A Survey on Vision Autoregressive Model,"Autoregressive models have demonstrated great performance in natural language processing (NLP) with impressive scalability, adaptability and generalizability. Inspired by their notable success in NLP field, autoregressive models have been intensively investigated recently for computer vision, which perform next-token predictions by representing visual data as visual tokens and enables autoregressive modelling for a wide range of vision tasks, ranging from visual generation and visual understanding to the very recent multimodal generation that unifies visual generation and understanding with a single autoregressive model. This paper provides a systematic review of vision autoregressive models, including the development of a taxonomy of existing methods and highlighting their major contributions, strengths, and limitations, covering various vision tasks such as image generation, video generation, image editing, motion generation, medical image analysis, 3D generation, robotic manipulation, unified multimodal generation, etc. Besides, we investigate and analyze the latest advancements in autoregressive models, including thorough benchmarking and discussion of existing methods across various evaluation datasets. Finally, we outline key challenges and promising directions for future research, offering a roadmap to guide further advancements in vision autoregressive models.","Autoregressive (AR) models have recently driven significant progress in artificial intelligence, particularly through models like the GPT series [1, 2, 3, 4, 5] and other large language models (LLMs) [6, 7, 8] that excel at solving a variety of natural language processing tasks. These models employ a straightforward yet powerful “next-token prediction” strategy, allowing them to generate coherent and contextually relevant text by predicting each subsequent word in a sequence. The success of AR models can be attributed to two key characteristics: (1) scalability, as scaling laws [9, 10] enable researchers to predict the performance of larger models based on smaller ones, optimizing resource allocation and guiding model development; and (2) generalizability, as AR models can adapt to new and unseen tasks without requiring task-specific training [1, 3]. These promising characteristics enable AR models to address language tasks with unprecedented effectiveness, revealing their potential toward general-purpose AI systems. Inspired by the success of AR models in natural language processing, recent studies have extended AR models to visual generation tasks. Notable examples include models like VQVAE [11], VQGAN [12], DALL-E [13], and Parti [14], which convert continuous images into discrete tokens through image tokenizers. This conversion allows AR models to generate images through a next-token prediction approach similar to that used in language processing. Visual tokenization unifies the representation of text and images by treating both as sequences of discrete tokens, making them compatible with sequence-to-sequence modeling techniques. As a result, these models can leverage the architectures similar to the GPT series [1, 2, 3] to learn effectively from large collections of text-image pairs. Beyond visual generation, AR models have also advanced visual understanding, particularly within the area of multimodal understanding [15, 16, 17, 18, 19], where they are designed to perceive and integrate multiple modalities. In multimodal tasks, AR models are trained to interpret visual input and generate coherent textual sequences, making them powerful tools for applications requiring a deep understanding of both visual and textual information. For example, multimodal large language models (MLLMs) like LLaVA [15] utilize LLMs to interpret visual input alongside text, allowing them to answer questions about images, generate descriptive captions, and engage in dialogue with detailed visual context. Through this design, AR-based MLLMs demonstrate great potential for advancing versatile visual understanding capabilities in AI applications. Given the achievements of AR models in visual generation and visual understanding, recent works attempt to assemble the two types of capabilities into a unified AR model that can handle both visual generation and understanding. For example, Transfusion [20] demonstrates this integration by combining the next-token prediction objective commonly used in language modeling with diffusion processes for image generation. By jointly training on both text and image data, Transfusion [20] can handle both discrete text tokens and continuous image data within a single transformer architecture, enabling it to perform a wide range of multimodal tasks and bridging the gap between visual understanding and visual generation. Moreover, AR models show strong capabilities in both understanding and generation across other domains, such as video [21], where they handle tasks like video captioning, generation, and scene interpretation. Despite the significant progress and growing interest in AR models for vision research, there is currently a lack of a systematic survey to provide an overarching view of existing methods, challenges, and potential future directions. This paper aims to fill this gap by conducting a comprehensive survey of AR models across a diverse range of vision tasks, categorized by task type, including image generation, image understanding, and other areas. We approach this survey from multiple perspectives, covering the background of AR models, relevant datasets, methodologies, benchmarks, as well as current research challenges and open directions. Our goal is to offer the community a clear overview of what has been achieved, the challenges that remain, and the promising directions for future research in AR models for vision. The main contributions of this work can be summarized in three key aspects. First, we provide a systematic and comprehensive review of the applications of AR models in vision, developing a taxonomy of existing methods and highlighting their major contributions, strengths, and limitations. Second, we investigate and analyze the latest advancements in AR models, including thorough benchmarking and discussion of existing methods across various evaluation datasets. Third, we identify and discuss several challenges and promising directions for future research in AR models, aiming to guide the community in addressing open questions and advancing the field. Figure 1: A timeline of representative autoregressive models in vision. We are witnessing rapid growth in this field. More works can be found in our released GitHub page, which is updated daily. {forest} for tree= grow=east, parent anchor=east, child anchor=west, align=center, l=2cm, s sep=0.5cm, edge=draw=black, -latex, rounded corners, draw, minimum size=0.5cm, fill=blue!10, edge path= [draw, -latex] (!u.east) – +(1mm,0) —- (.child anchor)\forestoptionedge label; , font= [Datasets for Autoregressive Models in Vision, calign=edge midpoint [3D Shape and Scene Generation, calign=edge midpoint, [ ShapeNet, ModelNet ] ], [Medical Image Datasets, calign=edge midpoint, [ BraTS, LUNA16, MIMIC-CXR, RibFrac, TCIA Covid19, AMOS22, ISLES2022, AbdomenCT-1K, Totalsegmentator, Verse 2020, RSNA-2022-CSFD, RSNA-2020-PED, STOTIC, FLARE22, FLARE23, DeepLesion dataset, Task03 Liver, Task06 Lung, Task07 Pancreas, Task08 Hepatic Vessel, Task09 Spleen, Task10 Colon ] ], [Multi-modal Generation Datasets, calign=edge midpoint, [ WikiHow, WIT, DeepSeek-VL, MME, GQA, Capfusion-120M, Capfusion dataset, LAIONaesthetics-12M, LLaVA-v1.5mix-665K, LLaVA-Pretrain558K, DataComp, COYO700M, GenHowTo ] ], [Motion Generation Datasets, calign=edge midpoint, [ Human3.6M, MPII Human Pose, AMASS, HumanLong3D, HumanMusic, KIT, HumanML3D, KIT-ML ] ], [Image Editing, calign=child edge, [ ImageNet, ADE20K, COCOStuff, LAION-Aesthetics, MultiGen-20M ] ], [Image-to-Image Translation Datasets, calign=edge midpoint, [ Cityscapes, Edges2Shoes/Edges2Handbags ] ], [Text-to-Image Generation, calign=edge midpoint, [ MSCOCO, Flickr30K, Text2Art Dataset, PartiPrompts, CC12M, CC, YFCC100m, Redcaps, MJHQ-30K, GenEval, DPG-Bench, VILA1.5-13B, T2I-CompBench, ] ] [Video Generation Datasets, calign=edge midpoint, [ UCF-101, Kinetics-600, Vimeo-90K, SkyTimelapse, Text2Video Dataset, AudioSet, AVA-Action, Moments-in-Time (MiT), Something-Something v2 (SSV2) ] ], [Image Generation Datasets, calign=edge midpoint, [ ImageNet, CelebA-HQ, CIFAR-10/100, LSUN, LAION-5B, WebLI, JourneyDB, LAION-High-Resolution111https://huggingface.co/datasets/laion/laion-high-resolution, FFHQ, OpenImages ] ] ] Figure 2: Datasets for Autoregressive Model in Vision"
https://arxiv.org/html/2411.08651v1,Estimating unknown parameters in differential equations with a reinforcement learning based PSO method,"Differential equations offer a foundational yet powerful framework for modeling interactions within complex dynamic systems and are widely applied across numerous scientific fields. One common challenge in this area is estimating the unknown parameters of these dynamic relationships. However, traditional numerical optimization methods rely on the selection of initial parameter values, making them prone to local optima. Meanwhile, deep learning and Bayesian methods require training models on specific differential equations, resulting in poor versatility. This paper reformulates the parameter estimation problem of differential equations as an optimization problem by introducing the concept of “particles” from the particle swarm optimization algorithm. In this framework, the solution is represented as a swarm of particles, each embodying a candidate solution through its position and velocity. The particles iteratively update through mutual interactions, facilitating convergence toward an optimal solution. Building on reinforcement learning-based particle swarm optimization (RLLPSO), this paper proposes a novel method, DERLPSO, for estimating unknown parameters of differential equations. We compared its performance on three typical ordinary differential equations with the state-of-the-art methods, including the RLLPSO algorithm, traditional numerical methods, deep learning approaches, and Bayesian methods. The experimental results demonstrate that our DERLPSO consistently outperforms other methods in terms of performance, achieving an average Mean Square Error of 1.13\times 10^{-05}, which reduces the error by approximately 4 orders of magnitude compared to other methods. Apart from ordinary differential equations, our DERLPSO also show great promise for estimating unknown parameters of partial differential equations. The DERLPSO method proposed in this paper has high accuracy, is independent of initial parameter values, and possesses strong versatility and stability. This work provides new insights into unknown parameter estimation for differential equations.","At every moment, vast amounts of data are being collected through various human activities. Uncovering the hidden dynamics from these data is a fundamental yet challenging problem across many different fields [1]. Ordinary differential equations (ODEs) model the rates of change in dynamic processes across time or space. They are extensively used to describe complex systems in science, physics, economics, pharmacokinetics, neurophysiology, and systems biology [2]. Accurately estimating the parameters of equations is crucial in scientific research for drawing reliable and valid conclusions. Unknown or inaccurately estimated parameters can lead to results that misrepresent reality, hindering our understanding of scientific phenomena and laws. Therefore, accurately estimating these parameters before analyzing the system is particularly important to avoid such issues [3]. ODEs are prevalent in various research fields, but universally accepted methods for estimating their parameters are lacking. This limitation hampers our understanding and prediction of system behavior, highlighting the critical need to estimate unknown parameters in ODES. There are two commonly used methods for parameter estimation in ODEs: numerical solution methods and non-parametric methods. Numerical methods use the least squares method to fit the ODE solutions to observed data, providing parameter estimates that accurately represent actual behavior. However, due to the lack of analytical solutions for many ODEs, these methods can be computationally intensive. In contrast, non-parametric methods bypass explicit ODE solutions, utilizing smoothing techniques for estimation. While this reduces computational overhead, it complicates optimization, making it more sensitive to noise and prone to converging on local optima. Edsberg et al. [4] used numerical methods for parameter estimation, which have some drawbacks: using ODE solvers can increase computational complexity [3], and optimization methods may heavily rely on initial parameter values, making them prone to getting trapped in local optima. To improve computational efficiency, Varah [5], Ramsay and Silverman [6], as well as Chen and Wu [7], proposed a simplified two-stage method to avoid the direct numerical solution of ODEs. In the first stage, the state functions and their first-order derivatives are estimated from observed data and treated as fixed variables. In the second stage, the parameters are estimated using standard least squares methods. This method performs well for simple ODEs. However, accuracy may diminish when estimating higher-order derivatives in many ODEs. To improve efficiency and increase estimation accuracy, Ramsay et al. [8] proposed the parameter cascade method, a generalized smoothing technique. This method models the unknown solutions of ODEs as a linear combination of B-splines, applying penalized smoothing to the observed data. The penalty term, defined by the ODEs, helps prevent overfitting of the non-parametric function. Recent advancements in Bayesian methods for parameter estimation in ODEs have been notable. Wakefield [9] and Lunn [10] applied Bayesian methods to pharmacokinetic models. However, the computational intensity of this Bayesian approach stems from the need to numerically solve the ODEs at each iteration of the Markov Chain Monte Carlo (MCMC) process, significantly affecting efficiency. Huang et al. [11] proposed a Bayesian method that replaces ODEs constraints with probabilistic expressions and integrates them with non-parametric data fitting into a joint likelihood framework, using a MCMC sampling scheme. However, this method requires prior knowledge of the structure of ODEs, and may not yield satisfactory results for complex ODEs. The development of machine learning algorithms has significantly enhanced the numerical solution and parameter estimation of ODEs. Brunton et al. [12] combined sparse regression and machine learning with nonlinear dynamical systems to extract governing equations from noisy data, addressing the challenge of idenfitying control equations. However, this method relies on the chosen measurement variables and function basis. If chosen improperly, it may not be able to identify an accurate sparse model. Raissi et al. [13, 14] used physics-informed neural networks (PINNs) to estimate parameters in various physical systems. In this approach, parameters are incorporated as part of the network training process, and once the network training is completed, the optimized parameter values can be obtained. Additionally, Neural Ordinary Differential Equations (Neural ODEs) [15] extend deep neural networks to continuous-time applications, offering high memory efficiency, strong flexibility, and effectiveness in time series tasks. Challenges remain, however, in computational costs and parameter tuning, particularly for complex systems and large datasets, necessitating further optimization of model efficiency and training duration. Arloff et al. [16] proposed a two-step method to address that avoids numerical challenges in stiff ODEs via polynomial approximation and reduce the parameter search space using the Particle Swarm Optimization (PSO) algorithm. This approach is effective for complex stiff ODEs, identifying feasible solutions with reduced computational load. However, it only narrows the parameter space and the quality of its solution relies on the accuracy of the polynomial approximation. PSO is a swarm intelligence algorithm known for its strong global search capability, simple parameter configuration, rapid convergence, ease of implementation, robustness, and wide applicability in complex optimization problems. Wang et al. [17] developed a large-scale optimization algorithm, RLLPSO, which integrates PSO with reinforcement learning (RL) to enhance the speed and accuracy of convergence. We reformulates parameter estimation of differential equations as an optimization problem, and introduce a novel method, DERLPSO, to solve it. DERLSPO enhances RLLPSO with several novel strategies, i.e., logarithmic initialization, reinitialization mechanisms, and bottom-up update strategy, to achieve higher convegence speed, as well as more accurate and stable estimation. The DERLPSO method is independent of initial parameter values and exhibits high versatility and generalization capabilities. It has been tested on three types of ODEs: Lorenz, FitzHugh-Nagumo, and Lotka-Volterra equations, and three types of partial differential equations (PDEs): Heat, Transient convection-diffusion, and Helmholtz equations. The performance of DERLPSO has been compared with the state-of-the-art methods, including the RLLPSO algorithm, traditional numerical methods, deep learning approaches, and Bayesian methods. The remainder of this paper is organized as follows. Section 2 details the proposed algorithm. Section 3 outlines the experimental procedures and discusses the results. Section 4 concludes with insights for future research."
https://arxiv.org/html/2411.08641v1,DipMe: Haptic Recognition of Granular Media for Tangible Interactive Applications,"While tangible user interface has shown its power in naturally interacting with rigid or soft objects, users cannot conveniently use different types of granular materials as the interaction media. We introduce DipMe as a smart device to recognize the types of granular media in real time, which can be used to connect the granular materials in the physical world with various virtual content. Other than vision-based solutions, we propose a dip operation of our device and exploit the haptic signals to recognize different types of granular materials. With modern machine learning tools, we find the haptic signals from different granular media are distinguishable by DipMe. With the online granular object recognition, we build several tangible interactive applications, demonstrating the effects of DipMe in perceiving granular materials and its potential in developing a tangible user interface with granular objects as the new media.","Granular materials are commonly seen in our daily lives. As a continuous deformable media, granular materials such as sand or beads has been introduced to tangible user interface (TUI) to preview the landscape or adjust the stiffness of input devices [1, 2, 3, 4]. While different rigid objects have been used in TUI to give flexible control of virtual content [5, 6, 7, 8, 9], to the most of our knowledge, the potential of interacting with different types of granular objects has not been exploited. TUI has proved to play a vital role in everyday tasks. It makes things more concrete and provides embodiment effects through physicality [10]. For example, typical studies such as Project Zanzibar [11], which showed a flexible mat to communicate with tangible objects placed on its surface. It also supports sensing a user’s touch and hover hand gestures, which opened up the possibility of novel digital experiences. De Tinguy et al. [12] proposed that portable devices could be used to simulate the experience of interacting with different objects. Schmitz et al. [13] proposed a fabrication pipeline and sensing approach that enabled object recognition of tangibles on capacitive touchscreens. Yan et al. use the LaserShoes [14] to achieve real-time inference, which cooperate with human under different circumstances. At the same time, they used the laser speckle imaging technique and the LaserShoes could distinguish the surface textures that appear. However, existing TUI with granular materials only involves a pre-specified type of granular media. Ishii et al. [1] proposed the concept of continuous tangible user interface (Sandscape) using granular materials. They argued that granular materials could bridge the gap between physical and digital forms because of their continuous physical properties. Users could interact with models made of sand and the shape of the granular media was converted into a digital height field to preview land scapes. Kazi et al. [2] developed a digital canvas (Sandcanvas) to simulate sand drawings. They studied the gestures on touchscreens and incorporated sand simulators to reproduce sand drawings on a virtual screen. Follmer et al. [3] proposed using granular materials to build Jamming user interface. They exploited computer-controlled jamming of granular particles to show its ability in controlling the stiffness of shape-changing objects. In this work, we will introduce dipping as a method to sense different types of granular media and use multiple types of granular media for interactive applications. If users would like to fully exploit the interaction experience or map different types of granular objects into the virtual world, existing solutions are not ready for users to interact with different granular objects. One of the missing features is to enable the input device to understand the type of granular media. To understand the different types of media, various methods have been developed for object recognition based on vision [15, 14], inertia data [16, 17, 18], acoustic signals [19, 20], or electromagnetic signals [21, 22] for various human-computer interactions. Vision-based methods are typical ways for object recognition. However, during the interaction with the granular media, the shape changes of the material, the occlusion or the varying lighting conditions may all affect the recognition results. Furthermore, vision-based solutions require capturing the interaction scene, which may also bring concerns to users in terms of privacy. Haptic recognition is also an important modality for object recognition in the field of computer-human interaction (CHI). Force or tactile signals are used in previous works with machine learning techniques. For example, tactile information has been used for learning the grasp signature for object recognition [23] or human-environment interactions [24]. Wu et al. [25] proposed to encode contact information for object recognition on an interactive fabric. VibEye [26] used the vibration passing through the object to determine the identity of an object. The vibrotactile information received by the finger was represented with a spectrogram and used for object recognition. Researchers also proposed to install proximity sensors on objects to recognize the grasping events [27]. However, it is costly to customize different objects by embedding sensors inside them. While humans naturally learn to recognize types of granular material through physical interactions, it is challenging for computers to classify them, especially in user interaction scenarios. Soil, a typical granular medium, is commonly studied in geoscience using the cone penetrometer test (CPT) to ascertain geotechnical properties and identify soil stratigraphy [28, 29]. Geologists increasingly employ force data collected from experiments along with machine learning techniques for tasks such as soil spatial mapping, recognition, and classification [30, 31]. However, traditional CPT is costly and time-consuming, requiring heavy and precise tools and strict perpendicular testing directions [32]. In contrast, DipMe provides an inexpensive and easily deployable alternative, allowing for testing with a casual dipping operation, making it more suitable for tangible interactions with particulate media. Machine learning has revolutionized object recognition in recent years, with techniques such as Time Series classification (TSC) [33] utilizing inception modules with a fully convolution network (FCN) [34], a robust temporal feature network (RTFN) [35], and a convolution neural network (CNN) [36]. Additionally, researchers have proposed Multivariate Time Series Classification (MTSC) [37]. These machine learning tools have also been leveraged for object recognition in the human-computer interaction community using multimodal signals [38, 39, 40, 41, 42]. We propose the use of a more advanced machine learning algorithm, based on the encoder part of the transformer model and multi-channel mechanism, to achieve autonomous recognition of force signal series, surpassing conventional algorithms in performance. Figure 1: DipMe: system overview (left) and applications (right). A user is allowed to dip the device into different granular media. We collect the force and torque signals and use machine learning techniques to recognize the type of the granular material from the multichannel time series data. With the tracked motion of DipMe and the recognized type of granular material, we demonstrate several applications including a virtual drawing interface and a virtual music instrument built with DipMe. In this work, we propose DipMe, a haptic solution for recognizing different types of granular media. Our insight is that humans may probe fingers into the granular particles to feel their haptic experience. Even if they cannot see the granular media, they can still recognize its type based on the force perceived by the fingers. To this end, we develop a device to simulate the probing process and equip the device with force-sensing ability. During the interaction, users are allowed to first probe to test the granular material. We employ modern machine learning techniques to encode the differences of the force signals collected from probing granular materials. We test our method in various interaction conditions and positive recognition results (92.78% accuracy with 10 users over 6 granular media) are obtained. With the capability of recognizing types of granular materials, we demonstrate several interactive applications to show the potential of DipMe in developing new tangible user interface with granular media. Our contributions can be summarized as follows: • We propose a new device to intelligently recognize the types of granular material based on haptic information; • We develop new tangible interactive applications that allows user to interact with different types of granular media."
https://arxiv.org/html/2411.08622v1,"Precision-Focused Reinforcement Learning Model for
Robotic Object Pushing","Non-prehensile manipulation, such as pushing objects to a desired target position, is an important skill for robots to assist humans in everyday situations. However, the task is challenging due to the large variety of objects with different and sometimes unknown physical properties, such as shape, size, mass, and friction. This can lead to the object overshooting its target position, requiring fast corrective movements of the robot around the object, especially in cases where objects need to be precisely pushed. In this paper, we improve the state-of-the-art by introducing a new memory-based vision-proprioception RL model to push objects more precisely to target positions using fewer corrective movements.","Humans intuitively interact with objects in everyday situations, often without explicitly planning or thinking about how objects will behave. Non-prehensile object manipulation is an important skill for robots that are designed to assist humans. This work focuses on object pushing, a sub class of robotic manipulation that is crucial e.g. for service robots working in a kitchen, but it can also be beneficial for industrial applications. Consider a robot that has to grasp a cup that is placed on a shelf behind other objects. A simple strategy to reach the desired cup is to push the other items aside. Pushing can also be considered as an alternative to pick-and-place, e.g. if objects are too heavy or too large to be grasped. In addition, fragile objects that are likely to be damaged when grasped can be pushed. However, object pushing is demanding for robots due to the large variety of objects that all behave differently depending on their shape, size, mass, and friction. The task becomes even more challenging, considering that not all physical properties, such as mass or friction, are directly observable. These unknown properties can lead to the object overshooting its target position, requiring fast corrective movements of the robot around the object that are particularly difficult to model explicitly, as they require decisions about when a correction is necessary, how to adapt the pushing direction, and how to plan the corresponding movement. Corrections are even more challenging if the approach should generalize to objects with varying physical parameters. The difficulty of modeling such corrective movements is also evident in other recent work. \citeauthorcong_self-adapting_2020 [cong_self-adapting_2020] report that their model for object pushing based on a recurrent neural network (RNN) and model predictive control (MPC) cannot properly switch pushing sides, i.e. the model is not able to perform corrective movements. Additionally, the authors also train a RL agent as a model-free baseline. In contrast to the data-driven model, the RL agent learns to properly perform corrective movements. These results show that it is reasonable to further investigate RL in the context of object pushing. \citeauthorcong_reinforcement_2022 [cong_reinforcement_2022] therefore proposed a vision-proprioception model for planar object pushing, which is a state-of-the-art RL model that uses latent representations to encode the characteristics of an object. This vision-proprioception model is the starting point of this work. We investigate the following problem: a robot has to precisely push objects with varying physical parameters from starting positions to goal positions, both of which are randomly chosen. The main contribution of this work is the adaption of the vision-proprioception model to push objects more precisely to target positions by using fewer corrective movements around an object. Originally, the task was considered successful if the distance between the center of the object and the goal is smaller than 5\,cm [cong_reinforcement_2022], which is a threshold that is commonly used in the literature [plappert_multi-goal_2018, gallouedec_panda-gym_2021, xu_cocoi_2021]. However, this is a comparatively large tolerance, particularly if pushing is considered as an alternative for pick-and-place, which is often used to precisely reposition objects. Therefore, we decrease the threshold to 1\,cm. The vision-proprioception model is adapted by providing the agent with the complete episode history of observations, using a gated recurrent unit (GRU) [cho_learning_2014] as a feature extractor and improving the sampling of object parameters during training. Fig. 1 provides an overview of our approach. Figure 1: Schematic Overview. We adapt a state-of-the-art RL model to push objects more precisely to target positions by improving the sampling of object parameters and adding a gated recurrent unit to provide the agent with a memory. Figure 2: Model Architecture. We concatenate the Cartesian (x,y) EE position with the latent object and goal states generated by an encoder trained prior to the RL agent. The observations of the entire episode are stored in a memory buffer to be processed by a GRU-layer. The hidden state of the most recent time step is used as the feature vector for actor and critic MLPs."
https://arxiv.org/html/2411.08605v1,Lo-MARVE: A Low Cost Autonomous Underwater Vehicle for Marine Exploration,"This paper presents Low-cost Marine Autonomous Robotic Vehicle Explorer (Lo-MARVE), a novel autonomous underwater vehicle (AUV) designed to provide a low cost solution for underwater exploration and environmental monitoring in shallow water environments. Lo-MARVE offers a cost-effective alternative to existing AUVs, featuring a modular design, low-cost sensors, and wireless communication capabilities. The total cost of Lo-MARVE is approximately EUR 500. Lo-MARVE is developed using the Raspberry Pi 4B microprocessor, with control software written in Python. The proposed AUV was validated through field testing outside of a laboratory setting, in the freshwater environment of the River Corrib in Galway, Ireland. This demonstrates its ability to navigate autonomously, collect data, and communicate effectively outside of a controlled laboratory setting. The successful deployment of Lo-MARVE in a real-world environment validates its proof of concept.","††footnotetext: Proc. of the 12th International Conference on Control, Mechatronics and Automation (ICCMA 2024), London, UK, November 11-13, 2024, https://www.iccma.org/. 2024. Autonomous Underwater Vehicles (AUVs) have emerged as indispensable tools for exploring the vast and largely unexplored depths of the oceans. Their ability to operate independently, navigate complex underwater environments, and collect valuable data has revolutionized fields such as marine science, oceanography, and resource exploration. AUVs are equipped with advanced sensors, communication systems, and propulsion mechanisms, enabling them to perform tasks that would be impractical or dangerous for human divers. The development of robust and reliable AUV technologies is crucial for addressing pressing global challenges, including climate change, marine conservation, and sustainable resource management. Advanced AUVs, such as Vityaz-D, have reached depths of over 10km [Alekseev et al.(2021)]. These highly capable AUVs offer advanced capabilities and can reach significant depths. The main limitation of these advanced AUVs is that they can be prohibitively expensive. Purchasing such AUVs presents a barrier to researchers with modest budgets who wish to conduct research on AUVs. This provides the motivation for research into low cost AUVs. There have been many low cost AUVs published in the literature. For example LoCo was proposed in 2020 [Edge et al.([n. d.])]. Studies such as this demonstrate that it is possible to develop capable AUVs for lower cost. The proposed AUV aims to provide a more accessible and affordable option for researchers and organizations seeking to conduct underwater exploration and data collection. The main advantage of the proposed AUV is its low cost when compared to existing low cost AUVs. The contributions of this paper are: • The design and development of a very low cost AUV for marine robotics research. • To deploy the proposed AUV in the field to demonstrate it’s functionality. The rest of the paper is structured as follows. Section 2 will give an overview of the research in the literature in low cost AUV development. Section 3 will outline the design of the proposed ultra low cost AUV. Section 4 will then outline the experimental procedure for evaluating the AUV. The results of AUV testing are presented in Section 5. Finally, Section 6 will draw conclusions from this research."
https://arxiv.org/html/2411.08587v1,DeepUQ: Assessing the Aleatoric Uncertainties from two Deep Learning Methods,"Assessing the quality of aleatoric uncertainty estimates from uncertainty quantification (UQ) deep learning methods is important in scientific contexts, where uncertainty is physically meaningful and important to characterize and interpret exactly. We systematically compare aleatoric uncertainty measured by two UQ techniques, Deep Ensembles (DE) and Deep Evidential Regression (DER). Our method focuses on both zero-dimensional (0D) and two-dimensional (2D) data, to explore how the UQ methods function for different data dimensionalities. We investigate uncertainty injected on the input and output variables and include a method to propagate uncertainty in the case of input uncertainty so that we can compare the predicted aleatoric uncertainty to the known values. We experiment with three levels of noise. The aleatoric uncertainty predicted across all models and experiments scales with the injected noise level. However, the predicted uncertainty is miscalibrated to \rm{std}(\sigma_{\rm al}) with the true uncertainty for half of the DE experiments and almost all of the DER experiments. The predicted uncertainty is the least accurate for both UQ methods for the 2D input uncertainty experiment and the high-noise level. While these results do not apply to more complex data, they highlight that further research on post-facto calibration for these methods would be beneficial, particularly for high-noise and high-dimensional settings.","Physically and statistically interpretable uncertainties are critical for applications in science and industry. Uncertainty quantification (UQ) in deep neural networks has gained significant attention, with recent work exploring taxonomies of uncertainties, including domain, epistemic, and aleatoric uncertainties, e.g., [4, 5, 10]. Aleatoric uncertainty, \sigma_{\rm al}, is significant because, unlike epistemic uncertainty, it is not a result of model limitations but rather an inherent property of the data. In many cases, aleatoric uncertainty is exactly known because it is produced by a well-understood physical process, allowing us to anticipate not only its expected amplitude but also its distributional characteristics. For instance, in astrophysics, the Poisson distribution111The Poisson distribution can be approximated by a Gaussian when the photon rate \lambda is large. characterizes ‘shot’ or photon noise, while the Normal distribution characterizes read and other forms of thermal or electronic noise. Developing a coherent framework for benchmarking aleatoric uncertainty estimates from deep networks and assessing their calibration is needed to ensure that the predicted aleatoric uncertainty aligns with our scientific expectations. UQ methods broadly fall under the categories of Bayesian methods (e.g., Bayesian Neural Networks (BNNs) [15, 24, 19]), Bayesian model averaging (e.g., Deep Ensembles [14], MC Dropout [9]), and Evidential Deep Learning [26] (e.g., Deep Evidential Regression [1, 16]). In addition, a class of methods for uncertainty calibration [18] exist separately in the statistical literature and have recently gained popularity as post-processing tools (e.g., conformal prediction [2]). Other work has explored formalized comparison of UQ methods (e.g., [6, 21, 3, 25, 7]). [21, 25] compare aspects of predictive uncertainty distributions, and [7] present an uncertainty toolbox for comparing predictive uncertainties; all of these methods do so without access to true uncertainty values. Of the few studies testing the exact calibration of predicted uncertainties [6, 3], some do not vary data dimensionalities or uncertainty injection types [6], while others vary these factors but do not report mean aleatoric uncertainty or propagate input uncertainty, preventing direct comparison to expected values [3]. Quantifying how noise on the input variable affects the predictions of aleatoric uncertainty on the output variable from deep learning methods is of critical importance, especially in computer vision, and has not yet received much attention in the literature (e.g., [20, 29]). The bulk of previous work on aleatoric uncertainty has focused mostly on assessing the predicted aleatoric uncertainty on the output variable y via injecting uncertainty directly on y (for a review, see [11]). Recently, the statistical field of input uncertainty has intersected with the deep learning literature under the umbrella of UQ (for a review, see [27]). Experiments have focused on propagating input uncertainty through a neural network for regression using a Laplace Approximation [29] as well as through a Taylor series expansion and Monte Carlo sampling approach with a multi-layer perception [27]. Assessing input uncertainty is inherently more complex, requiring tractable functional relationships between input and output variables when propagating the uncertainty onto the output variables. We present a study of regression on tabular (0D) and imaging (2D) data that investigates aleatoric uncertainty for cases where uncertainty is injected on either the input x or the output y variables, providing a more comprehensive understanding of aleatoric uncertainty in regression tasks. By injecting uncertainty onto the input variable and propagating it to the output variable, we can assess the exact calibration of the predicted uncertainty estimate. We design a set of desiderata for how the predicted aleatoric uncertainty should behave: i) the predicted uncertainty should scale with the injected uncertainty; ii) the aleatoric uncertainty should be well-calibrated (within \rm{std}(\sigma_{\rm al}) of the true uncertainty value); and iii) these desiderata should hold for both data dimensionalities and both uncertainty injection types (input and output). We do this all for a very simple set of experiments; we caution the reader against applying the conclusions here to all types of data, including real-world datasets."
https://arxiv.org/html/2411.08583v1,An Empirical Examination of the Evaluative AI Framework,"This study empirically examines the “Evaluative AI” framework, which aims to enhance the decision-making process for AI users by transitioning from a recommendation-based approach to a hypothesis-driven one. Rather than offering direct recommendations, this framework presents users pro and con evidence for hypotheses to support more informed decisions. However, findings from the current behavioral experiment reveal no significant improvement in decision-making performance and limited user engagement with the evidence provided, resulting in cognitive processes similar to those observed in traditional AI systems. Despite these results, the framework still holds promise for further exploration in future research.","In recent years, AI has gained substantial attention for their increasingly sophisticated performance in various applications (Barredo Arrieta et al., 2020; Rong et al., 2022; Albrecht, 2016; MacCarthy, 2019). However, their significant limitation compared to simpler methods is their commonly opaque “black box” nature, making it difficult to understand how inputs generate outputs (Guidotti et al., 2018). This is particularly problematic in high-stakes areas like medicine, economics, or law, where understanding the decision-making process is crucial (Rudin, 2019). As a result, the lack of transparency and comprehensibility often leads to distrust and underreliance among potential users, despite the accuracy of these decision-support systems (Jacovi et al., 2021; Mahmud et al., 2022; Zhang et al., 2020). This challenge has spurred the development of several explanatory methods and a surge in interest in Explainable AI (XAI). Initially, it was hoped that XAI would enhance understanding and trust in AI models, thereby improving decision-making quality among users. However, as summarized by recent studies (Lai et al., 2023b; Schemmer et al., 2022; Vasconcelos et al., 2023; Bertrand et al., 2023; Rogha, 2023; Schemmer et al., 2023), the results are mixed. While XAI might indeed improve understanding (Ribeiro et al., 2018), higher transparency can make models less comprehensible (Poursabzi-Sangdeh et al., 2021). Explanations can improve subjective perception (Bertrand et al., 2023), but also might increase cognitive load (You et al., 2022; Herm, 2023; Ghai et al., 2020) and reduce efficiency (Lai et al., 2023b). This has led to a situation where users often engage superficially with explanations and develop an overreliance on AI (Chromik et al., 2021; Buçinca et al., 2021; Bansal et al., 2021; Chen et al., 2023), shifting from the original problem of underreliance. Given that AI is not infallible and often makes better decisions than humans (Mnih et al., 2015; Nori et al., 2023), a calibrated level of trust is essential for a trade-off that encourages user to rely more on AI, while avoiding blind trust (Wischnewski et al., 2023; Vered et al., 2023). To address the issue of overreliance, various strategies have been developed, such as cognitive forcing functions (Buçinca et al., 2021) and user-adapted, selective explanations (Lai et al., 2023b). This paper discusses another approach to improve human-AI interaction: the “Evaluative AI” framework proposed by Miller (2023). Critiquing the limited success of existing XAI methods, Miller argues that these methods do not align well with the cognitive processes involved in decision-making. He suggests a paradigm shift from recommender-driven systems to a hypothesis-driven approach, based on the Data/Frame Theory (Klein et al., 2007) and abductive reasoning (Peirce, 2009), to better support decision-makers in exploring hypotheses rather than receiving direct recommendation by AI. This study empirically investigates the effectiveness of the proposed framework in enhancing decision-making by examining its impact on performance, efficiency, and subjective perception. The focus is on one specific element of the framework: offering evidence for and against potential option without providing direct recommendations. Rather than giving a recommendation and explaining it, the framework refrains from making any recommendations. Instead, it offers evidence supporting and opposing each option, which is only displayed if requested by the decision-maker. This studies research question is: • RQ: Can a decision support system that offers evidence for and against potential options, without providing direct recommendations, improve the decision-making process? Currently, only three studies directly apply Miller’s framework: Castelnovo et al. (2023) developed a contrastive explanation technique for ranking classifications, and Le et al. (2024b) created a tool for image classification, though neither has undergone empirical testing. During the development of the present study, an empirical evaluation by Le et al. (2024a) was conducted, comparing a hypothesis-driven approach with recommendation-driven and explanation-only methods. They found that the hypothesis-driven approach improved decision quality without increasing decision time, and participants cognitively engaged with the evidence, thereby considering the uncertainty of the underlying models. This current study differs in several respects. Compared to Le et al. (2024a), the task here is significantly more objective and realistic for participants. While their task involved classifying a subjective house price into low, medium, or high using six features, the task in this study is to estimate whether an income is above or below the median based on 20 features. This study provides a more detailed picture, as it includes a control group without any AI assistance and a group that receives both recommendations and evidence. Another difference lies in the incentive design; in this study, more incentive per task was offered to simulate a higher-stakes situation. In a pretest, it was found that evidence presented in bar chart format (as used in Le et al. (2024a)) was not well understood, so textual descriptions of the evidence were added here. Lastly, in Le et al. (2024a) experiment, low-level evidence was shown by default, which could potentially lead to anchoring effects and influence the decision-making process. In this study, no evidence is shown by default, allowing decision-makers the freedom to choose and gives further opportunities for behavioral analysis. The results of the present study paint a different picture than those of Le et al. (2024a). Overall, the findings indicate that the “Evaluative AI” framework in this experiment did not improve decision-making performance. They also reveal that participants engaged only superficially with the provided pro and con evidence, despite all AI systems influencing the decision-making processes leading potentially to cognitive offloading."
https://arxiv.org/html/2411.08582v1,Intelligent Algorithms For Signature Diagnostics Of Three-Phase Motors,"The application of machine learning (ML) algorithms in the intelligent diagnosis of three-phase engines has the potential to significantly enhance diagnostic performance and accuracy. Traditional methods largely rely on signature analysis, which, despite being a standard practice, can benefit from the integration of advanced ML techniques. In our study, we innovate by combining state of the art algorithms with a novel unsupervised anomaly generation methodology that takes into account physics model of the engine. This hybrid approach leverages the strengths of both supervised ML and unsupervised signature analysis, achieving superior diagnostic accuracy and reliability along with a wide industrial application. Our experimental results demonstrate that this method significantly outperforms existing ML and non-ML state-of-the-art approaches while retaining the practical advantages of an unsupervised methodology. The findings highlight the potential of our approach to significantly contribute to the field of engine diagnostics, offering a robust and efficient solution for real-world applications.","The reliability and efficiency of three-phase engines is critical for numerous industrial applications, which makes their accurate and timely diagnosis essential. Traditional diagnostic methods for these engines predominantly rely on signature analysis, a technique that examines the engine’s operational patterns to detect anomalies [1]. While signature analysis has become a de-facto standard due to its effectiveness, it has some substantial limitations, and the growing complexity of modern engines and the vast amounts of data they generate require more advanced and precise diagnostic frameworks [2]. At the same time, machine learning (ML) and artificial intelligence (AI) have emerged as essential tools integrated into various aspects of modern life, from recommendation algorithms [3] to healthcare [4] applications. The potential for advancement and innovation in these fields is immense. Despite this, the application of ML in industrial settings remains underexplored, primarily due to the scarcity of publicly available labeled datasets, especially with malfunctioning engines Ṫhis lack of data poses significant challenges when transitioning ML solutions from experimental phases to full-scale production, especially given the complexities and variability of real-world conditions [5]. One critical area where ML can significantly impact industrial applications is in the diagnosis and maintenance of three-phase engines. These engines, also known as three-phase induction motors, are a cornerstone of industrial operations because of their robustness, efficiency, and reliability [6]. They are prevalent in numerous applications, including pumps, compressors, conveyors, and fans, making their accurate and timely diagnosis crucial to ensuring uninterrupted industrial productivity and safety [7]. While the use of ML-based fault detection in experimental setups provides effective and reliable solutions [8], in the context of real-life implementation, this field remains poorly studied, highlighting a significant opportunity for innovation and improvement. One of the main reasons for this is that most current ML approaches require labeled data, even for the binary classification of healthy and faulty time series, and the classification of specific fault types requires even more labeled data. This is problematic as companies are unwilling to damage expensive equipment to generate the necessary labeled datasets. In this research, we investigate the detection of defects in three-phase induction motors using modern machine learning techniques, complemented by a novel signature-guided unsupervised anomaly generation methodology. Our goal is to review existing solutions for the fault detection problem based on time series data collected from engines, and to develop a robust approach that mitigates the need for extensive labeled datasets. To address this challenge, we propose a novel approach that combines signature analysis with generative neural networks and ResNet-based [9] convolutional neural networks to generate physically accurate synthetic anomalies. These synthetic anomalies are then used to train supervised machine learning models, enhancing their ability to detect and classify real-world anomalies. Our approach effectively integrates the strengths of both supervised and unsupervised methods, providing a practical solution for accurate and reliable fault detection in three-phase induction motors without dependence on large-labeled datasets. We have adapted the state-of-the-art unsupervised models AnomalyBERT [10] and VAE-LSTM [11] to serve as unsupervised baselines for induction motor fault detection. The experiments demonstrate a strong outperformance and potential of the proposed approach over the existing baselines. The next section of the manuscript will formally define the problem at hand, introduce the existing solutions, and outline the methodology and limitations of the approaches explored and proposed. Next, the proposed algorithm is introduced. Finally, the manuscript concludes with the results and discussion, summarizing the findings and their implications."
https://arxiv.org/html/2411.08561v1,LogLLM: Log-based Anomaly Detection Using Large Language Models,"Software systems often record important runtime information in logs to help with troubleshooting. Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems. Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language. In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs). LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences. Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics. Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process. Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability. Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.","Ensuring high availability and reliability is crucial for large-scale software-intensive systems [1, 2]. As these systems become more complex and expansive, the occurrence of anomalies becomes unavoidable [3, 4]. Even a minor issue can lead to performance degradation, data integrity problems, and substantial losses in both customers and revenue. Therefore, anomaly detection is vital for maintaining the health and stability of complex software-intensive systems [5]. Software-intensive systems typically produce console logs that record system states and critical runtime events [6]. Engineers can utilize this log data to evaluate system health, identify anomalies, and trace the root causes of issues. However, due to the potentially vast volume of logs, manually analyzing them for anomalies can be both labor-intensive and prone to mistakes [7]. Consequently, log-based anomaly detection has emerged as a key area in automated log analysis, focusing on the automatic identification of system anomalies through log data. Numerous deep learning-based methods [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22] for log-based anomaly detection have been proposed. These methods typically employ sequential deep learning models such as LSTM [23] and transformers [24]. These methods can be further divided into reconstruction-based methods [8, 9, 10, 11, 12, 13, 14, 15] and binary classification-based methods [16, 17, 18, 19, 20, 21, 22]. Reconstruction-based methods involve designing and training a deep neural network to reconstruct input log sequences, with anomalies detected based on reconstruction errors. The underlying principle is that anomalous samples cannot be accurately reconstructed. Binary classification-based methods, on the other hand, involve designing a binary classifier to classify samples as either normal or anomalous. These methods often require labeled anomalies for training purposes. It is recognized that system logs are documented in natural language and contain a significant amount of semantic information. Nevertheless, traditional deep learning-based methods struggle to effectively capture this information. In recent years, significant advancements have been achieved in LLMs, such as GPT-4 [25], Llama 3 [26], and ChatGLM [27]. These models are characterized by their vast parameter sizes and are pretrained on substantially larger datasets, ranging from several gigabytes to terabytes in size. This extensive pretraining equips them with remarkable language comprehension abilities, enabling superior performance in tasks such as summarization, paraphrasing, and instruction following even in zero-shot scenarios [28]. Existing methods that utilize LLMs for log-based anomaly detection can be categorized into prompt engineering-based [29, 7, 30, 31] and fine-tuning-based [32, 33, 34, 35, 3, 36, 37, 38, 39, 40] approaches. Prompt engineering-based methods leverage the zero/few-shot capabilities of LLMs to detect anomalies based solely on the models’ internal knowledge. However, these methods often struggle to customize solutions for specific datasets, leading to suboptimal detection performance. Fine-tuning-based methods integrate LLMs into deep neural networks and tailor them to user-specific datasets. Nevertheless, these methods encounter challenges such as limited semantic understanding, suboptimal LLM utilization (relying solely on LLMs for semantic information extraction), and insufficient consideration of input data format, which can lead to memory overflow. To tackle the aforementioned challenges, we propose LogLLM, a novel log-based anomaly detection framework that harnesses LLMs. Unlike traditional methods that rely on log parsers for template extraction, LogLLM preprocesses log messages using regular expressions, thereby streamlining the entire process. LogLLM, a fine-tuning-based method, utilizes BERT, a transformer encoder-based model, to extract semantic vectors from log messages. Additionally, it employs Llama, a transformer decoder-based model, to classify log sequences. To ensure coherence in log semantics, we introduce a projector that aligns the vector representation spaces of BERT and Llama. Our framework is trained using a novel three-stage procedure designed to enhance both performance and adaptability. As we know, LLMs frequently face out-of-memory challenges due to their extensive parameter sizes [41]. Directly inputting the entire log sequence (by concatenating log messages into a long string) into Llama can lead to out-of-memory issues and potentially confuse the LLM, making it difficult to focus on key points for distinguishing anomalies. By adopting BERT to summarize each log message, LogLLM effectively mitigates these problems. Compared to other methods, LogLLM fully exploits the capabilities of LLMs for log-based anomaly detection. We conduct experiments across four public datasets, and the results demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, where new log templates frequently emerge due to software evolution, it effectively captures the semantic meaning of log messages and detects anomalies accurately. The ablation study confirms the effectiveness of the three-stage training procedure. The main contributions of our work are as follows: • We introduce LogLLM, a novel log-based anomaly detection framework leveraging LLMs. This study marks the first attempt to simultaneously employ transformer encoder-based and decoder-based LLMs, specifically BERT and Llama, for log-based anomaly detection. • We propose a novel three-stage procedure to optimize the training and coordination of different components within the deep model, enhancing both performance and adaptability. • We conduct extensive experiments on four publicly available real-world datasets, demonstrating that LogLLM achieves exceptional performance."
https://arxiv.org/html/2411.08552v1,Leveraging Pre-Trained Neural Networks to Enhance Machine Learning with Variational Quantum Circuits,"Quantum Machine Learning (QML) offers tremendous potential but is currently limited by the availability of qubits. We introduce an innovative approach that utilizes pre-trained neural networks to enhance Variational Quantum Circuits (VQC). This technique effectively separates approximation error from qubit count and removes the need for restrictive conditions, making QML more viable for real-world applications. Our method significantly improves parameter optimization for VQC while delivering notable gains in representation and generalization capabilities, as evidenced by rigorous theoretical analysis and extensive empirical testing on quantum dot classification tasks. Moreover, our results extend to applications such as human genome analysis, demonstrating the broad applicability of our approach. By addressing the constraints of current quantum hardware, our work paves the way for a new era of advanced QML applications, unlocking the full potential of quantum computing in fields such as machine learning, materials science, medicine, mimetics, and various interdisciplinary areas.","Quantum machine learning (QML) is an emerging interdisciplinary field that integrates the principles of quantum computing and machine learning [1, 2, 3, 4, 5, 6]. With the rapid development of quantum computing hardware, we are in the noisy intermediate-scale quantum (NISQ) era that admits only a few hundred physical qubits to implement QML algorithms on NISQ devices in the presence of high levels of quantum noise [7, 8, 9]. Variational quantum circuit (VQC) is a promising building block for a QML architecture for processing data and making predictions [10, 11, 12]. The VQC block is composed of parameterized quantum circuits that can be optimized by employing a stochastic gradient descent (SGD) algorithm to minimize a loss function in a black-propagation manner [13, 14, 15, 16]. The VQC’s resilience against quantum noise errors admits it to be applicable in many QML applications [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. However, the number of available qubits constrains the VQC’s representation power, and the amount of training data limits its generalization power [27, 28, 29]. Based on pre-trained neural networks, our theoretical findings introduce new bounds that allow the representation power independent of the number of qubits. We enhance the generalization power by requiring only a smaller target dataset. Notably, this target dataset pertains to quantum data that is significantly different from the source data used to train a classical neural network. Thus, this work focuses on both theoretical and empirical advancements in using pre-trained neural networks for VQC, including: 1. Demonstrating that pre-trained neural networks can enhance the representation and generalization powers of VQC blocks. 2. Validating our theoretical insights with experimental results from semiconductor quantum dot classification and human genome transcription factor binding site (TFBS) prediction. Unlike the end-to-end learning approach in hybrid quantum-classical architectures, we keep the pre-trained neural network’s parameters fixed without further tuning. This shows that this can enhance the VQC’s representation and generalization capabilities. This strategy allows us to utilize large pre-trained neural networks, including cutting-edge large language models with frozen parameters, to scale up QML using VQC blocks. More importantly, we are the first to establish the theoretical benefits of using pre-trained neural networks for VQC and to pioneer their application in quantum data scenarios. We show the architecture of pre-trained neural networks for VQC in Figure 1. A classical neural network \mathcal{X} is pre-trained with another neural network \mathcal{Y} following an end-to-end learning pipeline on a generic dataset D_{A} for a generic task T_{A}. The well-trained neural network Pre-\mathcal{X} is then transferred to a hybrid quantum-classical model comprised of classical module Pre-\mathcal{X} and a quantum component VQC. In the fine-tuning stage, only the VQC’s parameters need further update on a target dataset D_{B} given a target task T_{B}. Besides, the pre-trained neural network is closely related to the state-of-the-art foundation model of generative artificial intelligence, e.g., large language models like Generative Pre-trained Transformers (GPT) [30, 31], which can generate context-enriched features before going through the VQC block. Figure 1: The architecture of using pre-trained neural networks for VQC. (a) A classical model \mathcal{X} is pre-trained with another classical one \mathcal{Y} on a generic dataset D_{A} for a generic task T_{A}. (b) the pre-trained classical model Pre-\mathcal{X} is transferred to a hybrid quantum-classical architecture, where the classical model Pre-\mathcal{X} is frozen without a further parameter adjustment, and the VQC’s parameters need to be trained based on a target dataset D_{B} for a target task T_{B}. Recent studies have shown that hybrid quantum-classical models have achieved remarkable empirical results in numerous machine-learning applications. These models were developed using an end-to-end learning approach, where the parameters of both classical and quantum models are jointly optimized with the training data. However, in our work, we keep the parameters of the classical neural network fixed and only allow the VQC’s parameters to be trainable. Specifically, we perform an error performance analysis on our proposed method, demonstrating that pre-trained neural networks can enhance the VQC’s representation and generalization capabilities. Based on the error performance analysis, a generalized loss can be decomposed into the sum of three error components: approximation error, estimation error, and optimization error. The approximation error is associated with the representation power, and the estimation and optimization errors jointly correspond to the generalization power. By separately providing upper bounds on the three error terms, we offer a theoretical foundation on the algorithm of a pre-trained neural network for VQC, which exhibits theoretical improvement to the VQC model by comparing our newly derived upper bounds with the established ones for VQC. Our theoretical results are shown in Table 1 and summarized as follows: Table 1: Summarizing our theoretical results of using pre-trained neural networks for VQC (abbreviated as Pre-\mathcal{X}+VQC) and comparing them with VQC in [32]. Error Component Pre-\mathcal{X}+VQC VQC Approximation Error \tilde{\mathcal{O}}\left(\sqrt{\frac{C(\mathbb{F}_{\mathcal{X}})}{|D_{A}|}}% \right)+\mathcal{O}\left(\frac{1}{\sqrt{M}}\right) \tilde{\mathcal{O}}\left(\frac{1}{\sqrt{U}}\right)+\mathcal{O}\left(\frac{1}{% \sqrt{M}}\right) Estimation Error \tilde{\mathcal{O}}\left(\sqrt{\frac{C(\mathbb{F}_{V})}{|D_{B}|}}\right) \tilde{\mathcal{O}}\left(\sqrt{\frac{C(\mathbb{F}_{V})}{|D|}}\right) Conditions on Optimization Error Pre-trained NNs for VQC PL assumption Optimization Error \beta R^{2}+R\sqrt{\frac{L^{2}+\beta^{2}R^{2}}{T_{\rm sgd}}} \approx 0 • Approximation error: we provide an error upper bound on the approximation error with the form as \tilde{\mathcal{O}}\left(\sqrt{\frac{C(\mathbb{F}_{\mathcal{X}})}{|D_{A}|}}% \right)+\mathcal{O}\left(\frac{1}{\sqrt{M}}\right), where \mathbb{F}_{\mathcal{X}} denotes the functional class of pre-trained neural network Pre-\mathcal{X}, C(\cdot) is the measurement of the intrinsic complexity of the functional class, |D_{A}| and M refer to the amount of training data and the counts of quantum measurement. Compared with our previous upper bound on the VQC’s approximation error, our newly derived upper bound is independent of the number of qubits U, allowing for a few qubits in practice to achieve a small approximation error by scaling down the term \sqrt{\frac{C(\mathbb{F}_{\mathcal{X}})}{|D_{A}|}}. • Estimation error: our upper bound on the estimation error is taken in the form as \tilde{\mathcal{O}}\left(\sqrt{\frac{C(\mathbb{F}_{V})}{|D_{B}|}}\right), where C(\cdot) also denotes the measurement for the complexity of functional class, \mathbb{F}_{V} refers to the functional class of VQC and |D_{B}| denotes the number of target data for the VQC training. Our newly derived upper bound is reduced to a small value with increasing target data in D_{B}. In comparison with our previous upper bound on the VQC’s estimation error, the target data can be limited to a small scale to obtain a well-trained VQC model, which means that the number of target data in D_{B} could be much smaller than the amount of training data in D for VQC. • Optimization error: we demonstrate that the SGD algorithm returns an upper bound as \beta R^{2}+R\sqrt{\frac{L^{2}+\beta^{2}R^{2}}{T_{\rm sgd}}}, where R, \beta, and L are pre-defined hyper-parameters and T_{\rm sgd} denotes the number of epochs in the VQC training process. Unlike the necessary setup of Polyak-Łojasiewicz (PL) condition [33] for VQC to ensure a small optimization error, our proposed QML approach shows that it does not require such a prior condition. By controlling the hyperparameters R, \beta, and L, we can establish a connection between the optimization error and the constraints for the SGD algorithm. Besides, we assess and confirm the effectiveness of pre-trained neural networks for VQC through a practical use case involving the classification of semiconductor quantum dots and the prediction of transcription factor binding sites (TFBS) in the human genome. QDs are promising candidates for creating qubits, which are the fundamental components of NISQ devices. Our experiments focus on QD autotuning, which involves identifying charge state transition lines in two-dimensional stability diagrams for binary class pattern classification [34, 35, 36]. For TFBS prediction, the task relates to proteins that regulate gene expression by binding to specific DNA regions, such as promoters, and either activating or repressing gene expression. Each transcription factor has a distinct binding motif, a TFBS pattern. This prediction task involves quantum data derived from DNA sequences, making it well-suited for our QML paradigm [37, 38, 39]."
https://arxiv.org/html/2411.08537v1,MLV-Net: Rater-Based ajority-abel oting for Consistent eningeal ymphatic essel Segmentation,"Meningeal lymphatic vessels (MLVs) are responsible for the drainage of waste products from the human brain. An impairment in their functionality has been associated with aging as well as brain disorders like multiple sclerosis and Alzheimer’s disease. However, MLVs have only recently been described for the first time in magnetic resonance imaging (MRI), and their ramified structure renders manual segmentation particularly difficult. Further, as there is no consistent notion of their appearance, human-annotated MLV structures contain a high inter-rater variability that most automatic segmentation methods cannot take into account. In this work, we propose a new rater-aware training scheme for the popular nnU-Net model, and we explore rater-based ensembling strategies for accurate and consistent segmentation of MLVs. This enables us to boost nnU-Net’s performance while obtaining explicit predictions in different annotation styles and a rater-based uncertainty estimation. Our final model, MLV2-Net, achieves a Dice similarity coefficient of 0.806 with respect to the human reference standard. The model further matches the human inter-rater reliability and replicates age-related associations with MLV volume.","The lymphatic system — part of the immune system and responsible for the drainage of waste products — stretches across the entire human body and can often be found alongside blood vessels of the circulatory system. In the brain, the glymphatic system (Iliff et al., 2012) takes a similar role in that it clears waste products. To this end, meningeal lymphatic vessels (MLVs), located alongside the dural venous sinuses, transfer interstitial fluids and macromolecules to deep cervical lymph nodes (Louveau et al., 2015). An impairment in the MLVs’ functionality, potentially coupled with morphological changes such as thickening, has been linked to aging (Albayram et al., 2022) as well as to clinical conditions like Alzheimer’s (Goodman et al., 2018), multiple sclerosis (Louveau et al., 2018), and Parkinson’s disease (Ding et al., 2021). Yet, MLVs have only recently been described in 3D FLAIR MRI (Albayram et al., 2022), and their segmentation has only been done manually so far. However, manual annotation of MLVs is difficult and time-consuming due to their ramified structure, cf. Figure 1. Moreover, the training of automatic segmentation models on expert-annotated data is challenging due to the high inter-rater variability. Related work Deep neural networks for medical image segmentation are commonly trained to remove this variablility (Guo et al., 2024; Hatamizadeh et al., 2022; Ronneberger et al., 2015). However, this approach does not model the reality where disagreement about the true contours of a structure often exists (Warfield et al., 2004). This issue is especially problematic for newly discovered structures, such as MLVs, which bear enormous potential for innovative findings but for which a common notion of their appearance does not (yet) exist. Notably, a few dedicated methods for rater-aware segmentation were developed (Kohl et al., 2018; Mirikharaji et al., 2021; Warfield et al., 2004; Zhang et al., 2023). These approaches yielded effective results for certain standard applications, e.g., skin lesion (Mirikharaji et al., 2021) or brain tumor segmentation (Zhang et al., 2023), but transferring them to new tasks is difficult due to the large number of hyperparameters involved. These choices are non-trivial, not reproducible, and subject to the developer’s experience and preferences (Isensee et al., 2021). At the same time, the best segmentation results are typically obtained with nnU-Net (Isensee et al., 2024), which provides a versatile framework for hyperparameter selection. Unfortunately, nnU-Net cannot model the variability in segmentations provided by different raters — a functionality essential for trustworthy and comprehensible clinical predictions. We close this gap and develop a rater-based ensembling strategy for nnU-Net that keeps its architecture intact and augments it with the ability to replicate individual raters’ annotation styles. Contribution We present the first automatic method for segmentation of MLVs from 3D FLAIR MRI. To achieve accurate and reliable segmentation of the ramified structure, we made the following technical contributions. First, we developed an innovative rater-aware training scheme for the popular nnU-Net model that takes into account the different raters involved in the creation of the training set. This enables nnUNet to learn individual raters’ segmentation styles and to explicitly predict a set of plausible segmentations. In a second step, we aggregate the predictions with a weighted majority-label voting scheme for best segmentation accuracy. In addition, we obtain a rater-based uncertainty prediction from the model. Finally, since the volume of MLVs is usually of utmost importance for downstream analyses, we derive error boundaries of the model’s predicted volumes with respect to the ground-truth volume. Table 1: Composition of the annotated and raw datasets used in this study. Name Used for Joint annot. #Annotations #Images by all raters per image Training set Training & validation ✗ 1 27 IRR set Testing (inter-rater reliability) ✗ 4 2 Consensus set Testing (accuracy) ✓ 1 4 Raw set Testing (downstream analysis) N/A N/A 22"
https://arxiv.org/html/2411.08533v1,ACROSS: A Deformation-Based Cross-Modal Representation for Robotic Tactile Perception,"Tactile perception is essential for human interaction with the environment and is becoming increasingly crucial in robotics. Tactile sensors like the BioTac mimic human fingertips and provide detailed interaction data. Despite its utility in applications like slip detection and object identification, this sensor is now deprecated, making many existing valuable datasets obsolete. However, recreating similar datasets with newer sensor technologies is both tedious and time-consuming. Therefore, it is crucial to adapt these existing datasets for use with new setups and modalities. In response, we introduce ACROSS, a novel framework for translating data between tactile sensors by exploiting sensor deformation information. We demonstrate the approach by translating BioTac signals into the DIGIT sensor. Our framework consists of first converting the input signals into 3D deformation meshes. We then transition from the 3D deformation mesh of one sensor to the mesh of another, and finally convert the generated 3D deformation mesh into the corresponding output space. We demonstrate our approach to the most challenging problem of going from a low-dimensional tactile representation to a high-dimensional one. In particular, we transfer the tactile signals of a BioTac sensor to DIGIT tactile images. Our approach enables the continued use of valuable datasets and the exchange of data between groups with different setups.","Tactile feedback is gaining significant attention in robotics[1, 2]. Tactile sensors leverage various information modalities, come in diverse shapes and sizes, and are implemented in a wide range of technologies. This diversity makes the exchange of data and trained models challenging. Moreover, as sensor technology improves, datasets become obsolete. For instance, BioTac by SynTouch was a high-end tactile sensor, designed like a human fingertip. It has an elastomer covering a rigid core filled with an incompressible conductive fluid. The sensor outputs voltage readings from 19 internal electrodes, capturing changes in the fluid. These readings are processed as time-series signal data [3, 4, 5]. The BioTac has been proven useful in various applications such as detecting object slips and the direction of slips [6, 7] or identifying objects [8]. However, this sensor is now deprecated. Consequently, many influential existing datasets, such as the BioTac SP direction of slip dataset [7], the BioTac SP grasp stability dataset [6] or the BioTac 2P grasp stability dataset [9], are now obsolete. These datasets capture sensor outputs, specifically BioTac signals, recorded while the sensors are mounted on robotic hands that grasp various objects under different conditions, with the stability of the grasps being evaluated. Despite their obsolescence, such datasets remain important, as grasp stability and slip detection continue to be an active field of research [10]. Furthermore, designing and collecting similar datasets is a time-consuming and complex task. It requires careful consideration of various factors, such as the choice of sensors and their resolution, the data collection methods, and the labeling process, among other requirements. Hence, there is a need to convert existing useful datasets into formats compatible with newer sensor modalities, even if they involve different robotic or sensor configurations. This allows researchers to leverage intrinsic information still relevant to specific tasks, while also saving time and resources by avoiding the need to collect entirely new datasets. To this end, we propose ACROSS, a versatile approach for transferring tactile data between sensors of varying resolutions, including low-to-high, high-to-high, and high-to-low resolution transfers [11]. We demonstrate the effectiveness of ACROSS by converting low-resolution tactile (time series) data from a BioTac sensor into a high-resolution vision-based DIGIT sensor [12]. Our method enables the utilization of existing datasets gathered with outdated sensors, avoiding the tedious process of gathering data from scratch. Moreover, it facilitates a way to transition between two intrinsically distinct tactile sensor modalities, e.g., signal data to visual representations. Additionally, we provide an openly available dataset comprising over 155K unique 3D mesh deformation pairs from interactions involving BioTac and DIGIT sensors. This dataset includes various types of indenters, the force exerted on each sensor, and rendered images of the scenes. The source code, dataset, and neural network checkpoints can be found on our website: https://wzaielamri.github.io/publication/across."
https://arxiv.org/html/2411.08504v2,Towards Objective and Unbiased Decision Assessments with LLM-Enhanced Hierarchical Attention Networks,"How objective and unbiased are we while making decisions? This work investigates cognitive bias identification in high-stake decision making process by human experts, questioning its effectiveness in real-world settings, such as candidates assessments for university admission. We begin with a statistical analysis assessing correlations among different decision points among in the current process, which discovers discrepancies that imply cognitive bias and inconsistency in decisions. This motivates our exploration of bias-aware AI-augmented workflow that surpass human judgment. We propose BGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding, Gated Residual Connections and Multi-Head Attention. Using it as a backbone model, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow, which simulate real-world decision-making. In our experiments, both the proposed model and the agentic workflow significantly improves on both human judgment and alternative models, validated with real-world data. Source code is available at: https://github.com/junhua/bgm-han.","Field Format Sample Entry GCEA [GCEA] School:{school}, UAS:{score}; Grades:{subject grade pairs} [GCEA]: School:HCI, UAS:90.0; Grades:H1 PROJECT WORK A, H1 GENERAL PAPER A, H2 ECONOMICS A, H2 CHEMISTRY A, … GCEO [GCEO] {subject-grade pairs} [GCEO] HIGHER CHINESE B3, ENGLISH A1, ELEMENTARY MATHEMATICS A2, … Leadership [Leadership] {activity}, Level:{role}, Year:{year}, Category:{type}, Participation:{level}. (Multiple entries) [Leadership] Mind Sports Club, Level:Captain, Year:2023, Category:Sports, Participation:Executive Committee PIQ [PIQ{n}]{essay text}, where n = 1,2,3,4,5 [PIQ1] My first visit to the university in Primary 5 for a Learning Journey left me with a deep impression… OfferType Offered / Not Offered Offered Analysis Structured text (profile analysis by LLM) This candidate demonstrates a strong academic profile, particularly in STEM, with straight As… Table 1. Data Format and Sample Entry Decision making in high-stake scenarios is often done by human experts leveraging their domain expertise and experience to optimize decision quality (Alur et al., 2024). However, subjectivity and cognitive biases, such as anchoring bias (Haag et al., 2024) and confirmation bias (Echterhoff and et al., 2024), are often difficult to detect and avoid (Kahneman and Tversky, 2023). Mitigating decision biases is critical to ensure long-term sustainable outcomes and fairness to stakeholders, especially in high-stakes environments (Ghai and Mueller, 2022). Recent studies propose artificial intelligence (AI) systems to enhance manual decision processes, such as fairness-aware AI systems guiding decision-makers toward more impartial choices (Yang and et al., 2024), Explainable AI frameworks that identify potential biases and improve judgment accuracy (Haag et al., 2024), and a human-AI collaborative to audit and mitigate social biases (Ghai and Mueller, 2022). Despite the promising recent work in augmenting human expertise and mitigating various biases, the overall advancements are underwhelming due to several reasons. Firstly, the complexity and context-specific nature of cognitive biases make it challenging for AI systems to accurately detect and mitigate (Kahneman and Tversky, 2023). Secondly, the limited interpretability of AI models can reduce trust in high-stakes settings (Haag et al., 2024). Lastly, real world scenarios often rely on sensitive or proprietary data. The lack of accessible data impose tremendous challenges in such research activities (Smith et al., 2024). In this regard, this work aims to contribute to AI-augmented, bias-aware decision making in a real world setting, such as university admissions. Current processes involve complex, semi-structured data that requires nuanced assessment across multiple dimensions. Through statistical analysis, we identified non-trivial discrepancies between human evaluations and final outcomes, suggesting inconsistencies and cognitive biases. This motivates our approach, which seeks not only to automate but also to enhance decision consistency and reduce subjective influences through structured AI interventions. We propose BGM-HAN an enhanced Hierarchical Attention Network with Byte-pair Encoded, Gated Residual Connetions and Multihead Attention, which takes an leverage hierarchical learning approach to better capture and interpret multi-level semi-structured data. Using BGM-HAN as backbone, we introduce a Shortlist-Analyze-Recommend (SAR) agentic workflow to simulate existing human decision processes. In our experiments, the proposed models outperform different categories of baseline models. While comparing to current human evaluation, our proposed workflow introduces over 9.6% improvement in F1-score and accuracy. The promising results uncovers potentials in integrating hierarchical learning with LLM-augmentation to perform automated decision making with implicit fairness and consistency in high-stakes, real-world decision-making environments. Contributions. In summary, this paper makes the following novel contributions: Firstly, we propose a statistical approach to identify cognitive bias and inconsistency in a real-world decision making process, i.e., university admission assessments. Secondly, we propose a hierarchical learning model, BGM-HAN, that effectively represents multi-level semi-structured data and experimentally outperforms baseline models of multiple categories. Lastly, we propose an agentic workflow, \mathcal{W}_{SAR}, that mimics existing human decision process. \mathcal{W}_{SAR} mitigates inconsistency and cognitive bias across different decision makers, and empirically outperforms human evaluation by over 9.6% in F1-score and accuracy for in decision recommendation."
https://arxiv.org/html/2411.08478v1,Learning Model Agnostic Explanations via Constraint Programming,"Interpretable Machine Learning faces a recurring challenge of explaining the predictions made by opaque classifiers such as ensemble models, kernel methods, or neural networks in terms that are understandable to humans. When the model is viewed as a black box, the objective is to identify a small set of features that jointly determine the black box response with minimal error. However, finding such model-agnostic explanations is computationally demanding, as the problem is intractable even for binary classifiers. In this paper, the task is framed as a Constraint Optimization Problem, where the constraint solver seeks an explanation of minimum error and bounded size for an input data instance and a set of samples generated by the black box. From a theoretical perspective, this constraint programming approach offers PAC-style guarantees for the output explanation. We evaluate the approach empirically on various datasets and show that it statistically outperforms the state-of-the-art heuristic Anchors method.","With the increasing influence of machine learning systems in our daily lives, there is a pressing need to comprehend the reasoning behind their predictions. However, the current black-box nature of predictive models such as, for example, ensemble methods, kernel techniques or neural networks, often leaves users bewildered. In order to address this challenge, the field of Interpretable Machine Learning (IML) has emerged, which focuses on developing new learning and explanation techniques to make predictive models more transparent and comprehensible [6, 24]. A common task in IML is explaining the predictions made by a classifier on data instances in terms that are easy for humans to understand. For example, when evaluating a personal loan application based on tabular data about the applicant, features such as a stable income, a low debt-to-income ratio, and the availability of a co-signer may have a collective impact on the loan approval. When the classifier is viewed as a black box with only query access to some instance-to-label function f, the set of features S used to explain the output f(\bm{x}) of an instance \bm{x} is referred to as model agnostic explanation [21, 26, 27]. One important aspect of model agnostic explanations is their flexibility. Such explanations can indeed be provided without any knowledge of the internal structure and parameters of the predictive model. This property is not only useful for deciphering the predictions made by complex models but also for maintaining confidentiality in cases where the internal components of the model cannot be revealed. However, flexibility is not the only criterion for assessing the quality of explanations. Explaining the predictions of black-box models involves balancing two other important criteria. The first is the precision, or fidelity [24], of the explanation in approximating the model’s predictions. An explanation S for an instance \bm{x} and a model f is considered abductive if changes in feature values outside of the explanation S do not affect the model’s output f(\bm{x}). When f is a Boolean function, the conjunction of features in the abductive explanation S can be viewed as an implicant of f that covers the instance \bm{x}. Abductive explanations have received significant attention in the literature due to their perfect fidelity [3, 7, 8, 13, 14]. The second criterion to consider is the conciseness of the explanation, which is limited by the human ability to reason about multiple interacting factors. As Miller conjectured in 1956 [23], this cognitive limit is of seven plus or minus two elements, which has been confirmed by numerous experiments in cognitive science (see e.g. [28]). Abductive explanations suffer from a significant weakness in this regard, as their size is often uncontrollable. Even the smallest abductive explanation may require too many features to be logically sound and comprehensible to humans. Therefore, finding a balance between precision and conciseness is crucial when generating explanations for black-box models. The concept of probabilistic explanation, introduced by Blanc et al. [4] and Waeldchen et al. [32], captures this balance. Informally, the precision error \epsilon_{f,\bm{x}}(S) of an explanation S for an instance \bm{x} and a model f is defined by the probability that a random instance \bm{z} is classified differently from \bm{x} by f, when \bm{z} and \bm{x} agree on all the features in S. In other words, \epsilon_{f,\bm{x}}(S) is the probability of making a mistake in predicting the black-box response f(\bm{x}) using only the features in S. With this notion in hand, the goal of the present study is to find a model-agnostic explanation S that is no larger than a small constant k, while minimizing the error \epsilon_{f,\bm{x}}(S) for a given input \bm{x} and a black-box model f. This optimization task is very computationally intensive. Even in the model-specific case where we have a Boolean circuit representation of f, finding the minimizer S of \epsilon_{f,\bm{x}}(S) while satisfying the cardinality constraint |S|\leq k is \textsf{NP}^{\textsf{PP}}-hard [32]. This difficulty persists even in the case where f is represented by a binary decision tree, as the problem remains NP-hard [1]. 1.1 Contributions This paper introduces a framework for learning explanations that are independent of the model, consisting of two main ideas. As any explanation can be represented as an if-then rule [27], we consider a hypothesis class that includes all rules whose body is a subset of features of the data instance \bm{x} to be explained, and whose head is the prediction f(\bm{x}). We first demonstrate that an efficient agnostic PAC learner for this hypothesis class can lead to an efficient model-agnostic explainer. However, since this hypothesis class is equivalent to the class of monotone monomials, the learning problem remains NP-hard. Nevertheless, we next show that it can be formulated as a Constraint Optimization Problem (COP) using a few linear constraints and channeling constraints. By combining these two ideas, the optimal value attained by the constraint solver provides a bounded approximation of the objective value for the precision function \epsilon_{f,\bm{x}}(\cdot). From an empirical standpoint, our approach is compared with the well-known model-agnostic explainer Anchors [27]. By conducting experiments on various datasets, we show that our approach outperforms Anchors in terms of precision, while using a reasonable amount of time for the solver. 1.2 Related Work In the field of Interpretable Machine Learning, researchers have proposed various techniques to understand the behavior of machine learning models. Post hoc explanations are one such technique that aims to provide insights into how a model works without affecting its performance. These explanations give additional knowledge about the prediction f(\bm{x}) made by a model f on a specific data instance \bm{x}. Common types of post hoc explanations are feature-based explanations [21, 27], which interpret the prediction f(\bm{x}) by identifying the most relevant features of the instance \bm{x}, and example-based explanations [18, 20], which improves the comprehension of f(\bm{x}) using counterfactual or influential examples. Feature-based explanations, examined in this study, can be categorized into two types: model-specific and model-agnostic explanations. 1.2.1 Model-Specific Explanations. By exploiting the structure and parameters of the predictive model f, model-specific approaches aim to provide precise and mathematically verifiable explanations. Notably, when f is a Boolean classifier, a common explanation for predicting the output f(\bm{x}) of an input instance \bm{x} is a subset-minimal collection of features S such that the restriction \bm{x}_{S} of \bm{x} to S determines f(\bm{x}). Such an abductive explanation [14], also called sufficient reason [8], is perfectly accurate since \bm{x}_{S} is a prime implicant of f that covers \bm{x} [30]. Although finding an abductive explanation is generally an NP-hard problem, tractable cases have been identified for various model classes including, among others, decision trees [2, 11, 17], Naive Bayes classifiers [22], and monotone threshold functions [7]. Actually, even for intractable classes such as decision lists and random forests, empirical results indicate that abductive explanations can be quickly found in practice using constraint-based approaches [3, 13, 15]. In order to overcome the uncontrollable size of abductive explanations, model-specific approaches have recently focused on probabilistic explanations, which convey a natural trade-off between conciseness and precision [5, 16]. In simple terms, a (k,\varepsilon)-probabilistic explanation for a classifier f and an instance \bm{x} is a subset S of features such that |S|\leq k and \epsilon_{f,\bm{x}}(S)\leq\varepsilon. Unfortunately, finding probabilistic explanations is generally more difficult than finding abductive explanations. Indeed, the problem of deciding whether a Boolean circuit representation of f admits a (k,\varepsilon)-probabilistic explanation for an instance \bm{x} is complete for \textsf{NP}^{\textsf{PP}}[32], a complexity class beyond the reach of modern solvers. 1.2.2 Model-Agnostic Explanations. Unlike model-specific explanations, model-agnostic explanations do not rely on any assumptions about the inner workings of the predictive model f. Instead, f is treated as a black box and accessed through membership queries. In post-hoc explanations, model-agnostic techniques seek a surrogate that approximates the model f in some local neighborhood of the instance to be explained \bm{x}. This neighborhood is explored using sampling and perturbation strategies. When the model is a classifier, the surrogate provided by the Lime approach [26] and its variants [9, 33] takes the form of a linear threshold function. Similarly, in the Shap approach [21], the returned explanation is a linear function where each entry approximates the Shapley value of the corresponding feature that contributes to the outcome. Arguably, the Anchors approach [27] is the most relevant work to our study. It represents the surrogate of the model f in the neighborhood of the instance \bm{x} as an easy-to-understand if-then rule. This rule can be described as a set S of features which jointly determine the outcome f(\bm{x}). Although model-agnostic approaches are appealingly flexible, most of them are heuristic, meaning that they do not provide any theoretical guarantees regarding the quality or size of the explanations they generate. In practice, these approaches often return incorrect explanations [12], which can make them even less reliable. Such observations highlight the necessity of alternative techniques that preserve the flexibility of model agnosticism but provide precision and conciseness guarantees. To the best of our knowledge, only Blanc et al. [4] have researched this area. Their objective is to find approximations of (k,\varepsilon)-probabilistic explanations for random data instances, using only query access to the model f. Based on some techniques in implicit learning, their result provides PAC-style guarantees on the quality and size of the approximate explanations. However, their approach is difficult to apply in practice since its runtime complexity is prohibitive. Our framework, in contrast, provides similar guarantees, using a restricted number of calls to an NP-oracle, implemented by a constraint solver."
https://arxiv.org/html/2411.08460v1,Trap-MID: Trapdoor-based Defense against Model Inversion Attacks,"Model Inversion (MI) attacks pose a significant threat to the privacy of Deep Neural Networks by recovering training data distribution from well-trained models. While existing defenses often rely on regularization techniques to reduce information leakage, they remain vulnerable to recent attacks. In this paper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to mislead MI attacks. A trapdoor is integrated into the model to predict a specific label when the input is injected with the corresponding trigger. Consequently, this trapdoor information serves as the ""shortcut"" for MI attacks, leading them to extract trapdoor triggers rather than private data. We provide theoretical insights into the impacts of trapdoor’s effectiveness and naturalness on deceiving MI attacks. In addition, empirical experiments demonstrate the state-of-the-art defense performance of Trap-MID against various MI attacks without the requirements for extra data or large computational overhead. Our source code is publicly available at https://github.com/ntuaislab/Trap-MID.","Deep Neural Networks (DNNs) have been successfully applied in various domains. However, training DNNs could involve sensitive data like facial recognition and medical diagnosis, which raises privacy concerns. Model Inversion (MI) stands as one of the important privacy attacks aimed at reconstructing private data within specific classes from a well-trained model. For example, an adversary may recover the training images of specific identities from a facial recognition system. MI attacks were first introduced by Fredrikson et al. [1, 2], reconstructing private attributes from low-capacity models. After that, Zhang et al. [3] proposed Generative Model-Inversion (GMI) attacks to reconstruct private images from DNNs, utilizing Generative Adversarial Network (GAN) as a general prior. This GAN-based framework has been widely adopted by later attacks [4, 5, 6, 7, 8, 9, 10, 11]. Among them, PLG-MI [8] achieves state-of-the-art attack performance. Previous works also demonstrated the efficacy of MI attacks under black-box [9, 10, 12] or label-only [11, 13] settings. In this paper, we focus on defending against white-box attacks, which pose a more challenging scenario. Most existing defenses focus on reducing the information leakage through Differential Privacy (DP) [1, 3], dependency regularization [14, 15], or manipulating the loss landscape [16]. However, these methods remain vulnerable to recent MI attacks [16]. In contrast, recent works proposed to mislead MI attacks by prompting models to classify fake samples as the protected class with high confidence [17, 18, 19]. Although effective, these misleading-based strategies face challenges, including additional data requirements and substantial computational overhead. Furthermore, they typically protect only a single or a limited set of classes, while other defenses aim to secure all classes simultaneously. Sharing a similar idea, Shan et al. [20] introduced Trapdoor-enabled Adversarial Detection (TeD) against targeted adversarial attacks, which aims to change the model behaviors by applying adversarial perturbations to the input data. Instead of training a robust model against such perturbations, TeD shows that injecting trapdoors into the models can mislead the adversarial attacks to result in samples with similar features to poisoned data, thereby empowering the adversarial detection by measuring their similarity to the trapdoor signatures. Inspired by previous misleading-based defenses [17, 18, 19] and TeD [20], we propose Trapdoor-based Model Inversion Defense (Trap-MID), which deceives MI attacks by incorporating trapdoors as the ""shortcuts"". We discuss the key properties of trapdoor triggers necessary for misleading these attacks, and experiments show that Trap-MID outperforms existing methods in defending against MI attacks. Our contributions can be summarized as follows: 1. We propose a trapdoor-based defense, Trap-MID, to preserve privacy by misleading MI attacks. Through extensive experimentation, it presents state-of-the-art defense performance against various MI attacks. 2. To the best of our knowledge, we are the first to establish the connection between MI defenses and trapdoor injection techniques. We theoretically discuss the importance of trapdoor effectiveness and naturalness in misleading MI attacks and showcase its efficacy with empirical experiments. 3. Compared to previous trapping defenses, our trapdoor-based framework is more computationally and data-efficient, without large computational overhead or additional data."
https://arxiv.org/html/2411.08447v1,Learning Dynamic Cognitive Map with Autonomous Navigation,"Inspired by animal navigation strategies, we introduce a novel computational model to navigate and map a space rooted in biologically inspired principles. Animals exhibit extraordinary navigation prowess, harnessing memory, imagination, and strategic decision-making to traverse complex and aliased environments adeptly. Our model aims to replicate these capabilities by incorporating a dynamically expanding cognitive map over predicted poses within an Active Inference framework, enhancing our agent’s generative model plasticity to novelty and environmental changes. Through structure learning and active inference navigation, our model demonstrates efficient exploration and exploitation, dynamically expanding its model capacity in response to anticipated novel un-visited locations and updating the map given new evidence contradicting previous beliefs. Comparative analyses in mini-grid environments with the Clone-Structured Cognitive Graph model (CSCG), which shares similar objectives, highlight our model’s ability to rapidly learn environmental structures within a single episode, with minimal navigation overlap. Our model achieves this without prior knowledge of observation and world dimensions, underscoring its robustness and efficacy in navigating intricate environments.autonomous navigation; active inference; cognitive map; structure learning; dynamic mapping; knowledge learning","Humans effortlessly discern their position in space, plan their next move, and rapidly grasp the layout of their surroundings (1, 2) when faced with ambiguous sensory input (3). Replicating these abilities in autonomous artificial agents is a significant challenge, requiring robust sensory systems, efficient memory management, and sophisticated decision-making algorithms. Unlike humans, artificial agents lack inherent cognitive abilities and adaptive learning mechanisms, particularly when confronted with aliased observations, where sensory inputs are ambiguous or misleading (4). To replicate human navigational abilities, an agent must capture the dynamic spatial layout of the environment, localise itself and predict the consequences of its actions. Most attempts to achieve this combine those fundamental elements in SLAM algorithms (Simultaneous Localisation and Mapping), often based on Euclidian maps (5, 6). However, these methods require substantial memory as the world expands. Other strategies involve deep learning models, which depend on large datasets and struggle to adapt to unexpected events not encountered during training (7). A more efficient alternative lies in cognitive graphs or maps and learning a mental representation of the world from partial observations (8, 9), creating a symbolic structure of the environment (10, 11). Cognitive graphs, by definition, represent a ”mental understanding of an environment” derived from contextual cues like spatial relationships (12). Alongside this structure, the ability to imagine the outcomes of actions enables more reliable navigation decisions based on preferences (13, 14). Our approach integrates those biological capabilities into a unified model. Using visual observations and proprioception (15), we construct a cognitive map through a generative model, enabling navigation with an Active Inference (AIF) framework. This model links states by incorporating observations and positions through transitions, as illustrated in Figure 1b), showing the processed observation of the agent and c) presenting the resulting cognitive graph. Figure 1: a) From a full 3 by 3 rooms mini-grid environment (16, 17) to b) rooms observation layout as perceived by the agent and the path it has taken between rooms - composed of a line from black to white-, c) shows the agent final internal topological graph (cognitive graph) linking all the locations between them. Using Bayesian inference, the model predicts future states and positions, growing its cognitive map by forming prior beliefs about un-visited locations. As new observations are made, the agent updates its internal model, dynamically refining its representation of the environment (11). This continual adjustment allows the agent to effectively navigate complex environments by anticipating and learning from uncharted areas (18). Our internal positioning system draws inspiration from the neural positioning system found in rodents and primates, aiding in self-localisation and providing an intrinsic metric for measuring distance and relative direction between locations (15, 3, 19). To achieve goal-directed navigation and exploration, we employ AIF to model the agent’s intrinsic behaviour in a biologically plausible way. Unlike methods relying on pre-training for specific environments, our approach introduces a navigation and dynamic cognitive map growing based on the Free Energy (FE) principle. This map is inspired by mechanisms observed in animals, such as border cells for obstacle detection (20) and the visual cortex for visual perception. It continuously expands by predicting new observations and adapting dynamically to changing environmental structures. This work aims to develop an autonomous agent that can determine where it is, decide where to navigate, and learn the structure of complex, unknown environments without prior training, mimicking the adaptability and spatial awareness observed in biological organisms. Traditional exploration approaches and deep learning models struggle in dynamic settings, requiring extensive memory and pre-collected datasets to predict future settings, or they face difficulties in adapting to untrained situations. The challenge is to design a model that allows agents to autonomously build, update, and expand an internal map based on current sensory data and past beliefs, efficiently managing ambiguous observations (such as aliased states) and responding flexibly to unexpected environmental changes. Our contribution to this problem encompasses several key aspects: • Proposing a novel dynamic cognitive mapping approach that allows agents to predict and extend their internal map over imagined trajectories, enabling anticipatory navigation and rapid adaptation to new environments. • Developing a navigation model that operates without pre-training or prior exposure, allowing the agent to successfully explore and make decisions in unfamiliar environments. • Proposing a flexible navigation behaviour fully explicit by relying upon the AIF framework. • Outperforming in environmental learning and decision-making efficiency the Clone-Structured Cognitive Graph (CSCG) model (21), a prominent model for cognitive map representation (21). • Showcasing robust adaptability, where the model responds seamlessly to dynamic environmental changes, replicating rat maze-like scenarios, thus emphasising its practical application in flexible, real-world navigation tasks. • Incorporating biologically inspired processes like border cells and visual cortex perception, our agent’s navigation strategy is theoretically grounded and scalable to more realistic settings."
https://arxiv.org/html/2411.08433v1,3D Multi-Object Tracking with Semi-Supervised GRU-Kalman Filter,"3D Multi-Object Tracking (MOT), a fundamental component of environmental perception, is essential for intelligent systems like autonomous driving and robotic sensing. Although Tracking-by-Detection frameworks have demonstrated excellent performance in recent years, their application in real-world scenarios faces significant challenges. Object movement in complex environments is often highly nonlinear, while existing methods typically rely on linear approximations of motion. Furthermore, system noise is frequently modeled as a Gaussian distribution, which fails to capture the true complexity of the noise dynamics. These oversimplified modeling assumptions can lead to significant reductions in tracking precision. To address this, we propose a GRU-based MOT method, which introduces a learnable Kalman filter into the motion module. This approach is able to learn object motion characteristics through data-driven learning, thereby avoiding the need for manual model design and model error. At the same time, to avoid abnormal supervision caused by the wrong association between annotations and trajectories, we design a semi-supervised learning strategy to accelerate the convergence speed and improve the robustness of the model. Evaluation experiment on the nuScenes and Argoverse2 datasets demonstrates that our system exhibits superior performance and significant potential compared to traditional TBD methods.The code is available at https://github.com/xiang-1208/GRUTrack.","I INTRODUCTION Multi-Object Tracking (MOT)[1] is a crucial research topic within the field of computer vision and serves as a foundational technology in numerous intelligent applications, such as autonomous driving, traffic flow analysis, security surveillance, robotics, and action recognition. At present, with the increasing performance of Multi-Object Detection (MOD)[2, 3, 4, 5], Multi-Object Tracking (MOT) methods based on the “Tracking-by-Detection” (TBD) [1, 6, 7] have demonstrated superior accuracy and robustness. These TBD methods follow the motion process of the tracked object, contrasting with “Joint Detection and Tracking” (JDT) approaches [8, 2, 9] which do not perform as well. In general, TBD methods update the state of tracked objects incrementally by constructing motion model and employing recursive Bayesian filter estimator. However, due to the varied motion characteristics of different objects within the scene, a single state space (SS) and estimator parameter cannot well match the different motion characteristics between various categories, which reduces the consistency between the motion state update and the actual, leading to false matching and inaccurate state update. Some approaches[7, 10, 11] take note of this and design motion parameters or association strategies for each class to be more relevant to the different characteristics of different classes. However, these methods fail to fundamentally solve the problem of multi-category differences. On the one hand, with the continuous addition or refinement of categories, it is not only tedious to design motion models for each category, but also too dependent on the designer’s experience. On the other hand, although it is possible to capture different types of motion characteristics by continuously refining the categories, most methods still use model-based state filters, such as Kalman Filter (KF)[12] and Extended Kalman Filter (EKF)[13, 14]. The effectiveness of these models depends on the accuracy of the state model and the validity of the motion hypothesis. In the actual MOT, the latent state of the system is nonlinear and complex, and it is even difficult to be accurately described as a tractable state model. In this case, model-based state estimators typically simplify the motion dynamics by linearizing the process and assuming the system noise follows a Gaussian distribution. This assumption does not match the actual situation, and these modeling inaccuracies often bring the loss of tracking precision to the system. To this end, we propose a partially learnable MOT method by introducing a Gated Recurrent Unit (GRU)-based Kalman filter into the motion module of TBD. This method can replace the traditional manual model design in a data-driven manner, thus eliminating the need to design a unique SS and estimator for each class. Specifically, we use multiple GRUs to simulate the loops in Recursive Bayesian Filtering. The model automatically learns the noise distribution, state transition matrix and observation matrix. It avoids the mismatch of noise modeling and the loss of precision caused by linearizing the state transition and observation function. This is doable in theory because neural network-based state estimation has been shown to capture the motion characteristics of complex processes[15, 16], which is also applicable to state transitions in MOT. Figure 1: The pipeline of our proposed method at frame n. T^{D}_{n} is the trajectories updated by associating upper observations D_{n} and using the motion module. Our design focuses on two parts, one is GRU-Kalman Filter: it uses three GRUs to simulate the Kalman filtering process. The second is Semi-Supervised learning, which uses dataset annotations and pseudo-labels generated by a parallel Kalman filter for joint training. However, it is not feasible to directly use the learnable Kalman Filter in MOT. On the one hand, due to the partial annotation of the dataset, the amount of trainable data is small, which is easy to overfitting. On the other hand, since the annotations and trajectories are associated by a hand-designed association strategy, the errors of association will introduce error supervision into the system. Therefore, we propose to parallel a Kalman filter during the training process to generate pseudo-labels for those unlabeled data for semi-supervised training. The experimental results prove the effectiveness of our strategy. Specifically, our contributions are as follows: • We propose a data-driven MOT method by using a GRU-based motion module to avoid the precision loss of traditional methods for noise mismatch modeling and motion process linearization. • We design a pseudo-label-based semi-supervised method, which greatly expands the amount of available training data and label robustness, so that the system can converge in fewer training cycles. • We evaluate our method on the nuScenes[17] and Argoverse2[18] datasets. Our system demonstrates performance comparable to traditional MOT systems and strong level of generalization, while obviating the need for manually designing a model for each object category."
https://arxiv.org/html/2411.08432v1,One STEP at a time: Language Agents are Stepwise Planners,"Language agents have shown promising adaptability in dynamic environments to perform complex tasks. However, despite the versatile knowledge embedded in large language models, these agents still fall short when it comes to tasks that require planning. We introduce STEP, a novel framework designed to efficiently learn from previous experiences to enhance the planning capabilities of language agents in future steps. Concretely, STEP functions through four interconnected components. First, the Planner takes on the task, breaks it down into subtasks and provides relevant insights. Then the Executor generates action candidates, while the Evaluator ensures the actions align with learned rules from previous experiences. Lastly, Memory stores experiences to inform future decisions. In the ScienceWorld Wang et al. (2022) benchmark, our results show that STEP consistently outperforms state-of-the-art models, achieving an overall score of 67.4 and successfully completing 12 out of 18 tasks. These findings highlight STEP’s potential as a framework for enhancing planning capabilities in language agents, paving the way for more sophisticated task-solving in dynamic environments. 111Project page with code: https://github.com/minhtuong201/step.git","Autonomous agents that incorporate Large Language Models (LLMs) as integral cognitive systems Sumers et al. (2023) have demonstrated significant capabilities in addressing a diverse range of interactive tasks e.g., mathematical problems Cobbe et al. (2021); Hendrycks et al. (2021), programming challenges Zhuo et al. (2024); Jimenez et al. (2024), and logical reasoning Tafjord et al. (2021); Saparov and He (2023). Nonetheless, their performance tends to diminish in dynamic scenarios, such as Web navigation Zhou et al. (2024b); Yao et al. (2023a) and Open-ended environments Wang et al. (2022); Shridhar et al. (2021), which require robust reasoning capabilities of the agents. A key contributing factor to language agents’ efficiency in long tasks is the notion of memory Sumers et al. (2023). The recent approaches Majumder et al. (2023); Zhao et al. (2023a) guided the agent to store reflections on their experience of solving a task Shinn et al. (2023) in memory, and then to retrieve these to improve future attempts Yao et al. (2023c). The use of verbal refinements rather than updating model parameters, these techniques are more flexible than conventional Reinforcement Learning (RL) methods. However, this memory module often lacks a retrieval mechanism. Additionally, complex tasks that cannot be solved in a single attempt also require effective planning and goal decomposition mechanisms (we will explain this in more detail later in this paper). In this paper, we take a close look at the memory utilization and planning capabilities of the language agent. More concretely, we propose STEP - a novel framework for Stepwise Planning which consists of a Planner, an Executor, and an Evaluator. Upon receiving a task from the environment, the Planner decomposes it into manageable subtasks and retrieves relevant information from the Memory. After receiving messages, the Executor then generates action candidates, which are subsequently evaluated by the Evaluator Madaan et al. (2023). Once an action is approved and sent back to the environment, the agent receives an observation and determines whether the subtask requires refinement. After completing an episode, the agent generates learning insights Majumder et al. (2023), which are stored in the Memory for future attempts. A key aspect is that the Planner not only breaks down tasks but also dynamically distils relevant insights from previous attempts to enhance the current task trace. This iterative process maximizes the efficiency of the memory system, ensuring continuous learning and adaptation. We evaluate STEP within ScienceWorld Wang et al. (2022) - a dynamic, text-based environment designed to simulate complex scientific tasks. Our results demonstrate that STEP consistently outperforms state-of-the-art (SOTA) models, achieving an overall score of 67.4. The model successfully completes 12 out of 18 tasks, ranking first in 11 tasks."
https://arxiv.org/html/2411.08424v1,A Heterogeneous Graph Neural Network Fusing Functional and Structural Connectivity for MCI Diagnosis,"Brain connectivity alternations associated with brain disorders have been widely reported in resting-state functional imaging (rs-fMRI) and diffusion tensor imaging (DTI). While many dual-modal fusion methods based on graph neural networks (GNNs) have been proposed, they generally follow homogenous fusion ways ignoring rich heterogeneity of dual-modal information. To address this issue, we propose a novel method that integrates functional and structural connectivity based on heterogeneous graph neural networks (HGNNs) to better leverage the rich heterogeneity in dual-modal images. We firstly use blood oxygen level dependency and whiter matter structure information provided by rs-fMRI and DTI to establish homo-meta-path, capturing node relationships within the same modality. At the same time, we propose to establish hetero-meta-path based on structure-function coupling and brain community searching to capture relations among cross-modal nodes. Secondly, we further introduce a heterogeneous graph pooling strategy that automatically balances homo- and hetero-meta-path, effectively leveraging heterogeneous information and preventing feature confusion after pooling. Thirdly, based on the flexibility of heterogeneous graphs, we propose a heterogeneous graph data augmentation approach that can conveniently address the sample imbalance issue commonly seen in clinical diagnosis. We evaluate our method on ADNI-3 dataset for mild cognitive impairment (MCI) diagnosis. Experimental results indicate the proposed method is effective and superior to other algorithms, with a mean classification accuracy of 93.3%.","\IEEEPARstart Magnetic Resonance Imaging (MRI) has emerged as a valuable tool in neuroscience, offering deeper objective insights into neurological disorders and their underlying mechanisms. Previous researches have demonstrated the valuable contributions of resting-state functional MRI (rs-fMRI) and Diffusion Tensor Imaging (DTI) among multiple MRI modalities in understanding brain structure and function [1, 2]. Specifically, brain functional connectivity (FC) constructed from rs-fMRI imaging can capture spontaneous neuronal activity and reveal intrinsic connections between brain regions [3, 4] while brain structural connectivity (SC) constructed from DTI imaging is able to provide crucial insights into the integrity of white matter structures and the identification of neural fiber abnormalities [5, 6]. In recent years, many studies find that alterations in neuronal functioning are directly related to alterations in white matter structure [7, 8], which has given rise to a lot of work on dual-modal fusion of FC and SC. Since FC and SC can be conveniently described by graphs, framework based on graph neural networks (GNNs) have become a popular choice to identify brain disorders combining dual-modal information [9, 10, 11, 12]. There are mainly two popular ways to fuse dual-modal information with GNNs, which are feature-level fusion and connectivity-level fusion. Specifically, in feature-level fusion, the same backbone is applied to different modalities to extract functional and structural features separately, then features are concatenated or weighted summed together as the fused feature for further analysis. While in connectivity-level fusion, usually a summative homogeneous graph (i.e., graph that have only one type of node and one type of edge) will be constructed based on fused structural-functional connectivity, and the fused structure-function features will be extracted from the summative graph. While these approaches are able to fuse dual-modal features, there is more information of connectivity yet to be explored. Firstly, there is no feature interaction in the above feature fusion methods, which leads to insufficient feature learning and reduced classification performance. Although connectivity-level fusion provides more consistent feature embedding than feature-level fusion, the summative graphs constructed in homogeneous way may disrupt the inherent heterogeneity between FC and SC, such as differences in feature spaces and graph topologies. These problems urge a new dual-modal fusion approach to better synthesize the information from FC and SC to identity brain disorders. Heterogeneous graph (HG) provides new ways of describing the complex connectivity in real world [13], which drives the development of heterogeneous graph neural networks (HGNNs) in fields such as social network analysis and bioinformatics [13, 14]. Considering that integrating FC and SC into HG can effectively preserve the heterogeneous information in the two modalities, we pursue a new dual-modal fusion method based on HG in present work. There are several challenges needed to be addressed yet. Specifically, i) different types of relations in HG are usually defined through meta-path which is semantic dependent [15, 16], meaning that meta-path needs to follow the inherent connectivity within the modality as well as to reveal interactive connectivity between modalities. ii) Pooling strategies for HGs need to cope with more complex relations, and directly applying pooling strategies designed for homogeneous graphs [17, 18] can lead to feature confusion among different types of nodes in HGs. iii) Differences in the incidence of various brain diseases lead to sample imbalance, which in turn affects the classification performance of graph networks. Therefore, we propose several effective mechanisms to address these challenges. Firstly, in constructing HG, in order to capture node relationships within rs-fMRI or DTI modality and relationships among cross-modal node pairs, we propose to define homo-meta-path and hetero-meta-path. Based on existing rs-fMRI and DTI studies, we naturally utilize blood oxygen level dependency information to construct FC and white matter fibers to construct SC, which serve as homo-meta-paths. On the other hand, we propose to establish hetero-meta-path from node-level and community-level based on structure-functional coupling [19] and brain community searching [20] to capture cross-modal relationships. Secondly, as existing fusion methods based on GNNs are not capable for HGs, we introduce a novel HG pooling strategy that not only comprehensively considers heterogeneous topology but also can avoid feature confusion among different types of nodes. Thirdly, to address the common issue of sample imbalance in brain disorder datasets, we propose a novel HG augmentation method leveraging the adaptability of HG. The main contributions of our present work can be summarized as follows: 1) We propose a novel HGNN to fuse rs-fMRI and DTI information for MCI diagnosis. In constructing HG, we propose to establish homo-meta-path reflecting unimodal connectivity and hetero-meta-path reflecting dual-modal inter-relations, where structure-function coupling and brain community searching are used in establishing hetero-meta-path. 2) We introduce a novel HG pooling strategy which can automatically balance homo- and hetero-meta-path, effectively leveraging heterogeneous information and preventing feature confusion after pooling. 3) We propose an HG augmentation method leveraging the adaptability of HG to address the issue of sample imbalance, which is a common factor that affects the performance of diagnostic model in classification of brain disorders. The proposed method is validated using a mild cognitive impairment (MCI) dataset sampled from ADNI-3 dataset. Experimental results indicate that our method can achieve remarkable performance for MCI identification. The structure of this paper is organized as follows. Section 2 introduces the most relevant concepts. In Section 3, we introduce materials used in this work. Section 4 introduces details of proposed method. In Section 5, we introduce experimental settings, state-of-the-art methods and present experimental results. Section 6 discusses the influence of proposed key mechanisms in our method. We conclude this letter in Section 7."
https://arxiv.org/html/2411.08414v1,Material Property Prediction with Element Attribute Knowledge Graphs and Multimodal Representation Learning,"Machine learning has become a crucial tool for predicting the properties of crystalline materials. However, existing methods primarily represent material information by constructing multi-edge graphs of crystal structures, often overlooking the chemical and physical properties of elements (such as atomic radius, electronegativity, melting point, and ionization energy), which have a significant impact on material performance. To address this limitation, we first constructed an element property knowledge graph and utilized an embedding model to encode the element attributes within the knowledge graph. Furthermore, we propose a multimodal fusion framework, ESNet, which integrates element property features with crystal structure features to generate joint multimodal representations. This provides a more comprehensive perspective for predicting the performance of crystalline materials, enabling the model to consider both microstructural composition and chemical characteristics of the materials. We conducted experiments on the Materials Project benchmark dataset, which showed leading performance in the bandgap prediction task and achieved results on a par with existing benchmarks in the formation energy prediction task.","The prediction of material properties is an important aspect of materials engineering applications, such as the discovery of novel materialsHamilton et al. (2024) with specific properties and the assessment of the reliability of materials in useZhao et al. (2024). Computational methods based on quantum mechanics (e.g. density functional theory, DFT) play a key role in predicting the physical and chemical properties of materials, but the high computational complexity, high cost and long computation time of such methods severely limit their applicability to large-scale materials systems. In recent years, machine learning methods have been adopted by an increasing number of research institutes due to their potential to efficiently and accurately predict material properties, opening up new avenues for rapid screening and optimisation of materialsMueller et al. (2016); Kong et al. (2021); Zhang et al. (2023); Hwang et al. (2023); Banik et al. (2024). In these machine learning models, the characterisation of the material’s crystal structure is crucial. The crystal structure of a material is usually modelled by the smallest cell containing all the constituent atoms in different coordinates, repeated infinitely many times in 3D space on a regular lattice, making the material structure periodic in nature. By modelling the material crystal structure with Graph Neural Networks (GNN), representations based on geometrical structural information have been constructed to enable prediction of material propertiesXie and Grossman (2018b); Schütt et al. (2017); Choudhary and DeCost (2021b); Yan et al. (2024b); Xie and Grossman (2018a); Chen et al. (2019); Louis et al. (2020); Choudhary and DeCost (2021a); Choudhary K (2021); Isayev et al. (2017); Yan et al. (2024a). These research methods have introduced various strategies such as the introduction of geometrical features such as multi-scale information, symmetry features, bond lengths and bond angles to improve the prediction accuracy of various crystalline properties. Das K et al.Das et al. (2023) fused the multimodal features of the crystal structure and the textual representation of the structure to capture both the local domain features in the crystal structure and the global chemical features in the textual representation for the prediction of crystalline material properties. Although the method introduces multimodal data, the quality and consistency of the textual description cannot be guaranteed and must be generated using RobocrystallographerGanose and Jain (2019), making the data pre-processing process complex and dependent. Although these studies can effectively model crystal structures and their inherent properties, existing methods are purely data-driven, focusing on exploring the intrinsic topological structure and structural rules of crystal structures without incorporating any chemical prior knowledge. The lack of this crucial elemental-level information often leads to inaccurate performance predictions when these models are applied to complex material systems containing different types of elements with significant variations in their properties. In this study, we propose a multimodal fusion framework, ESNet, to enhance the accuracy of crystalline material property predictions by integrating element attributes with crystal structure features. First, we construct an element attribute knowledge graph that systematically captures the chemical and physical properties of elements (such as electronegativity, atomic energy, and modulus). Using an embedding model to encode this knowledge graph, we reveal deeper relationships among various elements, yielding a rich representation of elemental features. Then, ESNet jointly learns both element attribute and crystal structure features, allowing the model to represent material microcomposition and chemical characteristics from a more comprehensive perspective. For the extraction of crystal structure features, we reference the ComFormerYan et al. (2024a) approach. Extensive experiments on the Materials Project benchmark dataset validate the effectiveness and superiority of ESNet in predicting key material properties such as band gap and formation energy. Results indicate that the multimodal representation combining element attributes and crystal structure significantly improves model performance, offering a novel approach to crystalline material property prediction."
https://arxiv.org/html/2411.08400v1,BAMAX: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning,"Autonomous robots collaboratively exploring an unknown environment is still an open problem. The problem has its roots in coordination among non-stationary agents, each with only a partial view of information. The problem is compounded when the multiple robots must completely explore the environment. In this paper, we introduce Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX), a method for collaborative exploration in multi-agent systems which attempts to explore an entire virtual environment. As in the name, BAMAX leverages backtrack assistance to enhance the performance of agents in exploration tasks. To evaluate BAMAX against traditional approaches, we present the results of experiments conducted across multiple hexagonal shaped grids sizes, ranging from 10x10 to 60x60. The results demonstrate that BAMAX outperforms other methods in terms of faster coverage and less backtracking across these environments.","Autonomous exploration by robots in unknown environments has diverse applications such as search and rescue, environmental monitoring, and disaster management, and is still an open challenge [7]. Moreover, individual robots often struggle with limitations in coverage, efficiency, reliability, resiliency, and adaptability when operating in complex and dynamic environments. To overcome these challenges, multi-agent collaborative systems have gained attention [19]. By leveraging collective knowledge and coordinating actions, these multiple agents can explore the environment more effectively, leading to improved coverage, robustness, and information exchange [17, 3]. However, collaborative strategies may encounter challenges such as navigating local extrema or overcoming dead ends [14]. In this paper we present an approach called Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX, for short). Multi-agent reinforcement learning is chosen as the foundation for our approach due to its capability to enable agents to learn optimal behaviors through interactions with the environment, leveraging rewards and penalties to enhance decision-making abilities. Our method allows multiple robots to autonomously explore and construct maps within hexagonal mazes, incorporating the crucial capability to backtrack to previously known open positions when encountering obstacles. This paper has two main contributions as follows. The first one is the guarantee of full exploration. Our approach leverages the collective abilities of multiple robots to facilitate efficient navigation, overcome walls, and achieve complete coverage of the entire grid. The second contribution is an ability to scale to multiple sizes of hexagonal grids. In the rest of the paper, we will discuss how these contributions are made using the BAMAX method. The section 2 provides an overview of the related works, covering both traditional methods and the application of intelligent agent navigation with Deep Reinforcement Learning (DRL). In section 3, we present our approach, detailing the creation of the environment that addresses our specific problem statement, and the setup of the Reinforcement Learning (RL) framework. Additionally, this section delves into the specifics of our proposed algorithm and the underlying network architecture. The section 4 focuses on the experimentation conducted to evaluate the performance of our method in comparison to other traditional approaches. Thereafter, the section discusses the results obtained from our experiments, offering analysis and comparison of our method with alternative approaches. Finally, the section 5 concludes the paper."
https://arxiv.org/html/2411.08378v1,Physics Informed Distillation for Diffusion Models,"Diffusion models have recently emerged as a potent tool in generative modeling. However, their inherent iterative nature often results in sluggish image generation due to the requirement for multiple model evaluations. Recent progress has unveiled the intrinsic link between diffusion models and Probability Flow Ordinary Differential Equations (ODEs), thus enabling us to conceptualize diffusion models as ODE systems. Simultaneously, Physics Informed Neural Networks (PINNs) have substantiated their effectiveness in solving intricate differential equations through implicit modeling of their solutions. Building upon these foundational insights, we introduce Physics Informed Distillation (PID), which employs a student model to represent the solution of the ODE system corresponding to the teacher diffusion model, akin to the principles employed in PINNs. Through experiments on CIFAR 10 and ImageNet 64x64, we observe that PID achieves performance comparable to recent distillation methods. Notably, it demonstrates predictable trends concerning method-specific hyperparameters and eliminates the need for synthetic dataset generation during the distillation process. Both of which contribute to its easy-to-use nature as a distillation approach for Diffusion Models. Our code and pre-trained checkpoint are publicly available at: https://github.com/pantheon5100/pid_diffusion.git.††footnotetext: * These authors contributed equally to this work and are listed in alphabetical order.††footnotetext: {\dagger} Corresponding Author.","Diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2021b; Ho et al., 2020) have demonstrated remarkable performance in various tasks, including image synthesis (Dhariwal & Nichol, 2021; Nichol et al., 2022; Ramesh et al., 2022; Saharia et al., 2022a), semantic segmentation (Baranchuk et al., 2022; Wolleb et al., 2022; Kirillov et al., 2023), and image restoration (Saharia et al., 2022b; Whang et al., 2022; Li et al., 2022; Niu et al., 2023). With a more stable training process, it has achieved better generation results that outperform other generative models, such as GAN (Goodfellow et al., 2020), VAE (Kingma & Welling, 2013), and normalizing flows (Kingma & Dhariwal, 2018). The success of diffusion models can mainly be attributed to their iterative sampling process which progressively removes noise from a randomly sampled Gaussian noise. However, this iterative refinement process comes with the huge drawback of low sampling speed, which strongly limits its real-time applications (Salimans & Ho, 2022; Song et al., 2023). Recently, Song et al. (2021b) and Karras et al. (2022) have proposed viewing diffusion models from a continuous time perspective. In this view, the forward process that takes the distribution of images to the Gaussian distribution can be viewed as a stochastic differential equation (SDE). On the other hand, diffusion models learn the associated backward SDE through score matching. Interestingly, Song et al. (2021b) demonstrate that diffusion models can also be used to model a probability flow ODE system that is equivalent in distribution to the marginal distributions of the SDE. In addition, Physics Informed Neural Networks (PINNs) have proven effective in solving complex differential equations (Raissi et al., 2019; Cuomo et al., 2022) by learning the underlying dynamics and relationships encoded in the equations. Building upon these developments, we propose a distillation method for diffusion models called Physics Informed Distillation (PID), a method that takes a PINNs-like approach to distill a single-step diffusion model. Our method trains a model to predict the trajectory at any point in time given the initial condition relying solely on the ODE system. During training, we view the teacher diffusion model as an ODE system to be solved by the student model in a physics-informed fashion. In this framework, the student model approximates the ODE trajectories, as illustrated in Figure 1, without explicitly observing the images in the trajectory. In detail, our contributions can be summarized as follows: • We propose and analyze Physics Informed Distillation (PID), a knowledge distillation technique heavily inspired by PINNs that enables single-step image generation, providing theoretical bounds for the method. • Through experiments on CIFAR-10 and ImageNet 64x64, we showcase our approaches’ effectiveness in generating high-quality images with only a single forward pass. • We demonstrate that similar to PINNs where the performance improvements saturate at a sufficiently large number of collocation points, our approach with a high enough discretization number performs best, showcasing its potential as a knowledge distillation approach that does not need additional tuning of method specific hyperparameters. Figure 1: An overview of the proposed method, which involves training a model \mathbf{x}_{\theta}(\mathbf{z},\cdot) to approximate the true trajectory \mathbf{x}(\mathbf{z},\cdot)."
https://arxiv.org/html/2411.08367v1,Surprisingly Popular Voting for Concentric Rank-Order Models,"An important problem on social information sites is the recovery of ground truth from individual reports when the experts are in the minority. The wisdom of the crowd, i.e. the collective opinion of a group of individuals fails in such a scenario. However, the surprisingly popular (SP) algorithm [prelec2017solution] can recover the ground truth even when the experts are in the minority, by asking the individuals to report additional prediction reports–their beliefs about the reports of others. Several recent works have extended the surprisingly popular algorithm to an equivalent voting rule (SP-voting) to recover the ground truth ranking over a set of m alternatives. However, we are yet to fully understand when SP-voting can recover the ground truth ranking, and if so, how many samples (votes and predictions) it needs. We answer this question by proposing two rank-order models and analyzing the sample complexity of SP-voting under these models. In particular, we propose concentric mixtures of Mallows and Plackett-Luce models with G(\geq 2) groups. Our models generalize previously proposed concentric mixtures of Mallows models with 2 groups, and we highlight the importance of G>2 groups by identifying three distinct groups (expert, intermediate, and non-expert) from existing datasets. Next, we provide conditions on the parameters of the underlying models so that SP-voting can recover ground-truth rankings with high probability, and also derive sample complexities under the same. We complement the theoretical results by evaluating SP-voting on simulated and real datasets.","The recovery of ground truth from individual reports is one of the most vital aspects of social information sharing and online discourse. The wisdom of the crowds phenomenon refers to the observation that the collective value of a group of noisy individual opinions can be used to recover the ground truth [galton1949vox]. Such a collective value cancels out the biases of individual opinions when the number of participants is large and is often deployed to recover the ground truth on online polling and Q&A platforms (e.g. Reddit). However, when the experts are in the minority, approaches that rely on the collective opinion of a group of individuals fail to recover the ground truth. The Surprisingly Popular (SP) algorithm [prelec2017solution] is a promising technique capable of recovering the ground truth even when experts are in the minority. In addition to asking individuals’ opinion (aka vote), it asks them to predict how they believe the majority’s answer is (aka prediction). The SP algorithm then picks the outcome which is surprisingly popular i.e. whose actual frequency in the votes is greater than its average predicted frequency. It provably recovers the ground truth as the number of individuals grows, even with a minority of experts. This approach has been extended to voting rules, called SP-voting, in order to recover the ground truth rankings over a set of m alternatives. The naive application of SP-algorithm to voting requires that individuals submit their prediction as a distribution over m! possible permutation of alternatives, which implies that the amount of information elicited from each voter is exponential in m. Surprisingly, SP-voting has been shown to effectively recover the ground truth in practice even when predictions are limited to a set of size m, providing a substantial improvement over classical voting rules by focusing on eliciting the most likely top alternative or ranking [hosseini2021surprisingly]. Furthermore, SP-voting has been extended to partial ranks where the voters provide reports (votes and predictions) over subsets of size k with k\ll m [hosseini2024surprising]. While SP-voting has been shown to be effective in full or partial rankings, we are yet to fully understand when SP-voting can recover the ground truth ranking, and if so, how many samples (votes and predictions) it needs. To the best of our knowledge, this question is unexplored even for the basic SP algorithm. The main difficulty of analyzing such algorithms is that they are non-parametric i.e. they don’t make any assumptions about the underlying distribution of votes and predictions, and it’s not immediately clear what type of parametric models would be a good fit for real-world datasets and are also amenable to analysis under the surprisingly popular framework. For the setting of partial rankings, \citeauthor*hosseini2024surprising [hosseini2024surprising] performed a preliminary analysis of SP-voting under a mixture of Mallows model with two groups. However, we observe that the real datasets need more than two groups and more general rank-order models. Thus, we ask the following questions: What general rank-order models can explain ranking datasets (both votes and predictions) with a ground truth ranking? Furthermore, can we analyze SP-voting under such rank-order models, and determine its sample complexity, and conditions for identifying the ground truth ranking? 1.1 Our Contributions We propose various rank-order models with a ground truth ranking, and analyse the SP-voting rule under these models. In particular, our contributions are the following. • We propose two rank-order models, the Concentric Mixture of Mallows and the Concentric Mixture of Plackett-Luce, and generalize them to accommodate populations of G\geq 2 groups. • We derive the conditions required for the identification of ground truth ranking under the SP-voting and the proposed concentric rank-order models. The derived conditions highlight a tension between the fraction of different groups and the ""expertise"" (i.e. noise levels) of different groups. • To evaluate practical viability, we fit these models to real-world datasets for populations with G=2 and G=3 groups. When G=3, besides the expert and non-expert groups, we identify an intermediate group of voters of large fraction that explains the observed datasets better than prior approaches with two groups. • Furthermore, we generate synthetic data based on these models and provide empirical results on the sample complexity of SP-Voting, comparing it against the Copeland rule. Finally, experiments on real-world datasets show that SP-voting performs significantly better than the Copeland voting rule even when the dataset size is small. 1.2 Related Work The challenge of ground truth recovery using the wisdom of the crowd has been extensively explored in social choice theory \parencitegalton1949vox, de2014essai, surowiecki2005wisdom. Several vote aggregation rules \parencitede2014essai, borda1781m, copeland1951reasonable, young1977extending have been proposed based on this concept to aggregate voters’ preferences and recover the underlying ground truth. However, this approach falters when the majority of participants are misinformed \parencitesimmons2011intuitive, biased \parencitechen2004eliminating, or when expert opinions are underrepresented within the population \parenciteprelec2017solution. To address this limitation, \citeauthor*prelec2017solution [prelec2017solution] introduced the Surprisingly Popular (SP) algorithm, which requires voters to provide two types of information: their individual vote and their prediction of the consensus vote. This framework has since been used to incentivize truthful behaviour in agents [schoenebeck2021wisdom, schoenebeck2023two], mitigate biases in academic peer review [lu2024calibrating], elicit expert knowledge [kong2018eliciting], forecast geopolitical events [debmalya2020effectiveness], and aggregate information [chen2023wisdom]. However, \citeauthor*prelec2017solution [prelec2017solution]’s SP algorithm becomes impractical when the objective is to recover true ordinal ranking, since it necessitates information across all m! possible vote configurations. The surprisingly popular algorithm was extended to recover full rankings while reducing its complexity to \binom{m}{2} votes, making it more practical for smaller values of m \parencitehosseini2021surprisingly. Further extending this line of work, SP-Voting has been generalized to handle any number of alternatives, while also introducing mechanisms for partial preference elicitation to improve the efficiency of ground truth recovery \parencitehosseini2024surprising. However, it is still unclear under what conditions SP-Voting is effective for a large number of alternatives when eliciting rankings. Specifically, the structure of the voting population and whether their voting behavior can be mathematically modeled need to be studied in detail. The modeling of ranked data can be approached from two perspectives: modeling the population of voters and modeling the ranking process itself \parencitemarden1996analyzing. To date, the SP-Voting framework has been examined primarily by classifying voters into two distinct groups. Our work extends this analysis by generalizing it to account for any number of groups, denoted as G. In terms of modeling the ranking process, several probabilistic models have been developed to represent voter preference generation. These include Order Statistic models, such as the Thurstonian model \parencitethurstone2017law; Pairwise Comparison models, like the Bradley-Terry model \parencitebradley1952rank; Multistage models, such as the Plackett-Luce model \parenciteluce1959possible, plackett1954reduction; and Distance-based models, like the Mallows’ model \parencitemallows1957non, among others. \citeauthor*marden1996analyzing [marden1996analyzing] provides a more comprehensive review of these models. The SP-Voting framework was recently studied under the assumption that voters’ preferences are drawn from an underlying probability distribution known as the Concentric Mixture of Mallows model, a variant of Mallows’ model \parencitehosseini2024surprising. In this work, we extend the SP-Voting framework by investigating two different vote distribution assumptions: the distance-based Mallows’ model and the multistage Plackett-Luce model. Specifically, we build on prior work by extending the Mallows’ model to account for G groups, allowing for a more general analysis of voter populations. Additionally, we propose a novel Concentric Plackett-Luce Mixture model, a variant of the multistage Plackett-Luce model, which similarly incorporates G groups."
https://arxiv.org/html/2411.08341v1,"Generative AI for Data Augmentation in Wireless Networks: Analysis, Applications, and Case Study","Data augmentation is a powerful technique to mitigate data scarcity. However, owing to fundamental differences in wireless data structures, traditional data augmentation techniques may not be suitable for wireless data. Fortunately, Generative Artificial Intelligence (GenAI) can be an effective alternative to wireless data augmentation due to its excellent data generation capability. This article systemically explores the potential and effectiveness of GenAI-driven data augmentation in wireless networks. We first briefly review data augmentation techniques, discuss their limitations in wireless networks, and introduce generative data augmentation, including reviewing GenAI models and their applications in data augmentation. We then explore the application prospects of GenAI-driven data augmentation in wireless networks from the physical, network, and application layers, which provides a GenAI-driven data augmentation architecture for each application. Subsequently, we propose a general generative diffusion model-based data augmentation framework for Wi-Fi gesture recognition, which uses transformer-based diffusion models to generate high-quality channel state information data. Furthermore, we develop residual neural network models for Wi-Fi gesture recognition to evaluate the role of augmented data and conduct a case study based on a real dataset. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we discuss research directions for generative data augmentation.","As an indispensable and fundamental technology, wireless networks enable users to access the Internet, connect devices, and communicate wirelessly. The growing complexity and diversity of wireless networks have facilitated the development of Deep Learning (DL)-based wireless communications and networking[1]. Since wireless networks generate large amounts of data from user activities, channel states, and network conditions, DL can efficiently distill high-level features and information from these data, which have complex structures and inner correlations[1], and enable intelligent applications such as Digital Twins (DTs), autonomous driving, and metaverse. Nevertheless, the availability of vast amounts of high-quality wireless data is a major determinant of the effectiveness of DL models[2]. Specifically, • Dynamic wireless channels: Due to multipath fading, shadowing, and interference, the environment in wireless channels is highly variable[3], which makes it difficult to collect high-quality wireless data. • Limited measuring equipment: High-quality wireless data collection often requires extensive deployment of specialized hardware and software equipment[3], which is costly and time-consuming. Therefore, obtaining high-quality and diverse wireless data for the progress of DL-based wireless communications and networking is challenging[2, 3]. Data augmentation is an effective technique to solve the problem of limited labeled datasets in model training[4]. The core idea of data augmentation is to synthetically increase the size of training datasets by modifying existing data, thus enhancing the robustness and generalization of learning algorithms[4]. Data augmentation techniques have been extensively applied in fundamental domains. For example, in Computer Vision (CV), data augmentation methods apply geometric or color transformations, such as cropping, flipping, and color channel changes[5]. However, traditional data augmentation methods may not be applicable to wireless data. Specifically, traditional data augmentation methods are carefully designed for certain domains and only perform simple transformations on the original datasets, rather than enriching data features from existing data[4]. Most importantly, traditional data augmentation methods lack consideration of the inherent characteristics and structures of wireless data[5], which cannot guarantee the quality and diversity of augmented wireless data. Hence, novel techniques for effective wireless data augmentation are urgently needed. Generative Artificial Intelligence (GenAI) is obtaining the full spotlight after the release of Large Language Model (LLM)-based chatbots by tech giants such as OpenAI’s ChatGPT, Google’s Bard, and Microsoft’s Bing Chat[6]. Unlike discriminative AI models that focus primarily on explicitly learning decision boundaries between classes, GenAI models excel at learning the underlying distribution, patterns, and features of input data, thus generating new data instances that resemble the original dataset[7]. Due to its transformative power in data generation, GenAI has recently gained significant attention in the realm of wireless networks[6], where real-world wireless data is often scarce, incomplete, costly to acquire, and difficult to model or comprehend[2], enabling the emergence of GenAI-driven data augmentation. Specifically, GenAI models can generate high-quality and diverse wireless data as a supplement, and the synthetic data can be combined with real data to augment the training dataset[6], which solves the wireless data acquisition challenge and can effectively improve the performance of DL models in wireless communication and networks. There are currently some preliminary studies on using GenAI models for data augmentation in wireless networks[2, 4, 8, 3]. For instance, the authors in [2] utilized conditional latent diffusion models to generate high-diversity and high-quality Radio Frequency (RF) data at low costs. In [3], the authors leveraged a Denoising Diffusion Probabilistic Model (DDPM) to generate channel data in multiple speed scenarios, where the DDPM can capture the underlying distribution of wireless channel data with limited data volume. Finally, a CsiNet model was used to validate the effectiveness of the proposed approach. However, the above works do not provide a general tutorial on how to implement data augmentation using GenAI technology in wireless networks. To this end, this paper focuses on answering two questions, i.e., “Why GenAI is conducive to data augmentation in wireless networks” and “How to use GenAI techniques to achieve wireless data augmentation.” To the best of our knowledge, this is the first work that systemically explores the potential and effectiveness of generative data augmentation in wireless networks. Our main contributions are summarized as follows: • We begin with a brief review of data augmentation techniques in the basic domains of images, text, and graphics, then discuss potential advantages brought by effective data augmentation techniques in wireless networks, and finally present the limitations of traditional data augmentation techniques in wireless networks. • We review typical GenAI models and their applications in data augmentation, and summarize the benefits of generative data augmentation. We then explore the effectiveness of GenAI-driven data augmentation for wireless applications from the physical, network, and application layers, which presents a specific GenAI-driven data augmentation architecture for each application. • We propose a general Generative Diffusion Model (GDM)-based data augmentation framework for Wi-Fi gesture recognition, where we use transformer-based diffusion models to generate high-quality and diverse Channel State Information (CSI) data. We finally conduct a case study based on a real dataset, where we develop Residual neural Network (ResNet) models to evaluate the performance gains achieved by augmented CSI data. Simulation results demonstrate the effectiveness of the proposed framework. TABLE I: A Summary of Traditional Non-AI and GenAI Methods for Typical Data Augmentation."
https://arxiv.org/html/2411.08334v1,Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval,"Existing multimodal retrieval systems often rely on disjointed models for image comprehension, such as object detectors and caption generators, leading to cumbersome implementations and training processes. To overcome this limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a text retriever with the ability to understand multimodal queries via dynamic modality interaction. Ret-XKnow leverages a partial convolution mechanism to focus on visual information relevant to the given textual query, thereby enhancing multimodal query representations. To effectively learn multimodal interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset automatically constructed from visual dialogue datasets. Our dataset construction process ensures that the dialogues are transformed into suitable information retrieval tasks using a text retriever. We demonstrate that our approach not only significantly improves retrieval performance in zero-shot settings but also achieves substantial improvements in fine-tuning scenarios. Our code is publicly available: https://github.com/yeongjoonJu/Ret˙XKnow.","With the growing demand for information retrieval across diverse applications, such as internet search and knowledge-based question answering, precise and efficient retrieval from multimodal queries involving pairs of images and text has emerged as a critical challenge. In such multimodal queries, each modality independently provides insufficient information to retrieve the desired passages within a knowledge base, necessitating the integrated understanding of the visual and textual queries. Existing Vision-Language (VL) retrievers (Qu et al. 2021; Luo et al. 2021; Gao et al. 2022; Lin et al. 2023) often depend on disjointed models for object detection or image captioning to provide visual information. The reliance on disjointed models complicates the training process (e.g., the models should be fine-tuned for separate tasks in domain adaptation) and increases the likelihood of propagating erroneous predictions. The utilization of the captioning model also lacks the fine-grained information embedded within the images. Previous approaches (Lin et al. 2023; Luo et al. 2023) have attempted to address these drawbacks. However, as shown in Fig. 1, they result in lower performance in a zero-shot setting than a text retriever that does not use image information despite their pre-training for the image-text alignment. Lin et al. (2023) introduce token-level embeddings and utilize two types of visual representations: textual description of the image and feature-based visual embeddings with regions of interest by an object detector. They pre-train the retriever to map token-level visual embeddings into the linguistic space of a text retriever and then fine-tune it by adding image captions to the textual queries. Such the retriever captures fine-grained features of the image by employing visual embeddings with captions. They also facilitate modality interaction between the textual query and the image by relying on textual information, but the mechanism also results in complex implementations and inefficient retrieval due to multiple steps. Figure 1: Zero-shot retrieval performance on OK-VQA (Google Search). Ret-XKnow outperforms the text-based retriever, while other multimodal retrievers fall short, relying on the textual query in the pre-training stage. Luo et al. (2023) present an end-to-end approach that projects multimodal features encoded via self-attention into linguistic space with a pre-training task called VL-ICT, to detach the dependency on the disjointed modules. They automatically construct a pre-training dataset by applying the Inverse Cloze Task (ICT) (Lee, Chang, and Toutanova 2019) to a multimodal knowledge base. However, this approach has significant limitations. First, the dataset does not adequately reflect the variety and complexity of real-world queries, as it only removes the title or caption from a sentence extracted as the query without considering the image. Second, in the constructed pairs of a multimodal query and the corresponding passage, the passage can often be matched solely with the textual content of the query. This occurs because the target passage is selected from the content following a sentence with a title or caption, thereby hindering learning rich image representations. To tackle these issues, we propose two approaches: (1) an end-to-end Retriever to eXpand visual Knowledge, Ret-XKnow, and (2) a Visual Dialogue-to-Retrieval (ViD2R) pre-training dataset constructed from visual dialogues containing distinct relevant passages for various queries related to the same image. Ret-XKnow endows a text retriever with the understanding of multimodal queries in the context of efficient information retrieval, inspired by the concept of partial convolutions (Liu et al. 2018), which fill undesired pixels with surrounding pixel information. We compress visual embeddings to focus on the visual information relevant to the textual query by leveraging the relevance scores between visual embeddings and textual query representations as an adaptive mask. We only attach a vision encoder to the text retriever with only a few layers, utilizing output embeddings of the penultimate layer in the vision model for fine-grained visual representations. Our model architecture does not allow the direct intervention of textual query features in the pre-training stage, achieving modality interaction without fusing text features with image features. Through this architecture, we introduce both the late-interaction mechanism (Khattab and Zaharia 2020) for pre-indexing documents and the modality interaction without requiring an additional document encoder and disjointed models. Recent advances in multimodal language models have produced several multimodal dialogue datasets (Zhu et al. 2023; Liu et al. 2023; Wang et al. 2023; Huang et al. 2023) for training models to perform tasks based on visual content. These datasets consist of multi-turn sessions with query-response pairs centered around a single image, providing precise and comprehensive information pertinent to the query and image. The response with detailed information can improve multimodal retrieval tasks by linking image understanding with complex textual queries. Whereas, such datasets are not appropriate for directly training retrievers due to the gap between explicit responses and broader passages. To bridge this gap, we transform the visual dialogue datasets into a format suitable for retrieval tasks through three simple steps: pre-processing, neural filtering, and response-to-passage conversion. Our construction process is applicable in diverse domains and modalities since our approach only requires multimodal dialogue datasets and sets of documents related to the target domain. Our retriever, Ret-XKnow pre-trained with the ViD2R dataset, outperforms various baselines in zero-shot retrieval performance across four multimodal datasets in an end-to-end manner. Furthermore, we demonstrate that the pre-training dataset curated via our construction method effectively mitigates the issue of overlooking visual features during the pre-training stage, leading to remarkable performance in fine-tuning settings. Our contributions are summarized as follows: • We propose Ret-XKnow, an end-to-end multimodal retriever that overcomes the limitations of disjointed models by dynamically focusing on visual features relevant to the textual query. • We introduce the ViD2R dataset, which transforms visual dialogue datasets into a format suitable for training VL retrievers, leading to significant improvements in zero-shot retrieval performance. • We demonstrate the comprehensive adaptability of Ret-XKnow by fine-tuning three downstream tasks. Our end-to-end retriever even shows comparable performance on baseline methods utilizing image captioning."
https://arxiv.org/html/2411.08324v1,"Are LLMs Prescient? A Continuous Evaluation 
using Daily News as the Oracle","Many existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to the emergence of new models and training data. These benchmarks also fall short in assessing how LLM performance changes over time, as they consist of static questions without a temporal dimension. To address these limitations, we propose using future event prediction as a continuous evaluation method to assess LLMs’ temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict “future” event outcomes. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy, the performance degradation pattern persists, highlighting the need for continuous model updates.","Traditional Large Language Model (LLM) benchmarks are often static, and do not reflect real-world information that evolves over time. This presents two significant challenges. First, as LLMs are updated, there is a risk that the static benchmarks become outdated and more vulnerable to data leakage, where their content might end up in the training data of newer models. This undermines the reliability of performance assessments on these benchmarks (Sainz et al.,, 2023; Xu et al.,, 2024; McIntosh et al.,, 2024; Li and Flanigan,, 2024). Second, the static benchmarks often lack the temporal information to track the model’s performance variations over time (McIntosh et al.,, 2024). This creates a need for evaluation methods that stay relevant over time and incorporate temporal dynamics. Daily news provides a natural setting for continuous evaluation of LLMs. Since the world is constantly changing, a benchmark designed around forecasting the next day’s news will never be out of date by construction. In addition to enabling continuous evaluation, forecasting is itself a longstanding challenge with significant implications across various domains, including healthcare, finance, and policymaking (Tetlock and Gardner,, 2016; Dempsey et al.,, 2017; Gillingham et al.,, 2018; Lopez-Lira and Tang,, 2023). While human experts have traditionally made such forecasts, machine learning models, particularly LLMs, have emerged as promising alternatives due to their capability to learn from vast and diverse corpora (Halawi et al.,, 2024; Ye et al.,, 2024; Yan et al.,, 2023). Several recent forecasting question-answer (QA) datasets have been developed (Jin et al.,, 2021; Zou et al.,, 2022; Zhang et al.,, 2024), however, they are limited in either size, scope, or do not continuously keep pace with the rapidly changing world. More critically, the extent to which LLMs’ predictive abilities change over time remains underexplored. In this work, we propose Daily Oracle—a continuous evaluation benchmark that uses automatically generated QA pairs from daily news to assess how the future prediction capabilities of LLMs evolve over time. The QA pairs are generated on a daily basis, consisting of True/False (TF) and Multiple Choice (MC) questions across various categories such as business, politics, and arts. Unlike traditional reading comprehension tasks, these QA pairs are designed to challenge LLMs to predict future events based on their own existing knowledge, effectively evaluating their temporal generalization and forecasting abilities. We continuously evaluate various LLMs, both with and without access to a limited archive of news articles. Our experiments reveal that LLMs experience an average performance decline of 20.14% on True/False (TF) questions and 23.26% on Multiple Choice (MC) questions between January 2020 and September 2024, with degradation becoming more pronounced before and after the models’ knowledge cutoff dates. Although models utilizing Retrieval Augmented Generation (RAG) (Lewis et al.,, 2020) can demonstrate improved prediction performance, the downward trend persists, suggesting an ongoing challenge in maintaining temporal generalization. Overall, our benchmark highlights the challenges posed by outdated pertaining data in LLMs, and underscores the necessity for continuous model updating to keep up with the constantly evolving stream of real-world information. To summarize, our key contributions are two-fold: • Continuous Forecasting Evaluation Benchmark: We present Daily Oracle, the largest and most up-to-date forecasting dataset, composed of automatically generated QA pairs. This benchmark continuously evaluates LLMs’ temporal generalization and future prediction abilities using daily news, ensuring relevance over time and offering a challenging evaluation framework. • Empirical Findings on Performance Degradation: Our experiments demonstrate a consistent decline in LLM prediction accuracy over time, even when supplemented with recent “open-book” information. The persistent degradation highlights the challenges posed by outdated pretraining data in LLMs and underscores the necessity for continuous model updating."
https://arxiv.org/html/2411.08302v1,R3HF: Reward Redistribution for Enhancing Reinforcement Learning from Human Feedback,"Reinforcement learning from human feedback (RLHF) provides a paradigm for aligning large language models (LLMs) with human preferences. This involves the initial training of a reward model based on pairwise human feedback. The reward model is subsequently utilized in reinforcement learning to assess the scores of each generated sentence as a whole, further guiding the optimization of LLMs. However, current approaches have a significant shortcoming: They allocate a single, sparse, and delayed reward to an entire sequence of output. This may overlook some significant individual contributions of each token towards the desired outcome. To overcome this limitation, our paper proposes a novel reward redistribution method called R3HF, which facilitates a more fine-grained, token-level reward allocation. Specifically, our method treats the reward prediction task of the reward model as a regression problem. As a result, the redistributed rewards are computed by evaluating the specific contribution of each token to the reward model’s output. This detailed approach improves the model’s understanding of language nuances, leading to more precise enhancements in its performance. Our method is crafted to integrate seamlessly with most current techniques while incurring minimal computational costs. Through comprehensive experiments across diverse datasets and tasks, we have verified the effectiveness and superiority of our approach111Warning: The Appendix contains example data that may be offensive or harmful..","Large language models (LLMs) have showcased remarkable adaptabilities across various tasks, with applications spanning fields like psychology [12], education [49, 19], and medical support [46, 27]. However, as LLMs become increasingly sophisticated, the complexity of their decision-making processes and outputs also escalates, introducing potential risks such as the propagation of bias [14, 48], generation of misinformation [24, 29], and potential harm [16, 15]. This underscores the critical need for effective alignment [35, 50, 26, 11] of LLMs. Such alignment aims to guide the models to better comprehend and prioritize human preferences, ensuring their operations are in tune with human values and ethics. Reinforcement learning from human feedback (RLHF) [10, 29, 5] is an advanced paradigm that incorporates human feedback into LLM training. This approach typically unfolds in three primary stages, which is shown in Figure 1 (left). The initial stage involves supervised fine-tuning (SFT) applied to the target domain. Subsequently, the second stage develops and trains a reward model on data that reflect human preferences. The final stage is dedicated to refining the language model using reinforcement learning algorithms with the learned reward model. Though RLHF technology has demonstrated its effectiveness in various scenarios, it also presents a significant drawback that hampers the training efficiency of the model. Traditional reward models typically assess the overall effectiveness of an entire generated sequence, assigning a score only after delivering the final token, with the other tokens receiving a score of zero. This reward structure, being both sparse and delayed, challenges the model in recognizing the impact of individual tokens. An intuitive example is illustrated in Figure 1(right). Consider a question-answering task with the prompt, “Was Walt Disney the original creator of Mickey Mouse? <end>” and the generated response, “Yes, Walter Elias Disney was indeed the original creator of Mickey Mouse.” The reward model assigns a positive evaluation score of 0.8. However, when treating the entire sentence as an episode, traditional methods only allocate a score of 0.8 to the “<end>” token, potentially hindering the efficient optimization of LLMs. Meanwhile, the initial tokens in a sequence can significantly influence the subsequent generation, a nuance that current methodologies often struggle to accommodate effectively. In the example, the word “Yes” is the most crucial token in the generated sequence that influences the overall score, yet it receives a reward of zero. This highlights the urgent need for methodologies that better recognize and reward the contribution of each token. Figure 1: Left: The training paradigm of reinforcement learning from human feedback typically encompasses three stages. Our proposed method is applied in the final stage, where we redistribute the holistic rewards at the terminal time-step to provide a fine-grained and immediate reward for each generated token. This approach aims to more effectively guide the optimization of Large Language Models (LLMs). Right: An example of reward redistribution, where the sum of the fine-grained rewards is equivalent to the original sparse reward. To address this shortcoming, in this paper, we introduce a novel approach: Reward Redistribution for enhancing Reinforcement learning from Human Feedback (R3HF). The foundational concept of our method involves assigning individual credits to each token within the generated sentences, thus providing a nuanced signal for optimization tailored to LLMs. As the example in Figure 1(right), since “Yes” is the most crucial token according to the reward model, it receives the highest reward signal. Similarly, other tokens receive different rewards, which can be either positive or negative. The sum of the rewards for all tokens in the sequence is equivalent to the original overall reward score. To achieve this goal, our approach is operationalized through the framework of Sequence-Markov Decision process (SDPs) [4], wherein the allocation of rewards is not constrained by the Markov property. Concretely, we conceptualize the reward model as a regression model, predicting each sequence-wide return from the entire state-action sequence. By adopting this perspective, we allocate to each token a portion of credit relative to its incremental impact on the reward model compared to the preceding time-step. The credits can be inferred through temporally differentiated computation, providing more granular guidance over the language generation process. Compared to state-of-the-art RLHF approaches, our R3HF offers the following advantages: (1) Learning Efficiency. By providing token-level rewards, our method significantly enhances learning by offering immediate and relevant information. This approach avoids the limitations of delayed rewards that may be less informative. Consequently, it facilitates more accurate fine-tuning of language models, leading to considerable advancements in language generation that are more closely aligned with human feedback. (2) Independence from Human Labeling. The redistributed rewards do not depend on extensive human labeling of data. The reward model itself dynamically assigns value to each token based on its contribution to the overall sequence, thus reducing the need for labor-intensive labeling efforts. This feature facilitates the rapid incorporation of feedback without being bottlenecked by the pace of human annotation, streamlining the training process. (3) Seamless Integration. Our method is designed for easy application across most mainstream RLHF paradigms, requiring only minimal modification that involves simple recomputation of rewards. This compatibility ensures that existing RLHF methods can be effortlessly enhanced with our token-level reward redistribution technique, boosting their effectiveness without necessitating extensive overhaul or complex re-engineering. To evaluate the efficacy of our approach, we have conducted a series of experiments across a diverse set of tasks, including summarization, question-answering, and harmfulness mitigation&helpfulness enhancement. Moreover, we have applied our reward distribution technique to various established RLHF methods. The empirical results from these experiments indicate that by training with such fine-grained feedback, our method consistently exhibits improved performance across all tested tasks."
https://arxiv.org/html/2411.08297v1,TowerDebias: A Novel Debiasing Method based on the Tower Property,"Decision-making processes have increasingly come to rely on sophisticated machine learning tools, raising concerns about the fairness of their predictions with respect to any sensitive groups. The widespread use of commercial black-box machine learning models necessitates careful consideration of their legal and ethical implications on consumers. In situations where users have access to these “black-box” models, a key question emerges: how can we mitigate or eliminate the influence of sensitive attributes, such as race or gender? We propose towerDebias (tDB), a novel approach designed to reduce the influence of sensitive variables in predictions made by black-box models. Using the Tower Property from probability theory, tDB aims to improve prediction fairness during the post-processing stage in a manner amenable to the Fairness-Utility Tradeoff. This method is highly flexible, requiring no prior knowledge of the original model’s internal structure, and can be extended to a range of different applications. We provide a formal improvement theorem for tDB and demonstrate its effectiveness in both regression and classification tasks, underscoring its impact on the fairness-utility tradeoff.","In recent years, the rapid development of machine learning algorithms and their extensive commercial applications have become increasingly relevant across domains like cybersecurity, healthcare, e-commerce, and more (Sarker, 2021). As these models increasingly guide critical decision-making processes with real-world consequences for consumers, a noteworthy concern has emerged: algorithmic fairness in machine learning (Wehner and Köchling, 2020; Chen, 2023). The primary objective of ensuring fairness in machine learning is to reduce the impact of sensitive attributes—such as race, gender, and age—on an algorithm’s predictions. A seminal case in the field of fair machine learning is the development of the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm by Northpointe, designed to assess a criminal’s likelihood of recidivism. The goal of this tool is to assist judges in making sentencing decisions. However, COMPAS came under intense scrutiny after an investigation by ProPublica, which revealed evidence of racial bias against black defendants compared to white defendants with similar profiles (Angwin et al., 2016). Northpointe contested these findings, asserting that their software treated Black and White defendants equally. In response, ProPublica issued a detailed rejoinder, making its statistical methods and findings publicly available (Angwin and Larson, 2016). While we do not take a position on the specific dispute, this case highlights the critical importance of addressing fairness in machine learning. A key issue with “black box” algorithms like COMPAS is that the purchaser cannot easily remove the sensitive variables (S), such as race, and rerun the algorithm. The source code of the algorithm is typically not accessible, and the training data is often unavailable as well. This raises a crucial question: how can we reduce or eliminate the influence of S in such cases? This paper aims to address this challenge. 1.1 The Setting We consider prediction of a variable Y from a feature vector X and a vector of sensitive variables S. The target Y may be either numeric (in a regression setting) or dichotomous (in a two-class classification setting where Y = 1 or Y = 0). The m-class case can be handled using m dichotomous variables with regards to the material presented here. Consider an algorithm developed by a vendor, such as COMPAS, which was trained on data (Y_{i},X_{i},S_{i}),i=1,\dots,n. This data can be assumed to originate from some specific data-generating process, with the algorithm’s initial objective being to estimate E(Y|X,S). However, the client who purchased the algorithm wishes to exclude S and instead estimate E(Y|X)111Quantities of the form E(), P(), Var() and so on refer to the probability distribution of the data generating process.. In the regression setting, we assume squared prediction error loss to minimize error. In the classification setting, we define: E(Y|X,S)=P(Y=1|X,S) where the predicted class label is given by: \operatorname*{arg\,max}_{i=0,1}P(Y=i|X,S) This approach minimizes overall misclassification rate by selecting the class with the highest predicted probability. Several potential use cases for employing tDB can be identified: (a) The client has no knowledge of the internal structure of the black-box algorithm and lacks access to the original training data. Thus, the client will need to gather their own data from new cases that arise during the tool’s initial use. (b) Client has no knowledge of the inner workings of the black-box algorithm but is given the training data. (c) User of the algorithm, potentially even the original developer, knows the black-box’s details and possesses the training data. He/she is satisfied with the performance of the algorithm, but desires a quick, simple way to remove the influence of S in its predictions. In each setting, the individual aims to predict new cases using only X. The client either does not know S or chooses to disregard it. In other words, although the algorithm provides estimates of E(Y|X,S), the goal is to use E(Y|X) instead as the prediction instead. In other words, even though the algorithm gives us an estimated E(Y|X,S), we wish to instead use estimated E(Y|X) as our prediction. In this paper, we present tDB as an innovative approach to modify the predictions of the original algorithm, bypassing the “black box” nature of the model. 1.2 Introducing the Fairness-Utility Tradeoff Many commercial black-box models may include sensitive variables, which raises significant ethical and legal concerns. In the pursuit of fairness, a fundamental Fairness-Utility tradeoff emerges: a delicate balance between fairness and predictive accuracy (Gu et al., 2022; Sun et al., 2023). This tradeoff highlights that prioritizing fairness often leads to a reduction in accuracy. However, the extent of this tradeoff is influenced by the specific fairness metrics employed and their implementation details. This paper explores the impact of applying tDB on the fairness-utility tradeoff across different datasets, encompassing both regression and classification tasks. 1.3 Paper Outline The paper is organized as follows: Section 2 reviews prior literature on fair machine learning, focusing on relevant methods and proposed fairness metrics; Section 3 introduces the towerDebias algorithm and provides supporting proofs demonstrating fairness improvements; Section 4 presents empirical results of tDB on multiple datasets; and Section 5 concludes with a discussion and future directions."
https://arxiv.org/html/2411.08286v1,Hashing for Protein Structure Similarity Search,"Protein structure similarity search (PSSS), which tries to search proteins with similar structures, plays a crucial role across diverse domains from drug design to protein function prediction and molecular evolution. Traditional alignment-based PSSS methods, which directly calculate alignment on the protein structures, are highly time-consuming with high memory cost. Recently, alignment-free methods, which represent protein structures as fixed-length real-valued vectors, are proposed for PSSS. Although these methods have lower time and memory cost than alignment-based methods, their time and memory cost is still too high for large-scale PSSS, and their accuracy is unsatisfactory. In this paper, we propose a novel method, called \underline{\text{p}}r\underline{\text{o}}tein \underline{\text{s}}tructure \underline{\text{h}}ashing (POSH), for PSSS. POSH learns a binary vector representation for each protein structure, which can dramatically reduce the time and memory cost for PSSS compared with real-valued vector representation based methods. Furthermore, in POSH we also propose expressive hand-crafted features and a structure encoder to well model both node and edge interactions in proteins. Experimental results on real datasets show that POSH can outperform other methods to achieve state-of-the-art accuracy. Furthermore, POSH achieves a memory saving of more than six times and speed improvement of more than four times, compared with other methods.","Proteins play essential roles in biological systems by binding with various ligands. Proteins with similar structures often share similar functions. Hence, protein structure similarity search (PSSS), which tries to search proteins with similar structures, plays a crucial role across diverse domains from drug design to protein function prediction and molecular evolution. Traditional PSSS methods, which are often called alignment-based methods, directly calculate alignment on the protein structures. Representative alignment-based methods include TM-align [1], MATT [2] and several others [3, 4, 5]. Since identifying an optimal alignment between a pair of structures is an NP-hard problem [6], alignment-based methods are typically highly time-consuming with high memory cost even if heuristic techniques are adopted in these methods. Taking TM-align as an example, aligning a protein structure with 200 amino acids against the SCOPe database [7] with 14323 protein structures takes approximately 30 minutes. Note that this is the time cost for only a single query. The memory cost for storing the SCOPe database is approximately 4GB. With the development of Cryo-EM and protein structure prediction techniques, such as Alphafold 2 [8], RoseTTAFold [9] and ESMFold [10], the number of proteins with known structures grows rapidly. For example, Alphafold 2 has predicted structures with high confidence for over 200 million proteins, which has been used to construct the Alphafold database [11]. On large-scale datasets like Alphafold database, alignment-based methods will have huge time and memory cost, even become infeasible. To reduce the cost for PSSS, alignment-free methods, which represent protein structures as fixed-length real-valued vectors, are proposed. Existing alignment-free methods can be categorized into non-learning methods and learning-based methods. Non-learning methods [12, 13] generate vector representations based on hand-crafted features. Learning-based methods [14, 15] generate vector representations by learning from data. PSSS is performed by calculating and ranking the similarity between the vector of the query and those of the database. With vector representation, alignment-free methods have lower time and memory cost than alignment-based methods. However, it is still time-consuming to calculate the similarity between real-valued vectors, and the memory cost for storing real-valued vectors is still too high for large-scale datasets. Furthermore, the accuracy of existing alignment-free methods is unsatisfactory because these methods have limitation in representing the complex protein structure. More specifically, the hand-crafted features in non-learning methods fail to model the complex and irregular three-dimensional protein shape. Existing learning-based methods suffer from the lack of expressive features and cannot model edge interactions in proteins. In this paper, we propose a novel method, called protein structure hashing (POSH), for PSSS. The main contributions of POSH are outlined as follows: • To the best of our knowledge, POSH is the first hashing method for PSSS. • POSH learns a binary vector (or called hash code) representation for each protein structure, which can dramatically reduce the time and memory cost for PSSS compared with real-valued vector representation based methods. • Furthermore, in POSH we also propose expressive hand-crafted features and a structure encoder to well model both node and edge interactions in proteins. • Experimental results on real datasets show that POSH can outperform other methods to achieve state-of-the-art accuracy. Furthermore, POSH achieves a memory saving of more than six times and speed improvement of more than four times, compared with other methods."
https://arxiv.org/html/2411.08278v2,Knowledge Bases in Support of Large Language Models for Processing Web News,"Large Language Models (LLMs) have received considerable interest in wide applications lately. During pre-training via massive datasets, such a model implicitly memorizes the factual knowledge of trained datasets in its hidden parameters. However, knowledge held implicitly in parameters often makes its use by downstream applications ineffective due to the lack of common-sense reasoning. In this article, we introduce a general framework that permits to build knowledge bases with an aid of LLMs, tailored for processing Web news. The framework applies a rule-based News Information Extractor (NewsIE) to news items for extracting their relational tuples, referred to as knowledge bases, which are then graph-convoluted with the implicit knowledge facts of news items obtained by LLMs, for their classification. It involves two lightweight components: 1) NewsIE: for extracting the structural information of every news item, in the form of relational tuples; 2) BERTGraph: for graph-convoluting the implicit knowledge facts with relational tuples extracted by NewsIE. We have evaluated our framework under different news-related datasets for news category classification, with promising experimental results.","Figure 1. Illustration of Knowledge Base (left) and Large Language Model (right), with the former typically storing structured knowledge explicitly and the latter holding unstructured knowledge implicitly. Large Language Models have garnered widespread enthusiasm lately as conversational agents for diverse applications. Noticeably, ChatGPT (Schulman et. al., 2022) takes queries and creates responses, Google’s Bard (Manyika, 2023) can produce creative replies due to its coding and multilingual capabilities, ChatSonic (Coombe, 2023) generates update-to-date replies with the aid of Google Search for accurate and informative content creation, among others. Meanwhile, a simple and yet powerful language representation model, known as Bidirectional Encoder Representations from Transformers (BERT)(Kenton and Toutanova, 2019), was introduced. As an LLM family member, BERT comprises a stack of transformer encoders and is pre-trained via unlabeled texts. Most LLMs are fine-turnable using small amounts of domain-specific data for improving the performance of the given domain tasks. In particular, the BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks (Kenton and Toutanova, 2019), such as question answering and language inference, with little changes to the models. Conventional LLMs usually take massive amounts of unstructured text data for pre-training, before made them available for general applications. Although a pre-trained LLM can produce replies to its inputs (such as Web news items), it does not inherently capture structural and relational information of tokens existing in its every input, presenting an opportunity for improving the quality of replies. To this end, we resort to knowledge bases (KBs), which are created by pre-processing Web news items individually to extract structural and relational information of token in each item, such as the triggers, the arguments, the temporal relations, etc. This KB creation is undertaken in the rule-based manner automatically without requiring any relation-specific training data. Pre-processing news items extracts a number of relational tuples (Arg1, Pred, Arg2) per item, realized by the News Information Extractor (NewsIE) we have developed. The KBs created out of news items make it possible to produce better replies to inputs (i.e., news items) that are fed to a given LLM for processing, by complementing the LLM outputs. This results from the fact that LLM outputs contain no structural information while the KBs hold structural and relational information of individual news items. In this article, we consider the framework that lets KBs support LLMs for improving Web news processing performance. The framework consists of an LLM and a graph convolutional network (GCN) (Defferrard et al., 2016), which is inputted with the LLM output and KBs for graph-convolutive operations to let KBs complement the LLM output. The GCN serve to convolute relational tuples with the implicit knowledge facts obtained by the LLM, which is fine-tuned by Web news items for improving the performance of news processing. The fine-tuned LLM takes raw Web news themselves directly as its input. While any LLM may be adopted to form the framework, we use BERT (Kenton and Toutanova, 2019) as an example LLM in this article, realized our BERTGraph. Without making any change to BERT for use in BERTGraph, our framework is in contrast to prior designs, like KG-BERT (Yao et al., 2019) and K-BERT (Liu et al., 2020), which change the BERT input format, and Relphormer (Bi et al., 2022), which modifies the BERT encoder structure. However, any change to the input format or the encoder structure of BERT destroys its original embedding, requiring prohibitively expensive LLM model re-training, making our BERTGraph more favorable in practice and able to fully utilize implicit knowledge learned by BERT. More details of the BERTGraph framework can be found in Subsection 3.1.1 For the BERT output and KBs to be compatible with the GCN input, a text-to-graph adapter is devised, as stated in Subsection 3.1.2. We have implemented BERTGraph for experimentally evaluating its news category classification performance under three publicly available Web news datasets, N24News (Wang et al., 2021a), SNOPES (Snopes, 2023), and politifact (Politifact, 2023). Our evaluation results demonstrate that BERTGraph outperforms its BERT counterpart for evaluated news datasets, in terms of all performance metrics but the precision of politifact, with 0.27 versus 0.28. The remainder of this paper is organized as follows. Section 2 presents related background and prior work. Section 3 overviews the proposed BERTGraph design, with its two key components detailed in sequence. Section 4 describes the implementation of BERTGraph and then provides experimental results under different news datasets for news category classification. Section 5 concludes this paper."
https://arxiv.org/html/2411.08257v1,GPTree: Towards Explainable Decision-Making via LLM-powered Decision Trees,"Traditional decision tree algorithms are explainable but struggle with non-linear, high-dimensional data, limiting its applicability in complex decision-making. Neural networks excel at capturing complex patterns but sacrifice explainability in the process. In this work, we present GPTree, a novel framework combining explainability of decision trees with the advanced reasoning capabilities of LLMs. GPTree eliminates the need for feature engineering and prompt chaining, requiring only a task-specific prompt and leveraging a tree-based structure to dynamically split samples. We also introduce an expert-in-the-loop feedback mechanism to further enhance performance by enabling human intervention to refine and rebuild decision paths, emphasizing the harmony between human expertise and machine intelligence. Our decision tree achieved a 7.8% precision rate for identifying “unicorn” startups at the inception stage of a startup, surpassing gpt-4o with few-shot learning as well as the best human decision-makers (3.1% to 5.6%).","Thanks to their explainability, decision trees are among the most intuitive and popular methods in machine learning (second only to linear regression). Their tree-like structure enables users to easily follow the decision-making process, with each split representing an interpretable rule based on the input data. This transparency is valuable in domains such as the Venture Capital (VC) industry where the stakes are high for each investment decision. In practice, however, decision trees often underperform when faced with non-linear, high-dimensional datasets and are inherently unsuitable for text-rich and multi-modal datasets. Thus, our work to extend decision trees is motivated by this fundamental question: how to incorporate LLMs? 05101520Best GPTreemodelGPTreew/expertGPTreeTier-1 seedfundsgpt-4oIndexingstrategy17.97.87.25.63.11.9Precision (%) Figure 1: Comparison of Different Models/Methods In recent years, Large Language Models (LLMs) have emerged as powerful tools capable of capturing the intricacies of natural language: models such as gpt-4o and gpt-1o preview have showcased exceptional capabilities in advanced reasoning and multi-modal tasks. However, LLMs are often seen as “black boxes” due to their elusive architectures. In addition, prompt engineering techniques like chain-of-thought and tree-of-thought, combined with extensive prompt chaining, are frequently required to produce accurate and contextually relevant responses. Not only does this require considerable human intervention and expertise but is also prone to trial-and-error procedures in crafting prompts that provide meaningful outputs. Therefore, we address another question: how to design a robust and explainable approach that minimizes human intervention while maintaining high performance? In this paper, we present GPTree, a novel framework that combines the explainability of decision trees with the advanced reasoning capabilities of LLMs, also incorporating an efficient expert-in-the-loop feedback system. Figure 2: GPTree pipeline In summary, our work makes the following contributions: • We introduce an LLM-powered decision tree model to dynamically split samples using a combination of LLM inference, code-based and clustering nodes, giving users the full explainability of traditional decision trees along with the flexibility of working with unstructured text and potentially multimodal datasets. • We eliminate the need for feature engineering and prompt chaining, instead replacing it with our expert-in-the-loop feedback mechanism. This further enhances performance by enabling a human expert to refine and rebuild decision paths post-training, leveraging the cooperation potential between human expertise and machine intelligence. • We conduct a comprehensive empirical evaluation of our approach within the VC landscape, where explainable decision-making is essential. Our experimental analysis demonstrates the effectiveness of our approach at identifying “unicorn” startups in comparison to human decision-makers within the industry, based on data collected from over 115K US-based companies founded more than 8 years ago."
https://arxiv.org/html/2411.08254v1,VALTEST: Automated Validation of Language Model Generated Test Cases,"Large Language Models (LLMs) have demonstrated significant potential in automating software testing, specifically in generating unit test cases. However, the validation of LLM-generated test cases remains a challenge, particularly when the ground truth is unavailable. This paper introduces VALTEST, a novel framework designed to automatically validate test cases generated by LLMs by leveraging token probabilities. We evaluate VALTEST using nine test suites generated from three datasets—HumanEval, MBPP, and LeetCode—across three LLMs—GPT-4o, GPT-3.5-turbo, and LLama3.1 8b. By extracting statistical features from token probabilities, we train a machine learning model to predict test case validity. VALTEST increases the validity rate of test cases by 6.2% to 24%, depending on the dataset and LLM. Our results suggest that token probabilities are reliable indicators for distinguishing between valid and invalid test cases, which provides a robust solution for improving the correctness of LLM-generated test cases in software testing. In addition, we found that replacing the identified invalid test cases by VALTEST, using a Chain-of-Thought prompting results in a more effective test suite while keeping the high validity rates.","⬇ 1def max_area(height): 2””” You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]). Find two lines that together with the x-axis form a container, such that the container contains the most water. Return _the maximum amount of water a container can store_.**Notice** that you may not slant the container. 3**Example 1:**Input:** height = [1,8,6,2,5,4,8,3,7]**Output:** 49 4**Explanation:** The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. 5**Example 2:**Input:** height = [1,1]**Output:** 1””” Tests Generated by GPT4o ⬇ 1 Function Input Expected Output 21. assert max_area ([1,3,2,5,25,24,5]) == 24 @✓@ 32. assert max_area([1,2,3,4,5,6,7,8,9,1])== 8 @✗@ 43. assert max_area([1,2,3,4,5,6,7,1,8,9])== 18 @✗@ 54. assert max_area([1,1,1,1,1,1,1,1,1,1])== 9 @✓@ ⬇ 1 Corresponding Token Probabilities 21. [43,44,98,99,94,99,99], [@99@] 32. [54,38,70,80,92,38,79,92,61,31], [@46@] 43. [50,33,56,71,77,80,75,67,86,98], [@35@] 54. [90,92,92,99,99,99,99,99,99,92], [@99@] Figure 1. An example of test case generation using GPT4o. The check mark indicates a valid test case and the cross mark indicates an invalid test case. LLMs have been applied in various software development tasks, including software testing, design, requirements engineering, code generation, maintenance, deployment, and more (Fan et al., 2023; Wang et al., 2024). Automated generation of unit tests is crucial for ensuring the accuracy of individual software components. Unit tests examine isolated sections of code, helping developers detect issues early and verify that each function operates as intended. LLMs are particularly promising for improving the efficiency of unit test case generation and its automation. This automation lessens the developers’ burden and has the potential to improve both test coverage and quality (Wang et al., 2024). In addition, unit test case generation is a critical component of many LLM-based code generation tools, such as Reflexion (Shinn et al., 2024), LATS (Zhou et al., 2023), AgentCoder (Huang et al., 2023a), EPiC (Taherkhani et al., 2024), and LDB (Zhong et al., 2024). Several studies have addressed the generation of valid test cases and the refinement of test suites (Li and Doiron, 2023; Guilherme and Vincenzi, 2023; Yuan et al., 2023; Li and Yuan, 2024; Sollenberger et al., 2024). These works typically rely on code execution to validate test cases and employ LLMs to enhance test suite quality. While previous studies address the refinement of test cases and, in some cases tackle the issue of generating invalid test cases using LLMs, to the best of our knowledge no study directly addresses this critical question: How can we determine whether an LLM-generated test case is valid, when the code under test is either unavailable or its correctness is unknown? Note that in most practical scenarios, the code under test during the testing phase might have bugs, thus a failed test case may indicate a bug in the code or an invalid test case. Therefore, the test’s result is not enough as a verdict for test validation. In addition, there are scenarios where the code is not even available before the test case, e.g., in the Test Driven Development (TDD) process, as well as in many recent LLM-based code generation tools (Shinn et al., 2024; Taherkhani et al., 2024), where the test cases are required as part of the code generation process. Validating test cases is a preliminary step before evaluating their effectiveness. While test case evaluation involves determining whether the test cases are adequate, often using metrics such as code coverage or mutation testing, validation focuses on determining whether the test case verify the intended functionality. That is, the assertions correctly define the expected and the actual results. This distinction is crucial: validation precedes evaluation and is inherently more challenging because it requires an understanding of the expected behavior of the function. LLMs frequently generate invalid test cases, even with state-of-the-art (SOTA) models, such as GPT-4o, and even in widely used benchmarks like HumanEval. For example, as discussed later in this paper, the ratio of valid test cases to total test cases generated by a SOTA model like GPT-4o on the MBPP dataset is as low as 0.71. This ratio is even lower for other LLMs and datasets, highlighting the difficulty LLMs face in generating valid test cases. Therefore, identifying and discarding or even fixing these invalid test cases is essential, before integrating them into the project. If invalid test cases are incorporated, they can mislead developers (or code generation tools) and lead to unintended consequences. For example, Figure 1 presents a sample of test cases generated by GPT-4o. In this example, the LLM produced two valid and two invalid assertion statements (while more tests can be generated, we present only four for brevity). The correct outputs for the second and third assertions should be 20 (not 8), and 25 (not 18), accordingly. In this paper, we use a hallucination-aware approach to predict invalid test cases generated by LLMs. Hallucination in the context of Natural Language Generation refers to the phenomenon where models generate text that is either nonsensical or unfaithful to the provided source content (Ji et al., 2023). One of the most common ways to detect hallucinations in LLMs is to use token probabilities as used in (Kadavath et al., 2022; Ledger and Mancinni, 2024; Quevedo et al., 2024; Huang et al., 2023b; Fadeeva et al., 2024; Varshney et al., 2023; Orgad et al., 2024). In LLMs, logits reflect the model’s confidence in each token being the next in a sequence. To transform these scores into probabilities, the softmax function is applied, producing a probability distribution where each token is assigned a probability based on its logit. The token with the highest probability is typically selected as the next in the generated sequence. In this paper, when we refer to a test case, we mean a unit test with a single assertion that verifies one behavior of the function under test. Therefore, we use “assertion” and “test case” interchangeably. In Figure 1, for each assertion, we extracted the token probabilities associated with the function input from the left-hand side of the assertion and the expected output from the right-hand side. For example, in the assertion assert maxarea([1, 3, 2, 5, 25, 24, 5]) == 24, the token probability of the expected output token 24 is 99, while the function input tokens 1,3,2,5,25,24,5 have probabilities of 43,44,98,99,94,99,99, respectively. Comparing the token probabilities of assertions 1 and 4 with those of assertions 2 and 3, we observe that invalid test cases exhibit lower token probability scores, either in the function input or the expected output tokens. This is expected, as LLMs are prone to generating invalid test cases when they are uncertain about the assertions’ input/output, which is often the result of LLM’s hallucination (generating assertions that contradict the function’s description). This observation motivated us to develop VALTEST, a tool that leverages the token probabilities of an LLM-generated test case to predict its validity. We utilized three datasets—HumanEval, MBPP, and LeetCode—and three LLMs—GPT-4o, GPT-3.5-turbo, and LLama3.1 8b—to generate nine test suites for evaluating VALTEST. These new test suites were necessary as existing test suites for these datasets in previous works do not include token probability information. After generating the tests, we extracted various feature sets representing the statistical measures of token probabilities for both the function input and the expected output of assertions. We executed each test case on its correct code under test to label each case as either valid or invalid. Using these labeled cases, we trained an ensemble machine learning model using a k-fold approach to predict the validity of each test case, in the evaluation set. Based on our predictions, we either discarded the invalid test cases identified by our model or applied a Chain-of-Thought (CoT) prompting technique to correct the invalid cases. We evaluated the test suites using validity rate, mutation score, and code coverage metrics both before and after applying VALTEST. Our results demonstrate that VALTEST improves the validity rate from 6.2\% up to 24\% across different LLMs and datasets, accordingly. Token probabilities in the expected output section are key indicators for distinguishing valid from invalid test cases. Features extracted from these probabilities show significant differences between valid and invalid, with valid cases having higher token probabilities and fewer hallucinations. Furthermore, expected output features have a stronger impact on VALTEST’s accuracy than function input features. Additionally, we highlight a trade-off between validity rate and mutation or code coverage scores in VALTEST. Moreover, combining CoT prompting with VALTEST to fix invalid test cases increases the mutation score by 2.9% to 6.7%, resulting in a more comprehensive test suite. The main contributions of this paper are as follows: (1) To the best of our knowledge, VALTEST is the first work to explore the validation of LLM-generated test cases using token probabilities. (2) We demonstrate the effectiveness of VALTEST in test case validation across three common benchmark datasets and three SOTA LLMs. (3) We show how VALTEST can be used to replace invalid test cases with valid ones to increase mutation score and code coverage of the LLM-generated test suites. We also release the data and source code for our experiments to facilitate replication and extension by other researchers (https://github.com/HamedTaherkhani/VALTEST)."
https://arxiv.org/html/2411.08249v1,Retrieval Augmented Time Series Forecasting,"Retrieval-augmented generation (RAG) is a central component of modern LLM systems, particularly in scenarios where up-to-date information is crucial for accurately responding to user queries or when queries exceed the scope of the training data. The advent of time-series foundation models (TSFM), such as Chronos, and the need for effective zero-shot forecasting performance across various time-series domains motivates the question: Do benefits of RAG similarly carry over to time series forecasting? In this paper, we advocate that the dynamic and event-driven nature of time-series data makes RAG a crucial component of TSFMs and introduce a principled RAG framework for time-series forecasting, called Retrieval Augmented Forecasting (RAF). Within RAF, we develop efficient strategies for retrieving related time-series examples and incorporating them into forecast. Through experiments and mechanistic studies, we demonstrate that RAF indeed improves the forecasting accuracy across diverse time series domains and the improvement is more significant for larger TSFM sizes.","The success of large language models (LLM) has motivated a broader push toward developing foundation models for other modalities. Time-series analysis, in particular, stands to directly benefit from recent advancements in sequence modeling techniques. Indeed, there has been significant progress in new time-series architectures [57, 54, 58, 24, 33, 27, 56, 7, 43, 28], tokenization strategies [33, 5, 1, 39, 49], and more recently, time-series foundation models such as Chronos [1]. These advances hold the premise to enhance accuracy, robustness, and few-shot learning capabilities of future time-series models. On the other hand, there is a notable shift from standalone models to compound AI systems [20, 35, 22, 21, 17] where LLMs are integrated with external databases and advanced prompting strategies to accomplish complex tasks. In particular, retrieval augmented generation (RAG) [22], has become a key component of LLM pipelines during recent years [23]. In essence, RAG aims to facilitate factual and up-to-date generation by retrieving query-related documents from external databases. Notably, RAG also mitigates the need for retraining the model to incorporate fresh data or fine-tuning it for individual application domains. In the context of time-series forecasting, we expect RAG to be beneficial for several reasons. First, time-series data is inherently dynamic, heterogeneous, and context-dependent, making it challenging to forecast accurately without access to relevant external context. Second, time-series data often exhibits complex patterns and dependencies that are difficult to capture via traditional forecasting methods – particularly in scenarios where rare events such as earthquakes, financial crises, or elections are of interest. Indeed, celebrated approaches in time series analysis, such as motif discovery, matrix profile, and dynamic time warping [3, 55, 31], are inherently about identifying and matching complex time-series patterns. Incorporating RAG holds the promise to augment time-series models with these capabilities to utilize external knowledge bases. Figure 1: Overview of the Retrieval Augmented Forecasting (RAF) framework. Top left: The original query is used to retrieve the best-matching time series (RTS 1, RTS 2, RTS 3, …). Bottom left: We utilize the best match (RTS 1) to form the retrieved context and retrieved future. Bottom right: These segments are then augmented with the original time series to produce an augmented input for forecasting. Top right figure displays the forecasts generated by Chronos Base. RAF outperforms the base model and returns a forecast closer to the actual future values. In this work, we provide a thorough investigation of the benefits of RAG in time-series forecasting. We introduce the Retrieval Augmented Forecasting (RAF) framework for time-series foundation models described in Figure 1. We find that RAF substantially enhances the forecast accuracy, especially in out-of-domain datasets (novel scenarios) where the TSFM may lack the domain-specific information. For example, a traffic dataset [54] has properties unique to transportation, such as the periodicity during a day, which are quite different from those of the M1 dataset [10] in finance. RAF facilitates the model to capture these properties by augmenting the the query with top matching patterns and harnessing the in-context learning capability of the TSFM. This way, it also provides a resource-efficient alternative to fine-tuning methods. Our main contributions are as follows: 1. We introduce RAF as a principled forecasting framework for TSFMs. We formalize RAF in Section 2, where we establish that transformer-based architectures are capable of RAF and contrast the retrieval performance under various signal-to-noise ratio conditions (see Figure 2). Notably, Chronos Mini fails to provide the correct forecast even if we use the query and its true future values as the retrieved example, whereas Chronos Small and Base can do so. These indicate that retrieval-augmented forecast is an emerging capability of large time-series foundation models. 2. We describe and study two variants of RAF: Naive RAF, which we will simply refer to as RAF, and Advanced RAF. RAF employs the model as a black-box without adjusting the model weights, whereas Advanced RAF additionally fine-tunes the model for retrieval-augmented generation, both yielding significant performance gains. As detailed in Table 3, we assess probabilistic and point forecasting performances on Chronos, comparing both RAF types against standard methods with and without fine-tuning. Advanced RAF surpasses all baselines, highlighting the effectiveness of our framework and its synergy with fine-tuning. Additionally, RAF closely matches the performance of fine-tuned models, providing a new method to improve TSFM performance. 3. We examine RAF with two model sizes – Chronos Mini and Base – shedding light on its effectiveness across different model capabilities, as shown in Figure 3 and Tables 1 and 2. These evaluations reveal that the relative improvement of RAF increases as the model size grows, in line with the empirical findings of RAG in large language models [12, 22, 16]. 1.1 Related Works Time Series Forecasting. Among the most popular deep learning approaches for time series forecasting are RNN-based and transformer-based models. A line of research in RNN-based models include [7, 52, 45, 43], while transformer-based models feature, among many others [58, 57, 54, 56, 27, 28, 26, 34], ForecastPFN [6], TimePFN [48], and the model employed in this study: Chronos [1]. While most references above highlight domain-specific models, there is an emerging trend in zero-shot forecasting. A line of work there includes [38, 37, 15, 6, 1]. ForecastPFN [6] and TimePFN [48] are trained exclusively on a synthetic dataset using the Prior-data Fitted Networks (PFNs) framework, a concept originally introduced by [32]. Chronos, on the other hand, is trained on real time-series data corpora, which is augmented with synthetically generated time-series examples via Gaussian processes to improve generalization. Time Series Foundational Models describes large models trained on extensive datasets, enabling them to recognize patterns across various time-series data domains [25]. A notable line of work includes Moirai [53], Lag-Llama [42], TimeGPT-1 [9], and Chronos [1]. Chronos proposes a scaling and quantization technique to train standard LLM architectures such as T5 [41] and GPT-2 [40], demonstrating state-of-the-art zero-shot and few-shot capabilities. Retrieval-Augmented Generation (RAG) is a key component in modern LLM pipelines, improving model generation in knowledge intensive tasks. A line of work includes those of [22, 12, 14, 18, 8, 4, 29]. Furthermore, recent studies have focused on utilizing Retrieval-Augmented Generation (RAG) for time-series forecasting [44, 51, 46]. [44] explores its application in agentic settings, [51] targets the domain of customer service, and [46] investigates mining temporal logic specifications from data. Our work sets itself apart by focusing on retrieval augmented generation in the context of time series foundational models. To the best of our knowledge, this is the first study to explore RAG in TSFMs."
https://arxiv.org/html/2411.08248v1,Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach,"Deep learning underpins most of the currently advanced natural language processing (NLP) tasks such as textual classification, neural machine translation (NMT), abstractive summarization and question-answering (QA). However, the robustness of the models, particularly QA models, against adversarial attacks is a critical concern that remains insufficiently explored. This paper introduces QA-Attack (Question Answering Attack), a novel word-level adversarial strategy that fools QA models. Our attention-based attack exploits the customized attention mechanism and deletion ranking strategy to identify and target specific words within contextual passages. It creates deceptive inputs by carefully choosing and substituting synonyms, preserving grammatical integrity while misleading the model to produce incorrect responses. Our approach demonstrates versatility across various question types, particularly when dealing with extensive long textual inputs. Extensive experiments on multiple benchmark datasets demonstrate that QA-Attack successfully deceives baseline QA models and surpasses existing adversarial techniques regarding success rate, semantics changes, BLEU score, fluency and grammar error rate.","Question-answering (QA) models, a key task within Sequence-to-Sequence (Seq2Seq) frameworks, aim to enhance computers’ ability to process and respond to natural language queries. As these models have evolved, they have been widely adopted in real-world applications such as customer service chatbots[40], search engines [69], and information retrieval in fields like medicine [19] and law [35]. However, despite the significant progress in deep learning and natural language processing (NLP), these models remain vulnerable to adversarial examples, leading to misinformation, privacy breaches, and flawed decision-making in critical areas [23, 8, 15, 52]. This highlights the importance of understanding how adversarial examples are generated from the attackers’ perspective and potential defense mechanisms — an area that remains under-explored. QA models are expected to comprehend given texts and questions, providing accurate and contextually relevant answers [50]. These models primarily address two types of questions: Informative Queries and Boolean Queries. The Informative Queries typically begin with interrogative words such as “who,” “what,” “where,” “when,” “why,” or “how,” requiring detailed and specific information from the provided context. Although models like T5 [43], LongT5 [14], and BART [27], which follow an encoder-decoder structure, have demonstrated strong performance, they still suffer from the maliciously crafted adversarial examples. Initially, studies like “Trick Me If You Can” [57] primarily relied on human annotators to construct effective adversarial question-answering examples. This methodology, however, inherently constrained scalability and increased resource demands. As research progressed, automated approaches for attacking textual classifiers in QA models emerged. Gradient-based methods, as employed in RobustQA [63], UAT [56], and HotFlip [10], were developed to identify and modify the most influential words affecting model answers. Building upon a deeper understanding of QA tasks, subsequent studies explored more targeted strategies. For instance, Position Bias [24], TASA [5], and Entropy Maximization [49] investigated the manipulation of sentence locations and the analysis of answer sentences to identify vulnerable parts of the context. These approaches refined the attack methods by applying modifications through paraphrasing or replacing original sentences, thus enhancing the effectiveness of adversarial examples. However, these methods encounter two primary challenges: 1) None of these attack methods is suitable for both “informative queries” and “boolean queries”. 2) Constraining the search space for optimal vulnerable words to answer-related sentences compromises attack effectiveness; meanwhile, targeting entire sentences proves inefficient [17]. In addition, Boolean Queries seek a simple binary “Yes” or “No” answer. Models like BERT [7], RoBERTa [70], and GPT variants [22, 3, 1, 51], which excel at sentence-level understanding and token classification, are widely used for Boolean QA tasks. These models leverage their deep contextual understanding of language to accurately determine whether a given statement is true or false, making them state-of-the-art baselines for the task. Researchers have proposed various approaches to target boolean classifiers in the context of Boolean Queries attacks. Attacks like [31, 12, 18, 66, 46], which involve adding, relocating, or replacing words, are based on the influence that each word has on the prediction. They retrieve word importance by the output confidence to the level or with gradient. However, gradient calculation is computationally intensive and ineffective when dealing with long context input, and knowing victim models’ internal information is unrealistic in practice. We present QA-Attack, an adversarial attack framework tailored for both Informative Queries and Boolean Queries in QA models. QA-Attack uses a Hybrid Ranking Fusion (HRF) algorithm that integrates two methods: Attention-based Ranking (ABR) and Removal-based Ranking (RBR). ABR identifies important words by analyzing the attention weights during question processing, while RBR evaluates word significance by observing changes in the model’s output when specific words are removed. The HRF algorithm combines these insights to locate vulnerable tokens, which are replaced with carefully selected synonyms to generate adversarial examples. These examples mislead the QA system while preserving the input’s meaning. This unified attack method improves both performance and stealth, ensuring realistic applicability for both types of queries. In summary, our work makes the following key contributions: • We present QA-Attack with a Hybrid Ranking Fusion (HRF) algorithm designed to target question-answering models. This novel approach integrates attention and removal ranking techniques, accurately locating vulnerable words and fooling the QA model with a high success rate. • Our QA-Attack can effectively target multiple types of questions. This adaptability allows our method to exploit vulnerabilities across diverse question formats, which significantly broadens the scope of potential attacks in various real-world scenarios. • QA-Attack generates adversarial examples by implementing subtle word-level changes that preserve both linguistic and semantic integrity while minimizing the extent of alterations, and we conduct extensive experiments on multiple datasets and victim models to thoroughly evaluate our method’s effectiveness in attacking QA models. The rest of this paper is structured as follows. We first review QA system baselines and adversarial attacks for QA models in Section 2. Then we detail our proposed method in Section 3. We evaluate the performance of the proposed method through extensive empirical analysis in Section 4. We conclude the paper with suggestions for future work in Section 5."
https://arxiv.org/html/2411.08241v1,A Social Outcomes and Priorities centered (SOP) Framework for AI policy,"[Abstract]Rapid developments in AI and its adoption across various domains have necessitated a need to build robust guardrails and risk containment plans while ensuring equitable benefits for the betterment of society. The current technology-centered approach has resulted in a fragmented, reactive, and ineffective policy apparatus. This paper highlights the immediate and urgent need to pivot to a society-centered approach to develop comprehensive, coherent, forward-looking AI policy. To this end, we present a Social Outcomes and Priorities centered (SOP) framework for AI policy along with proposals on implementation of its various components. While the SOP framework is presented from a US-centric view, the takeaways are general and applicable globally.","Developments in the field of AI have been rapid and continue to gather momentum. The application of AI spans a variety of domains in both public and private sectors, and continue to accelerate. Further, recent developments in AI such as Generative AI have resulted in a fundamental evolution of the types of AI systems – systems that are not just stochastic but also manifest themselves in ways that exhibit previously unknown characteristics and behaviors (e.g., hallucinations 1, 2). Approaches that rely on studying these systems as natural systems are bound to be very deficient in understanding their behavior, let alone quantifying them, since both the uses and coverage of these systems are neither limited nor well-understood. To add to that, the underlying system design isn’t static and continues to evolve. We are sorely lacking a good understanding on the workings as well as proper, principled, tractable evaluation and validation of these AI systems. Unlike even a couple of decades ago, this rapid evolution is accompanied by parallel developments in other relevant areas such as computing, semiconductors, large scale data availability, rapid adoption and integration capabilities, and relatively much shorter time to market -– i.e., reach to both enterprise and retail consumers. There are market competitive forces further pushing the AI technology makers (interestingly both in the industry and academia) to release products and services powered by these technologies at an increasing pace. Hence, the cumulative effects of these developments, market forces, and the competitive pressures are posing a unique set of challenges – quite unlike what we have seen with critical technologies in the past where either the knowledge, ability to develop, or access (to the technology itself, or its building blocks) was typically restricted. Consequently, maturity of AI models and systems notwithstanding, their adoption has picked up significant pace 3 (the estimates in 3 are likely overly optimistic for GenAI adoption but the overall intent and industry efforts certainly continue to push for further penetration of technology, and other AI methods and techniques are already deeply entrenched in many areas). As adoption grows so do the challenges. These challenges are both specific and potentially systemic. More importantly, the debate on the effects and impact of AI on humanity tends to largely be held in abstract. On one end, proponents maintain AI as a type of silver bullet solution to everything that ails us, while the arguments on the other end maintain that AI poses existential risk in mostly overt ways. It can be argued that AI poses systemic risks in various ways that can have significant deleterious implications for our society – in fact they are manifesting already. However, most of the AI risk debate is around some hypothetical future scenario when AI systems will take over and make decisions bypassing human control (an advanced AGI scenario which we are far from and neither have a realistic timeline nor understanding of) that can bring about catastrophic consequences on humanity – scenarios such as nuclear warfare and bio-weapon risks.†††These and other catastrophic risks are very real in today’s world but currently contingent on human actions not a consequence of AI’s automated-decision-making risk. We humbly contend that these two extreme projections on the spectrum of AI’s opportunities and risks not just misrepresent the stakes but distract us collectively from both much needed urgent actions and an informed lasting policy framework – one that can address our present and immediate needs and realistically and pragmatically scale with the growth of AI and other developments. Neither the hyperbole around the all-encompassing benefits of AI, nor the doomsday scenarios of AGI taking over human decision making seem imminent and realistic enough currently so as to be actionable upon, but the risks and challenges arising from various relevant applications of AI are already here, and presently. Even when there are relatively more calibrated efforts in outlining the risks from autonomous AI systems 4, 5, these are typically futuristic scenarios for which the current state and sophistication of AI doesn’t suffice. And this is an important point. Both – the intentional hype and/or overestimation of AI’s current capability and the haste in putting immature, unreliable and untested AI capabilities in production to automate various critical tasks – are themselves dangerous. They not just expand the risk vectors but also open up possibilities for unintended system failures leading to catastrophic consequences. Among the present challenges and imminent risks from AI potentially in conjunction with other concurrent technological developments that confront us are: • Introduction of risks to products and services. Both due to a lack of proper testing framework and lack of incentive or requirement to do so, AI products and capabilities often lack robust testing, evaluation, validation and verification (V&V). This results in a lack of rigorous vetting of the products entering the market and decision-making workflows. Among the most well-known recent examples of such risks are the hallucination and deepfake issue with GenAI products 6, 7, 8. However, there are several additional examples leading to safety- and privacy-concerns among others 9, 10, 11, 12. • Security vulnerability. Rapid introduction of half-baked products in business critical applications have opened up new fronts on data and security risks, and cyber-security lines of attacks. These risks are not just in terms of adversarial attacks and system compromise but also data compromise resulting from the manner in which the AI technologies are modeled and deployed 13, 14, 15, 9, 16. • Impact on society and democracy. From potential for social engineering, election interference, mis- and dis-information, (misleading) partial information, to sowing division are only some of the risks that we need to contend with because of the combination of AI, mobile, and data technologies introduced and adopted across the world 17, 18. • National security implications on important areas including IP, business competitiveness, cyber-security, and critical infrastructure 19, 20, 21. • Lack of our collective capability to sufficiently understand and foresee, let alone address, the impact of AI-powered products and services be it on labor and workforce, social equity and equality, markets and consumers, or corporate responsibility 22, 23, 24. • Ethics and fairness challenges and concerns. These range all the way from data utilization, governance, and IP violation of human generated content to responsible use and societal implications around bias, discrimination, potential for social engineering, intended and unintended deleterious consequences 25, 26. • Risks and concerns emanating from relatively widespread open-source availability of novel AI technological developments (both in the US and globally) 27, 28, 29, 30, 31. • Challenges from AI in conjunction with other technology areas including quantum computing, blockchain, robotics, and Internet of Things (IoT) that may currently be escaping our focus and foresight 32, 10. • Less understood systemic and systematic effects of AI-driven products on existing functions and areas such as education, healthcare, psychology, and social harmony 33, 34, 35. Various policy efforts across the globe at both industry and government levels have been and continue to be proposed. While most of these efforts are mainly guidance and lack teeth, the others are frameworks that focus on the specific sub-areas and/or technical aspects. All of them further are decoupled from the intended outcomes and lack clear definition and metrics of success. Consequently, we continue to discuss, hype, and claim advances on the AI policy front but have little to show for it in terms of intended social and societal results. Even when the proposals discuss the outlines of the expected outcomes or benefits to society, they are grossly missing in specifics and roadmap to achieve desired outcomes. Different policy elements and areas continue to be proposed independently without necessarily informing or coordinating with each other. Furthermore, we lack a core strategy to manage and reconcile them. Consequently, even though there is a lot of activity and efforts around building robust AI policies, standards, and guardrails, they are bound to miss in most cases since they are neither anchored in the expected outcomes nor systematically managed within a clear framework. Our current policy efforts fall under two broad categories. The first set of efforts focuses on placing very high-level, rather vague guidance that AI systems should benefit society and do no harm. While this is a worthy vision, it doesn’t give any clarity or specifics on how to achieve that goal – what outcomes and effects are desirable? What are the ones that society can adapt to and accommodate? Which outcomes are deleterious? Which are outright dangerous and risky, to be avoided at all costs?. There is also an inherent subjectivity involved in such specifics based on the application, domains, user-groups, sensitivity of the technologies involved, sensitivity and the level of risk involved, and short-term and accumulated risk scenarios. They do not clarify what type of overall policy approach can advance our objective of maximizing AI’s benefits while minimizing the risks, and how it would be achieved. That is, we are missing a consensus social prioritization of the intended outcomes and an associated policy framework along with regulatory and standardization mechanism. Naturally, in the absence of any anchor or specificity, combined with the vagueness of guidance, these sets of policy suggestions have been rendered inactionable. As a consequence, most of the focus and energy of the AI policy discourse has been usurped by the second set of efforts - those focused just on the technical aspects of AI, resulting in a solely technology-centered approach. However, there is little on which this approach is anchored in terms of its ability to achieve the intended outcomes. Unfortunately, society is mostly missing from the work on AI policy for society. What we need is a comprehensive policy framework guided by social priorities and associated outcomes. This should account for various aspects of AI development and deployment life-cycle, AI’s implications on different application areas and domains, resultant effects and risks on various realms of society, implications on national and corporate security, accountability and risk mitigation from AI-driven offerings, and guardrails against unwanted and unintended near- and long-term consequences, all anchored in outcomes that support the societal priorities including democracy, fairness, equity and equality, information provenance, privacy, and security. That is, the intended outcomes should inform the policy priorities and the use of AI in various areas. This would not just help us optimally secure against the risk of AI but will also help maximize the use and adoption of AI for social benefits. Moreover, the innovation in the field can be guided productively and efficiently against this backdrop of social priorities. As a simple example, hallucinations in LLM’s is a well recognized problem and likely impossible to be completely solved in the way GenAI algorithms are designed currently 1, 2. Hallucinations can pose a very high risk in various areas such as defense or safety-critical application and should be managed both in technical and utilization sense. However, there are areas where they may not pose a critical risk –- areas such as creative art (assuming concerns around IP and copyrighted materials are resolved). Hence, policy framework should be flexible on where and under what circumstances is AI adoption helpful and permissible, and under what conditions they should have guardrails or can be outright impermissible. Also, various application and domains would demand different levels of stringency of such guardrails but this can be achieved only if there is a clear understanding and agreement on what types of outcomes are desired and allowed. Similarly, AI recommender systems can have fewer guardrails when applied to shopping recommendation but significantly stringent ones for critical areas such as content recommendation on social media to minimize the unintended and undesired consequences. There is an additional dimension on permissible uses of AI despite the guardrails and an associated accountability and enforcement mechanism - that of adversarial or unethical actors. For instance, there are various proposals around dealing with deepfakes relying mostly on the developers adhering to the established protocols around using AI in a principled manner. This, of course, doesn’t protect us from nefarious or unethical uses for which the legal framework remains unprepared. An example would be the role of deepfakes in degradation of women as detailed in 36, 8. Not only does this exemplify the unwanted use of AI but also the limits of our societal safeguards in extending protection against such uses. It would be very difficult to address such issues if the view on AI’s use is entirely technology-centered and decoupled from broader social and societal priorities. The social implications and costs are immense and AI in such cases is just an enabling technology that accelerates the technical path for these unwanted efforts (see, for instance, 33 for various dangers to the youth from social media that need our immediate and continued attention). A framework that reconciles our social priorities with the associated guardrails against AI’s use as well as the accountability-, legal- and enforcement-apparatus is critically important. This paper proposes a Social-Outcomes and Priorities based (SOP) framework for AI Policy. The SOP framework re-frames the discussion of AI policy in a society-centered approach, a departure from a technology-centered approach that has been adopted so far. The paper outlines the core components of the SOP framework along with suggestions on their implementation, recommendations on how the framework can leverage the existing policy, regulatory, and legal apparatus and further advises on how these can be enhanced, strengthened and updated in the context of an evolving AI landscape. The rest of the paper is organized as follows: Sec. 2 provides background on the main arguments currently driving the policy discussion resulting in fragmented technology-centered efforts, and their limitations. Sec. 3 sets up the discussion on the main constituents that an effective policy framework should cover. Sec. 4 then introduces the main proposal – a social-outcomes and priorities centered (SOP) framework for AI policy. It also details the main functional components of this framework along with proposals on their implementation. Even though the implementation proposals for various functions are presented from a US-centric viewpoint, the core framework is applicable globally. Sec. 5 then details how an SOP framework can be beneficial and contextualizes it with the application. We finally conclude with a call for action in Sec. 6 highlighting both the urgency and importance of implementing such an effective policy framework."
https://arxiv.org/html/2411.08227v1,"DPU: Dynamic Prototype Updating for Multimodal 
Out-of-Distribution Detection","Out-of-distribution (OOD) detection is crucial for ensuring the robustness of machine learning models by identifying samples that deviate from the training distribution. While traditional OOD detection has predominantly focused on single-modality inputs, such as images, recent advancements in multimodal models have shown the potential of utilizing multiple modalities (e.g., video, optical flow, audio) to improve detection performance. However, existing approaches often neglect intra-class variability within in-distribution (ID) data, assuming that samples of the same class are perfectly cohesive and consistent. This assumption can lead to performance degradation, especially when prediction discrepancies are indiscriminately amplified across all samples. To address this issue, we propose Dynamic Prototype Updating (DPU), a novel plug-and-play framework for multimodal OOD detection that accounts for intra-class variations. Our method dynamically updates class center representations for each class by measuring the variance of similar samples within each batch, enabling tailored adjustments. This approach allows us to intensify prediction discrepancies based on the updated class centers, thereby enhancing the model’s robustness and generalization across different modalities. Extensive experiments on two tasks, five datasets, and nine base OOD algorithms demonstrate that DPU significantly improves OOD detection performances, setting a new state-of-the-art in multimodal OOD detection, including improvements up to 80\% in Far-OOD detection. To improve accessibility and reproducibility, our code is released at https://github.com/lili0415/DPU-OOD-Detection.","Out-of-distribution (OOD) detection aims to identify samples that differ from the in-distribution (ID) data in ways that challenge the model’s ability to generalize [16, 36, 14, 25, 2]. It is crucial for enhancing the safety and robustness of machine learning models across various domains, such as autonomous driving [8, 26], medical imaging [19], robotics [3, 9], and other applications [4, 50, 48, 28, 15, 21, 43, 12, 42]. In recent years, numerous OOD detection algorithms have been developed, spanning classification-based and distance-based methods [49, 30, 17, 31, 40, 7]. Traditionally, OOD detection has focused on single-modality inputs, such as images or videos. With the emergence of large Vision-Language Models [37, 52], researchers are exploring OOD detection with the assistance of language modalities [33, 46, 29]. However, their evaluations remain limited to benchmarks containing only images. Effectively leveraging multimodal features (e.g., video, optical flow, and audio) remains an open challenge, requiring further research. Figure 1: Performance of our DPU applied to four base OOD methods in the Multimodal Far-OOD Detection task (§4.2), using HMDB51 as the ID dataset and Kinetics600 as the OOD dataset. Red symbols denote the OOD methods enhanced by DPU, demonstrating that DPU significantly improves their performances. Current Work. Dong et al. [11] introduced the first multimodal OOD benchmark and framework, identifying the phenomenon of modality prediction discrepancy. This phenomenon reveals that softmax prediction discrepancies across different modalities are negligible for ID data but significant for OOD data. By amplifying this prediction discrepancy during training, they observed improvements in OOD detection performance. However, a key assumption in previous multimodal OOD detection studies is that all samples within a given class are entirely in-distribution [11, 49], implying perfect cohesion among samples within the same class. This assumption rarely holds true in real-world applications, where intra-class variability is common. As a result, applying uniform discrepancy intensification across all training samples can degrade the model’s ID prediction accuracy [11]. When the discrepancy is intensified on class-center samples—typically exhibit consistent predictions across all modalities—this consistency is disrupted, causing significant model confusion and performance degradation, as shown in Appx. C.4 case study. Our Proposal. To tackle the challenge of intra-class variations in existing multimodal OOD detection methods, we introduce a novel approach called Dynamic Prototype Updating (DPU). DPU dynamically adjusts the multimodal prediction discrepancy to ensure high intra-class cohesion and clear inter-class separation, leveraging instance-level training invariance. The core idea of DPU is to update the prototype representations [24, 27] of each class at a dynamic, sample-specific rate, resulting in more precise and robust model performance. These prototype representations act as central reference points, capturing the key features of each class (see Fig. 2 for an overview). To establish a reliable representation space, we first introduce the Cohesive-Separate Contrastive Training procedure (§3.3), which applies marginal contrastive learning to strengthen intra-class cohesion while preserving distinctions between individual samples. Building on this, we design the Dynamic Prototype Approximation mechanism (§3.4), which adaptively refines prototype representations based on observed sample variances. This adaptive updating helps mitigate the negative impact of outliers on prototype evolution, stabilizing the learning process. Using these refined prototypes, we further adjust the multimodal prediction discrepancy for each sample according to its similarity to its class prototype (§3.5). Finally, OOD models make predictions by leveraging both the joint probability distribution across all modalities and the distinct information from each modality. In summary, we make the following contributions: • New Observations in Multimodal OOD Detection. We are the first to identify and explore the negative impact of intra-class variations within ID data for OOD detection. • Novel, Model-Agnostic Framework. We propose DPU, a flexible, plug-and-play method that effectively handles intra-class variations and is compatible with various existing OOD detection models. As shown in Fig. 1, DPU enhances performance across various OOD methods. • Effectiveness. Comprehensive experiments demonstrate the effectiveness of the proposed method across two tasks, five datasets, and nine base OOD methods. DPU significantly improves the performance of all benchmark models, achieving new state-of-the-art results, including improvements of around 10% across all metrics for Near-OOD detection and up to 80% for Far-OOD detection."
https://arxiv.org/html/2411.08212v1,"PERFT: Parameter-Efficient Routed 
Fine-Tuning for Mixture-of-Expert Model","The Mixture-of-Experts (MoE) paradigm has emerged as a promising approach for scaling transformers with improved resource utilization. However, efficiently fine-tuning MoE models remains largely underexplored. Inspired by recent works on Parameter-Efficient Fine-Tuning (PEFT), we present a unified framework for integrating PEFT modules into the MoE mechanism. Our framework, aligned with the core principles and architecture of MoE, encompasses a comprehensive set of design dimensions including various functional and composition strategies. By combining design choices within our framework, we introduce Parameter-Efficient Routed Fine-Tuning (PERFT) as a flexible and scalable family of PEFT strategies tailored for MoE models111Code available via https://anonymous.4open.science/r/PERFT-MoE/.. Extensive experiments adapting OLMoE-1B-7B and Mixtral-8\times7B for various commonsense and arithmetic reasoning tasks demonstrate the effectiveness, scalability, and intriguing dynamics of PERFT. Additionally, we provide empirical findings for each specific design choice to facilitate better application of MoE and PEFT.","As modern transformer Vaswani et al. (2017) models continue to scale up, Mixture-of-Experts (MoE) (Shazeer et al., 2017) has emerged in recent years as a promising solution to the trade-off between performance and cost, yielding notable results in a series of frontier models (Jiang et al., 2024; Reid et al., 2024; Dai et al., 2024; Qwen, 2024; Grok, 2024). Leveraging the sparsity inherent to transformer models, MoE significantly reduces the computational costs while maintaining model capacity, yet these advantages do not translate to efficient fine-tuning on downstream tasks. The full fine-tuning of MoE models remains prohibitively expensive due to their immense number of expert parameters. Besides, the routing mechanism among sparsely-activated experts poses unique challenges previously unseen in dense models (Wang et al., 2024). These challenges necessitate exploring specially designed Parameter-Efficient Fine-Tuning (PEFT) techniques for adapting sparse MoE models without incurring the full cost of fine-tuning all parameters. PEFT solutions, such as adapters (Houlsby et al., 2019) and LoRA (low-rank adaptation; Hu et al., 2022), have gained considerable attention on dense models. Hybrid approaches combining elements from different PEFT methods have also shown promising results (He et al., 2022; Hu et al., 2023; Zhang et al., 2023). With the rise of MoE architectures, recent studies have explored PEFT solutions incorporating MoE-inspired structures for dense models (Zadouri et al., 2023; Dou et al., 2023; Luo et al., 2024; Li et al., 2024; Gao et al., 2024; Wu et al., 2024). However, designing PEFT strategies specifically tailored for MoE models remains largely underexplored. To this end, we present a unified framework focused on incorporating diverse PEFT modules directly into the MoE mechanism. Different from previous PEFT solutions that operate in isolation from the underlying MoE architecture, our framework focuses on the core principles and unique challenges of MoE architecture. We introduce two key design dimensions. Functional strategies define the internal mechanisms of the introduced PEFT module, including the architecture inside individual PEFT modules, the multiplicity of PEFT modules, and the routing mechanism among them. Compositional strategies describe how PEFT modules interact with the original MoE architecture, including operating as shared PEFT experts or embedded PEFT experts. To rigorously characterize the behavior of adapting MoE with each strategies, we also provide empirical analyses that offer insights into understanding and optimizing configurations on these dimensions. By combining design choices within our framework, we introduce Parameter-Efficient Routed Fine-Tuning (PERFT), a flexible and scalable family of PEFT strategies tailored for MoE modules, as shown in Figure 1. These methods cover a range of architectural designs with varying levels of scale, sparsity, and routing dynamics. At the core of PERFT is PERFT-R (Routed), which introduces an independent routing mechanism among multiple PEFT experts, enabling task-specific expert activation patterns. We also study PERFT-E (Embedded), which utilizes the pre-trained router, and PERFT-D (Dense) and PERFT-S (Single), which employ always-activated PEFT experts without routing. These variants cover a wide range of functional and compositional strategies, allowing for a systematic exploration on the trade-offs between parameter efficiency, sparsity, and routing in fine-tuning MoE modules. Figure 1: Illustration of a default MoE layer and the PERFT family. PERFT-R, the primary variant, holds an independent routing among the introduced PEFT experts. PERFT-E embeds PEFT experts within the original MoE module and directly utilizes its routing patterns. PERFT-D and PERFT-S simply work as independent shared expert(s) alongside the MoE module. Extensive experiments are conducted on OLMoE-1B-7B (Muennighoff et al., 2024) and Mixtral-8\times7B (Jiang et al., 2024) for commonsense and math reasoning tasks. Our results demonstrate that PERFT enables different levels of efficient adaptation of MoE LLMs while maintaining competitive performance. With an equivalent level of activated trainable parameters, PERFT-R achieves improvements of up to 17.2% and 12.3% over MoE-agnostic baseline methods for OLMoE-1B-7B’s average performance in each domain. We also demonstrate and empirically analyze our findings on the optimal scaling, sparsity, and routing configurations for each specific design choice. We hope to provide insights for improving future MoE and PEFT approaches and contribute to a broader understanding of adaptation strategies for modern large-scale models. The primary contributions of our work are as follows: 1. We introduce a unified framework of PEFT techniques tailored for MoE modules. This encompasses multiple dimensions of design strategies, offering a novel perspective. 2. By combining the design choices within this unified framework, we propose PERFT as a flexible and scalable family of strategies for adapting MoE modules. 3. Extensive experiments adapting OLMoE-1B-7B and Mixtral-8\times7B for commonsense and arithmetic reasoning tasks validate the effectiveness, scalability, and intriguing dynamics of PERFT. We provide empirical findings and analysis for each specific design choice."
https://arxiv.org/html/2411.08197v1,What Representational Similarity Measures Imply about Decodable Information,"Neural responses encode information that is useful for a variety of downstream tasks. A common approach to understand these systems is to build regression models or “decoders” that reconstruct features of the stimulus from neural responses. Popular neural network similarity measures like centered kernel alignment (CKA), canonical correlation analysis (CCA), and Procrustes shape distance, do not explicitly leverage this perspective and instead highlight geometric invariances to orthogonal or affine transformations when comparing representations. Here, we show that many of these measures can, in fact, be equivalently motivated from a decoding perspective. Specifically, measures like CKA and CCA quantify the average alignment between optimal linear readouts across a distribution of decoding tasks. We also show that the Procrustes shape distance upper bounds the distance between optimal linear readouts and that the converse holds for representations with low participation ratio. Overall, our work demonstrates a tight link between the geometry of neural representations and the ability to linearly decode information. This perspective suggests new ways of measuring similarity between neural systems and also provides novel, unifying interpretations of existing measures.","The computational neuroscience and machine learning communities have developed a multitude of methods to quantify similarity in population-level activity patterns across neural systems. Indeed, a recent review by \textciteklabunde2023similarity catalogues over thirty approaches to quantifying representational similarity. Many of these measures quantify similarity in the shape or representational geometry of point clouds. For example, recent papers (e.g. [williams2021generalized, Ding2021]) leverage the Procrustes distance and other concepts from shape theory—an established body of work that formalizes the notion of shape and ways to measure distance between shapes [kendall2009shape, dryden2016statistical]. Other measures of representational similarity are not quite this explicit but still emphasize desired geometric invariance properties. For example, work by \textcitekornblith2019 popularized centered kernel alignment (CKA) by emphasizing its invariance to isotropic scaling, translation, and orthogonal transformations. These are precisely the same invariances specified by classical shape theory [kendall2009shape, dryden2016statistical]. Earlier work by \textciteraghu2017svcca argued for a more flexible invariance to affine transformations, and they used canonical correlations analysis (CCA) for this purpose. Contemporary work in neuroscience is replete with similar geometric reasoning and quantitative frameworks [Kriegeskorte2021]. Here we ask: What does the similarity between neural representations, potentially in terms of shape or geometry, imply about the similarity between functions performed by those neural systems? Potentially, not very much. \textciteMaheswaranathan2019 showed that recurrent neural networks with different architectures performed the same task with similar dynamical algorithms, but with different representational geometry. More recently, \textcitelampinen2024learned showed that representational geometry can be biased by other nuisance variables, such as the order in which multiple tasks are learned. Similar limitations of representational geometry measures are discussed in [dujmovic2022some, davari2023reliability]. On the other hand, several basic observations suggest that geometry and function ought to be related. Consider, for example, the common practice of using linear models to decode task-relevant variables from neural population activity [alain2017understanding, Kriegeskorte2019]. The premise behind these analyses is that anything linearly decodable from system A is readily accessible to layers or brain regions immediately downstream of A. Therefore, information that is linearly decodable from A may relate to functional role played by A within the overall system [Cao2022]. Notably, the invariances of typical representational similarity measures—translations, isotropic scalings, and orthogonal transformations—closely coincide with the set of transformations that leave decoding accuracy unchanged. For example, translations and rotations of neural population activity will not impact the accuracy of a linear decoder with an intercept term and an L_{2} penalty on the weights. Thus, if we accept the premise that decoding accuracy is a proxy—even, perhaps, an imperfect proxy—for neural system function, then representational geometry may be a reasonable framework to capture something about this function [kriegeskorte2019peeling]. We provide a unifying theoretical framework that connects several existing methods of measuring representational similarity in terms of linear decoding statistics. Specifically, we show that popular methods such as centered kernel alignment (CKA) [kornblith2019] and similarity based on canonical correlations analysis (CCA) [raghu2017svcca] can be interpreted as alignment scores between optimal linear decoders with particular weight regularizations. Additionally, we study how the shape of neural representations is related to decoding by deriving bounds that relate the average decoding distance and the Procrustes distance. We find that the Procrustes distance is a more strict notion of geometric dissimilarity than the expected distance between optimal decoder readouts, in that a small value of the Procrustes distance implies a small expected distance between optimal decoder readouts but the converse is not necessarily true. This formalizes recent observations by \textcitecloos2024differentiable, who found that high Procrustes similarity implied high CKA similarity in practical settings."
https://arxiv.org/html/2411.08195v1,An Explainable Machine Learning Approach for Age and Gender Estimation in Living Individuals Using Dental Biometrics,"Objectives: Age and gender estimation is crucial for various applications, including forensic investigations and anthropological studies. This research aims to develop a predictive system for age and gender estimation in living individuals, leveraging dental measurements such as Coronal Height (CH), Coronal Pulp Cavity Height (CPCH), and Tooth Coronal Index (TCI). Methods: Machine learning models were employed in our study, including Cat Boost Classifier (Catboost), Gradient Boosting Machine (GBM), Ada Boost Classifier (AdaBoost), Random Forest (RF), eXtreme Gradient Boosting (XGB), Light Gradient Boosting Machine (LGB), and Extra Trees Classifier (ETC), to analyze dental data from 862 living individuals (459 males and 403 females). Specifically, periapical radiographs from six teeth per individual were utilized, including premolars and molars from both maxillary and mandibular. A novel ensemble learning technique was developed, which uses multiple models each tailored to distinct dental metrics, to estimate age and gender accurately. Furthermore, an explainable AI model has been created utilizing SHAP, enabling dental experts to make judicious decisions based on comprehensible insight. Results: The RF and XGB models were particularly effective, yielding the highest F1 score for age and gender estimation. Notably, the XGB model showed a slightly better performance in age estimation, achieving an F1 score of 73.26%. A similar trend for the RF model was also observed in gender estimation, achieving a F1 score of 77.53%. Conclusions: This study marks a significant advancement in dental forensic methods, showcasing the potential of machine learning to automate age and gender estimation processes with improved accuracy. Clinical Significance: Accurate age and gender predictions hold importance across diverse domains, including forensic investigations involving both living and deceased individuals. Moreover, beyond its forensic applications, age and gender estimation based on dental measurements holds clinical significance in dental diagnostics and treatment planning.","Accurate age and gender estimation using dental features are crucial across various fields, including forensic science and clinical dentistry [2]. Dental structures, known for their resistance to external factors, are a reliable source for age estimation [8]. Key dental measurements, such as Coronal Height (CH), Coronal Pulp Cavity Height (CPCH), and Tooth Coronal Index (TCI), are closely linked to ageing due to the reduction in dental pulp volume over time [25]. Traditional methods of age estimation in forensic sciences using dentition are commonly applied to identify unknown corpses or human remains [26]. However, these traditional methods sometimes lack precision and reliability. Since much of the research is focused on human remains, these methods typically require the extraction of teeth. Consequently, they are often deemed time-consuming and ethically unsuitable, thereby facing reluctance to adopt. Some recent advancements suggest the use of dental radiographs to measure dental pulp for age and gender determination in the living [18]. A widely employed non-destructive approach for age and gender estimation, utilized by both dentists and forensic experts, involves analyzing radiographs of the upper and lower jaw to assess the pulp area [24]. Although this method does not involve physical alteration of the teeth, it requires the expertise of experienced dental professionals for precise predictions. This study explores a range of machine learning (ML) techniques, which offer a promising approach for age and gender determination in living individuals based on dental pulp measurements due to its ability to handle complex patterns and large datasets. Several ML models, such as Cat Boost Classifier (Catboost) [19], Gradient Boosting Machine (GBM) [17], Ada Boost Classifier (AdaBoost) [14], Random Forest (RF) [5], eXtreme Gradient Boosting (XGB) [6], Light Gradient Boosting Machine (LGB)[11], and Extra Trees Classifier (ETC) [22], were explored in this study, which provide diverse capability of capturing nuanced relationships between features and target variables. Additionally, data balancing techniques were adopted to mitigate biases and enhance accuracy. Furthermore, the ML models enable continual refinement through iterative learning, potentially improving accuracy and reliability over traditional methods. This interdisciplinary approach harnesses computational power to advance dental diagnostics, offering a non-invasive and potentially more accurate means of age and gender estimation. Moreover, ensemble learning methods combined with non-trainable combiners were investigated in this study. This approach leverages multiple models’ strengths, addressing individual model limitations to enhance overall predictive performance. The primary goals of this study are two-fold: 1) to examine the effectiveness of CH, CPCH, and TCI in estimating age and gender; and 2) to evaluate the performance of ensemble ML models in improving the accuracy and reliability of age and gender estimation. By integrating diverse models, we aim to create a robust and versatile framework for age and gender estimation using dental biometrics. Furthermore, to address the issue of non-interpretable results in ensemble models, eXplainable Artificial Intelligence (XAI) techniques, specifically SHAP (SHapley Additive exPlanations), were adopted in our study, which aims to enhance the interpretability of predictions from the proposed model, empowering dental experts to make more informed decisions by gaining insights into the underlying factors influencing the predictions. The remainder of this paper is organized as follows: Section II presents related work, highlighting existing methods and their limitations. Section III details the proposed methods, including those for data collection, feature extraction, machine learning and ensemble learning for age and gender estimation, and explainable AI (XAI). Section IV provides experimental results and discussions, and finally, Section V concludes the paper, summarizing key findings and outlining avenues for future research."
https://arxiv.org/html/2411.08187v1,TractoEmbed: Modular Multi-level Embedding framework for white matter tract segmentation,"White matter tract segmentation is crucial for studying brain structural connectivity and neurosurgical planning. However, segmentation remains challenging due to issues like class imbalance between major and minor tracts, structural similarity, subject variability, symmetric streamlines between hemispheres etc. To address these challenges, we propose TractoEmbed, a modular multi-level embedding framework, that encodes localized representations through learning tasks in respective encoders. In this paper, TractoEmbed introduces a novel hierarchical streamline data representation that captures maximum spatial information at each level i.e. individual streamlines, clusters, and patches. Experiments show that TractoEmbed outperforms state-of-the-art methods in white matter tract segmentation across different datasets, and spanning various age groups. The modular framework directly allows the integration of additional embeddings in future works.","Diffusion MRI (dMRI) [2, 1] facilitates the non-invasive examination of the brain’s white matter (WM) microstructural organization. A crucial component of the dMRI analysis pipeline is fiber tractography [3, 23, 22], which tracks fibers or streamlines under anatomical constraints from the dMRI signal received from the scanner(refer to Section 3). Tract Segmentation involves dividing the streamlines into distinct, anatomically meaningful tracts, with each tract corresponding to a specific white matter pathway. These tracts can be broadly grouped into 3 types based on structural connectivity, i.e. Association, Commissural, and Projection Fibers. Each type is further subdivided based on its specific structural connectivity and function, allowing for more granular distinctions. Through the segmentation process, it becomes possible to conduct quantitative studies of white matter (WM), which is important in understanding neurological disorders such as Alzheimer’s, and Parkinson’s [16], the effect of tumors on segmenting fiber streamlines, etc. In addition, tract segmentation is also crucial for preoperative neurosurgical planning [14], as it helps identify eloquent white matter areas and determine optimal surgical approaches that minimize post-operative damage. Tract segmentation is also extensively used to visualize particular segments for focused examination by clinicians. However, this process is typically performed by expert Neuroanatomists using their knowledge of brain anatomy to divide fibers into multiple bundles. As a result, it is very time-consuming and can vary between experts, affecting the consistency and reliability of the results. Taking challenges associated with manual tract segmentation, various techniques have been developed over the years. These techniques range from classical methods to ATLAS-based and distance-based algorithms [8, 9, 21, 25] (refer to Section 2). An ATLAS refers to a standardized reference that allows spatial mapping of neuroimaging data from different studies (refer to Table 1) and modalities. They approximate the shape, location, and brain region boundaries in a common coordinate space, facilitating the comparison of brain structure and function across individuals. These methods require significant manual intervention and are prone to age-related brain changes, also their effectiveness depends on the alignment and quality of the ATLAS. Considering the limitations of manual and classical methods, as well as the importance of tract segmentation, machine learning, and deep learning-based frameworks have been proposed for automatic tract segmentation [33, 4, 28]. Deep learning algorithms can learn information from shape, structure, relative location, fiber orientations, etc. However, a notable drawback is that these models often fail in classifying streamlines that are linear in shape due to over-reliance on shape, such as striato-thalamo-pallido projection fibers, which in existing methods, require global reference along with streamlines [7]. Additionally, when neurosurgeons are concerned with segmenting only a specific set of streamlines, global tractography can become a computational overhead. Due to these complexities in streamline classification-based tract segmentation, each method inherently has a certain drawback. To address this, we propose TractoEmbed, a modular framework that combines multi-level embeddings extracted from hierarchical data representations specifically at streamline, patch, and cluster levels (refer to Fig. 1). Our approach surpasses state-of-the-art (SOTA) results in tract segmentation. In this work, we present an approach with the following major contributions: 1. We introduce TractoEmbed, a novel modular multi-embedding framework, which leverages learning task-specific encoders to embed data representations, and generate embeddings. Where the encoders and their hyperparameters are selected after rigorous experimentation. 2. We propose novel hierarchical and descriptive streamline data representations. These representations includes spatial information about regional patches, neighboring streamlines and the streamline itself, providing a comprehensive understanding of the streamline characteristics. In contrast to recent advances, our method leverages minimal neighbouring streamlines, hyperlocal streamlines, enhancing robustness to practical clinical settings 3. It is demonstrated that TractoEmbed framework, generalizes across various datasets encompassing different age groups (refer Table 1). Additionally, the framework is modular at the embedding level, allowing researchers to integrate their own learnable embeddings to achieve even richer representations of streamline data. This modularity enhances the flexibility and adaptability of the framework."
https://arxiv.org/html/2411.08182v1,"SCORE: Syntactic Code Representations for Static
Script Malware Detection","As businesses increasingly adopt cloud technologies, they also need to be aware of new security challenges, such as server-side script attacks, to ensure the integrity of their systems and data. These scripts can steal data, compromise credentials, and disrupt operations. Unlike executables with standardized formats (e.g., ELF, PE), scripts are plaintext files with diverse syntax, making them harder to detect using traditional methods. As a result, more sophisticated approaches are needed to protect cloud infrastructures from these evolving threats. In this paper, we propose novel feature extraction and deep learning (DL)-based approaches for static script malware detection, targeting server-side threats. We extract features from plain-text code using two techniques: syntactic code highlighting (SCH) and abstract syntax tree (AST) construction. SCH leverages complex regexes to parse syntactic elements of code, such as keywords, variable names, etc. ASTs generate a hierarchical representation of a program’s syntactic structure. We then propose a sequential and a graph-based model that exploits these feature representations to detect script malware. We evaluate our approach on more than 400K server-side scripts in Bash, Python and Perl. We use a balanced dataset of 90K scripts for training, validation, and testing, with the remaining from 400K reserved for further analysis. Experiments show that our method achieves a true positive rate (TPR) up to 81% higher than leading signature-based antivirus solutions, while maintaining a low false positive rate (FPR) of 0.17%. Moreover, our approach outperforms various neural network-based detectors, demonstrating its effectiveness in learning code maliciousness for accurate detection of script malware.","Script-based malware has emerged as a potent threat vector, frequently leveraged in attacks against Linux systems due to the versatility, portability, and ease of use of modern scripting languages. As this threat becomes more prevalent, cloud environments and Linux systems have also become prime targets, especially for script malware written in server-side languages like Python, Perl, and shell scripts. These malicious scripts can serve as standalone threats, such as denial-of-service bots, ransomware, or backdoors, or function as components in multi-stage attacks by facilitating payload delivery and execution. With capabilities comparable to traditional executable malware, including data exfiltration and system resource abuse, these script-based malware poses a significant challenge for detection mechanisms that rely on static signatures or traditional machine learning (ML) techniques. The threat posed by script malware targeting cloud infrastructure is particularly concerning. These malicious scripts can directly access and manipulate underlying cloud resources, providing attackers with powerful capabilities. For instance, a recent Python-based credential harvester and hacking tool called Legion and AlienFox have been observed targeting AWS console credentials, SNS, S3, and SES services [1, 2]. Attackers can leverage the hijacked cloud infrastructure for activities like mass spamming, phishing campaigns, and privilege escalation. More importantly, the prevalence of script malware attacks has surged in recent years, with reports indicating a 100% increase since 2017 and such attacks accounting for 40% of all cyberattacks as of 2020 [3]. By early 2024, the frequency had nearly doubled again over the prior two years [4]. Given this rapid growth and powerful capabilities of scripts [5, 6, 7] mostly used in cloud environments, developing effective detection methods specifically targeting script malware is a critical security need. Malware analysis broadly spans static and dynamic analysis methods. Dynamic analysis executes the malicious file in a highly-controlled “sandbox” environment, and observes the malicious behavior directly [8]. Static analysis, on the other hand, analyzes the structure, properties, and code of a malicious file without executing it, and attempts to infer that same malicious behavior from these related data. As a result, static analysis is often cheaper, safer, and faster than dynamic analysis [9]. Typically, threat intelligence in static analysis tools—such as classical antivirus scanning products—vends as signatures, or rules, coalesced into a pattern-matching database that specifies malicious content that, if observed in a certain file, designates it ‘malicious’. However, such signature-based methods are often ineffective against more advanced malware that use simple code transformations to evade detection [9, 10]. ML-based representation learning of malware, in contrast, presents the opportunity to learn the malicious behavior patterns to capture what static signatures cannot [11, 12, 13, 14, 15, 16]. However, most light-weight ML models focus on learning code behavior from unstructured byte-strings [17, 18], while code language models are too slow and costly due to their billions of parameters. There is a need for reasonably sized ML models that can learn more global, generalizable, invariant, robust, and/or functional patterns of malicious structure in code compared to simple byte-string patterns. In contrast to single-format executables, analyzing scripts presents unique challenges. Scripts are often domain-specific, requiring specialized knowledge to extract features and model its behavior accurately for every specific script language. As the feature extraction becomes more sophisticated, complexity increases. On the other hand, byte-strings alone might not contain as much structured information as well-designed features. In this paper, we propose feature extraction methods that directly target these structures of scripts and detection methods that aim to understand the context of these scripts. In this paper, we address these research questions: • RQ1: How can code from server-side programming languages be effectively represented as features for scalable malware detection using deep learning (DL)? Which feature representation method provides the most useful information to detect malicious behavior? • RQ2: Can DL-based models learn the complex structure of scripts, and which types of DL models most accurately identify malicious code behavior? • RQ3: Does the best-performing DL-based model surpass conventional rule-based malware detectors in accuracy and threat coverage when applied to scripts? To address these questions, firstly, we propose script malware detection methods for server-side languages by leveraging popular code parsing libraries to extract features from the plain-code. We present two approaches for representing code as features: SCORE-H which is based on syntactic code highlighting and SCORE-T which is based on abstract syntax trees (ASTs). While SCORE-H parses the keywords in a sequentially hierarchical level, SCORE-T parses a program’s syntactic structure into a hierarchical tree representation. Both approaches pair their syntactic structure features with raw byte-strings of the scripts for more information-rich representations. Secondly, we propose malware detection models: a sequential model (SM) and a graph representation learning (GRL)-based model. SM contains multiple layers of convolutional neural networks (CNNs) to extract hierarchical embeddings from SCORE-H features, followed by a recurrent neural network (RNN), e.g., bi-directional long short-term memory networks (bi-LSTM). Moreover, we propose a variation of SM with simpler CNN embeddings for serialized SCORE-T features. Finally, our GRL-based model leverages graph embeddings obtained by graph similarity learning and contains a ML-based detector, such as XGBoost [19], for detecting static malicious behavior. System overview of our proposed approaches is shown in Figure 1, where each end-to-end model represents a malware detector, e.g., SM with SCORE-H features, SM with SCORE-T features and GRL model with SCORE-T features. We compare the proposed approaches against commercial antiviruses (AVs), a byte-level feature-based malware detector [18], a sequential and a graph-based neural network malware detector that utilize ASTs [12], and finally an ML detector that utilizes embeddings from CodeBERT, a multi-lingual foundation model pre-trained on Natural Language (NL) - Programming Language (PL) pairs [14]. Our SM shows significant performance for serialized features due to the sequentially written form of scripts. Serialization of the tree structure is highly significant and determined by the traversal method, such as breadth first traversal (BFT) and depth first traversal (DFT). When we have tree/graph structured features and access to threat labels during training, GRL-based model improves the detection performance of the SM for DFT. On the other hand, SM with BFT serialization shows the best performance of all the methods considered in this paper. Overall, all of our proposed approaches address the limitations of AVs, byte-level and AST-based approaches as well as token-based approaches in the literature by leveraging advanced code parsing capabilities and deep learning (DL) techniques to detect script malware attacks. Our contributions can be summarized as follows: 1. We introduce novel feature extraction techniques tailored for server-side script languages, including a syntactic highlighting-based extractor that represents code functionality as a sequence and an AST-based extractor that captures deeper understanding of code through hierarchical syntactic representation. These extractors serve as programming language tokenizers and are integrated with a sequential neural network, enabling an understanding of code rather than relying solely on pattern matching techniques. We further incorporate this hierarchical structure into embeddings by GRL. To the best of our knowledge, these approaches are novel in the realm of malware detection. 2. We have curated a comprehensive collection of malicious and benign scripts. Our extensive evaluations, coupled with comparisons to existing methodologies, demonstrate that our approaches: • Outperform a commercial AV, an open source AV, ML-based byte-level malware detectors, AST-based sequential and graph neural network detectors, and a pre-trained CodeBERT-based malware detector. • Provide coverage for more than 95% of high-priority threats, including cryptominers, ransomware, and credential stealers."
https://arxiv.org/html/2411.08171v1,Comprehensive and Comparative Analysis between Transfer Learning and Custom Built VGG and CNN-SVM Models for Wildfire Detection,"Contemporary Artificial Intelligence (AI) and Machine Learning (ML) research places a significant emphasis on transfer learning, showcasing its transformative potential in enhancing model performance across diverse domains. This paper examines the efficiency and effectiveness of transfer learning in the context of wildfire detection. Three purpose-built models – Visual Geometry Group (VGG)-7, VGG-10, and Convolutional Neural Network (CNN)-Support Vector Machine(SVM) CNN-SVM – are rigorously compared with three pretrained models – VGG-16, VGG-19, and Residual Neural Network (ResNet) ResNet101. We trained and evaluated these models using a dataset that captures the complexities of wildfires, incorporating variables such as varying lighting conditions, time of day, and diverse terrains. The objective is to discern how transfer learning performs against models trained from scratch in addressing the intricacies of the wildfire detection problem. By assessing the performance metrics, including accuracy, precision, recall, and F1 score, a comprehensive understanding of the advantages and disadvantages of transfer learning in this specific domain is obtained. This study contributes valuable insights to the ongoing discourse, guiding future directions in AI and ML research.","Transfer learning has emerged as a focal point in contemporary Artificial Intelligence (AI) and Machine Learning (ML) research due to its transformative impact on model performance across diverse domains [1]. The collective efforts of leading companies and research institutions underscore the pivotal role transfer learning plays in enhancing efficiency across various facets of human life. This surge in popularity can be attributed to the adaptability and efficiency transfer learning imparts to AI and ML applications. In the context of AI/ML, transfer learning refers to leveraging pretrained models initially trained on a source dataset and applying them to a target dataset with shared domain characteristics [2]. Notably, in computer vision [1], models are often trained on various iterations of the ImageNet dataset [3], which comprises an extensive array of object classes, including animals, everyday objects, and specific species of flora and fauna. This dataset continually evolves, incorporating new classes and images to enhance its comprehensiveness. Researchers have dedicated significant efforts to developing models capable of accurately classifying objects within the ImageNet dataset, achieving remarkable accuracies often exceeding 99\% [4]. For professionals engaged in specific classification tasks, such as identifying microorganisms or various car models, transfer learning offers a notable advantage. Rather than undertaking the laborious process of training a model from scratch, practitioners can access pretrained neural network weights and adapt them to specific domain-related tasks. Transfer learning presents broad applicability across diverse fields of study, including computer vision, Large Language Models (LLMs), language translation, and chatbots. Figure 1: Sample of images in the training dataset. These images include synthetic images created by data augmentation techniques like rotation, translation, scaling, brightness adjustment, and the introduction of Gaussian noise (for more details visit [1]). I-A Computer Vision and Language Models Computer Vision: Transfer learning significantly impacts computer vision by leveraging pretrained models on extensive datasets such as ImageNet. This approach expedites the learning process, enhancing efficiency in tasks such as object detection, image segmentation, and facial recognition [1]. The ability to transfer knowledge from one visual domain to another enables models to discern intricate patterns and features with remarkable accuracy. Large Language Models (LLMs): In natural language processing and LLMs, transfer learning has revolutionized the landscape. Models like Bidirectional Encoder Representations from Transformers (BERT) are pretrained on vast corpora, enabling them to grasp intricate language nuances and contextual relationships. These pretrained language models serve as a foundation for diverse Natural Language Processing (NLP) tasks, including sentiment analysis, text summarization, and question answering. Language Translation: Transfer learning proves instrumental in language translation tasks. By pre-training models on multilingual datasets, these models gain an understanding of language structures and semantics across different languages. Fine-tuning for specific language pairs leads to more accurate and contextually relevant translations, contributing to the development of more accessible and effective communication tools. Chatbots: The development of intelligent chatbots benefits significantly from transfer learning. Pretrained models, equipped with extensive linguistic knowledge, comprehend user queries, understand context, and generate coherent responses. This approach mitigates the need for exhaustive training on specific dialog datasets, allowing developers to create chatbots adept at natural language understanding and generation. I-B Pretrained Models vs Custom Built Models Utilizing pretrained models offers substantial benefits in terms of data efficiency, as these models are initially trained on extensive and varied datasets, enabling them to grasp general features and patterns. This proves advantageous when dealing with limited labeled data for a particular task, as the pretrained model has already acquired useful representations from a broader context. Transfer learning further streamlines the process by taking a pretrained model and fine-tuning it on a task-specific dataset, leading to faster convergence and requiring less labeled data compared to training from scratch. The efficiency gains extend to computation time and resources, as training large neural networks from the beginning is computationally intensive, whereas pretrained models save time by having completed a significant portion of the learning process. Additionally, pretrained models serve as effective feature extractors, particularly leveraging lower-level features that are transferable across different tasks. The models’ ability to generalize well to diverse inputs, especially when the pre-training dataset aligns with the target domain, contributes to improved performance on new, unseen data. Furthermore, in scenarios involving related but not identical domains, pretrained models support domain adaptation through fine-tuning, enabling effective performance on specific tasks within the target domain. The collaborative nature of the research community enhances accessibility, with many open-source pretrained models in computer vision available for practitioners, fostering the development and utilization of state-of-the-art models for diverse applications. I-C Contributions This paper applies and studies the performance measure of transfer learning in the context of wildfire detection. To assess the performance measure of transfer learning utilization, three custom-built models, namely, Visual Geometry Group (VGG) VGG-7, VGG-10, and Convolutional Neural Network (CNN)-Support Vector Machine(SVM) CNN-SVM are compared relative to three pretrained models – VGG-16, VGG-19, and Residual Neural Network (ResNet) ResNet101. The proposed study shows that transfer learning can successfully capture the complexities of wildfires, incorporating challenging variables (e.g., time of day, varying lighting conditions, and diverse terrains) as presented in Fig. 1. In this paper, we show how transfer learning can address the intricacies of the wildfire detection when compared to custom-built models trained from scratch. Performance metrics, such as accuracy, precision, recall, and F1 score, are used to provide a comprehensive understanding of transfer learning advantages and disadvantages in the wildfire detection domain. I-D Structure The rest of the paper is organized as follows: Section II describes the problem formulation and dataset preprocessing. Section III illustrates the research methodology and segmentation. Section III presents custom built CNN-SVM, VGG-7, and VGG-10, and pretrained VGG-16, VGG-19, and ResNet101. Section IV illustrates the comparative results, performance analysis, and test cases. Finally, Section V concludes the work."
https://arxiv.org/html/2411.08147v1,Large Language Models Can Self-Improve in Long-context Reasoning,"Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose SeaLong, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of SeaLong, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, SeaLong achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.111The repository can be accessed at https://github.com/SihengLi99/SEALONG.","Prompt Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct HotpotQA MuSiQue 2WikiMQA HotpotQA MuSiQue 2WikiMQA Default 55.5 33.0 66.0 60.0 54.0 77.0 Direct answer 49.0 28.5 55.0 61.5 51.5 74.0 Think step-by-step (Kojima et al., 2022) 62.5 50.5 77.5 75.5 62.5 85.0 Fact-and-reflection (Zhao et al., 2024b) 67.0 49.0 76.5 78.0 62.0 84.0 Plan-and-solve (Wang et al., 2023a) 64.0 49.5 82.0 74.0 68.5 85.5 Table 1: Comparison of various prompting methods. The best result is highlighted in bold. Large language models (LLMs) with long-context processing capabilities have spurred a range of novel applications, such as repository-level coding assistance (Jimenez et al., 2024), multi-document analysis (Wang et al., 2024b) and autonomous agents (Ma et al., 2024). Delivering high-quality service in these domains requires LLMs to reason effectively over long contexts, necessitating the retrieval of essential details and integration of dispersed information throughout the reasoning process. While recent advancements have enabled LLMs to attain near-perfect accuracy on the needle-in-a-haystack (NIAH; Kamradt (2023); Li et al. (2024b)) task (Hsieh et al., 2024; Yen et al., 2024b; Dubey et al., 2024), which involves locating evidence within vast amounts of irrelevant text, substantial performance declines persist on tasks that require reasoning over long contexts (Levy et al., 2024; Hsieh et al., 2024; Vodrahalli et al., 2024; Yen et al., 2024b; Li et al., 2024a), limiting their applicability in real-word scenarios. To address this limitation, recent studies have investigated fine-tuning LLMs to improve long-context reasoning, with effective data synthesis as a primary challenge. Two main approaches have emerged: one relies on human annotations (Chen et al., 2024b; Li et al., 2024c, a), which are expensive and difficult to scale; the other leverages expert models, such as GPT-4o (Hurst et al., 2024), for data synthesis. For example, Bai et al. (2024); Zhang et al. (2024c, b) apply self-instruct techniques (Wang et al., 2023b) to create long-context instruction-following data. Despite substantial progress, the dependence on pre-existing expert models limits the potential for achieving more advanced capabilities. This work investigates whether LLMs can self-improve in long-context reasoning. Drawing on evidence of LLMs’ near-perfect long-context retrieval and strong reasoning abilities in general domains (Bubeck et al., 2023; Zhong et al., 2024), we hypothesize that LLMs have untapped potential in long-context reasoning. Our preliminary studies show that refined prompting strategies achieve notable improvements over both default prompting methods and direct answer requests. Furthermore, scaling the number of sampled outputs reveals a marked performance gap between the optimal outputs and those derived via greedy search. These results suggest that LLMs hold substantial potential to advance in long-context reasoning. Inspired by these observations, we propose a Self-improving method for rEAsoning over LONG-contexts (SeaLong). This involves first sampling multiple reasoning trajectories from the LLM, then scoring each based on Minimum Bayes Risk (MBR) (Bickel and Doksum, 1977), which prioritizes outputs that are more consistent with others. This idea is intuitive, as reasoning trajectories that deviate from the majority are more likely to be hallucinations (Manakul et al., 2023; Farquhar et al., 2024). Following this, we can either conduct supervised fine-tuning using high-scoring outputs or apply preference optimization by utilizing both high-scoring and low-scoring outputs. We apply SeaLong to several leading LLMs and conduct evaluations on multiple long-context reasoning tasks (Bai et al., 2023; Yang et al., 2018; Trivedi et al., 2022; Ho et al., 2020; Dasigi et al., 2021). The results reveal that LLMs can self-improve in long-context reasoning. Specifically, SeaLong raises the score of Llama-3.1-8B-Instruct (Dubey et al., 2024) from 50.8 to 55.0. Additionally, SeaLong enables Qwen-2.5-14B-Instruct (Yang et al., 2024a) to outperform its 32B variant (54.7 vs. 53.1). In comparison to previous synthetic data, SeaLong demonstrate notable improvement without requiring human or expert model annotation. We hope that SeaLong can pave the way for self-improving approaches in long-context scenarios, supporting the continual advancement of LLM capabilities."
https://arxiv.org/html/2411.08060v1,"Online Collision Risk Estimation via Monocular
Depth-Aware Object Detectors and Fuzzy Inference","This paper presents a monitoring framework that infers the level of autonomous vehicle (AV) collision risk based on its object detector’s performance using only monocular camera images. Essentially, the framework takes two sets of predictions produced by different algorithms and associates their inconsistencies with the collision risk via fuzzy inference. The first set of predictions is obtained through retrieving safety-critical 2.5D objects from a depth map, and the second set comes from the AV’s 3D object detector. We experimentally validate that, based on Intersection-over-Union (IoU) and a depth discrepancy measure, the inconsistencies between the two sets of predictions strongly correlate to the safety-related error of the 3D object detector against ground truths. This correlation allows us to construct a fuzzy inference system and map the inconsistency measures to an existing collision risk indicator. In particular, we apply various knowledge- and data-driven techniques and find using particle swarm optimization that learns general fuzzy rules gives the best mapping result. Lastly, we validate our monitor’s capability to produce relevant risk estimates with the large-scale nuScenes dataset and show it can safeguard an AV in closed-loop simulations.","Over the past decade, autonomous vehicles (AVs) have attained great development and can be seen on public roads nowadays. However, it is still possible to hear AV accidents, especially in corner cases such as severe weather conditions or the emergence of rare objects [1]. To ensure the safety of AVs and allow their wider deployment, it is important to have run-time monitors that can identify such performance-hindering situations. Correspondingly, regulations and industrial standards such as EU AI Act [2] and ISO 21448 [3] also demand the inclusion of monitoring mechanisms in safety-critical autonomous systems. In the literature, in fact, one can find various run-time monitoring techniques. Focusing on planning and control, many work derive collision risks based on ego and traffic information, e.g., driving path deviation or time-to-collision to other agents [4]. These approaches, nonetheless, often assume perfect perception, which is generally not the case. In this work, we relax such an assumption and attempt to identify hazards as early as possible in an AV software stack, such that the controller can trigger a safety maneuver in time. Holding a similar mindset, several studies have suggested to monitor the object detection function. For instance, some propose algorithms that check the spatial or temporal consistency of the set of detected objects [5, 6]. Despite effective, one crucial limitation of the existing work is the relevance between the identified anomalies and the actual safety risk of the AV. For example, these monitors may raise a warning for an object that is far from the AV, which actually poses a low risk. Likewise, they only examine the set of detected objects and ignore potential misses (i.e., false negatives of the object detector), which are more likely risk-relevant. Figure 1: The overall monitoring framework using alignment measures between two sets of predictions to infer ego vehicle’s collision risks. The dashed lines mark an offline optimization process using a previously presented risk-correlating metric, USC [7]. Hence, in this work, we aim to find safety-related errors of the object detector and use them to characterize an AV’s collision risks. Fig. 1 depicts the overall framework. In particular, our previous work presented a design-time safety-focused metric, called uncompromising spatial constraints (USC), which highly correlates with AV collision rates [7]. This work’s objective, therefore, is to reproduce it and generate a relevant risk level during operation. To achieve this, there are two challenges: • The first and main challenge lies in the lack of ground-truth labels at run time. To overcome it, we ask the critical question whether employing a separate object retrieval pipeline and measuring the inconsistencies from the original object detector’s predictions can serve as a proxy to the ground truths. Specifically, considering cost factors and the recent breakthrough in monocular depth estimation, we employ the state-of-the-art ZoeDepth [8] and implement image processing techniques to retrieve safety-critical objects. Our key insight, thereby, is an experimental validation that confirms the inconsistencies between the two sets of predictions, measured by Intersection-over-Union (IoU) and depth discrepancy, are closely linked to safety-related errors in the 3D object detector when compared to ground-truth data. • With the confirmed correlation, the second challenge is how to associate them with the desired risk quantifier, USC. Our solution is to use fuzzy logic, which can flexibly model non-linear functions while tolerating potential imprecision [9]. Concretely, we build a fuzzy inference system (FIS) using three distinct knowledge- and data-driven approaches and finally find the one adopting the global particle swarm optimization algorithm gives the best result within a separate testing dataset. To demonstrate the efficacy of our framework, we run it with a state-of-the-art 3D object detector, PGD [10], across the large-scale nuScenes dataset [11] in six conditions, including nominal scenes, night scenes, scenes with rare objects, and scenes augmented with rain, snow, or Gaussian noises. In addition, we implement our monitor on a baseline AV in simulation and showcase that it can indeed infer a useful risk indicator that helps protect the AV. Altogether, our work can be seen as a novel attempt to derive collision risk estimates from the object detection function using a single data source, i.e., images from one monocular camera. Moreover, as we shall show in later sections, the framework offers good interpretability and adaptability, allowing developers to continuously analyze and improve the monitor."
https://arxiv.org/html/2411.08041v1,GraphAide: Advanced Graph-Assisted Query and Reasoning System,"Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.","Large Language Models (LLMs) Brown et al. (2020) represent the cutting edge of generative artificial intelligence (GenAI) and machine learning research and development. They have been adopted at an extraordinary rate across a range of disciplines, including scientific research, engineering, economics, and social sciences. By enabling domain experts to utilize pre-trained, ready-to-use models, LLMs have democratized the application development process, catalyzing innovations in both scientific and technological domains. The development of LLM-based applications remains a dynamic area of research, offering superior performance in tasks such as summarization, correlation, and inference across various input sources Zhang et al. (2024). Knowledge Graphs (KGs) are formal representations of key concepts as entities and the relationships between them Berners-Lee et al. (2023); Bizer et al. (2023). KGs serve as powerful tools that can be employed in diverse user environments as foundational reference knowledge sources. However, constructing these KGs presents substantial challenges due to the scale of input data, the heterogeneity of use-case-specific concepts, and the costs associated with KG construction. LLMs have demonstrated impressive accuracy in extracting relevant entities and forming relationships, significantly contributing to KG construction. A major challenge in developing accurate and consistent capabilities based on LLMs is hallucination Yao et al. (2023), which occurs when LLMs generate non-existent facts in response to user queries. The memorization of training data by LLMs and the subsequent reliance on corpus-based heuristics are significant factors contributing to hallucinations McKenna et al. (2023). Addressing this issue is crucial for the reliability and trustworthiness of LLM-derived applications. Retrieval-augmented generation (RAG) Lewis et al. (2020) is a well-established design pattern aimed at mitigating hallucination in Large Language Models (LLMs) by providing additional context during the generation of responses to user queries. This additional context is domain-specific and acts as grounding for the model’s generative capabilities. A foundational RAG-based system employs a vector database to store embeddings of a domain-specific knowledge corpus and utilizes semantic similarity algorithms to retrieve the context relevant to the user query. This context is then combined with the user query and forwarded to the LLM, serving as a guardrail for its generation process. The RAG-based approach has demonstrated efficacy in reducing hallucination by constraining the LLM’s generative output to the localized region of the provided context. However, a vector search-based method is constrained by the semantic similarity between the query and the corpus. It fails to leverage the structural context, such as the relationships between documents within the corpus, their metadata, or the associated reasoning. Knowledge Graphs (KGs) offer a robust solution to these limitations by efficiently storing domain-specific information and establishing relationships between information sources dispersed across documents in the corpus. KGs can also significantly enhance various components of an LLM-based application, including retrieval, summarization, and inference. By integrating KGs into the design pattern, the system can utilize both the semantic and structural context, thereby improving the accuracy and consistency of the generated responses. This comprehensive approach enables a more nuanced understanding and utilization of the knowledge corpus, offering a sophisticated strategy to further reduce hallucination and enhance the application’s overall performance."
https://arxiv.org/html/2411.08028v1,Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data,"In real-world NLP applications, Large Language Models (LLMs) offer promising solutions due to their extensive training on vast datasets. However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required. To address these limitations, smaller models are typically preferred for deployment. However, their training is hindered by the scarcity of labeled data. In contrast, unlabeled data is often readily which can be leveraged by using LLMs to generate pseudo-labels for training smaller models. This enables the smaller models (student) to acquire knowledge from LLMs (teacher) while reducing computational costs. This process introduces challenges, such as potential noisy pseudo-labels. Selecting high-quality and informative data is therefore critical to enhance model performance while improving the efficiency of data utilization. To address this, we propose LLKD that enables Learning with Less computational resources and less data for Knowledge Distillation from LLMs. LLKD is an adaptive sample selection method that incorporates signals from both the teacher and student. Specifically, it prioritizes samples where the teacher demonstrates high confidence in its labeling, indicating reliable labels, and where the student exhibits a high information need, identifying challenging samples that require further learning. Our comprehensive experiments show that LLKD achieves superior performance across various datasets with higher data efficiency.","Large Language Models (LLMs) such as LLaMA Touvron et al. (2023) and GPT-4 Achiam et al. (2023) have demonstrated superior language understanding abilities in many real-world NLP applications Schopf et al. (2022); Thirunavukarasu et al. (2023); Zhao et al. (2023) due to the vast knowledge acquired from pre-training on extensive corpora. However, deploying LLMs is resource-intensive with high memory requirements, computational costs, and increased latency during inference, especially when additional fine-tuning is needed for specific tasks Shoeybi et al. (2019). To tackle these limitations, smaller models Liu (2019); Devlin (2018) are often preferred due to their lower resource demands. Nonetheless, smaller models are not as powerful as LLMs Kaplan et al. (2020) and typically require further training for specific tasks using labeled data, as they usually do not have the capacity to capture broad knowledge. Without the guidance of labeled data, self-supervised training on smaller models may lead to suboptimal performance, as these models struggle to generalize across diverse tasks and often fail to learn task-specific features effectively Goyal et al. (2019). This challenge is further hindered by the high cost of obtaining task-related labeled data. While unlabeled data is generally more abundant, it cannot be directly utilized without proper labeling, posing a significant challenge for model training. One promising approach is to use LLMs to generate pseudo-labels for unlabeled data, which can then be used to train smaller models. This strategy allows smaller models to benefit from the extensive knowledge embedded in the LLM while reducing computational costs. This process can be seen as a form of knowledge distillation Mishra and Sundaram (2021); Zhou et al. (2023). However, this approach presents challenges. Pseudo-labels generated by LLMs may be noisy or unreliable, potentially degrading the performance of the student model. In some cases, the availability of unlabeled data may also be limited, particularly for domain-specific tasks where data collection is challenging Adams et al. (2017); Xiao et al. (2018). Thus, achieving data efficiency is crucial—not only to reduce the impact of noisy pseudo-labels but also to ensure that representative data samples are selected for optimal training. A potential solution is to select data that not only has high pseudo-label quality but is also informative for the student model. However, as the student model continuously updates during training, identifying informative knowledge throughout this process remains a challenge. Several existing works have proposed methods for data selection in the knowledge distillation process Mishra and Sundaram (2021); Zhou et al. (2023); Li et al. (2021). However, most of these approaches Zhou et al. (2023); Li et al. (2021) rely on datasets with true labels and do not consider the challenge of noisy pseudo-labeled samples, which can lead to suboptimal performance. While some methods Kontonis et al. (2024); Iliopoulos et al. (2022) address unlabeled data, they often overlook the student model’s learning progress or fail to consider data efficiency. Therefore, it is beneficial to develop a method that enables the student model to learn from the most valuable data while improving the data efficiency by reducing the amount of training data required. To address these challenges, we propose LLKD that enables Learning with Less computational resources and less data for Knowledge Distillation from LLMs. It is an adaptive sample selection method for each training step that considers the student’s dynamic learning status. We prioritize samples where the teacher model exhibits high confidence in its labeling, indicating reliable pseudo-labels Mishra and Sundaram (2021), and the student model shows high uncertainty, pointing to challenging examples that require further learning Zhou et al. (2023). Specifically, we design two types of thresholds at each training step based on teacher confidence and student uncertainty, selecting overlapping samples that meet these criteria from both models’ perspectives. This data selection strategy promotes efficient knowledge transfer from the LLM to the smaller model, ensuring that the most informative samples are used for training while reducing the amount of data needed, thereby improving data efficiency. We apply LLKD to a fundamental NLP task, text classification, and present comprehensive evaluation across various datasets. The results demonstrate that LLKD significantly enhances model performance, achieving superior results with higher data efficiency."
https://arxiv.org/html/2411.08024v1,Leonardo vindicated: Pythagorean trees for minimal reconstruction of the natural branching structures,"Trees continue to fascinate with their natural beauty and as engineering masterpieces optimal with respect to several independent criteria. Pythagorean tree is a well-known fractal design that realistically mimics the natural tree branching structures. We study various types of Pythagorean-like fractal trees with different shapes of the base, branching angles and relaxed scales in an attempt to identify and explain which variants are the closest match to the branching structures commonly observed in the natural world. Pursuing simultaneously the realism and minimalism of the fractal tree model, we have developed a flexibly parameterised and fast algorithm to grow and visually examine deep Pythagorean-inspired fractal trees with the capability to orderly over- or underestimate the Leonardo da Vinci’s tree branching rule as well as control various imbalances and branching angles. We tested the realism of the generated fractal tree images by means of the classification accuracy of detecting natural tree with the transfer-trained deep Convolutional Neural Networks (CNNs). Having empirically established the parameters of the fractal trees that maximize the CNN’s natural tree class classification accuracy we have translated them back to the scales and angles of branches and came to the interesting conclusions that support the da Vinci branching rule and golden ratio based scaling for both the shape of the branch and imbalance between the child branches, and claim the flexibly parameterized fractal trees can be used to generate artificial examples to train robust detectors of different species of trees.","There is no doubt natural trees are as beautiful and inspiring as they are ingeniously optimal from the engineering point of view [1]-[6], [13]-[20]. Ensuring the most efficient water and nutrients transport from roots to leaves up to a 100m, optimally resisting fractures from gusty winds, efficiently balancing photosynthesis with transpiration while successfully branching out to maximize the access to the space and light are just some of the tree’s ingenious mechanisms that make it survive and thrive for up to thousands of years despite nature or human inflicted troubles. Many algorithmic generative tree models have been proposed [5]-[9], some of which successfully approached photo-realism quality required for the fast expanding computer graphics and animation industry. An emerging observation from these attempts is that while being so complex, the tree appears to be also beautifully simple, with its branching structure following similar recursive generative patterns that inspired attempts to model them by fractals [2]-[4], [11]-[20]. From the minimalist point of view, reconstructing the trees by fractals is the most appealing, since it offers the shortest and most essential description of how to generate the naturally looking tree structure and for that reason it is also more likely to offer explanations for the fundamental engineering principles guiding the natural tree creation, which we are keen to explore. What gives the natural tree its signature look is the branching structure eventually leading to the green leaves. Although in general we observe examples of trees with junctions splitting out into more than two branches, the dominant observation is, also reinforced by human imposed selective breeding, that of the apical dominance of the strongest stem (trunk), which typically leads to binary branch attachments or forking that leaves a pair of branches after the junction [1]. There have been many attempts to describe and explain the branching structure of the natural tree [13]-[20]. The first dating back over 500 years ago is attributed to the Renaissance painter and polymath Leonardo da Vinci, who proposed the rule of intersection area preservation when passing through any junction of the tree. In the original Italian, the eighth rule Leonardo wrote in his notebook on drawing trees reads ”Ogni biforcazione di rami insieme giunta ricompone la grossezza del ramo che con essa si congiunge”, i.e. “all the branches of a tree at every stage of its height when put together are equal in thickness to the trunk they branch from”. This da Vinci rule implies that the total sum of intersections of all the branches at the terminal level is equal to the intersection of the main trunk. In other words, if a tree’s branches were folded upward and squeezed together, the tree would look like one big trunk with the same thickness from top to bottom. Such model gained the support of the pipe model [15], that considers the tree a collection of vascular vessels connecting roots to the leaves, as well elastic similarity model [16],[17], which assumes fixed rate of branches weight-imposed deflection per unit of their length, later proven to result with the optimal (even) distribution of fracture risk implying globally minimized probability of fracture if the da Vinci rule truly holds [14]. However simple and beautiful the da Vinci rule may seem, recent reviews analyzing experimental work conclude that it does not hold in general conditions [18]. A pipe model has been partially dismissed due to the fact that the percentage of the tree cross-section responsible for the vascular transport appears to be only around 5% and its active biomass is distributed just under the surface. Earlier experimental work in [19], hinted at the evidence that the total intersection area of the children branches grows faster than the parent’s intersection. Other researchers relied on simulation, examining da Vinci’s rule at the light of bio-mechanical models. A study by R. Minamino and M. Tateno [28] computed the ratio between the cross-sectional area of the branches to the cross-sectional area of the trunk at the branching point, showing that bio-mechanical models agree with da Vinci’s rule when the branching angles of daughter branches and the weights of lateral daughter branches are small; however, the models deviate from da Vinci’s rule as the weights and/or the branching angles of lateral branches increase. Grigoriev et al. [20] disputed that the length of the branch is not sufficiently accounted for in the branching models, while pointing at the evidence that longer than expected branches are also thinner than what would be expected from the da Vinci rule. Based on the Fourier intensity plot analysis of the tree images he further claims that the tree branching structures follow the behavior of the logarithmic fractal structures that preserve the rule of the total surface area when crossing through the junctions instead of da Vinci’s intersection area [20]. This major adjustment proposition seems to explain much broader tree species including very different branching structures of the slender birch and the bulky branches of the oak trees. This theory received some backing of some micro-biologist community on the grounds that the alive part of the tree truly resides in its outer layer just below the surface, hence it is the branches’ surface area instead of intersection what appears to best guide the branching structures of the natural trees. Recently, S. Sopp and R. Valbuena [29] proposed a metabolic approach to organism scaling and described a model based on allometry. The idea is that real trees do not preserve branches shape or size, but rely on an external metric, hydraulic resistance, adjusting the rate of conduit widening and coalescence to preserve it. According to the authors, hydraulic resistance preservation optimizes the carbon employed to develop the tree vascular system. From this principle, Sopp and Valbuena derived an elegant inverse relationship between the widening of conduits from the tree top to base and the tapering of branch volume from the base to the top. Their relationship contradicts - or better, improves on - the da Vinci rule, providing results closer to experimental evidence on tree growth. Still, even Sopp and Valbuena’s inverse relation achieves its elegance at the expense of some simplification, as their underlying growth model limits conduit coalescence to the distal part of the conductive system. In the future, more complex metabolic models of tree growth may emerge that improve accordance with experimental data by taking into account tissue micro-structures or even plant genetics. Still, the increase in complexity is likely to make such models cumbersome. In this paper we take a different approach: rather than using advanced knowledge on plant metabolism to derive a rule that matches available experimental data, we improve on da Vinci’s rule by providing an agile mathematical framework for the construction of branching structures; then we use Machine Learning to assess their closeness to natural ones. We aim to show that the intuition behind da Vinci’s rule still holds: mathematically minimal reconstructions can provide pictorial representation of natural branching structures that provide the visual experience of harmony and beauty of the natural tree shape. With all the evidence at our disposal, Bosmans’s Pythagorean tree fractals [11] appear to be the best starting point of our investigation focused on the simplest and most realistic fractal reconstruction of the natural tree. The remainder of the paper is organized as follows. A general description of Pythagorean trees model for generating fractal trees with visual examples is provided in Section 2. Their parametric generalization accounting for da Vinci’s rule and variable branching angle, along with an efficient recursive software implementation are introduced in Section 3. Section 4 covers detailed experiments with several Convolutional Neural Networks (CNN) transfer-trained to detect natural trees in images and deployed to classify various families of generalized fractal trees generated from our parametric model and attempts to identify which parametric combinations produce the most naturally looking fractal trees. Finally brief conclusions and plans for further research are drawn in Section 5."
https://arxiv.org/html/2411.08003v1,Can adversarial attacks by large language models be attributed?,"Attributing outputs from Large Language Models (LLMs) in adversarial settings—such as cyberattacks and disinformation—presents significant challenges that are likely to grow in importance. We investigate this attribution problem using formal language theory, specifically language identification in the limit as introduced by Gold and extended by Angluin. By modeling LLM outputs as formal languages, we analyze whether finite text samples can uniquely pinpoint the originating model. Our results show that due to the non-identifiability of certain language classes, under some mild assumptions about overlapping outputs from fine-tuned models it is theoretically impossible to attribute outputs to specific LLMs with certainty. This holds also when accounting for expressivity limitations of Transformer architectures. Even with direct model access or comprehensive monitoring, significant computational hurdles impede attribution efforts. These findings highlight an urgent need for proactive measures to mitigate risks posed by adversarial LLM use as their influence continues to expand.","References [1] Dana Angluin. Inductive inference of formal languages from positive data. Information and Control, 45(2):117–135, 1980. [2] Umar Anwar, Aziz Saparov, Julia Rando, Daniel Paleka, Michael Turpin, Pete Hase, et al. Foundational challenges in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932, 2024. [3] Robert Axelrod and Radoslav Iliev. Timing of cyber conflict. Proceedings of the National Academy of Sciences, 111(4):1298–1303, 2014. [4] Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020. [5] Alexander Bick, Adam Blandin, and David J. Deming. The rapid adoption of generative ai. Working Paper w32966, National Bureau of Economic Research, Cambridge, MA, September 2024. [6] Rishi Bommasani, Dilara Soylu, Thomas I. Liao, Kathleen A. Creel, and Percy Liang. Ecosystem graphs: The social footprint of foundation models. March 2023. [7] Manuel Cebrian. A time-critical crowdsourced computational search for the origins of covid-19. Nature Electronics, 4(7):450–451, 2021. [8] Nicholas A Christakis and James H Fowler. Connected: The surprising power of our social networks and how they shape our lives. Little, Brown Spark, 2009. [9] Brian Edwards, Allen Furnas, Steve Forrest, and Robert Axelrod. Strategic aspects of cyberattack, attribution, and blame. Proceedings of the National Academy of Sciences, 114(11):2825–2830, 2017. [10] E.M. Gold. Language identification in the limit. Information and Control, 10(5):447–474, 1967. [11] Kent Johnson. Gold’s theorem and cognitive science. Philosophy of Science, 71(4):571–592, 2004. [12] Jon Kleinberg and Sendhil Mullainathan. Language generation in the limit. arXiv preprint arXiv:2404.06757, 2024. [13] William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah Smith. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. arXiv preprint arXiv:2010.09697, 2020. [14] Mark Ed Newman, Albert-László Ed Barabási, and Duncan J Watts. The structure and dynamics of networks. Princeton university press, 2006. [15] Oak Ridge Leadership Computing Facility. Frontier supercomputer. [16] Bin Peng, Srikumar Narayanan, and Christos Papadimitriou. On limitations of the transformer architecture. arXiv preprint arXiv:2309.06863, 2023. [17] Nicole Perlroth. This is how they tell me the world ends: The cyberweapons arms race. Bloomsbury publishing, 2021. [18] Iyad Rahwan, Manuel Cebrian, Nicholas Obradovich, Josh Bongard, Jean-François Bonnefon, Cynthia Breazeal, Joshua W Crandall, Nicholas A Christakis, Iain D Couzin, Michael O Jackson, et al. Machine behaviour. Nature, 568(7753):477–486, 2019. [19] Yuval Shavit, Shuchi Agarwal, Miles Brundage, Saurabh Adler, Courtney O’Keefe, Riley Campbell, and David G Robinson. Practices for governing agentic ai systems. Research Paper, OpenAI, December 2023, 2023. [20] Lutz Strobl, William Merrill, Gregory Weiss, Daniel Chiang, and Dana Angluin. What formal languages can transformers express? a survey. Transactions of the Association for Computational Linguistics, 12:543–561, 2024. [21] Fabio Urbina, Filippa Lentzos, Cédric Invernizzi, and Sean Ekins. Dual use of artificial-intelligence-powered drug discovery. Nature Machine Intelligence, 4(3):189–191, 2022. [22] Marcin Waniek, Petter Holme, Manuel Cebrian, and Talal Rahwan. Social diffusion sources can escape detection. Iscience, 25(9):104956, 2022. [23] Marcin Waniek, Petter Holme, Katayoun Farrahi, Rémi Emonet, Manuel Cebrian, and Talal Rahwan. Trading contact tracing efficiency for finding patient zero. Scientific reports, 12(1):22582, 2022. [24] Jiajia Xu, Alice Smith, Liam Johnson, and Kevin Lee. Autoattacker: A large language model guided system to implement automatic cyber-attacks. arXiv preprint arXiv:2403.01038, 2024."
https://arxiv.org/html/2411.07983v1,"Gini Coefficient as a Unified Metric for Evaluating 
Many-versus-Many Similarity in Vector Spaces","We demonstrate that Gini coefficients can be used as unified metrics to evaluate many-versus-many (all-to-all) similarity in vector spaces. Our analysis of various image datasets shows that images with the highest Gini coefficients tend to be the most similar to one another, while images with the lowest Gini coefficients are the least similar. We also show that this relationship holds true for vectorized text embeddings from various corpuses, highlighting the consistency of our method and its broad applicability across different types of data. Additionally, we demonstrate that selecting machine learning training samples that closely match the distribution of the testing dataset is far more important than ensuring data diversity. Selection of exemplary and iconic training samples with higher Gini coefficients leads to significantly better model performance compared to simply having a diverse training set with lower Gini coefficients. Thus, Gini coefficients can serve as effective criteria for selecting machine learning training samples, with our selection method outperforming random sampling methods in very sparse information settings.","Similarity in vector spaces quantifies how closely two or more vectors resemble each other. Similarity is often quantified using metrics such as cosine similarity, dot product, or Euclidean distance Strang (2014). These measures are crucial for machine learning objectives like clustering, classification, and recommendation systems, as they help describe relationships between data points within their vector space Strang (2019); Hardt & Recht (2022). Similarity search is an important one-versus-many method used to find items in a dataset that are similar to a query item Zezula et al. (2010). Yet, there is not a sufficient metric to assess the quality of many-versus-many similarity. For example, there is an insufficient method and metric to determine how similar all vectors in a dataset are with one another. Herein, we propose the use of the Gini coefficient as a unified metric for assessing many-versus-many similarity in vector space (Figure 1). Figure 1: Illustration of our proposal: the Gini coefficient can serve as a unified, singular metric to assess the many-versus-many similarity of vectors. The calculation involves the ratio between the area under the line of equality A (light blue) and the area under the Lorenz curve B (dark blue). The Gini coefficient for many vectors is determined by the ratio of these areas: A/(A+B)."
https://arxiv.org/html/2411.07955v1,"How to discover short, shorter, and the shortest proofs of unsatisfiability: a branch-and-bound approach for resolution proof length minimization","Modern software for propositional satisfiability problems gives a powerful automated reasoning toolkit, capable of outputting not only a satisfiable/unsatisfiable signal but also a justification of unsatisfiability in the form of resolution proof (or a more expressive proof), which is commonly used for verification purposes. Empirically, modern SAT solvers produce relatively short proofs, however, there are no inherent guarantees that these proofs cannot be significantly reduced. This paper proposes a novel branch-and-bound algorithm for finding the shortest resolution proofs; to this end, we introduce a layer list representation of proofs that groups clauses by their level of indirection. As we show, this representation breaks all permutational symmetries, thereby improving upon the state-of-the-art symmetry-breaking and informing the design of a novel workflow for proof minimization. In addition to that, we design pruning procedures that reason on proof length lower bound, clause subsumption, and dominance. Our experiments suggest that the proofs from state-of-the-art solvers could be shortened by 30—60% on the instances from SAT Competition 2002 and by 25—50% on small synthetic formulas. When treated as an algorithm for finding the shortest proof, our approach solves twice as many instances as the previous work based on SAT solving and reduces the time to optimality by orders of magnitude for the instances solved by both approaches.","For the last two decades, the field of propositional satisfiability has experienced explosive growth, which has led to a wide range of applications of SAT solvers. Some of the common examples include model checking (?), AI planning (?), and combinatorial designs (?). A trait common to many domains relying on SAT solving is that unsatisfiable formulas have a meaningful domain interpretation (e.g., the correctness properties in verification domains), rather than merely being artifacts of poor modeling. However, as many of these domains have rigorous demands for the results produced by a solver, this introduces the following question: it is straightforward to check that there is a solution once it is reported, but how to check the claim that there are none? Figure 1: CaDiCaL proof length comparison against the shortest proofs on random unsatisfiable 3-CNFs. These developments created a demand for independent verification of the solver output. The standard way of achieving this is known as proof logging (?), referring to the techniques that record the sequence of inferences a solver makes while establishing unsatisfiability. Established tools, such as DRAT-trim (?), then use this information to verify that the solver has made no incorrect inferences by ensuring that the proof log adheres to the standard imposed by the checker, thus playing a crucial role in building trust in the reliability of modern SAT solvers. Thus, the role of a SAT solver now extends beyond reporting a SAT/UNSAT result; it can also be seen as an inference engine within some pre-defined formal proof systems. Some common examples include resolution (?), extended resolution (?), or resolution asymmetric tautology (?). The proof systems provide a formal underpinning to the proof-logging techniques, as they establish the inferences the solver can declare during the proof logging. While SAT solvers are commonly used to derive proofs and empirically succeed in finding short proofs, the discovered proofs are not necessarily the shortest available ones. To substantiate this point, we have compared the proofs for small random unsatisfiable 3-CNF formulas produced by CaDiCaL (?), a state-of-the-art SAT solver, with the shortest possible proofs for these formulas discovered with our approach. This comparison is summarized in Figure 1; for formulas with \geq 30 clauses, CaDiCaL proofs are typically at least 50\% longer than the optimal ones, with many instances exhibiting at least twice the optimal length. These results reveal that the CaDiCaL proofs can be short yet far from being the shortest. In computational terms, this result not only indicates the opportunity to improve CaDiCaL reasoning on these formulas but also estimates the “room for improvement” in terms of the number of solver steps. Given that, we can think about finding proofs of unsatisfiability as solving an optimization problem, where the feasible set is the set of all proofs starting from a given formula and the objective is the proof length. Unfortunately, discovering short proofs is intuitively an exponentially harder problem than discovering valid proofs—even a single proof may be exponentially larger than the input formula (?)—which is a likely reason why few works report successful proof minimization results. The seminal work by ? (?) addresses this problem for resolution proofs: they propose a SAT encoding for valid proofs of a specific formula, which can be submitted to a SAT solver to either discover a resolution proof of a given length or show that none exists. This encoding, coupled with appropriate symmetry-breaking constraints, has been successfully used to establish upper bounds on the shortest proof length for a given number of clauses. However, the aforementioned approach suffers from two significant limitations. First, as the search space for this problem grows with the size of the formula and the length of proof, the empirical evaluation of this approach suggests that it is feasible only for formulas with up to a dozen clauses. Second, this approach enumerates lower bounds on the proof length until the proof of the target length is discovered, which is then declared optimal; in particular, at no time before the termination, is there any “short” proof that could be returned, for example, when the allotted time runs out. Their work also points out one of the major reasons why the proof minimization problem is difficult, namely, that the search space is highly symmetric, and proposes symmetry-breaking constraints to remedy this issue. While this technique has substantially improved the performance of a SAT solver on the proof minimization problems, we point out that this does not resolve all permutation symmetries. This observation suggests a direction for further algorithmic improvements that we pursue in our methodology. In this paper, we propose a novel branch-and-bound algorithm for minimizing the proof length that addresses all the aforementioned concerns. The key contributions supporting this approach are (a) a representation of the proof breaking all symmetries stemming from clause permutations, (b) a procedure for deriving lower bounds on the proof length that generalizes the proposal from ? (?) for the case of arbitrary formulas, which we use in our approach to stop searching for proofs once the bound becomes high enough, and (c) a dominance relation on proof prefixes that detects the proof steps that cannot improve upon the proofs explored earlier in the search. Combined in a single workflow, this approach is capable of reducing the proof length of a state-of-the-art solver by 25% to 50% on synthetic formulas (e.g., 3-CNFs, graph coloring formulas), and by 30% to 60% on UNSAT formulas of SAT Competition 2002. In addition, this algorithm substantially improves the complete enumeration of proofs, as suggested by the comparisons with the encoding of ? (?), where our approach solves twice as many instances and terminates faster by at least an order of magnitude in the majority of solved instances. We also discover a limitation of our approach that is connected to the memory consumption of the resolution proofs, namely, that it works consistently until the proofs exceed 10^{6} steps. The remainder of the paper is structured as follows. Section 2 introduces the key concepts of propositional logic and the necessary context about the resolution proof system. We review the state-of-the-art approaches to proof minimization, as well as further context about the proof complexity, in Section 3. The main novel contribution of this paper is introduced in Section 4, which describes the branch-and-bound procedure for enumerating the resolution proofs of an unsatisfiable formula and the optimizations used within it to improve the search time. We evaluate the performance of our approach, both for the time to optimality and for the proof length, in Section 5. Finally, we conclude and discuss further research directions in Section 6."
https://arxiv.org/html/2411.07942v1,"Towards Low-bit Communication for
Tensor Parallel LLM Inference","Tensor parallelism provides an effective way to increase server large language model (LLM) inference efficiency despite adding an additional communication cost. However, as server LLMs continue to scale in size, they will need to be distributed across more devices, magnifying the communication cost. One way to approach this problem is with quantization, but current methods for LLMs tend to avoid quantizing the features that tensor parallelism needs to communicate. Taking advantage of consistent outliers in communicated features, we introduce a quantization method that reduces communicated values on average from 16 bits to 4.2 bits while preserving nearly all of the original performance. For instance, our method maintains around 98.0% and 99.5% of Gemma 2 27B’s and Llama 2 13B’s original performance, respectively, averaged across all tasks we evaluated on.","The use of large language models (LLMs) [6, 8, 20, 21] has ballooned in countless areas due to their impressive capabilities. Even so, with the enormous size of server LLMs, inference-time efficiency becomes a dire issue for those who own the models and those who use them. Fortunately, techniques like sequence parallelism [9] and tensor parallelism [14, 19] distribute the computational load of transformer-based LLMs [23] onto different devices. However, these methods require communication between devices, which is especially a concern when serving models, so cheaper networking would greatly cut costs. In addition, as LLMs increase in size, we need to distribute them over more devices, which further drives up communication costs. A natural idea is to try quantization, but this comes with challenges. Quantization methods for LLMs have largely focused on weights [4, 7, 10, 15, 18] or multiplication between low-bit weights and low-bit input features [1, 5, 24, 25, 27]. These methods are most useful for hardware-constrained settings, but their savings are not as relevant to tensor parallel server LLMs. In particular, current LLM quantization methods keep the output features of each attention and feedforward block at high precision (BF16 or FP16) to preserve performance. However, this is exactly what needs to be communicated in tensor parallelism. There has been work in quantized communication for distributed inference [12, 13, 16], but applications in LLMs have been limited. Consequently, the main challenge with quantization is to find a way to communicate low-bit output features from tensor parallelized attention and feedforward blocks across devices while preserving the original model’s performance. Thankfully, there are a couple observations that we can leverage. First, the communicated features have consistent structures. Looking at the aggregated quantization ranges for each feature across a calibration set in Figure 1 (details in Section 3), we observe that a small number of features have enormous ranges, potentially resulting in large quantization errors. Second, tensor parallelism can counteract feature quantization error. Theoretically, instead of uniformly distributed quantization errors, quantizing the partial sums prior to synchronization results in aggregated errors that follow the Irwin-Hall distribution, approximately Gaussian as the number of devices increases. This means tensor parallelism synchronization pushes quantization errors to be clustered around 0. We take advantage of both observations in our method design. Figure 1: Sorted aggregated quantization ranges, \bar{\bm{R}}_{j}, of each each attention (left) and feedforward (right) block in Gemma 2 27B, with the mean across all layers in red. Values are scaled such that the max range for each layer is set to 1. We propose a solution to reduce tensor parallelism communication cost from 16 bits to less than 4.2 bits per value with minimal performance degradation. Inspired by the extra care taken for outlier features in many LLM quantization methods, the main idea of our algorithm is to determine a static set of features that are kept in BF16 while quantizing everything else to 4 bits without perturbing the weights. In Section 2, we formulate the problem with compressed communication for tensor parallelism. Next, Section 3 illustrates our method to select features to maintain at higher precision and how they are used during inference. Then, Section 4 showcases the performance of our method across multiple LLMs and tasks. For example, our method preserves around 98% of Gemma 2 27B’s original performance despite communicating only about 1/4 of the information."
https://arxiv.org/html/2411.07940v2,"Automatic dataset shift identification to support
root cause analysis of AI performance drift","Shifts in data distribution can substantially harm the performance of clinical AI models. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, root causes of dataset shifts are varied, and the choice of shift mitigation strategies is highly dependent on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework, effectively distinguishing between prevalence shift (caused by a change in the label distribution), covariate shift (caused by a change in input characteristics) and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the importance of self-supervised encoders for detecting subtle covariate shifts and propose a novel shift detector leveraging both self-supervised encoders and task model outputs for improved shift detection. We report promising results for the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts, using four large publicly available datasets.","Machine learning models are notoriously sensitive to changes in the input data distribution, a phenomenon commonly referred to as dataset shift [1, 2, 3, 4]. This is particularly problematic in clinical settings, where dataset shift is a common occurrence and may arise from various factors [5]. Changes in the frequency of disease positives over time or across geographical regions cause prevalence shift [6, 7] (also known as label shift). The use of different acquisition protocols or scanners [8, 9, 10], or a change in patient demographics [11, 12] can induce shifts in image characteristics, known as covariate shift. We illustrate examples of real-world shifts in Fig. 1. Dataset shift can dramatically affect the performance of AI and lead to clinical errors such as misdiagnosis [13, 4, 14]. It is recognised as the fundamental barrier hindering AI adoption [15, 16]. It is hence crucial to implement safeguards allowing not only effective detection of the presence of shifts, but importantly, reliable identification of the root causes. Comprehensive shift detection and identification frameworks are key for the safe deployment and continuous monitoring of AI in clinical practice. Dataset shifts can be detected at deployment time by using statistical testing to compare the distributions of incoming test data to the distribution of the reference data (representative of the data used to validate the deployed AI model). Significant progress has been made in this field where state-of-the-art methods can detect various types of real-world shifts [17, 18, 19, 20]. Shifts between test and reference data can either be detected at the output level by comparing distributions of model outputs, or at the input level by comparing low-dimensional feature representations of input images [19] (see Methods, Dataset shift detection methods for more details). In this study, we show that different types of shifts require different shift detection approaches. On the one hand, comparing model output distributions allows for the reliable detection of shifts directly related to the downstream task, such as changes in prevalence. On the other hand, we show that for shifts orthogonal to the downstream task, such as changes in image acquisition protocols, comparing output distributions is not sufficient. For such shifts, test and reference data need to be compared at the input level using rich feature representations. We demonstrate that self-supervised neural network image encoders [21], trained without using any task-specific annotations, yield excellent low-dimensional feature representations for shift detection. Figure 1: Examples of dataset shifts in medical imaging. Reliably detecting and identifying the nature of the shift is crucial to enable the safe deployment of machine learning systems applications. In this work, we propose the first shift identification framework able to reliably detect and classify any detected shift as (i) prevalence, (ii) covariate or (iii) mixed prevalence and covariate shift. While detecting dataset shifts is important, it is insufficient for the safe deployment of AI. Besides knowing that there is a problem, we need to be able to identify its root cause to take the necessary actions, implement preventive measures, and safeguard against harm caused by AI errors. To our knowledge, no solution yet exists to identify the root cause of dataset shifts. The precise identification of the type of shifts is critical for selecting appropriate mitigation strategies. For example, prevalence shifts can often be mitigated with lightweight output recalibration techniques [14, 22, 23, 24], but these rely on the assumption that no other types of shift are present. In contrast, covariate shifts require more advanced domain adaptation techniques or model fine-tuning [25, 26, 27, 28, 4, 29]. The difficulty is that a change in image characteristics may cause similar changes in the distribution over model outputs as a change in disease prevalence [4], and determining the cause of an observed shift can be challenging. Applying the wrong mitigation technique may, in the best case, be ineffective in resolving the shift or, in the worst case, severely harm model performance or calibration. In Appendix A, we show an example of the consequences of applying a prevalence shift correction algorithm to the wrong type of shift on model calibration, further motivating the need for dataset shift identification methods. Despite its paramount importance, automatic dataset shift identification has remained an open problem. Figure 2: Overview of the dataset shift identification pipeline proposed in this work. We leverage both task model outputs and features from self-supervised encoders for detecting and identifying dataset shifts. Contrarily to previous works, we do not simply detect the presence of shifts but also add a second step able to identify the nature of the shift. Our method effectively separates cases of (i) prevalence shift (a change in label distribution); (ii) covariate shift (a change in image characteristics) and (iii) covariate and prevalence shift (both). Our shift identification pipeline can be divided into two stages: (A) shift detection, followed by (B) shift identification. For shift detection, we combine signals from model outputs and generic features (low-dimensional representations of images) to detect whether a shift is present in the test set (2). If a shift is detected we proceed to shift identification. Identification starts with estimating the prevalence in the test set (3), then we resample the reference set to match the estimated test set prevalence (4). We then first compare feature distributions between prevalence-adjusted reference and test set (5): if differences are no longer significant after adjusting the prevalence, the shift is attributed to prevalence shift. Conversely, if differences persist after adjusting the prevalence, then covariate shift is necessarily present. In this case, we compare model output distributions to determine whether prevalence shift is also present (6). Precisely, if there were significant differences in model output distributions before adjusting the prevalence, but this shift disappears after adjusting the prevalence, we know that prevalence shift is also responsible for the observed shift, in this case we conclude that the observed shift is a case of mixed shift (prevalence + covariate shifts). Else, we conclude that the shift is attributed to covariate shift only. In this work, we address this issue by proposing the first dataset shift identification framework capable of identifying the root cause of the underlying shift, effectively separating (i) prevalence shift, (ii) covariate shift and (iii) mixed shift (both prevalence and covariate shifts). Our shift identification framework consists of two stages and is summarised in Fig. 2. First, the ‘shift detection’ module identifies whether a shift is present. Secondly, given that a shift has been detected, our ‘shift identification’ module characterises the type of shift. An in-depth evaluation across three different clinical applications (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts demonstrates that our framework accurately distinguishes between prevalence shifts, covariate shifts, and mixed shifts across various scenarios. We conclude that the proposed method may play an important role towards safe deployment and clinical adoption of medical imaging AI."
https://arxiv.org/html/2411.07871v1,Leveraging Multimodal Models for Enhanced Neuroimaging Diagnostics in Alzheimer’s Disease,"The rapid advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have shown great potential in medical diagnostics, particularly in radiology, where datasets such as X-rays are paired with human-generated diagnostic reports. However, a significant research gap exists in the neuroimaging field, especially for conditions such as Alzheimer’s disease, due to the lack of comprehensive diagnostic reports that can be utilized for model fine-tuning. This paper addresses this gap by generating synthetic diagnostic reports using GPT-4o-mini on structured data from the OASIS-4 dataset, which comprises 663 patients. Using the synthetic reports as ground truth for training and validation, we then generated neurological reports directly from the images in the dataset leveraging the pre-trained BiomedCLIP and T5 models. Our proposed method achieved a BLEU-4 score of 0.1827, ROUGE-L score of 0.3719, and METEOR score of 0.4163, revealing its potential in generating clinically relevant and accurate diagnostic reports.","Degenerative diseases are conditions that gradually damage and destroy parts of the cells of the nervous system, particularly in areas such as the brain. These diseases typically develop slowly, and the effects and symptoms generally manifest in the later stages. Among the most common degenerative diseases is Alzheimer’s, which is estimated to currently affect 6.9 million Americans aged 65 or older, and remains the fifth-leading cause of death [1]. Alzheimer’s disease (AD) develops in different stages, and is the most common cause of dementia, accounting for 60-80% of all cases [2]. Currently, there is no definitive cure; however, early diagnosis of this condition can lead to a slowdown in its progression and an improvement in the patient’s quality of life. To meet the need for fast and accurate diagnoses, there is an increasing reliance on automatic diagnostic systems based on machine learning methods. Diagnostic reports provide essential textual descriptions and are important for the early diagnosis and treatment of the disease. The interpretation of these reports can indeed influence patient outcomes. However, interpreting biomedical images to generate diagnostic reports can take a considerable amount of time, even for the most experienced clinicians [1, 3, 4]. Deep learning methods, particularly those based on Convolutional Neural Networks (CNNs) and transformer architectures, have shown great potential in the detection of Alzheimer’s disease, thanks to their ability to learn complex patterns and representations from large-scale datasets [5]. However, the use of CNNs presents several limitations, such as the inability to capture long-term dependencies and the absence of an attention mechanism. Furthermore, these models are frequently criticized for their lack of interpretability. The hierarchical and non-linear nature of their processing can make it challenging to understand how they arrive at a particular decision [6]. Additionally, one of the main challenges is the efficient integration of medical images with structured data. Despite the potential benefits of combining these data, their integration can affect the quality of classification performance. To address these limitations, recent research has explored the potential of more advanced language models and multimodal approaches. These models can process and integrate various data types, showing promising results in terms of accuracy and interpretability for Alzheimer’s disease [7]. Transformers leverage their attention mechanisms to capture global contextual information, which can make them more interpretable in decision-making processes particularly for tasks that require understanding long-range dependencies or a comprehensive view of the context [8, 9, 10]. To the best of our knowledge, the generation of synthetic diagnostic reports for Alzheimer’s disease using multimodal approaches has not been previously explored in the literature. Specifically, our research question in this work is to determine how synthetic diagnostic reports can bridge the gap between existing neuroimaging datasets and the training requirements of Visual Language Models (VLMs) and Large Language Models (LLMs) for Alzheimer’s diagnosis. With this in mind, the main contributions of our work are as follows: • We generate synthetic diagnostic reports to address the lack of textual data in neuroimaging, facilitating the fine-tuning of multimodal models for Alzheimer’s diagnosis; • We propose a framework using BiomedCLIP and T5 to combine visual features from MR images with clinical descriptions, extending multimodal model applications to neuroimaging datasets with a particular focus on the OASIS; • We integrate MR images and clinical data to analyze both visual and non-visual information, capturing relationships between brain morphology and cognitive decline to improve diagnostic accuracy; • Finally, we evaluate the quality of the generated reports using BLEU, ROUGE, and METEOR metrics on the OASIS-4 dataset, using synthetic reports as ground truth. The structure of the paper is as follows. Section II provides an overview of the recent use of LLMs and VLMs for generating diagnostic reports based on biomedical images, emphasizing the challenges in neuroimages. The OASIS cohorts are presented in Section III, with a particular focus on the main cohort: OASIS-4. Section IV provides an overview of the models, techniques, and overall approach used in the study. Section V presents the results obtained from the models, analyzing their performance and the quality of the generated reports, while highlighting strengths and areas for improvement. Finally, Section VI concludes the study, discusses the model’s current limitations, and outlines some directions for future research."
https://arxiv.org/html/2411.07841v1,Federated Learning for Discrete Optimal Transport with Large Population under Incomplete Information,"Optimal transport is a powerful framework for the efficient allocation of resources between sources and targets. However, traditional models often struggle to scale effectively in the presence of large and heterogeneous populations. In this work, we introduce a discrete optimal transport framework designed to handle large-scale, heterogeneous target populations, characterized by type distributions. We address two scenarios: one where the type distribution of targets is known, and one where it is unknown. For the known distribution, we propose a fully distributed algorithm to achieve optimal resource allocation. In the case of unknown distribution, we develop a federated learning-based approach that enables efficient computation of the optimal transport scheme while preserving privacy. Case studies are provided to evaluate the performance of our learning algorithm.","Optimal transport is a fundamental framework used to design efficient resource distribution schemes between two parties, typically referred to as sources and targets [1]. Its application spans various domains, including supply chains (e.g., distributing raw materials from factories to manufacturers), task allocation (e.g., assigning tasks to employees), and more recently, domain adaptation in machine learning [2]. However, the traditional optimal transport paradigm assumes that complete information about all participants is available and that the transport network can be explicitly characterized [3]. This assumption is limiting in modern contexts, such as supply chains and machine learning, where the scale of the network can be vast and dynamic, involving many target nodes requesting resources. In such scenarios, it is often infeasible for resource providers to acquire comprehensive knowledge about the preferences of all target nodes. Therefore, the standard optimal transport framework must be extended to accommodate large-scale, heterogeneous populations of targets with incomplete information. In this paper, we present a new optimal transport framework designed to address the challenges posed by large populations with varying preferences. The heterogeneity of target nodes is represented by a type distribution function, categorizing target nodes based on their preferences for resources. We focus on two key settings: one where the target type distribution is known and one where it is unknown. In the first case, we propose a fully distributed algorithm that optimally allocates resources among the target nodes. In the second case, where the target type distribution is unavailable to the source nodes (transport planners), we introduce a federated learning approach. This approach enables the source nodes to collaboratively and efficiently update transport schemes as new information about the target nodes is gradually collected. Our federated learning algorithm is particularly advantageous in scenarios where privacy is a concern, as it allows each target node to calculate local solutions without sharing private data directly with the central planner [4]. Instead, the local solutions are aggregated to form a global transport plan. This method has practical applications in privacy-preserving systems [5] and mobile computing environments [6]. The paper is organized as follows. Section II establishes the large-scale discrete optimal transport framework for resource allocation. Section III develops a distributed algorithm to compute the optimal transport plan when the target’s type distribution is known, and Section IV proposes a federated learning algorithm to compute the solution when such information is unknown. Section V presents case studies to showcase the developed mechanism. Section VI concludes the paper. I-A Related Works I-A1 Federated Learning with Heterogeneous Data Extensive research has explored the application of federated learning algorithms to heterogeneous data. One of the primary challenges in federated learning is managing the inherent heterogeneity of data, which arises due to differences in statistical distributions, model architectures, and data representations across local devices. This variability complicates the process of aggregating local solutions at the central planner, as the non-uniform nature of the data makes it difficult to apply a standardized method across all local updates. Prior studies [7, 8, 9] have identified these challenges, categorizing them into different forms of heterogeneity, including statistical heterogeneity, model heterogeneity, and data space heterogeneity. Various strategies have been proposed to address these issues. For example, [8] and [10] offer solutions aimed at mitigating these complexities with [10] specifically introducing a clustering-based approach to enhance the robustness of the algorithm. In our work, we tackle these challenges by categorizing target nodes into distinct types based on probability distribution functions (PDFs). This categorization enables the federated learning algorithm to be efficiently applied within the optimal transport framework, even in scenarios involving large populations with incomplete information. I-A2 Optimal Transport and Federated Learning Several studies have explored the integration of federated learning with the optimal transport framework to address the inherent challenges in federated learning. For instance, [11] introduces a method called Federated Prompts Cooperation via Optimal Transport, which employs prompt learning to tackle issues such as heterogeneity. Their approach involves learning in both local and global settings to capture target preferences while also establishing consensus among target nodes. Another significant challenge in federated learning is the non-identical distribution of data across participants. In this regard, [12] proposes using optimal transport as a core learning algorithm to address these discrepancies, enhancing the efficiency and robustness of federated learning in such environments."
https://arxiv.org/html/2411.07814v1,Community Research Earth Digital Intelligence Twin (CREDIT),"Recent advancements in artificial intelligence (AI) numerical weather prediction (NWP) have significantly transformed atmospheric modeling. AI NWP models outperform physics-based systems like the IFS on several global metrics while requiring far fewer computational resources. Despite these successes, existing AI NWP models face limitations related to their training datasets and timestep choices, often leading to artifacts that hinder model performance. To begin to address these challenges, we introduce the Community Research Earth Digital Intelligence Twin (CREDIT) framework, developed at NSF NCAR. CREDIT provides a flexible, scalable, and user-friendly platform for training and deploying AI-based atmospheric models on high-performance computing systems, offering an end-to-end pipeline for data preprocessing, model training, and evaluation that democratizes access to advanced AI NWP. We showcase CREDIT’s capabilities on a new AI NWP model: WXFormer, a novel deterministic vision transformer designed to autoregressively predict atmospheric states while mitigating common AI NWP model pitfalls, such as compounding error growth, through techniques like spectral normalization, padding, and extensive multi-step training. To show the flexibility of CREDIT and to have a state-of-the-art model comparison we train the FUXI architecture within the CREDIT framework. Our results demonstrate that both FuXi and WXFormer, when trained on 6-hourly hybrid sigma-pressure level ERA5, generally outperform IFS HRES on 10-day forecasts, potentially offering significant improvements in efficiency and forecast accuracy. The modular nature of the CREDIT platform enables researchers to experiment with various models, datasets, and scaled training options, fostering collaboration and innovation in the scientific community.","Rapid advancements in artificial intelligence (AI) numerical weather prediction (NWP) have shaken the foundations of the meteorological community. Spurred by the release of the WeatherBench framework [1] on the ERA5 reanalysis dataset [2], multiple teams spanning motivated individuals [3], universities [4, 5], tech companies [6, 7, 8], non-profits [9], and government agencies [10] have developed a variety of AI NWP models that have quickly advanced and surpassed the headline global verification scores of the ECMWF integrated forecast system (IFS) global model. In addition to improved verification scores, the AI NWP models require orders of magnitude fewer computational resources to run than conventional NWP. The combination of improved forecast performance at minimal cost opens the door for a flurry of new possibilities in how we can interact with NWP models, including much larger ensembles, more rapid updates, and potentially improved forecast performance relative to traditional NWP. Unfortunately, these seemingly major advancements come with some caveats that have become more apparent once the meteorological community started meticulously investigating these AI models. Despite these significant advancements, however, a deeper look into the published AI NWP models reveals common limitations, particularly regarding the data used for training. Most of these models, including the ECMWF IFS NWP model, rely on just five state variables (temperature, u-wind, v-wind, water vapor mixing ratio, and surface pressure), while all other variables are diagnosed from those or are updated solely within parameterizations. Additionally, the IFS vertical coordinate system uses hybrid sigma-pressure levels (i.e., model levels), which follow terrain near the surface and relax to pressure levels aloft. The WeatherBench ERA5 dataset uses pressure level data instead, which works well aloft but intersects with terrain near the surface. ERA5 persists the surface values at pressure levels that are below terrain height with the exceptions of temperature and geopotential height, which are extrapolated [11]. The existing AI NWP models are still able to produce successful forecasts, but these data choices may be causing artifacts in the predictions that then have to be addressed by complex choices in terms of architecture, training procedure, time-stepping, and post-processing. Additionally, most of the published AI NWP models use a 6-hour timestep, or in the case of Pangu-Weather [8], a mixture of models each with a different timestep to delay the accumulation of regression artifacts. Stepping forward the 1-hour Pangu-Weather model results in dramatic error growth after roughly 12 hours, and the error curve with time does not follow the chaotic error growth pattern expected of physics-based models. Our approach aims to address some of the deficiencies of current AI NWP models through several key improvements. We utilize a more fit-for-purpose training dataset that better represents the complexities of atmospheric dynamics. This is complemented by a carefully selected set of input variables that capture essential meteorological processes. Additionally, we employ a computationally efficient and scalable neural network architecture, adapted to handle the intricacies of weather and climate prediction across various temporal and spatial scales. Central to our work is the Community Research Earth Digital Intelligence Twin (CREDIT) framework, developed by the Machine Intelligence Learning for Earth Systems (MILES) group at the NSF National Center for Atmospheric Research (NCAR). While other groups have released model weights and necessary code to run these models, CREDIT aims to provide a comprehensive, end-to-end pipeline. It is designed to scale on standard HPC systems while remaining easily accessible and fully supported. It is fully supported on NSF NCAR’s Derecho supercomputer, a globally recognized resource for atmospheric and climate research. This framework represents a significant step towards democratizing access to weather and climate emulation technologies. By providing a robust, user-friendly platform that encompasses the entire modeling process—from data preprocessing to model training and evaluation—CREDIT enables the broader scientific community, including researchers, educators, and enthusiasts, to engage with and contribute to advanced atmospheric modeling without the typical barriers to entry. To demonstrate the versatility and power of the CREDIT platform, we present two key components in this paper. Firstly, we showcase CREDIT’s ability to support and modify existing models from the literature, such as the FuXi model [5]. This capability allows researchers to build upon and refine established approaches within a standardized framework. Secondly, we introduce WXFormer, a new vision transformer model developed within the CREDIT framework. WXFormer is the latest advancement in deterministic AI-driven atmospheric modeling, specifically designed to autoregressively predict the state of the atmosphere at a selected time resolution. We present 6-hour intervals in this manuscript as many other models currently do and comment on challenges involved with smaller time steps. WXFormer implements several improvements to mitigate compounding error growth, such as spectral normalization of neural network layers, and integrates physical knowledge into its datasets through static variables like solar radiation at the top of the atmosphere. WXFormer incorporates padding techniques to handle the spherical nature of Earth in its global weather simulations. The model employs boundary padding along the map boundaries of [0°-360°] longitude and [-90°-90°] latitude, addressing the challenges posed by polar regions and the dateline. Additionally, spectral normalization is utilized to enhance model stability during training and inference. These design choices enable WXFormer to maintain data continuity and physical consistency across the entire globe, allowing for extended simulation periods. To effectively handle the spherical nature of the Earth in its simulations, WXFormer employs circular padding along the 0-360° longitude line, wrapping data from one edge to the opposite edge to simulate periodic boundaries. This ensures seamless transitions across the dateline. Additionally, a 180-degree shift is applied to align the data correctly at the poles before padding. The top rows from the North Pole are flipped upside down and added above the original data, while the bottom rows from the South Pole are also flipped and added below. This method guarantees smooth transitions at the poles, respecting the convergence of longitudes and preventing discontinuous seams in the data. These enhancements allow WXFormer to perform simulations over extended time periods, depending on the training protocol and goals. The selection of the CrossFormer vision transformer [12] as the backbone for WXFormer provides several advantages over vanilla ViT models. Primarily, it facilitates a hierarchical attention scheme, allowing for smaller patch sizes without dramatically increasing the model size or memory footprint. This architecture also supports efficient scaling across multiple GPUs, surpassing the capabilities of graph-based networks and enabling effective management of escalating data volumes and complexity. With WXFormer we aim to strike an optimal balance between attention window size and the number of parameters, thereby optimizing computational resources. Moreover, its design yields faster performance compared to similarly sized models, promoting rapid iterations and facilitating real-time applications in atmospheric modeling. In this manuscript, we present a comprehensive evaluation of the CREDIT framework and specifically WXFormer’s performance compared to the FuXi model, providing multiple metrics that demonstrate the model’s fidelity across different timescales, from short-term weather predictions to longer-term atmospheric state projections. We also address the model’s shortfalls, acknowledging the challenges that are pervasive in the broader landscape of autoregressive machine-learned atmospheric models. By openly discussing these limitations, we aim to foster a transparent dialogue within the community and pave the way for future improvements in AI-driven atmospheric science."
https://arxiv.org/html/2411.07796v1,PatchCTG: Patch Cardiotocography Transformer for Antepartum Fetal Health Monitoring,"Antepartum Cardiotocography (CTG) is vital for fetal health monitoring, but traditional methods like the Dawes-Redman system are often limited by high inter-observer variability, leading to inconsistent interpretations and potential misdiagnoses. This paper introduces PatchCTG, a transformer-based model specifically designed for CTG analysis, employing patch-based tokenisation, instance normalisation and channel-independent processing to capture essential local and global temporal dependencies within CTG signals. PatchCTG was evaluated on the Oxford Maternity (OXMAT) dataset, comprising over 20,000 CTG traces across diverse clinical outcomes after applying the inclusion and exclusion criteria. With extensive hyperparameter optimisation, PatchCTG achieved an AUC of 77%, with specificity of 88% and sensitivity of 57% at Youden’s index threshold, demonstrating adaptability to various clinical needs. Testing across varying temporal thresholds showed robust predictive performance, particularly with finetuning on data closer to delivery, achieving a sensitivity of 52% and specificity of 88% for near-delivery cases. These findings suggest the potential of PatchCTG to enhance clinical decision-making in antepartum care by providing a reliable, objective tool for fetal health assessment. The source code is available at https://github.com/jaleedkhan/PatchCTG","Antepartum Cardiotocography (CTG) plays a pivotal role in fetal health monitoring, serving as a critical assessment tool in prenatal care. Using ultrasound-based techniques to record Fetal Heart Rate (FHR) and uterine activity, CTG provides clinicians with data on fetal well-being through the examination of heart rate variability and response patterns to uterine contractions. Established methods like the Dawes-Redman (DR) computerised CTG system [1] offer valuable criteria for interpreting CTG patterns, enhancing clinical decisions that help mitigate risks of adverse outcomes such as neonatal acidaemia, hypoxia and stillbirth [2]. Despite its widespread adoption in clinical settings, CTG analysis suffers from high intra- and inter-observer variability. Studies indicate that clinical assessments can overlook 35-92% of FHR patterns [3, 4], and inter-observer agreement may be as low as 29%, with false positive rates reaching up to 60% [5, 6]. These limitations underscore the necessity for more reliable, objective methods of fetal monitoring. Recent advancements in artificial intelligence and machine learning, particularly deep learning, have demonstrated considerable potential in automating and improving the accuracy of CTG interpretation. By leveraging deep learning models, researchers have advanced the detection of adverse outcomes in CTG signals through feature extraction, noise reduction and classification tasks, providing more consistent assessments than manual interpretation [7]. Transformers excel at handling sequential data, including biomedical time series, due to their ability to capture complex temporal dependencies by dynamically learning correlations across input elements, which makes them highly promising for CTG analysis [8]. With self-attention mechanisms, Transformers capture complex temporal dependencies in CTG data, focusing on relevant segments of FHR and uterine activity patterns. Despite these advancements, several challenges persist in applying deep learning to CTG-based fetal health monitoring. Existing models fall short in capturing the physiological responses in CTG, largely due to signal variability across patients, monitoring conditions and clinical contexts [9]. Issues like insufficient data diversity, high computational costs and a lack of generalizability across different clinical settings often limit the performance of deep learning models on CTG data. Addressing these challenges requires specialised models that can adapt to different temporal patterns while maintaining robust performance. In this paper, we introduce Patch Cardiotocography Transformer (PatchCTG), a patch-based transformer model designed to classify adverse and normal outcomes in antepartum CTG recordings reliably. The PatchCTG model builds on recent advancements in patch-based Transformers for time series [10], which demonstrate promising performance in sequence compression and feature representation by segmenting signals into patches. Unlike traditional CTG analysis approaches, PatchCTG applies instance normalisation and channel-independent processing to manage distribution shifts and capture the distinct temporal dynamics of FHR and uterine activity. By leveraging patch-based tokenisation and self-attention, PatchCTG provides enhanced computational efficiency and adaptability to longer temporal windows, making it well-suited for CTG data, where signal length and variability pose significant modelling challenges. The main contributions of this study are as follows: • We introduce PatchCTG, a Transformer-based architecture tailored for CTG signals through patch-based segmentation, instance normalisation and channel-independent processing. This design effectively captures both local and global temporal dependencies by segmenting signals into patches, mitigating distribution shifts through instance normalisation and allowing separate modelling of FHR and uterine contraction channels. These architectural choices address the non-stationarity of CTG data, enabling more accurate classification of adverse and normal outcomes. • The PatchCTG model is rigorously evaluated on a subset of the extensive Oxford Maternity (OXMAT) dataset [11], which includes over 20,000 CTG traces with diverse clinical outcomes. Our experimentation involves cohort balancing, sequence standardisation and multiple temporal thresholds to ensure model robustness and clinical applicability. Detailed results demonstrate the promising performance of PatchCTG across various classification thresholds and time windows relative to delivery. • We employed the Optuna hyperparameter optimisation framework [12] to identify the best configuration of PatchCTG for fetal health classification. This systematic approach finetunes model depth, attention heads, embedding dimensions, dropout rates and other key parameters, ensuring that PatchCTG achieves high predictive performance while maintaining generalizability across CTG samples and cohort variations. • We benchmarked the performance of PatchCTG against the traditional Dawes-Redman algorithm [1] and an optimised hybrid deep learning model, providing a comprehensive comparison that underscores the advantages of PatchCTG in handling temporal dependencies and delivering clinically relevant sensitivity and specificity. The remainder of the paper is structured as follows: Section 2 reviews related work in antepartum fetal monitoring using machine learning. Section 3 presents the proposed PatchCTG model architecture, and Section 4 presents the experimental setup and results, including dataset preprocessing, hyperparameter optimisation, performance evaluation and benchmark comparison. Section 5 discusses the experimental results of PatchCTG, its potential impact on clinical practice and future directions for its development, which is followed by the conclusion in Section 6."
https://arxiv.org/html/2411.07739v1,"Unlocking Legal Knowledge with Multi-Layered Embedding-Based
Retrieval","This work addresses the challenge of capturing the complexities of legal knowledge by proposing a multi-layered embedding-based retrieval method for legal and legislative texts. Creating embeddings not only for individual articles but also for their components (paragraphs, clauses) and structural groupings (books, titles, chapters, etc), we seek to capture the subtleties of legal information through the use of dense vectors of embeddings, representing it at varying levels of granularity. Our method meets various information needs by allowing the Retrieval Augmented Generation system to provide accurate responses, whether for specific segments or entire sections, tailored to the user’s query. We explore the concepts of aboutness, semantic chunking, and inherent hierarchy within legal texts, arguing that this method enhances the legal information retrieval. Despite the focus being on Brazil’s legislative methods and the Brazilian Constitution, which follow a civil law tradition, our findings should in principle be applicable across different legal systems, including those adhering to common law traditions. Furthermore, the principles of the proposed method extend beyond the legal domain, offering valuable insights for organizing and retrieving information in any field characterized by information encoded in hierarchical text.","The increasing volume and complexity of legal corpora pose significant challenges for legal professionals, including those in legislative consultancy, where the efficient access and analysis of legal texts are critical. Traditional keyword-based search methods often fall short in capturing the nuances of legal language and the intricate relationships within legal documents ([1, 2]). Recent advancements in Generative Artificial Intelligence (GenAI) and Retrieval Augmented Generation (RAG) systems offer promising avenues for more efficient and accurate legal information retrieval. Embeddings, which are dense and compact vector representations of text, effectively capture the meanings of words, phrases, or documents ([3, 4]). Legislative documents, including bills and legal normative statutes, inherently have a hierarchical structure. This intrinsic hierarchy calls for an approach with variable granularity, capable of representing both smaller segments and broader groupings of legal texts through embeddings. This paper proposes a multi-layered embedding-based retrieval method that captures the semantic content of legal texts. By creating embeddings for articles,111In Brazilian law, ‘article’ is the fundamental unit within legislative texts, while in U.S. law, ‘Section’ within codes and statutes are the comparable unit of division that detail specific provisions of the law. In this work we will use the term ‘article’ as the basic unit of articulation, akin to a ‘section’ in U.S. legislation (see Figure 3). We refrain from using the term ‘section’ for the basic units since its literal translation in Portuguese (‘seção’) actually denotes a grouping of the basic units (‘artigos’). their components, and their structural groupings, we aim to provide a more nuanced and comprehensive representation of legal knowledge. Our proposed approach enables RAG models to respond to user queries with varying levels of detail, ranging from small portions to comprehensive sections."
https://arxiv.org/html/2411.07722v1,"Assessing and Mitigating Multimodal Knowledge Conflicts
in Document Understanding","Multimodal large language models (MLLMs) have shown impressive capabilities in document understanding, a rapidly growing research area with significant industrial demand in recent years. As a multimodal task, document understanding requires models to possess both perceptual and cognitive abilities. However, current MLLMs often face conflicts between perception and cognition. Taking a document VQA task (cognition) as an example, an MLLM might generate answers that do not match the corresponding visual content identified by its OCR (perception). This conflict suggests that the MLLM might struggle to establish an intrinsic connection between the information it “sees” and what it “understands.” Such conflicts challenge the intuitive notion that cognition is consistent with perception, hindering the performance and explainability of MLLMs. In this paper, we define the conflicts between cognition and perception as Cognition and Perception (C&P) knowledge conflicts, a form of multimodal knowledge conflicts, and systematically assess them with a focus on document understanding. Our analysis reveals that even GPT-4o, a leading MLLM, achieves only 68.6% C&P consistency. To mitigate the C&P knowledge conflicts, we propose a novel method called Multimodal Knowledge Consistency Fine-tuning. This method first ensures task-specific consistency and then connects the cognitive and perceptual knowledge. Our method significantly reduces C&P knowledge conflicts across all tested MLLMs and enhances their performance in both cognitive and perceptual tasks in most scenarios.","In recent years, multimodal large language models (MLLMs) (gpt, 2023; Team et al., 2023; gpt, 2024; Chen et al., 2024; Bai et al., 2023; Ye et al., 2024; Li et al., 2024a) have witnessed rapid development and have demonstrated remarkable capabilities across a wide range of multimodal tasks (Antol et al., 2015; Mathew et al., 2021; Hossain et al., 2019). Particularly in the field of document understanding (Cui et al., 2021; Xu et al., 2020; 2021; Huang et al., 2022; Gu et al., 2022; Luo et al., 2023), which has high academic and industrial value, significant research efforts with MLLMs have been made (Zhang et al., 2023a; Ye et al., 2023a; b; Luo et al., 2024; Wang et al., 2023; Hu et al., 2024), yielding promising results. As a multimodal task, document understanding requires models to accurately perceive visual content (perception) and then generate coherent responses (cognition) based on that perception. However, current MLLMs often face conflicts between perception and cognition. For example in Figure 1 (a), GPT-4o (gpt, 2024) recognizes the text in a certain region of an image as “Doral” through its OCR capability (perception) but responds to a related information extraction question with the text “Doraf” (cognition). This conflict suggests that the GPT-4o might struggle to establish an intrinsic connection between what it “sees” and what it “understands.” Statistical analysis further underscores this issue, as Figure 1 (b) shows, with leading MLLMs like GPT-4o and Qwen-VL-Max (Bai et al., 2023) achieving 69.60% and 79.98% consistency between perception and cognition (Section 3). In this paper, we define intrinsic conflicts between cognitive knowledge and perceptual knowledge within MLLMs, which result in inconsistencies in responses related to cognition and perception, as Cognition and Perception (C&P) knowledge conflicts (Section 2.1). C&P knowledge conflicts serve as a critical factor undermining the explainability of MLLM responses, as these conflicts challenge the intuitive notion that cognition is consistent with perception. Unlike previous research on multimodal knowledge conflicts (e.g., hallucination) (Zhai et al., 2024; Li et al., 2023; Guan et al., 2024; Liu et al., 2023a), which focuses solely on conflicts within either cognition or perception, we highlight, for the first time, the conflicts that arise between the two. We systematically assess C&P knowledge conflicts in the current five MLLMs (Section 3), focusing on document understanding. Here, the cognitive task is document-related VQA, while the perceptual task is OCR. The experimental results show significant C&P knowledge conflicts in current MLLMs, underscoring the need to mitigate these conflicts. To address this, a novel method called Multimodal Knowledge Consistency Fine-tuning is introduced, which includes three fine-tuning tasks (Section 4). Specifically, motivated by the Generator-Validator (GV) framework (Li et al., 2024b), we conduct two task-specific fine-tuning tasks: the Cognition Consistency task and the Perception Consistency task. The purpose of these two tasks is based on our belief that ensuring C&P consistency starts with maintaining task-specific consistency. Furthermore, to establish an inner connection between cognitive and perceptual knowledge, the third fine-tuning task is designed: the C&P Connector task. Comprehensive experiments are conducted on three open-source MLLMs across two series and two parameter sizes. The results indicate that multimodal knowledge consistency fine-tuning significantly improves C&P consistency, with all three MLLMs achieving at least a 34% improvement (Section 5.2). Moreover, in most scenarios, our method also enhances MLLM performance in both cognitive and perceptual tasks (Section 5.4). Our main contributions are as follows: • To the best of our knowledge, we are the first to identify and introduce the concept of Cognition and Perception knowledge conflicts, a form of multimodal knowledge conflicts, in MLLMs. • A systematic evaluation is conducted on current MLLMs to assess the Cognition and Perception knowledge conflicts in document understanding, showing that such conflicts are commonly present in current MLLMs. • A novel method called Multimodal Knowledge Consistency Fine-tuning is introduced to mitigate the C&P knowledge conflicts in current MLLMs. Extensive experiments on six public document understanding benchmarks in three MLLMs demonstrate the effectiveness of the proposed method."
https://arxiv.org/html/2411.07691v1,New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook,"Thanks to the explosive growth of data and the development of computational resources, it is possible to build pre-trained models that can achieve outstanding performance on various tasks, such as neural language processing, computer vision, and more. Despite their powerful capabilities, pre-trained models have also sparked attention to the emerging security challenges associated with their real-world applications. Security and privacy issues, such as leaking privacy information and generating harmful responses, have seriously undermined users’ confidence in these powerful models. Concerns are growing as model performance improves dramatically. Researchers are eager to explore the unique security and privacy issues that have emerged, their distinguishing factors, and how to defend against them. However, the current literature lacks a clear taxonomy of emerging attacks and defenses for pre-trained models, which hinders a high-level and comprehensive understanding of these questions. To fill the gap, we conduct a systematical survey on the security risks of pre-trained models, proposing a taxonomy of attack and defense methods based on the accessibility of pre-trained models’ input and weights in various security test scenarios. This taxonomy categorizes attacks and defenses into No-Change, Input-Change, and Model-Change approaches. With the taxonomy analysis, we capture the unique security and privacy issues of pre-trained models, categorizing and summarizing existing security issues based on their characteristics. In addition, we offer a timely and comprehensive review of each category’s strengths and limitations. Our survey concludes by highlighting potential new research opportunities in the security and privacy of pre-trained models.","The recent years have seen rapid advancements in large artificial intelligence (AI) models, such as large language models and large vision models, leading to the concept of pre-trained models. Today, pre-trained models are viewed as large-scale encoders with sophisticated architectures and vast numbers of parameters, initially trained with advanced objectives using extensive datasets. Due to the explosive growth of data and advancements in computing technology, researchers can now scale training data and model architecture to create pre-trained models, such as GPT (Radford, 2018; Radford et al., 2019) and BERT (Devlin et al., 2019), that store foundational knowledge in their huge parameters. By fine-tuning these models for specific tasks, the rich knowledge they encode can enhance various downstream applications. As a result, using pre-trained models has become a common practice in developing and improving task-specific models. While enjoying the convenience brought by pre-trained models, people are increasingly worried about potential security risks. For example, membership inference attacks (Liu et al., 2021)(Carlini et al., 2023) can reveal information about specific contents of a training dataset, and jailbreak attacks (Deng et al., 2024)(Rando and Tramèr, 2024)(Li et al., 2023a) can mislead model to generate harmful responses. However, in addition to various safety issues that have been widely studied in traditional models, the powerful capabilities of pre-trained models also bring new safety issues that do not exist in traditional models. Thus, it is important to fill these gaps and establish higher standards for pre-trained model protection. Some studies have been conducted to summarize the security and privacy issues in large models. However, few of them provide deep and comprehensive insight into the root causes of new safety issues in large models. We find that these new safety issues are introduced from the different training strategies and the large-scale dataset. Due to these reasons, there has been a huge gap between pre-trained models, which can be applied to different tasks, and traditional models, which focus on one specific task. For example, the “pre-trained/fine-tune/inference” strategy is popular in the current research area compared to the traditional “training/inference” strategy, wherein novel attacks emerge to attack pre-trained models in the fine-tuning process. These methods may be in line with traditional attack methods, but there are still updates in specific details, such as attacking the special training strategy of pre-trained models like Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022). There are still variations within pre-trained models. Most notably, larger models tend to exhibit stronger functionality. This raises the question: what unique security and privacy issues arise as model size increases, and why do these issues occur? Answering this is a long-term endeavor. Although it remains challenging to fully explain why large models have powerful capabilities, an initial approach could involve examining security and privacy issues in smaller pre-trained models and exploring how they generalize to larger models. This would help identify common and differing security and privacy issues, as well as similarities and differences in attack and defense strategies across model sizes. There are various concepts and multiple attack/defense methods in this field, and the boundary between pre-trained models and traditional models is often blurred. These challenges prompted us to conduct a comprehensive survey to summarize, analyze, and classify security issues of pre-trained models, along with the corresponding defensive countermeasures. Our analysis points out the unique characteristics of various attacks and defenses in this domain and the differences between different methods. In addition, we propose a novel taxonomy to categorize the state-of-the-art methods in the literature. Our contributions can be itemized as follows: • We proposed a novel taxonomy of current attack/defense techniques to pre-trained models based on their attack/defense stages and specific strategies. • We comprehensively summarized state-of-the-art attack/defense techniques based on the proposed taxonomy, showing their benefits and shortcomings. • We reviewed methods for attacking and defending pre-trained models of different scales, summarizing their commonalities and differences. • We provided critical and deep discussions on the open security and privacy issues in pre-trained models, as well as pointing out possible further research directions. We hope this work will help to comprehensively review and evaluate the security and privacy risks of pre-trained models. Establishing a standardized evaluation system would allow for more accurate risk assessments, ultimately increasing users’ confidence in pre-trained models."
https://arxiv.org/html/2411.07690v1,World Models: The Safety Perspective,"With the proliferation of the Large Language Model (LLM), the concept of World Models (WM) has recently attracted a great deal of attention in the AI research community, especially in the context of AI agents. It is arguably evolving into an essential foundation for building AI agent systems. A WM is intended to help the agent predict the future evolution of environmental states or help the agent fill in missing information so that it can plan its actions and behave safely. The safety property of WM plays a key role in their effective use in critical applications. In this work, we review and analyze the impacts of the current state-of-the-art in WM technology from the point of view of trustworthiness and safety based on a comprehensive survey and the fields of application envisaged. We provide an in-depth analysis of state-of-the-art WMs and derive technical research challenges and their impact in order to call on the research community to collaborate on improving the safety and trustworthiness of WM.","Recent years have witnessed rapid advancements in transformer-based generative models [minaee2024largelanguagemodelssurvey], with their capabilities expanding from natural language processing (NLP) to multimodal applications[wu2023multimodalllm]. Frontier models such as SORA[videoworldsimulators2024], LINGO-1[lingo1], and GAIA-1[hu2023gaia] demonstrate an unprecedented ability to generate remarkably realistic videos, suggesting an initial grasp of fundamental worldly principles like physics and spatiotemporal continuity, achieved solely through training on video and language datasets. This emerging capability opens new avenues for research, as understanding world models is crucial for developing next-generation intelligent systems. The concept of data-driven world models, initially proposed in 2017 using recurrent neural network (RNN) or long-short-term memory (LSTM) architectures[NEURIPS2018_worldmodel_ha], showed promise but was limited by constraints in sequence length, memory, and parallel capability. These early experiments demonstrated only modest capabilities in relatively simple simulated gaming environments. The advent of transformer-based methods has led to significant improvements, with recent experimental results showing encouraging progress. Consequently, many contemporary AI agent architectures now incorporate world models as a cornerstone component[lecun2022path]. Our paper focuses on world models for a specific class of agents known as embodied AI agents, which interact with the physical world. We examine these world models from the point of view of their safety, addressing a crucial gap in current research. Auto-regressive generative models suffer from inherent deficiencies such as hallucination[huang2023surveyhallucinationlargelanguage, liu2024sorareviewbackgroundtechnology]. This limitation poses significant risks in safety-critical applications like robotics and autonomous driving systems (ADS)[tian2024drivevlmconvergenceautonomousdriving], sparking controversial discussions[10531702]. Despite the current attention, we observe a lack of comprehensive analysis regarding the safety aspects of world models for embodied AI agents. Our paper aims to address this gap by providing a concise yet thorough review and investigation, followed by an in-depth analysis from a safety perspective. Finally, we identify high-priority research directions. The main contributions of this paper are summarized as follows: • We conduct a literature survey of recent achievements in world model research and show the development of techniques for realizing world models in chronological order. • We address the identified safety issues of the world models on embodied AI applications such as autonomous driving. • We propose possible approaches for future research to facilitate the future of trustworthy world models. The paper is structured as follows: Section II presents a current definition of world models, offering an in-depth investigation and taxonomy of state-of-the-art approaches. We retrospectively review the development path of modern world model approaches across various application perspectives; Section III analyzes the safety-related deficiencies of current approaches from a critical perspective; Section IV proposes a research agenda highlighting high-priority topics to improve the safety of world models. By addressing these crucial aspects, we aim to bring clarity and contribute to the current debate on world models in embodied AI and promote the development of safer and more trustworthy intelligent systems."
https://arxiv.org/html/2411.07634v1,Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel Machine Scheduling,"Scheduling problems pose significant challenges in resource, industry, and operational management. This paper addresses the Unrelated Parallel Machine Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent Reinforcement Learning (MARL) approach. The study introduces the Reinforcement Learning environment and conducts empirical analyses, comparing MARL with Single-Agent algorithms. The experiments employ various deep neural network policies for single- and Multi-Agent approaches. Results demonstrate the efficacy of the Maskable extension of the Proximal Policy Optimization (PPO) algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced scenarios, Multi-Agent approaches reveal challenges in cooperative learning but a scalable capacity. This research contributes insights into applying MARL techniques to scheduling optimization, emphasizing the need for algorithmic sophistication balanced with scalability for intelligent scheduling solutions.","Scheduling problems constitute a subset of optimization challenges that find widespread applications across various sectors, encompassing resource management [1], industry [2, 3], and operational management [4]. In particular, production scheduling, an essential facet of manufacturing, revolves around the efficient and cost-effective allocation of limited resources to support production processes. In this context, implementing flexible and intelligent strategies for industrial scheduling emerges as an imperative. The primary objective of scheduling problem optimization is to identify an advantageous combination of decision variables within a defined search space. These decision variables dictate the order in which processes or tasks are assigned to a set of machines, typically entailing complex combinatorial problems. These problems often involve optimizing multiple objectives, depending on the system demands [5]. In industrial settings, the overall aim predominantly implies minimizing job completion times while accommodating other objectives, such as resource utilization or environmental considerations. It is important to note that real-world industrial scheduling problems imply exploring extensive search spaces, usually culminating in NP-hard problem instances [6]. This inherent intricacy poses complex challenges to the research and development community. This characteristic has attracted considerable attention from the research community, as exemplified in the comprehensive survey by Allahverdi et al. [7]. Notwithstanding the advancements in this field, current approaches have notable limitations [8], encompassing computational complexity [9] and the capacity to generalize and adapt to diverse problem instances [10, 11]. In response to these constraints, contemporary research has increasingly employed advanced decision-making techniques. Deep Learning techniques have obtained significant attention in intelligent decision-making systems within the research community [12]. Specifically, Reinforcement Learning (RL) approaches have emerged as valuable solutions to addressing scheduling in complex environments [13]. This machine learning paradigm relies on utilizing an intelligent agent that learns from the actions it takes and the rewards it receives in response to those actions. Reinforcement Learning models exhibit adaptability to non-deterministic environments, making it a flexible approach for optimization within dynamic and uncertain environments. This paper presents a study of a Multi-Agent Reinforcement Learning (MARL) approach to addressing an optimal job scheduling problem. The research is performed in the Unrelated Parallel Machine scheduling problem (UPMS) with setup times and resources proposed by Fanjul et al. [14], a single-stage job scheduling problem variant. This research presents the novelty of employing a MARL approach that extends beyond traditional methods. In this sense, the work reviews the RL environment deployed in the study and conducts empirical analyses, comparing the MARL approach with various Single-Agent algorithms. The contribution aims to provide a practical evaluation of the efficacy of addressing complex decision-making problems like UPMS. The remainder of the article is structured as follows: Section 2 provides an overview of reinforcement learning on scheduling problems. Section 3 formally describes the problem addressed in the paper. Section 4 describes the implementation details. Section 5 introduces the proposed approach and implementation insights. Section 6 presents the evaluation and validation process conducted in the experiments. Finally, section 7 contains the concluding remarks and outlines some areas for further research."
https://arxiv.org/html/2411.07618v1,Direct Preference Optimization Using Sparse Feature-Level Constraints,"The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often experience computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves an above 5% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.","Aligning large language models (LLMs) with human values and practical objectives is a critical challenge in AI development (Wang et al., 2023). Post-training methods, including fine-tuning (Wei et al., 2022; Chung et al., 2024) and alignment strategies (Tunstall et al., 2023), have played a significant role in refining LLM behavior. Among these, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022) has emerged as a leading technique, integrating human feedback to guide models towards producing valuable and useful outputs. Despite its success, RLHF involves complex mechanisms such as reward modeling and policy gradients, which introduce significant training complexity and computational cost (Zheng et al., 2023b; Rafailov et al., 2024). To address these limitations, Direct Preference Optimization (DPO) (Rafailov et al., 2024) has been proposed as a more efficient alternative. Unlike reward-based methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), DPO directly adjusts the model’s output probabilities based on human preferences, reducing training complexity and computational cost. DPO-like approaches can offer a more stable and faster alignment process by bypassing the challenges associated with reward models and policy updates, making it a compelling solution for efficient LLM alignment since DPO uses a reference model to stabilize post-training. Recent advancements in DPO focus on mainly two directions: efficiency i.e., further simplifying the constraints of DPO, and controllability i.e., keeping the balance between alignment and generation diversity. In terms of simplicity, methods like SimPO (Meng et al., 2024) and Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024) eliminate the need for a reference model by using the average log probability of sequences as an implicit normalizer, thereby reducing memory usage and computational demands. However, DPO’s performance is sensitive to the strength of constraints from the reference policy (Liu et al., 2024), and these reference-free alignment approaches (Hong et al., 2024; Meng et al., 2024) can compromise control, resulting in unstable training. In terms of controllability, Token-level Direct Preference Optimization (TDPO) (Zeng et al., 2024) introduces token-level rewards and sequential Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951) to tackle issues related to linguistic coherence, diversity, and stability. However, it comes at the cost of increased computational complexity, introducing an additional sequential KL and depending on reference models, complicating the loss computation. Figure 1: Left. The DPO objective loss function and its two main improvement directions: SimPO and TDPO. SimPO focuses on simplifying the reference model, while TDPO concentrates on controlling the alignment process to enhance generation diversity. Right. The pipeline of FPO consists of sparse autoencoders and the feature-level MSE constraints. A natural hypothesis arises: “Is there a method that can strike the right balance between efficiency and controllability?” In response, we propose FPO, Feature-level Constrained Direct Preference Optimization (See Figure 1), introducing an efficient and controllable method for constraining the model at the feature level. Here a feature refers to a salient piece of information for the model decision (Huben et al., 2024). Intuitively, adjusting the model using feature-level preferences allows fine-grained adjustment that minimizes the side impact, by avoiding the negative influence of spurious features in course-grained control such as token level regularization (Zeng et al., 2024). To achieve that, we derive the FPO objective by contrasting SimPO and DPO, showing the constraint term that SimPO misses. We then add such a term by introducing the feature-level constraints as an alternative to the costly sequential KL (Zeng et al., 2024). We use Sparse Autoencoders (SAEs) (Huben et al., 2024), which generate representations where only a few features are active, enhancing computational efficiency (See Figure 2 Right). Furthermore, regularization in the coefficient space promotes sparsity, stability, and uniqueness in the model’s representations. Since SAEs produce sparse representations, only a few dozen out of 16,000 features are active at any given time (Lieberum et al., 2024). Compared to SimPO, FPO is as efficient in memory and time complexity, yet has improved controllability due to feature-level constraints; compared to constraint-based methods like TDPO, FPO matches the computational and memory efficiency of methods such as SimPO, and has potentially improved performance as feature-level control can give stronger generalization than token-level control. A contrast between FPO, DPO, SimPO and TDPO is shown in Figure 1. Our experiments demonstrate that FPO consistently outperforms state-of-the-art methods based on different sizes of backbone LLMs, achieving up to 5% absolute improvements in win rate (See Table 2) based on AlpacaEval-2 and Arena-Hard benchmarks, up to 0.5 scores on MT-Bench and competitive output diversity. By constraining the shifts of these features during the training process, we can achieve results that meet or even exceed the effectiveness of sequential KL, at a significantly lower computational cost (17.6% reductions compared to TDPO2 as shown in Figure 4 Left). Additionally, we introduce detailed ablation studies to show that our method maintains a stable performance over different temperatures and the selection of SAE layers. Overall, we show that FPO enjoys the efficiency of SimPO by using the offline reference control, while also the constraint quality of sequential KL by using the sparse feature-level constraints. To our knowledge, this is the first approach that integrates sparse feature-level constraints into LLM alignment. By incorporating sparse autoencoders with token-level DPO, FPO makes practically meaningful and theoretically solid improvements over existing preference optimization methods along three dimensions: simplicity of implementation, efficiency, and generation diversity."
https://arxiv.org/html/2411.07586v1,A Comprehensive Survey of AI-Driven Advancements and Techniques in Automated Program Repair and Code Generation,"Bug fixing and code generation have been core research topics in software development for many years. The recent explosive growth in Large Language Models has completely transformed these spaces, putting in reach incredibly powerful tools for both. In this survey, 27 recent papers have been reviewed and split into two groups: one dedicated to Automated Program Repair (APR) and LLM integration and the other to code generation using LLMs. The first group consists of new methods for bug detection and repair, which include locating semantic errors, security vulnerabilities, and runtime failure bugs. The place of LLMs in reducing manual debugging efforts is emphasized in this work by APR toward context-aware fixes, with innovations that boost accuracy and efficiency in automatic debugging. The second group dwells on code generation, providing an overview of both general-purpose LLMs fine-tuned for programming and task-specific models. It also presents methods to improve code generation, such as identifier-aware training, fine-tuning at the instruction level, and incorporating semantic code structures. This survey work contrasts the methodologies in APR and code generation to identify trends such as using LLMs, feedback loops to enable iterative code improvement and open-source models. It also discusses the challenges of achieving functional correctness and security and outlines future directions for research in LLM-based software development.","Large Language Models (LLMs) have steadily gained popularity in the field of automated software engineering, including primarily bug fixing (Ruan et al., 2024b) (Wolff et al., 2024) (Song et al., 2024) (Zhang et al., 2024) (Meng et al., 2024a) and code generation (OpenAI, 2021) (Wang et al., 2021) (Guo et al., 2021). Over the past decade, APR and code generation have greatly risen in use (Lee et al., 2024) (Lyu et al., 2024) and hence, have also warranted a great amount of research into the subject. A lot of tools that employ APR and natural language processing for code generation have been developed (OpenAI, 2021) (Wang et al., 2021) (Guo et al., 2021), which use an array of different techniques, including implementing Abstract Syntax Trees (ASTs), using varying heuristics for ranking plausible patches, patterns, and context-matching, among others. Using LLMs in code-related tasks has significantly improved the quality and speed of automating programming and discovering bugs in code. These tasks include summarizing code, generating code based on natural language requests, fixing bugs in pre-existing code, and understanding relatively large and complex repositories. However, in this paper, we’ll only be covering the research and studies done in the field of code generation and bug fixing, and to make understanding the work done in these fields easier, we’ve divided the tools and papers that we’ve covered into these two categories. LLMs have widely gained usage in these tools due to the natural advantage of having been trained on extremely large datasets and billions of parameters. Hence, it is easier to employ large LLMs to do particular tasks pertaining to programming. This leads to impressive performance and sizable advantages when compared to training models from scratch (Ruan et al., 2024b) (Song et al., 2024) (Zhang et al., 2024). Simultaneously, the task of employing LLMs in APR and code generation is also extremely complex and encompasses various areas with their own extensive research, such as benchmarking, repair scenarios (syntactic errors, semantic errors, etc.), repair techniques (recompilation, binary rewriting, etc.) testing of repairs (patch generation, input testing, coevolution) among others. Hence, understanding the work already done in this field can prove to be quite complex and time consuming. This survey paper aims to summarize the research and work already accomplished in this growing field to assist others in gaining a better understanding of how these tools work, their performance in practical scenarios, the areas they work on, and their limitations. We’ve collected 27 papers and summarized various factors about them, including the LLMs they use, the programming languages they work on, and, by extension, the difficulties faced in building language-agnostic APR tools, the approach they take to repair bugs and generate code, and the challenges that are still being worked on in the field. In conclusion, this paper aims to: (1) Collect research done on APR and code generation using LLMs and summarize the goals achieved. (2) Elucidate the repair scenarios these tools can be used for and the programming languages they work on. (3) How LLMs are integrated into the workflow of repairing and generating code and the challenges faced in doing so. (4) The limitations of using LLMs in code-related tasks and cases which are still being worked on."
https://arxiv.org/html/2411.07563v1,Improving Grapheme-to-Phoneme Conversion through In-Context Knowledge Retrieval with Large Language Models,"Grapheme-to-phoneme (G2P) conversion is a crucial step in Text-to-Speech (TTS) systems, responsible for mapping grapheme to corresponding phonetic representations. However, it faces ambiguities problems where the same grapheme can represent multiple phonemes depending on contexts, posing a challenge for G2P conversion. Inspired by the remarkable success of Large Language Models (LLMs) in handling context-aware scenarios, contextual G2P conversion systems with LLMs’ in-context knowledge retrieval (ICKR) capabilities are proposed to promote disambiguation capability. The efficacy of incorporating ICKR into G2P conversion systems is demonstrated thoroughly on the Librig2p dataset. In particular, the best contextual G2P conversion system using ICKR outperforms the baseline with weighted average phoneme error rate (PER) reductions of 2.0% absolute (28.9% relative). Using GPT-4 in the ICKR system can increase of 3.5% absolute (3.8% relative) on the Librig2p dataset.","Text-to-Speech (TTS) synthesis aims to convert written text into speech with correct pronunciation and natural prosody. Driven by the rapid development of deep learning, neural TTS systems such as [1, 2, 3, 4] have been widely applied in practical human-machine interactions. To bridge the gap between linguistic text and acoustic pronunciation, Grapheme-to-Phoneme (G2P) serves as a front-end module in the current TTS systems to map written text to their corresponding pronunciation representations. As any erroneous phonetic mapping could be propagated to the downstream modules and directly lead to mispronunciation, G2P plays an essential role in guaranteeing correct synthesis. The initial works for G2P models adopt rule-based approaches, and later statistical models, such as Hidden Markov Models (HMMs), and Conditional Random Fields (CRFs) [5, 6, 7, 8] was used to model the relationship of grapheme and phoneme. For the last decade, the development of deep learning led to the emergence of various neural G2P models, which leverage various network structures including Recurrent Neural Networks [9], Long-Short Term Memory [10, 11] and convolutional networks [12]. Besides, transformer-based auto-regressive models have also been explored in G2P tasks [13, 14]. More recently, SoundChoice [15] was proposed as a sentence-level G2P model. It employed homograph loss to resolve homograph disambiguation better, together with technologies such as curriculum learning and BERT embeddings [16], achieving promising performance on phoneme generation and homograph detection. Table 1: Ambiguity example: The same written word ”wound” has different pronunciations ”AW” versus ”UW” depending on different contextual meanings. Sentence Meaning Phoneme His string was wound very tight describing the action of turning or twisting something around W AW N D Let me see the wound on your leg An injury to living tissue caused by a cut, blow, or other impact W UW N D However, building accurate G2P datasets remains a laborious task, requiring extensive human annotation. Moreover, existing models often lack interpretability, making it difficult to understand the rationale behind certain phoneme outputs. Therefore, there is a need for more advanced techniques that can leverage contextual information effectively into the G2P conversion process. To this end, inspired by the remarkable success that Large Language Models (LLMs) like GPT-4 [17] demonstrated strong linguistic capabilities [18, 19] and competence in related tasks like Automatic Speech Recognition (ASR) error correction [20], contextual G2P conversion module with in-context knowledge retrieval from GPT-4 is proposed in this paper to provide richer and extensively semantic information in helping resolve the disambiguation gap in G2P mappings. The best-performing G2P conversion system using in-context knowledge retrieval outperforms the baseline without context information with phoneme error rate (PER) reductions of 2.0% absolute (28.9% relative) and homograph accuracy increase of 3.5% absolute (3.8% relative) on the Librig2p dataset. Figure 1: LLM In-context Knowledge Retrieval of an example sentence: 1. Input word, and corresponding sentence to the system; 2. Tag each word if it is in the dictionary and is a homograph. 3. if the word is in the dictionary and is a homograph, then let LLM find the case where the usage of the word is closest to the context of the word input; 4 if the word is in the dictionary and is not homograph, then get the recorded phonemes directly; 5. If the word is not in the dictionary, then let LLM generate the phonemes of it considering the context. The main contributions of this paper are summarized as follows: 1) To the best of our knowledge, this is the first work that leverages in-context knowledge retrieval from GPT-4 for handling the disambiguation challenge in G2P mapping process for G2P systems. In contrast, previous research has been limited to data augmentation and architectural modifications, overlooking the potential of leveraging LLMs’ contextual capabilities. 2) The efficacy of utilizing in-context knowledge retrieval in G2P conversion systems is extensively shown on the Librig2p task and provides insights on their potential to benefit other non-context based G2P architectures. 3) The highest accuracy rate of 95.7% and the lowest weighted average phoneme error rate of 4.9 % were obtained on Librig2p dataset compared to other non-context based methods, which demonstrates the potential of in-context knowledge retrieval for context-aware speech synthesis applications."
https://arxiv.org/html/2411.07518v1,LLM App Squatting and Cloning,"Impersonation tactics, such as app squatting and app cloning, have posed longstanding challenges in mobile app stores, where malicious actors exploit the names and reputations of popular apps to deceive users. With the rapid growth of Large Language Model (LLM) stores like GPT Store and FlowGPT, these issues have similarly surfaced, threatening the integrity of the LLM app ecosystem. In this study, we present the first large-scale analysis of LLM app squatting and cloning using our custom-built tool, LLMappCrazy. LLMappCrazy covers 14 squatting generation techniques and integrates Levenshtein distance and BERT-based semantic analysis to detect cloning by analyzing app functional similarities. Using this tool, we generated variations of the top 1000 app names and found over 5,000 squatting apps in the dataset. Additionally, we observed 3,509 squatting apps and 9,575 cloning cases across six major platforms. After sampling, we find that 18.7% of the squatting apps and 4.9% of the cloning apps exhibited malicious behavior, including phishing, malware distribution, fake content dissemination, and aggressive ad injection.","Mobile app squatting [19], where attackers publish apps with identifiers (e.g., app or package names) that mimic popular apps, such as through typosquatting (e.g., changing “Facebook” to “Fecebook”), is a growing threat in the mobile ecosystem. Hu et al.[19] identified over 10,553 squatting apps targeting the top 500 apps on Google Play, with more than 51% classified as malicious and some reaching millions of downloads. These counterfeit apps pose serious risks, including data theft and malware infections. Despite mitigation efforts by platforms, the sheer number of apps and sophisticated squatting tactics make detection and prevention difficult. Inspired by the extensive research on mobile app squatting, we have turned our attention to similar threats within emerging Large Language Model (LLM) app stores [45]. With the rise of LLMs, such as ChatGPT [27], Gemini [14], and Claude [10], there has been a proliferation of applications that leverage these models in diverse domains, including chatbots, content generation tools, and virtual assistants [6, 9, 11, 13, 28, 29]. LLM-powered applications have gained immense popularity due to their ability to perform complex tasks, leading to the creation of entire app ecosystems around them. However, as these LLM app stores continue to expand rapidly, we observe that they are becoming fertile ground for LLM app squatting attacks similar to those in traditional mobile app markets, as shown in Figure 1. In this context, squatting primarily occurs at the app identifier level, where attackers create apps with names that closely mimic legitimate ones to deceive users. For example, squatting could manifest as subtle name changes or the addition of enticing words, such as “Canva Pro”, tricking users into believing they are using an official or enhanced version of a popular app. Moreover, LLM app stores have significantly lowered the barrier to entry for developers. This democratization of development allows individuals from various backgrounds, even those with limited programming experience, to create and publish apps. While this inclusivity fosters innovation, it also makes it easier for attackers to clone the entire LLM app not only the app’s name but also its functionality and behavior. We refer to this more insidious form of attack as LLM app cloning, where the cloned app mirrors the legitimate one in nearly every aspect, making it even harder for users to discern the difference. To comprehensively investigate squatting and cloning in LLM app stores, we focus on six prominent LLM app stores (i.e., GPT Store [28], FlowGPT [13], Poe [29], Coze [11], Cici [9], and Character.AI [6]) that have gained significant traction due to the widespread adoption of LLM-powered applications. In our study, we develop a tool, LLMappCrazy, designed to automatically detect squatting and cloning instances within these ecosystems. Using LLMappCrazy, we systematically examine app identifier variations and functional cloning across GPT Store, identifying potential 5,834 name squatting apps and 6094 name cloning apps. And we also detect other features of apps in six LLM app stores. Our results reveal the scope of the problem: we found 3,509 squatting apps, and 9,575 cloned apps, confirming that this phenomenon is not isolated to mobile app markets but is rapidly spreading into the LLM domain. The findings indicate that 18.7% of the squatting apps and 4.9% of cloning apps exhibited malicious behavior, and some of them had amassed significant user downloads, further exacerbating the security risks faced by users of LLM-based applications. Contributions. We make the following main contributions: 1. To the best of our knowledge, this is the first detailed investigation into squatting and cloning attacks within LLM app stores. 2. We develop LLMappCrazy, a tool that detects squatting and cloning apps using 14 squatting-generation techniques and advanced semantic analysis. 3. Using LLMappCrazy, we find 5,834 name squatting apps and 6094 name cloning apps; We conduct a large-scale empirical study across six LLM app stores, identifying 3,509 squatting apps, and 9,575 cloning apps. 4. We find that 18.7% of the identified squatting apps and 4.9% of cloning apps exhibit malicious behavior, including phishing, malware, and ad injection. And we identify 227 apps that exhibit a high degree of similarity in various features to other apps in GPT Store. 5. We study the impact of LLM app squatting and cloning, discovering that these apps have reached up to 2.7 million conversations, posing significant risks to platform trust."
https://arxiv.org/html/2411.07426v1,Evaluating Detection Thresholds: The Impact of False Positives and Negatives on Super-Resolution Ultrasound Localization Microscopy,"Super-resolution ultrasound imaging with ultrasound localization microscopy (ULM) offers a high-resolution view of microvascular structures. Yet, ULM image quality heavily relies on precise microbubble (MB) detection. Despite the crucial role of localization algorithms, there has been limited focus on the practical pitfalls in MB detection tasks such as setting the detection threshold. This study examines how False Positives (FPs) and False Negatives (FNs) affect ULM image quality by systematically adding controlled detection errors to simulated data. Results indicate that while both FP and FN rates impact Peak Signal-to-Noise Ratio (PSNR) similarly, increasing FP rates from 0% to 20% decreases Structural Similarity Index (SSIM) by 7%, whereas same FN rates cause a greater drop of around 45%. Moreover, dense MB regions are more resilient to detection errors, while sparse regions show high sensitivity, showcasing the need for robust MB detection frameworks to enhance super-resolution imaging.","Ultrasound imaging has long been a cornerstone of clinical diagnostics, valued for its versatility and accessibility. However, traditional US imaging faces an inherent limitation in spatial resolution, which is fundamentally constrained by the physics of acoustic waves [1]. The resolution limit is approximately half the wavelength of the employed acoustic wave, creating a barrier to visualizing fine anatomical structures. Super-Resolution UltraSound (SR-US) represents a quantum leap in vascular imaging resolution by providing the means to reconstruct the vascular network and achieving a resolution close to one-tenth of the imaging wavelength of conventional diffraction-limited ultrasound [2]. Ultrasound Localization Microscopy (ULM) currently represents the only method that combines cost-effectiveness, non-invasiveness, and non-ionizing properties for comprehensive in vivo imaging of microvasculature. Through SR-US, low-resolution ultrasound images are transformed into high-resolution images, enabling improved diagnostic accuracy. Studying the vascular system can aid in the diagnosis and treatment of cardiovascular [3] and neurodegenerative disease [4] and cancer [5]. This wide-ranging applicability underscores ULM’s potential to revolutionize our understanding of microvascular systems in research and clinical settings. Christensen-Jeffries et al. [6] made a significant breakthrough in ultrasound imaging with their pioneering technique of tracking individual microbubbles (MBs) within the vascular system. Their work demonstrated that both acoustic super-resolution and sub-resolution velocity imaging can be achieved using standard image data from clinical ultrasound systems. Despite its promise, SR-US faces significant hurdles in its transition to clinical applications, namely the insufficient detection of MBs within a specified accumulation period, the time required for data collection and the imaging depth [7]. These limitations constrain the technique’s ability to provide high-resolution, real-time imaging in clinical settings, in the sense that optimizing any one of these factors often necessitates compromises in the others. These challenges collectively highlight the complexities involved in advanced imaging techniques, particularly in medical and diagnostic applications. In this context, Single-Image Super-Resolution (SISR) has gained prominence as a technique aimed at enhancing the resolution of individual low-resolution images by reconstructing their high-resolution counterparts [8, 9, 10, 11]. However, these methods depend on the assumption of isolated scatterers within the region of interest [12] which is no longer correct as MB trajectories intersect. To address the limitations of conventional methods, researchers have turned to learning-based approaches. In recent years, deep learning algorithms have emerged as the dominant paradigm in ULM, consistently achieving state-of-the-art performance across various benchmarks and applications. While the introduced networks often share certain components or characteristics, they can differ significantly in their overall structure and specific implementations [13, 14, 15, 16, 17]. These methods mark significant progress in deep learning-based localization for ultrasound imaging; However, they rely on numerous parameters that often need to be manually set, either through informed estimation or trial and error. As localization solutions vary significantly depending on training and simulation parameters, the importance of proper parameter selection becomes even more critical. However, there has been little to no systematic analysis of this selection process, and our aim here is to address this gap. Choosing the correct threshold for detection scores can be crucial for balancing precision and recall, which directly affects the performance of the model and, subsequently, the SR map. The optimal threshold depends on several factors, including the imaging parameters, the tolerance for False Positives (FPs) and False Negatives (FNs), and the distribution of MB sizes and locations in the data. In this paper, we: 1. Investigate the trade-off between sensitivity and specificity, with an emphasis on identifying which factor has a more significant impact on performance. 2. Assess multiple metrics of SR maps to evaluate this trade-off across frames, focusing on different density regions. 3. Explore the influence of varying center frequencies, on the relationship between sensitivity and specificity."
https://arxiv.org/html/2411.07279v1,"The Surprising Effectiveness of 
Test-Time Training for Abstract Reasoning","Language models have shown impressive performance on tasks within their training distribution, but often struggle with novel problems requiring complex reasoning. We investigate the effectiveness of test-time training (TTT)—updating model parameters temporarily during inference using a loss derived from input data—as a mechanism for improving models’ reasoning capabilities, using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through systematic experimentation, we identify three crucial components for successful TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and augmentations (3) per-instance training. TTT significantly improves performance on ARC tasks, achieving up to 6\times improvement in accuracy compared to base fine-tuned models; applying TTT to an 8B-parameter language model, we achieve 53% accuracy on the ARC’s public validation set, improving the state-of-the-art by nearly 25% for public and purely neural approaches. By ensembling our method with recent program generation approaches, we get SoTA public validation accuracy of 61.875%, matching the average human score. Our findings suggest that explicit symbolic search is not the only path to improved abstract reasoning in neural language models; additional test-time applied to continued training on few-shot examples can also be extremely effective.","Large-scale neural language models (LMs) excel at performing tasks that occur in their training data, and often elementary variations or compositions of those tasks (Brown et al., 2020; Todd et al., 2024). Given natural language task specifications or a small number of examples, LMs often successfully infer the desired task and produce an appropriate output. But can LMs also solve new problems, involving non-trivial reasoning, planning, or string manipulation of a kind very different from their pre-training data? This question is central to understanding the novel skill acquisition capabilities of current AI systems, which has been proposed as a key measure of intelligence (Chollet, 2019). For complex and novel tasks, it is often difficult to obtain a correct answer simply by sampling from an LM (Wu et al., 2023). However, a significant finding in recent years has been that LM performance can be substantially improved by augmenting LM decoding with additional test-time computation. Methods in this category include chain-of-thought prompting (Wei et al., 2022), sampling with majority voting (self-consistency; Wang et al., 2022), code execution (Brown et al., 2024; Snell et al., 2024; Damani et al., 2024), and search (Yao et al., 2024). One scaling strategy that has gained recent attention is test-time training (TTT), in which models are updated through explicit gradient steps based on test-time inputs (Krause et al., 2018; 2019). This method differs from standard fine-tuning as it operates in an extremely low-data regime—typically via an unsupervised objective on a single input, or a supervised objective applied to one or two in-context labeled examples. Modern versions of this approach was proposed for vision models by Sun et al. (2020), and also applied to sequence models by Gandelsman et al. (2022). The design space for TTT approaches is large, and there is currently a limited understanding of which design choices are most effective for LMs (and specifically for novel-task learning). In this paper, we systematically study the impact of various TTT design choices, as well as its interaction with pre-training and sampling schemes. We evaluate these methods in the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), a collection of extremely challenging few-shot visual reasoning problems. ARC is an ideal benchmark for testing the limits of LM generalization as it presents novel tasks, in a novel format, requiring nontrivial search and inference capabilities. Current language models perform poorly on ARC. Most successful approaches have relied on program synthesis techniques (Butt et al., 2024; Ainooson et al., 2023; Huang et al., 2023), though recently Cole et al. (2024) reported promising results using TTT on the benchmark. We identify several crucial ingredients for effective application of TTT to few-shot learning: (1) initial fine-tuning on synthetic tasks similar to those encountered at test time, (2) an augmented, leave-one-out task generation strategy for constructing the test-time dataset, (3) per-instance adapter training and (4) a self-consistency (Wang et al., 2022) approach under invertible transformations. With careful choices of these components, TTT can significantly improve LM performance on ARC—increasing accuracy by up to a factor of six over a 1B model, and achieving state-of-the-art results for published, purely neural models on the ARC task with a 8B model. Indeed, our results show that when equipped with test-time training, ordinary LMs can match or exceed the performance of many neuro-symbolic approaches on ARC. Our main contributions111Our implementation can be found at this link. are: 1. We identify and systematically analyze the key components needed for test-time training on ARC tasks, with a a novel test time training data generation and self-consistency component. 2. We achieve state-of-the-art results among published neural approaches on the ARC validation set: • 53% accuracy on the public validation set with a 8B parameter model. • 61.875% accuracy when ensembled with program synthesis approaches, comparable to average human performance on the dataset. 3. We demonstrate that tasks that could only be solved by program synthesis previously can be solved with fully neural approaches equipped with our TTT framework. These results challenge the assumption that symbolic components are strictly necessary for solving such complex tasks. Instead, they suggest that the critical factor in solving novel reasoning problems may be the allocation of proper computational resources during test time, perhaps independently of whether these resources are deployed through symbolic or neural mechanisms."
https://arxiv.org/html/2411.08034v2,"Scaling Properties of Diffusion Models 
for Perceptual Tasks","In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and amodal segmentation under the framework of image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perceptual tasks. Through a careful analysis of these scaling properties, we formulate compute-optimal training and inference recipes to scale diffusion models for visual perception tasks. Our models achieve competitive performance to state-of-the-art methods using significantly less data and compute. We release code and models at scaling-diffusion-perception.github.io.","Diffusion models have emerged as powerful techniques for generating images and videos, while showing excellent scaling behaviors. However, diffusion models can also be utilized for downstream prediction for inverse vision problems. In this paper, we present a unified framework to perform depth estimation, optical flow estimation, and amodal segmentation with a single diffusion model, as illustrated in Figure 1. Previous works such as Marigold (Ke et al., 2024), FlowDiffuser (Luo et al., 2024), and pix2gestalt (Ozguroglu et al., 2024) demonstrate the potential of repurposing image diffusion models for various inverse vision tasks individually. Building on this foundation, we perform an extensive empirical study, establishing scaling power laws for depth estimation, and display their transferability to other perceptual tasks. Using insights from these scaling laws, we formulate compute-optimal recipes for diffusion training and inference. We are the first to show that efficiently scaling compute for diffusion models leads to significant performance gains in downstream perceptual tasks. Recent works have also focused on scaling test-time compute to enhance the capabilities of modern LLMs, as demonstrated by OpenAI’s o1 model (OpenAI, 2024). As Noam Brown noted, “It turned out that having a bot think for just 20 seconds in a hand of poker got the same boosting performance as scaling up the model by 100,000x and training it for 100,000 times longer.” In our experiments, we realize a similar trade off between allocating more compute during training versus test-time for diffusion models. We scale test-time compute by exploiting the iterative and stochastic nature of diffusion to increase the number of denoising steps. By allocating more compute to early denoising steps, and ensembling multiple denoised predictions, we consistently achieve higher accuracy on these perceptual tasks. Our results provide evidence of the benefits of scaling test-time compute for inverse vision problems under constrained compute budgets, bringing a new perspective to the conventional paradigm of training-centric scaling for generative models. Figure 1: A Unified Framework: We fine-tune a pre-trained Diffusion Model (DM), for visual perception tasks. We take a RGB image, and a conditional image (i.e. next video frame, occlusion mask, etc.), along with the noised image of the ground truth prediction. Our model generates predictions for visual tasks such as depth estimation, optical flow prediction, and amodal segmentation, based on the conditional task embedding. We train a generalist model that can perform all three tasks with exceptional performance."
https://arxiv.org/html/2411.08033v1,: Interactive Point Cloud Latent Diffusion for 3D Generation,"While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs a Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multi-view image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing methods in both text- and image-conditioned 3D generation.","3D content generation holds great potential for transforming the virtual reality, film, and gaming industries. Current approaches typically follow one of two paths: either a 2D-lifting method or the design of native 3D diffusion models. While the 2D-lifting approach (Shi et al., 2023b; Liu et al., 2023b) benefits from leveraging 2D diffusion model priors, it is often hindered by expensive optimization, the Janus problem, and inconsistencies between views. In contrast, native 3D diffusion models (Jun & Nichol, 2023; Lan et al., 2024; Zhang et al., 2024) are trained from scratch for 3D generation, offering improved generality, efficiency, and control. Despite the progress in native 3D diffusion models, several design challenges still persist: (1) Input format to the 3D VAE. Most methods (Zhang et al., 2024; Li et al., 2024) directly adopt point cloud as input. However, it fails to encode the high-frequency details from textures. Besides, this limits the available training dataset to artist-created 3D assets, which are challenging to collect on a large scale. LN3Diff (Lan et al., 2024) adopt multi-view images as input. Though straightforward, it lacks direct 3D information input and cannot comprehensively encode the given object. (2) 3D latent space structure. Since 3D objects are diverse in geometry, color, and size, most 3D VAE models adopt the permutation-invariant set latent (Zhang et al., 2023a; Sajjadi et al., 2022; Zhang et al., 2024) to encode incoming 3D objects. Though flexible, this design lacks the image-latent correspondence as in Stable Diffusion VAE (Rombach et al., 2022), where the VAE latent code can directly serve as the proxy for editing input image (Mou et al., 2023b; a). Other methods adopt latent tri-plane (Wu et al., 2024; Lan et al., 2024) as the 3D latent representation. However, the latent tri-plane is still unsuitable for interactive 3D editing as changes in one plane may not map to the exact part of the objects that need editing. (3) Choice of 3D output representations. Existing solutions either output texture-less SDF (Wu et al., 2024; Zhang et al., 2024), which requires additional shading model for post-processing; or volumetric tri-plane (Lan et al., 2024), which struggles with high-resolution rendering due to extensive memory required by volumetric rendering (Mildenhall et al., 2020). In this study, we propose a novel 3D generation framework that resolves the problems above and enables scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. The resulting method, dubbed GaussianAnything, supports multi-modal conditional 3D generation, including point cloud, caption, and image. Specifically, we propose a 3D VAE that adopts multi-view posed RGB-D(epth)-N(ormal) renderings as the input, which are easy to render and contain comprehensive 3D attributes corresponding to the input 3D object. The information of each input view is channel-wise concatenated and efficiently encoded with the scene representation transformer (Sajjadi et al., 2022), yielding a set latent that compactly encodes the given 3D input. Instead of directly applying it for diffusion learning (Zhang et al., 2024; Li et al., 2024), our novel design concretizes the unordered tokens into the shape of the 3D input. Specifically, this is achieved by cross-attending (Huang et al., 2024b) the set latent via a sparse point cloud sampled from the input 3D shape, as visualized in Fig. 2. The resulting point-cloud structured latent space significantly facilitate shape-texture disentanglement and 3D editing. Afterward, a DiT-based 3D decoder (Peebles & Xie, 2023; Lan et al., 2024) gradually decodes and upsamples the latent point cloud into a set of dense surfel Gaussians (Huang et al., 2024a), which are rasterized to high-resolution renderings to supervise 3D VAE training. After the 3D VAE is trained, we conduct cascaded latent diffusion modeling on the latent space through flow matching (Albergo et al., 2023; Lipman et al., 2023; Liu et al., 2023c) using the DiT (Peebles & Xie, 2023) framework. To encourage better shape-texture disentanglement, a point cloud diffusion model is first trained to carve the overall layout of the input shape. Then, a point-cloud feature diffusion model is cascaded to output the corresponding feature conditioned on the generated point cloud. The generated featured point cloud is then decoded into surfel Gaussians via pre-trained VAE for downstream applications. In summary, we contribute a comprehensive 3D generation framework with a point cloud-structured 3D latent space. The redesigned 3D VAE efficiently encodes the 3D input into an interactive latent space, which is further decoded into high-quality surfel Gaussians. The diffusion models trained on the compressed latent space have shown superior performance in text-conditioned 3D generation and editing, as well as impressive image-conditioned 3D generation on general real world data."
https://arxiv.org/html/2411.08027v1,LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models,"Physical reasoning is an important skill needed for robotic agents when operating in the real world. However, solving such reasoning problems often involves hypothesizing and reflecting over complex multi-body interactions under the effect of a multitude of physical forces and thus learning all such interactions poses a significant hurdle for state-of-the-art machine learning frameworks, including large language models (LLMs). To study this problem, we propose a new physical reasoning task and a dataset, dubbed TraySim. Our task involves predicting the dynamics of several objects on a tray that is given an external impact – the domino effect of the ensued object interactions and their dynamics thus offering a challenging yet controlled setup, with the goal of reasoning being to infer the stability of the objects after the impact. To solve this complex physical reasoning task, we present \operatorname{LLMPhy}, a zero-shot black-box optimization framework that leverages the physics knowledge and program synthesis abilities of LLMs, and synergizes these abilities with the world models built into modern physics engines. Specifically, \operatorname{LLMPhy} uses an LLM to generate code to iteratively estimate the physical hyperparameters of the system (friction, damping, layout, etc.) via an implicit analysis-by-synthesis approach using a (non-differentiable) simulator in the loop and uses the inferred parameters to imagine the dynamics of the scene towards solving the reasoning task. To show the effectiveness of \operatorname{LLMPhy}, we present experiments on our TraySim dataset to predict the steady-state poses of the objects. Our results show that the combination of the LLM and the physics engine leads to state-of-the-art zero-shot physical reasoning performance, while demonstrating superior convergence against standard black-box optimization methods and better estimation of the physical parameters. Further, we show that \operatorname{LLMPhy} is capable of solving both continuous and discrete black-box optimization problems.","Many recent Large Language models (LLMs) appear to demonstrate the capacity to effectively capture knowledge from vast amounts of multimodal training data and their generative capabilities allow humans to naturally interact with them towards extracting this knowledge for solving challenging real-world problems. This powerful paradigm of LLM-powered problem solving has manifested in a dramatic shift in the manner of scientific pursuit towards modeling research problems attuned to a form that can leverage this condensed knowledge of the LLMs. A few notable such efforts include, but not limited to the use of LLMs for robotic planning (Song et al., 2023; Kim et al., 2024), complex code generation (Tang et al., 2024; Jin et al., 2023), solving optimization problems (Yang et al., 2024; Hao et al., 2024), conduct sophisticated mathematical reasoning (Trinh et al., 2024; Cherian et al., 2024), or even making scientific discoveries (Romera-Paredes et al., 2024). Figure 1: Frames from an example dynamical sequence in our TraySim dataset. The left-most frame shows the first frame of the scene with many objects on the tray and is going to be impacted by a black pusher (right-bottom). The subsequent frames show the state of the system at the 25-th, 50-th, and the 200-th time step (each step is 0.01s). Our task is for a model to reason through the dynamics of the system and predict the stability of each object on the tray at the end of the episode in a zero-shot manner, when provided as input only the first frame of the sequence. While current LLMs seem to possess the knowledge of the physical world and may be able to provide a plan for solving a physical reasoning task (Singh et al., 2023; Kim et al., 2024) when crafted in a suitable multimodal format (prompt), their inability to interact with the real world or measure unobservable attributes of the world model, hinders their capability in solving complex physical reasoning problems (Wang et al., 2023; Bakhtin et al., 2019; Riochet et al., 2021; Harter et al., 2020; Xue et al., 2021). Consider for example the scene in Figure 1. Suppose a reasoning model is provided as input only the first image (left-most) and is asked to answer the question: Which of the objects will remain upright when the tray is impacted by the black pusher with a velocity of 4.8 m/s?. To answer this question, the model must know the various physical attributes of the system, including the masses of the objects, friction coefficients between the contacts and their respective contact forces, stiffness of the objects, among others. While, a large sophisticated machine learning model (such as an LLM) may be able to provide an educated guess based on the intuitive physics of the system, a useful solution would demand a more intricate reasoning strategy in estimating the real-world physics and dynamics; such complex dynamics may be difficult or even impossible to be correctly learned solely from training data. Conversely, advancements in graphics hardware and software have led to the development of advanced physics engines capable of simulating realistic world models Makoviychuk et al. (2021); Bear et al. (2021); Todorov et al. (2012); Gan et al. (2020). Thus, rather than having the LLM to learn the world physics, our key idea is to consider using a physics engine in tandem with the LLM, where the LLM may use its world knowledge for generating scene-based reasoning hypotheses while the simulator is used to verify them within the physical world model. To study this problem, we consider the novel task of predicting the dynamics of objects and their stability under the influence of an impact – an important problem for a variety of robotic applications (Gasparetto et al., 2015; Ahmed et al., 2020). In this paper, we consider this problem in a challenging setting using our new dataset, TraySim, in which the impact is caused by a pusher colliding to a tray that holds several objects of varied sizes, masses, and centers of gravity, with the goal of predicting the dynamics of each of the object instances. We cast this task as that of answering physical reasoning questions. Specifically, as illustrated in Figure 1, TraySim includes simulated video sequences consisting of a tray with an arbitrary number of objects on it and given the first video frame of a given scene, the task of the reasoning model is to infer which of the objects on the tray will remain upright after the impact when the system has stabilized. As is clear from Figure 1, solving this task will require the model to derive details regarding the physical properties of each of the objects and their contacts, as well as have the ability to imagine the system’s dynamics through multi-body interactions influenced by the various internal and external forces from the impact. Our task presents a challenging reasoning setup for current machine learning models, including LLMs. To solve this task, we propose \operatorname{LLMPhy}, a black-box optimization setup combining an LLM with a physics engine that leverages the program synthesis abilities of the LLM to communicate with the engine for solving our task. \operatorname{LLMPhy} operates in two phases: i) a parameter estimation phase, where \operatorname{LLMPhy} is used as a continuous black-box optimization module towards inferring the physical parameters of the objects, including the friction, stiffness, damping, etc. from a given example video sequence, and ii) a scene understanding phase, where the LLM-simulator combination is used as a discrete black-box optimizer to reconstruct the problem layout for synthesizing the setup within the simulator for execution. Our framework builds a feedback loop between the LLM and the physics engine, where the LLM generates programs using its estimates of physical attributes; the programs are executed in the simulator, and the error from the simulations are fed back to the LLM as prompts to refine its estimates until a suitable convergence criteria is met. Note that we do not assume any differentiablity properties of the simulator, which makes our setup highly general. This allows the approach to function as a black-box optimization framework, enabling its use with a wide range of simulators without the need for gradient-based methods. While we may generate unlimited data using our simulation program, given the zero-shot nature of our setup, we synthesized 100 sequences in our TraySim dataset to demonstrate the effectiveness of \operatorname{LLMPhy}. Each sample in TraySim has two video sequences: i) the task sequence of which only the first frame is given to a reasoning agent, and ii) a parameter-estimation video sequence which has a lesser number of instances of each of the object types appearing in the task sequence; the latter sequence has an entirely different layout and dynamics of objects. To objectively evaluate the performance, we cast the task as a physical question answering problem, where the LLM is required to select the correct subset of answers from the given candidate answers. Our results on TraySim show that \operatorname{LLMPhy} leads to promising improvements in performance against alternatives, including using Bayesian optimization, CMA-ES, and solely using an LLM for physical reasoning. Interestingly, we also find that \operatorname{LLMPhy} estimates the physical parameters better and powerful recent LLMs – such as OpenAI o1-preview – show trends of superior optimization convergence. Before moving forward, we summarize below our main contributions: • We consider the novel task of reasoning over complex physics of a highly dynamical system by combining LLMs with non-differentiable physics engines. • We propose a zero-shot reasoning framework \operatorname{LLMPhy}, that combines the reasoning and program synthesis abilities of an LLM with the realistic simulation abilities of a physics engine. This approach is used to estimate the physical parameters of the model, the scene layout, and synthesizing the dynamical scene for inferring the solution. • We introduce a novel synthetic multi-view dataset: TraySim, to study this task. The dataset consists of 100 scenes for zero-shot evaluation. • Our experiments using \operatorname{LLMPhy} on the TraySim dataset demonstrate promising results against related baselines, highlighting its potential for tackling complex physics-based reasoning tasks that involves both discrete and continuous optimization sub-tasks."
https://arxiv.org/html/2411.08019v1,Language Models as Causal Effect Generators,"We present a framework for large language model (LLM) based data generation with controllable causal structure. In particular, we define a procedure for turning any language model and any directed acyclic graph (DAG) into a sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM is a causal model with user-defined structure and LLM-defined structural equations. We characterize how an SD-SCM allows sampling from observational, interventional, and counterfactual distributions according to the desired causal structure. We then leverage this procedure to propose a new type of benchmark for causal inference methods, generating individual-level counterfactual data without needing to manually specify functional relationships between variables. We create an example benchmark consisting of thousands of datasets, and test a suite of popular estimation methods on these datasets for average, conditional average, and individual treatment effect estimation, both with and without hidden confounding. Apart from generating data, the same procedure also allows us to test for the presence of a causal effect that might be encoded in an LLM. This procedure can underpin auditing LLMs for misinformation, discrimination, or otherwise undesirable behavior. We believe SD-SCMs can serve as a useful tool in any application that would benefit from sequential data with controllable causal structure.","All causal inference methods have to deal with the same fundamental underlying problem: we can only observe one state of the world at a given moment in time. If we wish to reason about what caused what, we — in some way or other — will have to reason about a hypothetical world (or worlds) that we didn’t observe. This issue is often referred to more formally as The Fundamental Problem of Causal Inference [28]: a given individual can only receive one treatment at a time, so we can only ever observe one of their potential outcomes (or counterfactuals). Overcoming this fundamental problem not only makes causal inference a challenge to perform, but also a challenge to evaluate with realistic data. Our main focus — data generation with known causal structure — is thus an important tool both for the development and the evaluation of methods that deal with causality. A well-established strategy for causal effect estimation uses randomization of a treatment to estimate average treatment effects (ATEs) [50], capturing the effect of a treatment on average across many individuals. Because ATEs are feasible to isolate with proper study design, many realistic datasets exist with ‘true’ or at least widely-accepted ATEs [38, 15, 30]. If, instead, inferences about individual treatment effects (ITEs) are desired, making additional modeling assumptions can allow us to reason about counterfactuals at an individual level; unfortunately, so long as counterfactuals remain unobservable, these assumptions remain untestable. This means that there is no easy way to collect realistic data for ITEs.111It is common in literature on individual-level causal inference to draw no distinction between ITEs and conditional average treatment effects (CATEs) [55], but we emphasize these two quantities as distinct: the CATE is the conditional expectation of the ITE, which typically will not explain all ITE variation [39]. In particular, the counterfactual outcome \tilde{y} and/or treatment assignment \tilde{t} must always be manually generated, even if covariates \tilde{x} are based on real data (see, e.g., [42]). Our method is aimed at using LLMs to provide this type of data in a flexible manner. (a) (b) Figure 1: (a) A sequence-driven structural causal model (SD-SCM) uses a language model to sample data according to any desired causal relationships between variables (blue solid arrows represent an example set of possible DAGs). Any variables whose values are sampled from the language model will potentially share the language model as a common cause (red dashed arrows), unless sampled manually, e.g., uniformly. (b) However, the effects of red dashed arrows are not necessarily significant. For example, the variable weather is a strong confounder (top) or strong collider (bottom), depending on which DAG is chosen. All structural equations are determined implicitly by whatever has already been encoded in the language model (see Section 3.3 for the full example). Contribution. We define a procedure for turning any language model and any DAG into a sequence-driven structural causal model (SD-SCM). We characterize in Section 3 how an SD-SCM provides access to observational, interventional, and counterfactual distributions over sequential data according to the desired causal structure, where structural equations are defined implicitly via language model (see Figure 1). In Section 4, we use SD-SCMs to create an example benchmark for causal effect estimation and test a suite of popular estimation methods across ATE, CATE, and ITE estimation. Section 5 demonstrates how this same technique can underpin the auditing of language models for (un)desirable causal effects. All benchmark datasets as well as code for SD-SCM-based data generation is available on GitHub.222https://github.com/lbynum/sequence-driven-scms We believe SD-SCMs can be used across many applications as a general tool for imposing causal structure on sequentially generated data."
https://arxiv.org/html/2411.08017v1,Wavelet Latent Diffusion (WaLa): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings,"Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into a wavelet-based, compact latent encodings. Specifically, we compress a 256^{3} signed distance field into a 12^{3}\times 4 latent grid, achieving an impressive 2,427× compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 256^{3} resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model’s scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities: https://github.com/AutodeskAILab/WaLa.","Training generative models on large-scale 3D data presents significant challenges. The cubic nature of 3D data drastically increases the number of input variables the model must handle, far exceeding the complexity found in image and natural language tasks. This complexity is further compounded by storage and streaming issues. Training such large models often requires cloud services, which makes the process expensive for high-resolution 3D datasets as these datasets take up considerable space and are slow to stream during training. Additionally, unlike other data types, 3D shapes can be represented in various ways, such as voxels, point clouds, meshes, and implicit functions. Each representation presents different trade-offs between quality and compactness. Determining which representation best balances high fidelity with compactness for efficient training and generation remains an open challenge. Finally, 3D representations often exhibit complex hierarchical structures with details at multiple scales, making it challenging for a generative model to capture both global structure and fine-grained details simultaneously. Figure 2: WaLa generates 3D shapes across various input modalities (see appendix for more). To address these challenges, current state-of-the-art methods for large generative models typically employ three main strategies. The first strategy involves using low-resolution representations, such as sparse point clouds (Nichol et al., 2022c; Jun & Nichol, 2023b), low-polygon meshes (Chen et al., 2024b), or coarse grids (Cheng et al., 2023; Sanghi et al., 2023b). While these approaches reduce computational complexity, they are limited in their ability to model the full distribution of 3D shapes, struggle to capture intricate details, and often lead to lossy representations. The second approach represents 3D shapes through a collection of 2D images (Yan et al., 2024a) or incorporates images (Hong et al., 2023; Li et al., 2023a; Liu et al., 2024; Xu et al., 2023b; Siddiqui et al., 2024; Bensadoun et al., 2024) into the training loss. However, this method suffers from long training times due to the need for rendering and can fail to capture internal details of 3D shapes, as it primarily focuses on external appearances. The third strategy introduces more compactness into the input representations (Hui et al., 2024; Zhou et al., 2024; Ren et al., 2024; Yariv et al., 2024; Xiong et al., 2024; Zhang et al., 2024) to reduce the number of variables the generative model must handle. While these representations can be sparse (Ren et al., 2024; Yariv et al., 2024; Xiong et al., 2024), they are often irregular or discrete in nature making it challenging to be modeled via neural networks and can still be relatively large compared to image or natural language data (Hui et al., 2024; Zhou et al., 2024), thus making it difficult to scale the model parameters efficiently. One prominent compact input representation is wavelet-based representation, which includes Neural Wavelet (Hui et al., 2022), UDiFF (Zhou et al., 2024), and wavelet-tree frameworks (Hui et al., 2024). These methods utilize wavelet transforms and their inverses to seamlessly convert between wavelet spaces and high-resolution truncated signed distance function (TSDF) representations. They offer several key advantages: data can be easily compressed by discarding selected coefficients with minimal loss of detail, and the interrelationships between coefficients facilitate efficient storage, streaming, and processing of large-scale 3D datasets compared to directly using TSDFs (Hui et al., 2024). However, despite these benefits, wavelet-based representations remain substantially large, especially when scaling up for large-scale generative models. For example, a 256^{3} TSDF can be represented as a wavelet-tree of size 46^{3}\times 64 (Hui et al., 2024), which is equivalent to a 1440\times 1440 RGB image. Scaling within this space continues to pose significant challenges. In this work, we build upon the wavelet representation described above and introduce the Wavelet Latent Diffusion (WaLa) framework. This framework further compresses the wavelet representation to obtain compact latent encodings without significant information loss, thereby efficiently enabling us to scale a diffusion-based generative model within this space. Starting with a truncated signed distance function (TSDF) of a shape, we first convert it into 3D wavelet tree representation as in Hui et al. (2024). Then, we train a convolution-based VQ-VAE model with adaptive sampling loss and balanced fine-tuning to compress a 256^{3} TSDF into a 12^{3}\times 4 grid, achieving a remarkable 2,427\times compression ratio while maintaining an impressive reconstruction without a significant loss of detail. For example, as shown in Table 1, an Intersection over Union (IOU) of 0.978 is achieved on the GSO dataset. Compared to other representations, this approach requires fewer input variables for the generative model while retaining high reconstruction accuracy. Consequently, the generative model does not need to model local details and can focus on capturing the global structure. Moreover, by significantly reducing the number of input variables that the generative model must handle due to this compression, we enable the training of large-scale 3D generative models with up to a billion parameters, producing highly detailed and diverse shapes. WaLa also supports controlled generation through multiple input modalities without adding significant inductive biases, making the framework flexible and adaptable beyond single-view 3D reconstruction tasks. As a result, our model generates 3D shapes with complex geometry, plausible structures, intricate topologies, and smooth surfaces. This is demonstrated in Figures 1 and 2, where high-quality 3D meshes can be obtained by applying marching cubes to the SDF generated from different input modalities such as text, sketch, low-resolution voxel, point cloud, single-view, and multi-view images. In summary, we make the following contributions: • We introduce a Wavelet Latent Diffusion (WaLa) framework that tackles the dimensional and computational challenges of 3D generation with impressive compression while maximizing fidelity. • Our large billion-parameter model generates high-quality 3D shapes within two to four seconds, significantly outperforming state-of-the-art benchmarks in 3D shape generation. • Our model demonstrates exceptional versatility, accepting diverse input modalities such as single/multi-view images, voxels, point clouds, depth data, sketches, and textual descriptions (see Figure 1 and 2), making it applicable to a wide range of 3D modeling tasks. • To foster reproducibility and stimulate further research in this domain, we release what we believe is, to the best of our knowledge, the largest 3D generative model to date that works across various input modalities, comprising approximately one billion parameters. The model is available at https://github.com/AutodeskAILab/WaLa. Table 1: 3D representations compared on GSO dataset (Downs et al., 2022): Intersection over Union (IoU) for accuracy & number of input variables for generative models to evaluate complexity. Representation IoU Number of Input Variables Ground-truth SDF (256^{3}) 1.0 16,777,216 (\sim 64\text{MB}) Point Cloud (Nichol et al., 2022a) 0.8642 12,288 (\sim 0.05\text{MB}) Latent Vectors (Jun & Nichol, 2023a) 0.8576 1,048,576 (\sim 4\text{MB}) Coarse Component (Hui et al., 2022) 0.9531 97,336 (\sim 0.4\text{MB}) Wavelet tree (Hui et al., 2024) 0.9956 1,129,528 (\sim 4.3\text{MB}) WaLa 0.9780 6,912 (\sim 0.03\text{MB})"
https://arxiv.org/html/2411.08013v2,Investigating the Effectiveness of Explainability Methods in Parkinson’s Detection from Speech,"Speech impairments in Parkinson’s disease (PD) provide significant early indicators for diagnosis. While models for speech-based PD detection have shown strong performance, their interpretability remains underexplored. This study systematically evaluates several explainability methods to identify PD-specific speech features, aiming to support the development of accurate, interpretable models for clinical decision-making in PD diagnosis and monitoring. Our methodology involves (i) obtaining attributions and saliency maps using mainstream interpretability techniques, (ii) quantitatively evaluating the faithfulness of these maps and their combinations obtained via union and intersection through a range of established metrics, and (iii) assessing the information conveyed by the saliency maps for PD detection from an auxiliary classifier. Our results reveal that, while explanations are aligned with the classifier, they often fail to provide valuable information for domain experts.","Parkinson’s disease (PD) is a progressive neurodegenerative disorder primarily marked by the deterioration of dopaminergic neurons in the midbrain. This degeneration leads to a range of motor and non-motor symptoms, including tremors, bradykinesia, cognitive impairment, and depression [1, 2, 3]. Importantly, during the prodromal stages of PD, patients often start to exhibit speech impairments, which can serve as early indicators of the disease [4, 5]. Given the non-invasive, cost-effective, and automated nature of speech analysis, researchers have increasingly focused on this approach as a promising avenue for the early detection of Parkinson’s disease [6]. Despite substantial advances in PD classification using speech analysis, model interpretability remains underexplored. In clinical settings, explainable AI (XAI) is essential for providing clear, clinically relevant insights, crucial for the acceptance of automated systems in clinical trials. Many XAI techniques exist for interpreting model predictions, with post-hoc explanation methods among the most widely used [7]. Here, we focus on two different sets of approaches: Perturbation-based and Gradient-based post-hoc explanation methods. Perturbation-based methods assess feature importance by modifying input data, while gradient-based methods use gradients of predictions with respect to inputs [8]. Both approaches support local and global explanations, are model-agnostic, and offer valuable insights into the model’s behaviour. This paper systematically evaluates several key perturbation and gradient-based techniques to determine their effectiveness in highlighting PD-relevant speech features, aiming to enhance transparency and clinical utility in PD detection from speech. Our experimental results show that, although explanations are aligned with the classifier, they often fail to provide insights that are truly informative for domain experts. These methods may lack the level of interpretability required for practical use, emphasizing the need for more effective explainability approaches that connect model behavior with human understanding in specialized domains."
https://arxiv.org/html/2411.08010v1,ExpressivityArena: Can LLMs Express Information Implicitly?,"While Large Language Models (LLMs) have demonstrated remarkable performance in certain dimensions, their ability to express implicit language cues that human use for effective communication remains unclear. This paper presents ExpressivityArena, a Python library for measuring the implicit communication abilities of LLMs. We provide a comprehensive framework to evaluate expressivity of arbitrary LLMs and explore its practical implications. To this end, we refine the definition and measurements of “expressivity,” and use our framework in a set of small experiments. These experiments test LLMs in creative and logical tasks such as poetry, coding, and emotion-based responses. They are then evaluated by an automated grader, through ExpressivityArena, which we verify to be the most pragmatic for testing expressivity. Building on these experiments, we deepen our understanding of the expressivity of LLMs by assessing their ability to remain expressive in conversations. Our findings indicate that LLMs are capable of generating and understanding expressive content, however, with some limitations. These insights will inform the future development and deployment of expressive LLMs. We provide the code for ExpressivityArena alongside our paper.","Large Language Models (LLMs) OpenAI (2023); Touvron et al. (2023) are disrupting many domains where human communication is essential, including education OpenAI (2023), customer support Radford et al. (2019), legal services Chern et al. (2024), and healthcare Bubeck et al. (2023). Increasing parameter count in LLMs has resulted in better performance in a multitude of downstream tasks such as language translation, text summarizing, and question-answering Devlin et al. (2018); Brown et al. (2020). This performance is typically measured in terms of the number of errors OpenAI (2023), contextual understanding Brown et al. (2020), versatility Bai et al. (2024), problem-solving skills Bubeck et al. (2023), etc. Given that much of human communication is implicit Knepper et al. (2017), expressivity may represent an important aspect of creating “human-like” output in models, improving output quality and user trust in many applications Huang et al. (2024). In order for LLMs to generate text to communicate in a natural way, it is critical that they convey both explicit information and implicit information. In this context, we define expressivity as the implicit communication of information Apresyan (2018). For instance, in a conversation about a movie, explicit information would be “I thought the movie went on far too long” while implicit information may be expressed as “I kept checking my watch during the movie.” The fact remains the same: the speaker thought the movie was too long, but the second statement requires a level of interpretation. Expressivity may come through various metaphors, lexical choices, etc. in daily communication, and may take the form of a different speech act entirely. Aside from emotions, the speaker may also indicate other information about themselves. Word choice, such as slang, may implicitly communicate one’s regional background, level of education, or other identities Green (2016). Figure 1: ExpressivityArena tests LLMs on their ability to implicitly express information. Expressivity is an indispensable facet of human communication, and without it, LLMs will not be able to communicate as naturally, potentially undermining users’ trust in those models. Additionally, the ability to express and control implicit communication—such as certain emotions, tone, or style—in a model’s output opens up new use cases. This capability is particularly useful when maintaining a specific mood, writing style, or conversation flow is important. For instance, it enables LLMs to engage in more fluid and natural conversations by adjusting their responses to match the desired emotional tone or stylistic preferences. This study aims to quantitatively measure the expressivity of the state-of-the-art LLMs. To this end, we focus on the following research questions (RQs): • RQ1: Are LLMs capable of exhibiting expressivity? • RQ2: Can LLMs remain expressive through the course of a conversation? In order to answer these questions, we present ExpressivityArena, a framework to evaluate expressivity of LLMs. First, we set up a grader to objectively evaluate Wimsatt et al. (1946); Benedetto et al. (2013) outputs generated by various LLMs. Since evaluating thousands of LLM-generated outputs is an onerous task for humans, following recent success of using AI evaluators Lee et al. (2023); Bai et al. (2024); Chern et al. (2024), we also use independent LLMs as evaluators. Unlike human whose performance drift over time due to fatigue Boksem et al. (2005) when reading long texts, machine learning models maintain consistency of evaluation. We first established the validity of the grader with a human study. We then conducted experiments for tasks with varying degrees of expressivity—poetry generation and code generation—to answer the first question. We found that LLMs have wildly varying degrees of expressivity, and that models tended to be less expressive while generating code than while generating poetry, suggesting that models perform worse in low-expressivity domains. We then tested if models were able to maintain expressivity throughout the course of a simulated conversation, testing the expression of emotions and professions. We found that models became less expressive over time when expressing emotions, but became more expressive over the course of a conversation when expressing professions."
https://arxiv.org/html/2411.07990v1,Derivational Morphology Reveals Analogical Generalization in Large Language Models,"What mechanisms underlie linguistic generalization in large language models (LLMs)? This question has attracted considerable attention, with most studies analyzing the extent to which the language skills of LLMs resemble rules. As of yet, it is not known whether linguistic generalization in LLMs could equally well be explained as the result of analogical processes, which can be formalized as similarity operations on stored exemplars. A key shortcoming of prior research is its focus on linguistic phenomena with a high degree of regularity, for which rule-based and analogical approaches make the same predictions. Here, we instead examine derivational morphology, specifically English adjective nominalization, which displays notable variability. We introduce a new method for investigating linguistic generalization in LLMs: we fit cognitive models that instantiate rule-based and analogical learning to the LLM training data and compare their predictions with those of the LLM, allowing us to draw direct conclusions regarding underlying mechanisms. We compare the performance of GPT-J on a set of nonce adjectives with that of a high-performing rule-based model and a competitive analogical model. As expected, both models explain the predictions of GPT-J equally well for adjective classes with regular nominalization patterns. However, for adjective classes with variable nominalization patterns, the analogical model provides a much better match. Furthermore, GPT-J’s behavior is sensitive to the individual word frequencies, even for regular complex forms — a behavior that is consistent with an analogical account of regular forms but not a rule-based one. These findings refute the hypothesis that GPT-J’s linguistic generalization on adjective nominalization involves rules, suggesting similarity operations on stored exemplars as the underlying mechanism. Finally, we highlight a difference between linguistic generalization in humans and LLMs: while humans generalize based on types, LLMs generalize based on tokens, which we show has detrimental effects on their predictions. Overall, our study suggests that analogical processes play a bigger role in the linguistic generalization of LLMs than previously thought.","Results Table 2: Comparison with cognitive models. The table shows real and nonce examples for the four examined adjective classes, the counts of corresponding derivatives in the Pile as well as the results of rule-based and exemplar-based analogy models evaluated against GPT-J. The evaluation measure is accuracy. We highlight the highest accuracy value (i.e., the best-matching cognitive model) in each row in boldface — for the two adjective classes where there is a winner (i.e., -ive and -ous), this is the token-based GCM model. We highlight accuracy values that are significantly (p<.05) worse than the highest accuracy value in each row with a †. Examples Counts MGL GCM Regularity Suffix Real Nonce -ity -ness Type Token Type Token High -able available tegornable 11,081 1,034 .893 .893 .893 .893 -ish selfish friquish 0 1,502 .997 .997 .997 .997 Low -ive sensitive cormasive 4,508 2,438 †.658 .662 †.622 .688 -ous luminous momogorous 1,372 2,450 †.657 †.613 †.610 .703 Generalization to Nonce Words We compare the linguistic generalization behavior of GPT-J with that of two high-performing cognitive models (Materials and Methods, Cognitive Models): the Minimal Generalization Learner [MGL; 73, 74] and the Generalized Context Model [GCM; 22, 23, 75]. The MGL is a rule-based model that we have selected because it undertakes to capture detailed patterns of variation that earlier, simpler models did not capture. The GCM is an exemplar-based analogy model that was previously applied to variability in inflectional morphology [28, 31]. The comparison will give us some evidence about the underlying mechanism; if, for example, GPT-J agrees consistently with the MGL over the GCM, this would suggest that it uses a similar (i.e., rule-based) mechanism under the hood. The MGL and GCM models can be fit to either word types or word tokens. The inventory of word types corresponds to the list of words in a mental lexicon; only the existence of a word in the language is taken into account, and not its frequency in the training data. In an inventory of word tokens, each occurrence of a word in the training data is treated as a separate instance, with the result that more frequent words have more instances than less frequent words. We consider both settings, because the contrast between behaviors governed by type frequencies and those governed by token frequencies is a major theme in cognitive research on the lexicon. We focus on English adjective nominalization and examine four adjective classes (i.e., sets of adjectives ending in the same suffix), two of which clearly prefer -ity or -ness, and two of which are less regular while still showing an overall tendency towards one of the two suffixes (Table 2). We train the cognitive models on all adjective-derivative pairs that meet the following three criteria: (i) the adjective belongs to one of the four adjective classes in question; (ii) the derivative ends in -ity or -ness; (iii) both the adjective and the derivative occur in the Pile. For evaluation, we use UniPseudo [76] to generate 50 nonce adjectives for each of the four adjective classes (Materials and Methods, Nonce Adjectives). We check that both the generated nonce adjective and the two corresponding derivatives have a frequency of zero in the Pile, i.e., they have never been seen by either the cognitive models or GPT-J, thus providing an ideal test set for probing linguistic generalization. We then feed all nonce adjectives into the cognitive models and determine which of the two competing derivatives they prefer. For GPT-J, we measure the probability that it assigns to the two derivatives resulting from adding -ity and -ness to the adjectives (Materials and Methods, GPT-J). Specifically, we use GPT-J’s autoregressive language modeling head to compute the log probabilities for the tokens into which the derivatives are split by the tokenizer and sum them. We take the derivative with the higher total log probability as the preferred one. Since prior research has shown that varying prompts (i.e., the texts used to elicit LLM responses) can heavily affect LLM behavior [77], we repeat this procedure with 12 different prompts (Supporting Information). If not stated otherwise, the presented results are averaged over prompts. (a) Type-based MGL (b) Token-based MGL (c) Type-based GCM (d) Token-based GCM (e) GPT-J Figure 1: Distribution of preferred nominalization type (specifically, ratio of -ness derivatives) for unseen nonce adjectives, for rule-based models (a, b), exemplar-based models (c, d), and GPT-J (e). Models based on types are shown on the left (a, c), and models based on tokens are shown on the right (b, d). The ratio is computed as the number of -ness predictions divided by the total number of predictions (i.e., -ness and -ity predictions). As shown in Figure 1, both MGL and GCM — in the type-based as well as the token-based setting — make completely consistent predictions for the two adjective classes that strongly prefer one affix. They always predict -ity for -able and -ness for -ish. Thus, both cognitive models reproduce the regular behavior that characterizes these two adjective classes. GPT-J also predicts -ity for -able, and it predicts -ness for -ish in all but two cases for just one of the prompts (turgeishity and prienishity). GPT-J is nearly as successful in capturing the regular cases as MGL and GCM, and these in turn match the predictions of GPT-J equally well (Table 2, upper panel). Thus, the regular adjective classes do not tell us whether GPT-J is more like a rule-base model or an analogical model. Moving to the two adjective classes that show more variability between -ity and -ness (i.e., -ive and -ous), both MGL and GCM generate variable outcomes with a higher rate of -ness for -ous than for -ity. However, the predictions differ substantially in detail: the cognitive models (in the type-based as well as the token-based setting) agree in only 54% of the adjective types (not shown in the figure). Crucially, the cognitive model that matches the predictions of GPT-J on these two adjectives classes best is the token-based GCM model (Table 2, lower panel). As a concrete example, we consider the nonce adjective pepulative. The MGL models map pepulative to a rule that prescribes -ity following -tive, which in the type-based as well as the token-based setting has the highest confidence of all competing rules and is hence selected by both MGL models. The GCM models, by contrast, are more strongly influenced by local similarity effects. While overall there are a larger number of -ity derivatives in the neighborhood of pepulative (e.g., for adjectives ending in -lative there are 88 derivatives with -ity vs. 27 with -ness), many of the adjectives particularly close to pepulative have -ness derivatives with a high token frequency (e.g., manipulativeness has a token frequency of 1,544 vs. 26 for manipulativity). This difference is reflected by the GCM models, where the type-based model predicts -ity, but the token-based model predicts -ness. GPT-J, on the other hand, prefers -ness for this example and hence matches the behavior of the token-based GCM model. Our results show that the generalization behavior of LLMs on linguistic phenomena with a high degree of variability is best explained as a result of analogical mechanisms. This new finding is in line with the observation that LLMs store a considerable amount of their training data in their model weights [33, 34, 35, 19], and it further suggests that these stored data actively contribute to the language skills displayed by LLMs. Our results are consistent with a model that generalizes all adjective nominalizations by analogy; they eliminate the possibility that all nominalizations are generated by rules. However, there remains the possibility that LLMs effectively use analogies in cases of variation and apply rules for adjective classes with a high degree of regularity. This possibility is suggested by earlier theories of inflectional morphology, proposing dual-mechanism models in which regular plurals and past tenses are created by rules, while irregular forms involve analogies [36, 37, 38]. To address this possibility, it is necessary to look into frequency effects for individual words, as discussed in prior work [78, 79]. We will do so in the next sections. Predictions for Seen Words According to cognitive theories, analogies are based on remembered examples. If the mechanism underlying GPT-J’s behavior is analogical, it must implicitly remember a large number of examples. As the first step in evaluating this inference, we ask how well GPT-J’s behavior matches the statistics of its training data. Accurately matching the training data, derivative by derivative, would imply that the distributed representations in GPT-J encode information about individual derivatives. We extend the four adjective classes examined so far and include six other adjective classes that can be nominalized with either -ity or -ness: -al, -ar, -ed, -ic, -ing, and -less. We can divide the ten adjective classes into four groups with similar degrees of competition between -ity and -ness (see Supporting Information for details): • -ed, -ing, -ish, -less (r-ness): this group exhibits the highest degree of regularity and almost always takes -ness. • -able, -al, -ar, -ic (r-ity): this group also exhibits a high degree of regularity (although somewhat lower than in the case of r-ness), with a strong tendency toward -ity. • -ous (v-ness): this adjective class exhibits a high degree of variability, with a slight tendency toward -ness. • -ive (v-ity): this adjective class also exhibits a high degree of variability, with a slight tendency toward -ity. We ask whether GPT-J treats adjectives from these four groups differently, and whether differences between the more regular and more variable ones correspond to differences in the training data. We draw upon the Pile and extract all derivatives ending in -ity and -ness whose bases belong to one of the 10 adjective classes. To decrease noise, we only extract derivatives whose bases also occur in the Pile and apply several filtering heuristics, such as excluding words with non-alphabetic characters (see Supporting Information). To include all productively formed derivatives, we do not impose a frequency threshold on the derivatives. (a) The Pile (b) GPT-J Figure 2: Ratio of bases preferring -ness in the Pile (a) and GPT-J’s predictions with one example prompt (b). Results are similar for the other prompts. The suffixes of the base (i.e., adjective classes) are grouped by degree of competition between -ity and -ness. The overall setup of probing GPT-J is identical to the comparison with the cognitive models: we measure the probability that GPT-J assigns to the two derivatives resulting from adding -ity and -ness to the adjectives, using the same set of prompts. Following this procedure, we evaluate GPT-J on all 48,995 bases from the Pile. If not stated otherwise, results are again averaged across prompts. Figure 2 compares, for each adjective class, the ratio of bases for which GPT-J prefers -ness compared to -ity with the statistics from the Pile. We find that the two distributions are very similar: almost no competition for the bases in r-ness (i.e., -ed, -ing, -ish, -less), little competition for the bases in r-ity (i.e., -able, -al, -ar, -ic), and strong competition for v-ness (i.e., -ous) and v-ity (i.e., -ive). The tendency towards -ity and -ness is also exactly as predicted based on the training data — the average correlation between the class-level -ity/-ness ratios in the training data (Figure 2(a)) and GPT-J predictions (Figure 2(b)) is 0.995 (\pm0.004; p< 0.001 for all prompts), measured using Pearson’s r. For multiple comparisons, p-values are corrected using the Holm-Bonferroni method [80]. Table 3: Match between preferred derivatives in the training data and derivatives preferred by GPT-J. Adjective Class Suffix Accuracy r-ness -ed .986\pm.007 -ing .989\pm.014 -ish .995\pm.004 -less .999\pm.001 r-ity -able .896\pm.082 -al .884\pm.073 -ar .896\pm.060 -ic .867\pm.090 v-ness -ous .788\pm.038 v-ity -ive .842\pm.012 Furthermore, GPT-J matches the training data statistics even on the level of individual bases: across all bases, the accuracy of GPT-J’s preference for one of the two derivatives compared against the training data (considered here as the ground truth) is 89.5% (\pm4.8%); the derivative preferred by GPT-J is generally the derivative that is more likely in the training data. Table 3 shows that there is variation between individual adjective classes, with bases in r-ness (-ed, -ing, -ish, -less) having above 95% accuracy, bases in r-ity (-able, -al, -ar, -ic) having above 85% accuracy, and bases in v-ity (-ive) and v-ness (-ous) having below 85% accuracy, but the general level of agreement is very high. Thus, GPT-J’s morphological preferences closely mirror the statistics of the data it was trained on. The fact that GPT-J very consistently prefers the derivative with the higher frequency in the training data, even in cases such as adjectives ending in -ive where the suffix alone is a bad predictor of -ity vs. -ness, suggests that it stores many derivatives in its model weights. This is again in line with an analogical mechanism. However, it is still possible that some of the high-regularity adjective classes (e.g., -ish) are handled by a rule, as suggested by dual-mechanism approaches. Next, we will disentangle these two hypotheses. Frequency Effects and Analogical Pressure To test whether at least part of GPT-J’s behavior on adjective nominalization can be explained by rules, we analyze the extent (on a log probability scale) to which GPT-J prefers the observed nominalized form over the alternative, non-observed nominalized form. We consider only cases for which just one outcome of nominalization is attested in the Pile. The score for the unattested alternative thus represents the latent competition from the new form that might be created by rule or analogy. The difference between the two scores can be viewed as reflecting GPT-J’s confidence in its decision to use a form that it has encountered during training. A large difference indicates high confidence, while a small difference reflects low confidence. For each adjective class, we create two sets: one in which the attested derivative has a low frequency in the Pile, f\in(0,10], and one in which the attested derivative has a high frequency in the Pile, f\in(100,\infty). If an adjective class is handled by a rule, the difference in frequency between the two sets should not affect GPT-J’s confidence to predict the attested derivative. This is because rule-based theories abstract away from individual words; once a rule has been acquired, regular complex forms are assumed to be generated on the fly, much like complex sentence structures are, rather than being stored in memory. Rule-based theories predict that only memorized exceptions to rules will exhibit frequency effects. Many researchers might suggest the following default rule for nominalization (taking NOM to be the underlying morpheme that may be spelled out as -ness or -ity): \text{NOM}\rightarrow\text{{-ness}} (1) Under this assumption, all forms in -ity would be memorized exceptions. Statistical theories of rule-formation, such as MGL, would induce the following narrower rule for r-ness, which has extremely strong statistical support (see Supporting Information for details): \text{NOM}\rightarrow\text{{-ness}}\>/\>\left\{\begin{aligned} &\text{{-ed}}\\ &\text{{-ing}}\\ &\text{{-ish}}\\ &\text{{-less}}\end{aligned}\right\}\>\underline{\hskip 11.38092pt} (2) The following subregularity for r-ity is also a strong candidate for status as a minor rule [81], given the low rate of exceptions if the structural description is met. \text{NOM}\rightarrow\text{{-ity}}\>/\>\left\{\begin{aligned} &\text{{-able}}% \\ &\text{{-al}}\\ &\text{{-ar}}\\ &\text{{-ic}}\end{aligned}\right\}\>\underline{\hskip 11.38092pt} (3) Empirically, the MGL model trained on the Pile contains versions of all of these rules. With respect to GPT-J, we have already seen that the most regular outcomes can be generated by analogy, but could they instead be generated by rule? We can address this question by probing whether the cases that fall under rules 1, 2, or 3 exhibit word frequency effects on GPT-J’s confidence in its decision. If a rule is active for a group of cases, its outputs should be exempt from word frequency effects since the outputs — and hence their individual frequencies — are not stored. Figure 3: Impact of word frequency on GPT-J’s confidence in its choice. x-axis: Log probability difference between the attested and unattested choices for low-frequency derivatives with f\in(0,10]. We have converted the log probabilities from base e to base 10 for better readability. y-axis: Relative increase in confidence for high-frequency derivatives with f\in(100,\infty). Each dot corresponds to GPT-J’s predictions for an adjective class given a specific prompt. Dots are colored by degree of competition between -ity and -ness. We added LOWESS lines for r-ness and r-ity. Dots at y=0\% indicate the expected behavior if r-ness and r-ity were handled by rule. Figure 3 shows GPT-J’s confidence (i.e., the natural log probability difference between the attested and the unattested derivative) for low-frequency derivatives with f\in(0,10] vs. the relative increase in confidence for high-frequency derivatives with f\in(100,\infty) for all adjective classes, divided into the four regularity-based groups. The relative increase in confidence is positive for all adjective classes and for all prompts, indicating that GPT-J is always more confident in its decision for the frequent than the rare derivatives, even for the r-ness class. This indicates that the model has stored distributed representations for all the derivatives, contrary to the predictions of the dual-mechanism model. Put differently, none of the adjective classes are handled by rule. Given that the cognitive models considered above (i.e., MGL and GCM) generate no examples of -ishity (reflecting the non-occurrence of such examples in the training data), it is striking that GPT-J exhibited any uncertainty at all about these cases. Recall, however, that it made actual errors as well (see Generalization to Nonce Words). This outcome can be attributed to the model paying some attention to parts of the word that precede the suffix of the base (e.g., -ish), even though these parts have no linguistic relevance to the form of the nominalization. Note that transformer-based language models such as GPT-J lack the ability to pay zero attention to any part of the input. Overall, the examined adjective classes exhibit a downward slope in Figure 3, which is also reflected by the r-ness and r-ity groups individually (indicated by trendlines). Thus, the more confident the model was in its decisions for the low-frequency group, the smaller the effect of word frequency on confidence. This finding is difficult to explain under the assumption of rules, but we will show that it is perfectly in line with analogy as the underlying generalization mechanism. More specifically, we will attribute the downward slope to variation in the analogical pressure for the different adjective classes. It is well established that frequent words have more robust representations in memory than rare words. This generalization plays a central role in our understanding of how analogy functions in morphology. During language production, analogical processes supply a complex word form if the form has never been encountered during learning. But, they also exert pressure on known forms stored in memory. The better the complex form has been learned as such, the less susceptible it is to these pressures and the more quickly and reliably it is retrieved in its remembered form. This mechanism operating over multiple generations is the one by which frequent forms can retain exceptional inflectional patterns while rare forms are regularized [82, 83]; for example, English say, take, come still retain their exceptional past tense forms (said, took, came) while previously irregular rare verbs, such as writhe, flay, spew do not [84]. A Bayesian model that formalizes analogical pressure, including the frequency effect just described, is developed in [85]. For the most regular r-ness adjective classes, there is little analogical pressure on any specific target form because there are few if any examples that have the suffix followed by -ity. GPT-J has high confidence in its answer, and the weak analogical pressure comes from a few forms that happen to have similarities in the stem. Word frequency, it appears, has a relatively small effect on the confidence when the confidence is already high. Lower confidence levels for the low-frequency forms towards the left of the graph represent cases in which the neighborhood of the target form is more heterogeneous. With increased competition, we observe that the effect of word frequency becomes greater. We quantify this by calculating the Shannon entropy of the distribution over -ity and -ness as the preferred form in the Pile, and using Pearson’s r as a measure of the correlation with the confidence increase for high-frequency derivatives. At r^{2}=0.75, p<0.001, this correlation is highly significant. Thus, the heterogeneity of the exemplar neighborhood for an adjective class predicts the extent to which GPT-J’s confidence relies on frequency — a finding that is exactly in line with the predictions of models assuming analogical pressure [e.g., 85] while being completely at odds with rule-based approaches, which do not assume such frequency effects to begin with. The left side of Figure 3 exhibits more variability than the right side. We believe that this variability is caused by local neighborhood effects and the interaction of these effects with the prompting mechanism. Recall from the discussion of the nonce word pepulative that analogical models are sensitive not merely to the overall statistics for the two competing nominalizations, but also to the similarity and frequency of the most similar neighbors (see Generalization to Nonce Words). These localized effects — which for the case of attested derivatives would also include semantic similarity — create a lumpy prediction landscape whose properties we do not try to quantify here. Meanwhile, the prompting mechanism is known to influence the focus and bias of the underlying transformer model [86]. Slightly different prompts direct the focus towards different parts of the lumpy landscape, and would hence produce noise in the datapoints for Figure 3. To sum up, our analysis suggests that GPT-J learns adjective nominalization by implicitly storing derivatives in its model weights. In cases where the exemplar neighborhood is highly homogeneous, GPT-J produces highly regular outputs, but nonetheless reveals evidence of analogical pressure in its confidence scores. While regular, or rule-like, behavior of LLMs has been observed before [e.g., 18], our results contextualize this finding in important ways, suggesting that rule-like behavior forms the end of a gradient characterized by varying levels of regularity. This result is not consistent with assuming a qualitative difference between forms derived by rule and stored exemplars. It is exactly in line with the predictions of exemplar-based analogy models [e.g., 28, 31, 32]. Human Use of Word Types Versus Tokens We have established that GPT-J relies on token-level analogical generalization. In contrast, previous studies have concluded that humans generalize over word types [87, 88, 89]: their propensity to generalize a word formation pattern depends on the number of distinct word types in the individual’s mental lexicon that support the pattern (referred to as the size of the lexical gang). This points to a difference between the morphological processing in humans and LLMs. We will now investigate this difference in greater detail, by comparing the predictions of GPT-J to judgments made by humans. Judgments of Nonce Words First, we make a direct comparison to GPT-J’s behavior for nonce words. 22 native English speaker volunteer annotators indicated their preference for the -ity versus the -ness derivative of each nonce adjective in our study (Supporting Information). Because GPT-J is not a state-of-the-art model, we also introduce an additional comparison, by asking whether a more recent model is more human-like in its judgments. Specifically, we evaluate GPT-4 [3] on the same set of adjectives (see Materials and Methods, GPT-4 for implementation details). If GPT-4 displays more human-like judgments than GPT-J, then the trend of improving LLMs through larger training sets and bigger model sizes will have paid off in this domain. Table 4: Human evaluation. The table shows the results of rule-based and exemplar-based analogy models as well as GPT-J and GPT-4 evaluated against human annotations. The measure is accuracy. MGL GCM LLMs Suffix Type Token Type Token GPT-J GPT-4 -able 1.000 1.000 1.000 1.000 .893 .960 -ish 1.000 1.000 1.000 1.000 .997 1.000 -ive .720 .680 .760 .700 .632 .440 -ous .560 .520 .640 .520 .503 .400 In Table 4, we take the derivative (with -ity or -ness) more often selected by humans as the ground truth. The table gives the accuracy of GPT-J, GPT-4, as well as the cognitive models considered above (i.e., MGL and GCM), measured against this human response. The type-based GCM model overall matches the human behavior best. While all cognitive models perfectly reproduce the homogeneous behavior for -able and -ish, the type-based GCM model better matches the human predictions for -ive and -ous, as reflected by large gaps compared to the second best model, type-level MGL (-ive: 4%, -ous: 8%). The token-based variants of MGL and GCM match the human behavior substantially worse than the type-based variants, which is exactly in line with what has been suggested in prior work [88, 74]. Moving to the results for GPT-J, it turns out to match the human responses worse than any of the cognitive models, for all four adjective classes. The gap compared to the best cognitive model, type-based GCM, is considerable, especially for -ive and -ous, amounting to roughly 13% in both cases. The picture is overall even worse for GPT-4. While the predictions for the high-regularity classes are good (almost always -ity for -able and -ness for -ish, like all other models), the match with humans is more than 10% worse than GPT-J for both -ive and -ous. Why do GPT-J and GPT-4 match the human behavior so much worse than the much simpler type-based GCM? The key factor, we argue, is that both of these LLMs are driven by the token frequencies of the words in the training data. Just as for GPT-J, the token-based GCM and MGL models match the behavior of GPT-4 better than the type-based models (see Supporting Information). Token-oriented behavior is desirable in that it results in highly realistic implicit knowledge of individual words, as we have seen above. However, humans step back from the frequencies of individual words when making generalizations about possible words. LLMs seem to lack the ability to do this. Furthermore, our results suggest that GPT-4’s over-reliance on token frequency is if anything worse than that of GPT-J. Thus GPT-4’s morphological generalization behavior seems to be even less human-like than that of GPT-J. This finding is reminiscent of recently reported ‘inverse scaling effects’, more specifically the tendency of larger LLMs to rely even more strongly on prior statistics from the training data than smaller models do [90]. Since the performance of the best model, the type-based GCM, leaves room for improvement, we can ask why its performance was not better. The single biggest discrepancy was that the GCM selected -ness after -ous more than the humans did. Our analysis does not deal with the possibility that some people may consider the affix -osity to be a unified affix bundle, along the lines suggested in [91, 92]; this would enhance its availability. Given that the GCM was fit to all word pairs attested in the Pile, the analysis also failed to allow for differences amongst human mental lexicons (whether in the entries themselves, or in the extent to which morphological relations between entries have been adduced). Other studies have found considerable variability amongst human participants in the area of derivational morphology in general, and in preference for -ity over -ness specifically [93, 94, 95, 96, 97]. In this context, it is noteworthy that all participants of our annotation study held at least a college degree (see Supporting Information for details), whereas much of the Pile consists of web data such as informal discussions on Reddit or StackExchange [72]. There is thus the possibility that there was a misalignment between the sociolect most strongly represented in the Pile and the ideolects sampled as part of our annotation study. Finally, the GCM works on the basis of word forms only, and has no way of taking into account similarities in meaning that also play a role in shaping morphological systems. In contrast, LLMs like GPT-J and GPT-4 do have the ability to consider similarities in meaning, but any advantage they may gain from their semantics appears to be more than offset by the drawbacks of their reliance on token frequencies. Familiarity of Complex Words Our results on nominalizations indicate that GPT-J and GPT-4 do not have a mental lexicon in the sense that humans do, in that they lack the ability to step back from word tokens and generalize over word types. Here, we present a brief demonstration that this observation pertains to morphologically complex words more generally, and not just to nominalizations. For this demonstration, we draw on the Hoosier Lexicon, a dataset of 19,320 English words that includes word frequencies and familiarity ratings on a seven-point Likert Scale [98]. An important finding of the original study was a dissociation between word frequency and rated familiarity; one might expect the two to be highly correlated, however some infrequent words are judged as much more familiar than their frequency would suggest. Needle et al. [99] identify morphological structure as an important factor contributing to this dissociation. A word like precancellation, with a recognizable prefix, stem, and affix seems familiar even though it is rare, on the strength of the familiarity of its parts. We analyze the n=2,835 words in the Hoosier lexicon that have a frequency of less than 10,000 in the Pile (corresponding to a frequency of roughly 1 in 50,000,000 words or less). Leveraging the CELEX dictionary [54] and methodology from prior work [100, 101], we use affix-stripping to identify the n=1,005 words that exemplify derivational morphology by virtue of being parsable as a simpler word plus any combination of affixes. n=1,830 words cannot be parsed in this way, and we consider them to be simplex words (see Supporting Information). For human judgments, we take the familiarity ratings reported by Nusbaum et al. [98]. We estimate the ‘familiarity’ that GPT-J assigns to a word as the log probability that it assigns in the context of neutral prompts (see Materials and Methods, Vocabulary Test for details). Comparing log probabilities to human familiarity ratings is justified because the probabilities assigned to words by language models are known to correlate with psycholinguistic measures of lexical access [e.g., reading times; 102], which for humans are impacted by familiarity to a larger extent than frequency [103]. (a) Humans (b) GPT-J Figure 4: Impact of morphological decomposability of words on their familiarity as rated by human annotators (a) and the log probability assigned to them by GPT-J (b). Results for humans are displayed in Figure 4(a). The average familiarity of words with a morphological parse (n=1,005) is significantly higher than the average familiarity of words with no morphological parse (n=1,830), t(2120.2)=19.2, p<.001 by a Welch’s t-test. This confirms the results reported by Needle et al. [99]. Due to this important factor, the correlation between familiarity and log frequency in the entire Hoosier lexicon proves to be modest according to a linear regression, F(1,19318)=11251.2, R^{2}=0.368, p<.001. For GPT-J, on the other hand, words with a morphological parse do not have any advantage (Figure 4(b)); quite the opposite, the estimated familiarity of words with a morphological parse is significantly lower than the estimated familiarity of words with no morphological parse for GPT-J, t(2285.9)=-4.9, p<.001. This outcome can be explained by the fact that the correlation between the log frequencies and the log probabilities assigned to the words by GPT-J is very high, F(1,19318)=58553.5, R^{2}=0.752, p<.001, and the target words without a parse have somewhat higher average frequency (m=4285.1) than those having a parse (m=4093.7). Experimental studies on wordlikeness judgments [99] and on speech perception [104, 105, 106] show that humans continually monitor for known words inside rare or novel words. This means that their type-level lexical representations are exploited during processing, and can cause rare words to seem familiar. GPT-J does not rely on this type-level mechanism and hence lacks the dissociation between frequency and familiarity that is caused by morphological structure."
https://arxiv.org/html/2411.07979v2,"Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization","Second-order optimization has been shown to accelerate the training of deep neural networks in many applications, often yielding faster progress per iteration on the training loss compared to first-order optimizers. However, the generalization properties of second-order methods are still being debated. Theoretical investigations have proved difficult to carry out outside the tractable settings of heavily simplified model classes – thus, the relevance of existing theories to practical deep learning applications remains unclear. Similarly, empirical studies in large-scale models and real datasets are significantly confounded by the necessity to approximate second-order updates in practice. It is often unclear whether the observed generalization behaviour arises specifically from the second-order nature of the parameter updates, or instead reflects the specific structured (e.g. Kronecker) approximations used or any damping-based interpolation towards first-order updates.Here, we show for the first time that exact Gauss-Newton (GN) updates take on a tractable form in a class of deep reversible architectures that are sufficiently expressive to be meaningfully applied to common benchmark datasets. We exploit this novel setting to study the training and generalization properties of the GN optimizer. We find that exact GN generalizes poorly. In the mini-batch training setting, this manifests as rapidly saturating progress even on the training loss, with parameter updates found to overfit each mini-batchatch without producing the features that would support generalization to other mini-batches. We show that our experiments run in the “lazy” regime, in which the neural tangent kernel (NTK) changes very little during the course of training. This behaviour is associated with having no significant changes in neural representations, explaining the lack of generalization.","Efficient optimization of overparameterized neural networks is a major challenge for deep learning. For large models, training remains one of the main computational and time bottlenecks. Much work has therefore been devoted to the development of neural network optimizers that could accelerate training, enabling researchers and engineers to iterate faster and at lower cost in their search for better performing models. Second-order optimizers, in particular, have been shown to deliver substantially faster per-iteration progress on the training loss (Martens and Grosse, 2015; Botev et al., 2017; George et al., 2018; Goldfarb et al., 2020; Bae et al., 2022; Petersen et al., 2023; Garcia et al., 2023), and much work has been done to scale them to large models via suitable approximations (Ba et al., 2017; Anil et al., 2021). However, the generalization properties of second-order optimizers remain poorly understood. Here, we focus on the training and generalization properties of the Gauss-Newton (GN) method, which – in many cases of interest – also encompasses natural gradient descent (NGD) (Martens, 2020). Theoretical studies of generalization in GN/NGD have been limited to simplified models, such as linear models (Amari et al., 2021) or nonlinear models taken to their NTK limit (Zhang et al., 2019). When applied to real-world networks and large datasets, GN/NGD has so far required approximations, such as truncated conjugate gradient iterations in matrix-free approaches (Martens et al., 2010), or block-diagonal and Kronecker-factored estimation of the Gauss-Newton / Fisher matrix (Martens and Grosse, 2015; Botev et al., 2017; George et al., 2018; Goldfarb et al., 2020; Bae et al., 2022; Petersen et al., 2023; Garcia et al., 2023). Those approximations are exact only in the limit of constant NTK (Karakida and Osawa, 2020), in which models cannot learn any features (Yang and Hu, 2021; Aitchison, 2020). To our knowledge, the only case in which exact and tractable GN updates have been obtained is that of deep linear networks (Bernacchia et al., 2018; Huh, 2020), which – despite exhibiting non-trivial learning dynamics (Saxe et al., 2013) – cannot learn interesting datasets nor yield additional insights into generalization beyond the linear regression setting. Critically, the use of necessary approximations makes it difficult to understand how much of the observed generalization (or lack thereof) can be attributed to the GN method itself, or to the various ways in which it has been simplified. Here, we derive an exact, computationally tractable expression for Gauss-Newton updates in deep reversible networks (Dinh et al., 2015; Mangalam et al., 2022). In reversible architectures made of stacked, volume-preserving MLP-based coupling layers (which we call RevMLPs), we show that it is possible to analytically derive a specific form of a generalized inverse for the network’s Jacobian. This generalized inverse enables fast, exact GN updates in the overparameterized regime. We highlight that, in contrast to the work of Zhang et al. (2019); Cai et al. (2019); Rudner et al. (2019); Karakida and Osawa (2020), we do not assume constant NTK, instead we only require the NTK to remain non-singular during training (Nguyen et al., 2021; Liu et al., 2022; Charles and Papailiopoulos, 2018) as, for example, in the mean-field limit (Arbel et al., 2023). Equipped with this new model, we study for the first time the generalization behaviour of GN in realistic settings. In the stochastic regime, we find that GN trains too well, overfitting single mini-batch at the expense of impaired performance not only on the test set, but also on the training set. To understand this severe lack of generalization, we conduct a careful examination of the model’s neural tangent kernel and show that the NTK remains almost unchanged during training, and that the neural representations that arise from after training are not different from those set by the network’s initialization. Thus, GN tends to remain in the “lazy” regime (Jacot et al., 2018; Chizat et al., 2019), in which representations remain close to those at initialization, lacking generalization. In summary: • We show that GN updates computed with any generalized inverse of the model Jacobian results in the same dynamics of the loss, provided that the NTK does not become singular during training (Theorem 4.3). • We derive an exact and tractable generalized inverse of the Jacobian in the case of deep reversible neural networks (Proposition 4.4). The corresponding GN updates have the same complexity as gradient descent. • We study the generalization properties of GN in models up to 147 million parameters on MNIST and CIFAR-10, and we show that neural representations do not change during training, as the model remains in the “lazy” regime."
https://arxiv.org/html/2411.07976v2,DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring,"Coronary artery disease (CAD), one of the most common cause of mortality in the world. Coronary artery calcium (CAC) scoring using computed tomography (CT) is key for risk assessment to prevent coronary disease. Previous studies on risk assessment and calcification detection in CT scans primarily use approaches based on UNET architecture, frequently implemented on pre-built models. However, these models are limited by the availability of annotated CT scans containing CAC and suffering from imbalanced dataset, decreasing performance of CAC segmentation and scoring. In this study, we extend this approach by incorporating the self-supervised learning (SSL) technique of DINO (self-distillation with no labels) to eliminate limitations of scarce annotated data in CT scans. The DINO model’s ability to train without requiring CAC area annotations enhances its robustness in generating distinct features. The DINO model is trained on to focus specifically on calcified areas by using labels, aiming to generate features that effectively capture and highlight key characteristics. The label-guided DINO (DINO-LG) enhances classification by distinguishing CT slices that contain calcification from those that do not, performing 57% better than the standard DINO model in this task. CAC scoring and segmentation tasks are performed by a basic U-NET architecture, fed specifically with CT slices containing calcified areas as identified by the DINO-LG model. This targeted identification performed by DINO-LG model improves CAC segmentation performance by approximately 10% and significant increase in CAC scoring accuracy.","Cardiovascular disease (CVD) is the leading cause of death globally, responsible for approximately 17.9 million fatalities in 2019, which constitutes 32% of all deaths worldwide Organization (2021). Coronary artery disease (CAD), a major cardiovascular disease affecting the blood vessels that feed the heart muscle, caused 371,506 deaths in the United States in 2022 for Health Statistics (2024). According to the most recent heart disease and stroke statistics report, approximately 5% of adults over the age of 20 have CAD in the United States Tsao et al. (2023). Early detection of CAD allows for timely interventions that can prevent progression of the disease and reduce the risk of life-threatening heart attacks. It can also lead to a wider range of treatment options, including lifestyle changes, medications or surgical procedures. The earlier CAD is detected, the more effective these treatments can be. Furthermore, overall treatment costs can be reduced by preventing more serious complications requiring intensive care or extensive procedures. Coronary artery calcium (CAC) scoring is considered a reliable tool for assessing cardiovascular disease tool and is generally recommended for use by various guidelines Knuuti et al. (2020). CAC scoring helps identify the presence and extent of calcified plaque in the coronary arteries, which is strongly associated with the risk of CAD and future cardiovascular events. The test is non-invasive and relatively simple, using computer tomography (CT) scans to measure calcium deposits without the need for invasive procedures. The risk categorized calcium scores reflect different risk categories for cardiovascular events Oudkerk et al. (2008). Higher CAC scores can lead to more aggressive management of risk factor, while lower scores might support a more conservative approach. It is crucial for the radiographer to evaluate the position of high-density voxels in order to detect coronary calcification. CAC is typically measured using the Agatston method, assessing calcium deposits in the coronary arteries by measuring calcium density and volume to calculate a total calcium score Agatston et al. (1990). The Agatston score interval is a system for assessing coronary artery calcium deposition and is usually categorized as follows: 0 (no calcium, low cardiovascular risk), 1-10 (minimal calcium, low risk), 11-100 (low levels of calcium, moderate risk), 101-400 (moderate calcium, high risk), and over 400 (high levels of calcium, very high risk) Hecht (2015). This range of scores helps determine the risk of heart disease and plays an important role in clinical decision-making. Currently, the clinical analysis of calcium scores is performed semi-automatically by a software tool used by the radiologist to identify calcium regions by individually checking the slide images of each patient. This CAC measurement can be attention-demanding, labor-intensive, and time-consuming. To address these issues, automated CAC scoring methods are being developed, which can help enhance accuracy, consistency, and efficiency in measurements Eng et al. (2021a); van Assen et al. (2021); Takahashi et al. (2023a). Clinically, contrast-enhanced coronary CT angiography is a powerful imaging technique that uses contrast agent to provide detailed images of the coronary arteries to detect obstructive lesions and other vascular abnormalities, but involves higher radiation exposure due to the need for additional imaging sequences and the use of contrast Wolterink et al. (2016). On the other hand, non-contrast ECG-gated CT scans focus on quantifying coronary artery calcium, synchronizing image acquisition with the cardiac cycle to minimize motion artifacts and accurately assess calcified plaque for cardiovascular risk evaluation. Traditionally, when the CAC score has been calculated using non-contrast ECG-gated CT scans, a significant association with clinical outcomes has been observed Takx et al. (2015); Chiles et al. (2015). However, other studies have shown that CAC calculation using non-contrast ECG non-gated CT data has excellent agreement with gated data Raygor et al. (2023); Liu et al. (2022); Zeleznik et al. (2021a); Kerndt et al. (2023a). In this study, we propose a self-supervised learning based CAC scoring method that uses both gated and non-gated CT images together. Most efforts in deep learning for image analysis have been using supervised learning. However, existing supervised learning methods face challenges when there is a lack of labeled data. In public datasets of CT scans, for instance, less than 10% of the total slices contain calcified areas, despite the overall abundance of scans. This scarcity of labeled, relevant data poses a significant challenge for supervised approaches. Additionally, considering that calcifications typically cover only 3-5 millimeters, it becomes evident how challenging this task is. This small size makes accurate detection even more difficult, underscoring the need for advanced methods capable of identifying such minute details in CT scans. Another learning paradigm that effectively addresses the mentioned data scarcity challenges is self-supervised learning (SSL) Nielsen et al. (2023); Truong et al. (2021); Huang et al. (2023b). In traditional computer vision tasks, there is typically a large amount of unlabelled data available for SSL methods. SSL holds significant promise for enhancing coronary artery calcium scoring, particularly in the context of limited labeled data. In traditional supervised learning approaches, obtaining sufficient labeled datasets can be challenging and time-consuming, especially in the clinical domain where calcified medical images are often rare. SSL allows models to learn from vast amounts of unlabeled data by extracting features and patterns without the need for explicit annotations Huang et al. (2023b). This capability is particularly valuable for CAC scoring, as it enables more robust training of algorithms to identify calcium deposits in coronary arteries. Furthermore, while several studies have successfully developed automated CAC scoring using supervised learning methods, there is currently no reported method using self-supervised learning model on both gated and non-gated non-contrast ECG CTs. In this study, we have implemented one of the most popular SSL training technique using vision transformers (ViT) and commonly known as DINO (self-distillation with no labels) Caron et al. (2021). Regarding the architectural design of ViT models, generated features by ViT model in DINO model make possible to use these features in different tasks such as classification, segmentation or detection. In our approach, ViT models trained with DINO technique are utilized to classify CT slices whether containing calcified area. When it is considered that the area covered by calcification in CT slices and the ratio of CT slices containing calcification areas to across all CT slices in dataset, generating the features highlighting and capturing calcification areas is a challenging task. To overcome this issue and generate the features capturing targeted areas’ specifications, we introduce a novel training method, Label-Guided DINO, aimed at enhancing SSL approaches by incorporating label guidance to capture more specific features such as calcified areas in the model’s training process. This new technique is designed to contribute to SSL methodologies and expand upon existing training frameworks. Additionally, it has been demonstrated that vision foundational models can be directed toward specific areas, enabling them to generate features that effectively capture desired characteristics. The contributions of this paper can be summarized as follows: • We propose a novel training technique, developed to train a DINO model for targeted tasks by guiding it to capture highly specific features in its generated representations. This approach enhances the model’s ability to focus on relevant characteristics within the data by leveraging label guidance to direct its attention. • We introduce the DINO-LG model, a self-supervised learning (SSL) framework adaptable to various CT scan types. Designed as a foundational model, DINO-LG is capable of highlighting specific features in CT scans, making it versatile for a wide range of applications and tasks across medical imaging domains. • Our proposed system seamlessly integrates self-supervised learning (DINO-LG), classification, and segmentation models to deliver a fully automated coronary calcium scoring solution, which reduces manual analysis requirements and enables consistent and efficient CAC assessment."
https://arxiv.org/html/2411.07975v1,JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation,"We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.","(a) Benchmark Performances. (b) Visual Generation Results. Figure 1: Multimodal understanding and image generation with JanusFlow. JanusFlow surpasses the state-of-the-art unified multimodal models and several task-specific understanding models on visual understanding benchmarks. It is also capable of generating high-quality images. The resolution of the images is 384\times 384. Large language models (LLMs) have demonstrated remarkable capabilities in learning diverse knowledge and generalizing to new scenarios [88, 68, 1, 8, 7]. Leveraging these capabilities, researchers have developed sophisticated models specialized in image comprehension [57, 56, 47, 2, 15, 49] and text-to-image generation [77, 74, 71, 23]. The field has recently shifted toward creating unified systems capable of handling both tasks simultaneously. One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models [25, 26, 84, 27, 19]. However, this approach introduces architectural complexity and potentially constrains the model’s capabilities through maintaining separate LLM and generative components. Alternative approaches [85, 95, 96, 103, 93] propose training a single LLM for both tasks, typically incorporating either diffusion models [32, 80] or vector-quantized autoregressive models [22, 83]. Our approach builds upon recent breakthroughs in rectified flow models [60, 55, 3, 23, 61], which provide a simple framework for generative modeling while delivering exceptional empirical performance [23, 45, 36]. Building on these advances, we propose JanusFlow, a powerful unified multimodal model that seamlessly integrates rectified flow with LLM architecture. Following a minimalist design principle, our architecture requires only a lightweight encoder and decoder to adapt the LLM for rectified flow operations. To optimize JanusFlow’s performance, we implement two key strategies: First, we maintain separate vision encoders for understanding and generation tasks, preventing task interference and thus enhancing comprehension capabilities. Second, we align the intermediate representations between generation and understanding modules during training, strengthening semantic coherence in the generation process. JanusFlow shows state-of-the-art performances in both multimodal comprehension and text-to-image generation compared to existing unified approaches, and even outperforms several specialized methods. Specifically, on text-to-image generation benchmarks, MJHQ FID-30k [48], GenEval [28] and DPG-Bench [34], JanusFlow achieves scores of 9.51, 0.63 and 80.09\%, surpassing established text-to-image models including SDv1.5 [75] and SDXL [71]. In multimodal comprehension benchmarks, JanusFlow attains scores of 74.9, 70.5 and 60.3 on MMBench [62], SeedBench [46], and GQA [35], respectively, exceeding specialized models such as LLaVA-v1.5 [56] and Qwen-VL-Chat [4]. Notably, these results are achieved with a compact LLM architecture with only 1.3B parameters."
https://arxiv.org/html/2411.07941v1,"DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays 
with Generative Adversarial Networks","Computed tomography (CT) provides highly detailed three-dimensional (3D) medical images but is costly, time-consuming, and often inaccessible in intraoperative settings (Organization et al. 2011). Recent advancements have explored reconstructing 3D chest volumes from sparse 2D X-rays, such as single-view or orthogonal double-view images. However, current models tend to process 2D images in a planar manner, prioritizing visual realism over structural accuracy. In this work, we introduce DuoLift Generative Adversarial Networks (DuoLift-GAN), a novel architecture with dual branches that independently elevate 2D images and their features into 3D representations. These 3D outputs are merged into a unified 3D feature map and decoded into a complete 3D chest volume, enabling richer 3D information capture. We also present a masked loss function that directs reconstruction towards critical anatomical regions, improving structural accuracy and visual quality. This paper demonstrates that DuoLift-GAN significantly enhances reconstruction accuracy while achieving superior visual realism compared to existing methods.","Radiography, or X-ray imaging, is a medical imaging procedure that produces 2D projection images of a scanned patient with relatively low radiation doses and fast acquisition speeds. In contrast, computed tomography (CT)(Buzug 2011) is an imaging modality that uses a series of X-rays (typically between 100 and 250) taken at different angles around 360 degrees from which a three-dimensional volume is reconstructed. A key advantage of CT imaging is its ability to provide precise spatial information about anatomical structures, such as the lungs, in 3D space. However, CT procedures are time-consuming and expensive. X-rays, by comparison, are quicker, less costly, and result in significantly less radiation exposure (Organization et al. 2011). 2D X-ray images lack the 3D spatial information necessary for medical problems requiring quantitative morphological analysis. Thus, reconstructing a 3D CT volume from orthogonal double-view X-rays or a single-view X-ray could provide approximate 3D information and combine X-ray and CT imaging benefits. Although full 3D precision cannot be expected in such an approach, a significant 3D context may be inferred by learning from data. The primary challenge in reconstructing a 3D CT image from two orthogonal X-ray images or a single-view X-ray image is the need for more spatial information, particularly along the axis perpendicular to the 2D X-ray image. X-ray measurements capture the attenuation of X-rays as they pass through various tissues along their path. As an inverse problem, CT reconstruction typically requires projections densely sampled over 180 degrees. Fewer projections increase the ambiguity in solving this inverse problem using conventional CT reconstruction methods, such as Filtered Backprojection (Chetih and Messali 2015). However, with recent advancements in deep learning and the availability of large-scale medical image datasets, researchers have developed CT reconstruction models (Ying et al. 2019; Kasten, Doktofsky, and Kovler 2020; Ge et al. 2022; Gao et al. 2023; Song et al. 2021; Kyung et al. 2023; Sun et al. 2023) that can reconstruct 3D volumes from orthogonal double-view X-rays or even single-view X-ray images by leveraging learned priors from datasets. Some studies concentrate on dense anatomical structures, such as the knees (Kasten, Doktofsky, and Kovler 2020), cervical vertebrae (Ge et al. 2022), and spine (Gao et al. 2023), which contain bones with high attenuation coefficients, resulting in high contrast in radiographic images. Other research focuses on soft tissue reconstruction, particularly the lung (Ying et al. 2019; Kyung et al. 2023; Sun et al. 2023), an air-filled organ with a low attenuation coefficient inside. However, existing methods primarily target the reconstruction of the lung’s overall shape and surrounding soft tissues, often neglecting the intricate internal structures, such as the numerous thin, tree-like pulmonary vessels. These delicate structures are relatively smaller than the lung, resulting in subtle and complex features in 2D radiographic images. To address the challenges in 2D-to-3D reconstruction, we considered a method of lifting 2D images and features to 3D in advance to mitigate the difficulty associated with learning the mapping relationship between different dimensions. So we introduce the Duo Lift Generative Adversarial Networks (DuoLift-GAN), using a masked loss function; our model enhances detail capture by aligning the reconstruction with the target chest volume. By replicating the original 2D images and their corresponding feature maps along a specific axis multiple times, we elevate 2D images and their features into 3D representations that preserve spatial relationships and spatial coherence, resulting in more accurate 3D feature maps and improved reconstruction quality. Additionally, for precise shape and contour reconstruction, we present DuoLift-CNN, which accurately reconstructs larger anatomical structures, such as the lungs, which are suitable for capturing the overall structure rather than fine and thin structures, such as the vessels. Moreover, we observed an intriguing phenomenon during our evaluation of the reconstruction results: while GANs tend to produce volumes with richer textural details, CNN models outperform them in numerical metrics. To further explore this discrepancy, we conducted an in-depth analysis of the evaluation metrics used for CT reconstruction, focusing on the Structural Similarity Index Measure (SSIM)(Wang et al. 2004), Peak Signal-to-Noise Ratio (PSNR)(Hore and Ziou 2010), and the Learned Perceptual Image Patch Similarity (LPIPS)(Zhang et al. 2018) metric. This trend is also evident in the quantitative results of X2CT(Ying et al. 2019). Moreover, we conduct the anatomical structure level quantitative analysis of the reconstruction quality via the Dice Coefficient (DICE)(Dice 1945) of the lung and vessel segmentation maps. With the broad spectrum of evaluation metrics from pixel level, anatomical structure level, to perceptual level, we conduct a comprehensive quantitative and qualitative analysis of our methods and the state-of-the-art method on one lung CT dataset, the LIDC-IDRI dataset. Our analysis evaluates the reconstruction performance and provides insightful observations for the chest CT reconstruction problem from orthogonal X-rays. In conclusion, our contributions include: 1. We introduce a novel 2D-to-3D reconstruction architecture with dual branches that independently elevate 2D images and their feature maps into 3D representations. These branches preserve spatial relationships and 3D structure, with their outputs merged into a unified 3D feature map. Additionally, we employ a masked loss to train the generator in conjunction with a discriminator, which improves the accuracy of generated textural details. 2. Our models, DuoLift-GAN and DuoLift-CNN, achieve the best performance on benchmarks compared to methods with available code. 3. We conduct an in-depth evaluation of the reconstruction quality on the anatomical structure level. 4. We will release our code and pre-trained model weights on GitHub, promoting further research and development."
https://arxiv.org/html/2411.07934v2,"Doubly Mild Generalization for Offline 
Reinforcement Learning","Offline Reinforcement Learning (RL) suffers from the extrapolation error and value overestimation. From a generalization perspective, this issue can be attributed to the over-generalization of value functions or policies towards out-of-distribution (OOD) actions. Significant efforts have been devoted to mitigating such generalization, and recent in-sample learning approaches have further succeeded in entirely eschewing it. Nevertheless, we show that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. To appropriately exploit generalization in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild action generalization and (ii) mild generalization propagation. The former refers to selecting actions in a close neighborhood of the dataset to maximize the Q values. Even so, the potential erroneous generalization can still be propagated, accumulated, and exacerbated by bootstrapping. In light of this, the latter concept is introduced to mitigate the generalization propagation without impeding the propagation of RL learning signals. Theoretically, DMG guarantees better performance than the in-sample optimal policy in the oracle generalization scenario. Even under worst-case generalization, DMG can still control value overestimation at a certain level and lower bound the performance. Empirically, DMG achieves state-of-the-art performance across Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offline to online learning and attains strong online fine-tuning performance.","Reinforcement learning (RL) aims to solve sequential decision-making problems and has garnered significant attention in recent years [53, 67, 74, 63, 12]. However, its practical applications encounter several challenges, such as risky exploration attempts [20] and time-consuming data collection phases [35]. Offline RL emerges as a promising paradigm to alleviate these challenges by learning without interaction with the environment [40, 42]. It eliminates the need for unsafe exploration and facilitates the utilization of pre-existing large-scale datasets [31, 48, 59]. However, offline RL suffers from the out-of-distribution (OOD) issue and extrapolation error [19]. From a generalization perspective, this well-known challenge can be regarded as a consequence of the over-generalization of value functions or policies towards OOD actions [47]. Specifically, the potential value over-estimation at OOD actions caused by intricate generalization is often improperly captured by the max operation [73]. This over-estimation will propagate to values of in-distribution samples through Bellman backups and further spread to values of OOD ones via generalization. In mitigating value overestimation caused by OOD actions, substantial efforts have been dedicated [19, 39, 38, 17] and recent advancements in in-sample learning have successfully formulated the Bellman target solely with the actions present in the dataset [37, 85, 92, 88, 21] and extracted policies by weighted behavior cloning [57, 80]. As a result, these algorithms completely eschew generalization and avoid the extrapolation error. Despite simplicity, this way can not take advantage of the generalization ability of neural networks, which could be beneficial for performance improvement. Until now, how to appropriately exploit generalization in offline RL remains a lasting issue. This work demonstrates that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. For appropriate exploitation of mild generalization, we propose Doubly Mild Generalization (DMG) for offline RL, comprising (i) mild action generalization and (ii) mild generalization propagation. The former concept refers to choosing actions in the vicinity of the dataset to maximize the Q values. However, the mere utilization of mild action generalization still falls short in adequately circumventing potential erroneous generalization, which can be propagated, accumulated, and exacerbated through the process of bootstrapping. To address this, we propose a novel concept, mild generalization propagation, which involves reducing the generalization propagation while preserving the propagation of RL learning signals. Regarding DMG’s implementation, this work presents a simple yet effective scheme. Specifically, we blend the mildly generalized max with the in-sample max in the Bellman target, where the former is achieved by actor-critic learning with regularization towards high-value in-sample actions, and the latter is accomplished using in-sample learning techniques such as expectile regression [37]. We conduct a thorough theoretical analysis of our approach DMG in both oracle and worst-case generalization scenarios. Under oracle generalization, DMG guarantees better performance than the in-sample optimal policy in the dataset [38, 37]. Even under worst-case generalization, DMG can still upper bound the overestimation of value functions and guarantee to output a safe policy with a performance lower bound. Empirically111Our code is available at https://github.com/maoyixiu/DMG., DMG achieves state-of-the-art performance on standard offline RL benchmarks [16], including Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG can seamlessly transition from offline to online learning and attain superior online fine-tuning performance."
https://arxiv.org/html/2411.07885v1,"INTRABENCH: 
Interactive Radiological Benchmark","Current interactive segmentation approaches, inspired by the success of META’s Segment Anything model, have achieved notable advancements, however they come with substantial limitations that hinder their practical application in real clinical scenarios. These include unrealistic human interaction requirements, such as slice-by-slice operations for 2D models on 3D data, a lack of iterative refinement, and insufficient evaluation experiments. These shortcomings prevent accurate assessment of model performance and lead to inconsistent outcomes across studies.IntRaBench overcomes these challenges by offering a comprehensive and reproducible framework for evaluating interactive segmentation methods in realistic, clinically relevant scenarios. It includes diverse datasets, target structures, and segmentation models, and provides a flexible codebase that allows seamless integration of new models and prompting strategies. Additionally, we introduce advanced techniques to minimize clinician interaction, ensuring fair comparisons between 2D and 3D models. By open-sourcing IntRaBench, we invite the research community to integrate their models and prompting techniques, ensuring continuous and transparent evaluation of interactive segmentation models in 3D medical imaging.","Accurate segmentation of anatomical structures or pathological areas is crucial in fields like radiology, oncology, and surgery to isolate affected regions, monitor disease progression, treatment planning and guide therapeutic procedures. Traditional supervised medical segmentation models have demonstrated strong performance across a range of anatomies and pathologies (Isensee et al., 2020; 2023; Huang et al., 2023; Ulrich et al., 2023). However, their effectiveness remains heavily constrained by the amount and diversity of available training data, with the quality of human label annotations serving as a critical limiting factor. Consequently, fully autonomous AI solutions have not yet reached performance needed for widespread autonomous clinical applications. On the other hand, numerous semi-automatic segmentation techniques, not reliant on AI, are already in clinical practice to expedite manual annotation processes Hemalatha et al. (2018). These current ad hoc methods do not tap into the potential of AI-based automation to drastically reduce annotation time. A method that allows clinicians to segment any target with just a single click within the image could greatly enhance the efficiency of clinical workflows. The release of META’s Segment Anything (SAM) model represents a big leap towards making this potential a reality (Kirillov et al., 2023). ”SAM” is designed to segment any target through different user interaction methods, including point-based and bounding box prompts. This allows users to easily specify the area of interest by clicking on it or drawing a bounding box around it, making the segmentation process both flexible and intuitive. A particularly powerful feature is the ability for users to iteratively refine initial predictions by adding more positive or negative prompts. This advanced functionality, in contrast to traditional supervised segmentation methods, has attracted a lot of attention in the medical domain, and led to many studies evaluating and adapting SAM for 3D medical image segmentation (Roy et al., 2023; Deng et al., 2023; Hu et al., 2023; Zhou et al., 2023; Mohapatra et al., 2023; Cheng et al., 2023; Ma et al., 2024; Gong et al., 2023). Moreover, several researchers have been inspired by SAM’s capabilities to develop their own methods, often specifically designed for the 3D nature of radiological data (Du et al., 2024; He et al., 2024; Li et al., 2024; Wang et al., 2024). Although these domain-specific adaptations on medical data have shown promising progress, many published methods are plagued by pitfalls which obfuscate the efficacy of the models and prevent clinicians and researchers from determining the best methods for their use-cases: Figure 1: a) Current approaches require clinicians to interact with radiological images slice by slice, leading to increased workload. b) Some models operate natively in 3D and enable full 3D interaction. Only models that accept mask prompts allow iterative refinement of initial predictions with human guidance. Applying interactive 2D models to 3D data on a slice-by-slice basis (P1): Assuming clinicians will interact with each slice individually is unrealistic and undermines the efficiency improvements these methods aim for. Moreover, a slice-by-slice approach introduces an unfair bias when comparing 2D and 3D models, as 3D models typically require only a few interaction per image, leading to significantly fewer interactions and less supervision Cheng et al. (2023); Ma et al. (2024); Zhang & Liu (2023); Wu et al. (2024); Wong et al. (2024). Neglecting refinement (P2): Many studies assess interactive segmentation methods based on a single interaction step, overlooking the inherent ambiguities in radiological images (Ma et al., 2024; Du et al., 2024; Gong et al., 2023; Bui et al., 2024). Often, a second interaction may be necessary to specify which specific substructure the clinician wants to segment. This could be, e.g. a vessel within the liver, or the necrosis within a tumor, as exemplified in the well-known BraTs segmentation challenge (de Verdier et al., 2024). Furthermore, clinicians often want to adapt the segmentations to their clinic’s local protocol or refine them, particularly for targets with high inter-rater variability, like pathological structures (Fu et al., 2014; Benchoufi et al., 2020; Hesamian et al., 2019). Overall, there is a notable lack of research exploring realistic, iterative interaction methods for 2D models applied to 3D volumes. Obfuscated and insufficient evaluation (P3): With promptable models only recently garnering great attention, there is a lack of a standardized approach to evaluation, which has led to disparate and incomparable methods, which are at times even obfuscated or insufficient. We observed the following shortcomings: (i) Not specifying whether predictions were interactively refined or based on a single prompt with multiple points (Cheng et al., 2023; Wang et al., 2024). (ii) Being intransparent on the number of initial prompts given (Du et al., 2024). (iii) Using the best mask rather than the final mask after interactive refinement (Wang et al., 2024). (iv) Evaluating predictions slice-by-slice or on sub-patches of a 3D volume instead of evaluating on the full image (Roy et al., 2023; Ma et al., 2024; Cheng et al., 2023; He et al., 2024; Li et al., 2024). (v) Excluding targets considered ’too small’, hence neglecting valid targets such as small lesions that are neither tested nor trained on Ma et al. (2024); Cheng et al. (2023); Wang et al. (2024). (vi) Comparing against non-promptable models and SAM, rather than any other promptable model trained on medical data (Cheng et al., 2023; Ma et al., 2024; Gong et al., 2023; He et al., 2024). (vii) Lastly, overemphasizing segmenting healthy structures, such as organs, where existing supervised public models already perform well (Wasserthal et al., 2023; Ulrich et al., 2023), instead of focusing on pathologies, where interactive refinement could provide the greatest benefits (Wang et al., 2024; Zhang & Liu, 2023). To address these pitfalls, a benchmark is needed, aligning with the recent review paper from Marinov et al. (2024). To this end, we introduce IntRaBench, a reproducible and extendable Interactive Radiological Benchmark. Through it, we highlight the most performant 2D and 3D interactive segmentation and the best prompting methods in the radiological domain. In this paper, we present experiments carefully designed to replicate a clinical workflow as closely as possible, with the following key contributions: 1. IntRaBench, for the first time, enables a fair comparison of the most influential 2D and 3D interactive segmentation methods. By measuring the number of simulated interactions, a proxy for the ’Human Effort’, we test different prompting strategies that do not require a slice-wise interaction (P1). 2. We propose effective interaction strategies for refinement of predictions in a 3D volume, without requiring clinicians to interact with each individual slice (P2). 3. We provide a standardized evaluation protocol to generate prompts, select model outputs and compute the segmentation metrics on the entire image across ten datasets, covering various modalities and target structures, including small lesions (P3). Our benchmarking efforts include a performance comparison against leading interactive segmentation methods in the medical domain. 4. The extendable IntRaBench framework allows developers to a) easily evaluate a new method in a fair manner against established methods and b) easily develop and investigate new prompting strategies. Through open-sourcing IntRaBench, we invite researchers to integrate their methods into our framework, promoting continuous and equitable assessment that allows us to track the overall progress in the field of interactive 3D medical image segmentation reproducibly and transparently. Figure 2: IntRaBench overview. Although our evaluation is performed on entire 3D volumes, the benchmark accommodates both 3D and 2D interactive segmentation methods. While 3D model prompting is relatively straightforward, we introduce prompting and refinement strategies for 2D models that minimize the effort required from human interaction. The benchmark is designed to be extensible, and researchers are encouraged to propose and integrate additional methods seamlessly using our codebase particularly for areas marked by three dots."
https://arxiv.org/html/2411.07873v1,Diverse capability and scaling of diffusion and auto-regressive models when learning abstract rules,"Humans excel at discovering regular structures from limited samples and applying inferred rules to novel settings. We investigate whether modern generative AI systems can similarly learn underlying rules from finite samples and perform reasoning through conditional sampling. Inspired by Raven’s Progressive Matrices task, we designed GenRAVEN dataset, where each sample consists of three rows, and one of the 40 relational rules governing the object position, number, or attributes applies to all three rows. We trained generative models to learn the data distribution, where samples are encoded as 3×9×9 integer arrays to focus on rule learning. We compared two major families of generative models: diffusion models (EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their ability to generate structurally consistent samples and perform panel completion via unconditional and conditional sampling. We found that diffusion models excel at unconditional generation, producing novel and more consistent samples from scratch and memorize less, but perform less well in panel completion, even with advanced conditional sampling methods like Twisted Diffusion Sampler. Conversely, autoregressive models excel at completing missing panels in a rule-consistent manner but generate less consistent samples unconditionally. We observe diverse data scaling behaviors: for both model families, rule learning emerges at a certain dataset size – around thousands examples per rule. With more training data, diffusion models improve both their unconditional and conditional generation capabilities. However, for autoregressive models, while panel completion improves with more training data, unconditional generation consistency declines. Our findings highlight complementary capabilities and limitations of diffusion and autoregressive models in rule learning and reasoning tasks, suggesting avenues for further research into their mechanisms and potential for human-like reasoning.","1 Background Figure 1: Design of the study A. Example Raven’s progression matrix, and its encoding as a 3\times9\times9 integer array. The underlying rule is constant shape. B.C. Two families of generative models: Diffusion and autoregressive model, and their training method: denoising and predicting the next token. D. The 40 relational rules, with 5 rules held out during training. Human excels at discovering regular structure from a small number of samples, and they can further apply such rule to novel settings to generate new samples or complete missing parts based on the same rule. The Raven’s progressive matrix (RPM) [27] is a famous task in human reasoning literature. In the generative version of this task (GenRAVEN), the subject observes two complete rows of panels and is tasked to complete the third row in a manner that is consistent with the first two rows (Fig.1A). Ideally, the subjects need to infer the underlying rule consistent with the first two rows and apply it correctly to the third row. How can we train a general learning system to solve such reasoning task? If we conceptualize all rule-conforming samples as a joint distribution, then rule learning can be framed as a generative modeling problem or learning the correct joint. Further, reasoning about the missing panel can be framed as sampling from the conditional probability [25]. One conceptual problem is, that given finite training samples, the rule governing them, or the ‘true‘ joint distribution is under-specified. The rules or the distribution learned by the system should be affected by its inductive bias, be it human or AI. Given this ambiguity, we asked whether modern generative AI systems could learn the correct ""joint distribution"" given finite samples. If so, can they reason and fill in the missing parts in a sample through conditional sampling? In the current age of Generative AI, there are two prominent families of generative models: autoregressive models and diffusion models. The autoregressive model generally dominates discrete sequence data, such as language, music, genomics [4, 14, 13], while the diffusion model excels at continuous data, such as image, video, audio, molecule structure, robot trajectories [28, 34, 29, 5, 3]. Both of them are capable of unconditional generation and conditional sampling based on partial observation: e.g. prompting for autoregressive model or inpainting for diffusion models. Given their similar capability, it’s interesting to compare them back-to-back on the same reasoning task and see how their different modeling method might lead to different learning and scaling behaviors. In this work, we trained a diverse set of generative models from both the diffusion family (EDM, DiT, SiT) and autoregressive family (GPT, Mamba) to learn the data distribution and then use conditional sampling techniques to perform inference, i.e., reasoning about the missing panel. We studied their performance as a function of data scale and model scale. We found that generally, diffusion models excel at unconditional sampling from the joint, creating structurally consistent samples from scratch, but perform less well for sampling conditioned on given panels. Conversely, autoregressive models excel at completing missing panels in a rule-consistent manner, but they perform less well in generating consistent samples from scratch with unconditional sampling. They also exhibit diverse scaling behavior when the size of datasets is varied. Our results call for further investigation into the seemingly complementary capabilities and limitations of diffusion and autoregressive models."
https://arxiv.org/html/2411.07870v2,Trustful LLMs: Customizing and Grounding Text Generation with Knowledge Bases and Dual Decoders,"Although people are impressed by the content generation skills of large language models, the use of LLMs, such as ChatGPT, is limited by the domain grounding of the content. The correctness and groundedness of the generated content need to be based on a verified context, such as results from Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to a customized domain is that the generated responses are often incomplete, or the additions are not verified and may even be hallucinated. Prior studies on hallucination detection have focused on evaluation metrics, which are not easily adaptable to dynamic domains and can be vulnerable to attacks like jail-breaking. In this work, we propose 1) a post-processing algorithm that leverages knowledge triplets in RAG context to correct hallucinations and 2) a dual-decoder model that fuses RAG context to guide the generation process.","Adapting an LLM to a specific domain is challenging for several reasons: 1) Pre-trained LLMs cover general knowledge and cannot access private data (even during fine-tuning) due to privacy, copyright, and policy constraints. 2) The grounding of generated texts can change depending on specific contexts, such as domain or timestamp. Recent studies mostly focus on detecting hallucinations and using multiple LLMs when hallucinations occur. 3) Business logic and structured data, such as databases and private knowledge bases, are required when integrating customized LLMs into production systems and presenting them to customers or users. We offer two methods for correcting hallucinations (beyond merely detecting them Wan et al. (2024); Li et al. (2023a); Ji et al. (2023)): 1) Applying post-processing to generated texts using knowledge triplets, and 2) Proposing guided generation via Dual Decoders. Inspired by common practices like Retrieval-Augmented Generation (RAG) Li et al. (2024), which retrieves relevant grounding context and feeds it to an LLM for text generation, we address hallucinations in generated texts from two aspects: 1) Post-editing based on knowledge graphs extracted from the context, and 2) Infusing guided context that contains important knowledge triplets into a generic LLM. Our proposed methods also provide reasoning and create consistent results from generative LLMs, benefiting from both the generation and extraction capabilities of decoder-only LLMs and the groundedness of RAG via the second decoder on the guidance Le et al. (2020); Wang et al. (2022b). In this work, we elaborate on our real-world commercial application scenario of using LLMs to support customers with Microsoft product inquiries in copilots, where groundedness is key to success. Pre-trained LLMs often lack the relevant knowledge or cannot adapt promptly to changes in the product database updates. Different variants of large language models (LLMs), such as Phi-3.5 Abdin et al. (2024), ChatGPT Mohamadi et al. (2023), LLama-3 Dubey et al. (2024), and Gemma Team (2024), are proficient at producing fluent outputs for diverse user queries. Despite their human-like fluency in generating text across a wide range of prompts, large language models suffer from hallucinations (see examples in Figures 2, 3, 4), where parts or the entirety of the generated text lack faithfulness, factuality, or reasoning, yet are presented with a confident tone Ji et al., 2023. To mitigate and correct hallucinations, we leverage guided text generation. Grounding guidance Socher et al. (2013); Nickel et al. (2011); Lin et al. (2015); Wang et al. (2014); Bordes et al. (2013); Wang et al. (2022a); Grover and Leskovec (2016), such as knowledge graphs (KGs), has been shown to significantly improve the reliability and factuality of LLMs in recent studies, e.g., KELM Agarwal et al. (2020); Lu et al. (2021), SKILL Moiseev et al. (2022), K-DLM Zou et al. (2023), KEPLET Li et al. (2023b), and LUKE-Graph Foolad and Kiani (2023). Knowledge graphs typically consist of factual information represented explicitly in a semi-structured format, generally as [subject entity, relation, object entity] triples, e.g., (Bill Gates, was, the CEO of Microsoft) Han et al. (2019); Gardner et al. (2017). We collect and maintain such knowledge triplets and grounded context offline for RAG. Our contributions are as follows. 1) We correct hallucinations and out-of-domain outputs in generated texts from LLMs by leveraging a graph algorithm and provide reasoning using knowledge triplets extracted from both the guided context and the generated texts. 2) We propose a dual-decoder model that fuses guided context with natural language generation models, in which the decoders share the weights of a pre-trained LLM. 3) The proposed algorithm and model reduce the constraints on the maximum output length, in addition to correcting hallucinations, by returning or generating only outputs related to the prompt and the guided context."
https://arxiv.org/html/2411.07854v1,: Advancing Neural Text Generation for Portuguese,"Significant advances have been made in natural language processing in recent years. However, our current deep learning approach to language modeling requires substantial resources in terms of data and computation. One of the side effects of this data-hungry paradigm is the current schism between languages, separating those considered high-resource, where most of the development happens and resources are available, and the low-resource ones, which struggle to attain the same level of performance and autonomy. This study aims to introduce a new set of resources to stimulate the future development of neural text generation in Portuguese. In this work, we document the development of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting to 200 billion tokens. Via this corpus, we trained a series of decoder-transformers named Tucano. Our models perform equal or superior to other Portuguese and multilingual language models of similar size in several Portuguese benchmarks. The evaluation of our models also reveals that model performance on many currently available benchmarks used by the Portuguese NLP community has little to no correlation with the scaling of token ingestion during training, highlighting the limitations of such evaluations when it comes to the assessment of Portuguese generative language models. All derivatives of our study are openly released on GitHub111 github.com/Nkluge-correa/Tucano and Hugging Face222 huggingface.co/TucanoBR.","Over almost a decade, the deep learning paradigm has been the de facto mode of operation for many of the sub-fields involved in artificial intelligence research lecun2015deep ; goodfellow2016deep . Natural Language Processing (NLP) is a canonical depiction of the success story of deep learning nadkarni2011natural ; deng2018deep ; otter2020survey , where neural network approaches to machine learning have become the engine that powers many aspects of our current age of intelligent automation, with breakthroughs like word embeddings mikolov2013linguistic ; mikolov2013efficient and the transformer neural architecture vaswani2017attention being at the heart of this revolution. Another aspect of this developmental movement is using the self-supervised learning approach as an intermediate step to many language modeling tasks geiping2023cookbook . In essence, self-supervised learning is a training methodology for machine learning systems, where we leverage the vastness of available unlabeled data at our disposition to create pretraining tasks where labeling can happen on the fly. This results in systems with useful and downstream-applicable representations tied to the domain they were trained on hastie2009overview ; misra2020self ; geiping2023cookbook . This training approach has been responsible for some of the early breakthroughs of the field bengio2000neural ; mikolov2013efficient ; sutskever2014sequence ; bahdanau2014neural , which have now morphed into our standard training recipe for foundation models bommasani2021opportunities . Nonetheless, while the statement ""leverage the vastness of available unlabeled data at our disposition to create pretraining tasks"" can be true for languages like English or Chinese, where datasets can reach the 10^{13} tokens mark dolma ; young2024yi ; chen2023chinesewebtext ; penedo2024finewebdatasetsdecantingweb , and models are trained way past what scaling laws prescribe as compute-optimal zhang2024tinyllama ; parmar2024nemotron ; xue2024openmoe , the same cannot be said about the crushing majority of more than 7000 languages spoken around the world today rodrigues2023advancing ; lopes2024gl ; cohere2024gap . Hence, the prospect of training language models at the scale required to match what is done in such high-resource languages (even when compared to the state-of-the-art from 5 years ago)333Models like GPT-3 (2020) brown2020language were trained only on 300B tokens, which is still something not reproduced in a monolingual setting for most low-resource languages. is a far-fetched goal for most low-resource languages gandhe2014neural ; adams2017cross ; cruz2019evaluating ; muennighoff2023scaling ; correa2024teenytinyllama . To overcome this linguistic shortcoming, one of the approaches found in the literature is the development of multilingual models and datasets singh2024ayadatasetopenaccesscollection ; ustun2024ayamodelinstructionfinetuned ; aryabumi2024aya ; srivastava2024lolaopensourcemassively . In these models, the self-supervised pretraining stage is conducted with various languages. Models like mBERT devlin2018bert , mT5 xue2020mt5 , XLM-RoBERTa conneau2020unsupervised , mGPT shliazhko2022mgpt , XGLM lin2021few , BLOOM workshop2022bloom , PolyLM wei2023polylm , Aya ustun2024ayamodelinstructionfinetuned , and Llama 3 dubey2024llama are examples of this approach. On the other hand, the development of monolingual language models has also been explored and, at many times, shown to be a more successful approach to the multilingual one, like in the case of Finish virtanen2019multilingual , French martin-etal-2020-camembert , Catalan armengol2021multilingual , Chinese sun2021ernie , and Portuguese souza2020bertimbau ; rodrigues2023advancing ; correa2024teenytinyllama . Besides, as already pointed out by other works correa2024teenytinyllama , if based on raw pretraining instead of a fine-tuning approach, the monolingual approach can help developers escape the computational (i.e., models that are too expensive to run) and legal constraints (i.e., models that are restricted in terms of their licensing) of working with an already established foundation. However, advances in developing low-resource monolingual language models, such as those for Portuguese, remain limited, small in scale, undocumented, lacking standardization, and often reliant on repurposing models trained behind closed doors,444This is particularly true for the European and Brazilian variants, with other variants (e.g., Angolan Portuguese) even less represented or entirely absent. as will be discussed in the next section. These deficits also make it challenging to compare language models and evaluation benchmarks. At the same time, the effectiveness of the currently available benchmarks for Portuguese is also untested. In this work, we aim to address these challenges and build on existing studies to improve the status of generative language modeling research and development for Portuguese. In summary, our study offers the following advancements to the Portuguese NLP community: 1. The concatenation of a larger and more high-quality dataset for Portuguese language modeling (GigaVerbo). 2. The development of learned filters and datasets to improve text pre-processing for Portuguese. 3. Pushing self-supervised pretraining beyond the 500B tokens mark for Portuguese monolingual models. 4. The development of new, low-resource, efficient, and effective open-source foundation models for Portuguese (Tucano). 5. A critical assessment and comparison of currently available benchmarks for Portuguese language models. In Section 2, we review the current status of Portuguese Large Language Model (LLM) research and development, documenting the trends and deficits in the field. Section 3 describes the pretraining corpus used in this work. Section 4 and 5 contain the definition of our chosen tokenizer and the parameter space of different models we trained. In Section 6, we discuss the training and evaluation of our models. We also employ a simple alignment strategy to our more capable models, as will be discussed in Section 7. In Section 8, we present the results of our evaluation harness. Finally, Sections 9 and 10 provide an outlook for future studies and conclusion of our work."
https://arxiv.org/html/2411.07850v1,IAE: Irony-based Adversarial Examples for Sentiment Analysis Systems,"Adversarial examples, which are inputs deliberately perturbed with imperceptible changes to induce model errors, have raised serious concerns for the reliability and security of deep neural networks (DNNs). While adversarial attacks have been extensively studied in continuous data domains such as images, the discrete nature of text presents unique challenges. In this paper, we propose Irony-based Adversarial Examples (IAE), a method that transforms straightforward sentences into ironic ones to create adversarial text. This approach exploits the rhetorical device of irony, where the intended meaning is opposite to the literal interpretation, requiring a deeper understanding of context to detect. The IAE method is particularly challenging due to the need to accurately locate evaluation words, substitute them with appropriate collocations, and expand the text with suitable ironic elements while maintaining semantic coherence. Our research makes the following key contributions: (1) We introduce IAE, a strategy for generating textual adversarial examples using irony. This method does not rely on pre-existing irony corpora, making it a versatile tool for creating adversarial text in various NLP tasks. (2) We demonstrate that the performance of several state-of-the-art deep learning models on sentiment analysis tasks significantly deteriorates when subjected to IAE attacks. This finding underscores the susceptibility of current NLP systems to adversarial manipulation through irony. (3) We compare the impact of IAE on human judgment versus NLP systems, revealing that humans are less susceptible to the effects of irony in text.","Adversarial examples [1], crafted by adding imperceptible tiny perturbations to origin inputs maliciously, cause deep neural networks (DNNs) to fail blatantly. The secure issue, namely adversarial attack, is being widely concerned among researchers as soon as it was proposed. Extensive research has revealed that adversarial examples widely exist in many fields, e.g., computer vision (CV) [2], natural language processing (NLP) [3] and automatic speech recognition (ASR) [4]. Textual data is not as continuous as images which are capable of being perturbed imperceptibly with pixel noise. Instead, it is impossible to craft a factual imperceptible perturbation on a text due to its discrete nature. Furthermore, the grammar and semantics may be broken easily by changing even a character. The textual adversarial attack is confronted with greater challenges compared with images. A variety of textual adversarial attack models has been proposed in many NLP tasks, incorporating machine translation [5], question-answering system [3], sentiment analysis [6], et al. Spelling mistake [7], visually similar characters substitution [8], synonyms substitution [9] and sentence paraphrasing [10] are typical textual adversarial attack methods ranging from word-level to sentence-level while categorized by attacking granularity. However, there are still a few issues while assuming those methods in practical situations: 1) Subtle spelling mistakes can be recovered easily with spelling error correction [11]. 2) Words out of vocabulary may arise attention and alertness while exceeding averages in a text. 3) Word substitution and sentence paraphrasing may cause grammar to be broken or semantics deviated. Therefore, we consider a textual adversarial attacking method more practically. TABLE I: Examples of straightforward and ironic text. Straightforward 1: {CJK}UTF8gbsn他真是个糟糕的守门员，让对方进了六个球。 He is a really terrible goalkeeper, allowing the other side to score six goals. \hdashline[3pt/3pt] Ironic 1: {CJK}UTF8gbsn他真是个有天赋的守门员，让对方进了六个球。 He is a really talented goalkeeper, allowing the other side to score six goals. Straightforward 2: {CJK}UTF8gbsn那个男人真恶心，在公共场所随地吐痰。 That man is totally disgusting, spiting everywhere in public. \hdashline[3pt/3pt] Ironic 2(a): {CJK}UTF8gbsn那个男人真美味，在共场所随地吐痰。 That man is totally delicious, spiting everywhere in public. \hdashline[3pt/3pt] Ironic 2(b): {CJK}UTF8gbsn那个男人真优雅，在共场所随地吐痰。 That man is totally elegant, spiting everywhere in public. \hdashline[3pt/3pt] Ironic 2(c): {CJK}UTF8gbsn那个男人真优雅，在共场所随地吐痰。真是值得称赞啊。 That man is totally elegant, spiting everywhere in public. It is really praiseworthy. The irony is a kind of rhetorical device expressing a strong emotion referring to the opposite of literal meaning and needs to understand the actual meaning from context. Detecting irony is challenging while implementing it the model needs to have human-level language understanding ability. As far as we know, there are no studies considering converting text from straightforward to ironic as a method of generating textual adversarial examples orienting the NLP task of sentiment analysis presently. The cruxes of converting a text from straightforward into ironic are to turn the polarity of the evaluation words and make an ironic expansion appropriately when necessary. Specifically, there are at least three challenges here: 1) locating evaluation words, 2) substituting evaluation words with correct collocation, and 3) expanding text with appropriate ironic evaluation. Without loss of generality, we consider Chinese irony-based adversarial examples in this paper. As shown in Table I, Chinese words {CJK}UTF8gbsn“糟糕” is an evaluation to {CJK}UTF8gbsn“守门员” in first sentence, where {CJK}UTF8gbsn“糟糕” means “terrible” and {CJK}UTF8gbsn“守门员” means “goalkeeper”. It is necessary to locate the words {CJK}UTF8gbsn“糟糕” as an evaluation disclosing negative emotion and then substitute {CJK}UTF8gbsn“糟糕” with {CJK}UTF8gbsn“有天赋” which means “talented”. Humans are in capable of understanding the second sentence still exhibiting negative emotion with strong language comprehending ability, although the evaluation words {CJK}UTF8gbsn“有天赋” is an absolutely positive evaluation literally. Besides, it ought to be notice the substitution needs to consider collocation relation instead of substituting with antonym simply. For example, {CJK}UTF8gbsn“美味” is one of antonyms for {CJK}UTF8gbsn“恶心”, where {CJK}UTF8gbsn“美味” means “delicious” and {CJK}UTF8gbsn“恶心” means “disgusting”, but {CJK}UTF8gbsn“美味” is not supposed to collocate with {CJK}UTF8gbsn“男人”, which means “man”, referring to the context in fourth sentence, and it is supposed to be substituted with {CJK}UTF8gbsn“优雅” instead, which means “elegant”, as shown in fifth sentence. Furthermore, the whole sentence needs to be semantically smooth while to expand it with an ironic evaluation when necessary, as shown in sixth sentence. In this paper, we present a textual adversarial attacking method orienting the NLP task of sentiment analysis by rewriting a straightforward sentence into an ironic sentence, namely IAE (Irony-based Adversarial Examples). To the best of our knowledge, we are the first to use irony for textual adversarial examples generation. We summarize our major contributions as follows: • We propose IAE, a strategy based on the concept of a rhetorical device called irony for generating textual adversarial examples, which does not need to prepare irony corpus. • We show that the performance of various deep learning models substantially drops for sentiment analysis tasks when attacked by IAE. • We show that humans are only mildly or not at all affected by irony in contrast to NLP systems. II Literature review Our work connects to two strands of literature: textual adversarial examples and irony generation. Textual Adversarial Examples Existing textual adversarial attack models can be categorized into character-level, word-level, and sentence-level according to the perturbation levels of their adversarial examples. Character-level attacks disrupt the process of converting natural language text into numerical representations that computers can process, thereby causing model decision shifts. The manifestation of character-level attacks varies across different linguistic environments. In English, character-level attacks often exploit visual perturbations, such as inserting[12], deleting, swapping, and modifying[8] letters within words to create artificially constructed spelling errors. In the Chinese context, handwriting errors on paper do not occur in electronic input based on input methods. Therefore, character-level attacks in the Chinese environment often manifest as the use of homophones for substitution[13, 14] or visual decomposition of characters[15]. Word-level adversarial attacks achieve a shift in the semantic vector of the sample by perturbing the input sample at the word level, causing it to cross the decision boundary and thus leading to incorrect model outputs. Word substitution, as the core method of this strategy, includes various word replacement means such as word vector similarity[16], synonyms[17], and language model scoring[18]. Word-level adversarial attacks do not break the grammatical rules of the text and retain the original semantics to the greatest extent, thus performing better in terms of adversarial text quality and attack success rate. Coupled with the use of language models for control, it also ensures the fluency and smoothness of adversarial texts. Among them, text attacks based on synonym substitution have strong semantic retention and grammatical coherence, belonging to the most threatening category of text adversarial attacks, which have attracted widespread attention from researchers. Sentence-level adversarial attacks treat the entire original input sentence as the object of perturbation, carefully reconstructing the text content, that is, generating adversarial text that has the same semantics as the original input but causes the victim model to make decision errors. Common sentence-level adversarial attack methods include encoding and then re-decoding[19], adding irrelevant sentences[20], paraphrasing[21], etc. Irony Generation The field of irony generation, particularly within the Chinese linguistic context, remains largely unexplored, with limited research and development dedicated to this area. Zhu et al. [22] proposed a novel method that integrates reinforcement learning with style transfer techniques to generate ironic text. Their approach relies on a carefully designed reward system to guide the model towards producing text that effectively conveys irony. This method demonstrates the potential of combining advanced machine learning techniques with stylistic adjustments to achieve the nuanced expression of irony. Veale et al. [23] took a different route by exploring knowledge-based systems and shallow linguistic techniques, which they term ”mere re-generation,” for irony generation. This approach leverages existing knowledge structures and simple linguistic manipulations to introduce ironic elements into the text. While this method may not delve deeply into the complexities of language, it offers a more straightforward and potentially more accessible avenue for irony generation. In the closely related domain of sarcasm, Mishra et al. [24] presented a framework that utilizes reinforced neural sequence-to-sequence learning coupled with information retrieval strategies for sarcasm generation. To the best of our knowledge, our work represents the first instance of leveraging irony for the generation of textual adversarial examples. This application of irony in adversarial machine learning is groundbreaking, as it introduces a new dimension to the field of natural language processing security. It serves as a testament to the importance of understanding and incorporating advanced linguistic features, such as irony, into machine learning models to enhance their resilience against adversarial attacks. III Problem statement We assume access to a corpus of labeled sentences D=\{(s_{1},p_{1}),...,(s_{n},p_{n})\}, where s_{i} is a sentence and p_{i}\in L, the set of possible emotional polarity, i.e., L = {positive, negative}. We define s^{p}=(c,e,d), a sentence with emotional polarity p, where c is the central word of the sentence, e is the evaluation word that evaluating the central word c, and d is the detailed description of the evaluation. On this basis, we define emotional sentence s^{p} as a straightforward sentence or an ironic sentence while the evaluation e have emotional polarity p^{\prime}, while collocating with c, and p=p^{\prime}, or p\neq p^{\prime}. Generally, the irony is a negative sentence exhibiting positive evaluation. Thus, our goal is to build a model that takes as input sentence s, a negative emotional sentence exhibiting negative evaluation e^{\textrm{neg}}, and outputs a sentence s^{\prime} that retains the negative emotional polarity while exhibiting positive evaluation e^{\textrm{pos}}. Note that the concept of evaluation word we use is not equivalent to the sentiment word while sentiment word is an adjective with a clear emotional polarity. The emotional polarity of an evaluation word should be determined by the central word with which the evaluation word collocates. IV Approach Figure 1: An overview of our proposed IAE generator. In this section, we detail our irony-based textual adversarial attacking method, incorporating three parts: 1) an extractor of collocations between nouns and adjectives, 2) a strategy for evaluation word substitution, and 3) a strategy for ironic evaluation sentence generation. An overview of our IAE generator is shown in Fig. 1. Generally, it takes straightforward text as inputs and outputs ironic text. First, the central word and relevant evaluation word will be located, and then the evaluation word will be substituted with an opposite evaluation word among all possible alternatives, Finally, an appropriate ironic evaluation sentence, determined by local model, will be appended to the text for strengthening the effect of irony. Next, we describe the details of each component of IAE generator. IV-A Collocation extractor We design a collocations extractor to establish noun-adjective collocations tables, which also reveals probable emotional polarity between a noun with all collocated adjectives, as a library of alternatives for evaluation word substitution (see section IV-B). A host of observations were made on Chinese corpus with part-of-speech tagging and dependency parsing, and we found the noun-adjective collocations in a Chinese sentence are supposed to form the following two kinds of dependencies: 1) a subject-verb structure, or 2) an attributive structure (see examples in Table II). Note that the results of dependency parsing in Chinese may be different from English due to the differences in the two kinds of syntax rules. e.g., the words “weather” and “good” are supposed to form a subject-predicative in English instead of a subject-verb. Then we can extract plenty of collocations from a large corpus through the observations above, but the next key question is how to determine the emotional polarity of each noun-adjective collocation. Although we can use advanced sentiment analysis models to determine the overall emotional polarity of the sentence from which a noun-adjective collocation extracted, it is no guarantee the emotional polarity of a noun-adjective collocation will be consistent with the whole sentence. But, intuitively, the emotional polarity of a collocation should probably be positive if it mostly appears in sentences with a positive overall emotional polarity rather than negative. Hence, the polarity of collocation can be inferred by the following formulas: x=\frac{\textrm{Freq}_{\textrm{pos}}}{\textrm{Freq}_{\textrm{neg}}} (1) F(x)=\left\{\begin{array}[]{l}\textrm{positive}\ \ \ ,x>1\\ \textrm{negative}\ \ ,x<1\\ \textrm{manually},x=1\\ \end{array} (2) Where \textrm{Freq}_{\textrm{pos}} and \textrm{Freq}_{\textrm{neg}} are the frequencies of a collocation appearing in sentences with an emotional polarity of positive or negative respectively. The emotional polarity of a collocation is supposed to be positive when x>1, or negative when x<1, or decided manually when the result of x happens to be 1. Therefore, the noun-adjective collocations table, denoted as T, can be established by collecting collocations by dependency parsing and inferring their emotional polarities by counting and comparing the numbers of each emotional polarity of the sentences in which they occur. IV-B Evaluation word substitution The strategy for evaluation word substitution is the most important procedure to convert a straightforward sentence s to an ironic sentence s^{\prime} while s^{\prime} has the evaluation word e with emotional polarity p^{\prime}, which is opposite to the emotional polarity p of the whole sentence. Next, we describe our evaluation word substitution step by step. Locating. At the very beginning, the pairs of central word and relevant evaluation word are located by using part-of-speech tagging and dependency parsing together, which is similar to the strategy of extracting noun-adjective collocation (see secttion IV-A). Retrieving. The alternatives are retrieved among the table T by using central word c as an index. The whole procedure will terminate and return a general evaluation word (e.g., {CJK}UTF8gbsn“不错”, which is analog to “fine” in English) as the result while the central word does not exist or none of the positive evaluation words are retrieved. Determining. To determine what alternative evaluation word to substitute original, our strategy is to evaluate the quality (i.e., probability of sentence) of all alternative sentences S^{\prime} by N-gram language model while combining any possible collocation of central word and alternative evaluation word. Formally, for any s^{\prime}\in S^{\prime}, the probability is calculated by the following formula: P(s)=\prod\frac{\textrm{count}(w_{i-1}w_{i},D)+\delta}{\textrm{count}(w_{i},D)% +\delta} (3) where w_{i} is the i-th word in s^{\prime}, w_{i-1}w_{i} is the sequence composed of w_{i-1} and w_{i} sequentially, \textrm{count}(.,D) denotes the numbers of times a word or a sequence appears in D, and \delta is an additive smoothing parameter for the situation that some words are just not appearing in D. In practice, the smoothing parameter \delta can be set to 1 empirically. The alternative sentence s^{\prime} with the highest probability among S^{\prime} will be determined as the result of evaluation word substitution. TABLE II: Examples of sentences containing noun-adjective collocations and dependencies. Sentences Collocations Dependencies {CJK}UTF8gbsn天气这么好，应该出去透透空气。 The weather is so good for enjoying fresh air. {CJK}UTF8gbsn天气，好 weather, good subject-verb {CJK}UTF8gbsn那个男人真帅。 That man is so handsome. {CJK}UTF8gbsn男人，帅 man, handsome subject-verb {CJK}UTF8gbsn优美的音乐可以给人们带来享受。 Beautiful music can bring people enjoyment. {CJK}UTF8gbsn音乐，优美 music, beautiful attributive {CJK}UTF8gbsn她招待我们吃了一顿可口的午餐。 She served us a delicious lunch. {CJK}UTF8gbsn午餐，可口 lunch, delicious attributive IV-C Ironic evaluation appending Reversing the result of sentiment analysis by substituting the evaluation alone is often difficult while the context still exhibits original emotional polarity. But this problem can be solved by appending an evaluation, which is opposite to the polarity of real emotion for strengthening the ironic effect. It is easy to construct positive evaluations by composing positive adjectives and other grammatical constituents according to sentence patterns. However, the problems are how to choose an evaluation and how to guarantee the semantic smoothness of the whole sentence after evaluation appending. Our strategy is to construct general positive evaluations, which can collocate with almost objects and guarantee the semantic smoothness, as much as possible, and then to determine an evaluation appending to s^{\prime}. Inspired by the substitute black box attack (SBA) [25] which is utilizing the transferability of adversarial examples, we consider training the local model to substitute the victim model, then testing each alternative on the local model, and finally selecting the evaluation while the local model outputs a wrong prediction after appending. For the case that there is no effective adversarial example on the local model, we consider choosing the longest one. After determining the ironic evaluation, which is supposed to append to the sentence s^{\prime}, the final IAE is generated completely. V Experiments and Results In this section, we conduct comprehensive experiments to evaluate our IAE on the tasks of sentiment analysis. V-A Datasets and Victim Models We evaluate our IAE on the public reviews of Meituan111Meituan is a platform for ordering takeaway, which contains positive and negative user reviews and Amazon. The five-star and one-star reviews are taken as positive and negative text respectively. Because our IAE is only applicable to the examples with negative emotional polarity, we randomly select 500 examples with negative emotional polarity from each dataset as the test set, and then we divide the remaining examples into two balanced parts for training local and victim models. Details of the datasets are shown in Table III, where “Class #” refers to the number of labels, “Max. #W” means maximum length of sentences (number of words), “Min. #W” means minimum length of sentences (number of words), and “Avg. #W” means average length of sentences (number of words), “P. #” and “N. #” signify the number of text exhibiting positive and negative emotional polarity respectively. Besides, for comprehensive noun-adjective collocations extracting (see section IV-A), we collected 30111 nouns and 114383 related collocations from serveral Chinese corpus, including reviews on Meituan and Amazon, Sina weibo222a twitter like Chinese online platform comments, and online News corpus. For each noun, there are 1115 collocations at most and 1 collocation at least, with an average of 3.7. We choose three popular models for text classification, namely TextCNN [26], Bidirectional LSTM (BiLSTM) [27] and a fine-tuned BERT [28], used for evaluating our IAE. TextCNN has three convolutional filters of different kernel sizes (3, 4, 5), and their outputs are concatenated, pooled and fed to a fully-connected layer followed by an output layer. BiLSTM is composed of a 128-dimenional bidirectional LSTM layer, a dropout layer using a drop rate of 0.5, and an output layer. BERT is obtained by fine-tuning the Chinese BERT-Base model with 12-layer, 768-hidden, and 12-heads released by Google. The optimizer, learning rate, and loss function of all models are set to adam, 0.01, and cross-entropy respectively. Besides, we implement Chinese word segmentation, part of speech tagging, and dependency parsing using the third-party library released by Harbin Institute of technology [29]. V-B Baseline methods We implement two baseline methods based on important word substitution and compared them with ours for proving the contribution of this work. The two baseline methods are 1) visual-based substitution [8], which means substitute important words with visual similar chart, and 2) homonym-based substitution [13], which means substitute important words with others pronounced the same way but have different meanings. The important words refer to the words in the input text that make the most contribution to the model decision and the calculation algorithm of important words adopts [30]. TABLE III: Statistics for the datasets. Dataset Class # Max. #W Min. #W Avg. #W P. # N. # Meituan 2 237 2 18.96 6000 6500 Amazon 2 858 2 23.71 6000 6500 TABLE IV: Performance of victim models under attacking of IAE and two baseline methods on Meituan review dataset. Dataset Method Local Model Victim model WMD TextCNN BidLSTM Bert Meituan Origin N/A 0.885 0.894 0.934 N/A Visual-based TextCNN 0.312 0.702 0.880 1.497 BidLSTM 0.260 0.670 0.892 1.787 Bert 0.610 0.818 0.860 0.384 Homonym-based TextCNN 0.326 0.694 0.854 1.551 BidLSTM 0.304 0.698 0.856 1.784 Bert 0.606 0.832 0.846 0.411 Ours TextCNN 0.464 0.204 0.456 1.197 BidLSTM 0.324 0.416 0.542 1.041 Bert 0.700 0.876 0.370 0.808 Amazon Origin N/A 0.900 0.936 0.944 N/A Visual-based TextCNN 0.564 0.664 0.870 2.570 BidLSTM 0.346 0.400 0.914 2.643 Bert 0.860 0.830 0.836 2.480 Homonym-based TextCNN 0.648 0.702 0.844 2.559 BidLSTM 0.544 0.550 0.870 2.605 Bert 0.842 0.826 0.834 2.472 Ours TextCNN 0.338 0.322 0.602 2.344 BidLSTM 0.720 0.728 0.538 2.341 Bert 0.880 0.886 0.670 2.379 TABLE V: Human evaluation of emotional correctness and grammar smoothness. Dataset Emotional Correctness Grammar Smoothness Origin IAE Origin IAE Meituan 91 86 4.25 3.50 Amazon 90 84 4.50 3.75 V-C Attack performance The attack performance results of our IAE and two baseline methods are shown in Table IV. Note that only examples labeled with negative are used for test as the adversarial attack based on irony is only applicable to the examples with negative emotional polarity. We observe the adversarial examples generated by our irony-based attack cause the victim models to fail more seriously than the baseline methods in most conditions in most conditions. Specifically, the visual-based and homonym-based attack can hardly fool Bert models while our method can cause the accuracy of Bert from 89.8% to 37.0% at most, besides, the Word Mover’s Distances [31] between our IAE and clean examples are always smaller than those between adversarial examples generated by baseline methods and clean examples. V-D Human evaluation We ask 4 students with native Chinese language skill to evaluate the emotional correctness and semantic smoothness of successful IAE generated from Meituan and Amazon reviews. Specifically, we randomly select 100 IAE and 100 clean examples, and every student needs to evaluate the mixture of them. For evaluating emotional correctness, each student evaluates the true emotional polarity of each example and it is annotated as positive (negative) if two or more students evaluate a example as positive (negative). An extra human evaluator would participate in the evaluation if there are equal numbers of different evaluation on emotional polarity. For evaluating semantic smoothness, each student scores the semantic smoothness of each example with Likert scale ranging from 1 to 5 while 1 and 5 mean the semantics of a example is completely confused or fluent separately. We summarized the evaluation of all students and averaged the semantic smoothness of the IAE and the clean examples respectively. The results are shown in Table V and it shows that a lightly lower emotional correctness and semantic smoothness in IAE than clean examples, but the emotional correctness and semantic smoothness of IAE still reach 86 and 3.75 respectively. VI Discussion We studied how to regard irony as a textual adversarial perturbation in Chinese and it proved effective in sentiment analysis. There are differences between Chinese and other languages in grammar and habits, however, irony, as a rhetorical device in almost all languages, could be utilized as a general way of textual adversarial perturbation. The experiment of training the local model for generating effective adversarial examples also reveals some properties of transferability. First, the transfers between two models are non-symmetric. As we can see, the accuracy of victim model BERT is 54.2\% when generated IAE from local model BidLSTM, however, the accuracy of victim model BidLSTM is 87.6\% when generated IAE from local model BERT while testing on Meituan reviews dataset. It is similar to the findings in the study of the transferability of image adversarial examples [32], even though we focus on the text field. Second, the adversarial examples generated from the high-accuracy models may be less transferable. As we can see, BERT is the most accurate model among all models we use, however, the adversarial examples generated from BERT hardly mislead other models. We also found there are three major types of weaknesses in our methods, which affect the attacking performances. For analysis of the weaknesses, we sampled 100 failed IAE which mislead the victim model unsuccessfully or lose original sentiment. We found that 26% of the failures are due to the long length of input text which is more than 50 Chinese characters, 38% of the failures are due to the weak correlation between evaluative sentence and context, 29% of the failures are due to the imperfection of part-of-speech tagging and dependency parsing tools, and the remaining 7% of the failures have no significant type. The first type of failure is due to the obvious fact that the longer the text, the more negative content it contains, so it is difficult to change the label of model prediction by substituting an evaluation word or appending a generally positive evaluation sentence. The second type of failure is due to the weak correlation between the evaluative sentence and the context description. For example, for the sentence “{CJK}UTF8gbsn菜真的很难吃，还是去其他店吃好些” (The food is really unpalatable, and it’s better to go to another restaurant), where the context is not a correlational detail description to the evaluation of food, it is inappropriate to substitute the negative evaluation word “{CJK}UTF8gbsn难吃” (unpalatable) to a positive word “{CJK}UTF8gbsn好吃” (delicious) otherwise the emotional polarity of the text will change completely. The third type of failure is due to the dependency analysis tools, which is unable to analyze the dependency correctly all the time, while it is necessary to locate the evaluation word with part-of-speech tagging and dependency parsing. VII Conclusion and future work In this paper, we have introduced Irony-based Adversarial Examples (IAE), a novel method for generating adversarial text by transforming straightforward sentences into ironic ones. Our research has made several significant contributions to the field of adversarial attack. Firstly, we have introduced IAE as a strategy for generating textual adversarial examples that leverages irony. This method is innovative in that it does not depend on pre-existing irony corpora, thereby offering a flexible instrument for creating adversarial text across a spectrum of NLP tasks. Secondly, we have demonstrated empirically that the performance of several deep learning models on sentiment analysis tasks is markedly compromised when confronted with IAE attacks. This result highlights the vulnerability of current NLP systems to adversarial manipulations facilitated through irony. Thirdly, we have compared the effects of IAE on human judgment versus NLP systems, revealing a notable difference in susceptibility. Our findings indicate that humans are relatively more resilient to the influence of irony in text, contrasting with the performance of NLP models. Our future work will focus on enhancing the performance of IAE in longer texts and improving its generalization capabilities across different languages. This will involve addressing the complexities associated with maintaining ironic integrity over extended passages and adapting to the nuances of various linguistic contexts. Additionally, we are intrigued by the prospect of integrating more rhetorical devices into textual adversarial perturbations, beyond irony. For instance, exploring the use of metaphors to disrupt machine reading comprehension presents an exciting avenue for further research. By expanding the repertoire of rhetorical strategies employed in adversarial text generation, we aim to deepen our understanding of the interplay between language, context, and machine learning models, ultimately contributing to the development of more robust and nuanced NLP systems. References [1] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6572 [2] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep neural networks,” IEEE Trans. Evol. Comput., vol. 23, no. 5, pp. 828–841, 2019. [Online]. Available: https://doi.org/10.1109/TEVC.2019.2890858 [3] R. Jia and P. Liang, “Adversarial examples for evaluating reading comprehension systems,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, M. Palmer, R. Hwa, and S. Riedel, Eds. Association for Computational Linguistics, 2017, pp. 2021–2031. [Online]. Available: https://doi.org/10.18653/v1/d17-1215 [4] Q. Wang, B. Zheng, Q. Li, C. Shen, and Z. Ba, “Towards query-efficient adversarial attacks against automatic speech recognition systems,” IEEE Trans. Inf. Forensics Secur., vol. 16, pp. 896–908, 2021. [Online]. Available: https://doi.org/10.1109/TIFS.2020.3026543 [5] Y. Belinkov and Y. Bisk, “Synthetic and natural noise both break neural machine translation,” in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [Online]. Available: https://openreview.net/forum?id=BJ8vJebC- [6] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,” in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 8018–8025. [Online]. Available: https://aaai.org/ojs/index.php/AAAI/article/view/6311 [7] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, “Hotflip: White-box adversarial examples for text classification,” in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, I. Gurevych and Y. Miyao, Eds. Association for Computational Linguistics, 2018, pp. 31–36. [Online]. Available: https://www.aclweb.org/anthology/P18-2006/ [8] S. Eger, G. G. Sahin, A. Rücklé, J. Lee, C. Schulz, M. Mesgar, K. Swarnkar, E. Simpson, and I. Gurevych, “Text processing like humans do: Visually attacking and shielding NLP systems,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 1634–1647. [Online]. Available: https://doi.org/10.18653/v1/n19-1165 [9] S. Ren, Y. Deng, K. He, and W. Che, “Generating natural language adversarial examples through probability weighted word saliency,” in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. Màrquez, Eds. Association for Computational Linguistics, 2019, pp. 1085–1097. [Online]. Available: https://doi.org/10.18653/v1/p19-1103 [10] M. Iyyer, J. Wieting, K. Gimpel, and L. Zettlemoyer, “Adversarial example generation with syntactically controlled paraphrase networks,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 1875–1885. [Online]. Available: https://doi.org/10.18653/v1/n18-1170 [11] D. Pruthi, B. Dhingra, and Z. C. Lipton, “Combating adversarial misspellings with robust word recognition,” in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. Màrquez, Eds. Association for Computational Linguistics, 2019, pp. 5582–5591. [Online]. Available: https://doi.org/10.18653/v1/p19-1561 [12] B. Formento, C. S. Foo, L. A. Tuan, and S. K. Ng, “Using punctuation as an adversarial attack on deep learning-based NLP systems: An empirical study,” in Findings of the Association for Computational Linguistics: EACL 2023, A. Vlachos and I. Augenstein, Eds. Dubrovnik, Croatia: Association for Computational Linguistics, 2023, pp. 1–34. [13] W. Wang, R. Wang, L. Wang, and B. Tang, “Adversarial examples generation approach for tendency classification on chinese texts,” Ruan Jian Xue Bao/J. Softw., vol. 30, pp. 2415–2427, 2019. [Online]. Available: https://doi.org/10.13328/j.cnki.jos.005765 [14] N. Cheng, G. Chang, H. Gao, G. Pei, and Y. Zhang, “Wordchange: Adversarial examples generation approach for chinese text classification,” IEEE Access, vol. 8, pp. 79 561–79 572, 2020. [Online]. Available: https://doi.org/10.1109/ACCESS.2020.2988786 [15] H. Ou, L. Yu, S. Tian, and X. Chen, “Chinese adversarial examples generation approach with multi-strategy based on semantic,” Knowl. Inf. Syst., vol. 64, no. 4, pp. 1101–1119, 2022. [16] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,” in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 8018–8025. [17] S. Ren, Y. Deng, K. He, and W. Che, “Generating natural language adversarial examples through probability weighted word saliency,” in Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, 2019, pp. 1085–1097. [18] H. Zhang, H. Zhou, N. Miao, and L. Li, “Generating fluent adversarial examples for natural languages,” in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. Màrquez, Eds. Association for Computational Linguistics, 2019, pp. 5564–5569. [Online]. Available: https://doi.org/10.18653/v1/p19-1559 [19] W. Han, L. Zhang, Y. Jiang, and K. Tu, “Adversarial attack and defense of structured prediction models,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Association for Computational Linguistics, 2020, pp. 2327–2338. [20] B. Liang, H. Li, M. Su, P. Bian, X. Li, and W. Shi, “Deep text classification can be fooled,” in Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, J. Lang, Ed. Stockholm, Sweden: IJCAI, 2018, pp. 4208–4215. [Online]. Available: https://doi.org/10.24963/ijcai.2018/585 [21] Y. Xu, X. Zhong, A. Jimeno Yepes, and J. H. Lau, “Grey-box adversarial attack and defence for sentiment classification,” in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021. [22] M. Zhu, Z. Yu, and X. Wan, “A neural approach to irony generation,” CoRR, vol. abs/1909.06200, 2019. [Online]. Available: http://arxiv.org/abs/1909.06200 [23] T. Veale, “A massive sarcastic robot: What a great idea! two approaches to the computational generation of irony,” in Proceedings of the Ninth International Conference on Computational Creativity, ICCC 2018, Salamanca, Spain, June 25-29, 2018, F. Pachet, A. Jordanous, and C. León, Eds. Association for Computational Creativity (ACC), 2018, pp. 120–127. [24] A. Mishra, T. Tater, and K. Sankaranarayanan, “A modular architecture for unsupervised sarcasm generation,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds. Association for Computational Linguistics, 2019, pp. 6143–6153. [Online]. Available: https://doi.org/10.18653/v1/D19-1636 [25] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical black-box attacks against machine learning,” in Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates, April 2-6, 2017, R. Karri, O. Sinanoglu, A. Sadeghi, and X. Yi, Eds. ACM, 2017, pp. 506–519. [Online]. Available: https://doi.org/10.1145/3052973.3053009 [26] Y. Kim, “Convolutional neural networks for sentence classification,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, A. Moschitti, B. Pang, and W. Daelemans, Eds. ACL, 2014, pp. 1746–1751. [Online]. Available: https://doi.org/10.3115/v1/d14-1181 [27] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes, “Supervised learning of universal sentence representations from natural language inference data,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, M. Palmer, R. Hwa, and S. Riedel, Eds. Association for Computational Linguistics, 2017, pp. 670–680. [Online]. Available: https://doi.org/10.18653/v1/d17-1070 [28] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of deep bidirectional transformers for language understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4171–4186. [Online]. Available: https://doi.org/10.18653/v1/n19-1423 [29] W. Che, Y. Feng, L. Qin, and T. Liu, “N-LTP: A open-source neural chinese language technology platform with pretrained models,” CoRR, vol. abs/2009.11616, 2020. [Online]. Available: https://arxiv.org/abs/2009.11616 [30] J. Li, S. Ji, T. Du, B. Li, and T. Wang, “Textbugger: Generating adversarial text against real-world applications,” in 26th Annual Network and Distributed System Security Symposium, NDSS 2019, San Diego, California, USA, February 24-27, 2019. The Internet Society, 2019. [Online]. Available: https://www.ndss-symposium.org/ndss-paper/textbugger-generating-adversarial-text-against-real-world-applications/ [31] M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger, “From word embeddings to document distances,” in Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, ser. JMLR Workshop and Conference Proceedings, F. R. Bach and D. M. Blei, Eds., vol. 37. JMLR.org, 2015, pp. 957–966. [Online]. Available: http://proceedings.mlr.press/v37/kusnerb15.html [32] L. Wu and Z. Zhu, “Towards understanding and improving the transferability of adversarial examples in deep neural networks,” in Proceedings of The 12th Asian Conference on Machine Learning, ACML 2020, 18-20 November 2020, Bangkok, Thailand, ser. Proceedings of Machine Learning Research, S. J. Pan and M. Sugiyama, Eds., vol. 129. PMLR, 2020, pp. 837–850. [Online]. Available: http://proceedings.mlr.press/v129/wu20a.html Xiaoyin Yi received the M.S. degree in computer science from the Chongqing University of Posts and Telecommunications, China. She is currently a teacher with Chongqing College of Mobile Communication and pursuing the Ph.D. degree with Chongqing University of Posts and Telecommunications, China. Her research interests include cybersecurity within AI and AI security. Jiacheng Huang received the M.S. degree in instructional technology from the Hubei Normal University, China. He is currently pursuing the Ph.D. degree with Chongqing University of Posts and Telecommunications, China. His research interests include cybersecurity within AI, natural language processing, and AI security. \EOD"
https://arxiv.org/html/2411.07845v1,"Ethical Concern Identification in NLP:
A Corpus of ACL Anthology Ethics Statements","What ethical concerns, if any, do LLM researchers have? We introduce EthiCon, a corpus of 1,580 ethical concern statements extracted from scientific papers published in the ACL Anthology. We extract ethical concern keywords from the statements and show promising results in automating the concern identification process. Through a survey (N=200), we compare the ethical concerns of the corpus to the concerns listed by the general public and professionals in the field. Finally, we compare our retrieved ethical concerns with existing taxonomies pointing to gaps and future research directions.","Researchers are often asked to subscribe to ethical guidelines, e.g., the European Code of Conduct for Research Integrity ALLEA (2017) – or the ACM Code of Ethics111See https://www.aclweb.org/portal/content/acl-code-ethics for ACL’s guidelines. for publishing their work in the Association for Computational Linguistics (ACL). In addition, authors are often encouraged to write a so-called ethics statement, addressing the broader implications of their work or any ethical considerations. We ask: What ethical concerns are raised in such statements, and how do they compare with public perceptions? Is there a gap between academic and public concerns? Figure 1: Visualizing top 60 concerns in ACL ethics statements, reflecting term frequencies. As natural language processing technologies become more prevalent, understanding the ethical concerns raised by professionals will enable us to compare them with public concerns, helping to identify gaps and overlaps that can inform frameworks and solutions to existing and emerging problems. For this reason, we create EthiCon, an annotated corpus of ethics statements from the Proceedings of the 60th and 61st Annual Meeting of the Association for Computational Linguistics Muresan et al. (2022); Rogers et al. (2023). Our aim is twofold: to map out the concerns of the NLP community as they appear on the ethics statements, and to trace gaps and overlaps between NLP professionals and the general public. Our results show that laypeople express different ethical concerns than professionals, focusing on socio-economic and human-computer interaction issues, along with miscellaneous concerns like existential risks. This highlights the need for increased dialogue between researchers and the public to address these varying perspectives and an updated taxonomy covering both existing and emerging issues. Contributions. We provide a corpus of 1,580 ethics statements from the ACL Anthology. We identify the main issues that NLP researchers flag as ethically concerning in their work and show how LLMs could automate this process. Through a survey, we compare how laypeople and NLP professionals perceive the ethical concerns surrounding natural language processing. Lastly, we provide a comparison of the ACL and the survey ethical concerns to existing taxonomies of risks posed by Language Models. Finding a way to better automate the process will enable the comparison of ethical concerns across time and technical innovation and provide a better understanding of the impact of NLP research within the field and beyond. Figure 2: Examples from the identified categories of ethical concern statements."
https://arxiv.org/html/2411.07843v1,Chain Association-based Attacking and Shielding Natural Language Processing Systems,"Association as a gift enables people do not have to mention something in completely straightforward words and allows others to understand what they intend to refer to. In this paper, we propose a chain association-based adversarial attack against natural language processing systems, utilizing the comprehension gap between humans and machines. We first generate a chain association graph for Chinese characters based on the association paradigm for building search space of potential adversarial examples. Then, we introduce an discrete particle swarm optimization algorithm to search for the optimal adversarial examples. We conduct comprehensive experiments and show that advanced natural language processing models and applications, including large language models, are vulnerable to our attack, while humans appear good at understanding the perturbed text. We also explore two methods, including adversarial training and associative graph-based recovery, to shield systems from chain association-based attack. Since a few examples that use some derogatory terms, this paper contains materials that may be offensive or upsetting to some people.","In past years, many studies has shown that the adversarial examples can cause decision-making errors in natural language processing (NLP) systems Formento et al. (2023); Ou et al. (2022), even in large language models Wang et al. (2024) (LLMs). Howerver, existing adversarial attacks only consider the attack strategies in a direct way while ignoring the complexity of textual adversarial attacks in reality. For example, Chinese words {CJK}UTF8gbsn“幼稚”, which means “naive”, is an adjective with emotional tendency, but it will not be recognized as an emotional word by an emotion analysis system after being substituted with {CJK}UTF8gbsn“拿衣服”, which is a verb object phrase meaning to “take clothes”. The relation between {CJK}UTF8gbsn“幼稚” and {CJK}UTF8gbsn“拿衣服” is not a simple single-layer mapping, but a multi-layer mapping. Specifically, we associate {CJK}UTF8gbsn“幼稚” with “naive” first, which is the English translation of {CJK}UTF8gbsn“幼稚”, and then further associate it with {CJK}UTF8gbsn“拿衣服”, which is one of the Mandarin transliterations of “naive”. Note that the above is only a simple example of word substitution based on chain association while the associative ability of human being is complex. English has analogous cases. Take “screw”, a polysemy referring to “a metal object like a nail” or “having sex with someone”. Attackers online can replace “screw” with its related emoji to form offensive phrases like “{\mathchoice{\leavevmode@ifvmode\lower 0.0pt\tf@size*1/10\hbox{% \includegraphics[height=0.0pt]{screw.png}}}{\leavevmode@ifvmode\lower 0.0pt% \tf@size*1/10\hbox{\includegraphics[height=0.0pt]{screw.png}}}{% \leavevmode@ifvmode\lower 0.0pt\sf@size*1/10\hbox{\includegraphics[height=0.0% pt]{screw.png}}}{\leavevmode@ifvmode\lower 0.0pt\ssf@size*1/10\hbox{% \includegraphics[height=0.0pt]{screw.png}}}} you”, which utilizes human associations about the corresponding word of emoji and its polysemy. Note that this example is merely used as an analogy to explain our idea, in fact, this work only focuses on Chinese adversarial examples. In this work, we investigate to what extent advanced Chinese NLP systems are sensitive to chain association-based attack and explore various shielding techniques. Specifically, we first generate a chain association graph for Chinese characters based on the association paradigm for building search space of potential adversarial examples. Then, we regard generating adversarial examples as a problem of combinatorial optimization and introduce an discrete particle swarm optimization algorithm to search for the optimal adversarial examples. We show that advanced NLP models and applications are extremely vulnerable to our attack. To our best knowledge, we are the first to introduce chain association in adversarial attack. Furthermore, we also explore two methods to protect NLP systems from our attacks."
https://arxiv.org/html/2411.07826v1,Efficient Federated Finetuning of Tiny Transformers with Resource-Constrained Devices,"In recent years, \acpLLM through Transformer structures have dominated many machine learning tasks, especially text processing. However, these models require massive amounts of data for training and induce high resource requirements, particularly in terms of the large number of \acpFLOP and the high amounts of memory needed. To fine-tune such a model in a parameter-efficient way, techniques like Adapter or LoRA have been developed. However, we observe that the application of LoRA, when used in \acFL, while still being parameter-efficient, is memory and \acFLOP inefficient. Based on that observation, we develop a novel layer finetuning scheme that allows devices in cross-device \acFL to make use of pretrained \acpNN while adhering to given resource constraints. We show that our presented scheme outperforms the current state of the art when dealing with homogeneous or heterogeneous computation and memory constraints and is on par with LoRA regarding limited communication, thereby achieving significantly higher accuracies in \acFL training.","In recent years, \acpLLM have dominated various machine learning tasks, particularly next token prediction and text classification, while \acpVIT have closed the gap with \acpCNN in vision tasks. However, these models require massive amounts of data (e.g., huge quantities of text for \acpLLM) and are very power-hungry for training, as they have billions of parameters to adjust [li2020train]. As these \acpLLM, like GPT2 [radford2019language] or LLaMA [touvron2023llama], have billions of parameters and are trained on large quantities of text, they generalize well to many downstream tasks in the text domain. Similarly, in vision, multimodal Transformers can generalize well to detecting objects in an image [hu2021unit, wang2022git]. In many downstream applications, e.g., in next-word prediction or object classification on smartphones or \acIOT devices, deploying such large models imposes high resource requirements for inference, potentially causing high latency and high energy consumption. Although, through generalization, they can achieve the desired accuracy, for many tasks, they are not necessarily required, as tiny specialized models would suffice. In particular, we observe that tiny models that require \sim 60\times fewer resources (\acpFLOP) for inference compared to a lightweight GPT-2 124M model can perform similarly111Parts of the test set may be in GPT’s training set, hence increasing accuracy. in next-token prediction tasks (refer to fig. 1). However, these specialized models require a sufficient amount of problem-specific data to serve as a replacement. In many cases, this problem-specific data resides on resource-constrained edge devices (such as smartphones or \acIOT devices), is privacy-sensitive, and cannot be centrally stored and processed. While in centralized training, several hundred-watt server GPUs are available, edge devices are very limited in their resources and can only spend a few watts on training. Additionally, such devices can have heterogeneous capabilities [pfeiffer2023federated]. To make use of the devices’ data, training must be performed on the devices themselves. In recent years, \acFL has emerged as a privacy-sensitive alternative to centralized training and has shown success in many domains such as smartphone applications, healthcare, and robotics [chen2020fedhealth, yuan2020federated, ciftler2020federated, posner2021federated]. In this work, we study how tiny pretrained Transformer models can be adapted to downstream tasks using \acFL with heterogeneous resource-constrained devices. To adapt large models to downstream tasks, recently, popular techniques like Adapter [houlsby2019adapter] and LoRA [hu2021lora] have been introduced, mainly allowing the adaptation of such models in a parameter-efficient way. However, we observe that while being parameter-efficient, techniques like LoRA still require massive amounts of memory for training in the case of tiny models. The reason for this is that LoRA mainly reduces the memory overhead of gradients and optimizer states but does not lower the activation memory. For large models, this typically suffices as the weights and gradients account for most of the required training memory. We observe that the memory footprint of tiny language models and \acpVIT is mainly dominated by the activation memory (appendix A). Figure 1: Comparison of downstream performance and resource requirements for next-token prediction on Shakespeare [caldas1812leaf] using layer finetuning (with layers frozen from first to last, where each dot represents a specific number of layers being frozen) and LoRA with tiny Transformers pretrained on OpenWebText, having 3, 6, and 9 layers. We observe that while LoRA (with ranks 24, 12, 3) can achieve gains in communication efficiency, it requires significantly more peak memory and \acpFLOP to reach the same accuracy. Hyperparameters and details are provided in section 4.1. GPT2 is included to highlight inference costs (accuracy is based on GPT2’s tokenizer and test data may be part of GPT2’s training set). We compare LoRA against finetuning individual layers of a set of tiny \acpNN (Transformers with 3–12 layers, 3 heads, and an embedding size of 92) that were pretrained on OpenWebText [Gokaslan2019OpenWeb] and trained in a federated manner to perform next-token prediction on Shakespeare [caldas1812leaf] from the Leaf benchmark. From fig. 1, we can draw the following conclusion: For tiny language models, LoRA can achieve similar communication savings compared to layer finetuning. With LoRA, only the low-rank adapters need to be uploaded, while with layer finetuning, only the trained layers need to be uploaded. However, besides memory, we observe that compared to layer finetuning, LoRA requires significantly more computation (as backpropagation is needed from the last to the first layer). Motivated by that observation, we re-evaluate existing \acFL strategies that address resource constraints and heterogeneous devices in cross-device \acFL. Different from the state of the art, we assume that the \acFL models are already pretrained. Furthermore, we assume that we have a selection of differently sized \acpNN to choose from. We observe that when using pretrained \acpNN, layer finetuning outperforms LoRA-derived techniques [cho2023heterogeneous] as well as existing state-of-the-art methods [yao2021fedhm, alam2022fedrolex, diao2020heterofl, horvath2021fjord, kim2023depthfl]. Based on our observations, we propose a strategy that, through \acNN selection and layer finetuning, allows reaching the highest accuracy while adhering to a given device constraint. In summary, we make the following novel contributions: • We are the first to study resource-constrained cross-device \acFL with the availability of pretrained models. We rigorously evaluate existing \acFL strategies and observe that layer finetuning outperforms vanilla LoRA [hu2021lora] as well as recent \acFL adaptations for heterogeneity [cho2023heterogeneous]. Furthermore, with pretrained tiny \acNN models, layer finetuning also surpasses existing \acFL techniques [diao2020heterofl, alam2022fedrolex, horvath2021fjord, yao2021fedhm, kim2023depthfl]. Specifically, we study \acFL downstream tasks such as Shakespeare, CIFAR10, and CIFAR100. • Existing works assume a fixed \acNN that must be trained while adhering to device constraints. In this work, we assume a set of pretrained \acNN architectures is available for selection. Based on our observations, we propose \acfOURS, an architecture selection technique that achieves the highest accuracy given a device constraint. • We evaluate the performance of state-of-the-art techniques and \acOURS in heterogeneous settings. Additionally, our technique promotes greater fairness for weaker devices."
https://arxiv.org/html/2411.07795v1,InvisMark: Invisible and Robust Watermarking for AI-generated Image Provenance,"The proliferation of AI-generated images has intensified the need for robust content authentication methods. We present InvisMark, a novel watermarking technique designed for high-resolution AI-generated images. Our approach leverages advanced neural network architectures and training strategies to embed imperceptible yet highly robust watermarks. InvisMark achieves state-of-the-art performance in imperceptibility (PSNR\sim51, SSIM \sim 0.998) while maintaining over 97% bit accuracy across various image manipulations. Notably, we demonstrate the successful encoding of 256-bit watermarks, significantly expanding payload capacity while preserving image quality. This enables the embedding of UUIDs with error correction codes, achieving near-perfect decoding success rates even under challenging image distortions. We also address potential vulnerabilities against advanced attacks and propose mitigation strategies. By combining high imperceptibility, extended payload capacity, and resilience to manipulations, InvisMark provides a robust foundation for ensuring media provenance in an era of increasingly sophisticated AI-generated content. Source code of this paper is available at: https://github.com/microsoft/InvisMark.","The rapid advancement of generative AI (GenAI) technologies has revolutionized the creation of digital images, enabling the production of hyper-realistic deepfakes with unprecedented ease. While this technological leap offers exciting possibilities for creative expression, it simultaneously poses significant challenges to information integrity and public trust. The potential for these AI-generated images to be used in manipulating elections, damaging reputations, and undermining societal foundations underscores the urgent need for robust solutions to verify the origin and authenticity of digital content [18]. In response to these challenges, the Coalition for Content Provenance and Authenticity (C2PA) has emerged as a collaborative effort aimed at combating misinformation within the digital content ecosystem [31, 1]. The C2PA suggests adding signed provenance information directly into its metadata. However, this approach is vulnerable to metadata stripping by malicious actors or during content sharing on social media platforms [7, 8, 38]. In this case, the stripped provenance can potentially be recovered through soft bindings, such as fingerprinting or watermarking, from trusted repository. Fingerprinting techniques leverage near-duplicate search in trusted databases for recovering content provenance [7, 8, 38]. However, these matches often lack precision, necessitating human intervention for verification. Image watermarking offers an alternative solution by inserting an imperceptible identifier within the content itself [14]. This allows for exact matching and retrieval of associated provenance from databases. Traditional watermarking techniques have focused on embedding imperceptible patterns within images, either directly in pixel values or in transformed frequency domains. Pixel-based methods, such as least significant bit (LSB) embedding, are simple to implement but highly susceptible to removal [34]. Frequency-domain techniques, utilizing transforms like Discrete Wavelet Transform (DWT) or Discrete Cosine Transform (DCT), offer improved robustness to certain transformations and have seen industrial adoption, such as the commerical implementation of Stable Diffusion [4, 30]. Despite the improvements, frequency domain methods still suffer from vulnerability to relatively minor alterations to the image, limiting their robustness in real-world scenarios [21, 20, 29]. Figure 1: Overview of our method for watermark encoding and decoding. Watermark first passes through a preprocessing layer and then concatenates with the resized cover image. A MUNIT-based encoder generates watermark residuals, which are upscaled and added to the original image, producing the watermarked image. To ensure robustness, we select the top-k noises that yield the poorest watermark recovery from a pre-defined set of noises, these losses are incorporated into watermark training. The advent of GenAI has spurred the development of innovative watermarking algorithms that integrate seamlessly into the image generation process. These approaches include watermarking training images with pre-trained encoders and decoders, followed by fine-tuning generative models on these watermarked images [35]. Alternatively, some methods focus on fine-tuning only the decoder of a Latent Diffusion Model (LDM) while leaving the diffusion component unchanged [26]. However, these approaches are computationally intensive and often model-specific. Furthermore, their primary targeted applications have been in AI-generated image detection or user identification [32, 17], rather than content provenance tracking. In contrast, post-generation watermarking offers greater flexibility, as it can be applied to any image regardless of its origin. Methods like HiDDen [40] and RivaGAN [36] utilize encoder-decoder architecture to embed hidden messages within images. Subsequent works have proposed various improvements to enhance image quality and robustness [12, 15, 27, 33]. For instance, StegaStamp [33] introduces significant image perturbations between encoder and decoder, enabling the encoded image to withstand real-world distortions. TrustMark [10] proposed a scaling-based approach for watermarking images of any resolution. Beyond encoder-decoder architectures, SSL [17] suggests embedding watermarks within the self-supervised latent space by shifting image features into a designated area. RoSteALS [11] encodes messages in the latent space using a frozen VQVAE [16], but imperceptibility is constrained by VQVAE reconstruction quality. Despite these advancements, current watermarking algorithms face limitations in their effectiveness as a soft-binding solution for AI-generated images. Previous algorithms, often trained and evaluated on low-resolution images, encounter difficulties maintaining their performance when scaled to the higher resolutions, which is now standard in modern image generation models like Stable Diffusion and DALL\cdotE 3 [30]. Furthermore, the inherent tradeoff between capacity, imperceptibility, and robustness restricts watermark payload capacity, typically to under 100 bits. This limited capacity elevates the risk of ID collisions in the presence of bit errors, compromising the reliability of watermark extraction. Additionally, post-generation watermarking techniques can introduce perceptible artifacts, negatively impacting image quality and hindering adoption, especially in creative fields where visual fidelity is paramount. To address these challenges, we introduce InvisMark, a novel approach rooted in the insight that high-resolution images inherently possess the capacity to embed a multitude of imperceptible signals. By leveraging carefully crafted neural network architectures and training strategies, we can effectively harness this potential. Our contributions can be summarized as follows: Jpeg Compress. (min q) Brightness (bri.) Contrast (con.) Saturation (sat.) Gaussian blur (k,\sigma) Gaussian Noise (std.) Posterize (bits) 50 075/1.25 0.75/1.25 0.75/1.25 5, 0.1-1.5 0.04 4 ColorJiggle (bri., con., sat., hue) RGB shift (shift limit) Flip (prob.) Rotation (deg.) RandomErasing (scale, ratio) Perspective (scale) RandomResizedCrop (scale, ratio) 0.1, 0.1, 0.1, 0.02 0.05 1.0 0-10.0 0.02-0.1, 0.5-1.5 0.1 0.75-1.0, 3/4 - 4/3 Table 1: Noise settings used in watermark training and evaluation. For JPEG compression, q is the compression factor. For Gaussian blur, k is the kernel size and \sigma is the range for the standard deviation of the kernel. Additionally, for Brightness, Contrast and Saturation, only the lower and upper bound values are used to simulate noise, rather than random values within the range. All noise implementations utilize the Kornia library, except for JPEG compression, which employs the torchvision library. 1. Novel architecture: We apply resolution scaling during training and employ robust optimization techniques to enhance decoder resilience against common image transformations with minimal impact on encoded image quality. 2. State-of-the-art performance: InvisMark outperforms existing methods in both imperceptibility and robustness across AI-generated and non-AI-generated image datasets. 3. Larger Payload: We demonstrate the ability to embed 256 bits of watermarks while maintaining exceptional imperceptibility and robustness, expanding the practical applications of our method in real-world scenarios."
https://arxiv.org/html/2411.07794v1,Feature Fusion Transferability Aware Transformer for Unsupervised Domain Adaptation,"Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from labeled source domains to improve performance on the unlabeled target domains. While Convolutional Neural Networks (CNNs) have been dominant in previous UDA methods, recent research has shown promise in applying Vision Transformers (ViTs) to this task. In this study, we propose a novel Feature Fusion Transferability Aware Transformer (FFTAT) to enhance ViT performance in UDA tasks. Our method introduces two key innovations: First, we introduce a patch discriminator to evaluate the transferability of patches, generating a transferability matrix. We integrate this matrix into self-attention, directing the model to focus on transferable patches. Second, we propose a feature fusion technique to fuse embeddings in the latent space, enabling each embedding to incorporate information from all others, thereby improving generalization. These two components work in synergy to enhance feature representation learning. Extensive experiments on widely used benchmarks demonstrate that our method significantly improves UDA performance, achieving state-of-the-art (SOTA) results.","Deep neural networks (DNNs) have achieved remarkable breakthroughs across various application fields owing to their impressive automatic feature extraction capabilities. However, such success often relies on the availability of large labeled datasets, which can be challenging to acquire in many real-world scenarios due to the significant time and labor required. Fortunately, unsupervised domain adaptation (UDA) techniques [40] offer a promising solution by harnessing rich labeled data from a source domain and transferring knowledge to target domains with limited or no labeled examples. The essence of UDA lies in identifying discriminant and domain-invariant features shared between the source domain and target domain within a common latent space [44]. Over the past decade, as interests in domain adaptation research have grown, numerous UDA methods have emerged and evolved [50, 20, 24], such as adversarial adaptation, which focuses on discriminating domain-invariant and domain-variant features and acquiring domain-invariant feature representations through adversarial learning [52, 24]. Besides, deep unsupervised domain adaptation techniques usually employ a pre-trained Convolutional Neural Network (CNN) backbone [19]. Recently, the self-attention mechanism and vision transformer (ViT) [7, 48, 41] have received growing interest in the vision community. Unlike convolutional neural networks that gather information from local receptive fields of the given image, ViTs leverage the self-attention mechanism to capture long-range dependencies among patch features through a global view. In ViT and many of its variants, each image is partitioned into a series of non-overlapping fixed-size patches, which are then projected into a latent space as patch tokens and combined with position embeddings. A class token, representing the entire image, is prepended to the patch tokens. All tokens are then fed into a specific number of transformer layers to learn visual representations of the input image. Leveraging the superior global content capture capability of the self-attention mechanism, ViTs have demonstrated impressive performance across various vision tasks, including image classification [7], video understanding [11], and object detection [1]. Despite increasing interest, only a few studies have explored the application of ViTs for unsupervised domain adaptation tasks [44, 34, 43, 50]. In this work, we introduce a novel Feature Fusion Transferability Aware Transformer, designed for unsupervised domain adaptation. FFTAT builds upon TVT [44], the first ViT-based UDA model, by introducing two key components: (1) a transferability graph-guided self-attention (TG-SA) mechanism that enhances information from highly transferable features while suppressing information from less transferable features, and (2) a carefully designed features fusion (FF) operation that makes each embedding incorporate information from other embeddings in the same batch. Fig. 1 illustrates the transferability graph guided self-attention and feature fusion. From a graph view, vanilla self-attention among patches can be seen as an unweighted graph, where the patches are considered as nodes, and the attention between nodes is regarded as the edge connecting them. Unlike vanilla self-attention, our proposed transferability graph guided self-attention is controlled by a weighted graph, where the information communication between highly transferable patches is emphasized via a large-weight edge, and the information communication between less transferable patches is attenuated by a small-weight edge [48, 47]. The transferability graph is automatically learned and updated through learning iterations in the transferability-aware layer, where we design a patch discriminator to evaluate the transferability of each patch. The TG-SA allows for integrative information processing, facilitating the model to focus on domain-invariant features shared between domains and gather important information for domain adaptation. The Feature Fusion (FF) operation enables each embedding to integrate information from other embeddings. Different from recent work PMTrans [53] for unsupervised domain adaptation, our feature fusion occurs in the latent space rather than on the image level. These two new components synergistically enhance robust feature representation learning and generalization in UDA tasks. Extensive experiments on widely used UDA benchmarks demonstrate that FFTAT significantly improves UDA performance, achieving new state-of-the-art results. In summary, our contributions are as follows: • We introduce a novel transferability graph-guided attention mechanism in ViT architecture for UDA, enhancing performance by promoting attention between highly transferable features while suppressing attention between less transferable ones. • We propose a feature fusion technique that enhances feature learning and generalization capabilities for UDA. • Our proposed model, FFTAT, integrates transferability graph-guided attention and feature fusion mechanisms, resulting in notable advancements and state-of-the-art performance on widely used UDA benchmarks. Figure 2: The overview of the FFTAT framework. In FFTAT, source and target images are divided into non-overlapping fixed-size patches which are linearly projected into the latent space and concatenated with positional information. A class token is prepended to the image tokens. The tokens are subsequently processed by a transformer encoder. The Feature Fusion Layer mixes the features as illustrated in Fig. 1. The patch discriminator assesses the transferability of each patch and generates a transferability graph, which is used to guide the attention mechanism in the transformer layers. The classifier head and self-clustering module operate on source domain images and target domain images, respectively. The Domain Discriminator predicts whether an image belongs to the source or target domain."
https://arxiv.org/html/2411.07773v1,Likelihood as a Performance Gauge for Retrieval-Augmented Generation,"Recent work finds that retrieval-augmented generation with large language models is prone to be influenced by the order of retrieved documents in the context. However, the lack of in-depth analysis limits the use of this phenomenon for prompt engineering in practice. In this study, we posit that likelihoods serve as an effective gauge for language model performance. Through experiments on two question-answering datasets with a variety of state-of-the-art language models, we reveal correlations between answer accuracy and the likelihood of the question at both the corpus level and the instance level. In addition, we find that question likelihood can also indicate the position of the task-relevant information in the context. Based on these findings, we propose two methods that use question likelihood as a gauge for selecting and constructing prompts that lead to better performance. We demonstrate their effectiveness with experiments. In addition, our likelihood-based methods are efficient, as they only need to compute the likelihood of the input, requiring much fewer language model passes than heuristic prompt engineering methods that require generating responses. Our analysis deepens our understanding of how input prompts affect model performance and provides a promising direction for efficient prompt optimization.111Our code is available at https://github.com/lyutyuh/poptimizer.","Figure 1: A prompt with higher question likelihood tends to lead to a better answer. Prompt designing is crucial for large language models (LMs) when tackling downstream tasks with retrieval-augmented generation (RAG, Lewis et al., 2020). Well-designed prompts can boost LMs’ performance and lead them to generate responses that better meet users’ expectations (Gao et al., 2021; Izacard et al., 2024; Liu et al., 2024; Schulhoff et al., 2024; Ma et al., 2024, inter alia). Typically, under the RAG framework, a prompt consists of three major components—an instruction defining a task and providing general guidance, a specific question222Formally, in an input prompt, we refer to the segment that directly conveys the question or query expected to be solved by the LM’s output as a question. In our experiments, this contains the entire question sentence, including the punctuation. of the task, and a context comprising a set of documents retrieved by retrievers from some external source (Karpukhin et al., 2020; Ni et al., 2022). Much previous work has explored empirical approaches of prompt engineering, such as manually designing prompts that mimic human reasoning Wei et al. (2023); Yao et al. (2023). Recently, Liu et al. (2024) have shown that LM performance is substantially affected by the order of the retrieved documents in the context: Namely, the answer accuracy peaks when the gold document333In factual QA tasks, the document containing the ground truth answer as a substring is referred to as a “gold document”. is placed at the beginning or the end of the context. While extensive experimental results have been presented for validating the existence of such a phenomenon, there is a lack of insights into the underlying mechanism driving it, limiting its applicability in prompt designing and optimization for real-world applications. Figure 2: Besides answer accuracy (Liu et al., 2024), we find that the likelihood of question simultaneously fluctuates in a U-shaped curve as the gold document position within the context changes. The log-likelihood is computed per token in the question, with LLaMA-3-8B. In this work, we contend that the likelihood assigned by the LM to a question preceded by a given retrieval-augmented context can provide useful information on forecasting the LM’s accuracy in answering that question. For verification, we conduct comprehensive experiments on two QA benchmarks (NQ-Open and ELI5) with a variety of state-of-the-art open LMs (LLaMA-2, LLaMA-3, LLaMA-3.1, Mistral-v0.3, MPT). We focus on three functional components in the input prompt and LM output—namely the context, the question, and the gold answer—and analyze their log-likelihood.444In information theory, the negative log-likelihood is also called surprisal as it quantifies how “surprising” a particular outcome is to the estimator. At the corpus level, we find that LMs can better respond to questions with higher log-likelihoods; at the instance level, contexts with favorable document orders that lead to higher likelihoods of a particular question are also more likely to elicit better answers as shown in Figure 1. Besides, we find that changing the position of relevant information in the input context simultaneously affects question likelihood and answer accuracy, as illustrated in Figure 2. Thus, question likelihood can be deemed both a performance gauge and a strong indicator of the position of useful task-relevant information in the input context. Based on these findings, we propose a promising direction for prompt optimization with two specific methods. The first directly takes the prompt that leads to the highest question likelihood during random document shuffling. The other uses question likelihood as a gauge for task-relevant information and reorders the documents within the context to get a better prompt. Experiments show that our methods improve answer accuracy on the two datasets for both instruction-tuned and base models. In addition to effectiveness, our method is efficient because it only employs the encoding function of pretrained LMs and computes the likelihood for each token in the prompt. Because LM encoding can be parallelized, the computation time for encoding can be vastly shorter than decoding an LM response.555Encoding, also known as prefill phase or prompt phase, requires significantly fewer LM passes than decoding. Previous work (Kwon et al., 2023; Zhong et al., 2024) reports that the throughput of processing input prompts, measured by the number of tokens processed per second, can be up to three orders of magnitude larger than that of completion generation. To our knowledge, our work is the first to present in-depth analyses of the relation between question likelihood and model performance under the RAG framework. Our contributions in this paper are summarized as follows: • We hypothesize and prove that question likelihood positively correlates with answer accuracy at corpus level on NQ-Open and ELI5. • We also demonstrate a strong instance-level correlation and verify its generality on the two datasets, based on our hypothesis. • We reveal that question likelihood is an indicator of the position of task-relevant information in the context. • We validate the effectiveness and efficiency of using question likelihood as a gauge for prompt optimization and demonstrate that likelihood-based prompt optimization is a promising direction for future study."
https://arxiv.org/html/2411.07772v1,Automatic Album Sequencing,"Album sequencing is a critical part of the album production process. Recently, a data-driven approach was proposed that sequences general collections of independent media by extracting the narrative essence of the items in the collections. While this approach implies an album sequencing technique, it is not widely accessible to a less technical audience, requiring advanced knowledge of machine learning techniques to use. To address this, we introduce a new user-friendly web-based tool that allows a less technical audience to upload music tracks, execute this technique in one click, and subsequently presents the result in a clean visualization to the user. To both increase the number of templates available to the user and address shortcomings of previous work, we also introduce a new direct transformer-based album sequencing method. We find that our more direct method outperforms a random baseline but does not reach the same performance as the narrative essence approach. Both methods are included in our web-based user interface, and this—alongside a full copy of our implementation—is publicly available at https://github.com/dylanashley/automatic-album-sequencing","Album sequencing is the process of taking a music album and ordering it so that listening to it in that order produces a desired emotional response in the listener. Despite its importance in producing an impactful music album, album sequencing has received comparatively little attention from the artificial intelligence community. Our previous research [1] introduced a way to compress different kinds of media down into an ultra-low dimensional representation. This representation captures their relevancy to the overarching story induced by the ordering of the collection they belonged to, i.e., their narrative essence. This is accomplished by using neural networks and contrastive learning [2, 3]. Then, evolutionary algorithms are used to learn a set of template curves and a novel curve-fitting algorithm to fit the narrative essence of new media collections to these template curves. The above was principally done with music albums from the FMA dataset [4], though it is shown that this applies to other forms of media as well. There are two key issues with our previous work. First, our previous method requires knowledge of advanced machine learning techniques, making it inaccessible to many people who perform album sequencing. Second, it requires a complex pipeline with (1) a neural network to extract the narrative essence followed by (2) a separate evolutionary algorithm to learn a set of templates and then (3) a fitting algorithm to produce a final ordering. This is a highly complex and particularly problematic setup that does not allow information like the genre of an album to flow between the narrative essence and the final ordering, resulting in genre-agnostic templates. Here, we address both of the aforementioned issues. To address the latter issue, we introduce a new approach that replaces the full pipeline with a single Transformer [5, 6, 7]. While this does not outperform the more complicated pipeline proposed in our previous work, the new simpler pipeline still outperforms a random baseline, making it useful for automatic album sequencing. Next, to address the former issue, we implement and release a dedicated user-friendly web-based interface that allows a less technically inclined user to run both the narrative essence-based and the new simplified album sequencing approaches on the user’s own music. We release this interface alongside a complete implementation of our approach publicly at https://github.com/dylanashley/automatic-album-sequencing In summary, our contributions are as follows: (1) We introduce a new direct method to perform automatic album sequencing. (2) We show that, despite the simpler pipelineethod outperforms a random baseline. (3) We release a web-based user interface tool that makes automatic album sequencing accessible to a less technical audience."
https://arxiv.org/html/2411.07762v1,ASER: Activation Smoothing and Error Reconstruction for Large Language Model Quantization,"Quantization stands as a pivotal technique for deploying large language models (LLMs), yet it poses significant challenges particularly in achieving effective low-bit quantization. The limited numerical mapping makes the quantized model produce a non-trivial error, bringing out intolerable performance degradation. This paper is anchored in the basic idea of model compression objectives, and delves into the layer-wise error distribution of LLMs during post-training quantization. Subsequently, we introduce ASER, an algorithm consisting of (1) Error Reconstruction: low-rank compensation for quantization error with LoRA-style matrices constructed by whitening SVD; (2) Activation Smoothing: outlier extraction to gain smooth activation and better error compensation. ASER is capable of quantizing typical LLMs to low-bit ones, particularly preserving accuracy even in W4A8 per-channel setup. Experimental results show that ASER is competitive among the state-of-the-art quantization algorithms, showing potential to activation quantization, with minor overhead.","The proliferation of large language models (LLMs) places increasing demands on computation and storage (Vaswani et al. 2017; Devlin et al. 2018), and stretches the boundaries of contemporary hardware prowess. Regarding typical generative models such as Llama3.1-310B (Llama Team 2024) or Qwen2-72B (Yang et al. 2024), they require hundreds of gigabytes of VRAM, and often rely on multi-GPU cluster data centers for deployment. Researchers have been trying to compress these large models while maintaining their performance in a more compact form. Quantization (Frantar et al. 2022; Xiao et al. 2023) of large language models emerges as a pivotal strategy in optimizing the deployment and computational efficiency of these heavy neural architectures. Quantization involves converting the model’s weights and activations from high-precision floating-point numbers to lower-precision integers, whose primary goal is alleviating the computational and memory requirements without significantly compromising the model’s performance, enabling edge deployment. Take post training quantization (PTQ) as an example, current advancements try to smooth (Xiao et al. 2023), rotate (Liu et al. 2024) or transform tensor representations in models, showing promise of achieving quantization-friendly data range and remarkable compression rate. However, when it comes to low-bit quantization, even with the application of the aforementioned techniques, the error introduced by the quantized model becomes evident, leading to notable degradation in the evaluation metrics. As for typical quantization, the optimization objective can be formulated as minimizing \|\textbf{WX}-\textbf{W}_{q}\textbf{X}\|_{F}, where W and \textbf{W}_{q}=Q(\textbf{W}) are original model weight and its quantized one respectively, and X is the activation. We empirically find this inevitable quantization error has low-rank property, which prompts us to consider using a method similar to LoRA (Hu et al. 2021) to reconstruct this error. Specifically, the activation-weight quantization loss exhibits distinct low-rank property, with its singular value distribution featuring a small number of high values and a long-tail bulk of low ones, which varies both intra and inter Transformer blocks. Furthermore, we find the channels that produce the major error are consistent with the existence of the outliers. Figure 1: The framework of ASER. The LoRA-style matrices \textbf{L}_{A},\textbf{L}_{B} are generated to reconstruct the quantization error. We propose ASER (Activation Smoothing and Error Reconstruction), a low-rank compensation algorithm designed to enhance the efficacy of quantized models. Our method dynamically assesses the low-rank properties during the quantization process, whitens the relationship between quantization loss and singular values based on Cholesky factorization. Then, ASER identifies and smoothes these error-inducing outliers for separate compensation. Finally, we use LoRA-style skinny matrices to compensate the quantization error, with little computation overhead. The overall framework is shown in Figure 1. This lightweight yet powerful mechanism ensures robust performance of our quantized models, and this approach is orthogonal to any particular weight quantization method. Our experiments demonstrate that ASER remarkably recovers the quantization error, where W4A8 per-channel quantized model performs nearly equal capability related to the half-precision reference model. This work makes the following contributions: • We formulate the optimization objective of minimizing the discrepancy in model outputs, analyze the characteristics of quantization errors in LLMs quantization. • We propose a novel algorithm ASER, which includes error reconstruction using whitening SVD, and activation smoothing with outlier analysis. • Experimental results show ASER can significantly recover the performance of quantized model in W4A8 per-channel quantization, with little computational overhead."
https://arxiv.org/html/2411.07751v1,SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State Space Model,"Speech enhancement plays an essential role in various applications, and the integration of visual information has been demonstrated to bring substantial advantages. However, the majority of current research concentrates on the examination of facial and lip movements, which can be compromised or entirely inaccessible in scenarios where occlusions occur or when the camera view is distant. Whereas contextual visual cues from the surrounding environment have been overlooked: for example, when we see a dog bark, our brain has the innate ability to discern and filter out the barking noise. To this end, in this paper, we introduce a novel task, i.e. Scene-aware Audio-Visual Speech Enhancement (SAV-SE). To our best knowledge, this is the first proposal to use rich contextual information from synchronized video as auxiliary cues to indicate the type of noise, which eventually improves the speech enhancement performance. Specifically, we propose the VC-S2E method, which incorporates the Conformer and Mamba modules for their complementary strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and AudioSet datasets, where the results demonstrate the superiority of VC-S2E over other competitive methods. We will make the source code publicly available. Project demo page: https://AVSEPage.github.io/","In our daily living environments, speech signals are often distorted by various environmental background noises during their propagation. Speech enhancement (SE) is a task aiming at isolating the clean speech in the presence of noise interference, resulting in improved speech intelligibility and perceptual quality [1, 2, 3, 4]. It enables natural and effective Human-Robot Interaction (HRI) and plays a crucial role in various applications, such as hearing aids, mobile communication, automatic speech recognition [5, 6, 7], speaker verification [8], and speaker tracking [9, 10, 11]. These applications underscore the importance of SE in realistic scenarios. Traditional signal processing-based SE approaches, which are derived from the assumed properties on speech and noise, are incapable of suppressing highly non-stationary noise sources [12, 13, 14]. In the past decade, with the advent of deep learning technology and increased computational resources, supervised speech enhancement solutions has achieved great success [2]. Figure 1: Our proposed SAV-SE task where outputs from the audio and visual encoder are fused to refine and generate the enhanced audio. By incorporating the visual context from noise environments, it significantly enhances speech quality, particularly in situations where traditional audio-only techniques falter. Despite the significant strides made in the field, the challenge of noise reduction without inflicting artifacts on the speech signal persists, particularly in dynamic environments characterized by non-stationary and multi-source noise [15]. This difficulty is further compounded by the need to maintain the integrity of the speech signal, ensuring that the naturalness of the human voice is preserved. To address this challenge, researchers have been exploring cutting-edge signal processing methodologies and sophisticated machine learning paradigms. One promising solution involves the use of neural networks, which has demonstrated great capabilities in extracting features and separating signals from complex acoustic environments. A variety of network architectures are trained to learn the underlying patterns in noisy audio data, thus accomplishing the objective of speech enhancement [16]. Each of these models contributes unique strengths to the task of learning and generalizing from noisy audio data. For example, Multi-Layer Perceptrons (MLPs) are proficient in detecting intricate, non-linear data patterns, whereas Recurrent Neural Network (RNN) effectively manage the sequential dependencies in audio signals. Temporal Convolutional Network (TCN) excel in capturing long-range dependencies without suffering from the vanishing gradient problem that plagues standard RNN. The Transformer architecture, featuring self-attention, has transformed the field by allowing models to process any part of the input sequence, which is crucial for tasks involving widespread noise-speech relationships. The Mamba architecture [17], as the latest advancement, further extends the capabilities of noise reduction and speech enhancement. Researchers have increasingly acknowledged the importance of maintaining semantic, temporal, and spatial coherence between audio and video sources [18, 19]. This motivates attempts to use video information as a complement of audio input to recover details that are lost in audio-only scenarios. Existing Audio-Visual Speech Enhancement (AVSE) schemes often exploit temporal synchronized facial and lip movements to improve the clarity and perception of enhanced speech [20, 21, 22]. Despite outperforming audio-only SE systems, they are infeasible in many practical scenarios (e.g., outdoors or pandemic period) where human visual cues are not available. Moreover, inaccurate face or lip detection (e.g., in low-quality videos) may also result in degraded performance. In contrast, visual cues of environmental information, such as noise scenes or background objects emitting the noise, are easier to capture. It is more practical to use visual environmental cues to provide a valuable complement to speech enhancement. Thus, to fully leverage audio-visual information to enhance uni-modal learning, it is essential to consider these modality-specific attributes. In this paper, we introduce a novel AVSE framework, as illustrated in Figure 1, which uses visual information of the surrounding scenes as an auxiliary prompt to improve SE performance. Specifically, it addresses the limitations of current technologies, particularly in scenarios where an accurate capture of facial or lip information is not available. The contributions of this paper are summarized as follows: 1. We introduce a novel and more practical scene-aware AVSE task, namely SAV-SE. Unlike existing AVSE studies that rely primarily on visual facial and lip movements, this paper explores auxiliary visual contextual cues from the surrounding scenes to mitigate environmental background noise. 2. We are the first to explore selective State Space Model (SSM) for audio-visual speech enhancement. Specifically, we propose a Visual-prompting ConMamba for Scene-aware Speech Enhancement (VC-S2E), a novel approach that leverages audio-visual modalities to improve speech quality and intelligibility. Built upon innovative hybrid convolution-SSM architecture, ConMamba can capture both long-range global interactions and localized fine-grained feature patterns. 3. We comprehensively evaluate our proposed method across three widely used AV datasets. The results consistently confirm the superiority of our \text{VC-}\text{S}^{2}\text{E} over other competing methods in speech quality and intelligibility. Meanwhile, the visualization analysis illustrates that visual focal areas locate at the sounding object, demonstrating the contribution of visual scene information."
https://arxiv.org/html/2411.07728v1,No-Reference Point Cloud Quality Assessment via Graph Convolutional Network,"Three-dimensional (3D) point cloud, as an emerging visual media format, is increasingly favored by consumers as it can provide more realistic visual information than two-dimensional (2D) data. Similar to 2D plane images and videos, point clouds inevitably suffer from quality degradation and information loss through multimedia communication systems. Therefore, automatic point cloud quality assessment (PCQA) is of critical importance. In this work, we propose a novel no-reference PCQA method by using a graph convolutional network (GCN) to characterize the mutual dependencies of multi-view 2D projected image contents. The proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e., multi-view projection, graph construction, and GCN-based quality prediction. First, multi-view projection is performed on the test point cloud to obtain a set of horizontally and vertically projected images. Then, a perception-consistent graph is constructed based on the spatial relations among different projected images. Finally, reasoning on the constructed graph is performed by GCN to characterize the mutual dependencies and interactions between different projected images, and aggregate feature information of multi-view projected images for final quality prediction. Experimental results on two publicly available benchmark databases show that our proposed GC-PCQA can achieve superior performance than state-of-the-art quality assessment metrics. The code will be available at: https://github.com/chenwuwq/GC-PCQA.","In recent years, the development of three-dimensional (3D) visual information acquisition technology makes point clouds easier to obtain and gradually becomes a popular type of visual data. A 3D Point cloud is mainly used to describe a complete 3D scene or object, including geometric attributes (position of each point in 3D space), color attributes (RGB attributes of each point), and others (normal vector, opacity, reflectivity, time, etc.)[1]. Point clouds have been widely studied and used in a wide range of application scenarios such as 3D reconstruction[2, 3], classification and segmentation[4, 5], facial expression representation[6], autonomous driving[7, 8], and virtual reality[9], etc. Although point cloud can realistically record 3D objects through a large set of points, it also consumes a lot of memory, and it is difficult to achieve data transmission under limited network bandwidth[10, 11]. This new and effective data representation presents a challenge to the current hardware storage and network transmission. Therefore, in order to achieve efficient storage and transmission, compression of point clouds is necessary[12, 13, 14, 15]. However, point cloud compression may introduce artifacts, resulting in the degradation of point cloud visual quality. Point cloud visual quality is an important way to compare the performance of various point cloud processing algorithms. Effective point cloud quality assessment (PCQA) methods can not only help people evaluate the distortion degree of point clouds and the performance of compression algorithms but also be beneficial to optimize the visual quality of distorted point clouds. Thus, how to accurately assess the perceptual quality of point clouds has become a critical issue. Similar to image quality assessment (IQA), PCQA can also be divided into subjective and objective methods. The subjective method is mainly based on the perception of the human visual system (HVS). It is difficult to be widely applied because this kind of assessment requires a large number of participants to ensure the rationality and accuracy of the assessment results in a statistical sense. Currently, the results obtained from subjective assessment experiments are generally served as the ground-truth data for benchmarking different objective methods [16]. According to the participation of original point clouds, objective PCQA methods can have three categories: full reference (FR), reduced reference (RR), and no reference (NR). Since the original point clouds are not always available, NR-PCQA methods that do not rely on any original information as a reference are more suitable in practical applications. The traditional NR-PCQA methods [17, 18] generally predict the quality score by extracting quality-aware features based on the analysis of point cloud attributes such as geometry and color. Recently, the great success of deep learning in the field of NR-IQA has promoted the development of deep learning-based NR-PCQA metrics [19, 20, 21, 22, 23, 24]. The common practice of these deep NR-PCQA metrics is to directly apply the ordinary convolution operation on the point cloud for automatic feature learning in a data-driven manner. Nonetheless, point cloud is a typical kind of non-Euclidean data which is sparsely distributed over the 3D space, and a large number of useless pixels are also involved with pixel-by-pixel convolution, thus resulting in a huge waste of resources and inefficient data processing. In order to solve this problem, some related works try to represent non-Euclidean data with the graph which includes node information and complex adjacency relations between nodes. With the graph-based non-Euclidean data as input, the current works then introduce to use graph convolutional network (GCN) rather than traditional convolutional neural network (CNN) for more effective feature representation learning [25]. For instance, Thomas et al.[26] proposed to convert non-Euclidean data into a graph based on which the GCN is used to realize graph feature extraction. As a typical kind of non-Euclidean data, GCN has also been applied to many point cloud-based vision tasks, such as point cloud classification[27, 28], point cloud segmentation[29], point cloud data analysis[30], action recognition[31], etc. Moreover, it has also been applied to infer the perceptual quality of various multimedia data, e.g., traditional 2D images [32, 33], 360-degree images [34, 35], and meshes [36, 37]. Figure 1: We simulate the perceptual process of HVS to perform multi-view projection on the 3D point cloud and build the graph based on the projected images. The red node in graph convolution represents the central node of the current convolution process, and the red line represents the adjacency relationship. The central node will constantly exchange information with neighboring nodes to aggregate feature information from neighbors. Due to the strong capability of GCN in handling non-Euclidean data including 3D point cloud, this paper presents a novel GCN-based NR-PCQA method (GC-PCQA). One of the most critical issues is to effectively create a graph of the point cloud so that the GCN can be applied for feature learning. Since the goal of PCQA is to predict the quality of the test point cloud consistent with human perception, how to construct a highly perception-consistent graph of point clouds is the key to its success. It is known that the HVS reconstructs 3D objects in their mind based on multiple two-dimensional (2D) plane images observed from different viewpoints. In order to imitate the process of the HVS to perceive 3D objects, it is natural to perform multi-view projection on the point cloud to obtain a set of projected images with each corresponding to a specific viewpoint. Although these projected images are independent individuals, there is a certain extent of correlation between each other. Therefore, we regard all projected images as a set of non-Euclidean data and then establish a graph according to the dependencies between each individual projected image. Finally, GCN is applied to realize feature extraction from the constructed graph for quality prediction. The entire process is simply illustrated in Fig. 1. Experimental results demonstrate that our proposed GC-PCQA method outperforms state-of-the-art reference and non-reference PCQA methods on two public PCQA databases. Overall, the main contributions of this paper are as follows: 1. We perform multi-view projection on the point cloud to obtain a set of projected images based on which a highly perception-consistent graph is constructed to model the mutual dependencies of multi-view projected images. The graph nodes are defined with the projected images and connected by spatial relations between each other. 2. We perform GCN on the proposed graph to characterize the interactions between different projected images and aggregate the feature information of multi-view projected images for final quality prediction. The ablation study validates the effectiveness of the GCN architecture and the source code is available for public research usage. 3. We fuse the horizontally and vertically projected image features extracted by two GCNs that do not share weights to boost the performance. Experimental results show that the proposed GC-PCQA can predict subjective scores more accurately than the existing state-of-the-art PCQA metrics. The rest of this paper is organized as follows. In Section II, we introduce the related works. In Section III, we illustrate the proposed GC-PCQA with technical details. We conduct experiments and analyze the results in Section IV, and finally draw conclusions in Section V."
https://arxiv.org/html/2411.07688v1,Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG,"Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 \times 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image’s long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG’s core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient.","In the field of remote sensing (RS), ultra-high-resolution (UHR) images often cover vast areas, encompassing diverse landscapes and a wide range of geospatial features. For deep learning applications such as semantic segmentation, object detection, and change detection, processing these large-scale images directly poses significant challenges. The high spatial resolution results in massive image sizes (e.g. 100,000\times 100,000 pixels), making it difficult to directly train neural networks with such images due to the limitation in GPU memory. Additionally, the variability in scale, class distribution, and object sizes within these large images can lead to suboptimal performance if not handled properly. To address these, a common preprocessing step is to cut the original UHR images into smaller patches (e.g. 224\times 224 or 512\times 512) [1] [2] that can fit in regular deep learning workflows Multimodal Large Language Models (MLLMs, in this paper we specifically refer to Vision-Language Models using LLM as base model) have demonstrated remarkable potential in RS tasks, including image captioning [3], visual grounding [4], relation reasoning [5], object detection [6], and visual question answering (VQA) [7]. However, the input image resolutions for these Remote Sensing Multimodal Large Language Models (RSMLLMs) are often limited and relatively small compared with the original satellite image. For example, models like LLaVA 1.5 [8] and H2RSVLM [7] utilize image inputs of 336\times 336 pixels, while Geochat [3] and SkysenseGPT [5] process images at 504\times 504 pixels. Vision-language models (VLMs) specifically trained for RS, such as GeoRSCLIP [9] and RemoteCLIP [10], work with even smaller inputs, typically at 224\times 224 pixels. Figure 1: An example of a challenging question that requires analyzing small targets in a high-resolution image. Models such as GeoChat, LLaVA1.5, and V^{*} failed to answer. LLaVA1.5 with ImageRAG can answer correctly. In Figure 1, we illustrate how current MLLMs struggle to answer a challenging question that requires identifying small objects in a high-resolution (2086\times 2086) image. The model’s limitations in handling fine details and distinguishing small features become evident, leading to inaccurate responses when tasked with analyzing such intricate visual information (the model can answer correctly when the zoom-in image is provided). We identify four types of approaches for applying MLLMs to UHR RSI, each with its own set of limitations. The first approach involves resizing UHR images to a smaller size in order to be compatible with current MLLMs. However, this significantly reduces the visibility of small objects in the images, making them challenging to detect, even for humans. For instance, H2RSVLM [7] claims its difficulty in handling small objects, likely due to limitations in input image resolution of 336\times 336. The second approach divides UHR images into smaller patches that can be sequentially processed by MLLMs. While this allows for compatibility with existing model architectures, it results in the loss of global and relative information and relationships present in the original large-scale image, as only portions of the image are considered at a time. The third approach references techniques from general LLMs for managing long context, such as Positional Interpolation [11] and LongROPE [12]. Or adopting architecture from video MLLMs like LongVILA [13], which can extend the context window effectively. This approach could potentially enable the integration of entire UHR images while maintaining global information. However, it would necessitate retraining the models from scratch. The fourth approach employs guided visual search methods that focus on relevant patches, such as V^{*} [14], or hybrid architectures like LongLLaVA [15], which enable the processing of very large input images, and not neglect the small targets. Similar to the drawbacks of the third approach, this method also requires retraining the model and demands task-specific annotations, adding to the complexity and effort needed for implementation. Three crucial aspects for MLLMs to effectively handle UHR RSI are: (1) managing small targets, ensuring that the model can accurately aware and analyze fine details within images; (2) processing the UHR image in a way that integrates with MLLMs without significantly increasing the number of image tokens, which would lead to high computational costs; and (3) achieving these goals while minimizing the need for additional training or specialized annotation. To address these problems, we contribute the ImageRAG framework for RSMLLMs, which offers several key advantages. First, it retrieves and emphasizes relevant visual context from the UHR image based on the text query, allowing the MLLM to focus on important details, even tiny ones. Second, it integrates various external knowledge sources to guide the model, enhancing the understanding of the query and the UHR RSI. Lastly, ImageRAG is training-free, and requires no additional annotations, making it a practical solution for efficiently handling UHR RSI. ImageRAG is initially designed for RS, but it is suitable for general tasks and other specific domains, as long as building modules with corresponding data."
https://arxiv.org/html/2411.07686v1,Data-Driven Graph Switching for Cyber-Resilient Control in Microgrids,"Distributed microgrids are conventionally dependent on communication networks to achieve secondary control objectives. This dependence makes them vulnerable to stealth data integrity attacks (DIAs) where adversaries may perform manipulations via infected transmitters and repeaters to jeopardize stability. This paper presents a physics-guided, supervised Artificial Neural Network (ANN)-based framework that identifies communication-level cyberattacks in microgrids by analyzing whether incoming measurements will cause abnormal behavior of the secondary control layer. If abnormalities are detected, an iteration through possible spanning tree graph topologies that can be used to fulfill secondary control objectives is done. Then, a communication network topology that would not create secondary control abnormalities is identified and enforced for maximum stability. By altering the communication graph topology, the framework eliminates the secondary control layer’s dependence on inputs from compromised cyber devices helping it achieve resilience without instability. Several case studies are provided showcasing the framework’s robustness against False Data Injections and repeater-level Man-in-the-Middle attacks. To understand practical feasibility, robustness is also verified against larger microgrid sizes and in the presence of varying noise levels. Our findings indicate that performance can be affected when attempting scalability in the presence of noise. However, the framework operates robustly in low-noise settings.","Microgrids are cyber-physical systems with a hierarchical control framework that involves primary and secondary layers for voltage/frequency control and power-sharing regulations [1]. The microgrid secondary control layer is responsible for set-point tracking and relies on communication devices for nominal operations [2]. This makes the system vulnerable to stealth attacks compromising communication devices and manipulating data flow patterns [3]. Under the attacks’ influence, this layer computes erroneous control signals that propagate further to jeopardize nominal operation [4]. A necessary requirement for convergence of secondary control inputs is to ensure a spanning tree in the cyber (communication graph) topology [5]. If this spanning tree relies on compromised network devices, then it would feed untrustworthy inputs to the secondary controller, forcing it to compute erroneous control signals [6]. Hence, it is essential to ensure that the communication graph topology on which the microgrid secondary control layer is dependent is free from manipulations in the cyber layer [7, 8]. To achieve the objective, this paper presents a physics-guided Artificial Neural Network (ANN) framework that can identify the trustworthiness of the default communication topology by estimating abnormal secondary control outputs that it might create within the microgrid network. In this context, physics-guided means that the rationale behind using the ANN is rooted in the principles of (domain-specific) microgrid control dynamics. The microgrid local parameters are normally synchronous in the steady state as this is an essential objective of cooperative control action. However, [9] has already established that DIAs and other attack vectors like jamming lead to the disruption of cooperative synchronization [10]. This may be reflected as high error outputs from local secondary controllers. Hence, we seek to estimate the total sum of these outputs (via ANN-assisted regression) before the attack propagates to the secondary control layer. If the total sum is estimated to be unconventionally higher than the expected value (where the expected value is determined from microgrid steady-state behaviors during normal operation), a trigger is generated indicating the possible presence of a cyberattack. On the generation of a trigger, the proposed ANN model iterates through possible spanning tree graph topologies (each of which relies on a distinct set of network devices) to identify a topology that can achieve nominal functionality in a trustworthy manner. This topology is then enforced in the microgrid environment isolating and mitigating the cyberattack. We provide several case studies highlighting the proposed method’s resilience against False Data Injection (FDI) [11] and Man-in-the-Middle (MITM) attacks [12]. We also analyze the performance of the proposed framework when scaled up to larger microgrid sizes and in the presence of varying levels of noise."
https://arxiv.org/html/2411.07685v1,Fast Disentangled Slim Tensor Learning for Multi-view Clustering,"Tensor-based multi-view clustering has recently received significant attention due to its exceptional ability to explore cross-view high-order correlations. However, most existing methods still encounter some limitations. (1) Most of them explore the correlations among different affinity matrices, making them unscalable to large-scale data. (2) Although some methods address it by introducing bipartite graphs, they may result in sub-optimal solutions caused by an unstable anchor selection process. (3) They generally ignore the negative impact of latent semantic-unrelated information in each view. To tackle these issues, we propose a new approach termed fast Disentangled Slim Tensor Learning (DSTL) for multi-view clustering . Instead of focusing on the multi-view graph structures, DSTL directly explores the high-order correlations among multi-view latent semantic representations based on matrix factorization. To alleviate the negative influence of feature redundancy, inspired by robust PCA, DSTL disentangles the latent low-dimensional representation into a semantic-unrelated part and a semantic-related part for each view. Subsequently, two slim tensors are constructed with tensor-based regularization. To further enhance the quality of feature disentanglement, the semantic-related representations are aligned across views through a consensus alignment indicator. Our proposed model is computationally efficient and can be solved effectively. Extensive experiments demonstrate the superiority and efficiency of DSTL over state-of-the-art approaches. The code of DSTL is available at https://github.com/dengxu-nju/DSTL.","In many real-world applications, data can originate from different sources and feature collectors. For instance, we can utilize text posts, images, videos, user profiles, and social network connections to depict user behavior and interactions on social media platforms. To analyze sensor signals, we can decompose them into time and frequency domains. These are known as multi-view data, which often detail an object from various perspectives and provides a richer and more comprehensive understanding compared to single-view data. In recent years, the surge in multi-view data has sparked considerable interest in multi-view learning (MVL). MVL seeks to leverage the potential consistent and complementary information across multiple views to enhance generalization performance in downstream machine learning activities such as classification and clustering [1, 2, 3, 4, 5, 6, 7, 8]. Single-View Clustering (SVC) refers to the clustering of data comprising of a single view, while Multi-View Clustering (MVC) is to partition the multi-view data, leading to superior results than those obtained with SVC [9, 10, 11]. Most existing methods for MVC have demonstrated success, such as those employing matrix factorization [12, 13, 14], graph learning [15, 16, 17], and subspace learning [18, 19, 20]. Multi-view matrix factorization (MultiMF) effectively reduces the dimensionality of high-dimensional data and captures diverse underlying representations of multiple views. For example, Ma et al. [13] integrated multi-view linear discriminant analysis with MultiMF to leverage the intrinsic low-dimensional structure within the projection subspace. Liu et al. [14] unified MultiMF with partition generation to improve the clustering performance and efficiency for large-scale datasets. Graph-based methods strive to learn a unified graph from multiple views to delineate the pairwise similarities among data points. Huang et al. [16] proposed to formulate both the multi-view consistent graph and diverse graph in a unified framework. In [17], Wang et al. performed both anchor learning and graph construction to acquire an anchor graph to promote clustering efficiency. Another category is subspace-based MVC methods, which aim to infer latent representations from different views in a shared subspace. One representative example is the work of LMSC [18]. It integrated multiple views into a comprehensive latent representation subspace that encodes complementary information across different views. Unlike LMSC, Sun et al. [19] combined anchor learning and graph construction into a unified subspace to get a more discriminative clustering structure. Recently, several deep MVC approaches [21, 22, 23, 24] have also emerged to bolster clustering performance by harnessing the robust feature learning capabilities of deep neural networks. Although these MVC methods have achieved good results, they fail to fully explore the high-order correlations between each view. To solve this problem, tensor-based MVC methods have developed recently [25, 26, 27, 28, 29, 30], which usually stack the similarity graphs from all views into a three-order tensor to capture the cross-view correlations with tensor-based regularization. For instance, Zhang et al. [25] integrated self-expression based subspace representations from various views into a low-rank tensor, capturing the global structure and exploring correlations across multiple views. To obtain a more effective tensor for clustering, in addition to just considering the global structure and high-order correlations, Qin et al. made further strides by proposing two works: they investigated the local structures of similarity matrices from different views using the Markov chain[28], and explored pairwise correlations based on the reconstruction of a shared similarity matrix [29]. Additionally, To exploit the relationship between low-rank tensors and label indicator matrices, Fu et al. [30] unified low-rank tensor learning with spectral embedding into a framework. However, while these methods have shown success in capturing high-order correlations, their practical application is hindered by the stacking of large similarity graphs learned from multiple views into a tensor. This process incurs significant storage and computational complexity, making it unscalable for large datasets. Although some methods [31, 32] tried to address this problem by introducing bipartite graphs, they may result in sub-optimal solutions caused by an unstable anchor selection process. Furthermore, they often overlook the adverse effects of non-semantic information in each view, leading to the entanglement of latent semantic-unrelated and semantic-related features. As a consequence, the clustering performance may be compromised. To address the aforementioned challenges, we propose a novel approach called fast Disentangled Slim Tensor Learning (DSTL) for multi-view clustering. Specifically, Instead of concentrating on the multi-view graph structures, the DSTL approach directly leverages the matrix factorization technique to obtain low-dimensional features. Inspired by Robust Principal Component Analysis (RPCA) [33], we disentangle the features of each view to learn both semantic-unrelated and semantic-related representations. Subsequently, we construct two slim tensors that not only mitigate the adverse effects of latent semantic-unrelated information but also capture high-order consistency among multiple views. The semantic-unrelated slim tensor is assumed sparse with \ell_{1}-norm regularization, while the semantic-related slim tensor is assumed low-rank with tensor nuclear norm regularization. Additionally, we incorporate a consensus alignment indicator matrix to align semantic-related representations across views, guiding the disentanglement of latent features. In summary, we present the contributions of our model as follows: • We propose a novel fast MVC approach named DSTL, which combines latent multi-view features disentanglement for learning semantic representations and slim tensor learning for capturing cross-view correlations in a unified framework. • DSTL directly explores the high-order correlations among multi-view semantic-related representations via slim tensor learning. Latent feature disentanglement is adopted to mitigate the adverse effects of view-specific semantic-unrelated information. • The semantic-related representations are aligned across views to guide the disentanglement through a consensus alignment indicator. Experiments demonstrate the effectiveness and efficiency of DSTL compared to various state-of-the-art MVC methods. Figure 1: The overall framework of DSTL."
https://arxiv.org/html/2411.07654v1,Spike Talk in Power Electronic Grids,"Emerging distributed generation demands highly reliable and resilient coordinating control in microgrids. To improve on these aspects, spiking neural network is leveraged, as a grid-edge intelligence tool to establish a talkative infrastructure, Spike Talk, expediting coordination in next-generation microgrids without the need of communication at all. This paper unravels the physics behind Spike Talk from the perspective of its distributed infrastructure, which aims to address the Von Neumann Bottleneck. Relying on inferring information via power flows in tie lines, Spike Talk allows adaptive and flexible control and coordination itself, and features in synaptic plasticity facilitating online and local training functionality. Preliminary case studies are demonstrated with results, while more extensive validations are to be included as future scopes of work.","The energy consumption of data centers has become a major concern in modern society. As the distributed energy resources (DERs) are increasingly promoted, the carbon footprint is also becoming more critical in power grids where large amount of data are involved [1]. Meanwhile, distributed generation is escalating the demand for coordinating control in cyber-physical microgrids to ensure operational reliability. In turn, challenges persist thereupon, involving delays [2] and susceptibility to cyberattacks [3]. It is therefore of much value to study on a decentralized transition of the operation paradigm to address both challenges. Under this scenario, Talkative Power has been accordingly developed, aiming to co-transfer power and information along transmission lines [4]. System resilience is effectively improved, while additional energy consumption on the transmission line is inevitable. besides, as Talkative Power relies on request-respond information exchange protocol, its scalability is limited when multiple agents are involved in information exchange simultaneously. In contrast, we delve into the realm of a publish-subscribe protocol, where information is fetched locally as needed. Navigated by the biologically plausible neuron model [5, 6], spiking neural network (SNN) has emerged with great advantage in energy efficient computation due to its event-driven feature. Beyond the von-Neumann computing architecture activated by real numbers and perceptrons, SNN leverages a leaky-charge framework instead that are triggered by asynchronous spikes. Empowered by SNN, we formalize the Spike Talk tailored for microgrids, harnessing power flow dynamics to infer remote information locally. As spiking neurons and spiking neural networks have the features of synaptic plasticity and spike-timing-dependent plasticity (STDP), the neuromorphic infrastructure also shows potential in online learning and effectively reduces the data and energy requirements for training. Furthermore, the necessity for communication channels is eliminated, thus effectively addressing the resilience challenges in microgrids. This article thus delineates the infrastructure of Spike Talk and explains it from the perspective of its decentralized and online learning features. The remaining parts of this article is organized as follows: Section II introduces the inspiration of neuromorphic infrastructure for power grids from the von Neumann bottleneck. Section III elaborates on the online learning potential of Spike Talk by investigating the training principles. Section IV presents a case study, and Section V concludes the entire article. By discussing its inherent advantages, more promising real-world applications are implied, which should be the future scope of our work."
https://arxiv.org/html/2411.07650v1,"Understanding Audiovisual Deepfake Detection:
Techniques, Challenges, Human Factors
and Perceptual Insights","Deep Learning has been successfully applied in diverse fields, and its impact on deepfake detection is no exception. Deepfakes are fake yet realistic synthetic content that can be used deceitfully for political impersonation, phishing, slandering, or spreading misinformation. Despite extensive research on unimodal deepfake detection, identifying complex deepfakes through joint analysis of audio and visual streams remains relatively unexplored. To fill this gap, this survey first provides an overview of audiovisual deepfake generation techniques, applications, and their consequences, and then provides a comprehensive review of state-of-the-art methods that combine audio and visual modalities to enhance detection accuracy, summarizing and critically analyzing their strengths and limitations. Furthermore, we discuss existing open source datasets for a deeper understanding, which can contribute to the research community and provide necessary information to beginners who want to analyze deep learning-based audiovisual methods for video forensics. By bridging the gap between unimodal and multimodal approaches, this paper aims to improve the effectiveness of deepfake detection strategies and guide future research in cybersecurity and media integrity.","The proliferation of smart digital devices such as mobile phones, laptops, tablets, and other digital gadgets, coupled with the accessibility of social media platforms, has promoted the exponential growth of multimedia content (images, videos, and audio) on the internet. This growth is further fueled by technological advances [1], including various deep generative networks [2] [3]. However, this accessibility heightens the need for caution because it can lead to the prevalence of disinformation. Despite this, many people still stick to the trend of the antiquated phrase “seeing is believing” and share multimedia content without considering its authenticity or verifying its digital integrity. Deepfake technology, or sophisticated Artificial Intelligence (AI) models, enable deep learning (DL) tools to manipulate media (images, videos, and audio) to generate hyper-realistic fake content that deceives viewers. Deepfake is AI-generated media that has been deceptively altered by superimposing a source face in a video onto a target face, manipulating the speech in an audio clip, or both. The vast amount of data available online in the form of images, videos, and audio to train such models makes detecting such forgeries increasingly challenging. The impact of deepfakes is critical because we still trust photographic and audio recording evidence. The emergence of realistic and subtle production tools makes fake content incredibly believable and harder to distinguish from genuine content [4]. The rapid spread of harmful and uncontrolled content from fake media has serious imminent impacts and reduces trust in journalism and news providers [5] [6]. Deepfake media content can be exploited to fuel political or religious tensions between countries [7], spread misleading information or rumors between political parties [5] [8], deceive the public [5], engaging in revenge porn [8], defame celebrities [8], promote fraud and identity theft [9], and create political chaos or publicity in a campaign [10]. Generative Adversarial Networks (GAN) [2] and Variational Autoencoders (VAE) [3] are sophisticated DL models for generating counterfeit content. In GAN, the generator network and the discriminator network are the two main components, and these two networks are opposed to each other. The generator aims to generate plausible data, while the discriminator determines the real data from the fake data generated by the generator. Similarly, VAE is an unsupervised learning method consisting of encoder and decoder architectures. VAE is used to create high-quality, hyper-realistic fake content by merging and/or superimposing existing media (images or videos) onto source media for the purpose of deception. Currently, AI-synthesized videos are mainly divided into three different generation types [11] [12]. (1) Head puppetry/puppet master is a counterfeit video generation technique based on the target person animating like a puppet. (2) Face swap aims to generate a video of the target person by swapping the target person’s face with that of the source person while retaining the same facial expression as the target person. (3) Lip-sync is another deepfake video generation method whose main goal is to transform a person’s lips to be synchronized or consistent with the target audio. This technique tends to manipulate the lip region in such a way that the target of the attack appears to be saying things they never said in reality. In the past few years, immense progress in the field of automatic video editing and a great interest in face manipulation techniques have been noticed. Advances in manipulation tools and open-source codes allow even naive users to use deepfake technology like an expert in a few simple steps. This technological advancement has a wide range of positive applications in the fields of visual effects, photography, education, film industry, virtual reality, video games, cinema, and entertainment. However, it also poses significant challenges in terms of authenticity verification and prevention of malicious use. To overcome these challenges, researchers have made many attempts and proposed DL-based unimodal forgery detection methods [13, 14, 15, 16, 17]. Figure 1: Volume of research into audiovisual deepfakes between 2017 and 2023. The detection of visual manipulation in videos has long been the focus of researchers, while the identification of audio forgeries has often been overlooked. Recently, however, the trend of sound manipulation has grown rapidly along with visual alterations, leading to bimodal fabrication that enhances the authenticity of fake content and makes detection difficult [18, 19, 20, 21]. Fig. 1 highlights the research community’s growing interest and concern in audiovisual deepfakes. The number of publications on audiovisual deepfakes has increased significantly in recent years, demonstrating both beneficial progress and growing concerns. The integration of multimodality is proven to be beneficial in various research fields [22, 23, 24]. Consequently, researchers have used various DL techniques that exploit audio and visual features for video forgery detection. Nonetheless, existing media forensics research is lacking in investigations that analyze methods for generating and detecting video deepfakes using audio and visual modalities. Table I lists an overview of relevant studies. Our study was strongly motivated by the lack of attention paid to audiovisual deepfakes in surveys, highlighting the urgent need to focus research on audiovisual deepfakes, including their generation, how to mitigate their harmful effects, and a summary of existing audiovisual deepfake detection methods. TABLE I: Comparison of survey studies related to deepfake detection. Reference Year Contribution Verdoliva [25] 2020 A discussion of video deepfakes from a forensic perspective, with an emphasis on the limitations of current forensic detection methods. Mirsky et al. [26] 2021 An in-depth analysis of field-specific generation techniques and a brief discussion of detection methods. Yu et al. [27] 2021 A detailed analysis of forged video synthesis and detection techniques, with a focus on face manipulation. Rana et al. [28] 2022 A comprehensive review of deepfake detection methods proposed during 2018-2020. Nguyen et al. [29] 2022 A comprehensive overview of deepfake generation and detection techniques and a discussion of challenges and future research directions in the field. Masood et al. [30] 2023 An analysis of the generation and detection of audio and visual deepfakes and a discussion of datasets. Mubarak et al. [31] 2023 An analysis of audio, visual, and text-based deepfakes, with a focus on detection methods. Figure 2: Taxonomy of deepfakes. Generally speaking, as shown in Fig. 2, there are four types of deepfakes, namely text deepfakes, audio deepfakes, visual deepfakes, and audiovisual deepfakes. Audiovisual deepfakes are a combination of acoustic and visual manipulation that can enhance manipulated videos to make them look more believable, and have received a lot of attention in recent years. This study specifically provides an in-depth review of the latest audiovisual deep learning solutions to improve the detection of challenging video deepfakes. To the best of our knowledge, we are the first to perform a comprehensive analysis of existing DL-based methods that exploit audio and visual manipulations in videos for automatic deepfake detection. Our important contribution also includes a comprehensive discussion of publicly available datasets relevant to this task. The main contributions of our work are as follows: • We provide an unprecedented survey that systematically analyzes key detection and generation methods for audiovisual deepfakes, with a special emphasis on automatic video deepfake detection methods. • We highlight the challenges, limitations, and human perception in the field of audiovisual deepfake detection. Furthermore, we outline research directions for future developments in this field. • We summarize and present publicly available datasets that can be used to train multimodal/audiovisual deepfake detectors. The remainder of this paper is organized as follows. Section II introduces different types of deepfakes. Section III discusses video deepfake detection methods. Section IV classifies detection methods that exploit visual and acoustic streams. In Section V, we review publicly available datasets for audiovisual deepfake detection. Section VI presents performance metrics and evaluation. Section VII examines human perception of audiovisual deepfakes. Section VIII discusses several aspects of the deepfake challenge and potential research directions. Finally, Section IX concludes this survey."
https://arxiv.org/html/2411.07611v1,Multimodal Clinical Reasoning through Knowledge-augmented Rationale Generation,"Clinical rationales play a pivotal role in accurate disease diagnosis; however, many models predominantly use discriminative methods and overlook the importance of generating supportive rationales. Rationale distillation is a process that transfers knowledge from large language models (LLMs) to smaller language models (SLMs), thereby enhancing the latter’s ability to break down complex tasks. Despite its benefits, rationale distillation alone is inadequate for addressing domain knowledge limitations in tasks requiring specialized expertise, such as disease diagnosis. Effectively embedding domain knowledge in SLMs poses a significant challenge. While current LLMs are primarily geared toward processing textual data, multimodal LLMs that incorporate time series data, especially electronic health records (EHRs), are still evolving. To tackle these limitations, we introduce ClinRaGen, an SLM optimized for multimodal rationale generation in disease diagnosis. ClinRaGen incorporates a unique knowledge-augmented attention mechanism to merge domain knowledge with time series EHR data, utilizing a stepwise rationale distillation strategy to produce both textual and time series-based clinical rationales. Our evaluations show that ClinRaGen markedly improves the SLM’s capability to interpret multimodal EHR data and generate accurate clinical rationales, supporting more reliable disease diagnosis, advancing LLM applications in healthcare, and narrowing the performance divide between LLMs and SLMs.","The widespread adoption of electronic health records (EHRs) in healthcare has significantly advanced deep learning techniques. EHRs include a variety of data types, such as medical notes, laboratory (lab) test results, and chest X-ray (CXR) images. These datasets are extensively employed in various healthcare applications, including disease diagnosis, mortality prediction, and drug discovery Niu et al. (2024a); Xu et al. (2018); Laghuvarapu et al. (2024). Our research focuses on using multimodal EHRs, particularly medical notes and lab test results, for disease diagnosis. Usually, disease diagnosis is treated as a classification problem. However, with the development of pre-trained language models, generative methods have gained popularity. These methods excel in identifying clinical relationships between EHR inputs and diagnoses, often surpassing most classification models (Niu et al., 2024a). Despite their effectiveness, these generative approaches lack a clear and intuitive rationale for interpreting diagnosed results, which is essential for trustworthy healthcare. The increasing use of prompt learning (Li and Liang, 2021; Niu et al., 2024b), chain-of-thought (COT) Wei et al. (2022), and instruction tuning (Wei et al., 2021; Chung et al., 2024) has significantly advanced the capabilities of Large Language Models (LLMs) in natural language processing (NLP). These models excel across various tasks and support their outputs with rationales (Touvron et al., 2023; Achiam et al., 2023). In healthcare, advanced medical LLMs such as MedPaLM (Singhal et al., 2023), a 540B model trained on diverse medical Q&A datasets and MEDITRON (Chen et al., 2023), which utilizes LLaMA2 for medical reasoning, have shown notable performance improvements in domain-specific applications. Similarly, ALPACARE (Zhang et al., 2023), built on LLaMA2 and trained through instruction tuning with a specialized dataset created by GPT-4, demonstrates the potential of targeted training in enhancing the effectiveness of medical LLMs. However, tuning such large-scale LLMs for specific tasks often faces practical challenges due to their substantial computation demands. Moreover, EHRs are typical multimodal data; these models only improve medical Q&A accuracy on the single EHR modality. They do not significantly advance the generation of multimodal clinical rationales, especially when time series EHR analysis is included. To improve practical applications of LLMs in downstream tasks, a novel distillation method for small language models (SLMs) called “Distilling Step-by-Step” has been developed (Hsieh et al., 2023) and adapted for healthcare research (Kwon et al., 2024; Kang et al., 2024). Despite these innovations, diagnosing diseases requires extensive medical expertise. Without detailed pre-training on specialized datasets and domain knowledge support, SLMs often struggle to provide accurate diagnoses and dependable clinical rationales, even with advanced rationale distillation techniques (Kang et al., 2024). In response, KARD (Kang et al., 2024) has developed an additional knowledge retriever that extracts pertinent information from medical queries to support clinical rationale generation. However, relying on a comprehensive knowledge base can reduce the efficiency advantages of SLMs and also not directly enhance the SLM rationale generation abilities. Furthermore, this distillation approach still faces challenges in processing multimodal EHRs and producing multimodal rationale effectively. To address these research gaps, we introduce ClinRaGen, a multimodal EHR understanding and disease diagnosis model that simultaneously provides supportive multimodal clinical rationales enhanced by clinical domain knowledge augmentation. We utilize the advanced capabilities of LLMs to infuse concise domain-specific medical knowledge into SLMs. Instead of limiting ourselves to single modality-based rationale distillation from LLMs (Kang et al., 2024; Kwon et al., 2024), we introduce a sequential multimodal rationale distillation paradigm with our model ClinRaGen to understand multimodal EHRs (textual medical notes and time series lab test results). Specifically, to better utilize external domain knowledge and effectively interpret these two modalities, we developed a knowledge-augmented attention module, which encodes time series lab test results into understandable representations for SLMs. The multimodal inputs will directly enrich the SLMs’ capabilities for more accurate disease diagnosis and clinical rationale generation. Our comprehensive evaluation on two pubic medical datasets shows that SLMs can effectively interpret multimodal EHR data and generate accurate disease diagnosis and multimodal rationales with efficacy comparable to LLMs. Our main contributions are summarized as follows: • ClinRaGen represents a state-of-the-art multimodal EHR understanding framework designed to deliver accurate disease diagnoses through consistent multimodal clinical rationale by leveraging the external domain knowledge and the power of LLMs. • We introduce a novel knowledge-augmented attention mechanism to encode time series EHRs in a SLM-understandable manner and a sequential multimodal rationale distillation paradigm, effectively integrating multimodal EHRs and improving their representation in generated rationales. • Our evaluations on the MIMIC-III (Johnson et al., 2016) and MIMIC-IV (Johnson et al., 2023) datasets demonstrate that ClinRaGen effectively interprets multimodal EHR data and produces comparable clinical rationales with LLMs, bridging the gap between LLMs and SLMs in healthcare applications."
https://arxiv.org/html/2411.07602v1,Circuit Complexity Bounds for RoPE-based Transformer Architecture,"Characterizing the express power of the Transformer architecture is critical to understanding its capacity limits and scaling law. Recent works provide the circuit complexity bounds to Transformer-like architecture. On the other hand, Rotary Position Embedding (\mathsf{RoPE}) has emerged as a crucial technique in modern large language models, offering superior performance in capturing positional information compared to traditional position embeddings, which shows great potential in application prospects, particularly for the long context scenario. Empirical evidence also suggests that \mathsf{RoPE}-based Transformer architectures demonstrate greater generalization capabilities compared to conventional Transformer models. In this work, we establish a tighter circuit complexity bound for Transformers with \mathsf{RoPE} attention. Our key contribution is that we show that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \mathrm{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the arithmetic problem or the Boolean formula value problem. This result significantly demonstrates the fundamental limitation of the expressivity of the \mathsf{RoPE}-based Transformer architecture, although it achieves giant empirical success. Our theoretical framework not only establishes tighter complexity bounds but also may instruct further work on the \mathsf{RoPE}-based Transformer.","Recently, Large Language Models (LLMs), such as GPT-4 [1], Claude [3], Llama [55], and more recently, OpenAI’s o1 [60], have exhibited remarkable potential to revolutionize numerous facets of daily life, including conversational AI [40], AI agents [75, 21], search capabilities [60], and AI assistants [38, 23], among others. One of the most significant emergent capabilities of LLMs is their proficiency in handling long-context information, which is essential for effectively processing complex documents such as academic papers, official reports, and legal texts. LLMs also have demonstrated exceptional capabilities in tackling long-context tasks, such as zero-shot summarization [14, 79] and sustaining coherent, extended conversations [76, 56]. The o1 model from OpenAI [60] represents a major advancement in this field. By leveraging Chain-of-Thought (CoT) reasoning [74, 37] and incorporating Retrieval Augmented Generation (RAG) [48, 26], it showcases a level of expertise comparable to PhD-level problem solving, with both techniques heavily relying on extensive contextual understanding. Large Language Models (LLMs) are primarily built upon the Transformer architecture [69], which uses the self-attention mechanism as its core component. Given this foundational structure, an important question arises: what computational primitives can the components of the Transformer implement, and what problems can the entire system solve collectively? These questions are crucial for interpreting Transformers in a principled manner, understanding the potential limitations of their reasoning capabilities, and fostering trust in deployed Transformer-based systems. To address the aforementioned questions and to investigate the expressiveness of transformers, prior research has made significant strides. Recent studies, such as [58], have established two key results concerning both non-uniform and \mathsf{L}-uniform settings: first, any depth-d transformer with c\log n-precision can be simulated by a threshold circuit family with constant depth; second, such a transformer can also be simulated by a \mathsf{L}-uniform threshold circuit family of constant depth. Further advancing these findings, [57] demonstrate that \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} circuits are capable of simulating softmax-attention transformers. Building on this foundation, [16] refine these results by increasing the accuracy of approximation. They enhance the precision for softmax-attention transformers from O(\log n) to O(\operatorname{poly}(n)), confirming that these transformers fall within the \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} class. Additionally, they show that a softmax-attention transformer with an absolute error bound of 2^{-O(\operatorname{poly}(n))} is also contained within \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0}. On the other hand, first introduced by [61], Rotation Position Embedding (\mathsf{RoPE}) enhances Transformers by encoding both absolute and relative positional information through a rotation matrix, enabling greater sequence length flexibility, improved attention mechanism efficiency, and better performance on long-text tasks, exemplified by \mathsf{RoPE}-based language models that can summarize an entire book in a single pass. Due to these advantageous properties, \mathsf{RoPE} has been widely adopted in numerous empirical studies [20, 9, 12]. However, despite its considerable success, the underlying mechanisms of \mathsf{RoPE} remain largely unknown, posing an intriguing mystery in the field. A natural question arises: Does \mathsf{RoPE} enhance the expressiveness of the Transformer-based Large Language Model? This work aims to address this question from the perspective of circuit complexity, taking a step forward in theoretically understanding the underlying mechanisms of \mathsf{RoPE}. We present a rigorous theoretical investigation of \mathsf{RoPE}-based Transformers, establishing fundamental limits on their computational power. Our core approach involved a systematic examination of the circuit complexity for each component of the \mathsf{RoPE}-based architecture, from the basic trigonometric functions to the complete attention mechanism. Ultimately, we prove that these models can be simulated using uniform \mathsf{TC}^{0} circuits. Furthermore, we show that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, \mathsf{RoPE}-based Transformers with \operatorname{poly}(n)-precision, O(1) layers, and a hidden dimension d\leq O(n) are unable to solve either arithmetic formula evaluation or Boolean formula value problems. This finding is significant because it uncovers fundamental expressivity limitations of \mathsf{RoPE}-based architectures, even though they have shown empirical success in modern language models. Beyond Merrill and Sabharwal [58, 57] and Chiang [16], our contribution are summarized as follows: • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, constant-depth, \operatorname{poly}(n)-size can be simulated by a \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} circuit family (Theorem 4.8). • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the arithmetic formula evaluation problems (Theorem 5.8). • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the Boolean formula value problem (Theorem 5.9). Roadmap. In Section 2, we review the related work. In Section 3, we introduce some important computation concepts and Transformer definitions essential for the subsequent sections. In Section 4, we give the circuit complexity result of \mathsf{RoPE}-based Transformers. In Section 5, we give our hardness results. In Section 6, we summarizes our theoritical results."
https://arxiv.org/html/2411.07598v1,"Problem-Oriented Segmentation and Retrieval: 
Case Study on Tutoring Conversations","Many open-ended conversations (e.g., tutoring lessons or business meetings) revolve around pre-defined reference materials, like worksheets or meeting bullets. To provide a framework for studying such conversation structure, we introduce Problem-Oriented Segmentation & Retrieval (POSR)111Pronounced as “poser” (\textipa/\textprimstresspoz\textschwar/), a perplexing problem., the task of jointly breaking down conversations into segments and linking each segment to the relevant reference item. As a case study, we apply POSR to education where effectively structuring lessons around problems is critical yet difficult. We present LessonLink, the first dataset of real-world tutoring lessons, featuring 3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT® math problems. We define and evaluate several joint and independent approaches for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT), and large language models (LLMs) methods. Our results highlight that modeling POSR as one joint task is essential: POSR methods outperform independent segmentation and retrieval pipelines by up to +76\% on joint metrics and surpass traditional segmentation methods by up to +78% on segmentation metrics. We demonstrate POSR’s practical impact on downstream education applications, deriving new insights on the language and time use in real-world lesson structures.222Our code and dataset are open-sourced at https://github.com/rosewang2008/posr.","Across education, business, and science, many open-ended conversations like meetings or tutoring sessions are designed to address a set of pre-defined topics. As a prominent example, educators often shape their lessons around worksheet problems. Structuring lessons effectively is critical but challenging, as educators must allocate the right amount of time to different problems, while addressing different student learning needs (Haynes, 2010; Henderson, 1997; Panasuk and Todd, 2005). However, many novices or educators teaching large groups of students struggle with lesson structuring and often run out of time (Stradling and Saunders, 1993; Pozas et al., 2020; Deunk et al., 2018; Takaoglu, 2017; Hejji Alanazi, 2019). Figure 1: Problem-Oriented Segmentation and Retrieval (POSR) provides a framework for studying conversation structure around reference materials. For example, while conversations i,j discuss the same worksheet, POSR reveals that conversation i covers fewer problems than j but spends more time per problem. Providing evidence-based insights on lesson structuring is a key step towards addressing this challenge. These insights provide educators feedback on their teaching (Fishman et al., 2003; Kraft et al., 2018; Lomos et al., 2011; Desimone, 2009), tutoring platforms on training priorities (Hilliger et al., 2020; Gottipati and Shankararaman, 2018; Hilliger et al., 2022) and curriculum developers on material design (O’Donnell, 2008; Fullan and Pomfret, 1977). Unfortunately, obtaining insights on lesson structures at scale is challenging. The study of conversation structure around reference materials draws on concepts from two, typically distinct natural language processing (NLP) tasks: discourse segmentation to identify segments in the conversations and information retrieval (IR) to retrieve the relevant reference material for each segment. While each task has rich literature, studying them jointly reveals real-world challenges that existing works bypass. For example, discourse segmentation methods assume that conversations share the same structure (Ritter et al., 2010; Hearst and Plaunt, 1993; Chen and Yang, 2020), but education conversations have unique structures as teachers adapt their lessons to different needs. While prior IR work has studied supporting natural-language queries over conversations (Sanderson et al., 2010; Oard et al., 2004; Chelba et al., 2008), the reverse task of using open-ended conversation segments as queries for retrieving domain-specific reference materials has not received similar attention. To address these gaps, we make several key contributions. We define the Problem-Oriented Segmentation and Retrieval (POSR) task for jointly segmenting conversations and linking segments to relevant reference materials, such as worksheet problems (Figure 1). Unlike segmentation or retrieval alone, the joint POSR task reflects the realistic opportunities and challenges presented by knowing the potential reference topics (from the reference materials) for conversation segments. POSR provides a general framework for studying conversation structure around reference materials. As a case study, we apply POSR to the education setting. We contribute LessonLink, a novel dataset of real-world tutoring lessons featuring 3,500 segments, 116 SAT® math problems, and over 24,300 minutes of instruction. Our open-source dataset consists of real tutoring conversations paired with SAT® math worksheets, each conversation lasting about 1.5 hr long. Each conversation is segmented and each segment is linked with one of the 116 problems. To the best of our knowledge, this is the first dataset to include real-world conversations of unique structures linked with reference materials like worksheets. Evaluating POSR is challenging: Existing segmentation metrics do not measure time-weighed errors and existing metrics fail to reflect the subtle ways in which segmentation and retrieval errors interact. To address this, we contribute time-aware segmentation metrics adapted from standard line-based metrics (e.g., WindowDiff from Pevzner and Hearst (2002)) and introduce the Segmentation and Retrieval Score (SRS) to jointly measure segmentation and retrieval accuracy as the proportion of conversation where the retrieved item matches the ground truth. We define and evaluate a suite of segmentation, retrieval and POSR methods on LessonLink, including traditional segmentation methods like TextTiling (Hearst, 1997), popular IR methods like ColBERT (Khattab and Zaharia, 2020) and long-context large language models (LLMs) like Claude and GPT-4 Anthropic (2024); OpenAI (2024). Our results highlight the importance of POSR’s joint approach: POSR methods outperform independent segmentation and retrieval pipelines by up to +76\% on SRS metrics and traditional segmentation methods by up to +78% on segmentation metrics. However, several challenges remain. In domains with high privacy risks like education, companies are often unwilling to share data long-term due to privacy concerns. Moreover, while LLMs achieve strong POSR performance, their high API costs on long texts raise scalability concerns. Our findings motivate the need for more cost-effective, open-sourced methods that can deliver high accuracy on joint reasoning tasks like POSR. Finally, to further highlight the utility of POSR to real-world scenarios, we describe two novel applications of POSR to illustrate its potential for impacting evidence-based practices in education. First, through a linguistic analysis, we discover that tutors who spend more time on problems provide richer conceptual explanations. Tutors who spend less time provide procedural explanations. Second, POSR quantifies wide variability in how long tutors spend on the same problem. These examples point to opportunities for improving language and time-management practices."
https://arxiv.org/html/2411.07595v1,Entropy Controllable Direct Preference Optimization,"In the post-training of large language models (LLMs), Reinforcement Learning from Human Feedback (RLHF) is an effective approach to achieve generation aligned with human preferences. Direct Preference Optimization (DPO) allows for policy training with a simple binary cross-entropy loss without a reward model. The objective of DPO is regularized by reverse KL divergence that encourages mode-seeking fitting to the reference policy. Nonetheless, we indicate that minimizing reverse KL divergence could fail to capture a mode of the reference distribution, which may hurt the policy’s performance. Based on this observation, we propose a simple modification to DPO, H-DPO, which allows for control over the entropy of the resulting policy, enhancing the distribution’s sharpness and thereby enabling mode-seeking fitting more effectively. In our experiments, we show that H-DPO outperformed DPO across various tasks, demonstrating superior results in pass@k evaluations for mathematical tasks. Moreover, H-DPO is simple to implement, requiring only minor modifications to the loss calculation of DPO, which makes it highly practical and promising for wide-ranging applications in the training of LLMs.","Large language models (LLMs) have exhibited remarkable performance across various tasks (OpenAI et al., 2023; Dubey et al., 2024). However, large datasets often include data created for various purposes, and the models trained on these datasets are not always suitable for users’ specific needs. Additionally, some datasets include malicious text and code related to cyberattacks, posing risks of misuse by humans or the AI itself (Bender et al., 2021; Bai et al., 2022; Ji et al., 2023; Shevlane et al., 2023). Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022) is an effective approach to make an LLM follow human instructions and suppressing undesired outputs. In RLHF, a reward model is trained based on data evaluated according to human preferences. The LLM then learns to maximize rewards, aligning its outputs with human preferences. To prevent significant deviation from the original model, regularization using reverse KL divergence is added to the reward maximization process, and RL algorithms such as PPO (Schulman et al., 2017) are employed. However, RLHF has issues such as high computational costs, the reliance on a learned reward model, and the inherent instability and hyperparameter sensitivity of RL algorithms. To address these problems, Direct Policy Optimization (DPO) (Rafailov et al., 2023) has emerged and is now widely used. DPO proposes a loss function that directly optimizes the policy through a change of variables, eliminating the need for the reward model and allowing training with a simple binary cross-entropy loss. While more stable and lightweight than RLHF, DPO can optimize the same objective function as RLHF, which involves reward maximization and regularization with the reverse KL divergence. Other types of divergences have also been proposed to prevent deviation from the original model (Wang et al., 2024a), but reverse KL divergence, which enables mode-seeking estimation, is generally preferred for performance. (a) \min_{\pi}D_{\text{KL}}(\pi||\pi_{\text{ref}}) (b) \min_{\pi}D_{\alpha}(\pi||\pi_{\text{ref}}) Figure 1: For a Gaussian mixture model \pi_{\text{ref}}, \hat{\pi} that minimizes D_{\text{KL}} (left) and \hat{\pi} that minimizes D_{\alpha}=-\alpha H(\pi)+H(\pi,\pi_{\text{ref}}) with \alpha=0.6 (right). Using D_{\alpha} results in successful mode-seeking estimation. We point out that minimizing reverse KL divergence can cause the mode of the fitted distribution to fail to capture the mode of the target distribution. As shown in Figure 1, consider fitting a unimodal distribution to a multimodal distribution. We call the way of fitting a distribution mode-seeking when one of the modes of target distribution is captured by the fitted model as shown in the right side of Figure 1, and mode-covering when all the modes are covered as shown in the left side of Figure 1. In the case of mode-seeking, the fitted distribution discards other modes of the target distribution, resulting in smaller variance than the target distribution. However, reverse KL minimization can fail at mode-seeking fitting due to its nature of preserving variance, as illustrated in the left side of Figure 1. To enable variance reduction and encourage mode-seeking estimation, we generalize the loss function of DPO, named H-DPO, which allows for controlling the distribution’s entropy H(\pi) by modifying the regularization term. H-DPO can adjust the entropy of generations of the LLM during training using the hyperparameter \alpha in Equation 9 introduced later. By setting \alpha less than 1, it encourages the entropy to be reduced so that achieves mode-seeking fitting more successfully. The right side of Figure 1 demonstrates that our regularizer D_{\alpha}, a modification to the reverse KL, enables mode-seeking fitting even in cases where reverse KL fails, as shown on the left. Using our proposed loss with \alpha<1, the estimated policy distribution is expected to be sharper or more deterministic, which we consider a beneficial feature rather than a problem. Traditional LLMs use a softmax function with a temperature parameter to represent distributions over raw outputs, where the temperature is set to 1 during training. When LLMs are evaluated, a lower value such as 0.6 often performs better (Xu et al., 2022; OpenAI et al., 2023; Zhu et al., 2024). This post-training sharpening lacks guarantees of optimality for the objective function. In contrast, our proposed method trains the language model using an objective function aimed at sharpening the distribution, ensuring that this sharper distribution aligns with the objective function. Our main contribution is the alignment method H-DPO, which allows controlling entropy and encourages mode-seeking fitting more than DPO. The implementation of H-DPO is simple, requiring minimal modifications to DPO. Experiments included alignment based on Mistral-7B (Jiang et al., 2023) with the Zephyr framework (Tunstall et al., ; Tunstall et al., 2023). Compared to DPO, our proposed method allows for more diverse generations without losing performance, and shows superior accuracy and coverage across various tasks."
https://arxiv.org/html/2411.07589v1,Overhead-free User-side Recommender Systems,"Traditionally, recommendation algorithms have been designed for service developers. But recently, a new paradigm called user-side recommender systems has been proposed. User-side recommender systems are built and used by end users, in sharp contrast to traditional provider-side recommender systems. Even if the official recommender system offered by the provider is not fair, end users can create and enjoy their own user-side recommender systems by themselves. Although the concept of user-side recommender systems is attractive, the problem is they require tremendous communication costs between the user and the official system. Even the most efficient user-side recommender systems require about 5\times more costs than provider-side recommender systems. Such high costs hinder the adoption of user-side recommender systems. In this paper, we propose overhead-free user-side recommender systems, RecCycle, which realizes user-side recommender systems without any communication overhead. The main idea of RecCycle is to recycle past recommendation results offered by the provider’s recommender systems. The ingredients of RecCycle can be retrieved “for free,” and it greatly reduces the cost of user-side recommendations. In the experiments, we confirm that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly.","Recommender systems have been used in many web services (Linden et al., 2003; Geyik et al., 2019). It was estimated that 35 % of purchases on Amazon and 75 % of watches on Netflix came from recommender systems (MacKenzie et al., 2013). Recommender systems are indispensable both for businesses and users. Although traditional recommender systems aim only at conversion, many fine-grained demands for recommender systems have emerged. Users may want to receive fair recommendations (Kamishima et al., 2012a; Biega et al., 2018; Milano et al., 2020) or serendipitous recommendations (Chen et al., 2021; Anderson et al., 2020; Steck, 2018; Mladenov et al., 2020; Zheng et al., 2018), or users may want recommender systems to be transparent (Sinha and Swearingen, 2002; Balog et al., 2019) and steerable (Green et al., 2009; Balog et al., 2019). For example, on LinkedIn, recruiters may want to receive account recommendations that are fair in terms of gender and race to avoid (implicit) discrimination. A citizen who gathers information for election may want to receive both Republican and Democrat news equitably to avoid filter bubbles (Pariser, 2011). Cinema enthusiasts may want to receive recommendations that involve minor movies instead of popular movies that enthusiasts already know. However, there are too many kinds of demands, and the service provider cannot cope with all of them. Besides, service provider may not implement such functionalities on purpuse. For example, some service providers may intentionally choose to increase short-term conversions instead of caring the fairness of the platform. If the service provider does not implement fair recommender systems, users are forced to use unfair ones or quit the service. It has been considered that users have little ability to change the recommendations. In most cases, the only option available to the user is to wait until the service implements the functionality. Green et al. (2009) also pointed out that “If users are unsatisfied with the recommendations generated by a particular system, often their only way to change how recommendations are generated in the future is to provide thumbs-up or thumbs-down ratings to the system.” User-side recommender systems (Sato, 2022b) offer a proactive solution to this problem. Users can build their own (i.e., private, personal, or user-side) recommender systems to ensure recommendations are made fairly and transparently. Since the system is built by the user, it can be customized to meet specific criteria they want and add the functionalities they want. User-side recommender systems realize ultimate personalization. Table 1. Properties of user-side recommender systems. The definitions of these properties are shown in Section 4.4. Postprocessing (PP) applies postprocessing directly to the official recommender system, which is not sound when the list does not contain some sensitive groups (See also Section 4.1). PP PrivateRank (Sato, 2022b) PrivateWalk (Sato, 2022b) ETP (Sato, 2022d) Consul (Sato, 2022d) RecCycle (ours) Consistent ✓ ✓ ✗ ✓ ✓ ✓ Sound ✗ ✓ ✓ ✓ ✓ ✓ Local ✓ ✗ ✓ ✗ ✓ ✓ Overhead-free ✓ ✗ ✗ ✗ ✗ ✓ The concept of user-side recommender systems looks similar to steerable (Green et al., 2009) (or scrutable (Balog et al., 2019)) recommender systems at first glance. Steerable recommender systems also allow users to control the recommendation results. However, the key difference is that steerable systems are implemented by the service provider, while user-side recommender systems are built by the users themselves. What to steel is chosen by the service provider in traditional steerable recommender systems. If the recommender system in use is not steerable in the way they want, users cannot enjoy steerability and must wait for the service provider to implement it. By contrast, user-side recommender systems allow users to make the system steerable, even if the service provider implemented only a standard non-steerable system. Although user-side recommender systems are attractive, building them is challenging. End users do not have access to the data stored in the service’s database, unlike the developers employed by the service provider. Most modern recommender systems rely on user log data and/or item features to make recommendations. At first glance, it seems impossible to build an effective recommender system without such data. Sato (2022b) addressed this problem by using the official recommender systems provided by the target web service. Although the official recommender systems are black-box and possibly unfair, Sato’s methods turn them into fair and transparent ones on the user’s side by combining multiple outputs. However, existing user-side recommender systems issue multiple queries to the official (possibly unfair) recommender system to build a single (fair) recommendation list. In other words, these methods trade communication costs with fairness. The drawback of this approach is the communication cost. Even the most efficient user-side recommender systems, Consul, require 5 queries to build a recommendation list (Sato, 2022d). This means that Consul loads the service 5 times more. Such a high communication cost causes problems. First, the service provider may disfavor and prohibit such users’ activities to mitigate the load on the service. Second, end users cannot afford to pay the high API cost. Third, such systems are not suitable for real-time applications due to the response time of the multiple queries. We advocate that the communication cost between the end user and the service is crucial for effective user-side recommender systems. An ideal user-side system works as if it were an official system. The recommendation list should be shown to the user at the same time as the official system. However, existing user-side recommender systems require additional queries and thus require more loading time than the official system, which leads to a poor user experience. We propose overhead-free user-side recommender systems, RecCycle (recommendation + recycle), to address this problem. The main idea of RecCycle is to recycle past recommendation results presented by the provider’s recommender systems when the user uses the system as usual. These recommendation results used to be discarded once shown on the page. Sometimes, these recommendations are just shown on the page and do not catch the attention of the user due to the position and/or timing of the presentation. RecCycle “recycles” these information to create new recommendations on the user’s side. These information can be used “for free”, i.e., without any additional communication cost. All of the computation for RecCycle is done locally. RecCycle is so communication efficient that it can realize real-time user-side recommendations, and the user can enjoy the recommendations as if they were shown by the official system. RecCycle can be combined with existing user-side recommender systems. We will elaborate on the premise of RecCycle in the following sections. As a special case, we show that RecCycle can be combined with Consul (Sato, 2022d), which leads to consistent, sound, local, and overhead-free user-side recommender systems (Table 1). In the experiments, we confirm that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly. The contributions of this study are as follows: • We propose overhead-free user-side recommender systems, RecCycle, for the first time. • We show that RecCycle is consistent, sound, and local, as well as overhead-free. • We empirically validate that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly. • We deploy RecCycle in a real-world X (Twitter) environment and confirm that users can realize their own recommender system with specified functionalities they call for using RecCycle."
https://arxiv.org/html/2411.07585v1,Reinforcement Learning Framework for Quantitative Trading,"The inherent volatility and dynamic fluctuations within the financial stock market underscore the necessity for investors to employ a comprehensive and reliable approach that integrates risk management strategies, market trends, and the movement trends of individual securities. By evaluating specific data, investors can make more informed decisions. However, the current body of literature lacks substantial evidence supporting the practical efficacy of reinforcement learning (RL) agents, as many models have only demonstrated success in back testing using historical data. This highlights the urgent need for a more advanced methodology capable of addressing these challenges. There is a significant disconnect in the effective utilization of financial indicators to better understand the potential market trends of individual securities. The disclosure of successful trading strategies is often restricted within financial markets, resulting in a scarcity of widely documented and published strategies leveraging RL. Furthermore, current research frequently overlooks the identification of financial indicators correlated with various market trends and their potential advantages.This research endeavors to address these complexities by enhancing the ability of RL agents to effectively differentiate between positive and negative buy/sell actions using financial indicators. While we do not address all concerns, this paper provides deeper insights and commentary on the utilization of technical indicators and their benefits within reinforcement learning. This work establishes a foundational framework for further exploration and investigation of more complex scenarios.","The primary goal for investors in financial markets has always been the same, minimize trading risk and maximize profits, in the form of returns. Achieving such an objective involves using a systematic approach to predict the price of a security and the type of trends that are to follow. This is a complex and dynamic task, which makes it hard to understand which factors to include and which ones to exclude when making a decision. The research conducted in this sub-field has been primarily around designing automated trading systems which take over the responsibility of fundamental investor while also providing greater liquidity into the financial markets. Both supervised and unsupervised learning techniques have been used in the past, however, they have provided rather unfavorable results. Reinforcement Learning (RL) has gained attention in the last several years in this particular application, potentially offering novel solutions that allow for a more accurate price forecast, thereby allowing the trading agent to successfully interact with its environment to make optimal decisions. Additionally, the dynamic nature of financial data aligns well with the Markov Decision Process (MDP), which serves as the basis in addressing reinforcement learning challenges. The Markov Decision Process helps to evaluate which actions the agent should take considering the observation state and the environment that the agent is interacting with. The Markov Decision Process states that the future state will depend on the current state and action, it assumes all necessary information is captured in the current state. However, this can be problematic in the sense that time-series data is never static. That is to say the Markov Decision Process does not capture all relevant information from a historical perspective. In particular, ordinary stock market data consists of open, close, high, low, and volume per security, which is commonly presented in a sequential format over various time intervals. Given the nature of time-series data and its sensitivity to subtle changes, the introduction of technical indicators was proposed to help with creating more robust trading strategies. Furthermore, this helps to identify what characteristics and patterns bullish and bearish scenarios may entail. Today, the challenge remains in effectively utilizing these indicators to reliably forecast future price movements. The presence of false signals adds another layer of complexity into the mix. To address this, advanced techniques such as dimensionality reduction, feature selection, and extraction have been used through Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM), and more. In the experiments section, the models utilized 20 technical indicators as inputs. A more detailed explanation of many of these indicators can be found in the Technical Analysis book by John J. Murphy (Murphy, 1999)."
https://arxiv.org/html/2411.07574v1,Disentangling Tabular Data towards Better One-Class Anomaly Detection,"Tabular anomaly detection under the one-class classification setting poses a significant challenge, as it involves accurately conceptualizing “normal” derived exclusively from a single category to discern anomalies from normal data variations. Capturing the intrinsic correlation among attributes within normal samples presents one promising method for learning the concept. To do so, the most recent effort relies on a learnable mask strategy with a reconstruction task. However, this wisdom may suffer from the risk of producing uniform masks, i.e., essentially nothing is masked, leading to less effective correlation learning. To address this issue, we presume that attributes related to others in normal samples can be divided into two non-overlapping and correlated subsets, defined as CorrSets, to capture the intrinsic correlation effectively. Accordingly, we introduce an innovative method that disentangles CorrSets from normal tabular data. To our knowledge, this is a pioneering effort to apply the concept of disentanglement for one-class anomaly detection on tabular data. Extensive experiments on 20 tabular datasets show that our method substantially outperforms the state-of-the-art methods and leads to an average performance improvement of 6.1% on AUC-PR and 2.1% on AUC-ROC.","Tabular anomaly detection under the one-class classification setting presumes the availability of only one-class data, i.e., the normal class samples for training, while the goal during the test is to discern anomalies (Chandola, Banerjee, and Kumar 2009; Ruff et al. 2021). In practical scenarios, such as financial fraud detection, cyber intrusion detection, and medical diagnosis (Hilal, Gadsden, and Yawney 2022; Malaiya et al. 2019; Chen and Konukoglu 2018), this method tries to detect whether new data points conform to the pattern of observed normal samples, thereby identifying them as either normal or abnormal. Given that only normal instances are available during the training, the inherent challenge lies in extracting the invariant feature of normal data. An instance observed with patterns deviating from these characteristics is detected as an anomaly. However, the lack of prior knowledge on structures in tabular data poses a significant challenge for learning such knowledge (Shenkar and Wolf 2022). Figure 1: Visualization of MCM (Yin et al. 2024)’s fifteen soft masks and our two attention maps on the Thyroid dataset, which comprises six attributes per data point. The presented masks and attention maps are averages derived from all normal training samples. In MCM’s masks, darker colors indicate greater masking ratios while a higher attention weight in our attention maps. One reasonable way to mitigate this challenge is to focus on capturing the intrinsic correlation among attributes of normal samples. Once one or more attributes of a sample deviate anomalously, it is an anomaly, and the correlation among attributes becomes different from that exhibited in normal samples. The most recent MCM (Yin et al. 2024) proposes a learnable mask generator module with a diversity loss to create diverse soft masks (i.e., mask values range from 0 to 1 for input data). A reconstruction task is then designed to restore the original data from its masked variants. During inference, anomalous attributes will disrupt the correlations observed in the normal class, leading to a failure to reconstruct the original data from masked instances, thereby allowing anomalies to be detected. Despite its efficacy, the diversity loss may generate masks with nearly uniform values, inducing trivial solutions of the learned intrinsic correlations. Empirical evidence in Figure 1(a) suggests that the values of all soft masks are around 0.5. In this case, each attribute is scaled with the same value, indicating essentially no attribute is masked. Thus, the model learns the objective of reconstructing the original data from complete features instead of partial attributes, resulting in less effective extraction of the desired correlation. In this paper, we presume that attributes related to others in normal tabular data can be split into two111It can be multiple subsets. However, our empirical results indicate using two subsets is the best. non-overlapping and correlated subsets, termed as CorrSets, to ensure the distillation of the intrinsic correlation among attributes from normal instances. Building on this premise, this paper validates that disentangling those CorrSets from normal samples for reconstruction promotes the learning of normal samples’ inside correlations. Specifically, the sufficiently disentangled CorrSets are used to restore the whole original data individually, in which processes of the correlation inside normal data would be well captured by the model. Compared to MCM (Yin et al. 2024), which may generate ineffective masks, this approach guarantees that the model reconstructs the original data with partial attributes, thus strengthening its ability to extract the correlation within normal samples and yielding improvements in anomaly detection. Motivated by the aforementioned assumption, we propose a novel paradigm named Disent-AD, which efficiently disentangles CorrSets from normal tabular data and adequately captures the intrinsic correlation. For practical implementation, a two-head self-attention module is utilized to implicitly extract two distinct subsets in latent space. The attention maps depict the attributes of interest to the network. By learning two independent attention maps, the two attention heads focus on different regions of tabular data, enabling the implicit extraction of distinct attribute subsets. As shown in Figure 1(b), darker regions in attention maps indicate greater weights. Evidently, the attention module successfully concentrates on two non-overlapping attribute subsets. To ensure that disentangled subsets are correlated, a reconstruction task is performed to restore the original data by utilizing subset features extracted from either of the two attention heads. We train the attention module with the reconstruction task in an end-to-end fashion. During testing, samples with high reconstruction errors are detected as anomalies. Our contributions can be summarized as follows: • To our knowledge, this is one pioneering work that leverages the concept of disentanglement to enhance the efficiency of tabular one-class anomaly detection. • We propose a novel paradigm that learns the intrinsic correlation by disentangling two distinct and correlated attribute subsets from normal tabular data. • Extensive experiments conducted on 20 tabular datasets demonstrate the superiority of our method to state-of-the-art methods, with an average improvement of 6.1% and 2.1% in the AUC-PR and AUC-ROC, respectively."
https://arxiv.org/html/2411.07560v1,EUR/USD Exchange Rate Forecasting incorporating Text Mining Based on Pre-trained Language Models and Deep Learning Methods,"This study introduces a novel approach for EUR/USD exchange rate forecasting that integrates deep learning, textual analysis, and particle swarm optimization (PSO). By incorporating online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior performance compared to traditional econometric and machine learning models. The research employs advanced text mining techniques, including sentiment analysis using the RoBERTa-Large model and topic modeling with LDA. Empirical findings underscore the significant advantage of incorporating textual data, with the PSO-LSTM model outperforming benchmark models such as SVM, SVR, ARIMA, and GARCH. Ablation experiments reveal the contribution of each textual data category to the overall forecasting performance. The study highlights the transformative potential of artificial intelligence in finance and paves the way for future research in real-time forecasting and the integration of alternative data sources.","In the intricate tapestry of the global financial market, the exchange rate between the Euro and the US Dollar (EUR/USD) stands as a pivotal thread, weaving together the economic narratives of two powerhouse currencies. As a barometer of the economic relations between the Eurozone and the United States, the EUR/USD exchange rate holds profound implications for international trade, cross-border investments, and the overall health of the global economy. Accurately forecasting this exchange rate has become a holy grail for financial market participants, as it can provide a strategic edge in navigating the complex and ever-shifting currents of the foreign exchange market [1, 2]. Recent advancements in the field of EUR/USD exchange rate forecasting have been propelled by the advent of machine learning and the proliferation of big data. State-of-the-art methodologies have harnessed the power of deep learning architectures, such as Long Short-Term Memory (LSTM) networks, to capture the non-linear and temporal dependencies inherent in financial time series data [3, 4]. These models have shown promising results in predicting exchange rate movements by learning from vast amounts of historical data and uncovering hidden patterns that traditional econometric models might overlook [5]. However, despite the significant strides made by these cutting-edge approaches, there remain notable limitations. One critical drawback is the reliance on structured, quantitative data, such as historical prices and economic indicators [6]. While these data points provide valuable insights, they fail to fully capture the rich tapestry of qualitative information that shapes market sentiment and drives currency fluctuations. News articles, financial reports, and social media discussions often contain nuanced and real-time insights that can significantly influence exchange rate dynamics. Neglecting these qualitative data sources can lead to an incomplete understanding of the market and suboptimal forecasting performance. To address this challenge, we propose a novel approach that seamlessly integrates both quantitative and qualitative data to enhance the accuracy and robustness of EUR/USD exchange rate forecasting [7, 8]. Our methodology leverages the power of LLMs, specifically GPT-4, to process and extract relevant information from vast amounts of textual data. By employing advanced techniques such as prompt engineering and sentiment analysis, we enrich the predictive power of LSTM networks, enabling them to consider both historical price patterns and real-time market sentiments [9, 10]. Furthermore, we incorporate PPSO to fine-tune the hyperparameters of our model, ensuring optimal performance and adaptability to changing market conditions [11]. This research introduces several innovative contributions to the field of financial forecasting, enhancing existing methodologies and introducing novel approaches: • We demonstrate how to effectively integrate state-of-the-art LLMs for preprocessing and annotating financial data, significantly improving the quality and relevance of the dataset for forecasting purposes. This involves utilizing LLMs and prompt engineering techniques to filter noise from high-noise news sources, ensuring cleaner and more accurate data for analysis. • Our study showcases the combined use of twitter-RoBERTa-Large-topic-sentiment-latest and RoBERTa-Large models for extracting sentiment and identifying hidden relationships within textual data, thereby deepening and expanding the analytical scope. • We introduce an innovative approach to EUR/USD exchange rate forecasting by employing a PSO-LSTM model, a concept that is relatively novel in this domain. This method signifies a shift from traditional econometric models to a comprehensive framework that integrates qualitative and quantitative data for improved forecasting accuracy. The remainder of this paper is structured as follows: Section 2 provides a comprehensive literature review, positioning our research within the existing body of knowledge and highlighting the gaps we aim to address. Section 3 delves into the methodology and system design of our approach, including a detailed explanation of prompt engineering, LLMs, transformer architecture, and the PSO-LSTM model. Section 4 presents our empirical results, showcasing the performance of our model in comparison to baseline methods. Section 5 discusses the implications of our findings and conducts ablation studies to validate the contribution of each component of our approach. Finally, Section 6 concludes the paper, summarizing our key contributions, limitations, and outlining future research directions."
https://arxiv.org/html/2411.07559v1,: A memory-efficient gradient-based jailbreaking method for black box Multi-modal Large Language Models,"Jailbreaking methods, which induce Multi-modal Large Language Models (MLLMs) to output harmful responses, raise significant safety concerns. Among these methods, gradient-based approaches, which use gradients to generate malicious prompts, have been widely studied due to their high success rates in white-box settings, where full access to the model is available. However, these methods have notable limitations: they require white-box access, which is not always feasible, and involve high memory usage. To address scenarios where white-box access is unavailable, attackers often resort to transfer attacks. In transfer attacks, malicious inputs generated using white-box models are applied to black-box models, but this typically results in reduced attack performance. To overcome these challenges, we propose \ours, a method that bypasses the need for white-box access by leveraging zeroth-order optimization. We propose patch coordinate descent to efficiently generate malicious image inputs to directly attack black-box MLLMs, which significantly reduces memory usage further. Through extensive experiments, \oursachieves a high attack success rate across various models, surpassing previous transfer-based methods and performing comparably with existing white-box jailbreak techniques. Notably, \oursachieves a 95% attack success rate on MiniGPT-4 with the Harmful Behaviors Multi-modal Dataset on a black-box setting, demonstrating its effectiveness. Additionally, we show that \ourscan directly attack commercial MLLMs such as GPT-4o. Codes are provided in the supplement. Warning: This paper contains examples of harmful language and images, and reader discretion is recommended.","Recently, the safety alignment of Large Language Models (LLMs) (Ji et al., 2024) has drawn increasing attention because of the safety concern raised by the potentially harmful response (Bengio et al., 2024; Zhou et al., 2024). And this kind of safety alignment succeeds in the first place, where the user will not get any useful response when directly asking questions related to safety or privacy. However, recent works have proven that it is possible to “jailbreak” the safety alignment so that LLMs may answer the question that they should not by carefully designing prompts (Carlini et al., 2024; Wei et al., 2024). Later on, using gradient methods and other automatic technologies to find the powerful jailbreak prompts has been proposed so that the attack rate becomes much higher compared with hand-craft prompts (Zou et al., 2023; Liu et al., 2023b), which further demonstrates the vulnerability when obtaining the white-box models. With the success of LLMs (Achiam et al., 2023; Touvron et al., 2023; Chiang et al., 2023), Multi-modal Large Language Models (MLLMs), which handle both text and image inputs, have gained popularity (Liu et al., 2024b; Zhu et al., 2023; Liu et al., 2024a). Despite their capabilities in tasks such as image descriptions and visual question answering , MLLMs have been shown to be even more vulnerable to jailbreak attacks due to the additional modality (Qi et al., 2024; Sun et al., 2024; Liu et al., 2024c; Zhao et al., 2024). For example, Liu et al. (2023a) demonstrated that images containing specific text can assist in jailbreaking MLLMs. In white-box settings, where full access to model parameters is available, methods like generating malicious image inputs (Niu et al., 2024) or combining both text and image prompts (Shayegani et al., 2023) by optimization have proven effective in bypassing safety mechanisms. Similar to LLM jailbreaking, the most effective methods in MLLMs rely on calculating gradients to find inputs that induce harmful outputs. Figure 1: Comparison between white-box jailbreak, transfer jailbreak attack, and direct black-box jailbreak. Both white-box jailbreak and transfer jailbreak generate malicious inputs using white-box models while direct black-box attacks do not. In this paper, we focus on direct black-box jailbreak and prove our method can surpass transfer attacks and be comparable with white-box attacks. While gradient-based methods for white-box models have shown strong performance, the challenge of attacking black-box models remains underexplored. Black-box models, such as commercial MLLMs like GPT-4o (OpenAI, 2024), do not provide access to their internal parameters, making gradient-based attacks impossible. Most existing jailbreak methods for black-box models rely on transfer attacks, where malicious inputs generated on white-box models are used to indirectly attack black-box models (Zou et al., 2023; Niu et al., 2024; Dong et al., 2023). However, these transfer attacks often suffer from a significant reduction in success rate compared to direct attacks on white-box models (Niu et al., 2024). In this paper, instead of transferring the malicious prompts from white-box models, we propose \ours, a method that directly generates malicious image inputs for jailbreaking black-box MLLMs. \oursleverages zeroth-order optimization, which estimates gradients without accessing model parameters, to find malicious prompts capable of bypassing safety measures. One challenge with zeroth-order optimization is its susceptibility to high estimation errors in high-dimensional inputs. To mitigate this, \oursoptimizes only a specific part of the image, reducing the dimensionality of the problem and thereby minimizing estimation errors. Furthermore, \oursdoes not rely on backpropagation, resulting in significantly lower memory usage. Through extensive experiments, we show that \ourscan achieve a high attack success rate within reasonable queries as well as decrease memory usage when generating malicious prompts. Overall, we provide the comparison between different types of jailbreak methods in Fig. 1 and summarize our contribution as follows: 1. We propose \ours, which utilizes zeroth-order optimization technology to generate malicious images. To the best of our knowledge, \oursis the first method that aims at jailbreaking black-box MLLMs directly. 2. \ours reduces the memory usage and query complexity by only optimizing specific parts of the image, minimizing the impact of gradient noise. In detail, \oursallows us to attack 13B models in a single 4090 without any quantization. 3. We perform extensive experiments demonstrating that \oursconsistently achieves a high success rate across various MLLMs. In all black-box scenarios, \ourssurpasses transfer-based attack methods and performs on par with white-box approaches. For instance, \oursattains success rates of 98.2% on MiniGPT-4 using the MM-SafetyBench-T dataset and 95% with the Harmful Behaviors Multi-modal dataset. Besides, we use a showcase to demonstrate that it is possible for \oursto directly attack commercial MLLMs such as GPT-4o."
https://arxiv.org/html/2411.07546v1,"Contrastive Language Prompting to Ease 
False Positives in Medical Anomaly Detection","A pre-trained visual-language model, contrastive language-image pre-training (CLIP), successfully accomplishes various downstream tasks with text prompts, such as finding images or localizing regions within the image. Despite CLIP’s strong multi-modal data capabilities, it remains limited in specialized environments, such as medical applications. For this purpose, many CLIP variants—i.e., BioMedCLIP, and MedCLIP-SAMv2—have emerged, but false positives related to normal regions persist. Thus, we aim to present a simple yet important goal of reducing false positives in medical anomaly detection. We introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both positive and negative text prompts. This straightforward approach identifies potential lesion regions by visual attention to the positive prompts in the given image. To reduce false positives, we attenuate attention on normal regions using negative prompts. Extensive experiments with the BMAD dataset, including six biomedical benchmarks, demonstrate that CLAP method enhances anomaly detection performance. Our future plans include developing an automated fine prompting method for more practical usage.","Input Ground-truth A_{\textit{positive}} A_{\textit{negative}} A_{\textit{CLAP}} (Ours) Normal Brain MRI False positives True negatives Chest X-ray False positives True negatives Abnormal Brain MRI False positives True negatives Chest X-ray False positives True negatives Fig. 1: Generated attention maps by leveraging BiomedCLIP [1]. A_{\textit{positive}} and A_{\textit{negative}} are the attention maps obtained using positive or negative prompts only. A_{\textit{CLAP}} shows results of our proposal, dubbed Contrastive LAnguage Prompting (CLAP). CLAP leverages both positive and negative prompts. The negative prompts are used to attenuate false positive attention of normal regions. Fig. 2: Schematic diagram of our method. Existing positive prompt methods only utilize positive prompts. In this situation, the false positive attention issue remains. In comparison, our method CLAP successfully suppresses false positives by additionally exploiting negative prompts, shown in (a). After getting the attention map of CLAP, we employ the existing UAD model EAR [2]. We only replace the saliency map for mosaic obfuscation with an attention map from CLAP, shown in (b). Recent advancements in multi-modal models, particularly visual-language models (VLMs), have revolutionized various downstream tasks, such as image retrieval, captioning, and object localization. Among these, Contrastive Language–Image Pre-training (CLIP) [3] has demonstrated remarkable performance by leveraging natural language prompts to interpret visual data. This capability enables CLIP to handle a wide array of tasks without specialized fine-tuning. However, its application to highly specialized domains, such as medical imaging, has uncovered limitations. In the medical field, accurate anomaly detection is crucial for early diagnosis and treatment. Nevertheless, general-purpose models like CLIP often struggle with the intricacies of medical images, which contain subtle and features unique to medical imaging essential for identifying pathological regions. To improve performance in biomedical domains, various adaptations of CLIP, such as BiomedCLIP [1] and MedCLIP-SAMv2 [4], have been proposed to improve performance in biomedical domains. They shows surprising enhancement of medical reasoning compared to ordinary models, but the issue of false positives—incorrectly identifying normal regions as anomalous—remains prevalent. These false positives can lead to unnecessary medical procedures, increasing the burden on healthcare systems and potentially harming patients. To address this issue, we propose a novel method called Contrastive LAnguage Prompting (CLAP), which introduces a more refined way of leveraging natural language prompts for medical anomaly detection. By leveraging both positive and negative prompts, our method aims to find out lesions accurately with CLIP attention. Positive prompts guide the CLIP attention toward potential lesion regions, while negative prompts help attenuate the attention on normal regions, thereby reducing the occurrence of strong attention to false positives. This approach not only provides a more improved understanding of the medical image but also aligns with the demands for reliability in medical diagnostics by artificial intelligence. We can just determine whether disease or not based on the CLIP attention. Toward a more accurate diagnosis, we employ an unsupervised anomaly detection (UAD) method that features a reconstruction-by-inpainting strategy [2]. For this, we obfuscate strong attention regions, over \mu+0.674\sigma (Q3) [5] valued regions, by considering suspected disease regions. Then, we attempt to reconstruct obfuscated regions into normal patterns by U-Net which is trained with normal samples only. Finally, we determine the final disease based on the reconstruction error obtained. To evaluate the legitimacy of our proposal, we perform extensive experiments using BMAD dataset [6]. This dataset provides six benchmarks for five anatomies. Visual comparisons demonstrate that CLAP successfully overcomes issues of strong attention in non-lesion regions. In addition, we improved UAD performance compared to existing methods. Through this work, we aim to bridge the gap between general-purpose VLMs and the specific needs of medical anomaly detection. We conclude by discussing the potential for automating language prompt construction to further improve the usability of this approach in real-world clinical settings."
https://arxiv.org/html/2411.07536v1,Model Stealing for Any Low-Rank Language Model,"Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models.We assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24]. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the result in [KKMZ24] which also requires the unknown distribution to have high “fidelity” – a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.","Proprietary machine learning models are often highly confidential. Not only are their weights not publicly released, but even their architecture and hyperparameters used in training are kept a closely guarded secret. And yet these models are often deployed as a service, allowing users to make queries to the model and receive answers. These answers can take the form of labels or completions of prompts, and sometimes a model will even report additional information such as its confidence scores. This raises a natural question: Question. Are these black-box models actually secure, or is it possible to reverse engineer their parameters or replicate their functionality just from query access to them? This task is called model stealing and it threatens the security of proprietary models and the privacy of data they are trained on. Beyond nefarious reasons, it can also be used in model distillation [MCH+21], where we have trained a large and highly complex model and we want to transfer its knowledge to a much smaller model. It can also be a useful tool for identifying vulnerabilities, as those are often inherited by stolen models. In any case, model stealing continues to be a very active area of research. The influential work in [TZJ+16] showed that there are simple and efficient attacks on popular models like logistic regression, decision trees and deep neural networks that often work in practice. Since then, many new attacks and defenses have been formulated [HLXS21, HJB+21, RST19, WG18, JSMA19, OMR23, OSF19, WXGD20]. There are also approaches based on embedding watermarks [JCCCP21, ZWL23, LZJ+22] that make it possible to detect when one model has been stolen from another. In recent years, there has been particular interest in stealing large language models (LLMs). Various works have shown how to steal isolated components of a language model such as the decoding algorithm [NKIH23], prompts used to fine-tune the model [SZ24], and even the entire weight matrix of the last layer (the embedding projection layer) [CPD+24]. In this work, our main interest will be in theoretical foundations for stealing language models. As is all too familiar, proving rigorous end-to-end guarantees when working with modern machine learning models with all their bells and whistles seems to be an extremely difficult task. For example, while we can understand the training dynamics on multilayer neural networks in terms of gradient flow in the Wasserstein space of probability distributions [MMM19, NP23], it has turned out to be quite difficult to analyze these dynamics except in simplified settings with a high degree of symmetry. Even worse, there are strong lower bounds for learning deep neural networks [KS09] even with respect to nice input distributions [GGJ+20, DKKZ20, GGK20]. The task of reasoning about modern language models seems no easier, as they are built on transformers [VSP+17] with many building blocks such as word embeddings, positional encodings, queries, keys, values, attention, masking, feed-forward neural networks and layer normalization. Nevertheless there are often simplified models that abstract important features of more complex models and give us a sandbox in which to try to find theoretical explanations of empirical phenomena. For example, analyzing the dynamics of gradient descent when training a deep neural network is notoriously difficult. But in an appropriate scaling limit, and when the network is wide enough, it can be approximated through the neural tangent kernel [JGH18]. For recurrent neural networks, a popular approach is to analyze gradient descent on linear dynamical systems instead [HMR18]. Likewise for language models, it is natural to work with Hidden Markov Models (HMMs), which are in some sense the original language model, dating back to the work of Claude Shannon in 1951 [Sha51] and were the basis of other early natural language processing systems including the IBM alignment models. More broadly, we can consider a generalization called low-rank language models (Definition 1.1). This brings us to our main questions: Question. Is there an efficient algorithm for stealing HMMs from query access? What about more generally for low-rank language models? These questions were first introduced and studied in an exciting recent work of Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24]. However their motivation and framing was somewhat different, as we will explain. 1.1 Main Results Formally, we view a language model as a distribution \mathbb{H} over \mathcal{O}^{T} for some alphabet \mathcal{O} and sequence length T. For simplicity, we treat the sequence length as fixed. Following Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24], the rank of the distribution generated by a language model is defined as follows: Definition 1.1. [Low Rank Distribution] A distribution \mathbb{H} over \mathcal{O}^{T} for alphabet \mathcal{O} of size O and sequence length T is rank S if for all t<T, the O^{T-t}\times O^{t} matrix, \mathbf{M}^{(t)}, with entries equal to \Pr_{\mathbb{H}}[f|h] for f\in\mathcal{O}^{T-t} and h\in\mathcal{O}^{t} has rank at most S. In other words, a distribution is rank-S if for any t<T, the information in the prefix of length t can be embedded in an S-dimensional space such that the distribution of the future tokens can be represented as a linear function of this embedding. We note that low-rank distributions are expressive and encompass distributions generated by a Hidden Markov Model (HMM) with S hidden states (see Fact 2.2). Next, we formalize the setup for studying model stealing. We allow the learner to make conditional queries – that is, the learner can specify a history of observations, and then receives a random sample from the conditional distribution on the future observations. Formally, we have the following definition: Definition 1.2 (Conditional Query). The learner may make conditional queries to a distribution \mathbb{H} by querying a string h\in\mathcal{O}^{t} where 0\leq t<T. Upon making this query, the learner obtains a string f of length T-t drawn from the distribution \Pr_{\mathbb{H}}[f|h]. In this model, our goal is to design an algorithm that makes a total number of queries that is polynomial in S,O,T and learns an efficiently samplable distribution that is \epsilon-close in total variation distance to \mathbb{H}. This conditional query model was recently introduced by Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24]. Their motivation was two-fold: First, while learning an HMM from random samples is known to be computationally hard [MR05], in principle one can circumvent these barriers if we are allowed conditional samples. Second, a solution to their problem would generalize Angluin’s classic L^{*} algorithm which learns deterministic finite automata from membership queries [Ang87]. In terms of results, Kakade, Krishnamurthy, Mahajan and Zhang [KKMZ24] introduced a notion that they called fidelity and gave a polynomial time algorithm to learn any low-rank distribution (and thus any HMM) which has high fidelity through conditional queries. However this property does not always hold. Thus, their main question still remains: Is it possible to learn arbitrary low-rank distributions through conditional queries? Here we resolve this question in the affirmative. We show: Theorem 1.3. Assume we are given conditional query access to an unknown rank S distribution \mathbb{H} over \mathcal{O}^{T} where |\mathcal{O}|=O. Then given a parameter 0<\eta<1, there is an algorithm that takes \mathrm{poly}(S,O,T,1/\eta) conditional queries and running time and with probability 1-\eta outputs a description of a distribution \mathbb{H}^{\prime} such that \mathbb{H}^{\prime} is \eta-close in TV distance to \mathbb{H}. Moreover there is an algorithm that samples from \mathbb{H}^{\prime} in \mathrm{poly}(S,O,T,\log(1/\eta)) time. Note that crucially, the algorithm only makes conditional queries for learning the representation of \mathbb{H}^{\prime}. Once we have this learned representation, we may draw as many samples as we want without making any more queries to the original distribution \mathbb{H}. 1.2 Discussion Theorem 1.3 shows that we can efficiently learn any low-rank distribution via conditional queries. Thus, we can view our results as showing that in some sense, the rank of a distribution can be a useful proxy for understanding the complexity of model stealing, similar to how complexity measures, such as Bellman rank [JKA+17] and its relatives [FKQR21], are useful for understanding the statistical complexity of learning a near optimal policy in reinforcement learning. There is a key conceptual insight driving our algorithm. One of the challenges in learning sequential distributions is that the error can grow exponentially in the sequence length T. In particular if we imagine sampling from \mathbb{H} one token at a time, the low-rank structure ensures that we only need to keep track of an S-dimensional hidden state at each step. However, each step involves multiplication by a change-of-basis matrix and these repeated multiplications cause the error to grow exponentially. The key to mitigating this error blowup is that we combine each change-of-basis with a projection step, where we solve a convex optimization problem that performs a projection with respect to KL divergence. Crucially, projection in KL divergence has a contractive property (see Fact 3.9) that does not hold for other natural measures of distance between distributions, such as TV distance. We give a more detailed overview of our algorithm in Section 3. This is an interesting example where allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance. Of course, phrased in general terms, this is a driving philosophy behind OpenAI’s o1 model. But so far we have little theoretical understanding of the provable benefits of allowing more compute at inference time. 1.3 Related Work There has been a long line of work on learning HMMs from random samples. Mossel and Roch [MR05] gave the first polynomial time algorithms that work when under appropriate full rankness conditions on the transition and observation matrices. Other works gave spectral [HKZ12] and method of moments based approaches [AHK12]. Learning HMMs can also be thought of as a special case of learning phylogenetic trees [CGG01, MR05]. Other approaches assume the output distributions belong to a parametric family [KNW13], or study quasi-HMMs [HGKD15]. The main computational obstruction in this area is that HMMs, without full rankness conditions, can encode noisy parities [MR05], which are believed to be computationally hard to learn. In the overcomplete setting, where the hidden state space is allowed to be larger than the observation space, there are ways around these lower bounds. First, one can aim to predict the sequence of observations, rather than learning the parameters [SKLV18]. Under a natural condition called multi-step observability one can get quasi-polynomial time algorithms [GMR23]. Second, one can make structural assumptions that the transition matrices are sparse, well-conditioned, and have small probability mass on short cycles [SKLV17]. Alternatively there are polynomial time algorithms that work under the assumption that the transition and observation matrices are smoothed [BCPV19]. There are also related models called linear dynamical systems where the hidden states and observations are represented as vectors. In contrast, in HMMs the hidden states and observations take on only a finite set of possible values. There is a long line of work on learning linear dynamical systems too [HMR18, OO19, TP19, SRD19, SBR19, BLMY23a, CP22, BLMY23b]. However, these algorithms require various structural assumptions, and even the weakest ones, namely condition-number bounds on the called observability and controllability matrices, are known to be necessary [BLMY23a]. The conditional query model has also been studied for other statistical problems such as testing discrete distributions and learning juntas [CFGM13, CRS15, BC18, CJLW21]. However, in most of these settings, the goal of studying the conditional query model is to obtain improved statistical rates rather than sidestep computational hardness. We remark that there are also many classic examples of problems in computational learning theory where, when allowed to make queries, there are better algorithms [Jac97] than if we are only given passive samples."
https://arxiv.org/html/2411.07529v1,Evaluating ChatGPT-3.5 Efficiency in Solving Coding Problems of Different Complexity Levels: An Empirical Analysis,"ChatGPT and other large language models (LLMs) promise to revolutionize software development by automatically generating code from program specifications. We assess the performance of ChatGPT’s GPT-3.5-turbo model on LeetCode, a popular platform with algorithmic coding challenges for technical interview practice, across three difficulty levels: easy, medium, and hard. We test three main hypotheses: First, that ChatGPT solves fewer problems as difficulty rises (Hypothesis 1). Second, that prompt engineering improves ChatGPT’s performance, with greater gains on easier problems and diminishing returns on harder ones (Hypothesis 2). Third, that ChatGPT performs better in popular languages like Python, Java, and C++ than in less common ones like Elixir, Erlang, and Racket (Hypothesis 3). To investigate these hypotheses, we conduct automated experiments using Python scripts to generate prompts that instruct ChatGPT to create Python solutions. These solutions are stored and manually submitted on LeetCode to check their correctness. For Hypothesis 1, results show the GPT-3.5-turbo model successfully solves 92% of easy, 79% of medium, and 51% of hard problems. For Hypothesis 2, prompt engineering yields improvements: 14-29% for Chain of Thought Prompting, 38-60% by providing failed test cases in a second feedback prompt, and 33-58% by switching to GPT-4. From a random subset of problems ChatGPT solved in Python, it also solved 70% in Java, 50% in C++, and none in Elixir, Erlang, or Racket. These findings generally validate all three hypotheses.","Large language models have catalyzed significant advancements in various domains, including natural language processing [24], text generation [9], and now code generation [11]. LLMs have demonstrated skills in understanding, generating, and manipulating code, transforming the landscape of programming assistance. Large language models are trained on vast datasets comprising both natural language text and source code, which enables them to offer support in software development tasks. One of the more prominent LLMs is ChatGPT, a conversational AI model developed by OpenAI. The potential of these models to assist with programming has led to increased interest in their ability to improve programmer productivity, code correctness, and automated code generation from problem specifications. In this paper, we focus on exploring ChatGPT’s ability (primarily working with the low-cost GPT-3.5-turbo model) to solve coding problems in the popular programming language Python, using the platform Leetcode [8]. Leetcode is an online platform with more than 2000 coding challenges widely used for technical interview preparation and programming practice. The platform categorizes problems into three difficulty levels — easy, medium, and hard — covering a broad range of topics such as algorithms, databases, shell scripting, and concurrency. Each problem provides a detailed problem statement and typically includes 3-4 example input-output pairs. LeetCode’s integrated compiler and test cases make it an ideal tool for evaluating the correctness of solutions generated by GPT models, allowing for a structured and objective comparison. It is important to note that Leetcode, while useful as a benchmarking tool, does not fully capture the complexities of real-world coding environments. Our research is guided by three primary questions aimed at assessing the strengths and limitations of ChatGPT in coding tasks: 1. How effectively can ChatGPT solve Leetcode problems across varying levels of difficulty (easy, medium, and hard)? 2. Can prompt engineering and the selection of more effective demonstrations enhance ChatGPT’s initial coding results? 3. Which programming languages is ChatGPT most proficient in? One key challenge with language models like ChatGPT is the phenomenon of “hallucinations” - where the model may guess the user’s intent rather than seek clarification, respond sensitively to input phrasing, or produce plausible yet incorrect code [13]. These issues may arise from the limitations in the data set on which GPT-3.5-turbo was trained, particularly with respect to the code. LeetCode provides a robust framework of benchmarks and test cases that a solution must pass to be deemed correct. Even minor misunderstandings by ChatGPT can cause the generated code to fail in fulfilling its intended purpose. Given ChatGPT’s sensitivity to prompt phrasing, we also explore how the model’s performance can be improved through “prompt engineering”: designing more precise and effective prompts. In addition to Python, we examine ChatGPT’s proficiency across various programming languages, from popular ones like C++ and Java to less common languages such as Erlang, Elixir, and Racket. Through these investigations, we seek to understand better the strengths and limitations of ChatGPT’s code-generation capabilities. To conduct our evaluation, we developed a python script that interacts with ChatGPT via an API key, storing the model’s responses in corresponding files for each query. For each coding problem, we instructed ChatGPT to provide its solutions in python to ensure consistency in evaluation across all test cases. We tested 1,475 LeetCode coding problems, for each entering ChatGPT responses into the platform, and recording the result, including the number of passed test cases. These results were then analyzed to evaluate ChatGPT’s problem-solving performance and further test specific hypotheses. With respect to Question 1, our results show that ChatGPT 3.5 turbo performed well in solving LeetCode problems, with a high success rate of 92% for easy problems and 79% for medium ones. However, when tested on LeetCode’s hard category, the model’s performance dropped, achieving a 51% pass rate. These results highlight the model’s competence in simpler tasks, but also reveal its challenges when handling more complex coding problems. Regarding Question 2, our evaluation revealed that while chain-of-thought (CoT) prompt engineering improved ChatGPT performance in all problem difficulties, its impact was most significant on easier problems, with less impactful results on medium and hard problems. In contrast, incorporating test cases into the query — where the GPT-3.5-turbo model initially produced incorrect results — along with the expected outputs, provided even more significant improvements, particularly for medium and hard problems. Lastly, we observed that ChatGPT using the more advanced GPT-4 model, when paired with CoT prompting, also delivered strong performance, closely matching the results of adding in test cases where GPT-3.5-turbo initially failed, especially on more complex problems. This indicates that CoT prompting combined with error-focused adjustments can be highly effective, especially as the complexity of the problem increases. For example, in the easy category of problems, we found that CoT gave 29% improvements over the baseline, while providing failed test-cases gave 38% improvement, and switching to GPT-4 gave 33% improvement. For the medium and hard categories the performance of using GPT-4 and GPT-3.5-turbo with failed cases were more comparable. Lastly, looking at Question 3, we evaluated the performance of ChatGPT in five additional programming languages besides Python: C++, Java, Erlang, Elixir, and Racket. Python served as the baseline for comparison throughout our exploration of this question. ChatGPT was able to solve in C++ about 50% of a random sample of problems it could solve using Python and in Java it could solve about 70% of a random sample of problems it could solve using Python. Interestingly, some problems that ChatGPT failed to solve in Python were successfully solved in C++ or Java, indicating slight performance variations between these languages. However, for less common languages such as Erlang, Elixir, and Racket, the performance of ChatGPT was poor, with no problems successfully solved, regardless of whether they were solved in Python or not. This may be attributed to the lower frequency of these languages in the model’s training data set. We also present results from some further experiments where we evaluated which types of problems ChatGPT performed best/worst at, and whether there is a correlation between lines of code produced by ChatGPT and the correctness of the program. The subsequent sections of this paper dive deeper into the context, methodology, and outcomes of our study, providing a comprehensive analysis of ChatGPT’s performance and its implications for code generation tasks. In section 2, we provide a comprehensive review of the relevant literature on large language models (LLMs) and their application to code generation, highlighting prior research and existing challenges in the field. In section 3, we outline the experimental setup, including the process of data collection, the tools and techniques used to evaluate ChatGPT’s performance, and the criteria for assessing the correctness of its solutions. In section 4, we present a detailed analysis of ChatGPT’s performance across different categories of coding problems and programming languages. This section includes a quantitative assessment of its accuracy, efficiency, and limitations, supported by specific examples and findings from the experiments. We present our concluding comments and ideas regarding future work directions in section 5."
https://arxiv.org/html/2411.07528v1,Logs are All You Need in Security,"Large and Small Language Models (LMs) are typically pretrained using extensive volumes of text, which are sourced from publicly accessible platforms such as Wikipedia, Book Corpus, or through web scraping. These models, due to their exposure to a wide range of language data, exhibit impressive generalization capabilities and can perform a multitude of tasks simultaneously. However, they often fall short when it comes to domain-specific tasks due to their broad training data. This paper introduces SecEncoder, a specialized small language model that is pretrained using security logs. SecEncoder is designed to address the domain-specific limitations of general LMs by focusing on the unique language and patterns found in security logs. Experimental results indicate that SecEncoder outperforms other LMs, such as BERT-large, DeBERTa-v3-large and OpenAI’s Embedding (text-embedding-ada-002) models, which are pretrained mainly on natural language, across various tasks. Furthermore, although SecEncoder is primarily pretrained on log data, it outperforms models pretrained on natural language for a range of tasks beyond log analysis, such as incident prioritization and threat intelligence document retrieval. This suggests that domain-specific pretraining with logs can significantly enhance the performance of LMs in security. These findings pave the way for future research into security-specific LMs and their potential applications.","Transformers [65] are a breakthrough AI architecture that have facilitated the development of various Language Models (LMs) [27, 3]. These models can harness huge amounts of data to perform diverse tasks across language and other modalities such as audio, image and video. Some examples of LMs are BERT, RoBERTa, GPT, Gemini and PaLM [48, 19, 23, 17, 24, 25, 4, 5]. LMs can differ in their size, data sources, learning objectives, and are trained on large collections of text, such as Wikipedia, books, news articles, code, social media posts or web scraping. Certain language models, such as encoder-only models, are designed to encode the semantic and syntactic information of natural language into high-dimensional vectors, enabling them to perform downstream tasks like search, classification, summarization, translation, and question answering. Decoder-only models, on the other hand, are capable of generating natural language text by sampling from their probability distributions, producing coherent and fluent outputs. Some models, such as GPT-o and Gemini 1.5, extend these capabilities to multimodal content generation, creating images, audio, and video by utilizing a shared latent space across different modalities. Language models (LMs) have achieved state-of-the-art results across a wide range of benchmarks in natural language processing, coding, mathematics, and reasoning. They have also demonstrated impressive abilities to generate realistic and creative content, including stories, poems, songs, and jokes. These advancements in LMs have unlocked new opportunities and introduced challenges for AI research and applications, spanning areas such as natural language understanding, natural language generation, multimodal fusion, and knowledge extraction. However, LMs are not ideally suited to address domain-specific challenges, such as specialized vocabulary, terminology, knowledge, and logic, due to their design for broad and diverse applications. Previous studies highlight these limitations across several domains. In Biomedicine [31, 49], LMs struggle to capture complex relationships and semantics of biomedical entities and concepts. In Finance [66], LMs underperform compared to domain-specific counterparts. In Medicine [40], LMs lack alignment with clinical utility. In Security [38, 21], LMs fall short in domain-specific security knowledge. Similarly, in Software [33], LMs face challenges in interpreting and making sense of operational logs. Despite these limitations, the potential of Artificial Intelligence (AI), particularly generative AI, continues to attract significant interest from the security community [11, 2, 6]. These generative models hold potential as valuable tools for security professionals, serving as copilots to navigate the complexities of security tasks such as identifying phishing emails, crafting detections, or analyzing and summarizing incidents. However, the field of security presents unique challenges that can only be partially addressed by generative models. A notable challenge is the need for security professionals to handle a variety of data types beyond natural language texts, including logs and telemetries. These data are often heterogeneous, noisy, and voluminous, necessitating efficient and scalable processing and analysis methods. In this paper, we present SecEncoder, a small language model that is trained with security logs. SecEncoder is an encoder-only model, pretrained on a large corpus of security logs, which capture various events and activities related to security incidents and operations. SecEncoder aims to demonstrate the feasibility and utility of training a domain-specific language model on security logs at scale, and to provide a versatile and powerful model that can be applied to various security use cases. We evaluate SecEncoder on both intrinsic and extrinsic tasks, such as log analysis, anomaly detection, log search and incident classification. Our main contributions are: • We pretrain a security-specific small language model from scratch on a large and diverse corpus of security logs, which capture various events and activities related to security incidents and operations. We aim to train a versatile and powerful model that can generalize to various type of security use cases. • We evaluate SecEncoder using both intrinsic and extrinsic measures, using both internal and publicly available benchmarks. We also compare SecEncoder to the other LMs, such as BERT, DeBERTa and OpenAI’s embedding model (text-embeddings-ada-002), and show that SecEncoder outperforms the best results on most of the tasks, and also exhibits some unique and novel capabilities. • We present four real-world use cases for SecEncoder. Notably, some of these use cases such as incident classification and threat intelligence document retrieval demonstrate that, despite SecEncoder being primarily trained on logs, it can effectively generalize to other data modalities without specific training on them. This finding suggests that logs could serve as valuable data sources for pretraining language models across domains beyond security. • We discuss the limitations and future directions for SecEncoder, focusing on areas such as data quality and diversity, as well as improvements in robustness and inference speed. The remainder of this paper is structured as follows: Section 2 discusses related work, while Section 3 introduces the overall architecture and design. Section 4 details the various experiments conducted for testing and evaluation. Section 5 explains multiple real world use cases of SecEncoder and the corresponding results. Section 6 delves into limitations of SecEncoder and discusses future work, and finally, Section 7 provides the conclusion."
https://arxiv.org/html/2411.07521v2,Fair Summarization: Bridging Quality and Diversity in Extractive Summaries,"Fairness in multi-document summarization of user-generated content remains a critical challenge in natural language processing (NLP). Existing summarization methods often fail to ensure equitable representation across different social groups, leading to biased outputs. In this paper, we introduce two novel methods for fair extractive summarization: FairExtract, a clustering-based approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints. We evaluate these methods using Divsumm summarization dataset of White-aligned, Hispanic, and African-American dialect tweets and compare them against relevant baselines. The results obtained using a comprehensive set of summarization quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well as a fairness metric F, demonstrate that FairExtract and FairGPT achieve superior fairness while maintaining competitive summarization quality. Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that integrate quality and fairness into a single evaluation framework, offering a more nuanced understanding of the trade-offs between these objectives. This work highlights the importance of fairness in summarization and sets a benchmark for future research in fairness-aware NLP models.","Multi-document summarization, which condenses multiple documents into a concise summary, is a fundamental task in natural language processing (NLP). Summarization methods are typically either extractive, selecting the most important sentences, or abstractive, where the content is rephrased. Early research focused on summarizing formal text sources such as news articles. However, with the rise of social media, attention has shifted to summarizing user-generated content, which is diverse in style and language. Social media platforms bring together users from varied backgrounds, introducing linguistic diversity through informal language, slang, and emojis. This diversity raises the challenge of ensuring fairness in summarization ensuring balanced representation of various social groups. In social media, where public opinion is shaped, fair summaries are essential to include different perspectives and avoid underrepresentation of one or more social groups. In the context of social media, where millions of users contribute diverse perspectives, ensuring representation of this diversity in summaries becomes crucial. Social media platforms encompass a wide range of voices, including those from historically underrepresented or marginalized groups, making it essential that summarization methods capture this diversity fairly. Without proper representation, certain voices might be excluded or misrepresented, leading to biased summaries that skew public discourse (Binns,, 2017; Hutchinson and Mitchell,, 2018). The need for fairness in summarization is further heightened by the fact that user-generated content is often informal and marked by dialectal variations, requiring models to go beyond traditional summarization approaches (Pitsilis et al.,, 2018). Therefore, ensuring that all groups—across race, gender, and linguistic diversity—are fairly represented is critical for generating balanced summaries that reflect the diversity of public opinion (Dash et al.,, 2018). Despite advancements, bias remains a concern in automated summarization (Dash et al.,, 2019; Jung et al.,, 2019; Keswani and Celis,, 2021; Olabisi et al.,, 2022) as most existing summarization methods focus on quality but fall short in optimizing fairness. This gap leads to the key question: if a summarization method is optimized for fairness, how does it affect the overall summary quality? Previous studies suggest a trade-off between fairness and quality (Jung et al.,, 2019). Improving fairness can sometimes lower quality. While existing algorithms have made strides in balancing the two, none achieve perfect fairness. ChatGPT-EXT (Zhang et al.,, 2023) FairGPT (Ours) If you see on the news something about the Chicago Kitchen Clown Bandits then it will be referring me my friend Eten and I. Turns out not all White Castles are the same. Why do you push me away Chicago?! I mean I’m from Chicago. I’ll cheer for the Bears, but I’m a bigger 49ers fan. Is this new wave of Chicago Rap gonna be like the Hyphy movement? Don’t talk shot about Chicago, or those big shoulders will plow right into your little Boston ass. Nothing makes me happier than seeing the Bulls win #ChicagoBasketball #Bullieve. Don’t talk shot about Chicago, or those big shoulders will plow right into your little Boston ass. Nothing makes me happier than seeing the Bulls win #ChicagoBasketball #Bullieve. Truuu we tryna find sum to do too.. I dnt wanna b n Chicago if ain’t nobody here. Turns out not all White Castles are the same. Why do you push me away Chicago?! I mean I’m from Chicago. I’ll cheer for the Bears, but I’m a bigger 49ers fan. Is this new wave of Chicago Rap gonna be like the Hyphy movement? Table 1: Comparison of summaries by ChatGPT-EXT and FairGPT. Tweets from different groups are highlighted: Group 1 (e.g., White-aligned) and Group 2 (e.g., African-American). In this paper, we address two research questions: 1. How does achieving perfectly fair summaries affect overall quality? 2. How well do current methods perform when considering both fairness and quality? To illustrate the performance of fairness-aware summarization models, we compare summaries generated by ChatGPT-EXT (Zhang et al.,, 2023) and our proposed FairGPT model on a sample instance from Divsumm dataset (Olabisi et al.,, 2022). As shown in Table 1, FairGPT ensures equal representation of tweets from different groups, while ChatGPT-EXT shows a slight imbalance. We make the following contributions: • We propose FairExtract, a fair clustering-based extractive summarization method that achieves perfect fairness and is evaluated against baseline models using standard and composite quality-fairness metrics. • We develop FairGPT, a large language model-based extractive summarization method that enforces fairness through equal representation and accurate content extraction using the longest common subsequence. • We introduce composite metrics combining normalized quality scores with fairness, providing a comprehensive analysis of the quality-fairness trade-off in summarization models."
https://arxiv.org/html/2411.07506v1,FM-TS: flow matching for time series generation,"Time series generation has emerged as an essential tool for analyzing temporal data across numerous fields. While diffusion models have recently gained significant attention in generating high-quality time series, they tend to be computationally demanding and reliant on complex stochastic processes. To address these limitations, we introduce FM-TS, a rectified Flow Matching-based framework for Time Series generation, which simplifies the time series generation process by directly optimizing continuous trajectories. This approach avoids the need for iterative sampling or complex noise schedules typically required in diffusion-based models. FM-TS is more efficient in terms of training and inference. Moreover, FM-TS is highly adaptive, supporting both conditional and unconditional time series generation. Notably, through our novel inference design, the model trained in an unconditional setting can seamlessly generalize to conditional tasks without the need for retraining. Extensive benchmarking across both settings demonstrates that FM-TS consistently delivers superior performance compared to existing approaches while being more efficient in terms of training and inference. For instance, in terms of discriminative score, FM-TS achieves 0.005, 0.019, 0.011, 0.005, 0.053, and 0.106 on the Sines, Stocks, ETTh, MuJoCo, Energy, and fMRI unconditional time series datasets, respectively, significantly outperforming the second-best method which achieves 0.006, 0.067, 0.061, 0.008, 0.122, and 0.167 on the same datasets. We have achieved superior performance in solar forecasting and MuJoCo imputation tasks, significantly enhanced by our innovative t power sampling method. The code is available at https://github.com/UNITES-Lab/FMTS..","Figure 1: Comparison of FM-TS and diffusion-TS in terms of efficiency on Energy dataset under varying training epochs and number of forward evaluation steps. Time series data is fundamental to modern data analysis, serving as a cornerstone in diverse domains such as finance, healthcare, energy management, and environmental studies (Lim and Zohren, 2021; Ye et al., 2024; Dama and Sinoquet, 2021; Liang et al., 2024). However, acquiring high-quality time series data often presents significant challenges, including stringent privacy regulations, prohibitive data collection costs, and data scarcity in certain scenarios. These challenges highlight the potential benefits of synthetic time series data, which can provide a cost-effective solution for data scarcity, overcome privacy concerns, and offer flexibility in generating diverse scenarios representing a wide range of possible patterns and trends. To obtain high-quality synthetic data, there is a pressing need for advanced time series generation techniques that can produce realistic and diverse patterns, accurately reflecting real-world complexities and variations. Recent years have witnessed significant advancements in time series generation, ranging from VAE-based approaches (Desai et al., 2021; Xu et al., 2020) to diffusion models (Kong et al., 2021; Tashiro et al., 2021), demonstrate remarkable capabilities in capturing complex temporal dynamics. While these studies have paved new paths for time series modeling (Coletta et al., 2023; Yoon et al., 2019a), important challenges remain in theoretical foundations and computational efficiency. Diffusion models (Ho et al., 2020; Song et al., 2020a; b) are then utilized for time series generation, yield exceptional generative quality. They offer several advantages, including their ability to capture long-range dependencies and generate diverse, high-quality samples. However, diffusion models suffer from slow generation speeds and high computational cost due to the requirement of many steps to infer (see figure 1 and (Nichol and Dhariwal, 2021)). Moreover, diffusion models struggle to preserve the long-term dependencies and intricate patterns inherent in time series data (Rasul et al., 2021). Recently, rectified flow matching (Liu et al., 2022) has emerged as a promising generative modeling approach, because of its efficiency and capacity for scalability (Esser et al., 2024a). Rectified flow matching optimizes neural ordinary differential equation (ODE) to transport between distributions along approximately straight paths, solving a nonlinear least squares problem. This approach offers more efficient sampling than diffusion models through approximately straight paths, while providing a unified framework for generative modeling and domain transfer with theoretical guarantees on transport costs (Liu et al., 2022). In contrast to diffusion models, rectified flow matching directly maps the latent space to the data space, whereas diffusion models must learn to denoise data based on a scheduled noise-adding process. In addition, rectified flow matching requires only a single forward pass for sampling (Liu et al., 2022), significantly enhancing both efficiency and performance. Rectified flow matching has shown superior performance in various tasks, including image generation (Kim et al., 2024; Mehta et al., 2024; Kuaishou Technology, 2024). However, it has not yet been applied to time series generation, primarily due to the unique characteristics of time series data, such as temporal dependencies and potential seasonality. To address these challenges, we introduce FM-TS, a flow matching based framework for time series generation. Our method not only inherits the efficiency of rectified flow matching but can also generalize in both unconditional and conditional settings. The main contributions of this work are: • FM-TS consistently outperforms existing state-of-the-art methods across a variety of time series generation datasets with notable efficiency (see Figure 1). To the best of our knowledge, this work is the first to utilize rectified flow matching to time series generation. • For conditional time series generation, we also introduce a simple yet powerful sampling technique: t power sampling, a simple timestep shifting method (used in generation), which can boot performance of conditional generation quite a lot. • With our novel inference design, the model trained in an unconditional setting can seamlessly generalize to conditional tasks without requiring retraining and redundant gradient-based optimization steps like (Yuan and Qiao, 2024). The experiments on various tasks demonstrate that the proposed framework can significantly boost performance through rectified flow matching. We achieve most state-of-the-art, e.g., FM-TS can achieve context fid (lower is better) with 0.019, 0.011 on stocks, ETTh unconditional generation datasets while previous best result is 0.067, 0.061. On solar forecasting tasks, our method achieves an MSE of 213, outperforming the previous best result of 375 (Yuan and Qiao, 2024) by 43.2%."
https://arxiv.org/html/2411.07501v2,: Learned Augmented Residual Layer,"One of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which has led to significantly better model convergence and quality. Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures, the backbone of LLMs.In this paper we introduce Learned Augmented Residual Layer (LAuReL)—a novel generalization of the canonical residual connection—with the goal to be an in-situ replacement of the latter while outperforming on both model quality and footprint metrics. Our experiments show that using LAuReL can help boost performance for both vision and language models. For example, on the ResNet-50, ImageNet 1K task, it achieves 60\% of the gains from adding an extra layer, while only adding 0.003\% more parameters, and matches it while adding 2.6\times fewer parameters.","Model efficiency is of critical importance in the age of extremely large language and vision models. Even if a given model’s quality is good, its footprint metrics such as train-time compute required, inference latency, resident memory size, etc. dictate if it can be experimented with and/or deployed in real-world settings. These metrics are directly tied to the financial costs of deploying the model in production and user-perceived responsiveness of systems dependent on these models. Consequently, improving the Pareto-frontier of model quality vs footprint, via efficient deep learning methods has been an area of active research in the past few years. Areas of interests span from algorithmic techniques (Menghani, 2023), to efficient hardware (Sze et al., 2017), to best practices around model efficiency (Dehghani et al., 2022), etc. One of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which had led to significantly better model convergence and quality (He et al., ). Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures (Vaswani et al., 2017), the backbone of LLMs. In this paper we introduce learned augmented residual layer, LAuReL, which generalizes the canonical residual connection. Recall that deep-learning models with residual connections have a ‘block’ structure, with many blocks chained together between the input and final output; these could be convolution/identity blocks within a ResNet, a transformer block in a transformer encoder/decoder, etc. Within a block, a typical residual connection is given by: x_{i+1}=f(x_{i})+x_{i}. (1) Here, f(\cdot) can be any non-linear function such as attention, MLP, multiple non-linear layers, etc., x_{i} is the input to the said non-linear function, and x_{i+1} is the combined output of the non-linear function and the residual component. Refer to Figure 1 for an illustration. To simplify exposition, we ignore pre-processing functions such as layer norm, which can be folded into f(\cdot) without loss of generality. Figure 1: A standard residual connection. We assume the model to be divided into logical ‘blocks’, which is true for most modern architectures including transformers. The residual connection combines the output of a non-linear function f and the input to the said non-linear function. Here, f can be attention, MLP, or any other combination of non-linear layers."
https://arxiv.org/html/2411.07482v1,Enhancing Link Prediction with Fuzzy Graph Attention Networks and Dynamic Negative Sampling,"Link prediction is crucial for understanding complex networks but traditional Graph Neural Networks (GNNs) often rely on random negative sampling, leading to suboptimal performance. This paper introduces Fuzzy Graph Attention Networks (FGAT), a novel approach integrating fuzzy rough sets for dynamic negative sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS) systematically selects high-quality negative edges based on fuzzy similarities, improving training efficiency. FGAT layer incorporates fuzzy rough set principles, enabling robust and discriminative node representations. Experiments on two research collaboration networks demonstrate FGAT’s superior link prediction accuracy, outperforming state-of-the-art baselines by leveraging the power of fuzzy rough sets for effective negative sampling and node feature learning.","Link prediction has emerged as a crucial task in network analysis with extensive applications across diverse domains. In medical sciences, it aids in predicting protein-protein interactions and drug-target associations; in financial systems, it helps detect fraudulent transactions and assess credit risks; and in chemistry, it facilitates the discovery of novel molecular structures and chemical reactions. The ability to accurately predict potential connections in these complex networks has significant implications for scientific advancement and practical applications. Graph Neural Networks (GNNs) have demonstrated remarkable success in link prediction tasks, primarily due to their inherent capability to capture and process structural information in graph-structured data. However, a critical limitation in existing GNN-based approaches lies in their negative sampling methodology. Contemporary methods typically employ random sampling strategies to select negative edges, disregarding the rich semantic and structural information encoded in node representations. This oversight significantly hampers the training process, resulting in slower convergence rates and suboptimal model performance. An ideal negative sampling mechanism should not only leverage node embeddings effectively but also adaptively select high-quality negative samples based on the model’s current state, ensuring both dynamic responsiveness and sampling accuracy. While various methodologies have been explored to enhance link prediction accuracy, the potential of fuzzy rough sets—a mathematical framework for measuring fuzzy relations and handling uncertainty—remains largely unexplored in the context of GNNs and link prediction. This theoretical framework offers unique advantages in capturing imprecise relationships and handling ambiguous data structures, making it particularly suitable for network analysis tasks. To address these limitations and leverage the untapped potential of fuzzy rough sets, we propose a novel fuzzy rough sets-based negative sampling strategy called Fuzzy Negative Sampling (FNS). This approach systematically evaluates candidate negative edges through their fuzzy lower approximation values, selecting the top K candidates as negative training instances. Furthermore, we introduce Fuzzy Graph Attention Network (FGAT), an enhanced graph neural architecture designed to aggregate neighboring node information in a more robust and effective manner. The main contributions of this work can be summarized as follows: • We introduce FNS, a novel negative sampling framework that leverages fuzzy rough sets theory to identify high-quality negative edges, significantly improving the effectiveness of the training process in link prediction tasks. • We propose FGAT, an innovative graph attention network that incorporates fuzzy rough set principles to achieve more robust and discriminative node representations. • We conduct comprehensive experiments across two real-world datasets, demonstrating the effectiveness of our proposed framework."
https://arxiv.org/html/2411.07466v1,: A Challenging Mention Resolution Benchmark for LLMs,"Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models’ referential understanding. To address this, we introduce \dataset, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. \datasetfeatures long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained analysis of model performance. We evaluate both closed- and open-source LLMs on \datasetand observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones. We observe that pronominal mentions, which have limited surface information, are typically much harder for models to resolve than nominal mentions. Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy, highlighting the strong referential capabilities of state-of-the-art LLMs while also indicating room for further improvement.111Code for the paper is available at https://github.com/KawshikManikantan/IdentifyMe","Figure 1: Sample instance from the validation set of \dataset. The mention of interest is highlighted in the text. The answer options include frequently occurring entities in the text, and None of the Above. Coreference resolution (CR) is a fundamental task for text comprehension. While LLMs have made tremendous strides on a wide array of NLP tasks Brown et al. (2020), their performance on CR has been relatively underwhelming, with models struggling at even mention detection Le and Ritter (2023); Manikantan et al. (2024). Through extensive analysis, recent work by Gan et al. (2024) has identified that LLMs’ excellent referential understanding is underestimated in the typical CR setup due to the span-based output format being ill-suited for LLMs. They suggest adapting CR datasets and task metrics to support LLM evaluations. Along these lines, we create the \datasetbenchmark for mention resolution in an MCQ format, commonly used for LLM evaluations Hendrycks et al. (2021). To construct the benchmark, we use annotations from two long-text coreference benchmarks, namely LitBank Bamman et al. (2020) and FantasyCoref Han et al. (2021). To make the benchmark challenging, we restrict it to pronominal and nominal mentions and apply some heuristics for each mention type to filter out easily resolvable cases (Section 2.1). Each MCQ instance consists of text marked with the mention of interest, and the choices consist of frequently occurring entities in the text and the None of the Above (NoA) option. Figure 1 shows an example in \dataset, derived from LitBank. We evaluate both closed- and open-source models. On average, among the mention types, LLMs perform worse on pronominal mentions, which have limited surface information, than nominal mentions. The instances where None of the Above is the correct answer prove particularly challenging for all the models, with open-source models seeing a drop of more than 50%. With nested mentions, LLMs tend to frequently confuse between entities with overlapping mentions (e.g., his mother). The highest-scoring model GPT-4o scores 81.9% on \dataset, highlighting the strong performance of frontier LLMs while also indicating scope for further improvement in referential capabilities."
https://arxiv.org/html/2411.07464v1,BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks,"Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4. Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting. With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls. Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning. With 94.2% reduction in the cost (from $0.931 per run cost averaged over all tasks for GPT-4 single agent system to $0.054), our system is able to yield better average success rate of 32.95% as compared to GPT-4 single-agent system yielding 22.72% success rate averaged over all the tasks of MLAgentBench.","Although recent advances have shown that Large Language Models (LLMs) are adept at handling a vast array of applications ranging from natural language (Fang et al., 2024; Huang and Chang, 2023; Zhu et al., 2023; Yi et al., 2024) to code-related tasks (Zheng et al., 2024; Zan et al., 2023; Zhang et al., 2024a), this capability does not often translate to more complicated and nuanced tasks (Yeadon et al., 2024). Most code-related efforts involving LLMs (Guo et al., 2024; Huang et al., 2024; Zhong et al., 2024) are based on tasks such as HumanEval (Chen et al., 2021) and MBXP (Athiwaratkun et al., 2023), that have a relatively easier level of complexity that is far from what is experienced by data scientists. However, real-world engineering challenges demand nuanced problem-solving and intricate planning, often involving multiple rounds of strategizing, experimentation, and recalibration. LLM agent systems excel in simulating this iterative process, since they comprise of an environment containing code files, description files and data files and a pre-defined action space allowing interaction with the environment. This demonstrates their capability to address intricate engineering challenges effectively. (Zhang et al., 2024b). Transitioning to codifying Machine Learning (ML) applications brings its own challenges since they often involve training models on datasets, tuning hyperparameters, devising ways to improve performance, etc. These applications are not straightforward and require a deep understanding of the underlying algorithms and techniques along with specific libraries used for implementation of plans. Although there exist AutoML-based approaches for automating such tasks (He et al., 2021; Salehin et al., 2024), these offer limited flexibility since they typically operate within predefined constraints and search spaces in the form of possible configurations of architectures and/ or hyper-parameters, which may limit their ability to explore solutions out-of-distribution of the search space. While works such as ChatDev (Qian et al., 2023) and MetaGPT (Hong et al., 2023) have explored the capabilities of LLM Agents in a software development environment, there is a notable scarcity of research on utilizing LLM Agents for solving ML tasks. Recent works like MLCopilot (Zhang et al., 2024c) introduce an assistant for solving ML tasks. However, these architectures are limited in the types of problems they can address and must strictly follow task description formats that do not align with real-world scenarios. Additionally, such assistants only suggest solutions, leaving the actual burden of implementation to the user. To the best of our knowledge, MLAgentBench (Huang et al., 2023) is the only significant benchmark addressing ML problem solving capabilities of LLM Agents directly dealing with code. Although they get good performance on some tasks in their benchmark, they focus on single-agent systems using expensive LLMs such as GPT-4, which costs approximately $0.52-$2.9 per run, depending on the task. For the experiments they conduct, they go for 8 runs per task for 15+ tasks, leading to very high experimental cost of approximately $200+. With such larger models becoming increasingly expensive to use, there is a natural incentive to develop no-cost or low-cost systems using smaller, open-source models and making them equally capable for niche tasks. However, existing agent creation frameworks like AutoGen (Wu et al., 2023) do not prioritize cost-reduction. Replacing single-agent systems using expensive LLMs with single-agent smaller, open-source LLMs may not serve the purpose. Our initial experiments with replacing all LLM calls in Huang et al. (2023) for auto-generating codes for ML tasks, with no or low-cost LLMs, namely, Gemini-Pro (Team et al., 2023)111gemini-pro 1.0 API from https://ai.google.dev/tutorials/python_quickstart. The rate limit for the free or no-cost version is sufficient for conducting our experiments, however, we also include costs for a no-cost version with pay-as-you-go pricing, CodeLlama (Rozière et al., 2024)222https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf and Mixtral(Jiang et al., 2024)333Mixtral-8x7B-v0.1 https://huggingface.co/mistralai/Mixtral-8x7B-v0.1, yield very poor results for all of the tasks in a single-agent setting. In real-world setting any complicated tasks are rarely tackled by a single individual alone, especially when all the individuals do not possess the required expertise to perform the task. Instead, teams of engineers collaborate, with each member having a unique role (persona) and contributing unique expertise and skills to achieve the target with collective efforts. Past works on LLM agents have simulated this real-world setting by designing multi-agent frameworks (Li et al., 2024; Shen et al., 2024), combining LLM experts (Wang et al., 2023; Ding et al., 2024) and defining cascades (Chen et al., 2023; Yue et al., 2024; Zhang et al., 2023) for tasks such as code generation, reasoning, question answering, etc. Cascades refer to the chaining of LLMs in a progressive fashion, where, a weaker LLM is invoked first and if the response is not satisfactory then stronger LLMs are invoked. However, to the best of our knowledge multi-agent frameworks with open-source LLMs as agents have not be explored for engineering of ML tasks. In this paper, we address the gap of utilizing LLMs for solving ML tasks by proposing a system that leverages - (i) Multi-LLM Agents as a combination of experts using profiling, (ii) LLM Cascades, (iii) Efficient retrieval of relevant past observations, and (iv) our novel occasional ask-the-expert calls to GPT-4 444gpt-4-0125-preview https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo for planning. Our approach aims to bridge the divide between capabilities of cheaper LLMs and the requirements of complex ML tasks, offering a more cost-efficient and scalable solution. Through empirical analysis, we validate the following claims: • Our best performing multi-agent system using no-cost or low-cost versions of Gemini-Pro as the base LLM, is able to perform tasks at a fraction of the cost (on an average average $0.054 for no-cost and $0.120 for low-cost version per run per ML task in MLAgentBench Dataset) as compared to benchmarked single-agent GPT4 system presented in Huang et al. (2023) (on an average $0.931 per run per task) • With 94.2% and 87.1% cost reduction for the no-cost and low-cost Gemini-Pro versions, our best performing multi-agent system is able to yield better success rate of 32.95% averaged for all the tasks in MLAgentBench as compared to the GPT4 based single-agent system yielding 22.72% average success rate for all tasks. • Our best performing multi-agent system is able to achieve equal or better performance for 45.45% of tasks when compared to the GPT4-based Single-Agent system in Huang et al. (2023), whereas it yields comparable performance for other tasks"
https://arxiv.org/html/2411.07461v1,BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions,"We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap between descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic dense image captions with web-scale alt-text to generate factually grounded image captions. Our two-stage approach leverages large vision-language models and language models to create knowledge-augmented captions, which are then used to train a specialized VLM for scaling up the dataset. We train vision-language models on KALE and demonstrate improvements on vision-language tasks. Our experiments show the utility of KALE for training more capable and knowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/datasets/Salesforce/blip3-kale.","Dataset Scale (# of samples) Density (avg. words/caption) Knowledge-augmented? Captioner size (params) LAION-COCO1 600M 8.99 ✗ 0.5B ReCap-Datacomp-1B [15] 1.28B 49.43 ✗ 7B CapsFusion [28] 120M 22.74 ✓ 0.5B KALE 218M 67.26 ✓ 17B (stage 1) \rightarrow 2B (stage 2) Table 1: Comparison of open-source synthetic image-text datasets: We compare various datasets in terms of scale (number of samples), density (average number of words per sample), whether they are knowledge-augmented (meaning that the caption includes information found in image’s web scraped alt-text), and the size of the captioning model used to generate the descriptions. For KALE, we create an initial pool of 100M captions from a 17B parameter model and use it to distill a 2B parameter model that matches the performance of the larger 17B model. We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that advances the state of knowledge-augmented image captioning. KALE builds upon recent work in this area, particularly CapsFusion [28], which pioneered the use of large language models to fuse synthetically generated captions with alt-text to incorporate real-world knowledge. KALE makes two key contributions beyond CapsFusion: Scale and Density: While CapsFusion produced 120M samples with an average of 22.74 words per caption, KALE is significantly larger and denser. It contains 218M samples with an average of 67.26 words per caption - 1.82x the scale and nearly 3x the density of CapsFusion. Efficient Generation: We distill the knowledge augmentation process into a compact 2B parameter captioning model. This enables generation of high-quality captions comparable to much larger models like CogVLM-17B [25], but at a fraction of the cost. This efficiency allows us to scale up the dataset creation process. Our approach combines synthetic captions from VLMs with factual information from web-scale alt-text, creating rich image descriptions. We demonstrate that training on KALE improves performance across multimodal tasks compared to many previous purely synthetic or web-scraped datasets."
https://arxiv.org/html/2411.07451v1,"Optimizing Data Delivery: Insights from User Preferences on Visuals, Tables, and Text","In this work, we research user preferences to see a chart, table, or text given a question asked by the user. This enables us to understand when it is best to show a chart, table, or text to the user for the specific question. For this, we conduct a user study where users are shown a question and asked what they would prefer to see and used the data to establish that a user’s personal traits does influence the data outputs that they prefer. Understanding how user characteristics impact a user’s preferences is critical to creating data tools with a better user experience. Additionally, we investigate to what degree an LLM can be used to replicate a user’s preference with and without user preference data. Overall, these findings have significant implications pertaining to the development of data tools and the replication of human preferences using LLMs. Furthermore, this work demonstrates the potential use of LLMs to replicate user preference data which has major implications for future user modeling and personalization research.","As data and large language models (LLMs) continue to grow in prominence, it is crucial to identify the most effective ways to present data outputs, as the format, whether chart, table, or text, significantly influences how users engage with and interpret information (Tufte and Graves-Morris, 1983; Few, 2004). With datasets becoming larger and more complex, visualizations are increasingly necessary to help users digest the information effectively (Godfrey et al., 2016). The expansion of LLM use in data analysis adds another layer, making it essential to understand when these models should present different output formats. Moreover, individuals have varying preferences for data representation, driven by their unique characteristics, such as experience with data analysis and visualization, age, and work experience. This paper investigates these preferences, exploring how user characteristics shape their choice of data outputs, and how LLMs can adapt to deliver more personalized and intuitive results (Brown et al., 2020). Ultimately, by dynamically tailoring outputs based on user backgrounds, LLMs can offer a more customized and effective experience, helping users better understand and utilize data. Summary of Main Contributions. The key contributions of this work are as follows: • A comprehensive user survey and methods design. We outline the key components of the Amazon Mturk survey, detailing the respondent population, survey setup, specific user and data-related questions, and the instructions provided to participants. • An analysis of general data output preference results. The first research question examined the general population’s preferred data output for a given question, aiming to establish a baseline for data preferences without considering user characteristics. • An overview of data output preferences when organized by personal user characteristics. The 2nd research question explored how a user’s personal characteristics influence their data output preferences, focusing on experience with data analysis and visualization and their age. • An overview of data output preferences when organized by work experiences. The third research question explored how work experience, including industry and role, influences users’ data output preferences. • A comparison between human and GPT preferences. We used GPT to see if it could predict the human preference data we received throughout the study"
https://arxiv.org/html/2411.07444v1,Input-Based Ensemble-Learning Method for Dynamic Memory Configuration of Serverless Computing Functions,"In today’s Function-as-a-Service offerings, a programmer is usually responsible for configuring function memory for its successful execution, which allocates proportional function resources such as CPU and network. However, right-sizing the function memory force developers to speculate performance and make ad-hoc configuration decisions. Recent research has highlighted that a function’s input characteristics, such as input size, type and number of inputs, significantly impact its resource demand, run-time performance and costs with fluctuating workloads. This correlation further makes memory configuration a non-trivial task. On that account, an input-aware function memory allocator not only improves developer productivity by completely hiding resource-related decisions but also drives an opportunity to reduce resource wastage and offer a finer-grained cost-optimised pricing scheme. Therefore, we present MemFigLess, a serverless solution that estimates the memory requirement of a serverless function with input-awareness. The framework executes function profiling in an offline stage and trains a multi-output Random Forest Regression model on the collected metrics to invoke input-aware optimal configurations. We evaluate our work with the state-of-the-art approaches on AWS Lambda service to find that MemFigLess is able to capture the input-aware resource relationships and allocate upto 82% less resources and save up to 87% run-time costs.","The serverless computing paradigm is the latest cloud-native development model that enables application execution without the management of underlying resources. Serverless promotes the idea that a developer should be less concerned about the servers or infrastructure and focus more on productivity that adds value to the business. This shift of responsibility means offloading resource management tasks to the cloud service provider (CSP), such as resource allocation, application scaling and software updates. In the serverless landscape [1], Function-as-a-Service (FaaS) emerged as a microservices-inspired, event-driven execution model where function(s) are integrated with additional Backend-as-a-Service (BaaS) offerings like storage, networking and database services, to set-up an application. A serverless function is a stateless code fragment, executed on-demand within lightweight virtual machines (VM), microVMs or containers for short-term duration, and bills its resources as per usage. In 2014, Amazon Web Services (AWS) introduced AWS Lambda [2], [3] as its first FaaS offering, and since then, a range of FaaS services have emerged, including Google Cloud Functions [4], Azure Functions [5], and many open-source implementations such as OpenFaaS [6], Knative [7] and OpenWhisk [8]. In addition to serverless attributes such as on-demand scalability, zero idle-resource costs, and no resource management, FaaS uniquely features scale-to-zero capability where function resources are released after an extended period of inactivity, endorsing a multi-tenant resource-sharing and pay-per-use pricing model. FaaS has increasingly found its relevance in a variety of use cases like video streaming platform [9], multi-media processing [10], CI/CD pipeline [11], AI/ML inference task [12], and Large-Language-Model (LLM) query processing [13]. The operational model of FaaS hides the complex infrastructure management from end users and does not signify the absence of servers. A serverless function still requires resources, including computing, network and memory, for a successful execution. In the current FaaS implementations, a developer is responsible for requesting the right combination of resources to guarantee successful function execution. However, service providers only expose a small set of resource knobs, usually memory 111We refer to FaaS platforms like AWS Lambda that allow developers to provide only memory configuration and allocate CPU, network bandwidth, etc., in a proportional fashion. with proportionally allocated CPU, disk I/O, network bandwidth, etc. [14]. Prior studies [15][16][17] have identified that a higher memory configuration speeds up function execution and has a significant impact on its start-up performance and costs. However, the execution speedup is non-linear and has a diminishing marginal improvement with increasing memory allocations [18]. With limited observability into short-running functions and unaware of function performance, developers usually resort to speculative decisions for memory configuration or make experience-based ad-hoc decisions with an expectation to fulfil service level objectives (SLO) [19]. To validate such developer behaviour, an industry insight [20] reports the ease of controlling function execution duration via memory configuration, while 47% of production-level functions still run with the default memory configuration without exploring the entire configuration space. Additionally, selecting an optimal memory configuration from an exponentially large search space requires a careful understanding of the correlation between function performance and resource requirements. Hence, configuring the function with the right amount of memory that guarantees shorter execution times and lower execution costs is an intricate task. (a) Payload vs Duration matmul function metrics (b) Payload vs Duration linpack function metrics (c) Payload vs Memory Utilisation graph-mst function metrics (d) Payload vs Memory Utilisation matmul function metrics (e) Payload vs Memory Utilisation linpack function metrics (f) Payload vs Memory Utilisation graph-mst function metrics Figure 1: Function Metrics Insight - Payload vs Memory Utilisation vs Billed Duration TABLE I: List of collected function metrics Metric Name Description request_id unique function invocation ID payload function input parameter(s) memory_size amount of memory allocated to function memory_utilisation maximum memory measured as a percentage of the memory allocated to the function memory_used measured memory of the function sandbox billed_duration function execution time rounded to nearest millisecond billed_mb_ms total billed Gb-s, a pricing unit for function cold_start function cold start (true/false) init_duration amount of time spent in the init phase of the execution environment lifecycle function_error any function run-time error Recent research [17][21][22][23] that optimise the function resource allocation process has highlighted a drastic impact of input parameters on its performance. Additionally, a static memory configuration is used for concurrent function invocations while expecting similar performance for distinct function inputs. Therefore, setting a static memory configuration for all function invocations, regardless of their input, leads to a fluctuating performance with varying workload and input arguments. This performance unpredictability demands an input-argument-aware approach in determining the memory configuration for function invocations that balances execution cost and running time while reducing excess resource allocation. This input-based memory configuration has a two-fold effect of providing a more autonomous developer experience and a chance for CSPs to maximise resource utilisation and deliver a finer-grained, cost-effective pricing model for users. Additionally, existing efforts [17][21][22][23] to configure function resources either focus on an average-case function execution to recommend maximum used memory/resources or propose to re-run their solution for specific input parameters to optimise the memory allocation process. This may lead to higher run-time costs and resource wastage and on the other hand, running multiple models for previously unseen input values extends the data collection process as well as increases the model training and tuning complexity. Therefore, a solution is warranted that captures the relationship of input parameters with function resources to precisely model and predict the required memory configuration for successful execution and reducing excess resource allocation. To this end, we present MemFigLess, an end-to-end estimation and function memory allocation framework that makes input-aware memory configuration decisions for a serverless function. MemFigLess takes as an input the function details, such as the representative function input arguments, expected running time and cost SLOs and a range of memory allocations to explore. The framework executes an offline profiling loop to take advantage of a robust tree-based ensemble learning technique, multi-output Random Forest Regression (RFR), which analyses the relationship between input parameters and other function metrics such as execution time, billed cost, and function memory requirement. The RFR model is then exploited in an online fashion to make an optimal selection of memory configuration for individual function invocations. Additionally, the framework provides a feedback loop to re-train the model in a sliding-window manner with a new set of collected metrics to capture the performance variation."
https://arxiv.org/html/2411.07441v1,Automatically Detecting Online Deceptive Patterns in Real-time,"Deceptive patterns (DPs) in digital interfaces manipulate users into making unintended decisions, exploiting cognitive biases and psychological vulnerabilities. These patterns have become ubiquitous across various digital platforms. While efforts to mitigate DPs have emerged from legal and technical perspectives, a significant gap in usable solutions that empower users to identify and make informed decisions about DPs in real-time remains. In this work, we introduce AutoBot, an automated, deceptive pattern detector that analyzes websites’ visual appearances using machine learning techniques to identify and notify users of DPs in real-time. AutoBot employs a two-staged pipeline that processes website screenshots, identifying interactable elements and extracting textual features without relying on HTML structure. By leveraging a custom language model, AutoBot understands the context surrounding these elements to determine the presence of deceptive patterns. We implement AutoBot as a lightweight Chrome browser extension that performs all analyses locally, minimizing latency and preserving user privacy. Through extensive evaluation, we demonstrate AutoBot’s effectiveness in enhancing users’ ability to navigate digital environments safely while providing a valuable tool for regulators to assess and enforce compliance with DP regulations.","Deceptive patterns (DPs) are design elements that manipulate users into making unintended decisions while interacting with interfaces on applications or websites. These patterns exploit cognitive biases and psychological vulnerabilities to influence user behavior, often in ways that benefit the service provider at the user’s expense. They have become increasingly prevalent across digital environments, significantly impacting user experiences on social media, mobile devices, cookie consent banners, and even gaming applications [33]. Prior works have shown that DPs can result in financial loss [42], privacy breaches [8], or exploitation of vulnerable groups, including children [46]. A typical example of a DP is the intentionally convoluted subscription cancellation process, where users must navigate through multiple obscure options to terminate a service, as seen on platforms like www.dailymail.co.uk. Efforts to mitigate the effect of DPs have emerged from the policy and the technical sides. Regarding policy, regulations like CPRA [15] and the GDPR [4] have released guidance on deceptive patterns. On the technical front, researchers have explored classifying the existing types of deceptive patterns [11, 21, 33, 48, 44, 7, 16, 39, 32], assessing their effectiveness [30], and compiling different deceptive pattern cases [12]. Despite these ongoing efforts, deceptive patterns continue to pose significant challenges for both users and regulators. Users frequently encounter DPs in their digital interactions. For instance, sponsored advertisements are strategically placed at the top of search results, creating a false impression that they are the most relevant or popular items. Furthermore, regulators often lack the tools to assess and enforce compliance with regulations concerning deceptive patterns effectively. This technological gap leaves users vulnerable to sophisticated DPs. In this work, we propose a new paradigm to bridge this gap and improve the usability of online websites by developing a solution to detect deceptive patterns and automatically warn users in real time. Our solution makes users aware of deceptive patterns as they browse and enables regulators to assess and evaluate online services for deceptive patterns. Achieving this objective requires us to overcome several challenges. First, the lack of a standardized taxonomy and usable datasets complicates classification efforts. Second, the vast diversity of web designs and the ingenuity of designers in creating new deceptive techniques have led to a trade-off between scalability and accuracy in detection methods. For instance, existing solutions like disabling third-party cookies or using filter lists have proven inadequate in preventing user tracking, as highlighted by Chen et al. [16]. Third, user-friendly solutions must operate with minimal infrastructure to be practical for widespread adoption. We first create a standardized taxonomy of deceptive patterns to realize our objectives, building upon existing taxonomies. Leveraging this standardized taxonomy, we then built AutoBot, an automated, deceptive pattern detector that analyzes the website’s visual appearances, uses machine learning tools to identify deceptive patterns, and brings deceptive patterns to users’ attention in real-time. Specifically, AutoBot relies on an invariant behavior of deceptive patterns: the visual representation and how users perceive them. Using this invariant behavior, AutoBot applies a two-staged pipeline that identifies the deceptive patterns present in the website’s screenshot. First, AutoBot analyzes the website screenshot, identifying interactable elements using visual analysis and extracting textual features without relying on the HTML structure. We note that here, we rely on the insight that deceptive patterns work by manipulating what the users perceive; thereby, analyzing their visual appearance provides a comprehensive signal to be able to identify them. Next, AutoBot leverages a custom language model to understand the context surrounding these elements, including color, font size, and text, to determine the presence of deceptive patterns. We created an annotated dataset for deceptive patterns and designed a new training paradigm to fine-tune language models. We also note here that empirically, we have found that off-the-shelf language models like Gemini and ChatGPT often fail to detect deceptive patterns from screenshots, as shown in Figure 2. We showcase the usability of AutoBot by building a Chrome browser extension that (1) detects the deceptive patterns in real-time by performing the complete analysis locally and (2) annotates the current webpage to warn the users about the deceptive patterns. We also characterize the performance of AutoBot on several devices and find that the latency is less than one second on modern devices, which minimally impacts the usability of the websites. In this work, we put forth AutoBot, a automated solution to detect deceptive patterns on the web and make the following contribution: 1. We compile an annotated large-scale deceptive design dataset (D3) based on carefully curated taxonomy. We also introduce a novel pipeline to generate a diverse dataset of synthetically generated websites. 2. We show that our system, AutoBot, can detect deceptive designs present on the web and classify them per our taxonomy, with high recall resulting in most deceptive patterns being detected. 3. We leverage recent advances in optimizing LLM inference on consumer hardware to run our system, AutoBot, on device with real-time inference. Figure 1: Here we show how AutoBot highlights and informs the users about deceptive patterns on a site."
https://arxiv.org/html/2411.07404v1,"Controllable Context Sensitivity
and the Knob Behind It","When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context ( Paris is in England) and a question ( Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model’s performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior.","Language models are often prompted with a query and preceding context, e.g., in settings of in-context learning, retrieval-augmented generation, or document analysis. In such scenarios, the language model needs to integrate information from both the context and its prior knowledge stored in its parameters. In some cases, we may prefer the model to rely more on the context, e.g., to avoid hallucinating responses that may be based on outdated prior knowledge (Zhang et al., 2023); however, in other cases, we may prefer the model to rely more on its prior knowledge, e.g., to avoid being misled by misinformation provided in the context (Hong et al., 2024). As a motivating example, consider a document analysis setting in which a language model is asked to help understand an opinion article in a newspaper. It might first be asked to summarize, e.g., What is the main argument of this article?. In this case, the model should rely heavily on the context, i.e., the text of the article. Then, one might ask: What are some criticisms of this argument?. To answer this, the model ought to be skeptical; an opinion article may be written very authoritatively as if its arguments are established fact, or it may make some misleading claims to support its argument. To answer this critically, the language model must draw more upon its prior knowledge of the issue and related opinions than blindly following the context. More broadly, because the degree of context sensitivity depends highly on the use case, it would be desirable to be able to specify how much and whether the model should be influenced by the context versus its prior knowledge. Studies on the tension between context and prior knowledge have primarily focused on the setting of knowledge conflicts (Longpre et al., 2021), in which a given context directly contradicts information assumed to be in a model’s prior knowledge about a given query. For example, a language model trained on a sufficient amount of data should be able to reply to the query What’s the capital of France? with Paris. However, if the context The capital of France is London. is prepended to the query, the model needs to decide whether to respond based on the context ( London) or its prior knowledge ( Paris). Prior studies (Longpre et al., 2021; Du et al., 2024; Monea et al., 2024; Ortu et al., 2024) have shown that models will prefer drawing from context for some questions and prior knowledge from others; however, the exact manner by means of which the model selects among the sources is not well-understood. To this question of how, we hypothesize that there is a simple fundamental mechanism within the language model that facilitates the binary decision of whether to rely on the context or the prior knowledge. To guide our search for such a mechanism, we design and execute a structured recipe. First, we create the controllable context sensitivity (CCS) task which augments the standard knowledge-conflict setting with an intent, such as Ignore the context or Listen to the context. By disambiguating whether the model should follow context or prior knowledge through a simple addition to the prompt, we are able to identify and evaluate its behavior in both modes for the same context–query pair. We adapt models for this task using fine-tuning and in-context learning, then evaluate them on in-domain and out-of-domain test sets to assess whether they have developed a deeper ability to choose between context and prior knowledge beyond surface-level heuristics. In our case study on the Llama-3.1-8B family (Dubey et al., 2024), we find that both fine-tuning and in-context learning are moderately effective, with models excelling on in-domain test sets and significantly improving over zero shot baselines on out-of-domain test sets. Armed with models that can perform the CCS task reasonably well, we then explore the mechanisms that facilitate their behavior in this task. We hypothesize that for a model to solve this task, it must execute at least three high-level steps (not in any particular order): extracting an answer from prior knowledge, extracting an answer from the context, and deciding to answer with the context answer or the prior answer. We then seek to identify layers that may contain the model’s computations that are aligned with each step. To do so, we develop an algorithm that uses tools from mechanistic interpretability to find a targeted subset of layers at which activation patching (Meng et al., 2022) can switch a model from preferring the answer in the context to preferring the answer in its prior knowledge and vice versa. Then, building on ideas from distributed alignment search (Geiger et al., 2024), we identify a knob for the model’s decision between following context or prior in the form of a one-dimensional subspace. Despite locating such a knob on an instruction-tuned model fine-tuned on this task that states explicit intents, we show that it is even effective on non-finetuned and base models of the same family for prompts that do not state the intent. Furthermore, we show strong evidence that for models good at the CCS task, the two intents correspond to two distinct values in that subspace, while bad models fail to exhibit this distinction. We repeat this process for Gemma-2 9B and Mistral-v0.3 7B to find a similar story. Our results suggest that, across many types of large language models, one fundamental mechanism represented by a value in a one-dimensional subspace facilitates their ability to decide between following the context or its prior knowledge. These findings move toward developing more robust language models with controllable levels of reliance on context and prior knowledge. They further highlight how investigating models at a mechanistic level can yield high-quality interventions to control a model’s behavior."
https://arxiv.org/html/2411.07398v1,Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical Concern-related App Reviews,"With the increasing proliferation of mobile applications in our everyday experiences, the concerns surrounding ethics have surged significantly. Users generally communicate their feedback, report issues, and suggest new functionalities in application (app) reviews, frequently emphasizing safety, privacy, and accountability concerns. Incorporating these reviews is essential to developing successful products. However, app reviews related to ethical concerns generally use domain-specific language and are expressed using a more varied vocabulary. Thus making automated ethical concern-related app review extraction a challenging and time-consuming effort.This study proposes a novel Natural Language Processing (NLP) based approach that combines Natural Language Inference (NLI), which provides a deep comprehension of language nuances, and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract ethical concern-related app reviews at scale. Utilizing 43,647 app reviews from the mental health domain, the proposed methodology 1) Evaluates four NLI models to extract potential privacy reviews and compares the results of domain-specific privacy hypotheses with generic privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to privacy concerns; and 3) Uses the best NLI and LLM models further to extract new privacy reviews from the dataset. Results show that the DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the classification of app reviews. Then, using NLI+LLM, an additional 1,008 new privacy-related reviews were extracted that were not identified through the keyword-based approach in previous research, thus demonstrating the effectiveness of the proposed approach.","Mobile applications are created with specific user goals in focus [1]. A user goal can be defined as any conceptual aim the given system should fulfill [2]. For instance, Sharing Economy applications (like Uber and Airbnb) aim to enhance social capital and stimulate economic development in resource-limited areas [3]. In contrast, the goal of Health&Fitness applications is to encourage healthy habits among both children and adults [4]. However, due to intense market rivalry, the app development cycle often aims to produce functional applications within brief intervals (such as days or weeks), leading developers to stray from their initial objectives frequently. These divergences frequently bring forth ethical concerns such as declining mental health, bias, privacy violations, and manipulation [5, 6, 7, 8]. Applications that fail to sufficiently consider their users’ ethical concerns are often labeled as untrustworthy or even deserted by their users [9]. Thus, for applications to endure the market’s scrutiny, developers continuously keep track of user feedback through ratings and reviews found in app marketplaces (like Google Play Store). They typically analyze user feedback to gather insights on bug reports, feature suggestions, connectivity issues, resource consumption challenges (e.g., battery life), and interface problems [10, 11, 12, 13]. Numerous studies have investigated user perspectives on ethical concerns within software applications. Research conducted by Besmer et al. [14] and Nema et al. [15] underscores users’ concerns regarding privacy breaches and data security measures in mobile applications. The emergence of discriminatory algorithms and the potential for bias in software functionalities are also significant areas of concern, as highlighted by the findings of Tushev et al. [16] and Olson et al. [17]. Furthermore, manipulative design tactics that coerce users or take advantage of psychological weaknesses are increasingly worrisome, as noted by Olson et al. [18]. However, these investigations largely depend on keyword-based sampling from app reviews, which limits the ethical issues users address to a predetermined set of terms. To overcome this limitation, Harkous et al. [19] suggest using the NLI method. However, they rely on a set of generic privacy hypotheses (derived from generic privacy concepts) overlooking the fact that users’ ethical concerns are domain-dependent [1]. For instance, individuals using ridesharing services (e.g., Uber and Lyft) may raise concerns about the constant tracking of their location, while those utilizing financial platforms (e.g., Robinhood and Coinbase) might express concerns regarding the sharing of their social security or banking details with the application. Additionally, NLI with generic hypotheses identifies a high number of false positives (FP) that require further manual analysis to identify ethical concern-related reviews [19]. To address these challenges, in this paper, we propose a novel Natural Language Processing (NLP) based hybrid approach that combines Natural Language Inference (NLI) and a decoder-only Large Language Model (LLM) to mine ethical concern-related app reviews at scale. We use NLI with domain-specific hypotheses to determine potential ethical concern-related reviews and further process these reviews using LLMs to extract ethical concern-related app reviews. The main contributions of this study can be summarized as follows. • To the best of our knowledge, this is the first hybrid approach that utilizes NLI and LLM along with domain-specific privacy hypotheses to extract ethical concern-related app reviews. NLI+LLM demonstrated better results compared to generic privacy hypotheses utilized by Harkous et al. [19]. • We develop domain-specific hypotheses based on the Mental Health (domain-specific) privacy concepts provided by Iwaya et al. [20]. • We demonstrate that our proposed hybrid approach (NLI+LLM) can extract concern-related reviews that do not contain predefined wordings used in the keyword-based method in Ebrahimi et al [1]. • We open source our source code and dataset111https://github.com/AakashSorathiya/CHyMER of 1,008 privacy-related reviews (results from our study) that remained unidentified by the previous Ebrahimi et al’s [1] study which used a keyword-based approach. The rest of the paper is organized as follows. To determine the research gaps, Section II discusses related work. Section III presents the motivation for our research through examples. We define our research questions (RQs) and explain preliminaries in Section IV and V, respectively. In Section VI, we describe the dataset and explain our methodology in Section VII. Section VIII shows and discusses the results of our investigation. Section IX lists various threats to the validity of our investigation and Section X presents concluding remarks and future directions."
https://arxiv.org/html/2411.07392v1,Feature-Space Semantic Invariance: Enhanced OOD Detection for Open-Set Domain Generalization,"Open-set domain generalization addresses a real-world challenge: training a model to generalize across unseen domains (domain generalization) while also detecting samples from unknown classes not encountered during training (open-set recognition). However, most existing approaches tackle these issues separately, limiting their practical applicability. To overcome this limitation, we propose a unified framework for open-set domain generalization by introducing Feature-space Semantic Invariance (FSI). FSI maintains semantic consistency across different domains within the feature space, enabling more accurate detection of OOD instances in unseen domains. Additionally, we adapt a generative model to produce synthetic data with novel domain styles or class labels, enhancing model robustness. Initial experiments show that our method improves AUROC by 9.1% to 18.9% on ColoredMNIST, while also significantly increasing in-distribution classification accuracy.","Open-set domain generalization addresses the dual challenge of domain generalization (DG) and open-set recognition (OSR) by aiming to classify in-distribution (ID) instances under domain shifts while simultaneously detecting samples from unknown classes in the unseen target domain. Despite this setting better reflecting real-world scenarios, current methods often treat it as two separate problems due to its complexities. For instance, OOD samples with styles similar to IDs are difficult to detect, while ID samples in a shifted target domain are prone to being misclassified as OOD. Research in open-set domain generalization is relatively sparse, with only a few works [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] making progress compared to conventional approaches that handle either DG or OSR in isolation. However, these existing methods typically rely heavily on OOD training data [11], which is rarely available in training, or utilize meta-learning strategies and complex network architectures [12, 13, 14] that lack compatibility with state-of-the-art OOD detection techniques, such as Energy [15] and DDU [16]. To address these limitations, we propose an advanced technique that enforces feature-space semantic invariance to learn high-quality domain-invariant features, and leverages synthetic OOD data to increase the separability between ID and OOD. Our main contributions are as follows: • We introduce Feature-space Semantic Invariance (FSI) to enforce semantic consistency across domains within the feature space. By aligning semantic features across augmented samples, FSI enables the model to learn high-quality domain-invariant features, enhancing its generalizability to unseen domains. • We incorporate synthetic OODs generated from ID samples to establish clearer decision boundaries between ID and OOD instances, significantly enhancing the model’s robustness against OOD. • Preliminary experiments show our approach improves AUROC by 9.1% to 18.9% on the ColoredMNIST dataset, with a notable increase in ID classification accuracy. These results validate the model’s potential in open-set domain generalization, positioning it as a viable solution for applications with novel domains and classes. Figure 1: An illustration of the proposed framework. The framework samples instances from different domains, each characterized by unique variations (e.g., colors), aiming to learn a domain-invariant feature extractor that can be combined with state-of-the-art semantic OOD detectors to effectively address both domain generalization and open-set recognition challenges."
https://arxiv.org/html/2411.07391v1,Federated Learning Client Pruning for Noisy Labels,"Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, existing FL methods often assume clean annotated datasets, impractical for resource-constrained edge devices. In reality, noisy labels are prevalent, posing significant challenges to FL performance. Prior approaches attempt label correction and robust training techniques but exhibit limited efficacy, particularly under high noise levels. This paper introduces ClipFL (Federated Learning Client Pruning), a novel framework addressing noisy labels from a fresh perspective. ClipFL identifies and excludes noisy clients based on their performance on a clean validation dataset, tracked using a Noise Candidacy Score (NCS). The framework comprises three phases: pre-client pruning to identify potential noisy clients and calculate their NCS, client pruning to exclude a percentage of clients with the highest NCS, and post-client pruning for fine-tuning the global model with standard FL on clean clients. Empirical evaluation demonstrates ClipFL’s efficacy across diverse datasets and noise levels, achieving accurate noisy client identification, superior performance, faster convergence, and reduced communication costs compared to state-of-the-art FL methods. Our code is available at https://github.com/MMorafah/ClipFL.","Edge devices such as IoTs and mobile devices are increasingly ubiquitous, constituting a new computational platform for machine learning. Despite holding vast real-world data, these billions of devices often withhold their data due to privacy concerns. Federated Learning (FL) emerges as a decentralized machine learning paradigm, enabling model training with the collaboration of multiple clients while preserving privacy [30, 16]. FL shows promise in enhancing model performance without necessitating data sharing and finds applications across diverse domains [16, 45, 34, 6, 13]. However, FL encounters significant performance degradation in the presence of data heterogeneity [11, 25, 61]. Recent advancements introduce FL optimizers tailored to address data heterogeneity, achieving faster convergence [24, 18, 50, 38]. Despite these advancements, the majority of prior works operate under the assumption of accurately labeled and clean client data. In practice, however, acquiring precisely annotated clean datasets is arduous and resource-intensive, especially for edge devices lacking ample resources. Consequently, the labels in their datasets often contain noise. Unfortunately, FL experiences substantial performance degradation when confronted with noisy clients [56]. Thus, developing FL frameworks resilient to noisy labels is imperative. Several prior works propose methods to identify and rectify noisy samples using the global model’s predictions [53, 56]. For instance, FedCorr [53] presents a multi-stage FL framework that identifies noisy clients by measuring the local intrinsic dimensionality (LID) and corrects noisy labels using global model predictions. In order to control the negative impact of noisy client in achieving a well-trained reliable global model prior to label correction stage, they incorporate several techniques including client fraction scheduling scheme and local proximal regularization with mix-up. However, this approach heavily relies on a well-performing global model, which is challenging to obtain in the presence of data heterogeneity and noisy clients, leading to inaccurate label correction and suboptimal performance. Other approaches aim to mitigate the impact of noisy clients through client weighting strategies and robust local training methods [9, 57, 15]. For example, RHFL [9] utilizes symmetric cross-entropy loss during local training and introduces a client confidence re-weighting scheme to counteract the adverse effects of noisy labels during collaborative learning. However, these methods demonstrate limited efficacy and poor convergence, especially under high noise levels. In this paper, we address the challenge of noisy labels in FL by adopting an alternative approach compared to prior works. Rather than mitigating or correcting noisy clients, we propose a novel “Federated Learning Client Pruning” framework called ClipFL. ClipFL identifies noisy clients and excludes them from the FL training process. Our framework comprises of three phases. In the initial phase, we identify noisy clients based on their performance on a clean validation dataset, tracking the frequency of noisy identifications as Noise Candidacy Score (NCS) of each client. To mitigate the negative impact of noisy clients during server-side model aggregation, we only aggregate the top-m clients with the highest validation accuracy. Subsequently, in the second phase, we prune p\% of the clients with the highest noise candidacy score (NCS) in a one-shot manner. Finally, in the third stage, we conduct standard FL on the remaining clean clients to further refine the global model. (a) Prior works typically involve incorporating noisy clients into FL and addressing their negative impact through methods such as correcting noisy labels, re-weighting noisy clients, or designing robust local training methods. (b) In contrast to prior works, ClipFL introduces the Noise Candidacy Score (NCS) for each client, enabling robust identification of noisy clients from the FL process. Figure 1. Comparison between our approach and prior works: Prior works often face challenges related to poor convergence and reliance on a well-performing global model as a pseudo-labeler that corrects noisy labels, which can be difficult to obtain, especially in the presence of high noise levels and data heterogeneity (see Section 5.3). In contrast, ClipFL offers consistent performance improvements and reduces communication costs by robustly identifying noisy clients and excluding them from the FL process. We validate ClipFL on diverse datasets with varying noise levels for both IID and Non-IID data partitions, yielding several key observations: (1) ClipFL accurately identifies noisy clients with at least 80% accuracy on most of the cases. (2) ClipFL outperforms state-of-the-art (SOTA) FL optimizers without excluding noisy clients. (3) ClipFL surpasses existing FL methods addressing noisy labels, underscoring the effectiveness of our excluding approach. (4) ClipFL demonstrates faster convergence and reduced communication costs compared to both SOTA vanilla FL optimizers and FL methods designed to counteract noisy labels. We make our code publicly available at https://github.com/MMorafah/ClipFL. Contribution. Our contributions are threefold: • We introduce ClipFL, a novel federated learning client excluding method to address the challenge of noisy labels in FL. • ClipFL distinguishes clean clients from noisy ones through evaluation on a clean validation dataset and aggregates only the top-performing clients to mitigate the impact of noisy clients. • We empirically evaluate ClipFL across different datasets with varying noise levels for both IID and Non-IID data partitions, demonstrating significant performance improvements and reduced communication costs over SOTA FL methods. Organization. The rest of the paper is organized as follows: In Section 2, we discuss related works. Section 3 presents the background and problem formulation. Our proposed method is introduced in Section 4. Section 5 details our experimental results. We discuss our implementation and hyperparameters in Section 6. Finally, we conclude our work in Section 7."
https://arxiv.org/html/2411.07388v1,"Firing Rate Models as Associative Memory: 
Excitatory-Inhibitory Balance for Robust Retrieval","Firing rate models are dynamical systems widely used in applied and theoretical neuroscience to describe local cortical dynamics in neuronal populations. By providing a macroscopic perspective of neuronal activity, these models are essential for investigating oscillatory phenomena, chaotic behavior, and associative memory processes. Despite their widespread use, the application of firing rate models to associative memory networks has received limited mathematical exploration, and most existing studies are focused on specific models. Conversely, well-established associative memory designs, such as Hopfield networks, lack key biologically-relevant features intrinsic to firing rate models, including positivity and interpretable synaptic matrices that reflect excitatory and inhibitory interactions. To address this gap, we propose a general framework that ensures the emergence of re-scaled memory patterns as stable equilibria in the firing rate dynamics. Furthermore, we analyze the conditions under which the memories are locally and globally asymptotically stable, providing insights into constructing biologically-plausible and robust systems for associative memory retrieval.","The modelling of associative memory processes began in the early 1970’s and 1980’s with the mathematical formalization of Amari (Amari, 1972, 1977) and of Grossberg (Grossberg, 1983) and the elegant and explicit construction of Hopfield (Hopfield, 1982, 1984). The authors drew inspiration from the early successes of statistical physics in the description of glassy phenomena (Sherrington & Kirkpatrick, 1975; Mezard, Parisi & Virasoro, 1987), leveraging the average properties of simple interconnected units. The key idea was to define a network of neuron-like computational units, similar to those studied by McCulloch and Pitts (McCulloch & Pitts, 1943), and investigate memory retrieval as emergent processes. Within this context, the authors conceptualized associative memory networks as dynamical systems defined by ordinary differential equations (ODEs) having as stable equilibrium points the memory patterns to retrieve. The first key contribution was the definition of a Lyapunov function for the associative memory system that ensured global asymptotic convergence to the equilibria of the system. Thus, any initial condition for the system would lie in the basin of attraction of one of these equilibria, and the system trajectory will inevitably evolve towards it. The second key contribution was the explicit definition of the set of memory vectors as binary patterns taking values in \{-1,+1\}, analogously to ferromagnets in spin glasses. The binary representation of the memory vectors allowed for the explicit design of a synaptic matrix, that ensured the system’s equilibria precisely matched the intended memories. The effective combination of the two key contributions has catalyzed a wealth of subsequent research, both analytical and numerical, focusing on the fundamental properties of associative memory systems. Notably, many authors have studied the storage capacity (Amit, Gutfreund & Sompolinsky, 1987a, b; McEliece et al., 1987; Petritis, 1995) of associative memory systems, that is the maximum number of memories that can be stored in the synaptic matrix without compromising their stability. Subsequent works (Tsodyks & Feigel’man, 1988; Treves, 1990; Amit & Tsodyks, 1991) have extended beyond the binary spin structure proposed by Hopfield, enabling binary positive activations \{0,1\} and low levels of neural activity. These works retain the dynamic framework initially proposed by Grossberg and Hopfield, hereafter referred to as voltage equations, but apply non-negative activation functions to yield neuron firing rates. While positive activations allow interpreting synaptic matrix components as excitatory or inhibitory, the relationship between voltages and firing rates remains arbitrary and highly dependent on network parameters. This limitation has led associative memory researchers to explore models that directly connect empirically measurable quantities, like firing rates, with one another. Parallel to the advancements in the modeling of associative memory networks, detailed biophysical models of cortical circuits have received an increasing amount of attention due to their capability of generating synthetic data of EEG recordings, thus providing insight to experimentalists on the roots of the measured quantities. Beginning with the groundbreaking description of the neuron biochemical response by Hodgkin and Huxley (Hodgkin & Huxley, 1952), the characterization of neuronal properties by means of dynamical systems has become ever more pervasive. In the beginning, the mathematical characterization of neurons and neural processes focused on the extensive treatment of the microscopic properties, such as gating and diffusion of ions, and branched into the well known FitzHugh-Nagumo (FitzHugh, 1961) and Morris-Lecar (Morris & Lecar, 1981) models. However, the mathematical complexity of these models, combined with the computational limitations, constrained researchers to small-scale studies involving only a handful of neurons. To address these challenges, substantial efforts were devoted to the derivation of simplified mathematical models amenable to analytical treatment and large scale simulation. This effort led to the establishment of the class of models widely known as Integrate-and-Fire (Bresslof & Coombes, 2000; Brunel, 2000; Burkitt, 2006a, b). Despite their utility, using Integrate-and-Fire models for memory retrieval remains challenging due to the hybrid nature of their dynamics, switching between continuous dynamics and a hard reset to a given initial condition. The key idea for the formulation of biologically plausible associative memory systems was to consider an Integrate-and-Fire model and to average neural spikes over fixed time windows to derive a rate of firing (Ermentrout & Terman, 2010; Gerstner, 2014) for the neuron. The core of the newly proposed biologically plausible model, referred to as the firing rate model, lies in its use of a non-negative activation function that directly processes firing rates rather than membrane voltages. This approach effectively links two empirical observables across clusters of neurons. Despite its potential importance for neuroscience, designing firing rate systems—such as synaptic matrices and activation functions—so that specific memories appear as locally stable equilibria remains an underexplored area (Dayan & Abbott, 2005, Section 7.4). The primary contributions of this paper are (i) the design of a synaptic matrix that encodes memories as equilibrium points within the firing rate system, applicable to arbitrary activation functions, and (ii) an analysis of both local and global stability, building upon and expanding the foundational work presented in (Dayan & Abbott, 2005, Section 7.4). Specifically, we present a method to design a synaptic matrix of the firing rate model that guarantees the retrieval of a rescaled version of the prototypical memories. These prototypical memories are assumed to be equally sparse and equally correlated binary vectors, meaning that they share a common number of ones and of overlapping entries. In particular, our first theorem states the necessary and sufficient conditions that ensure the existence of the rescaled prototypical memories as equilibrium points for the firing rate dynamics. The proposed construction admits a biological interpretation of the synaptic components in terms of excitation, inhibition, and homeostatic regulation. Moreover, we frame the canonical prescription of Dayan & Abbott (Dayan & Abbott, 2005, Section 7.4) as a special case of our synaptic matrix construction. Additionally, we show that the emergence of “anti-memories” is possible only for pathological cases reducible to the use of Hopfield-type synaptic matrix, and explore the existence of spurious equilibria, particularly homogeneous ones. The second theorem goes on to establish sufficient conditions for the local asymptotic stability of the rescaled prototypical memories and, leveraging results from Grossberg (Grossberg, 1983) and Hopfield (Hopfield, 1984), we proceed by defining an energy function to analyze the global behavior of trajectories. Finally, we investigate numerically the tightness of these stability conditions and visualize the energy landscapes for two relevant examples. Notably, simulations reveal how the choice of a negative homeostatic strength results in wider stability regions over the space of parameters, compatibly with the canonical sign choice for the homeostatic term found in the literature. To enhance readability, the proofs of the technical results are deferred to the Appendix. Notation: We let n×m denote the set of n\times m matrices with real entries. The symbol \mathbbold{1}_{n} indicates an n-dimensional vectors of ones and I_{n} the n\times n identity matrix. Given a matrix A\in{}^{n\times m}, A^{\top} is the transpose of A. For a symmetric matrix A=A^{\top}, we write A\succ 0 (A\succeq 0) if A is positive definite (positive semidefinite, respectively). Moreover for two symmetric matrices A,B, we write A\succ B (A\succeq B) if A-B\succ 0 (A-B\succeq 0). Given a vector x\in{}^{n}, \mathrm{diag}(x) is the diagonal matrix with the entries of x as diagonal entries. If f(x) is a real-valued function, f^{\prime}(x) denotes the derivative of f. A function is weakly increasing if f(x_{1})\leq f(x_{2}) for all x_{1},x_{2}\in\real with x_{1}<x_{2} and strictly increasing if f(x_{1})<f(x_{2}) for all x_{1},x_{2}\in\real with x_{1}<x_{2}."
https://arxiv.org/html/2411.07376v1,Ensemble Learning for Microbubble Localization in Super-Resolution Ultrasound,"Super-resolution ultrasound (SR-US) is a powerful imaging technique for capturing microvasculature and blood flow at high spatial resolution. However, accurate microbubble (MB) localization remains a key challenge, as errors in localization can propagate through subsequent stages of the super-resolution process, affecting overall performance. In this paper, we explore the potential of ensemble learning techniques to enhance MB localization by increasing detection sensitivity and reducing false positives. Our study evaluates the effectiveness of ensemble methods on both in vivo and simulated outputs of a Deformable DEtection TRansformer (Deformable DETR) network. As a result of our study, we are able to demonstrate the advantages of these ensemble approaches by showing improved precision and recall in MB detection and offering insights into their application in SR-US.","Traditional ultrasound techniques encounter an inherent compromise between image clarity and penetration. Enhancing resolution through increasing frequency of the propagated waves, comes at the cost of increased tissue absorption, limiting the depth of effective imaging. Conversely, lower frequencies allow for deeper penetration but yield less detailed images [1]. Following the break-throughs in optical super-resolution imaging [2], Ultrasound Localization Microscopy (ULM) has been proposed, which employing the strong backscatter echo properties of ultrasound contrast agent microbubbles (MBs) can, theoretically, provide a ten-fold improvement in ultrasound blood flow imaging [3]. This advancement has significantly enhanced our understanding of disease states and progression, particularly in functional brain imaging [4], cancer [5] and diabetes [6]. To create super-resolution ultrasound (SR-US) images of the vascular maps, MBs injected into the bloodstream are identified, localized, and frequently monitored. The MBs, confined to blood vessels, enable detailed visualization of vascular structures [7]. The precision of MB localization in SR-US faces several challenges, such as Point Spread Functions (PSFs) distortion caused by MB vicinity, near-field imaging effects, variations in tissue speed of sound and attenuation [8, 9]. These factors collectively contribute to alterations in PSF shape, complicating accurate localization. On the other hand, while higher MB concentrations can reduce acquisition time, they may also lead to reduced localization precision as the MB signals coincide [10]. Addressing these issues requires careful consideration of imaging parameters, processing techniques, and potentially the development of novel algorithms that can account for these various factors to maintain localization accuracy across diverse imaging conditions. In the past few years, several methods of MB localization have been developed and evaluated. Conventional localization methods rely on the assumption of isolated scatterers within the region of interest [11]. However, this assumption breaks down as MB trajectories intersect. To tackle the problem of overlapping PSFs while allowing for higher MB concentrations and shorter acquisition times, various Deep Learning (DL) networks were introduced which rely on learning complex patterns from simulations and incorporating temporal context into the localization process. A common framework employed in many DL methods is the Convolutional Neural Network (CNN) with an encoder-decoder architecture with variations in the upsampling techniques and the specific building blocks employed for feature encoding [12, 13, 14]. Shin et al. [15] introduced LOCA-ULM, which enhances localization accuracy by incorporating temporal context from adjacent frames employing two U-nets in sequence. Building on this concept, Pustovalov et al. [16] recently proposed an architecture inspired by DECODE [17]. Their method utilizes 3D CNNs to incorporate temporal context, potentially allowing for the integration of information from a larger number of frames. Transformer-based architectures have gained popularity in ULM due to their effectiveness in capturing long-range relationships in images. The ULM-TransUnet framework [18] combines the strengths of U-Net architectures and Transformer models, enhancing performance through multi-scale feature integration and long-term contextual dependencies. Previously, we employed a DEformable DEtection TRansformer (DEDETR) solution for MB detection to better handle PSF deformations and variations in appearance, resulting in improved detection performance [19, 20]. Nevertheless, the effectiveness of any model in real-world applications is heavily based on meeting specific criteria related to data quality and hyper-parameter optimization. In these situations, ensemble learning can offer a valuable solution by combining multiple weaker models to achieve desired results [21, 22]. By leveraging the strengths of multiple models, ensembles can often achieve performance levels that surpass those of individual models while manifesting increased robustness and generalization capabilities. Beyond traditional ensemble methods, researchers have developed sophisticated techniques to refine the outputs of detection models by eliminating redundant bounding boxes. These approaches include Non-Maximum Suppression (NMS), Soft-NMS [23], NMW (Non-Maximum Weighting) [24], and Weighted Box Fusion (WBF) [25]. Each of these methods offers unique strategies for consolidating overlapping detections and improving overall detection accuracy. While combining multiple machine learning models is a widespread technique for enhancing overall performance across various applications, its implementation for ULM applications presents unique challenges. For one, most of the aforementioned techniques do not detect MBs as bounding boxes and rather focus on finding the centroid of the MBs. Secondly, choosing models that while capturing a variety of different shapes and sizes of MBs in different densities, also don’t overlap and repeat themselves. In this paper, we: 1. Propose a versatile ensemble framework for MB detection based on a transformer-based solution, i.e. DEDETR. This approach integrates multiple detectors to leverage their complementary strengths and to optimize the ensemble’s decision-making process, enhancing both the accuracy and robustness of our MB detection results. 2. Perform extensive empirical evaluation of the proposed ensemble framework and its associated ensemble strategies. Through experimentation across both simulation and in vivo datasets, we demonstrate notable improvement in the details of the generated super-resolution maps, including improved precision, recall, and RMSE scores. 3. Analyze the framework’s ability to mitigate individual detector weaknesses and adapt to diverse object scales and appearances, showcasing its effectiveness, generalizability, and robustness. (a) NMS (b) NMSW (c) Soft NMS (d) WBF (e) DEDETR (f) GT Fig. 1: Full-view SR maps of the simulation test dataset for different methods. (A) (B) (C) DEDETR (Patch 4) WBF GT Fig. 2: Zoomed-in boxes from different simulation SR maps, showing the results from each method (indicated by columns) for each of the (A), (B), and (C) boxes."
https://arxiv.org/html/2411.07335v1,"Multimodal Fusion Balancing Through
Game-Theoretic Regularization","Multimodal learning can complete the picture of information extraction by uncovering key dependencies between data sources. However, current systems fail to fully leverage multiple modalities for optimal performance. This has been attributed to modality competition, where modalities strive for training resources, leaving some underoptimized. We show that current balancing methods struggle to train multimodal models that surpass even simple baselines, such as ensembles. This raises the question: how can we ensure that all modalities in multimodal training are sufficiently trained, and that learning from new modalities consistently improves performance? This paper proposes the Multimodal Competition Regularizer (MCR), a new loss component inspired by mutual information (MI) decomposition designed to prevent the adverse effects of competition in multimodal training. Our key contributions are: 1) Introducing game-theoretic principles in multimodal learning, where each modality acts as a player competing to maximize its influence on the final outcome, enabling automatic balancing of the MI terms. 2) Refining lower and upper bounds for each MI term to enhance the extraction of task-relevant unique and shared information across modalities. 3) Suggesting latent space permutations for conditional MI estimation, significantly improving computational efficiency. MCR outperforms all previously suggested training strategies and is the first to consistently improve multimodal learning beyond the ensemble baseline, clearly demonstrating that combining modalities leads to significant performance gains on both synthetic and large real-world datasets.","Exploiting multimodal data has made significant progress, with advances in generalizable representations and larger datasets enabling solutions to previously unattainable tasks [27; 29; 37; 43; 42; 44; 50; 53; 62]. However, studies indicate that multimodal data is often utilized suboptimally, underperforming compared to ensemble unimodal models or even the best single modality [55; 60]. The expectation that adding a modality should improve performance, assuming independent errors and above-chance predictive power [17], is frequently contradicted in practice. Huang et al. [20] attribute this issue to modality competition, where one modality quickly minimizes training error, misdirecting and suppressing the learning of others. Factors like noise levels, relationship complexity with the target, feature dimensionality, and data quality can cause one modality to fit faster than another. This imply that adding task-relevant information doesn’t guarantee better performance, primarily due to complications during training. To address these issues, it’s crucial to monitor each modality’s contribution during training and apply corrective measures. Several balancing strategies have been proposed to tackle this issue [5; 6; 9; 10; 21; 26; 28; 40; 41; 54; 55; 57; 60]. A central aspect of these methods is estimating each modality’s contribution to the output. Most assume distributional independence between modalities on predicting the target, measuring contribution via unimodal performance [60; 26; 40; 6; 54]. Some methods bypass this assumption by estimating influence based on prediction differences between original and perturbed inputs [28; 21; 10]. Perturbations can take various forms, such as zeroing values [28], adding Gaussian noise [10], or using task-specific augmentations [21; 31]. These methods aim to amplify a modality’s influence by increasing the impact of perturbations on the output. However, this also makes the network more sensitive to these changes (e.g., noise), risks becoming overly reliant on the perturbations, and struggles to scale when multiple perturbations are required. Additionally, increasing the contribution of one modality can be achieved by overshadowing others, leading to an imbalance that undermines overall performance and makes the objective counterproductive. Given these challenges, how can we design an efficient regularization method that addresses multimodal competition, ensuring balanced and effective learning across all modalities? Figure 1: (Left) Illustration of the conditional mutual information (\operatorname{CMI}) terms, \operatorname{CMI}_{1}:I(X_{1};Y\mid X_{2}) and \operatorname{CMI}_{2}:I(X_{2};Y\mid X_{1}), representing the unique contributions (U_{1} and U_{2}) of each modality to the target. The shared task-relevant information (S) between the modalities is defined as I(X_{1};X_{2})-I(X_{1};X_{2}\mid Y). (Right) Accuracy as a function of the ratio between the unique information (U_{1}) from modality X_{1} and the shared information (S) between the modalities. Synthetic data are generated as X_{1}=N_{1}+Y,X_{2}=N_{1}+Y where N_{1},N_{2} are independent noise for each modality. We consider S the percentage of the datapoints that both modalities have information about the label Y, U_{1} and U_{2} when only one has with the other modality equating to noise for those datapoints. In the experiment we keep U_{2} constant while changing U_{1} and S. As U_{1} increases and S decreases, accuracy deteriorates, reflecting intensified multimodal competition. Among the various methods, including Singleloss, Multiloss, Ensemble, unimodally pretrained and finetuned encoders (Uni-Pre Fine), OGM [40], AGM [28], and MLB [26]our regularization method MCR demonstrates a slower decline in accuracy. For further detals prease refer to Section 4.1 In this paper, we introduce the Multimodal Competition Regularizer (\operatorname{MCR}), a loss function designed to promote the exploration of task-relevant information across all available modalities. By decomposing the joint mutual information (\operatorname{MI}), we separately model shared and unique task-relevant information within the modalities. To efficiently capture the unique information from each modality, we employ a computationally inexpensive permutation-based approach. Our method maximizes the lower bounds of each MI term to encourage the network to learn both shared and unique information, while minimizing upper bounds on terms to suppress task-irrelevant information. We frame the problem in a game-theoretic setting, exploring strategies that involve both collaboration and competition among modalities to address the conflicting objectives that arise when increasing all modalities’ contributions simultaneously. This approach allows their contributions to adapt dynamically, achieving balance during training. We extensively evaluate \operatorname{MCR} on synthetic datasets and several established real-world multimodal benchmarks, including action recognition on AVE [49] and UCF [45], emotion recognition on CREMA-D [4], human sentiment on CMU-MOSI [61], human emotions on CMU-MOSEI [63] and egocentric action recognition on Something-Something [15]. Our results demonstrate that \operatorname{MCR} is the first balancing method to significantly improve supervised multimodal training over the ensemble baseline across a variety of datasets and models. Our key contributions are summarized as follows: 1. An analysis of multimodal competition, defining the error increase caused in multimodal training, while demonstrating in our results that most previous methods do no outperform simple baselines, such as unimodal ensembles. 2. A novel multimodal training strategy, \operatorname{MCR}, designed to regularize multimodal competition, which includes: • Defining lower and upper bounds of the \operatorname{MI} terms, encouraging the exploration of information across all modalities. • Introducing a game-theoretic perspective where modalities form the players that compete for training resources, assisting the regularization through balancing the corresponding MI terms. • Suggesting latent-space perturbations as an efficient way to estimate the lower bound of the \operatorname{CMI} reducing the computational cost of multiple forward passes."
https://arxiv.org/html/2411.07320v1,"Richer Output for Richer Countries:
Uncovering Geographical 
Disparities in Generated Stories and Travel Recommendations","While a large body of work inspects language models for biases concerning gender, race, occupation and religion, biases of geographical nature are relatively less explored. Some recent studies benchmark the degree to which large language models encode geospatial knowledge. However, the impact of the encoded geographical knowledge (or lack thereof) on real-world applications has not been documented. In this work, we examine large language models for two common scenarios that require geographical knowledge: (a) travel recommendations and (b) geo-anchored story generation. Specifically, we study four popular language models, and across about 100K travel requests, and 200K story generations, we observe that travel recommendations corresponding to poorer countries are less unique with fewer location references, and stories from these regions more often convey emotions of hardship and sadness compared to those from wealthier nations. 111Upon publication, we will publicly release the data and code required to reproduce our evaluation.","Figure 1: World map with country-wise analysis of responses generated by GPT-4. Left: Average count of geographical entities mentioned in generated stories (correlated with the GDP per capita with Pearson r = 0.5). Right: Uniqueness scores for travel recommendations (Pearson r = 0.4 with GDP per capita). Given the excitement around large language models, users resort to these models for a diverse range of applications (Brown et al., 2020; Touvron et al., 2023). Based on our analysis of ShareGPT,222https://sharegpt.com/ a collection of user interactions with ChatGPT, 1.7% of queries are about travel recommendations, whereas 1.5% concern story generation. Such use cases make one wonder whether the generated travel itinerary for Mumbai is just as informative compared to New York City? Or is a generated story of a girl growing up in Nairobi just as relatable compared to another story based in Seattle? For these applications to be broadly useful, it is important that there are no (or few) geographical disparities. Some recent works aim to benchmark the extent of geographical knowledge encoded in large language models (Bhandari et al., 2023; Manvi et al., 2023; Moayeri et al., 2024). Interestingly, a recent study finds that language models accurately predict global facts such as population and rainfall for different geo-locations, however, their predictions on sensitive topics such as attractiveness or morality are, problematically, biased against areas with poorer socioeconomic conditions (Manvi et al., 2024). Similar in spirit, our work aims to quantitatively study model responses for two real-world scenarios that require geographical knowledge. In this work, we analyze over 300K responses from 4 language models, corresponding to requests for travel recommendations and geo-anchored story generations. These requests span over 150K locations across the globe. We quantify the informativeness and uniqueness of model responses, in addition to the emotions they express. We then compare these quantities with the socioeconomic indicators of different locations. Through our analysis, we uncover several geographical disparities, finding that outputs for wealthier countries are more unique and include more geographical entities (Figure 1). For Sub-Saharan African countries, we notice considerably less unique outputs compared to the North American region, with the average difference being about 40% across all models. Further, stories about poorer countries express considerably more hardship and sadness, with 65% more narratives depicting hardship for low-income countries compared to high-income countries. Despite many large corporations claiming to be egalitarian, e.g., OpenAI aims to develop intelligence that benefits all of humanity (OpenAI, 2024), many findings, including ones from this study, demonstrate how current models perpetuate western hegemony in the generated content Schwöbel et al. (2023); Bender et al. (2021), and more serious effort is needed to ensure that models serve the diverse population across the globe."
https://arxiv.org/html/2411.07308v1,X-DFS: Explainable Artificial Intelligence Guided Design-for-Security Solution Space Exploration,"Design and manufacturing of integrated circuits predominantly use a globally distributed semiconductor supply chain involving diverse entities. The modern semiconductor supply chain has been designed to boost production efficiency, but is filled with major security concerns such as malicious modifications (hardware Trojans), reverse engineering (RE), and cloning. While being deployed, digital systems are also subject to a plethora of threats such as power, timing, and electromagnetic (EM) side channel attacks. Many Design-for-Security (DFS) solutions have been proposed to deal with these vulnerabilities, and such solutions (DFS) relays on strategic modifications (e.g., logic locking, side channel resilient masking, and dummy logic insertion) of the digital designs for ensuring a higher level of security. However, most of these DFS strategies lack robust formalism, are often not human-understandable, and require an extensive amount of human expert effort during their development/use. All of these factors make it difficult to keep up with the ever growing number of microelectronic vulnerabilities. In this work, we propose X-DFS, an explainable Artificial Intelligence (AI) guided DFS solution-space exploration approach that can dramatically cut down the mitigation strategy development/use time while enriching our understanding of the vulnerability by providing human-understandable decision rationale. We implement X-DFS and comprehensively evaluate it for reverse engineering threats (SAIL, SWEEP, and OMLA) and formalize a generalized mechanism for applying X-DFS to defend against other threats such as hardware Trojans, fault attacks, and side channel attacks for seamless future extensions.","A horizontal and distributed supply chain is at the heart of the booming semiconductor industry (see Fig. 1). Creation of digital hardware such as integrated circuits (IC) typically involves the development of sub-designs (intellectual properties - IP) by small entities, the integration of 3rd party sub-designs with the in-house components at the main design house, IC layout creation, fabrication at a foundry, testing at a testing facility, and assembly by the original electronic manufacturer (OEM). This model reduces the time to market for digital systems, allows for specialization, and enables small businesses. All these steps are typically carried out by different entities in different geographical locations. Such a flow can lead to a series of problems, such as: (1) design or sub-design theft, (2) hardware cloning, and (3) malicious hardware modification/tampering. Microelectronic ICs and devices in the field also face diverse threats, such as power side channel attacks and reverse engineering. A variety of Design-for-Security (DFS) strategies such as IC metering [1], [2], watermarking [3], camouflaging [4], [5], split manufacturing [6], [7], logic locking [8, 9], gate parameter optimization [10], variable delay module insertion [11], and dummy logic insertion [12] have been developed to guarantee trust in the supply chain [13] and boost post-deployment IC/device security. However, security solutions often become obsolete with the emergence of novel attacks, while developing appropriate countermeasures requires extensive research effort, time, and expert resources. To expedite the solution search process (against novel vulnerabilities) and to create a human-understandable knowledge base of the said vulnerability, we propose an automated framework, X-DFS (EXplainable - Design For Security). X-DFS uses a heuristics-based search process to determine a large set of DFS candidate instances that might contribute towards the defense against a given vulnerability. These DFS candidates are then used to train an explainable AI model that is capable of: (1) Emitting DFS rules that can be used to efficiently mitigate the vulnerability; (2) Automatically apply these rules towards securing the design. Hence X-DFS not only secures the design, it can also help researchers obtain a deeper understanding of the vulnerability. The proposed framework is highly generalized in nature and can be applied to a wide range of vulnerabilities (with minor tweaks), while most state-of-the-art DFS techniques are typically hand-crafted for mitigating a specific vulnerability or a small set of vulnerabilities. X-DFS can modify a design towards mitigating a given vulnerability and at the same time can generate human-understandable design transformation rules. Such capabilities do not exist in current state-of-the-art DFS techniques. X-DFS is highly flexible (parameterized) and extremely efficient in terms of computation cost, while most state-of-the-art DFS techniques are static in nature (not highly configurable) and fail to work for larger designs (inefficient). Figure 1: Semiconductor supply chain security threats. Figure 2: Impacts of digital IC/design threats. Figure 3: Logic locking: (a) original netlist, (b) obfuscated netlist, and (c) synthesized netlist (structurally changed). We implement X-DFS as a highly parameterized robust framework and use it to perform a comprehensive effectiveness analysis for large-scale designs. We evaluate the X-DFS framework by testing it in the logic locking domain where X-DFS is used to automatically search for mitigation strategies (human-understandable) against three powerful logic locking attacks (SAIL [14], OMLA [15]), SWEEP [16]. X-DFS was able to learn how to defend against these attack models and at the same time extracted human-understandable rules that can be used by other logic locking frameworks (such as LeGO [17]) or human experts to carry out the locking process. In particular, we make the following research contributions: 1. Formalize a general framework and the core mechanisms for automatic exploration of the design-for-security search space for countering novel attack vector(s). 2. Design a set of algorithms (for reverse engineering attacks) that leverages this knowledge regarding an attack vector(s) to build an X-DFS model that can modify a given target design to be resilient against the attacks. 3. Define a methodology to extract and understand the defense rules (human understandable) that are learned by the X-DFS models. 4. Implement the proposed algorithms as a highly parameterized and scalable tool. 5. Qualitatively and quantitatively verify the efficacy of the X-DFS framework/tool against different reverse engineering threats (SAIL, SWEEP, and OMLA)."
https://arxiv.org/html/2411.07268v2,Target-driven Attack for Large Language Models,"Current large language models (LLM) provide a strong foundation for large-scale user-oriented natural language tasks. Many users can easily inject adversarial text or instructions through the user interface, thus causing LLM model security challenges like the language model not giving the correct answer. Although there is currently a large amount of research on black-box attacks, most of these black-box attacks use random and heuristic strategies. It is unclear how these strategies relate to the success rate of attacks and thus effectively improve model robustness. To solve this problem, we propose our target-driven black-box attack method to maximize the KL divergence between the conditional probabilities of the clean text and the attack text to redefine the attack’s goal. We transform the distance maximization problem into two convex optimization problems based on the attack goal to solve the attack text and estimate the covariance. Furthermore, the projected gradient descent algorithm solves the vector corresponding to the attack text. Our target-driven black-box attack approach includes two attack strategies: token manipulation and misinformation attack. Experimental results on multiple Large Language Models and datasets demonstrate the effectiveness of our attack method.","As large language models (LLMs) [21, 6] continue to advance in architecture and functionality, their integration into complex systems requires a thorough review of their security properties. Since the use of most LLMs relies on interface interaction, it is difficult to avoid the hidden danger of generative adversarial attacks. Therefore, it is significant to study adversarial attacks on large language models to improve the security and robustness of LLMs [28]. Previous attacks on LLMs mainly include white-box attacks and black-box attacks. White-box attacks assume that the attacker has full access to the model weights, architecture, and training workflow so that the attacker can obtain the gradient signal. The main method is gradient-based attacks, for example, Guo et al. [11] proposed a gradient-based distributed attack (GBDA), which, on the one hand, uses the Gumbel-Softmax approximation technique to make the adversarial loss optimization differentiable and, on the other hand, uses BERTScore and perplexity to enhance perceptibility and fluency. These methods can only attack open-source large language models but are powerless against more widely used closed-source LLMs such as ChatGPT. The attacker in a black-box attack can only observe the inputs and outputs of the model. They can provide input and receive feedback but cannot access any additional information about the model. Black box attacks [10] can be divided into letter level, word level, sentence level, and multi-layer level according to the attack granularity. Many black-box attacks use word replacement [13] to identify the most critical words in the text and replace them, or use some simple and general text enhancement methods [20], including synonym replacement, random insertion, random exchange, or random deletion. Black-box strategies cannot know the internal structure of large models, so most attack methods use heuristic strategies, where it is not clear how these heuristic strategies are related to the success rate of the attack, so they cannot effectively improve the success rate of the attack strategies. In our work, we assume that the model satisfies the conditional probability distribution p(y|x) and p(y|z) under the condition of clean text representation x and attack text representation z respectively, then maximize the KL divergence between the two distributions. Based on the above assumptions, we prove that maximizing the KL divergence is equivalent to maximizing the Mahalanobis distance between x and z. Subsequently, we transform the maximization problem of Mahalanobis distance into a convex optimization problem and estimate the variance of Mahalanobis distance through another convex optimization problem. Finally, the embedding corresponding to the optimal text is obtained by the projected gradient descent algorithm, which serves as a guide for us to generate disturbance information for the downstream task of cohesion. Experimental results on two black box attack strategies of token manipulation and misinformation attack verify the effectiveness of the proposed method. Our main contributions are summarized as follows: • We propose a new objective function maximizing the KL divergence of two conditional probabilities to guide the attack algorithm to achieve the best attack effect. • We’ve theoretically demonstrated that maximizing the KL divergence between normal and attack text is approximately equivalent to maximizing their Mahalanobis distance. This relationship clarifies how these statistical measures distinguish between normal and attack text in security analysis. • Based on the above theorem, we transformed the original problem into a convex optimization problem and obtained a vector representation of the attack text through projected gradient descent. Then we designed two new black-box attack methods based on token manipulation and misinformation attack strategies. Experimental results verify the effectiveness of our method."
https://arxiv.org/html/2411.07267v1,A Survey on Data Markets,"Data is the new oil of the 21st century. The growing trend of trading data for greater welfare has led to the emergence of data markets. A data market is any mechanism whereby the exchange of data products including datasets and data derivatives takes place as a result of data buyers and data sellers being in contact with one another, either directly or through mediating agents. It serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets.","Data is considered an invaluable resource in the digital economy. The last decades have witnessed the explosive growth of data. As raw material for acquiring knowledge and developing products, data generates value in an indirect way. After remodeling the commercial perspective of data, data is directly monetized like other material commodities nowadays. Individuals and organizations extensively trade datasets and derived data products. In this new vision, data is no longer the enabler of products, but also the product itself. Governments around the world are seizing this new opportunity. For example, the Chinese government unveiled a guideline to improve the market-based allocation of data factors, which is the first to list data as a production factor following land, labor, capital, and entrepreneurship (China, 2020). The United States created the Federal Data Strategy Action Plan aimed at leveraging data as a strategic asset (States, 2019). Driven by the tides of data monetization, data markets have emerged. Data markets, a nascent interdiscipline of computer science and economics, are growing rapidly and evolving in myriad research directions. The history of data markets can be traced back to 1986. A seminal work by Admati and Pfleiderer (1986) studies a market where traders purchase information from a monopolistic seller. The information they trade is the data endowed or produced by individual agents. To the best of our knowledge, the term “data market” is put forward by Keenan (2008) in 2008 for the first time in the literature. They propose to exchange spatial data collected by geographic information systems in the market. In 2011, Balazinska et al. (2011) present a vision of a more general data market where commodities are derivative data products. They outline key challenges in a relational cloud data market for the database research community. Since then, data markets have experienced rapid development. Koutris et al. (2012) design the first query-based data market; Deep and Koutris (2017) propose a scalable and flexible pricing framework for relational queries; Agarwal et al. (2019) design the first two-sided marketplace for trading training data directly; Chen et al. (2019a) introduce the first model-based data market; and more recently Liu et al. (2021b) propose the first end-to-end (model-based) data market involving the interactions among sellers, brokers, and buyers. With the growing demand for data transactions, many data marketplaces have sprung up, such as AWS Data Exchange (AWS, [n. d.]), Dawex (DAWEX, [n. d.]), BDEX (BDE, [n. d.]), Factual (Fac, [n. d.]), and Snowflake (Sno, [n. d.]). Data marketplaces are online transaction locations or exchanges that facilitate the buying and selling of data products. They are authorized to host data products and conduct data transactions for the benefit of stakeholders. We propose a definition of the data market in this survey as follows. A data market is any mechanism whereby the exchange of data products including datasets and data derivatives (such as query results and trained models) takes place as a result of data buyers and data sellers being in contact with one another, either directly or through mediating agents. The data market serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In data markets, the life chain of data covers the process of data search, productization, monetization in pricing and transaction, and finally destruction. Trading data products naturally raises privacy, security, and trust concerns, and faces regulatory barriers to achieving compliance and traceability. Whether in academia or industry, there are rich explorations on designing data markets, where different data markets vary from each other in terms of data products, underlying functions, and market mechanisms. In this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets. 1.1. Related Surveys The existing surveys on data markets can be generally categorized based on the scope: (1) surveys on academic research (Thomas and Leiponen, 2016; Driessen et al., 2022; Liang et al., 2018; Abbas et al., 2021), (2) surveys on industry status (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022), and (3) surveys on data pricing (Muschalle et al., 2012; Fricker and Maksimov, 2017; Zhang and Beltrán, 2020; Pei, 2022; Cong et al., 2022; Zhang et al., 2023a; Miao et al., 2023; Chi et al., 2023). Surveys on academic research. Efforts (Thomas and Leiponen, 2016; Abbas et al., 2021; Driessen et al., 2022; Liang et al., 2018) have been made to survey academic research for data markets within the whole lifecycle. Thomas and Leiponen (2016) provide managers with a literature review on the commercialization of big data. From a managerial and commercial perspective, they introduce six business models in the data ecosystem, led by data suppliers, data managers, data custodians, application developers, service providers, and data aggregators. Based on this taxonomy, they discuss the characteristics of the data ecosystem and conclude with the challenges faced by managers and corresponding guidelines in trading big data including pricing and privacy concerns which we reinforce in this paper. Abbas et al. (2021) examine 133 academic articles using a Service-Technology-Organization-Finance (STOF) model. They find that the existing literature on data marketplaces is primarily dominated by technology studies. Driessen et al. (2022) present a statistical analysis of works related to data markets up until 2021, discuss practical application areas for data markets, categorize the problems of designing data markets, and find corresponding solutions in the literature. Liang et al. (2018) use 4V (Volume, Velocity, Variety, and Value) to define big data and survey the lifecycle of trading big data, including data pricing, data trading, and data protection, for each of which they review corresponding issues and models. The above works (Thomas and Leiponen, 2016; Abbas et al., 2021; Driessen et al., 2022; Liang et al., 2018) mainly focus on techniques for trading data, while our survey covers state-of-the-art literature for trading general data products, including raw data and its derivatives such as queries, statistical inferences, and machine learning models. Moreover, our survey comprehensively covers key issues in main procedures in data markets from data search to data destruction. Surveys on industry status. There are four works (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022) conducting industry surveys on data marketplaces. Schomm et al. (2013) present an initial survey of data marketplaces and data vendors by investigating 46 data suppliers from twelve dimensions (type, time frame, domain, data origin, pricing model, data access, data output, language, target audience, trustworthiness, size of vendor, and maturity) up until Summer 2012. Li et al. (2018a) introduce policies of China for developing data markets and discuss concerns and research opportunities including preprocessing, pricing, security, privacy, and verifiability. Azcoitia and Laoutaris (2022) investigate 180 entities which trade data on the Internet, summarize different business models, and discuss open challenges. Kennedy et al. ([n. d.]) introduce different types of data marketplaces and describe data transaction lifecycle from the perspective of buyers and sellers. They also interview buyers and sellers to understand the current status and challenges of online data marketplaces in 2022. The above works (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022) provides an understanding of data marketplaces through practical investigations of entities and marketplaces. In contrast, our survey not only examines mainstream data marketplaces worldwide but also provides a list of government policies. Surveys on data pricing. There have been several surveys (Muschalle et al., 2012; Fricker and Maksimov, 2017; Zhang and Beltrán, 2020; Pei, 2022; Cong et al., 2022; Zhang et al., 2023a; Miao et al., 2023) specializing in data pricing, a subtopic that receives the most attention in data markets. Muschalle et al. (2012) investigate seven established vendors for their potential market situations, pricing approaches, and trends. Fricker and Maksimov (2017) report a literature survey of 18 papers regarding several research questions, including the maturity and targets of pricing models, types of data products, and pricing mechanisms. Zhang and Beltrán (2020) review novel data pricing studies and categorize data pricing methods based on data granularity and privacy. Pei (2022) starts with the economics of data pricing and reviews pricing models based on a set of fundamental principles. He also discusses the differences between digital products and data products, and the corresponding pricing methods. Very recently, Cong et al. (2022) survey data pricing methods in machine learning pipelines, including pricing raw data sets, pricing data labels, and pricing in collaborative machine learning models. Zhang et al. (2023a) categorize and review pricing methods for queries from the aspects of market structure, privacy notion, query type, and pricing method. Miao et al. (2023) classify data pricing techniques into three strategies and analyze thirteen pricing models. Chi et al. (2023) outline the fundamental concepts of data pricing, categorize data pricing strategies into query-based and privacy-based approaches, and offer an overview of data pricing from a data science standpoint. In addition to covering other data market functions, our survey includes a comprehensive analysis of data pricing that examines both revenue allocation for allocating compensations to data sellers and data product pricing for pricing data products to data buyers and their interactions. Furthermore, it systematically reviews emerging game-theoretic approaches to data pricing for the first time. In summary, while existing surveys approach data markets from either academic or industry perspective, our survey provides a comprehensive and general review of data markets covering both academic research and industry status including government policies across representative countries and domains. We also discuss the differences between data and other production factors and the corresponding impact on the design of data markets. While existing surveys investigate a few significant challenges in data markets, we study the interaction between key entities, summarize important desiderata for designing a well-functioning data market, and review techniques regarding data search, productization approaches, pricing mechanisms, data transactions, privacy concerns, etc, based on a formal framework as in Figure 2. Contributions. We present a comprehensive survey of data markets in both academia and industry. The purpose of this survey is to delve into subtopics of data markets in terms of computer science while covering mechanisms, regulations, and challenges in economics, law, and governance. The main contributions of this survey are summarized as follows. • Identify the unique properties of data and discuss the difference between data markets and other markets for the four production factors (land, labor, capital, and entrepreneurship). • Introduce the framework of data markets, formalize the abilities and restrictions of key roles, and illustrate the main procedures in the operations of data markets. • Present important desiderata for well-functioning data markets. • Summarize various methods of data search for various purposes, including crowdsourced dataset collection, dataset discovery in databases, data discovery in machine learning, and general dataset search. • Introduce various approaches to data productization based on versioning and data market categories. • Outline advertising strategies for data sellers and data purchase methods for data buyers in data transactions. • Review different approaches for revenue allocation and data product pricing, along with game-theoretic pricing methods. • Describe possible attacks on privacy preservation, fairness, profitability, and traceability from dishonest entities and corresponding solutions. • Present guidelines and regulations and investigate actual data marketplaces in representative countries and domains. • Discuss various open challenges and emerging directions for future research. Figure 1. The structure of the survey. 1.2. Structure of The Survey Figure 1 shows the structure of the survey. Section 2 first presents a diagram that shows different problems affecting data markets and their relationship. Section 3 summarizes important desiderata for building well-functioning data markets. Section 4 reviews methods of data search including crowdsourced dataset collection, dataset discovery in databases, dataset discovery in machine learning, and general dataset search. Section 5 describes techniques for data productization. Section 6 overviews strategies of both data buyers and data sellers in data transactions. Section 7 reviews studies investigating data product pricing and game-theoretic pricing. Section 8 reviews studies investigating revenue allocation. Section 9 deals with issues related to dishonest participants in untrusted data markets. Section 10 provides government policies and industry status in representative countries and regions. Section 11 draws a conclusion and discusses open challenges and opportunities for future work."
https://arxiv.org/html/2411.07252v1,High quality ECG dataset based on MIT-BIH recordings for improved heartbeats classification,"Electrocardiogram (ECG) is a reliable tool for medical professionals to detect and diagnose abnormal heart waves that may cause cardiovascular diseases. This paper proposes a methodology to create a new high-quality heartbeat dataset from all 48 of the MIT-BIH recordings. The proposed approach computes an optimal heartbeat size, by eliminating outliers and calculating the mean value over 10-second windows. This results in independent QRS-centered heartbeats avoiding the mixing of successive heartbeats problem. The quality of the newly constructed dataset has been evaluated and compared with existing datasets. To this end, we built and trained a PyTorch 1-D Resnet architecture model that achieved 99.24% accuracy with a 5.7% improvement compared to other methods. Additionally, downsampling the dataset has improved the model’s execution time by 33% and reduced 3x memory usage.","Heart disease is a major global cause of death, with heart arrhythmia accounting for 16% of deaths in the last two decades (World Health Organization, 2019) [1]. ECG signals are widely used for diagnosing heart disease due to their affordability, convenience, ease of use, and precision. Support Vector Machine (SVM), Multilayer Perceptron (MLP), and Decision Tree (DT) algorithms have been commonly used in machine learning to classify and detect anomalies in ECG signals [2, 3, 4]. Recent research in machine learning (ML) has found that Convolutional Neural Network (CNN)-based automated feature extraction techniques are scalable and highly accurate in ECG signal classification. Deep learning approaches, including those in [5, 6, 7], have achieved state-of-the-art results, even competing with human cardiologists in signal analysis. However, deep learning requires large amounts of data and computation and the usage of GPUs. Moreover, the performance of deep learning models is highly impacted by the quality of data. Thus, effective preprocessing techniques are crucial to enhance data quality, reduce data dimensionality, and minimize both model size and training and inference time. One of the challenges in ECG heartbeat classification is the limited number of public datasets available. The two largest publicly accessible datasets are the MIT [8] and PTB [9] datasets, which provide only continuous ECG recordings on the time domain and their corresponding labels."
https://arxiv.org/html/2411.07244v1,A Tutorial on Teaching Data Analytics with Generative AI,"This tutorial addresses the challenge of incorporating large language models (LLMs), such as ChatGPT, in a data analytics class. It details several new in-class and out-of-class teaching techniques enabled by AI. For example, instructors can parallelize instruction by having students interact with different custom-made GPTs to learn different parts of an analysis and then teach each other what they learned from their AIs. For another example, instructors can turn problem sets into AI tutoring sessions, whereby a custom-made GPT guides a student through the problems, and the student uploads the chatlog for their homework submission. For a third example, you can assign different labs to each section of your class and have each section create AI assistants to help the other sections work through their labs. This tutorial advocates the programming in the English paradigm, in which students express the desired data transformations in prose and then use AI to generate the corresponding code. Students can wrangle data more effectively by programming in English than by manipulating in Excel. However, some students will program in English better than others, so you will still derive a robust grade distribution (at least with current LLMs).","I thought my class was set. Over the prior two years, I had made an entirely new analytics course: I wrote a textbook on data science with R for MBA students (Bray 2023) and created a corresponding set of interactive R Markdown slide decks. The class went well in the previous year, and I was planning on coasting for the next few years. How wrong I was. ChatGPT debuted on November 30, 2022, a mere 120 days before my 2023 class was to begin. The terrible implications this AI had for my class dawned on me seven weeks later. I remember the moment the harsh reality set in. I copied the following exercise from my textbook into ChatGPT without further explanation or context: Exercise 3.43 We will now subject our sample to 10 data filters. • Start with alibaba_long and group_by() order. • filter() the grouped tibble accordingly: • Remove the orders with any() ""FAILURE"" action. For example, you should remove all the order = 87717 observations since this order’s 12th action was a ""FAILURE"". • Remove orders without exactly one ""ORDER"" action, one ""SIGNED"" action, and one ""CONSIGN"" action. • Remove orders that have an action before the ""ORDER"" action or after the ""SIGNED"" action. In other words, there should be an ""ORDER"" action at time = 0 and a ""SIGNED"" action at time = 1. • Remove orders that correspond to multiple shipper values. • Remove orders with day_count > 8. • Remove orders with more than 10 or fewer than 5 posted actions. • Remove observations with ""ORDER"" and ""SIGNED"" actions, because their time values are degenerate (mechanically being either 0 or 1). • ungroup() the filtered tibble and <- it into alibaba_long. • alibaba_long should have 102331 rows after this step. This was the most challenging question from my Alibaba lab, which replicates the analysis of Bray (2020). In 2022, around two dozen students visited my office hours to discuss this question. So you can imagine my astonishment when the chatbot produced a correct code solution on its first attempt—a fact that’s all the more impressive when you consider that alibaba_long is left undefined. I soon discovered that ChatGPT could solve nearly every question in my quizzes, lectures, and labs. Three weeks before students were slated to bid on it, ChatGPT had rendered my class an obsolete farce. I despaired, but not for long, as it soon dawned on me that I could teach one of the first-ever classes on coding with ChatGPT. What an opportunity! The first order of business was to rename the class to OPNS451 Data Science with Large Language Models and update the syllabus, which now begins with the following: Large Language Models (LLMs) such as ChatGPT are powerful. To maximize your productivity—and stay relevant—you should aim to delegate as much of your workflow to these language engines as possible. This means you should become comfortable processing and analyzing data with a computer language—such as R—that LLMs excel at reading and writing. For instance, ChatGPT can answer nearly every question in the R textbook I wrote. Switching to R equips you with the software equivalent of a genie in a bottle, capable of implementing and explaining almost every data transformation. The tool is a great equalizer; an MBA with an LLM can accomplish almost anything that an experienced data scientist can. I believe that MBAs now stand out as the finest data scientists since the critical analytics differentiator has shifted from technical expertise to business insight. MBAs will excel as data analysts because they understand the most meaningful questions to pursue. This class will teach you how to use LLMs to process and analyze data. The only challenge is transitioning from spreadsheets to a language. However, once you master the lingua franca of data science, you will be capable of communicating and collaborating with a machine of immeasurable power. This message struck a chord: in less than three years, my elective MBA class swelled from 21 students in one section to 162 students in three sections. (Besides these elective sections, I have one compulsory section for whom my class is a degree requirement.) I proposed to my students that we treat the class as experiment in AI—an opportunity to collectively anticipate how analytics education and practice will respond to generative AI. This experiment taught me several techniques for teaching data analytics with AI: • Recast homework assignments as AI tutoring sessions—doing so increase student satisfaction, engagement, and learning (Section 2.2.2). • Use the programming with English (PIE) method, which uses AI to translate the students’ natural language into a computer language. Students won’t believe it, but this method makes them more effective with R than with Excel (Section 2.1.2). Further, allowing PIE on formal assessments will not make the grade distribution degenerate (Section 2.1.3). • For graphs, have students program by picture, uploading to ChatGPT a hand-drawn mockup of a plot, and asking the chatbot to create the corresponding ggplot code (Section 2.3.2).111 I learned this programming by picture technique from Sébastien Martin. This technique highlights AI’s rapid progress, as it was only last year that Ellis and Slade (2023) wrote that “there are other types of tasks that ChatGPT is less capable (or sometimes incapable) of performing well, such as interpreting statistical output that is provided as an image.” ChatGPT can now seamlessly interpret a picture depicting a regression output. • Have students teach the class content to a GPT, and then quiz these chatbots to assess the quality of the students’ instruction. For example, I have students train GPTs on logistic regression, and then I pit the students’ GPTs against each other, determining which can best answer logistic regression questions (Section 2.3.3). • Have students teach each other by creating AI assistants for other students. Students love designing AI experiences—custom-made GPTs are an excellent creative outlet (Section 2.2.6). • Create a set custom-made GPTs to teach different things to different students, and then have students teach each other what they learned from their AIs, say, by recording a video for the rest of the class (Section 2.3.2). • Have students pitch solutions to a custom-made GPT, and then have the chatbot quickly identify the proposals that warrant the class’ attention (Section 2.3.2). • Create a GPT obstacle course: load different GPTs on different laptops scattered throughout the class and have students run between the various workstations (Section 2.3.3). • Use AI assistants for quick, in-class demonstrations (Section 2.3.4). • Wrap a GPT around your lecture and students will diligently keep up, as they hate when their chatbot instance falls out of sync. The experiment also taught me a few practical lessons: • Emphasize learning over thinking in homework assignments, as students will outsource the latter to the AI (Section 2.2.5). • Use AI to fill the void in your class left by AI (Section 2.2.5). • Don’t make AI assignments overly creative (Section 2.2.8). • Don’t overlook the programming language instruction (Section 2.1.4). • Halve homework group sizes, since every student now contributes two voices: their own and their AI’s (Section 2.2.9). • Explain that working with AI is a skill that improves with practice (Section 2.2.4). • Do not hire tutors, and by all means do not write a textbook (Section 2.2.7). My final advice is to incorporate AI into your class with confidence. There’s such a hunger for AI in the classroom that even failed LLM initiatives will earn you goodwill."
https://arxiv.org/html/2411.07243v1,Neuropsychology and Explainability of AI: A Distributional Approach to the Relationship Between Activation & Similarity of Neural Categories in Synthetic Cognition,"We propose a neuropsychological approach to the explainability of artificial neural networks, which involves using concepts from human cognitive psychology as relevant heuristic references for developing synthetic explanatory frameworks that align with human modes of thought. The analogical concepts mobilized here, which are intended to create such an epistemological bridge, are those of categorization and similarity, as these notions are particularly suited to the categorical ”nature” of the reconstructive information processing performed by artificial neural networks. Our study aims to reveal a unique process of synthetic cognition, that of the categorical convergence of highly activated tokens. We attempt to explain this process with the idea that the categorical segment created by a neuron is actually the result of a superposition of categorical sub-dimensions within its input vector space.","Within an explainability framework, the neuropsychology of artificial intelligence focuses on studying synthetic neural cognitive mechanisms, considering them as new subjects of cognitive psychology research. The goal is to make artificial neural networks used in language models understandable by adapting concepts from human cognitive psychology to the interpretation of artificial neural cognition. In this context, the notion of categorization is particularly relevant because it plays a key role as a process of segmentation and reconstruction of informational data by the neural vectors of synthetic cognition. Thus, in this study, the aim is to use the concept of categorization, as understood in human cognitive psychology (particularly in its relation to the notion of similarity), to apply it to the analysis of neural behavior and to infer certain synthetic cognitive processes underlying the observed behaviors."
https://arxiv.org/html/2411.07228v1,"Tooling or Not Tooling?
The Impact of Tools on Language Agents for Chemistry Problem Solving","To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents’ ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.","Large language models (LLMs) have demonstrated impressive problem-solving capabilities in many disciplines (wang2024mmlupro; yue2024mmmu; grossmann2023ai). When it comes to chemistry, LLMs still face significant challenges, such as incorrect calculation, lack of domain knowledge, or inability to perform certain tasks like reaction prediction (guo2023eighttask; mirza2024chembench). To address these limitations, LLM-based agents integrated with tools have been proposed to tackle chemistry-specific problems (wang2024agentsurvey; ramos2024chemistryagentsurvey). For example, ChemCrow (bran2023chemcrow) expands LLMs’ capabilities by incorporating 18 tools, ranging from web search to chemical reaction prediction. Similarly, Coscientist (boiko2023coscientist) integrates the control of cloud labs to enable LLMs to automate wet lab experiments. Despite the promise of these tool-augmented agents, existing evaluations have been largely qualitative and limited in scope. For example, ChemCrow is assessed with only 14 individual tasks mainly focusing on compound synthesis, and Coscientist’s evaluation involves merely six specific tasks. These narrow assessments leave a large gap in our understanding of how tool-augmented agents perform across diverse chemistry tasks in real-world applications. In this work, we conduct a comprehensive evaluation of LLM-based agents on different chemistry tasks to grasp a deep understanding of their potential and limitations. To explore and enhance the capabilities of agents in diverse and complex chemistry scenarios, we introduce ChemAgent, a new chemistry agent capable of handling a wide spectrum of tasks. It leverages the ReAct framework (yao2023react) and integrates 29 tools, such as a search tool for PubChem (kim2019pubchem), several molecular property predictors, as well as many practical tools present in ChemCrow. Then, we adapt two categories of real-world chemistry problems for systematic evaluation: specialized tasks and general questions. For specialized tasks, we use SMolInstruct (yu2024llasmol), which contains 14 types of specialized molecule- and reaction-centric tasks. For general questions, we use MMLU-Chemistry and GPQA-Chemistry, which are chemistry-related subsets of the MMLU (hendryckstest2021mmlu) and GPQA (rein2023gpqa) benchmarks, containing exam-like questions ranging from high school, college, to graduate level. Through comprehensive experiments, we show that: While ChemAgent substantially outperforms ChemCrow on all chemistry tasks, it does not consistently outperform the base LLMs without tools. In addition, the impact of tool augmentation is highly dependent on task characteristics. For specialized chemistry tasks involving professional molecular representations (e.g., SMILES (weininger1988smiles)) and specialized chemical operations (e.g., compound synthesis), augmenting LLMs with task-specific tools can yield substantial performance gains. Nonetheless, for general chemistry questions that require fundamental knowledge and extensive reasoning, ChemAgent cannot address these challenges adequately and underperforms the base LLMs. Further analysis along with a chemistry expert shows that ChemAgent’s underperformance on general chemistry questions is primarily due to delicate mistakes at intermediate stages of its problem-solving process, such as wrong reasoning steps and information oversight. Overall, our findings indicate that tool augmentation may introduce additional complexity that hinders LLM reasoning and thus does not always help in chemistry problem-solving. Future research may improve LLM-based agents for chemistry by optimizing cognitive load and enhancing reasoning and information verification abilities."
https://arxiv.org/html/2411.07200v1,"’Explaining RL Decisions with Trajectories’: 
A Reproducibility Study","This work investigates the reproducibility of the paper ""Explaining RL decisions with trajectories“ by Deshmukh et al. (2023). The original paper introduces a novel approach in explainable reinforcement learning based on the attribution decisions of an agent to specific clusters of trajectories encountered during training. We verify the main claims from the paper, which state that (i) training on less trajectories induces a lower initial state value, (ii) trajectories in a cluster present similar high-level patterns, (iii) distant trajectories influence the decision of an agent, and (iv) humans correctly identify the attributed trajectories to the decision of the agent. We recover the environments used by the authors based on the partial original code they provided for one of the environments (Grid-World), and implemented the remaining from scratch (Seaquest and HalfCheetah, Breakout, Q*Bert). While we confirm that (i), (ii), and (iii) partially hold, we extend on the largely qualitative experiments from the authors by introducing a quantitative metric to further support (iii), and new experiments and visual results for (i). Moreover, we investigate the use of different clustering algorithms and encoder architectures to further support (ii). We could not support (iv), given the limited extent of the original experiments. We conclude that, while some of the claims can be supported, further investigations and experiments could be of interest. We recognize the novelty of the work from the authors and hope that our work paves the way for clearer and more transparent approaches.","Reinforcement Learning (RL), formalized in the pioneering work of Sutton & Barto (2018), focuses on learning how to map situations to actions, in order to maximize a reward signal. The agent aims to discover which actions are the most rewarding by testing them. This addresses the problem of how agents should learn a policy that takes actions to maximize the cumulative reward through interaction with the environment. A recent pivotal focus in RL is the increasing attention on the explainability of these algorithms, a factor for their adoption in real-world applications. Precedent work in the field of XRL include Puiutta & Veith (2020), Korkmaz (2021) and Coppens et al. (2019). This reproducibility report focuses on the work of Deshmukh et al. (2023), which proposes an innovative approach to enhance the transparency of RL decision-making processes. Given the rising interest and applications of Offline RL (Levine et al. (2020),Kumar et al. (2020)), obtaining explainable decision is an important desideratum. Deshmukh et al. (2023) introduces a novel framework in the offline RL landscape. This new approach is based on attributing the decisions of an RL agent to specific trajectories encountered during its training phase. It counters traditional methods that predominantly rely on highlighting salient features of the state of the agent(Iyer et al. (2018)). Despite proposing a novel approach, no subsequent work has built upon it. We believe this is primarily due to the absence of code available online and the limited number of environments on which this method was tested, which reduces its practical applications. Therefore, we intend to not only validate the original results by Deshmukh et al. (2023), but also explore the robustness and generalizability of the proposed methodology across different environments and settings. Our reproducibility study is a step toward ensuring that advancements in this domain are both transparent and trustworthy, paving the way for a better understanding of RL systems."
https://arxiv.org/html/2411.07163v1,A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis: Evaluating Mental Health Sentiment on Social Media during COVID-19,"Monitoring public sentiment via social media is potentially helpful during health crises such as the COVID-19 pandemic. However, traditional frequency-based, data-driven neural network-based approaches can miss newly relevant content due to the evolving nature of language in a dynamically evolving environment. Human-curated symbolic knowledge sources, such as lexicons for standard language and slang terms, can potentially elevate social media signals in evolving language. We introduce a neurosymbolic method that integrates neural networks with symbolic knowledge sources, enhancing the detection and interpretation of mental health-related tweets relevant to COVID-19. Our method was evaluated using a corpus of large datasets (approximately 12 billion tweets, 2.5 million subreddit data, and 700k news articles) and multiple knowledge graphs. This method dynamically adapts to evolving language, outperforming purely data-driven models with an F1 score exceeding 92%. This approach also showed faster adaptation to new data and lower computational demands than fine-tuning pre-trained large language models (LLMs). This study demonstrates the benefit of neurosymbolic methods in interpreting text in a dynamic environment for tasks such as health surveillance.","Online platforms like X (formerly Twitter) are vital for capturing real-time public sentiment, especially during crises. With X generating approximately 500 million tweets daily [1], researchers have a substantial source of data to monitor public discourse [24, 53, 52, 48]. This capability is particularly crucial in contexts like the COVID-19 pandemic, which has significantly impacted mental health, evidenced by marked increases in anxiety, depression, and substance use [54]. However, most existing studies on the mental health analysis of social media data have been retrospective, limiting their effectiveness in providing timely insights. There is a pressing need for near-real-time analysis tools to swiftly detect and respond to emerging trends in big data, thereby enhancing the efficacy of public health interventions and policy responses. Despite the utility of neural network-based techniques such as large language models (LLMs), attention models, and word embedding models like Word2Vec [32] and GloVe [36], their application to dynamic social media language poses challenges. These models struggle with the flexibility needed to adapt to rapid changes in language, such as the emergence of terms like “Zoom fatigue” [18], which gained relevance during the pandemic as remote work became common. The reliance on vast computational resources further complicates rapid adaptation to new linguistic phenomena, as LLMs and other traditional models depend heavily on historical data. This limitation makes them less effective for real-time monitoring of evolving terms and trends crucial in health monitoring contexts. A neurosymbolic approach offers a more adaptable, efficient framework that integrates evolving linguistic data. To manage the rapid evolution of online language effectively, our neurosymbolic approach uniquely combines a domain-specific language model with extensive integration of several domain-specific and general-purpose knowledge bases (KBs). This integration facilitates the dynamic incorporation of emerging vocabulary and contextual shifts, enabling near-real-time analysis of online discourse. Utilizing a vast dataset of approximately 12 billion tweets, 2.5 million subreddit posts, and 700k news articles, our method leverages the Zero-Shot Semantic Encoding and Decoding Optimization (SEDO) framework—initially designed for image processing [19] and adapted here for mental health applications in social media text[11]. SEDO calculates relevance scores by evaluating the semantic similarity between new terms and established knowledge concepts, enhancing content representation, and enabling dynamic adaptation to context-dependent meanings. This method significantly improves over the less precise soft match approach previously used [11]. Additionally, our approach integrates multiple knowledge sources for managing semantic gaps [4], and its scalable design makes it applicable to a broad range of domains beyond mental health. Our neurosymbolic approach outperforms traditional data-driven models with an F1 score of >92\%, demonstrating adaptability and efficiency. It requires fewer computational resources than large language models. Additional experiments demonstrate that our approach excels compared to pre-trained LLMs. Further validation through an ablation study reveals the contribution of components to the overall performance of the method. Additionally, a triangulation study affirms the model’s adaptability, showing practical application to new, previously unseen datasets, thereby confirming its broader applicability. This paper is structured according to the progressive development and validation of our neurosymbolic approach. Following this introduction, a background and literature review presents the relevant theories and existing works. Our methods section elaborates on integrating symbolic AI with neural networks and details the adaptive techniques employed. The experimental setup and results sections then present the data sources, preparation processes, and the outcomes of our studies, including ablation and triangulation analyses. Finally, the discussion encapsulates the broader implications of our findings, assesses the advantages of our approach, and suggests avenues for future research."
https://arxiv.org/html/2411.07133v2,Stronger Models are NOT Stronger Teachers for Instruction Tuning,"Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models’ Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.","Instruction tuning (Figure 1) has been widely adopted to tailor the behavior of base Large Language Models (LLMs) to align with specific tasks and user intents (Zhang et al., 2023). This approach leverages instruction datasets, consisting of samples pairing an instruction with a corresponding response. The success of instruction tuning depends on the availability of high-quality instruction datasets. Initially, constructing these datasets required large human effort in generating and curating instruction-response pairs (Databricks, 2023; Zheng et al., 2024; Zhao et al., 2024), which is time-consuming and labor-intensive Liu et al. (2024b). To reduce the reliance on human-curated datasets, synthetic datasets generated by LLMs have surfaced as a viable solution Adler et al. (2024). Recent works, such as (Sun et al., 2023; Taori et al., 2023; Wang et al., 2023; Xu et al., 2024; Chen et al., 2024), have shown the strong potential of synthetic datasets in instruction tuning. While current research has primarily focused on using LLMs to create large, diverse, and high-quality instructions (Liu et al., 2024b), the selection of appropriate LLMs for generating corresponding responses remains largely unexplored. The common approach is to employ top-performing models (e.g., those leading on benchmarks (Fourrier et al., 2024; Chiang et al., 2024)) for response generation in instruction tuning. For instance, Llama-3.2-3B-Instruct uses responses generated by Llama-3.1-405B-Instruct (i.e., the largest model in Llama-3.1 family) for instruction tuning Meta (2024b). Additionally, most of the existing open synthetic datasets Teknium (2023); Xu et al. (2023a); Ding et al. (2023); Gallego (2023); Chen et al. (2024) depend on expensive, closed-source models like GPT-4 Achiam et al. (2023) and Gemini (Google, 2024) to produce responses. Figure 1: This figure demonstrates the process of instruction tuning and the scope of this paper. Is it always better to use the larger or stronger models as teachers? In this paper, we investigate the choice of the teacher model that generate responses during synthetic dataset generation, which we refer to as response generators, influence the instruction-following performance of the instruction-tuned LLMs. Specifically, given a base model and a set of high-quality instructions, we investigate the following research questions: RQ1: Which models are the most effective response generators for instruction tuning? To answer RQ1, we conduct extensive experiments with five base models, and fine-tune them on datasets generated by 20 response generators across seven model families: Qwen2, Qwen2.5, Llama 3, Llama 3.1, Gemma 2, Phi-3, and GPT-4. Our findings challenge common assumptions in the field, revealing a surprising result which we term the Larger Models’ Paradox: larger response generators (e.g., Llama-3.1-405B-Instruct) do not always enhance a base model’s instruction-following capabilities compared to their smaller counterparts within the same model family (e.g. Llama-3.1-70B-Instruct). Moreover, we find that open-source models (e.g., Gemma-2-9b-it and Qwen2.5-72B-Instruct) outperform GPT-4 as response generators. These findings question established practices and suggest more efficient and accessible approaches to create high-quality instruction datasets. To further explore the Larger Models’ Paradox, we investigate statistical metrics to reveal potential factors influencing the effectiveness of response generators given different base models. Here, we pose our second research question: RQ2: How can we determine the most effective response generators for a certain base model without instruction tuning? This question is crucial due to the significant computational costs associated with instruction tuning across multiple datasets generated by diverse response generators. Our investigation reveals that existing metrics in alignment data selection, including quality (Dubey et al., 2024), difficulty (Li et al., 2024c), and response length (Liu et al., 2023), fail to consider the compatibility between the base model being fine-tuned and the response generator, thus results in their inability to explain the Larger Models’ Paradox. To bridge this gap, we formulate the task of finding the most effective response generators as a risk-return problem. We solve this by calculating an Compatibility-Adjusted Reward (CAR), where compatibility serves as the risk factor. This compatibility is quantified by the average loss of responses on the base model being fine-tuned, with higher average loss indicating lower compatibility and thus higher risk. Our comparison of the proposed CAR with existing metrics demonstrates that it outperforms all baselines in predicting the effectiveness of response generators. We believe that our findings on the Larger Models’ Paradox and the proposed CAR can effectively guide future instruction tuning of LLMs. Instead of selecting response generators solely based on benchmark performance (e.g., GPT-4), practitioners should prioritize those with higher compatibility to better enhance the instruction-following capabilities of their LLMs."
https://arxiv.org/html/2411.07089v1,Towards Characterizing Cyber Networks with Large Language Models,"Threat hunting analyzes large, noisy, high-dimensional data to find sparse adversarial behavior. We believe adversarial activities, however they are disguised, are extremely difficult to completely obscure in high dimensional space. In this paper, we employ these latent features of cyber data to find anomalies via a prototype tool called Cyber Log Embeddings Model (CLEM). CLEM was trained on Zeek network traffic logs from both a real-world production network and an from Internet of Things (IoT) cybersecurity testbed. The model is deliberately overtrained on a sliding window of data to characterize each window closely. We use the Adjusted Rand Index (ARI) to comparing the k-means clustering of CLEM output to expert labeling of the embeddings. Our approach demonstrates that there is promise in using natural language modeling to understand cyber data.","Threat hunting is an open-ended cybersecurity exploration to detect abnormal behaviors that automated tools cannot easily find. Threat hunters continuously collect and analyze data from their corporate networks looking for indicators of compromise. The complexity and changing attack surface of industrial control systems creates an increasing need for tools to help hunters effectively detect threats within diverse, complex, and statistically noisy cyber data. This project addresses this gap by developing CLEM, which is capable of ingesting cybersecurity logs from cyberphysical systems and characterizing entities in them by creating vector identities and behavioral relationships. The key innovation is the use of large language models (LLMs) to model the linguistic structures found in cybersecurity logs allowing us to cluster machines according to their behaviors over time. The rest of this paper is divided as follows: Section II provides motivation for our work and methodologies. In Section III we provide a series of related papers spanning different models, approaches, and techniques. We introduce our main contribution, CLEM, in Section IV and our results in Section V. Finally, we provide future research ideas in Section VI and conclude our work in Section VII."
https://arxiv.org/html/2411.07087v2,OCMDP: Observation-Constrained Markov Decision Process,"In many practical applications, decision-making processes must balance the costs of acquiring information with the benefits it provides. Traditional control systems often assume full observability, an unrealistic assumption when observations are expensive. We tackle the challenge of simultaneously learning observation and control strategies in such cost-sensitive environments by introducing the Observation-Constrained Markov Decision Process (OCMDP), where the policy influences the observability of the true state. To manage the complexity arising from the combined observation and control actions, we develop an iterative, model-free deep reinforcement learning algorithm that separates the sensing and control components of the policy. This decomposition enables efficient learning in the expanded action space by focusing on when and what to observe, as well as determining optimal control actions, without requiring knowledge of the environment’s dynamics. We validate our approach on a simulated diagnostic task and a realistic healthcare environment using HeartPole. Given both scenarios, the experimental results demonstrate that our model achieves a substantial reduction in observation costs on average, significantly outperforming baseline methods by a notable margin in efficiency.","In traditional control systems, it is often assumed that all necessary information is readily available, which is seldom the case in practical scenarios [1, 29, 28, 27, 26]. The need to actively decide which observations to make adds a layer of complexity to the decision-making process, as it requires balancing the benefits of additional information against the costs of acquiring it [10, 12]. Additionally, tasks in virtual environments, such as those running on simulators, often disregard observation costs because the optimization goals—such as maximizing rewards or achieving specific objectives—are not inherently aligned with the expenses involved in acquiring observations [19, 9]. This disconnect allows for the assumption of complete knowledge of the environment, but ignoring observation costs in such models makes reinforcement learning applications diverge from real-world practice. In many real-world applications, particularly in healthcare, decision-making processes must account for the costs associated with actively obtaining observations. Medical assessments, diagnostic tests, and patient monitoring not only require financial resources but also demand significant time from healthcare professionals and patients alike. This inherent cost associated with information gathering necessitates strategies that judiciously balance the need for information with the resources available [3, 32]. Medical assessments can be regarded as sequential decision-making problems, where treatments are administered based on the patient’s current health states. These health states are inferred from observations, including physical examinations and clinical metrics. The challenge lies in making optimal decisions with limited and costly observations, which is critical for both patient outcomes and resource management [13, 21, 4]. Some examples of active observation actions within the full space are presented in Figure 1. Figure 1: Active observations within full state space This paper addresses the challenge of simultaneously learning an observation strategy and a control strategy in environments where observations are costly. In such settings, there is a fundamental trade-off between the cost of acquiring observations and the benefit they provide in making informed control decisions. We consider the simplest case of observation action design, where each observation action is a binary decision: whether to acquire a specific observation or not. This formulation leads to a total action space of 2^{|\mathcal{O}|}\times|\mathcal{A}_{\text{control}}|, where |\mathcal{O}| is the number of possible observations to make and |\mathcal{A}_{\text{control}}| is the number of control actions. This exponential growth in the action space introduces the curse of dimensionality, making the learning process significantly more complex than when learning a control policy alone. To tackle this challenge, we propose an iterative, model-free deep reinforcement learning approach that decomposes the sensing and control policies. By separating these two aspects, the learning algorithm can more efficiently navigate the enlarged action space. The model focuses on learning when and what to observe, alongside determining the optimal control actions, without requiring a complete model of the environment’s dynamics [23, 11]. We demonstrate the effectiveness of our method on two fronts of medical practice: a simulated Diagnostic Chain task designed to capture the essential elements of the problem, and a realistic healthcare environment using HeartPole [17]. These experiments showcase the model’s ability to make cost-effective observation decisions while achieving desirable control outcomes."
https://arxiv.org/html/2411.07038v1,Designing Reliable Experiments with Generative Agent-Based Modeling: A Comprehensive Guide Using Concordia by Google DeepMind,"In social sciences, researchers often face challenges when conducting large-scale experiments, particularly due to the simulations’ complexity and the lack of technical expertise required to develop such frameworks. Agent-Based Modeling (ABM) is a computational approach that simulates agents’ actions and interactions to evaluate how their behaviors influence the outcomes. However, the traditional implementation of ABM can be demanding and complex. Generative Agent-Based Modeling (GABM) offers a solution by enabling scholars to create simulations where AI-driven agents can generate complex behaviors based on underlying rules and interactions. This paper introduces a framework for designing reliable experiments using GABM, making sophisticated simulation techniques more accessible to researchers across various fields. We provide a step-by-step guide for selecting appropriate tools, designing the model, establishing experimentation protocols, and validating results.","In an era where artificial intelligence (AI) is reshaping countless fields, the research community of social sciences needs to adapt to the changes posed by these technologies [1, 2]. In particular, data quality and authenticity play a significant role in social sciences [3], where the conclusions drawn rely heavily on data collected, for instance, from surveys. There are many traditional ways of gathering data, such as public datasets or private surveys, but AI has led to innovative approaches, like using agent-based models (ABMs). In recent years, the use of this paradigm has gained significant attention across a variety of fields, from economics and social sciences to artificial intelligence and computational biology [4, 5, 6]. ABMs allow researchers to simulate complex situations by modeling the behaviors and interactions of individual agents within a given environment [7]. These models provide a powerful way to understand emergent phenomena—such as market dynamics, social behaviors, or ecological systems—that arise from the independent actions and interactions of individual agents, each following its own set of rules. In spite of their flexibility, these models face some limitations, particularly when dealing with complex environments. One of the main challenges is that the agents’ behaviors are programmed by the modeler based on assumptions or simplified rules. This rigid structure limits the ability to account for the full range of possible interactions that can emerge in real-world scenarios. However, it is at the intersection between generative modeling and agent-based modeling that we find a more complete approach: generative agent-based models (GABMs). They take this concept further by allowing the model to generate and learn behaviors based on patterns in the data, rather than being rigidly defined by pre-programmed rules. Additionally, agents can have the reasoning skills of large language models (LLMs), enabling them to incorporate more human-like decision-making processes [8]. This makes GABMs more complete and adaptable, as they can simulate not only interactions but also complex thought processes and strategies [9]. There are already many platforms designed for agent-based modeling, but Concordia, developed by Google DeepMind, stands out for its approach to GABMs [10]. As a platform integrating artificial intelligence techniques, it supports the design, execution, and analysis of complex simulations, enabling users to model large-scale scenarios with notable scalability and computational efficiency. Compared to other options, it benefits from ongoing development of its components, ensuring that users have access to the latest modeling techniques and facilitating the creation of highly customizable agents designed to meet specific research needs. However, leveraging these advantages requires careful experimental design to ensure reliability and accuracy, as poorly designed simulations risk producing misleading or unrealistic results. Such inaccuracies can undermine the platform’s capacity to yield valid and insightful conclusions. This guide aims to bridge that gap, offering best practices for researchers who wish to use Concordia to conduct accurate simulations. Overall, with this paper, we want to make these simulation techniques more accessible to a broader audience, including those who may not have extensive technical expertise in developing agent-based models. We provide a step-by-step framework for designing experiments, covering critical aspects such as selecting the right tools, designing robust GABM models, establishing experimentation protocols, and validating simulation outcomes. The remainder of the paper is organized as follows: Section 2 establishes the theoretical foundation for our proposed approach, explaining key concepts such as Generative Agent-Based Modeling, its role in experimentation, and the integration of AI tools within the framework. Next, Section 3 provides a practical guide for implementing the framework, outlining steps such as tool selection, agent design, experimentation protocols, and validation techniques. Therefore, this knowledge is used in Section 4, in which a case study showcases the implementation process, as well as the results. Finally, Section 5 concludes this study with a summary of its main findings, conclusions, and future work."
https://arxiv.org/html/2411.07008v1,Permutative redundancy and uncertainty of the objective in deep learning,"Implications of uncertain objective functions and permutative symmetry of traditional deep learning architectures are discussed. It is shown that traditional architectures are polluted by an astronomical number of equivalent global and local optima. Uncertainty of the objective makes local optima unattainable, and, as the size of the network grows, the global optimization landscape likely becomes a tangled web of valleys and ridges. Some remedies which reduce or eliminate ghost optima are discussed including forced pre-pruning, re-ordering, ortho-polynomial activations, and modular bio-inspired architectures.","The success of practical applications of deep networks trained with stochastic gradient descent and its variants is undisputed. Despite occasional progress, theoretical research on the foundations of deep networks is lagging behind their practical successes. Deep learning theories often rely on assumptions that are far from realistic, e.g., availability of infinite training data and access to statistical expectations, regularity – determinism, continuity, smoothness – of objective functions, and infinite production run times. Aleatoric uncertainty is often treated as a mere inconvenience which can be averaged away. Ignorance of epistemic uncertainty is common. This is unfortunate. Better theoretical foundations help guide practitioners to systematically reason about their models and consciously architect cognitive machines rather than stumble upon solutions through trial and error. It is evident that with sheer computing power, a deep enough and wide enough approximator can model almost any complex real-world phenomenon. The latest SoTAs often appear as competitions in tackling complex problems with brutal force rather than ingenuity and creativity. In all this excitement, have we forgotten that the point is not to compute more, but to create better - efficient, robust, safe, and transparent - solutions for real-world applications? Let us talk about efficiency. The brain is a super-efficient cognitive machine. It consumes only about 20 watts of energy, give or take a few watts. It does not pass a continuous flow of information through its internal pathways when these pathways are not needed. It does not learn by error backpropagation, nor does it adjust its connections using gradients. The brain does not optimize any global objectives in the sense we use the term optimization in deep learning. Nor is it a feed-forward machine. Deep learning [LBH15], for which feedforward monolithic architectures, learning by gradient optimization, and backpropagation are key, is a breakthrough in technology that opened many doors. At the same time, the regrettable side effect of this success is that it diverts talent and technical resources on a massive scale – away from other research and development directions in computational cognition. We are reaching the point of exponentially diminishing returns when using brute computing of deep networks to solve complex real-world problems. I would like to draw attention to these architectures’ fundamental limitations. Here, based on well-known but almost neglected theoretical and experimental results, I discuss the implications of gradient optimization of uncertain objectives and permutative redundancies of deep networks. I also discuss some remedies."
https://arxiv.org/html/2411.07006v1,Estimating Causal Effects in Partially Directed Parametric Causal Factor Graphs,"Lifting uses a representative of indistinguishable individuals to exploit symmetries in probabilistic relational models, denoted as parametric factor graphs, to speed up inference while maintaining exact answers. In this paper, we show how lifting can be applied to causal inference in partially directed graphs, i.e., graphs that contain both directed and undirected edges to represent causal relationships between random variables. We present partially directed parametric causal factor graphs as a generalisation of previously introduced parametric causal factor graphs, which require a fully directed graph. We further show how causal inference can be performed on a lifted level in PPCFGs, thereby extending the applicability of lifted causal inference to a broader range of models requiring less prior knowledge about causal relationships.","A fundamental problem for an intelligent agent performing reasoning under uncertainty is to compute the effect of an action on a certain random variable (randvar) on other randvars. When computing the effect of an action on a specific randvar, it is crucial to deploy the semantics of an intervention instead of performing a classical conditioning on that randvar [22, Chapter 4]. An intervention acting on a randvar R can be thought of as setting R to a fixed value and removing all incoming influences on the value of R. In practice, generally not all causal relationships in a given model are known and thus, only a partially directed graphical model is available. In such a partially directed graph, directed edges represent cause-effect relationships and undirected edges represent causal relationships whose direction is unknown. In this paper, we solve the problem of efficiently estimating causal effects of actions in partially directed lifted probabilistic models, denoted as parametric factor graphs. Lifted representations are not only more expressive than propositional models such as factor graphs, but also allow for tractable probabilistic inference with respect to domain sizes of logical variables by exploiting symmetries. Previous Work. The estimation of causal effects using causal graphical models in form of directed acyclic graphs in combination with observational data has been extensively studied in the literature (see, e.g., [22, 23, 27]). Some works incorporate causal knowledge into (propositional) factor graphs (which are originally undirected graphical models) to enable the estimation of causal effects in factor graphs [6, 29]. In practice, the underlying causal graph is often not fully known and hence, identifying and estimating causal effects when provided with observational data and a partially directed graph has been investigated [7, 10, 16, 24]. However, all of these works perform causal effect estimation on a propositional level and thus lack the expressivity of relational logic, for example to capture the relationships between individual objects. To represent individual objects and the relationships between them, Poole [25] introduces parametric factor graphs as lifted representations, which combine relational logic and probabilistic models, thereby allowing to encode that certain properties hold for groups of indistinguishable objects. In probabilistic inference, lifting exploits symmetries to speed up inference while maintaining exact answers [20]. Over the past years, both algorithms for symmetry detection [1, 8, 11, 13, 14, 15] allowing the construction of lifted representations such as parametric factor graphs as well as various lifted inference algorithms operating on parametric factor graphs have been developed and further refined [2, 3, 4, 5, 9, 18, 25, 28]. More recently, Luttermann et al. [12] introduce parametric causal factor graphs as an extension of parametric factor graphs allowing to incorporate causal knowledge into a lifted representation. Nevertheless, the authors assume that the causal relationships between the involved randvars are fully known, which is rarely the case in practical settings. Our Contributions. We introduce PPCFGs as a generalisation of PCFGs to obtain a formalism that compactly encodes a full joint distribution over a set of randvars and at the same time incorporates causal knowledge in the model, if available. The major advantage of a PPCFG over an PCFG is that not all causal relationships between the involved randvars need to be known, thereby reducing the amount of prior knowledge required and thus making the model more suitable for many practical settings. We further define d-separation in PPCFGs to reason about conditional independence. In addition to that, we present an algorithm to efficiently estimate causal effects in PPCFGs on a lifted level, i.e., a representative of indistinguishable objects is used for computations to speed up inference. Our algorithm identifies whether a causal effect can be uniquely determined from the given PPCFG and if so, outputs the causal effect. If the undirected edges in the PPCFG lead to a causal effect being not uniquely identifiable, our algorithm efficiently enumerates all possible causal effects while operating on a lifted level. Structure of this Paper. We begin by introducing necessary background information and notations. Afterwards, we present PPCFGs as a generalisation of PCFGs, allowing both for directed and undirected edges in the model and then define d-separation in PPCFGs. Thereafter, we provide an algorithm to efficiently estimate causal effects in PPCFGs before we conclude."
https://arxiv.org/html/2411.06927v1,Multi-modal Iterative and Deep Fusion Frameworks for Enhanced Passive DOA Sensing via a Green Massive HAD MIMO Receiver,"Most existing DOA estimation methods assume ideal source incident angles with minimal noise. Moreover, directly using pre-estimated angles to calculate weighted coefficients can lead to performance loss. Thus, a green multi-modal (MM) fusion DOA framework is proposed to realize a more practical, low-cost and high time-efficiency DOA estimation for a H2AD array. Firstly, two more efficient clustering methods, global maximum cos_similarity clustering (GMaxCS) and global minimum distance clustering (GMinD), are presented to infer more precise true solutions from the candidate solution sets. Based on this, an iteration weighted fusion (IWF)-based method is introduced to iteratively update weighted fusion coefficients and the clustering center of the true solution classes by using the estimated values. Particularly, the coarse DOA calculated by fully digital (FD) subarray, serves as the initial cluster center. The above process yields two methods called MM-IWF-GMaxCS and MM-IWF-GMinD. To further provide a higher-accuracy DOA estimation, a fusion network (fusionNet) is proposed to aggregate the inferred two-part true angles and thus generates two effective approaches called MM-fusionNet-GMaxCS and MM-fusionNet-GMinD. The simulation outcomes show the proposed four approaches can achieve the ideal DOA performance and the CRLB. Meanwhile, proposed MM-fusionNet-GMaxCS and MM-fusionNet-GMinD exhibit superior DOA performance compared to MM-IWF-GMaxCS and MM-IWF-GMinD, especially in extremely-low SNR range.","With the advancement of fifth generation (5G) and 6G mobile communication technologies such as massive multiple-input multiple-output (MIMO), the research on the combination of massive MIMO and direction of arrival (DOA) estimation [1, 2, 3] has gradually emerged as a hot spot. And it is widely used in target orientation identification and tracking[4], satellite detection[5], radar[6], directional modulation[8], as well as green wireless communications and networks[9, 10], etc. I-A Traditional-based methods Regular beamforming[11, 12], as one of the traditional DOA estimation methods, applied the Fourier transform to spatial spectrum estimation, but its angular resolution was affected by the “Rayleigh resolution limit”. To alleviate this limitation, the minimum variance method based on the least-mean-square criterion was proposed to improve the spatial resolving power[13]. However, achieving high-precision estimation with it required higher signal-to-noise ratios (SNR) and a greater number of snapshots. Subsequently, the multiple signal classification (MUSIC) method [14], the rotation-invariant subspace (ESPRIT) method [15], and various efficient derivative algorithms [16], such as the root-MUSIC, were proposed. However, these subspace classification methods can cause significant degradation of DOA performance due to inaccuracies in the signal and noise subspaces obtained from the covariance matrix decomposition, which also requires extensive computation. The maximum likelihood estimation method in [17] provided more stable DOA performance with low SNR and a small number of snapshots compared to subspace classification methods. Techniques that integrate the time-domain and spatial-domain characteristics of signals had received considerable attention in recent years, particularly spatial spectrum estimation methods based on higher-order cumulants. The literature [18] constructed matrices utilizing fourth-order cumulants and performed eigenvalue decomposition, followed by MUSIC for DOA estimation, achieving more significant estimation accuracy compared to traditional second-order cumulants, although it introduced increased computational complexity. To reduce this computational load, [19] formed a higher-order matrix using fourth-order cumulants and then employed ESPRIT for angle estimation. Furthermore, a high-resolution DOA estimation method that utilizes fourth-order accumulation to rapidly eliminate redundancy is proposed in [20]. The redundant data in the fourth-order accumulation is reduced by a selection matrix in descending order, and then the redundancy in the vectorization process is eliminated by applying the vectorized form of the fourth-order cumulant matrix transformation.Finally, DOA estimation is performed using the sparse representation method. In comparison with the conventional fourth-order cumulant method, this approach provides higher estimation accuracy and resolution, with enhanced capabilities for suppressing colored noise. Based on the above analysis, the DOA estimation method based on higher-order cumulants effectively suppresses both Gaussian and non-Gaussian noise and supports array expansion. Most researchers initially focused on one-dimensional (1D) DOA estimation based on uniform linear array (ULA) structures. However, two-dimensional (2D) DOA estimation, which provides both azimuth and elevation angles of the source, has gained considerable attention due to its greater practical research significance. Joint singular value decomposition (SVD) of two cross-correlation matrices was proposed in [21] to automatically pair and estimate 2D angles.However, the SVD of higher-order covariance matrices involved high computational complexity. In [22], the azimuth and elevation angles were estimated by two separate 1D ESPRIT algorithms, with pair-matching achieved through the cross-correlation matrix of the two received signals from each ULA. Despite this, the use of two 1D parameter estimations and subsequent pairing resulted in high computational loads. To resolve the issue of parameter paired, an enhanced propagator method with automatic parameter pairing based on two-parallel ULAs was introduced [23], showing improved estimation accuracy and reduced computational load compared to [24]. Due to its simpler array structure, larger array aperture and more accurate DOA estimation, 2D DOA estimation based on L-shaped arrays has become a significant research topic. In [25], a method was proposed that transforms 2D DOA estimation of an L-shaped array into two independent 1D DOA estimations, thus eliminating the requirement for pair matching in parallel structures and reducing computational effort. Additionally, [26] proposed a tensor-based iterative 2D DOA estimation method for L-shaped nested arrays to mitigate cross-terms caused by correlated co-array signals and noise components. This approach utilized high-order tensor decomposition to independently estimate azimuth and elevation angles, which were then effectively paired using the spatial cross-correlation matrix. I-B Deep learning-based methods With the advancement of deep learning (DL) [27, 28, 29], DL-based DOA estimation methods have emerged as a new research hotspot. The DOA estimation problem can be transformed into a neural network classification issue, where the network learns the mapping relationship between the input data and the DOA to classify spatial angles. Alternatively, it can be approached as a neural network regression problem. Numerous studies have demonstrated that the computational efficiency of DL-based DOA estimation algorithms has significantly improved. Fully connected neural network (FCNN) [30, 31] has been widely used in DOA estimation and related fields, but its large number of matrix multiplication operations resulted in high computational costs. Consequently, convolutional neural network (CNN), which uses convolutional operations instead of matrix multiplication in FCNN, have gradually emerged in the field of DOA estimation. [32] used CNN to estimate the noiseless covariance matrix of the generalized ULA and retrieved the DOA using root-MUSIC, thereby overcoming the grid mismatch issues encountered with grid-based methods. Moreover, improved CNN-based networks have been introduced to further enhance the DOA estimation performance. A multi-branch convolutional recurrent neural network with residual links and a weighted noise subspace network were introduced in [33] to improve the covariance matrix estimation, subspace partitioning, and peak-finding processes. Subsequently, multi-stage neural network-based DOA estimation was proposed to address the limitation of single neural networks in generalizing across all conditions. For example, a cascade neural network was proposed in [34] with two stages: the first stage used a CNN to obtain an initial DOA value, and the second stage used an FCNN to generate a tuning vector representing the difference between the true DOA and the closest discretization angle. The final DOA estimation was then obtained by adjusting the initial DOA estimate using the tuning vector. In [35], a cascade off-grid network consisting of an autoencoder composed of FC and a deep CNN with 2D convolutional layers was proposed, which used off-grid errors as labels to achieve off-grid DOA estimation based on its sparsity. The two-stage network discussed above facilitates in solving the mismatch caused by off-grid, thereby effectively improving DOA estimation performance. Moreover, given that the array outputs are complex-valued in the DOA estimation systems, complex valued neural network-based DOA estimation was suggested to extract the relevant DOA feature information from the complex-valued outputs of the arrays using complex parameters and complex arithmetic operations. Tan et al. [36] proposed a complex-valued convolutional neural network with phase normalization to extract explicit phase information from intermediate complex-valued feature maps for estimating the unknown source DOAs. A complex-valued residual angle estimation neural network containing complex linear and complex convolutional layers was introduced in [37], which combined the initial feature extraction module, the deep feature extraction module and the mapping module to perform DOA estimation. However, the labeled datasets are usually difficult to collect in realistic DOA estimation systems. Therefore, some semi-supervised learning[38], unsupervised learning[39], and transfer learning [40, 41] methods have been proposed to resolve the problem of limited labeled datasets. These methods can improve DOA estimation performance with less data and shorter training times compared to supervised learning-based methods. I-C Motivations and contributions Although the methods analyzed above can better balance the relationship between resolution, estimation accuracy, and computational complexity to improve DOA estimation performance at low SNR and with a small number of snapshots. However, most of the current studies conduct experimental validation using ideal target incident angles while neglecting noise. The ideal angle is difficult to realize because noise always exists in real communication. Moreover, directly using the angles estimated from advanced DOA methods, like root-MUSIC or DL, to calculate weighted coefficients can result in certain performance loss. Therefore, we will focus on how to implement a more practical, low-cost, high-performance as well as high time-efficiency passive DOA estimator. The critical contributions of this paper are summarized as follows: 1. On the basis of heterogeneous hybrid MIMO receive arrays, a multi-modal fusion DOA framework is established to achieve a low-latency, high-accuracy and more realistic DOA measurements. The framework involves three main steps: 1) FD subarray and all subarray groups of \rm{H}^{2}AD array form a rough DOA value and candidate angle sets, respectively, via the Root-MUSIC algorithm. 2) The true angle classes are distinguished from the candidate angle set using the clustering method based on the initial sample points provided by the FD subarray. 3) The two-part true angle sets are fused to achieve an improved DOA performance. 2. Based on the framework described above, an effective clustering methods, named global maximum similarity (GMaxCS), is proposed to attain more accurate true solutions from the candidate angle sets. These methods have the advantage of directly reflecting the angles between signal direction vectors and significantly enhancing the differentiation of direction information. A co-learning-assisted multi-modal iteration weighted fusion (MM-IWF)-based method is then proposed to fuse the two-part true angles using the estimated values. The clustering and fusion process described above results in two DOA estimation methods for eliminating phase ambiguity, named MM-IWF-GMinD and MM-IWF-GMaxCS. The weighted fusion coefficients can be more accurately obtained through iterative operations, making the final DOA estimation closer to the ideal values. Additionally, by utilizing the estimated angles to design the fusion coefficients, a more practical passive DOA estimator is achieved. Experimental results show that the proposed MM-IWF-GMaxCS outperforms MM-IWF-GMinD. 3. To further achieve a higher-accuracy DOA estimation, a co-learning-assisted multi-modal fusion network (fusionNet)-based on a three-layer FCNN is introduced to aggregate the inferred true angles of all subarray groups and a coarse DOA angle of FD subarray. The fusion weights, adaptively learned by fusionNet, are not directly correlated with the ideal DOA angles. Following the same pattern as the previous methods, the two novel corresponding approaches are called as follows: MM-fusionNet-GMinD and MM-fusionNet-GMaxCS. To validate the effectiveness of these approaches, the hybrid Cramér-Rao Lower Bound (CRLB) is utilized as a baseline. Simulation results illustrate that our proposed four clustering-fusion approaches–based co-learning DOA estimation can approach the desired DOA performance. Also, the proposed MM-fusionNet-GMinD and MM-fusionNet-GMaxCS show superior DOA estimation performance compared to the proposed MM-IWF-GMinD and MM-IWF-GMaxCS, especially in ultra low SNR range. The remaining parts of the study are arranged as follows. Section II shows the system model. A co-learning-assisted fusion DOA framework for heterogeneous hybrid structure is introduced in Section III. In Section IV, the DOA clustering and iteration weighted fusion methods for the proposed frameworks are detailed. Section V describes the high-performance fusionNet developed for DOA estimation. The CRLB of system model and computational complexity of our proposed approach are analyzed in Section VI. The simulation results are provided in Section VII and Section VIII offers conclusions. Notations: In this paper, uppercase letters and lowercase letters in bold typeface (i.e., \mathbf{B}, \mathbf{b}) are matrices and vectors, respectively. Signs (\cdot)^{H}, (\cdot)^{T},\|\cdot\|, |\cdot|, and \Re\{\cdot\} denote conjugate transpose, transpose, norm, modulus, and real part operations, respectively."
https://arxiv.org/html/2411.06833v1,Learning Interpretable Network Dynamics via Universal Neural Symbolic Regression,"Discovering governing equations of complex network dynamics is a fundamental challenge in contemporary science with rich data, which can uncover the mysterious patterns and mechanisms of the formation and evolution of complex phenomena in various fields and assist in decision-making. In this work, we develop a universal computational tool that can automatically, efficiently, and accurately learn the symbolic changing patterns of complex system states by combining the excellent fitting ability from deep learning and the equation inference ability from pre-trained symbolic regression. We conduct intensive experimental verifications on more than ten representative scenarios from physics, biochemistry, ecology, epidemiology, etc. Results demonstrate the outstanding effectiveness and efficiency of our tool by comparing with the state-of-the-art symbolic regression techniques for network dynamics. The application to real-world systems including global epidemic transmission and pedestrian movements has verified its practical applicability. We believe that our tool can serve as a universal solution to dispel the fog of hidden mechanisms of changes in complex phenomena, advance toward interpretability, and inspire more scientific discoveries.","From the Book of Changes in ancient China to the dialectical thinking in the West, there exists a common philosophical thought that the only constant is change. Undoubtedly, scientists have been striving to discover the laws of changes in complex phenomena, attempting to explain, forecast, and regulate all things [1], such as emergence [2], chaos [3], synchronization [4], and critical phenomena [5]. As a widely accepted modeling, the changing patterns of states from complex systems are generally governed by a set of nonlinear differential equations [6] as \dot{X}(t)=f(X(t),A,t), where X(t)\in\mathbb{R}^{N\times d} is the system states at time t, N and d are the number of system components (nodes) and the state dimension, respectively. A represents the extra information beyond the system states, such as topological interactions among system components. As shown in the above formula, the dynamic behaviors exhibited by complex systems are primarily contingent upon the intricate interdependence between their internal interactions A and dynamics governing equations f [6, 7]. This prompts people to seek reliable methodologies to formulate dynamics models of these complex systems [8, 9]. However, a remarkable challenge arises in this pursuit. In theoretically complete physical systems, laws of changes are delineated by well-discovered foundational principles [10, 11, 12], such as the electromagnetic laws dictating the microscale exchanges among propelled particles. For the majority of complex systems, f is agnostic, and equivalent foundational rules remain incompletely elucidated, such as global epidemic outbreak [13], extreme climate anomalies [14], and extinction of biological populations [15]. Consequently, this vague development has limited the exploration of these complex fields. Fortunately, in the current era of data acquisition gradually becoming easier, the emergence of data-driven technologies has assisted in increasing the frequency with which human experts discover system change patterns [16, 17, 18, 19, 20]. They can provide domain experts with richer and more meaningful inspiration for various fields, accelerating the process of scientific discovery, such as mathematics [21, 22] and physics [23, 24]. Although much excellent work has been developed to reconstruct the symbolic models for low-variate dynamics of complex systems [25], e.g., bivariate shear flow equation [26], trivariate metapopulation epidemic model [27], and up to 9-variate Newton’s law of gravitation [11], inferring governing equations for high-variate network dynamics remains important and challenging. This is mainly because the number of nodes N in network dynamics is usually large, such as the epidemic spreading with transmission areas or individual numbers ranging from tens to billions [13], and d is sometimes multi-dimensional, resulting in too many free variables (N\times d) in the equations and topological interactions with exponential growth, thereby increasing the complexity of inferring symbolic models [15]. At present, several cutting-edge work is attempting to deal with the discovery of governing equations from network dynamics [28, 20]. Two-phase sparse symbolic regression (TPSINDy) [8] simply parameterizes f as a learnable linear combination of pre-defined orthogonal or non-orthogonal elementary function terms. Although its equation inference efficiency is high, the rationality of pre-defined function terms directly affects the inference results, so sufficient and correct domain expert knowledge is usually required [29]. Another group of methods of using graph neural networks (GNN) to parameterize f overcomes excessive expert knowledge [30]. Still, due to the use of genetic programming (GP) to parse neural networks into symbolic equations, it brings the high-cost evolutionary search efficiency issue [10, 14]. Therefore, how to effectively balance expert knowledge and computational costs, while ensuring high computational efficiency, introducing only a small amount or no expert knowledge, lowering the threshold for use, and efficiently discovering governing equations remains a gap. To address the challenges above, we develop a universal neural symbolic regression tool that can automatically, efficiently, and accurately learn the changing patterns of complex system states by combining the excellent fitting ability from deep learning and the equation inference ability from pre-trained symbolic regression. Our analysis of various complex network dynamics scenarios from physics, biochemistry, ecology, epidemiology, etc., indicates that our tool has outstanding effectiveness and efficiency. It can accurately and efficiently discover the governing equations of network dynamics, even in the face of noisy and topologically missing data, and has achieved excellent results in chaotic systems and real-world systems including global epidemic transmission and pedestrian movements. We believe that our tool can serve as a new and general solution to eliminate the fog of hidden mechanisms of changes in complex phenomena from broad fields."
https://arxiv.org/html/2411.06824v1,"Combining Domain and Alignment Vectors to Achieve 
Better Knowledge-Safety Trade-offs in LLMs","There is a growing interest in training domain-expert LLMs that excel in specific technical fields compared to their general-purpose instruction-tuned counterparts. However, these expert models often experience a loss in their safety abilities in the process, making them capable of generating harmful content. As a solution, we introduce an efficient and effective merging-based alignment method called MergeAlign that interpolates the domain and alignment vectors, creating safer domain-specific models while preserving their utility. We apply MergeAlign on Llama3 variants that are experts in medicine and finance, obtaining substantial alignment improvements with minimal to no degradation on domain-specific benchmarks. We study the impact of model merging through model similarity metrics and contributions of individual models being merged. We hope our findings open new research avenues and inspire more efficient development of safe expert LLMs.","Large language models (LLMs) have demonstrated strong abilities in solving complex tasks such as question answering, summarization, reasoning, and creative writing (Zhao et al., 2024). However, these abilities are general-purpose, and LLMs can lack deep expertise in tasks requiring domain specialization (Ling et al., 2024). Naturally, there has been increasing research in developing domain-expert LLMs, either through complete pre-training on domain-specific data (Wu et al., 2023), continued pre-training of existing general-purpose LLMs (Sankarasubbu and Pal, 2024), or instruction-tuning pre-trained LLMs on domain data (Yue et al., 2023). While powerful, these domain expert models are often significantly less safe compared to their generalist counterparts as they either do not explicitly undergo safety alignment in case of pre-training from scratch and continual pre-training, or their safety alignment gets compromised due to domain-specific fine-tuning or instruction-tuning (Bhardwaj et al., 2024). Safety alignment of these domain expert models is crucial given their widespread adoption. However, this might be overseen due to a lack of resources, training data, alignment expertise, or concerns about potential degradation in the domain utility of models due to over-alignment – a phenomenon known as the alignment tax (Lin et al., 2024). Recently, model merging has emerged as an effective method for combining task-specific models into a single model without additional training (Ilharco et al., 2023). Model merging interpolates the parameters of multiple models, and has been extended to LLMs by leveraging task vectors. Task vectors capture the adjustments made to the weights of a general-purpose pre-trained model to create a task-specific one, calculated by subtracting the original model from the task model. Combining them has been shown to yield minimal performance degradation in multi-task evaluations (Yadav et al., 2023). Drawing inspiration from these findings, we expand the concept of task vectors to domain and alignment vectors for LLMs, computed by leveraging the corresponding domain adapted and aligned models of a pre-trained model, respectively. Leveraging state-of-the-art model merging methods, we propose MergeAlign, an efficient way to align domain expert models using their general-purpose instruction-tuned counterparts by interpolating domain and alignment vectors. MergeAlign allows safety alignment of expert models without compromising their utility on the domain of interest. We evaluate MergeAlign on two domains, namely medicine and finance, with instruction-pre-trained models (Cheng et al., 2024) using task arithmetic (Ilharco et al., 2023) as the basis for MergeAlign. We observe that the MergeAlign model experiences minimal performance degradation on the domain-specific benchmarks. However, it is able to achieve the alignment performance of the instruction-tuned general purpose model as evident from the evaluations using two safety benchmarks, achieved at very low cost. We further compare the merged model with the preference-tuned model, where the domain expert models undergo preference alignment training. Our evaluations show reduced knowledge-safety trade-offs in MergeAlign compared to preference tuning, while the former is also more cost-efficient. We further provide insights into the model similarity between the merged models and the preference-tuned models, and ablate on performing full model interpolation with Slerp (Shoemake, 1985) compared to using only the domain and alignment vectors. We hope our findings open a new avenue in researching more efficient alignment methods when dealing with domain-specific scenarios and promote development of safer models that are widely adopted for everyday uses. Our contributions can be summarized as, • We propose MergeAlign, an adaptation of model merging that efficiently endows domain-specific models with safety characteristics without compromising their utility. • We evaluate MergeAlign on models trained in two diverse domains, probing the alignment performance on two safety benchmarks. We observe that the merged model achieves strong alignment performance while retaining domain expertise. • Through extended comparisons with preference alignment methods such as DPO and ORPO and analyses using model similarity metrics, we provide justification for using merging as an effective, low-cost method to make domain expert models safer for widespread adoption."
https://arxiv.org/html/2411.06812v1,"Generative midtended cognition 
and Artificial Intelligence.","This paper introduces the concept of “generative midtended cognition”, that explores the integration of generative AI technologies with human cognitive processes. The term ""generative"" reflects AI’s ability to iteratively produce structured outputs, while ""midtended"" captures the potential hybrid (human-AI) nature of the process. It stands between traditional conceptions of intended creation, understood as steered or directed from with in , and extended processes that bring exo-biological processes into the creative process. We examine the working of current generative technologies (based on multimodal transformer architectures typical of large language models like ChatGPT), to explain how they can transform human cognitive agency beyond what the conceptual resources of standard theories of extended cognition can capture. We suggest that the type of cognitive activity typical of the coupling between a human and generative technologies is closer (but not equivalent) to social cognition than to classical extended cognitive paradigms. Yet, it deserves a specific treatment. We provide an explicit definition of generative midtended cognition in which we treat interventions by AI systems as constitutive of the agent’s intentional creative processes. Furthermore, we distinguish two dimensions of generative hybrid creativity: 1. Width: captures the sensitivity of the context of the generative process (from the single letter to the whole historical and surrounding data), 2. Depth: captures the granularity of iteration loops involved in the process. Generative midtended cognition stands in the middle depth between conversational forms of cognition in which complete utterances or creative units are exchanged, and micro-cognitive (e.g. neural) subpersonal processes. Finally, the paper discusses the potential risks and benefits of widespread generative AI adoption, including the challenges of authenticity, generative power asymmetry, and creative boost or atrophy.","You are surrounded by colleagues in a conference. You are about to explain your opinion about the French philosopher Gilles Deleuze: “He is very inspiring, but his writing is too …”, —you can’t quite find the right word, “… abstruse” says your colleague, “yeah, his writing is too abstruse (thanks)” you continue. That is the word you needed, the one you wanted but could not find. You do in fact hold the opinion that Deleuze is abstruse, you simply could not generate the sentence fluently, and you completed it by accepting the offered suggestion. You made it yours. At the current pace of evolution of generative technologies, it is not unreasonable to suggest a scenario in which similar conversations are increasingly generated (suggested) by AI (instead of your colleagues). How would this be possible? What kind of cognitive process would this be? How are this and parallel scenarios different to any technologically or socially extended cognitive processes we experienced before? How should we characterize them? Generative technologies, and Large Language Models in particular (systems like ChatGPT, Gemini, Llama, Mixtral, Claude, etc.), are deeply transforming agency. In a recent article, Barandiaran and Almendros (2024) introduce the concept of midtention to characterize the type of hybrid “intentional” agency that can result from a deep integration between human and LLM interaction: “Transformers are also bringing with them a much deeper meaning of extended agency (with deeper dialectical connotations). There is a form of extended agency that LLMs already offer that get more intentionally intimate than any previous known form. In fact, this extensional character is closer to the intentional character of the mind that deserves a proper name: midtentional. (…) The enormous complexity and regulatory capacity of the brain-body system (compared to that of the passive materiality of the tool and work environment) is now challenged by an ongoing activity of language automata, which are constantly reading us and writing (for) us. (…) This brings the power of transformer-human interaction closer to a proper cyborg agency, beyond any experience of instrumental, social or intersubjective agency we might have ever encountered before.” (Barandiaran & Almendros, 2024, pp. 29–30) In this paper we expand, deepen and generalise over this basic intuition beyond text-based LLMs to generative technologies. We develop the notion of “generative midtended cognition” as a new type of so-called “extended cognition” –that is, processes that are characterised as cognitive and are constituted by factors external to the cognizers brain-body. The meaning will unfold along the paper, but we shall advance that we have chosen the term “generative” to mean what, in different circumstances, might have been called “creative”, yet devoid of the strong connotations of the term. The term generative has, of course, also been chosen to name the generative AI technologies that have emerged recently (Akhtar, 2024; Jebara, 2012; Murugesan & Cherukuri, 2023). Altogether we want to stress processes that produce –generate– structured material outcomes: text, drawings, sound, voice, shapes, etc. Midtended or midtention is a neologism that wants to capture a space situated between traditional conceptions of intention or intended creation, that is, generated from within, and extended, processes that bring material exo-biological processes into the creative process. In the next section, we introduce so-called “generative AI technologies” and their internal workings. Then, we argue that existing theories of cognition that have incorporated external or environmental components into cognitive processing fall short of adequately capturing the new forms of cognition and agency that generative AI makes possible. Section 3 introduces the concept of generative midtention with the examples of drawing and writing. We then articulate the relationship between the concepts of intention and extension and characterise the singularity of midtended cognition. We provide an explicit definition of midtended cognition and distinguish two fundamental dimensions along which generative midtended cognition can be demarcated. Section 4 introduces some future scenarios that are relevant to deeper senses of generative midtended cognition, we evaluate some potential benefits and risks of authenticity, generative power asymmetry, and creative atrophy and alienation. Finally, section 5 recapitulates on the main ideas of the paper and offers some concluding remarks."
https://arxiv.org/html/2411.06810v1,JPEG AI Image Compression Visual Artifacts: Detection Methods and Dataset,"In recent years learning-based image compression methods have significantly improved and started to outperform conventional codecs. However, neural network approaches can unexpectedly introduce visual artifacts in some images. In this work, we propose methods to separately detect three types of artifacts (texture and boundary degradation, color change, and text corruption), to localize the affected regions, and to quantify the artifact strength. We consider only those regions which have been distorted just by the neural compression, while being recovered successfully by a conventional codec at a comparable bitrate. The proposed methods have been employed to collect artifacts for the JPEG AI verification model with respect to HM-18.0, the H.265 reference software. We processed about 350,000 unique images from the Open Images dataset with different compression quality parameters and created a dataset of 46,440 artifacts with validation using crowd-sourced subjective assessment. The proposed dataset and methods are valuable for testing neural network-based image codecs, identifying bugs in the codecs, and enhancing their performance. We make source code of the methods and the dataset publicly available.","Since 2018, researchers have actively pursued neural-based image compression and have demonstrated the superiority of neural codecs over classical models. In 2023 alone, more than 500 papers on neural compress methods have been published [], [], []. The capabilities of neural codecs have attracted considerable attention from both the scientific and industrial communities. As a result, the development of the JPEG AI standard for neural-based image compression has begun []. A neural codec based on the JPEG AI standard is already in development and has demonstrated 40% better performance compared to the advanced VVC intra codec. The standard is set to be published in October 2024, which will bring even more attention to the field. Neural codecs use deep learning models to compress images, that is, they learn nonlinear transformations, providing a more compact bit representation and better coding performance than traditional methods that use a predefined set of algorithms to compress data. As a result, neural codecs can produce higher quality images, but they can also introduce artifacts that are not present in traditional codecs. Therefore, the challenge arises to study the shortcomings of neural compression methods, develop metrics to detect neural compression artifacts, and create a dataset of such images. The assembled dataset can be used to test neural methods, identify their shortcomings and further improve them. Existing image quality assessment methods such as PSNR, SSIM and others are not suitable for this task for several reasons: 1. These algorithms respond poorly to small area neural compression artifacts that are nevertheless noticeable to humans, i.e., they correlate poorly with human perception of the image. 2. They are mainly aimed at obtaining a single number - an estimate of image quality, which does not allow to achieve artifact localization. 3. They are used as target metrics in most works on new compression methods, so it is incorrect to evaluate methods by them. 4. They assess image quality in general, i.e. they do not give any information about the type of artefact. The objective of this project was to develop metrics sensitive to neural artifacts, such as distortion of text, color, textures, and borders in compressed images. The metrics are designed to detect even minor distortions that significantly degrade the perception of the image. An artifact, in this context, is a distortion relative to the original image, present in an image compressed by a neural codec and absent or less noticeable in an image compressed by a classical codec. These metrics identify images that are less resilient to neural compression techniques compared to traditional algorithms. Using these metrics, a dataset was compiled, containing 53260 images with various types of artifacts and compression ratios. Additionally, a subjective verification of the automatically detected artifacts was performed to ensure accuracy. Our main contributions are: 1. Development of detection methods for three types of neural network compression artifacts. 2. Conducted subjective comparisons to validate identified artifacts. 3. Compilation of a dataset containing examples of neural compression artifacts. 4. Demonstrated higher correlation of our methods compared to existing image quality assessment methods with subjective evaluations."
https://arxiv.org/html/2411.06781v1,MP-PINN: A Multi-Phase Physics-Informed Neural Network for Epidemic Forecasting,"Forecasting temporal processes such as virus spreading in epidemics often requires more than just observed time-series data, especially at the beginning of a wave when data is limited. Traditional methods employ mechanistic models like the SIR family, which make strong assumptions about the underlying spreading process, often represented as a small set of compact differential equations. Data-driven methods such as deep neural networks make no such assumptions and can capture the generative process in more detail, but fail in long-term forecasting due to data limitations. We propose a new hybrid method called MP-PINN (Multi-Phase Physics-Informed Neural Network) to overcome the limitations of these two major approaches. MP-PINN instils the spreading mechanism into a neural network, enabling the mechanism to update in phases over time, reflecting the dynamics of the epidemics due to policy interventions. Experiments on COVID-19 waves demonstrate that MP-PINN achieves superior performance over pure data-driven or model-driven approaches for both short-term and long-term forecasting.","The COVID-19 pandemic has claimed over 7 million lives for just 4 years111https://www.worldometers.info/coronavirus/coronavirus-death-toll. Let us pause for a moment and consider a hard counterfactual question: Could the majority of these lives have been saved if we had predicted the spread better at the onset of the pandemic and acted more effectively? In fact, the world did all it could: modelling, forecasting, implementing lockdowns, developing vaccines, and much more. In the absence of proper understanding of the viruses’ nature and with limited data available when a wave just started, epidemiologists had to make assumptions in model-driven methods, such as those in the mechanistic SIR family [10] or in detailed agent-based simulations [11]. When some data became available, for example, after a month, data-driven methods, as preferred by the data science community, could be employed to detect trends in the time-series [25, 21]. A key open challenge is the complex interplay of evolving interventions, human factors and technological advances driving the epidemic waves [1]. Figure 1: A representative case of forecasting COVID-19 at 35 days of the wave. Our hybrid multi-phase method MP-PINN strikes a balance between model-driven and data-driven approaches, and hence is more accurate in both short/long-term forecasting. If anything, the extremely high death toll has profoundly demonstrated one thing: We have failed to predict the spread of COVID-19 virus variants. Fig. 1 clearly illustrates this failure. As seen, model-driven methods such as mechanistic SIR models capture the overall shape of the wave but are inadequate in reflecting current data and the changing reality on a daily basis. This might be due to the rigid and strong assumptions made at the modelling time. Data-driven methods, such as those using deep neural networks, fit the new evidence better but fail to capture the long-term underlying mechanisms. Clearly, a better approach is needed to (a) capture both the short-term and long-term processes [22, 18], and (b) dynamically calibrate the models in the face of new evidence [19]. To this end, we propose MP-PINN (which stands for Multi-Phase Physics-Informed Neural Network) to overcome these limitations. MP-PINN employs a recent powerful approach known as Physics-Informed Neural Network (PINN), which trains a neural network to agree with both empirical data and epidemiological models. However, PINNs alone are not sufficient to reflect reality: We must account for the complex interplay of evolving factors driving the epidemic waves, such as changing regulations, emerging information, and shifting public sentiment, all of which influence the pandemic’s trajectory. This is where MP-PINN comes in: Instead of assuming a single set of parameters for the entire wave, we allow the model to vary across multiple distinct phases, each represented by a set of SIR parameters. This brings adaptability into MP-PINN. We demonstrate MP-PINN on COVID-19 data sets collected from COVID-19 data from 21 regions in Italy in the first half of 2020. The results show that MP-PINN achieves superior performance in both short-term and long-term forecasting of COVID-19 spread. In particular, MP-PINN outperforms traditional SIR models, pure data-driven approaches (MLP), and single-phase PINNs. See Fig. 1 for a representative case demonstrating the efficacy of MP-PINN."
https://arxiv.org/html/2411.06735v1,Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data,"Current forecasting approaches are largely unimodal and ignore the rich textual data that often accompany the time series due to lack of well-curated multimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a carefully curated, time-aligned text and time dataset for multimodal forecasting. Our dataset is composed of sequences of numbers and text aligned to timestamps, and includes data from two different domains: climate science and healthcare. Our data is a significant contribution to the rare selection of available multimodal datasets. We also propose the Hybrid Multi-Modal Forecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and time series data using shared embeddings. However, contrary to our expectations, our Hybrid-MMF model does not outperform existing baselines in our experiments. This negative result highlights the challenges inherent in multimodal forecasting. Our code and data are available at https://github.com/Rose-STL-Lab/Multimodal_Forecasting.","Deep learning has become the predominant method in forecasting large-scale time series Zhou et al. (2022); Wang et al. (2022); Woo et al. (2023), but most existing methods consider time series as a single data modality. In practice, time series data do not exist in isolation and there are rich text meta-data available. Large Language Models (LLM) such as GPT Brown et al. (2020) and LLaMA Touvron et al. (2023) have demonstrated marvelous success in processing and understanding text data. It is intriguing to exploit text as an new modality to improve forecasting. On one hand, text can provide context to the dynamics governing the associated time series. On the other hand, generating text alongside numerical forecasts also offer valuable interpretations for the predictions. However, effectively combining and forecasting these two data modalities remains a complex challenge Zhang et al. (2024). The differences in structure and content between numerical time series and text data pose challenges to their simultaneous integration in forecasting models. Traditionally, the community has studied time series and natural language processing separately. To the best of our knowledge, there is no large-scale, well-curated, paired time series and text dataset for forecasting. The only exception is Time-MMD Liu et al. (2024) but their text data is synthetic. Several early attempts have been made at multimodal forecasting. Kumar et al. (2022); Obst et al. (2019) propose to augment time series forecasting with textual meta-data, but only predict time series. Other works proposed to treat time series forecasting as a “modified language task”. For example, LLMTime Gruver et al. (2023) proposes a zero-shot approach by representing the time series as a string of numerical digits inputs to LLMs. Time-LLM Jin et al. (2023) transforms the time series input into text prototype representation and projects the output back to numerical forecasts. Others use a pre-trained language model with fine-tuning or prompt engineering for forecasting Zhou et al. (2023); Cao et al. (2023). While these approaches show promise in leveraging LLMs for time series forecasting, they primarily focus on fitting numerical data into the language model paradigm rather than addressing the broader challenge of multimodal forecasting. In this work, we focus on the challenge of joint multimodal forecasting—forecasting both time series and textual event data simultaneously. To address this, we introduce the TimeText Corpus (TTC), a carefully curated, time-aligned text and time dataset for multimodal forecasting. Our dataset is composed of sequences of numbers and text aligned to timestamps, and includes data from two different domains: climate science and healthcare. Additionally, we propose a hybrid approach to model this data by integrating both modalities, aiming to exploit the complementary nature of numerical and textual data for better forecasts. Our contributions include: • Simultaneous Encoding of Multimodal Data: We propose and experiment with techniques for jointly encoding numerical and textual data into shared embeddings for simultaneous multimodal forecasting. • Multimodal Dataset Contributions: We curate two datasets: one from Mimic-III (medical), and the other from the National Weather Service (climate), which can serve as benchmarks for future research. In the following sections, we detail our TimeText Corpus (TTC) and propose the Hybrid Multi-Modal Forecaster (Hybrid-MMF), a model that jointly forecasts both text and time series data. Our experiments show that while our model demonstrates competitive performance, marginal improvements were observed over baselines in several cases."
https://arxiv.org/html/2411.06711v1,Anytime Probabilistically Constrained Provably Convergent Online Belief Space Planning,"Taking into account future risk is essential for an autonomously operating robot to find online not only the best but also a safe action to execute. In this paper, we build upon the recently introduced formulation of probabilistic belief-dependent constraints. We present an anytime approach employing the Monte Carlo Tree Search (MCTS) method in continuous domains. Unlike previous approaches, our method assures safety anytime with respect to the currently expanded search tree without relying on the convergence of the search. We prove convergence in probability with an exponential rate of a version of our algorithms and study proposed techniques via extensive simulations. Even with a tiny number of tree queries, the best action found by our approach is much safer than the baseline. Moreover, our approach constantly finds better than the baseline action in terms of objective. This is because we revise the values and statistics maintained in the search tree and remove from them the contribution of the pruned actions.","Casting decision-making under uncertainty as a Partially Observable Markov Decision Process (POMDP) is considered State-Of-The-Art (SOTA). Under partial observability the decision-making agent does not have complete information about the state of the problem, so it can only make its decisions based on its “belief” about the state. In a continuous domains in terms of POMDP state, the belief, in a particular time index, is the Probability Density Function (PDF) of the state given all concurrent information in terms of performed actions and received observations in an alternating manner, plus the prior belief. A POMDP is known to be undecidable [1] in finite time. Introducing various constraint formulations into POMDP is essential for, e.g., ensuring safety [2], [3] and efficient Autonomous Exploration [4]. Yet, the existing online approaches in anytime setting have problems and therefore fall short of providing reliable and safe optimal autonomy. This crucial gap we aim to fill in this paper. Similar to almost any online POMDP solver today such as MCTS, our method constructs a belief tree and uses the tree to represent the POMDP policy. We prune dangerous actions from the belief tree and revise the values and statistics that an MCTS tree maintains. Anytime, our search tree contains only the safe actions in accord to our definition of safe action, which will appear shortly. Our work lies in continuous domain in terms of actions and the observations. In such a setting, there are approaches to tackle averaged cumulative constraint using anytime MCTS methods [5], [6]. We now linger on the explanation of what the averaged constraint is. Under partial observability, namely in the POMDP setting, there are naturally two stages to consider in order to introduce a constraint. The first stage arises from the belief itself. Usually, at this stage, the state-dependent payoff operator is averaged with respect to the corresponding belief to obtain a belief-dependent one. It is then summed up to achieve a cumulative payoff. We use the term payoff to differentiate between reward operator and emphasize that a belief-dependent payoff constraint operator shall be as large as possible as opposed to the cost operator. The second stage arises from the distribution of possible future observations episodes. At this stage, commonly, the cumulative payoff is again averaged but with respect to future observations episodes and then thresholded, thereby forming an averaged cumulative constraint. Such a formulation is sufficient for ensuring safety in limited cases as we will further see in Section VI-A. This is because it permits deviations of the individual values within the summation. Let us now describe the MCTS methods mentioned above to tackle averaged cumulative constraint. The seminal paper in this direction is [7]. It leans on the rearrangement of the constrained objective using the occupancy measure described in [8]. Such a reformulation is appealing since it transforms the problem into linear programming bringing convexity to the table and enjoying from strong duality. The authors of [5] extend the approach from [7] to continuous spaces. Still, both papers [7] and [5] assure constraint satisfiability only at the limit of the convergence of the iterative procedure, namely in infinite time. Since these are iterative methods, to assure anytime constraint satisfiability we need to project the obtained occupancy measure at each iteration to the space defined by the constraint. If dual methods are involved [9] such a projection does not make much sense, e.g., the projection might lead to a step direction vector on the boundary of all the constraints, making it zero vector. Employing the primal methods in continuous spaces also appears to be problematic since the summations in [7] are transformed into integrals. The paper [6] provides some sort of anytime satisfiability by introducing high-level action primitives (options). Still, [6] suffers from limitations, e.g. it requires crafting low-level policies, meaning knowing how the robot shall behave a priori. In addition, the options shall be locally feasible. Additionally, for efficiency reasons, the duality based approaches perform a single tree query of the MCTS, instead of running MCTS until convergence in the maximization of the Lagrangian dual objective function phase (See section 8.5.2 in [9]) of dual ascend. In all three papers [7], [5], [6] the averaged cumulative constraint is enforced solely from the root of the belief tree. This is suboptimal since within a planning session it is not taken into account that the constraint will be enforced at the future planning sessions. In other words, the contemplation of a robot about the future differs from its actual future behavior. This aspect has been fixed by [10]. As we will further see in Section IV, our approach naturally handles this problem. Moreover, [10] assures fulfillment (admission) of the recursive averaged cumulative constraint anytime with respect to search tree constructed partially with the reward bounds and partially with rewards themselves. Yet, the algorithm presented in [10] requires that the value function is bounded on the way down the tree to assure the exploration. This is commonly achieved by assuming that the state-dependent reward is trivially bounded from above and below. This does not hold for general belief-dependent reward functions. Moreover, the exploration outlined in that paper is valid for discrete spaces only. All in all, the extension of that work to continuous spaces and belief-dependent rewards requires clarification. Support for general belief dependent rewards and payoff/cost operators and MCTS convergence We now clarify whether or not the mentioned above solvers support belief-dependent cost/payoff operators and rewards. It was suggested in [3],[4] that general belief-dependent payoff/cost operators are extremely important. As mentioned in [3] Value-at-Risk (VaR) and Conditional VaR (CVaR) over the distance to the safe space allow for control of the depth the robot can plunge into the obstacle. To rephrase that, these operators measure how bad the disaster (collision) will be. See Appendix D, for details. The Information Gain discussed in [4] is relevant for exploration. The paper [4] discussed the general belief-dependent averaged constraint of the form (38) in a high dimensional setting and in the context of Information Gain. The iterative schemes in [7], [5] lean on the convergence of MCTS. It has been shown in [11] that even in discrete spaces and with bounded rewards it can take a very long time for MCTS to converge. In the case of unbounded reward or the cost-augmented objective of [7], [5], the MCTS may converge slowly. If such an augmented reward has a large variance, it will be needed a huge amount of tree queries for action-value estimate (to be defined shortly) at each belief node of the belief tree to converge. The large variance can be the result of an unrestrained variability of the rewards or a large Lagrange multiplier. There are several constraint formulations for POMDP. Below we discuss the most prominent techniques one by one. Shielding POMDPs There is a growing body of literature on shielding POMDPs. The shield is a technique to disable the actions that can be executed by the agent and violate the shield definition. There are several shield definitions. Online methods [12], [13] in this category utilize Partially Observable Monte-Carlo Planning (POMCP) algorithm [14]. These works have the same problems we are solving in this paper: one way or another, the actions violating the shield definition participate in the planning procedure, yielding a suboptimal result. The work [13] enforces the shied outside the POMCP planning. As we further show, not considering safety in the future times, namely within the planning session, can lead to a suboptimal planning result. Chance Constrained (CC) Online Planning A recent work [15] tackles online planning with chance constraints in an anytime setting. This paper suggests using a Neural Network (NN) to approximate CC enforced, with an adaptive threshold, from each belief considered in the planning session. This work trains NN offline. Therefore the error stemming from the discrepancy of simulated and real data is unknown. Moreover, it is not clear how complex the NN shall be to achieve zero loss in training to ensure no error in CC approximation, so even if no discrepancy discussed before exists, the NN inference may be slow. In this method, dangerous actions do not participate in the planning session. Safe control Under Partial Observability There are a variety of robust control approaches natively tailored for continuous state/action/observation spaces [16],[17]. However, these methods are usually limited to very specific rewards/objectives and tasks, such as reaching a goal state or to be as close as possible to a nominal trajectory. Moreover, in both papers the system dynamics are control-affine. Without this assumption, it is not clear how to enforce the constraint through a derivative of the barrier function. I-A Contributions Below we list down our contributions in the same order as they appear in the manuscript. • By constraining directly the problem space and not the dual space we present an anytime MCTS based algorithm for safe online decision making with safety governed by a Probabilistic Constraint (PC). Our approach enjoys anytime safety guarantees with respect to the belief-tree expanded so far and works in continuous state, action and observation spaces. When stopped anytime, the action returned can be considered as the best safe action under the safe future policy (tree policy) expanded so far. Our search tree solely consists of safe actions. We prove convergence in probability with an exponential rate of our approach. • Another contribution on our end is constraining the beliefs with incorporated outcome uncertainty stemming from an action performed by the robot and without incorporating the received observation. This is alongside the constraint over the posterior belief with included last observation. To the best of our knowledge, no previous works do that. • We also spot a problem happening in duality based approaches arising from averaging unsafe actions in MCTS phase. Therefore, an additional contribution of ours is an analysis of this phenomenon. • We simulate our finding on several continuous POMDP problems. I-B Notation We use the \square as a placeholder for various quantities. The values in \square can be replaced by one of the respective options. We also extensively use the indicator function notation, which is \mathbf{1}_{A}(\square). This function equals to one if and only if \square{\in}A. By lowercase letters we denote the random variables of their realizations depending on context. By the bold font we denote vectors of operators in time of different lengths. We denote estimated values by \hat{\square}. I-C Paper Roadmap This paper proceeds with the following structure. Section II presents relevant background. Section III then formulates the problem. Section IV presents our approach. Section VI discusses our baseline. Section VII gives experimental validation of the proposed methodology. Finally, Section VIII concludes the paper."
https://arxiv.org/html/2411.06710v1,Model Fusion through Bayesian Optimization in Language Model Fine-Tuning,"Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model, one effective solution is model fusion, which combines multiple models in a parameter space. However, we observe a large discrepancy between loss and metric landscapes during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization. In addition, to effectively select hyperparameters, we establish a two-stage procedure by integrating Bayesian optimization processes into our framework. Experiments across various downstream tasks show considerable performance improvements using our Bayesian optimization-guided method. Code will be available at: https://github.com/chaeyoon-jang/bomf.git.","The field of Natural Language Processing (nlp) has significantly advanced with the pre-training of Transformer-based models on large amounts of texts without supervision. In general, these pre-trained networks are fine-tuned on supervised downstream tasks to solve particular tasks. The rise of Large Language Models (llms) such as gpt [50] and llama [63] has increased demands for huge memory and computing during fine-tuning on downstream tasks. In response, low rank approximation methods such as Low-Rank Adaptation (lora) [22] and Quantized Low-Rank Adaptation (qlora) [11] have been adopted recently to fine-tune the llm. However, the fine-tuning of Pretrained Language Models (plms) exhibits high sensitivity to marginal variations in hyperparameters such as learning rate and batch size, often leading to training failure and the performance drop of a fine-tuned model [45], while searching hyperparameters requires a vast amount of resources. An effective strategy to seek an optimal model among multiple candidates is model ensembling, which yields impressive performance on generalization and robustness [33]. However, traditional ensemble methods lead to several drawbacks including the space and computational costs that linearly scale with the number of models involved. These issues are particularly pertinent for llms, since individual models are costly to train and test. Therefore, we can alternatively utilize model fusion to aggregate multiple models into a single proficient model on a parameter space. One of its simplest forms, known as Stochastic Weight Averaging (swa) [25], involves taking the average of model parameters obtained during an optimization process. Despite its simplicity, swa and its variants have proven successful across various tasks, notably in computer vision [25, 42, 6, 46]. Recent advancement in this field is the concept of Model Soups, which has been introduced by Wortsman et al. [70]. This approach weight-averages a set of models, obtained from multiple fine-tuning runs with different hyperparameters to create a powerful model that outperforms both individual and ensemble models. The effectiveness of model fusion has predominantly been explored in the visual domain. For instance, while Model Soups have shown considerable improvements in image classification, they have not demonstrated superiority in the nlp tasks [70]. The existing averaging methods like swa make use of their ability to encourage a fused model to locate on the center of the flatter area near local optima [25, 20], as loss landscapes are analogous to metric landscapes in computer vision tasks. Unfortunately, for plms, loss landscapes are substantially mismatched to metric landscapes, so that the flat loss minimum found by swa does not necessarily correspond to the flat metric minimum making a simple averaging method fail to find a superior model. In this paper, we present a novel model fusion approach with an efficient hyperparameter selection strategy, denoted as Bayesian Optimization Model Fusion (bomf), specifically designed to fine-tune plms. To motivate our method, we start by illustrating two empirical analyses. Firstly, we demonstrate that the existing model fusion techniques are not suitable for plms. Secondly, we highlight that the optimal hyperparameters for plms exhibit consistency on varying the number of frozen layers or the rank used in the lora setting [22]. Based on these findings, we introduce our proposed method to build a single model, aggregated through the weighted combination of individual models. Supposing that evaluation metrics are non-differentiable, we employ Bayesian Optimization (bo) [5, 18], which is a black-box optimization technique, in developing our model fusion method. To the best of our knowledge, this is the first study that utilizes bo in the context of model fusion, in order to achieve the following objectives: • Utilization of Both Metrics and Loss Functions in Model Fusion. Instead of running bo with an averaged target metric, we use Multi-Objective Bayesian Optimization (mobo) that considers both metrics and loss functions for model fusion. Despite low correlations between loss and metric values, we find that incorporating loss values still serves as useful guidance. • Two-Stage Model Fusion. We devise our model fusion process as a two-stage bo procedure. One is for optimizing hyperparameters in fine-tuning and the other is dedicated to our model fusion method. The objective of the first stage is to maximize gains from the second stage to find hyperparameters leading to the optimal fused model after the bo of the second stage. We demonstrate the effectiveness of bomf on several nlp tasks, including both Natural Language Understanding (nlu) and Natural Language Generation (nlg), with roberta, Text-to-Text Transfer Transformer (t5) and llama. Through these comprehensive experiments, we assess the performance of bomf in diverse nlp tasks and uncover the interesting properties of our approach through various ablation studies."
https://arxiv.org/html/2411.06666v1,Adversarial Detection with a Dynamically Stable System,"Adversarial detection is designed to identify and reject maliciously crafted adversarial examples(AEs) which are generated to disrupt the classification of target models. Presently, various input transformation-based methods have been developed on adversarial example detection, which typically rely on empirical experience and lead to unreliability against new attacks. To address this issue, we propose and conduct a Dynamically Stable System (DSS), which can effectively detect the adversarial examples from normal examples according to the stability of input examples. Particularly, in our paper, the generation of adversarial examples is considered as the perturbation process of a Lyapunov dynamic system, and we propose an example stability mechanism, in which a novel control term is added in adversarial example generation to ensure that the normal examples can achieve dynamic stability while the adversarial examples cannot achieve the stability. Then, based on the proposed example stability mechanism, a Dynamically Stable System (DSS) is proposed, which can utilize the disruption and restoration actions to determine the stability of input examples and detect the adversarial examples through changes in the stability of the input examples. In comparison with existing methods in three benchmark datasets(MNIST, CIFAR10, and CIFAR100), our evaluation results show that our proposed DSS can achieve ROC-AUC values of 99.83%, 97.81% and 94.47%, surpassing the state-of-the-art(SOTA) values of 97.35%, 91.10% and 93.49% in the other 7 methods.","Adversarial examples(AEs) are intentionally perturbed examples containing artificial noises, inducing misclassification in deep neural networks(DNNs). This susceptibility of DNNs to AEs has raised extensive concerns Fawzi et al. (2018) due to the diverse security applications of DNNs, such as face recognition Sharif et al. (2016), autonomous driving Liu et al. (2023), and the medical domain Uwimana and Senanayake (2021), etc. Consequently, an advanced defense method is necessary to mitigate the risk from adversarial examples. A widely accepted hypothesis Szegedy et al. (2014) suggests that the success of AEs is attributed to their capability to shift the data flow from high-probability regions to low-probability regions outside the normal training domain of classifiers. Two types of defense methods: adversarial training and adversarial detection have been proposed based on the hypothesis of data flow patterns. Initially, the adversarial training Goodfellow et al. (2015) is introduced, in which researchers incorporate AEs into the training data to broaden the training data flow, thereby enhancing the robustness to AEs. Subsequently, adversarial detection emerges, aiming to extract manifold features from inputs to differentiate between benign and malicious inputs and subsequently reject the latter. For the reason that adversarial training leads to a notable decrease in classification accuracy Tsipras et al. (2019) and generalization ability Laidlaw and Feizi (2019), adversarial detection is widely adopted, as it does not weaken the original performance of models. Currently, many detection methods have been proposed and are mainly divided into mathematical statistical and variational analytical categories. Mathematical statistics methods, like Kernel Density and Bayesian Uncertainty (KDBU) Feinman et al. (2017) and Local Intrinsic Dimensionality (LID) Ma et al. (2018), calculate density information based on the intermediate layer distribution of input data. Furthermore, Lee et al. (2018) introduces the Mahalanobis distance to improve the assessment of high-dimensional data, while Joint statistical Testing across DNN Layers for Anomalies (JTLA) Raghuram et al. (2021) incorporates a meta-learning system to combine several existing methods. However, these mathematical-statistical methods, which are static and lacking considering dynamic characteristics of inputs over the input transformations, perform weakly in the generalization domain. Variational analytical methods calculate the variations of the outputs in the target model to differentiate AEs and NEs when the input is transformed by rotation, flip, shift, etc. Feature Squeezing (FS) Xu et al. (2018) involves bit reduction and shifting inputs, while Lightweight Bayesian Refinement (LIBRE) Deng et al. (2021) utilizes Bayesian network structures to extract the uncertainty of inputs and Expected Perturbation Score (EPS) Zhang et al. (2023) introduces a diffusion model to augment input diversities. However, these methods only empirically demonstrate the difference of output variations on both AEs and NEs in the target model, lacking theoretical support, resulting in unreliability to new attack strategies such as Square Andriushchenko et al. (2020). To solve these issues, we construct the Dynamically Stable System(DSS) based on the Lyapunov stability theory. Our system falls under the variational analytical category, as the stability module of the system involves disruption and restoration actions. In the stability module, we iteratively disrupt and restore the inputs to obtain dynamic stability features. Normal and noisy examples, when subjected to the disruption and restoration process, tend to maintain stability, as shown in Fig.1 (a) and (b). On the contrary, AEs exhibit a tendency to diverge as the system progresses and will be detected based on this tendency, as shown in Fig.1 (c). In the monitor module, we distinguish AEs from NEs based on different stability features provided by the stability module. Figure 1: The disruption and restoration actions through the stability module (a):the process of normal examples (b):the process of noisy examples (c):the process of adversarial examples Our main contribution can be summarized as below: • Firstly, we prove that the process of adversarial examples generation can be considered as a Lyapunov dynamic stable system, and we propose an example stability mechanism, in which a novel control term is proposed to be involved in the generation. The proposed control term is proven to ensure the normal examples achieve the original examples stability. That is, with the control term, the stability of outputs of normal examples will be few changes to that of the original examples, while the stability of outputs of adversarial examples will achieve high alternations to that of the original examples. • Then, based on the proposed example stability mechanism, we propose a Dynamically Stable System, which can effectively detect the adversarial examples according to the changes in the stability of our original examples. The proposed DSS consists of a stability module and a monitor module. In the stability module, the input original example is disturbed by the malicious gradient information with the proposed control term and then we introduce an inpainting model to repair the disturbed examples. In this way, the original normal examples will achieve similar stability to the repaired examples, while the adversarial examples will achieve much different stability to the repaired examples. Hence, the monitor module can detect the adversarial examples according to the changes in the stability of input original examples. • Lastly, extensive experiments are conducted to demonstrate the effectiveness of our proposed DSS in three benchmark datasets(MNIST, CIFAR10, and CIFAR100). The evaluation results show that our DSS outperforms the best results, i.e., achieving the average AUC of 99.83%, 97.81% and 94.47% respectively, in comparison with 7 existing adversarial detection methods. Furthermore, the evaluation in terms of generalization, intensity, ablation and sensitivity has been conducted as well and the results demonstrate the outstanding performance of our DSS compared to other methods."
https://arxiv.org/html/2411.06616v1,MEANT: Multimodal Encoder for Antecedent Information,"The stock market provides a rich well of information that can be split across modalities, making it an ideal candidate for multimodal evaluation. Multimodal data plays an increasingly important role in the development of machine learning and has shown to positively impact performance. But information can do more than exist across modes— it can exist across time. How should we attend to temporal data that consists of multiple information types? This work introduces (i) the MEANT model, a Multimodal Encoder for Antecedent information and (ii) a new dataset called TempStock, which consists of price, Tweets, and graphical data with over a million Tweets from all of the companies in the S&P 500 Index. We find that MEANT improves performance on existing baselines by over 15%, and that the textual information affects performance far more than the visual information on our time-dependent task from our ablation study.","Recently, multimodal models have garnered serious momentum, with the release of large pretrained architectures such as Microsoft’s Kosmos-1 Huang et al. (2023) and OpenAI’s GPT-4 OpenAI et al. (2023). Their general use has exploded in many domains, such as language and image processing Lu et al. (2019); Kim et al. (2021); Huang et al. (2023). Particularly interesting to this study is the deployment of multimodal models on time-dependent environments, where recent successes have shown that event driven models processing multiple modalities are far more performant on stock market tasks than previously state of the art (SOTA) algorithms focusing purely on price information Li et al. (2021); Zhang et al. (2022). Language data from news and social media sources have shown to greatly increase the performance of models for price prediction Li et al. (2021); Zhang et al. (2022); Bybee et al. (2023); Mittermayer and Knolmayer (2006); Xu and Cohen (2018). However, these approaches typically lack attention components specifically designed to process inputs with sequential, time-dependent information Li et al. (2021); Sun et al. (2017); Zhang et al. (2022); Xu and Cohen (2018). This type of data is particularly important when making predictions about stock prices or market movements, as price prediction is a time series task Zhang et al. (2022); Xu and Cohen (2018). In this work, we introduce MEANT, a multimodal model architecture with a novel, temporally focused self-attention mechanism. We extract image features using the TimeSFormer architecture Bertasius et al. (2021) to find relationships in longer range information (i.e a graph of stock prices over a month), while extracting language features from social media information to pick up more immediate trends (e.g.: Tweets pertaining to stock prices over a five day period). Furthermore, we release TempStock, a multimodal stock-market dataset that is designed to be sequentially processed in chunks of varying lag periods."
https://arxiv.org/html/2411.06606v1,Gen-AI for User Safety: A Survey,"Machine Learning and data mining techniques (i.e. supervised and unsupervised techniques) are used across domains to detect user safety violations. Examples include classifiers used to detect whether an email is spam or a web-page is requesting bank login information. However, existing ML/DM classifiers are limited in their ability to understand natural languages w.r.t the context and nuances. The aforementioned challenges are overcome with the arrival of Gen-AI techniques, along with their inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains.In this manuscript, we provide a comprehensive overview of the various work done while using Gen-AI techniques w.r.t user safety. In particular, we first provide the various domains (e.g. phishing, malware, content moderation, counterfeit, physical safety) across which Gen-AI techniques have been applied. Next, we provide how Gen-AI techniques can be used in conjunction with various data modalities i.e. text, images, videos, audio, executable binaries to detect violations of user-safety. Further, also provide an overview of how Gen-AI techniques can be used in an adversarial setting. We believe that this work represents the first summarization of Gen-AI techniques for user-safety.","Machine Learning and data mining techniques are used across domains to detect violation of user safety. Examples include classifiers used to detect whether an email is spam or a web-page is requesting bank login information. However, existing ML/DM classifiers are limited in their ability to understand natural languages w.r.t the context and nuances. The aforementioned challenges are overcome with the arrival of Gen-AI techniques, along with there inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains. In this manuscript, we provide an overview of the various domains across which user safety can be violated. In particular, we provide more information on how Generative Artificial Intelligence (Gen-AI) techniques can be used towards reduction of egregious abuse (e.g. phishing, malware, anomaly detection, counterfeit, fraud prevention), misinformation and disinformation (e.g. fake news , deepfake detection), increase in content moderation, awareness about mental health (e.g. cyber-bullying prevention, crisis support) and towards robust physical safety (e.g. accessibility, autonomous systems). Further, we discuss how Gen-AI techniques can be used across various data modalities. In particular, we present how Gen-AI techniques can be used to detect user-safety violations in text and rather outperform all previous techniques w.r.t NLP tasks such as entity recognition, question answering and sentiment analysis. Further, Gen-AI techniques with their inherent ability to parse and understand images provides an easy mechanism to detect image manipulation, deepfake detection. The advantages of Gen-AI techniques go beyond text and images to other data modalities such as videos, audio and executable binaries. We also discuss how Gen-AI techniques can be used in an adversarial setting. In particular, we present how these techniques can be used to attack at scale (e.g. mass spam). Further, the attacks become more intelligent as these techniques can target humans more effectively and engage with humans with a similar cognitive capacity. Gen-AI techniques along with reinforcement learning techniques can be used to create more sophisticated attacks while using feedback from the last failure. Further, Gen-AI techniques can also make these attacks look very personalized (e.g. deep-fakes) and second order effects (e.g. Gen-AI imitating human sounding text). The organization of the paper is as follows. In section II, we provide a comprehensive overview of the various user safety domains, where Gen-AI techniques can be applied. In section-3, we discuss the various data modalities across which Gen-AI techniques can be utilized to protect user safety violation. In Section-4, we present, how Gen-AI techniques can also be used in an adversarial setting. In Section-5, we present our opinion on what does the future look like for Gen-AI techniques. Finally, in section-6, we conclude this manuscript."
https://arxiv.org/html/2411.06601v1,OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control,"Efficient traffic signal control (TSC) is essential for modern urban mobility, but traditional systems often struggle to adapt to the complex and dynamic nature of city traffic. While Multi-Agent Reinforcement Learning (MARL) offers promising adaptive solutions, online MARL methods require a significant amount of interactions with the environment, which can be expensive and time consuming. Offline MARL addresses these concerns by leveraging historical traffic data for training, but it faces challenges due to the heterogeneity of behavior policies in real-world datasets—a mix of different controllers makes learning difficult. We introduce OffLight, a novel offline MARL framework specifically designed to handle heterogeneous behavior policies within TSC datasets. OffLight employs Gaussian Mixture Model Variational Graph Autoencoder (GMM-VGAEs) to model the complex distribution of behavior policies, enabling effective learning from diverse data sources. To enhance coordination between agents, we integrate Graph Attention Networks (GATs), allowing agents to make informed decisions based on aggregated information from neighboring intersections. Furthermore, OffLight incorporates Importance Sampling (IS) to correct for differences between the behavior and target policies and utilizes Return-Based Prioritized Sampling (RBPS) to focus on high-quality experiences, thereby improving sample efficiency. Extensive experiments across three real-world urban traffic scenarios—Jinan (12 intersections), Hangzhou (16 intersections), and Manhattan (196 intersections)—demonstrate that OffLight significantly outperforms existing offline RL methods. Notably, OffLight achieves up to a 7.8% reduction in average travel time and an 11.2% decrease in queue length compared to baseline algorithms, particularly in datasets with mixed-quality data. Our ablation studies confirm the effectiveness of OffLight’s components in handling data heterogeneity and enhancing learning performance. These results highlight OffLight’s ability to accurately model heterogeneous behavior policies, mitigate the impact of suboptimal data, and scale to large urban networks. By addressing the critical challenges of offline MARL in TSC, OffLight offers a practical and impactful solution for improving urban traffic management without the risks associated with online learning.","Efficient traffic signal control (TSC) is important for modern urban mobility, directly affecting congestion levels, travel times, and overall city livability. As urban populations grow and vehicular usage intensifies, traditional traffic management systems struggle to adapt to dynamic and complex traffic flows. Recent advancements in Multi-Agent Reinforcement Learning (MARL) offer promising solutions by enabling decentralized, adaptive, and intelligent control of traffic signals [1, 2]. Figure 1: General Offline MARL Framework for Traffic Signal Control While online MARL requires agents to interact with the environment in real-time—which can be impractical and unsafe in real-world traffic systems—offline MARL leverages historically collected traffic data to train agents without live experimentation [3]. This approach offers several advantages: • Extensive Data Utilization: Utilizes pre-collected traffic data, allowing agents to learn from diverse scenarios and rare events difficult to replicate online [4]. • Safety and Risk Mitigation: Eliminates risks associated with deploying untested policies in live traffic, ensuring only well-trained agents are implemented [5]. • Cost Efficiency and Scalability: Reduces the need for expensive simulations or field tests, facilitating deployment across various scales and environments [3]. • Accelerated Policy Development: Enables rapid iteration and refinement of control policies, expediting research and development [6]. Several offline reinforcement learning (RL) methods have been explored for TSC, often incorporating imitation learning to bootstrap training. For instance, DemoLight uses demonstrations from traditional controllers to initialize policies, enabling efficient exploration and faster convergence compared to pure RL approaches [7]. Cooperative Control for multi-intersection TSC combines imitation learning and deep RL to coordinate multiple intersections, highlighting the benefits of learning from expert demonstrations [8]. CrossLight begins with offline training using behavior cloning and transitions to online RL to adapt policies to new environments, showcasing the effectiveness of imitation learning for strong policy initialization [9]. Building upon these efforts, DataLight introduces an input space modeling technique that captures the spatial distribution of traffic using self-attention mechanisms, demonstrating improved policy performance without requiring live interactions [4]. Figure 1 represents a general framework of an offline RL algorithm. However, a key challenge in offline MARL for TSC remains unaddressed: real-world datasets often consist of a heterogeneous mix of behavior policies, making it difficult for agents to learn effective strategies. This heterogeneity can arise from a variety of factors, such as: • Local Heuristics: Many traffic signals operate on traditional rule-based controllers (e.g., fixed-time or reactive control). These heuristics are often suboptimal and vary between intersections [10]. • Varying Control Strategies: Over time, traffic systems may switch between different controllers (e.g., from manual control to AI-based systems), resulting in datasets that reflect a blend of expert and suboptimal policies [11]. • Temporal Variability: Traffic patterns fluctuate throughout the day, leading to variations in the policies being implemented during different time periods. For instance, a traffic signal might switch between morning peak-time strategies and off-peak strategies [10]. This heterogeneity hampers effective learning because: • Offline MARL methods are highly sensitive to the underlying distribution of policies in the dataset. Mixing high- and low-quality data complicates the learning process, leading to poor generalization [12]. • Mixed-policy data can introduce biases, making it harder for the agent to identify the most effective policies [13]. To address these challenges, we introduce OffLight, a novel offline MARL framework that combines Importance Sampling (IS) and Return-Based Prioritized Sampling (RBPS) to mitigate distributional shifts and focus on high-value experiences. This synergy between IS and RBPS enhances both sample efficiency and policy performance. GMM-VGAE models the diversity in behavior policies within the traffic network, capturing policy heterogeneity by disentangling them in the latent space. By leveraging Graph Neural Networks (GNNs), OffLight combines local observations across intersections into a structured global representation, which is crucial in traffic signal control where agent interactions are highly localized but collectively influence network-wide patterns. This enables OffLight to use IS effectively, correcting for distributional shifts and ensuring stable policy learning from varied data. Meanwhile, RBPS prioritizes high-return episodes, allowing OffLight to focus on the most informative experiences, thus accelerating convergence and optimizing performance. Key contributions of this paper include: • Developing OffLight, an offline MARL framework designed to handle heterogeneous behavior policies in traffic signal control (TSC) using a combination of Importance Sampling (IS) and Return-Based Prioritized Sampling (RBPS). • Introducing GMM-VGAE to accurately capture diverse behavior policies, supporting robust learning under policy heterogeneity and enabling effective IS. • Leveraging RBPS to prioritize high-return episodes, which enhances sample efficiency and accelerates convergence by focusing on the most informative experiences. • Demonstrating OffLight’s scalability and superior performance on real-world traffic datasets, where it consistently outperforms existing methods. The remainder of the paper is organized as follows. Section 2 reviews related work and provides necessary background on MARL and offline RL in the context of TSC. Section 3 details the OffLight framework, including its architecture and key components. Section 4 describes the experimental setup, and Section 5 presents the results and analysis. Finally, Section 6 concludes the paper and Section LABEL:sec:discussion discusses the key insights gained from this study and limitations of OffLight."
https://arxiv.org/html/2411.06559v1,"?
Model-Based Planning for Web Agents","Language agents have demonstrated promising capabilities in automating web-based tasks, though their current reactive approaches still underperform largely compared to humans. While incorporating advanced planning algorithms, particularly tree search methods, could enhance these agents’ performance, implementing tree search directly on live websites poses significant safety risks and practical constraints due to irreversible actions such as confirming a purchase. In this paper, we introduce a novel paradigm that augments language agents with model-based planning, pioneering the innovative use of large language models (LLMs) as world models in complex web environments. Our method, WebDreamer, builds on the key insight that LLMs inherently encode comprehensive knowledge about website structures and functionalities. Specifically, WebDreamer uses LLMs to simulate outcomes for each candidate action (e.g., “what would happen if I click this button?”) using natural language descriptions, and then evaluates these imagined outcomes to determine the optimal action at each step. Empirical results on two representative web agent benchmarks with online interaction—VisualWebArena and Mind2Web-live—demonstrate that WebDreamer achieves substantial improvements over reactive baselines. By establishing the viability of LLMs as world models in web environments, this work lays the groundwork for a paradigm shift in automated web interaction. More broadly, our findings open exciting new avenues for future research into 1) optimizing LLMs specifically for world modeling in complex, dynamic environments, and 2) model-based speculative planning for language agents. 111Github: OSU-NLP-Group/WebDreamer","Planning (Mattar & Lengyel, 2022)—the strategic search for optimal action sequences to achieve goals from initial states—has been fundamental to artificial intelligence since its inception, driving remarkable breakthroughs including superhuman performance in games like Go (Feng et al., 2023; Silver et al., 2016). Recent advances have demonstrated that integrating large language models (LLMs) with advanced planning algorithms (e.g., Yao et al. (2023a); Hao et al. (2023); Gu et al. (2023); Wang et al. (2024); Feng et al. (2023); Brown et al. (2024)) substantially enhances their performance on complex reasoning tasks beyond chain-of-thought (CoT) (Wei et al., 2022) approaches, with OpenAI’s o1 (OpenAI, 2024b) serving as a prominent example. These methods effectively scale inference-time compute and enable LLMs to explore multiple potential solution paths, which ultimately lead to more accurate outcomes. Figure 1: Schematic illustration of different strategies for web agents formulated as a search problem. Each node represents a webpage. (a) Reactive: The agent selects locally optimal actions without forward planning, often leading to suboptimal outcomes. (b) Tree search with real interactions: The agent explores multiple paths through active website navigation and permits backtracking (indicated by dashed arrows). However, in real-world websites, backtracking is often infeasible due to the prevalence of irreversible actions. (c) Model-based planning: The agent simulates potential outcomes (illustrated by cloud-bordered nodes) to determine optimal actions prior to real-world execution, thus minimizing actual website interactions while maintaining effectiveness. For visual clarity, only one-step simulated outcomes are depicted. Faded nodes indicate unexplored webpages, while green checkmarks and red crosses denote successful and unsuccessful outcomes, respectively. Alongside these developments, research into generalist web agents capable of planning and executing a sequence of actions to complete complex tasks across diverse websites has garnered significant interest (Deng et al., 2023; Zhou et al., 2023; Zheng et al., 2024; Koh et al., 2024a), partly due to the web’s potential as a complex yet realistic environment for driving agent research and development. However, applying existing planning algorithms to the online web environment presents formidable challenges. Chief among these challenges are the inherent safety risks associated with live website interactions (Liao et al., 2024), such as inadvertently submitting forms with sensitive information or triggering unintended transactions. These risks become even more pronounced when employing tree search algorithms (Koh et al., 2024b; Putta et al., 2024), as their exhaustive exploration can expose the agent to hidden vulnerabilities and unforeseen scenarios. Additionally, many online actions, such as confirming a purchase or sending an email, are irreversible, which further makes backtracking—a crucial component of planning algorithms—highly challenging, if not infeasible. One promising solution to address these challenges is model-based planning (Pascanu et al., 2017; Moerland et al., 2023), which equips agents with the ability to simulate interactions using a world model—a computational representation of environment dynamics. By simulating action sequences within this virtual environment, agents can explore potential outcomes safely, without directly interacting with live websites. This approach not only reduces safety risks but also preserves the agent’s capacity to explore and plan. Yet, the true challenge lies in creating a versatile world model that can faithfully capture the landscape of the ever-evolving Internet. While previous research demonstrates that LLMs can function as effective world models in simplistic settings like blocksworld (Hao et al., 2023) and gridworld (Kim et al., 2024), a bolder question emerges: Can LLMs rise to the challenge of modeling the vast, dynamic Internet? With their extensive pre-trained knowledge—spanning web structures, protocols, and user behaviors—LLMs are uniquely positioned to take on this task. Building on these insights, we present WebDreamer, a pioneering framework that leverages LLMs as world models to navigate the web (Figure 1). At the core of WebDreamer lies the concept of “dreaming”: before committing to any action, the agent uses the LLM to imagine the outcome of each possible step, expressed as natural language descriptions of how the state would change. These simulated outcomes are then evaluated based on their progress toward achieving the task objective. The most promising action is executed, and the process is repeated iteratively until the LLM determines that the goal has been reached (Section 4). To validate the effectiveness of WebDreamer, we evaluate it on two representative benchmarks that support online interaction: VisualWebArena (Koh et al., 2024a) and Mind2Web-live (Pan et al., 2024b). WebDreamer achieves substantial performance gains over reactive agents on both benchmarks, underscoring its practical value despite its conceptual simplicity. While tree search with actual interactions shows slightly superior performance on VisualWebArena, which features a controlled environment of three locally hosted websites, this method is rarely feasible in practical applications, given its inherent limitations regarding safety risks and the potential for irreversible actions in real-world websites. In contrast, our simulation-based approach offers a more flexible solution, balancing performance gains with practical applicability in real-world web navigation tasks. In summary, our work introduces a new direction for AI planning in complex, real-world environments like the web using world models simulated by LLMs. With WebDreamer, we tackle the dual challenges of safety and complexity in web navigation. Our results validate the potential of LLM-based world models for planning in complex web environments and highlight new opportunities for optimizing LLMs as world models and improving model-based planning algorithms for language agents."
https://arxiv.org/html/2411.06549v1,"In-Context Learning for Preserving Patient Privacy: 
A Framework for Synthesizing Realistic Patient Portal Messages","Since the COVID-19 pandemic, clinicians have seen a large and sustained influx in patient portal messages, significantly contributing to clinician burnout. To the best of our knowledge, there are no large-scale public patient portal messages corpora researchers can use to build tools to optimize clinician portal workflows. Informed by our ongoing work with a regional hospital, this study introduces an LLM-powered framework for configurable and realistic patient portal message generation. Our approach leverages few-shot grounded text generation, requiring only a small number of de-identified patient portal messages to help LLMs better match the true style and tone of real data. Clinical experts in our team deem this framework as HIPAA-friendly, unlike existing privacy-preserving approaches to synthetic text generation which cannot guarantee all sensitive attributes will be protected. Through extensive quantitative and human evaluation, we show that our framework produces data of higher quality than comparable generation methods as well as all related datasets. We believe this work provides a path forward for (i) the release of large-scale synthetic patient message datasets that are stylistically similar to ground-truth samples and (ii) HIPAA-friendly data generation which requires minimal human de-identification efforts.","Have you sent a message to your doctor recently? Because in the past few years, there has been a significant increase in use of patient-facing healthcare applications which allow patients to send textual messages to their provider Hansen et al. (2023). Electronic health record (EHR) applications such as Epic’s MyChart, for example, reportedly had 200 million active users as of early 2021, with 80 million joining within the previous 12 months ThisWeekHealth (2021). Unfortunately, this shift in doctor-patient communication paradigms has contributed significantly to doctor burnout as clinicians have not been provided with additional resources to handle the portal message surge Stillman (2023). To help reduce clinician workload, recent studies have explored the use of AI tools to optimize cognitively demanding tasks such as portal message triage Si et al. (2020); Gatto et al. (2022); Mermin-Bunnell et al. (2023), routing, Harzand et al. (2023), and response writing Nov et al. (2023); Kozaily et al. (2023); Athavale et al. (2023). However, most of these studies use sensitive data that cannot be released to the community. Some prior work have explored patient messaging through the lens of data sourced from public medical Q&A forums Gatto et al. (2022). However, such platforms operate assuming that the physician who will respond is unfamiliar with the patient. This produces patient messages that are highly dissimilar to actual portal message data, where personal relationships with clinicians and historical EHR data lead to patient messages that are stylistically different, often containing implicit references to EHR data and prior encounters with providers. See Appendix A for an illustrating example of this issue. Given the sensitive nature of patient message data, recent advancements in language modeling may be useful in generating realistic synthetic datasets which can be made public. For example, one could fine-tune a language model on data from the true distribution and then generate realistic synthetic data. Unfortunately, this paradigm may generate samples that leak sensitive patient information learned during training. Even privacy-preserving mechanisms such as Differential Privacy (DP) Wutschitz et al. (2022) cannot provide a risk-free guarantee that no sensitive patient attributes would be released during generation. This is because DP language models are trained not to leak full training data instances, but there remains a risk of generating a sensitive token (e.g., a patient’s last name) violating the patient’s privacy. Thus, a solution to this problem must generate data that reflects true patient portal conversational style, semantics, and structure, all while ensuring synthetic samples pose zero risk of protected health information (PHI) leakage. This issue can perhaps be solved via LLM prompting. LLMs have tremendous capacity to generate texts while following a set of instructions Lou et al. (2024), potentially removing the need for sensitive training data. One could thus prompt an LLM to generate a patient portal message containing a pre-defined set of details. Unfortunately, as discussed in prior work Wang et al. (2024); Gupta et al. (2024); Liu et al. (2024), LLMs are biased, highly-formal, and struggle to capture the natural voice of people from different identity groups. Thus, off-the-shelf applications of LLMs to this task are ill-suited to generate data that matches real patient message style. To address these challenges, we propose PortalGen, a two-stage, HIPAA-friendly, LLM-powered framework for the configurable generation of realistic patient message data. In stage 1, PortalGen uses few-shot prompting of LLMs to transform codes from healthcare databases into portal message prompts. This provides a means of generating diverse large-scale message corpora covering a wide variety of health situations. In stage 2, we use grounded generation with a small number of de-identified patient messages to convert prompts from stage-1 into patient messages. Grounded generation Veselovsky et al. (2023) is a technique that includes samples from the target distribution in the prompt, encouraging LLM outputs to be more stylistically and semantically faithful to the nuances of real training samples. PortalGen performs grounding with just 10 de-identified patient messages, providing a framework for researchers and institutions to release realistic synthetic patient portal message data without requiring large-scale de-identification efforts. Our results demonstrate that PortalGen produces data that is highly similar to real data, outperforming baseline data synthesis techniques and showing strong contrast with related public medical Q&A datasets."
https://arxiv.org/html/2411.06498v1,Barriers to Complexity-Theoretic Proofs that Achieving AGI Using Machine Learning is Intractable,"A recent paper [VRGA+24] claims to have proved that achieving human-like intelligence using learning from data is intractable in a complexity-theoretic sense. We point out that the proof relies on an unjustified assumption about the distribution of (input, output) pairs in the data. We briefly discuss that assumption in the context of two fundamental barriers to repairing the proof: the need to precisely define “human-like,” and the need to account for the fact that a particular machine learning system will have particular inductive biases that are key to the analysis. Another attempt to repair the proof, by focusing on subsets of the data, faces barriers in terms of defining the subsets.","In [VRGA+24] a claim is made that the authors “formally prove [in the paper that] creating systems with human(-like or -level) cognition (“AGI” for short, for the purposes of this paper) is intrinsically computationally intractable.” Here, we show that the paper falls short of formally proving the claim. We identify a key unproven premise that underlies the proof: that the distribution \mathcal{D} of tuples (s,b), with s denoting “situation” and b denoting “[human] behavior” in response to s can be an arbitrary (polytime-computable) distribution. If \mathcal{D} is a model of human behavior, both the marginal distribution of s and the conditional distribution P_{\mathcal{D}}(b|s) are in fact highly structured. For example, if s encodes natural images, the marginal distribution P_{\mathcal{D}}(s) would need to account for the hierarchical structure of natural images [SO01]. If P_{\mathcal{D}}(b|s) models human chess moves, the distribution would need to account (among other things) for the rules of chess. This means that many \mathcal{D}’s can be ruled out a-priori. As we argue below, the fact that, in the authors’ proof, \mathcal{D} denotes both the distribution of situation-behavior tuples and an arbitrary distribution means that the authors did not prove what they set out to prove. We argue that two critical issues must be resolved when attempting to repair the proof (although we make no claim that the proof could be repaired). • “Human-like” behavior must be defined precisely . • The fact that while an arbitrary function is not learnable due to No-Free-Lunch-Theorem-style results, some structured functions can be learned with appropriate inductive biases must be considered. Another attempt to repair the proof by focusing on subsets of the data also face a barrier. The paper is organized as follows. in Section 2 we introduce the Ingenia Theorem of [VRGA+24], along with the necessary context. In Section 3 we point out what we believe to be a central flaw in the proof. In Section 4, we identify what we see as the challenges that a correct version of the proof would have to overcome. We illustrate that not having met one of the challenges leaves the current proof vulnerable to a reductio ad absurdum argument (Section 4.1.1)."
https://arxiv.org/html/2411.06490v1,Hermes: A Large Language Model Framework on the Journey to Autonomous Networks,"The drive toward automating cellular network operations has grown with the increasing complexity of these systems. Despite advancements, full autonomy currently remains out of reach due to reliance on human intervention for modeling network behaviors and defining policies to meet target requirements. Network Digital Twins (NDTs) have shown promise in enhancing network intelligence, but the successful implementation of this technology is constrained by use case-specific architectures, limiting its role in advancing network autonomy. A more capable network intelligence, or “telecommunications brain”, is needed to enable seamless, autonomous management of cellular network. Large Language Models (LLMs) have emerged as potential enablers for this vision but face challenges in network modeling, especially in reasoning and handling diverse data types. To address these gaps, we introduce Hermes, a chain of LLM agents that uses “blueprints” for constructing NDT instances through structured and explainable logical steps. Hermes allows automatic, reliable, and accurate network modeling of diverse use cases and configurations, thus marking progress toward fully autonomous network operations.","Since the inception of large-scale cellular networks, researchers and the industry have aimed to automate their operation and management due to the costly and extensive human labor involved. However, communication systems are notorious for their dynamic nature, spanning from wireless channel and network load to unpredictable faults and errors that require network adaptation. Due to these characteristics, full network autonomy has not yet been achieved, and human presence in the operation loop is still prevalent at multiple levels of the hierarchical network stack. On the network autonomy scale [1], where at level 0 humans perform network operations entirely using manual procedures and level 5 refers to full network automation, current network operations lie in the middle, aiming to reach level 3 automation before 2026 [2]. Network Digital Twin (NDT) has emerged as a highly promising candidate to enhance the design, analysis, operation, automation, and intelligence of future mobile networks [3]. However, the impact of this technology toward full network autonomy is limited by the current design approach of NDTs where different use cases are mapped to distinct NDT architectures [4], as processing different types of data and modeling and optimizing distinct network functionalities within a unique piece of software is extremely complex. To break this barrier and take autonomy beyond this level, a more capable type of network intelligence is needed, which encompasses extensive knowledge about network operations and functionalities, and can streamline these operations – essentially functioning as a telecommunications brain. The notion of a telecommunications brain has been utilized in the literature before to refer to a large-scale intelligent entity capable of understanding the intricacies of the network, the various cause-and-effect relationships in its functionalities, and the ability to plan and predict the network behavior in advance. This intelligence continuously monitors the network state, promptly reacts to any unforeseen changes, and seamlessly adapts the network operations to new scenarios as they arise. Arriving at such a realm of telecommunications brain is an ambitious goal that would elevate network autonomy to new heights. Although the end goal is clear, the path to realizing a telecommunications brain is still an open challenge. Then, Large Language Models have emerged, revolutionizing the Artificial Intelligence (AI) field, especially Natural Language Processing (NLP), by propelling text generation, comprehension, and interaction to unprecedented levels of sophistication. Promptly, researchers in the network domain have identified LLMs as key enablers to pave the way to the telecommunications brain [5]. Particularly, researchers envisioned a realm where LLMs take over the driving seat of network operations and management. However, to this day, the application of LLMs in the telecommunications domain has been mostly successful as human add-ons, such as Retrieval Augmented Generation (RAG) systems [6] that fetches Third Generation Partnership Project (3GPP) standards information and conversational chatbot tools for wireless communication specifications [7]. There have also been recent successful implementations of LLMs in embedding network commands, such as setting the transmit power of a base station to a specific value, into actionable configuration files for network management [8]. In [9], the authors propose an LLM-based multi-agent framework designed to convert user requests into optimized power scheduling vectors by selecting from a set of predefined equations and solvers the most suitable for the intended task. These lines of work leverage the alignment between such translation tasks and the natural language proficiency of LLMs. In another line of work, to overcome the limitations of LLMs in the network domain, researchers advocated for multi-modal LLMs trained on wireless signals and network measurements to augment their capabilities [10]. However, creating such large multi-modal models presents significant challenges. These challenges include the need for extensive datasets of measurements and wireless signals, often proprietary to operators and vendors, the complexity of integrating diverse modalities, and the inherent weakness of LLMs in managing numerical operations and relationships [11]. Therefore, despite the current research hype around LLMs, the question remains open: do LLMs truly hold the key for achieving the so-called telecommunications brain and leading to full autonomy in telecommunication networks? This work aims to address this question through a multi-step approach. As a first step, we posit that, fundamentally, an LLM can be considered inching closer to becoming a telecommunications brain if it can grasp the causal relationships between network components, configurations, parameters, and their impact on network performance. In other words, this capability would allow LLMs to construct end-to-end network models– NDTs– to effectively handle new environments and unseen circumstances. Moreover, we demonstrate that, today, the most powerful LLMs, e.g., GPT-4o, struggle to perform well on understanding and modeling the network behavior, even when using advanced prompting techniques like chain-of-thought [12]. We shed light on the common pitfalls LLMs fall into and the typical mistakes they make. Our findings illustrate that despite their impressive capabilities, current LLMs are far from being autonomous agents capable of taking the driving seat for telecommunications network management and operations on their own. To address these pitfalls, we introduce Hermes: a comprehensive chain-of-agents LLM framework that tackles network modeling and automation through the elaboration of “blueprints” of NDTs. In this context, a blueprint is a set of step-by-step logical blocks autonomously designed and coded by the LLMs using their parametric knowledge of the telecommunications domain, rather than relying on the direct interpretation of network measurements as multi-modal models do [10]. By incorporating key components such as self-reflection steps and feedback mechanisms, along with a granular step-by-step logical approach, Hermes ensures the validity of these blueprints and their associated code to realize a NDT tailored to the tasked intent. We demonstrate how leveraging the blueprints of NDT significantly increases the reliability of the LLM in addressing diverse network modeling tasks, resulting in a more robust comprehension of network dynamics and operations."
https://arxiv.org/html/2411.06448v1,"Over-parameterized Student Model via Tensor Decomposition
Boosted Knowledge Distillation","Increased training parameters have enabled large pre-trained models to excel in various downstream tasks. Nevertheless, the extensive computational requirements associated with these models hinder their widespread adoption within the community. We focus on Knowledge Distillation (KD), where a compact student model is trained to mimic a larger teacher model, facilitating the transfer of knowledge of large models. In contrast to much of the previous work, we scale up the parameters of the student model during training, to benefit from over-parameterization without increasing the inference latency. In particular, we propose a tensor decomposition strategy that effectively over-parameterizes the relatively small student model through an efficient and nearly lossless decomposition of its parameter matrices into higher-dimensional tensors. To ensure efficiency, we further introduce a tensor constraint loss to align the high-dimensional tensors between the student and teacher models. Comprehensive experiments validate the significant performance enhancement by our approach in various KD tasks, covering computer vision and natural language processing areas. Our code is available at https://github.com/intell-sci-comput/OPDF.","Large-scale pre-trained models are gradually achieving remarkable milestones due to the exhibit of remarkable performance across various tasks [1, 2, 3, 4, 5, 6, 7]. These models leverage extensive pre-training data and parameters, enabling them to effectively encapsulate a significant breadth of world knowledge [8, 9] and exhibit strong generalization capabilities across diverse tasks [1, 10, 11, 12, 13]. Following this trajectory, the utilization of increased data and parameters has emerged as a notable trend in enhancing the performance of pre-trained models in recent years, leading to the number expansion of pre-trained model parameters from millions to billions [4, 14, 15]. Despite their impressive performance, the substantial storage demands and high computational complexity hinder the practical deployment of these models in real-world applications. Therefore, on the one hand, some studies focus on pre-training relatively smaller models (such as BERT-base-uncased [2]) on domain-specific or task-specific corpora [16, 17, 18]. However, due to the lesser over-parameterization of small models compared to large ones, their generalization capability often falls short, resulting in suboptimal fine-tuning performance on downstream tasks. On the other hand, model compression methods, such as pruning less informative parameters [19, 20, 21] or utilizing knowledge distillation (KD) [22] to transfer knowledge from larger models (teachers) to smaller ones (students), have been proposed. KD has swiftly diversified into numerous branches, primarily falling into two categories: i.e., logits-based [22, 23, 24, 25, 26] and features-based [27, 28, 29, 30] depending on the source of student model knowledge. Nevertheless, as student models have fewer trainable parameters and limited capacity, a significant performance gap remains between student and teacher models. To address the disparity between small and large models, this study aims to over-parameterize small student models as large ones during distillation training to enhance their generalization capability. Typically, most parameters of student models are stored as matrices. Through tensor decomposition techniques [31, 32, 33, 34] (e.g., Singular Value Decomposition), each matrix can be factorized into a set of matrices, effectively increasing the total number of parameters during distillation. Moreover, after convergence, the factorized matrices can be merged to reorganize the parameter matrix of the student model. This paradigm leverages the benefits of over-parameterization during training without increasing the inference latency of student models. However, incorporating tensor decomposition into over-parameterizing student models poses two major concerns that must be addressed. First, the potential information loss caused by tensor decomposition should be minimized, as small computation errors may accumulate and propagate exponentially within the stacked layers of student models. Second, in the over-parameterized student models, there is no effective mechanism to ensure the consistency of information between student and teacher models. Therefore, it is essential to choose appropriate tensor decomposition methods and design loss functions for high-order tensors to ensure the effective transfer of information from teacher to student models. To address the above issues, we introduce the matrix product operator (MPO) [34] technique as the tensor decomposition strategy. The MPO decomposition, widely used in quantum many-body physics, efficiently factorizes any matrix with arbitrary dimensions into a set of higher-dimensional tensors, which can reconstruct the original matrix in almost lossless conditions [34, 35, 36, 37]. These advantages make MPO an ideal method for over-parameterizing student models during distillation. Based on MPO, we also devise high-order tensor alignment losses for student and teacher models to ensure the effective transfer of information in tensor representation. Therefore, in this paper, we propose a general Over-Parameterization Distillation Framework, namely OPDF, to improve the performance of knowledge distillation. Given the parameter matrices of a student model, we first over-parameterize them through MPO decomposition and then utilize high-order tensor alignment losses to ensure efficient information transfer. This framework only modifies the distillation training process, making it applicable to various student models and natural language processing (NLP) and computer vision (CV) tasks. We conduct extensive experiments in both NLP and CV domains. Experimental results demonstrate that our OPDF significantly enhances the effectiveness of model distillation, e.g., improving BERT-base KD +1.6 on average. Moreover, our approach also enables the student model to achieve performance nearly on par with the teacher model, e.g., AD-KD+Ours (83.4) v.s. BERT-base (83.4) in average metric on GLUE."
https://arxiv.org/html/2411.06403v1,Mastering NIM and Impartial Games with Weak Neural Networks: An AlphaZero-inspired Multi-Frame Approach,"This paper provides a theoretical framework that validates and explains the results in the work with Bei Zhou [22, 24] experimentally finding that AlphaZero-style reinforcement learning algorithms struggle to learn optimal play in NIM, a canonical impartial game proposed as an AI challenge by Harvey Friedman in 2017. Our analysis resolves a controversy around these experimental results, which revealed unexpected difficulties in learning NIM despite its mathematical simplicity compared to games like chess and Go.To analyse these limitations, we introduce a class of “weak” neural network models (NN, RNN, LTST) characterised by polynomial size, constant depth, and constant precision in weights and thresholds, which belong to the complexity class \text{AC}^{0}. We prove that due to their inherent limitations in computing parity functions, these models cannot achieve optimal NIM play using single-frame representations (as tested in [24]). While this impossibility result is absolute, we prove that multi-frame approaches are not subject to the same theoretical barrier, though realising this potential in practice remains an open challenge.Our key contributions are as follows:We prove that by incorporating recent game history, these limited AlphaZero models can, in principle, achieve optimal play in NIM.We introduce a novel search strategy where roll-outs preserve game-theoretic values during move selection, guided by a specialised policy network.We provide constructive proofs showing that our approach enables optimal play within the \text{AC}^{0} complexity class despite the theoretical limitations of these networks.Beyond NIM, our findings offer new insights into how appropriate state representations and search strategies can overcome fundamental computational limitations in neural networks. This research demonstrates how constrained neural networks when properly designed, can achieve sophisticated decision-making even in domains where their basic computational capabilities appear insufficient.","AlphaZero has revolutionised the field of artificial intelligence by demonstrating unprecedented performance in complex games such as chess, Go, and shogi through self-play and reinforcement learning [16, 18, 17, 15]. While very deep, these games possess an underlying structure allowing expert players to evaluate positions intuitively. An impartial game like NIM (where both players have exactly the same moves available at each position) is well understood and has optimal strategies that are easy to compute. From a human cognitive point of view, partisan games like Go and chess (where players control different coloured pieces) have a different feel than impartial games. While many chess and Go positions are hard to evaluate, a strong human player generally has some idea (i.e., positive information) about a given position that can guide the evaluation and search. In a simple mathematical game like NIM, however, parity issues play a crucial part, and finding the optimal move is equivalent to determining if a list of parities all are 0. While easy to calculate, cognitively, this is not something a human can determine at a glance. A natural conjecture is that this cognitive difficulty also applies to the Neural Networks guiding the Monte Carlo Tree Search (MCTS) in AlphaZero-style self-learning algorithms. NIM, a foundational impartial game where players take turns removing objects from heaps, is an excellent testbed for exploring the limits of reinforcement learning in strategic settings where the neural networks that theoretically guide the search and evaluation should be relatively ineffective. This aligns with the broader investigation of AI capabilities in various game contexts, as highlighted by Friedman’s proposal of NIM as an AI challenge in 2017 [8]. The study of NIM and similar impartial games in the context of AI relates to fundamental questions in computational complexity theory. The class AC0, which consists of constant-depth, polynomial-size boolean circuits with unbounded fan-in AND and OR gates, has been shown to capture important aspects of the computational power of certain neural network models [10]. This complexity class arises naturally in neural networks where weights and thresholds are limited to fixed precision. Interestingly, AC0 can simulate several other computational models, including certain types of finite automata and small-depth decision trees [1]. However, AC0 is known to have limitations, particularly in its inability to compute simple functions like parity [9], which, as we shall show, is crucial for optimal play in NIM. This paper aims to bridge the gap between theory and the empirical investigations of AlphaZero’s performance in impartial games. We introduce a ”weak” neural network model class (Neural Networks (NN), Recurrent Neural Networks (RNN), and Long-Term Short-Term Attention (LTST) models) characterised by polynomial size, constant depth, and constant precision in weights and thresholds. These models belong to the complexity class AC0, and our experimental results [25, 24] suggest that practical neural networks often operate within similar computational constraints. Our analysis demonstrates that AlphaZero agents operating within these constraints cannot accurately evaluate NIM positions, as this task requires computing parity. This aligns with Friedman’s observation that while AI had made significant strides in complex games like chess and Go, simpler games like NIM might pose unexpected challenges due to their mathematical nature [8]. However, our main result shows that these limitations can be overcome in principle through a carefully designed search strategy using multi-frame representations. Several key points make NIM an interesting challenge for AI: 1. Perfect Play: Unlike chess or Go, humans can play NIM perfectly with a known strategy. This allows for a clear evaluation of AI performance against optimal play. 2. Scalability: NIM can be scaled in complexity by adjusting parameters such as the number of heaps and the maximum number of items per heap, allowing for systematic testing of AI learning capabilities. 3. Mathematical Nature: The winning strategy for NIM relies on mathematical concepts like binary representation and XOR operations, testing an AI’s ability to discover and utilise mathematical patterns. 4. Contrast with Human Learning: The way AI might learn to play NIM could differ significantly from how humans discover the optimal strategy, potentially revealing fundamental insights into machine learning processes. However, this paper’s main contribution is to show that by incorporating recent game history—e.g., the immediate previous positions—even the AC0 limited agents we consider can, in principle, achieve optimal play through a specific new kind of search strategy. 1.1 Addressing Controversial Experimental Results In joint work with Bei Zhou [24], we investigated AlphaZero-style reinforcement learning algorithms applied to the game of NIM. Our findings provided empirical evidence that a wide class of neural network models struggle to reliably compute parities of long bitstrings [25], which is crucial for evaluating NIM positions with many heaps. These results align with theoretical considerations related to the statistical neutrality of the parity function [20], and other works [6, 21]. This evidence that AlphaZero-style self-play algorithms had difficulties learning to master simple children’s games like NIM sparked debate within the AI community, challenging the notion of AlphaZero’s universal applicability across game domains. While our claims were substantiated through empirical experiments, they lacked rigorous theoretical justification. To address this controversy, we have developed a theoretical framework that validates our experimental observations and delineates the specific conditions under which AlphaZero-style algorithms falter or succeed in impartial games. Our approach involves: 1. Weak Neural Network Models: We introduce a class of “weak” neural network models (Neural Networks (NN), Recurrent Neural Networks (RNN), and LTST - Long-Term Short-Term attention models) characterised by polynomial size, constant depth, and constant precision in weights and thresholds. These models belong to the complexity class AC0. 2. Multi-Frame Representation: We demonstrate that game representation is essential. While our previous experiments used single frames as game states, we show that incorporating multiple frames (recent game history) can theoretically overcome fundamental computational barriers. 3. Novel Search Strategies: We introduce a new search strategy guided by a specialised policy network that allows our limited agents to achieve optimal play in principle when combined with multi-frame representation. By establishing the equivalence of certain neural network constraints to the AC0 complexity class, we provide a mathematical framework that explains our empirical observations: • Parity Function Difficulty: The inability to efficiently compute the parity function leads to inaccurate evaluation of NIM positions, especially as the number of heaps increases [25, 24] • Scaling Issues: The performance of AlphaZero-style algorithms deteriorates rapidly as the board size grows, with significant limitations observed even with seven heaps [24] • Noisy Labels: The self-play nature of the algorithm can be mimicked experimentally with supervised learning with noisy labels, further complicating the learning process, especially for parity-related functions [25] To illustrate our approach, Figures 1 and 2 contrast two architectural approaches. Figure 1 shows a basic single-frame neural architecture which, while simpler than AlphaZero’s actual implementation, helps illustrate the fundamental limitations of AC0-constrained networks. Figure 2 shows our multi-frame value-preserving approach. While AlphaZero also uses multiple frames in practice, our approach specifically leverages frame history to overcome the computational limitations of AC0 networks in computing position values. (A) Policy Network(B) Evaluation Network(C) MCTS(D) Moveguidevalueselect Figure 1: Single-frame game state representation with MCTS-based move selection (A) Policy Network(B) Evaluation Network(C) Value-Preserving MCTS(D) Move(E) Historyguidevalueselectinformstore Figure 2: Multi-frame representation with value-preserving search and rollout history feedback 1.2 Paper Organization The remainder of this paper is organized as follows: • Section 2 presents our theoretical framework and neural network models, establishing the foundations for understanding AC0 constraints. • Section 3 provides formal definitions and fundamental properties of impartial games, with a focus on NIM. • Section 4 presents our main impossibility and possibility results, including the crucial multi-frame approach. • Section 5 analyzes the learning dynamics of impartial games with neural networks. • Section 6 discusses practical implications and potential applications. • Section 7 explores generalizations to other impartial games. Our investigation opens several directions for future research: • Identifying general classes of scenarios where Neural Network-guided action recommendations benefit from including recent history • Using the weak AC0 Neural Network framework to identify hard AI-learning tasks and additional features that help overcome computational bottlenecks • Extending our approach to other impartial games and broader game classes"
https://arxiv.org/html/2411.06385v1,Class Granularity: How richly does your knowledge graph represent the real world?,"To effectively manage and utilize knowledge graphs, it is crucial to have metrics that can assess the quality of knowledge graphs from various perspectives. While there have been studies on knowledge graph quality metrics, there has been a lack of research on metrics that measure how richly ontologies, which form the backbone of knowledge graphs, are defined or the impact of richly defined ontologies. In this study, we propose a new metric called Class Granularity, which measures how well a knowledge graph is structured in terms of how finely classes with unique characteristics are defined. Furthermore, this research presents potential impact of Class Granularity in knowledge graph’s on downstream tasks. In particular, we explore its influence on graph embedding and provide experimental results. Additionally, this research goes beyond traditional Linked Open Data comparison studies, which mainly focus on factors like scale and class distribution, by using Class Granularity to compare four different LOD sources.","A knowledge graph is a data system comprising RDF triples structured as ‘subject-predicate-object’, enabling it to represent real-world entities and their associated relationships. Within the domain of information retrieval Reinanda et al. (2020) and question-answering Lan et al. (2021) tasks, knowledge graphs have traditionally assumed a prominent role. More recently, their significance has grown within the field of AI. They are integrated into language models as external repositories of knowledge, thereby augmenting the precision and reasoning capabilities of these models Schneider et al. (2022). Furthermore, knowledge graphs are harnessed to enrich content-related information within recommendation systems Shao et al. (2021). However, the efficacy of knowledge graphs relies on their quality. Consequently, various research efforts have been undertaken to measure the quality of knowledge graphs. In particular, it is essential to consider ontology, which plays a foundational role as the backbone of knowledge graphs. An ontology is a schema that formally declares classes, which represent types of instances, along with the predicates associated with each class and the hierarchical relationships between classes and predicates. Gruber (1993) defines ontology as ""explicit specifications of conceptualizations"". The comparison made in Raad and Cruz (2015) between a machine’s knowledge graph and a human’s level of knowledge suggests that both are crucial for decision-making. Just as humans rely on their knowledge to make decisions alongside their reasoning abilities, machines also heavily depend on their knowledge graph, or ontology, to enhance their reasoning capacity. For instance, when someone encounters the names ""Let it be"" and ""Hey Jude"", varying levels of comprehension may be observed among different individuals. Some individuals may fail to discern their meaning, while others may categorize them as ""songs"". Furthermore, certain people might even recognize that both songs share the same artist,The Beatles. There are various methods available for measuring the quality of ontology and knowledge graphs. However, in this study, we aim to assess how granular a knowledge graph is using the Class Granularity metric. The granularity of a knowledge graph, in essence, refers to the presence of numerous defined predicates and the high depth and breadth of the ontology. Despite its significance, there has been a lack of research into metrics for measuring granularity and its impact. According to Fernández et al. (2009), ""in general, richly populated ontologies, with higher depth and breadth variance are more likely to provide reliable semantic content"". Furthermore, in this study, we analyze the impact of granularity through graph embedding techniques and knowledge base question answering (KBQA). Additionally, we provide comparative analysis results on the level of granularity in Linked Open Data sources such as YAGO, DBpedia, and Wikidata, which, to our knowledge, have not been previously reported. The contributions of this study are as follows: • We propose a metric that takes into account both the ontology and the knowledge graph instances for measuring the granularity of knowledge graphs. • We provide the first comparative results on the level of granularity in Linked Open Data sources. • We conduct experiments to assess the impact of granularity on specific tasks."
https://arxiv.org/html/2411.06284v1,A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks,"This survey and application guide to multimodal large language models(MLLMs) explores the rapidly developing field of MLLMs, examining their architectures, applications, and impact on AI and Generative Models. Starting with foundational concepts, we delve into how MLLMs integrate various data types, including text, images, video and audio, to enable complex AI systems for cross-modal understanding and generation. It covers essential topics such as training methods, architectural components, and practical applications in various fields, from visual storytelling to enhanced accessibility. Through detailed case studies and technical analysis, the text examines prominent MLLM implementations while addressing key challenges in scalability, robustness, and cross-modal learning. Concluding with a discussion of ethical considerations, responsible AI development, and future directions, this authoritative resource provides both theoretical frameworks and practical insights. It offers a balanced perspective on the opportunities and challenges in the development and deployment of MLLMs, and is highly valuable for researchers, practitioners, and students interested in the intersection of natural language processing and computer vision.","Chapter 0 Introduction to Multimodal Large Language Models (MLLMs) 1 Definition and Importance of MLLMs Multimodal Large Language Models (MLLMs) represent a significant evolution in artificial intelligence (AI), enabling the integration and understanding of various input types such as text, images, audio, and video. Unlike unimodal models restricted to a single input type, MLLMs process multiple modalities simultaneously, providing a more comprehensive understanding that reflects real-world interactions. The key features and importance of MLLMs include: Cross-Modal Learning: MLLMs are trained on extensive datasets encompassing textual, visual, auditory, and sometimes sensory data. This capability allows them to create connections between different modalities, enabling tasks that require comprehension and generation of content across diverse data types. For example: • Text-to-Image Generation: MLLMs can generate detailed images from textual descriptions, revolutionizing creative industries like graphic design and advertising. Imagine describing a ”futuristic cityscape at sunset” and having an AI generate a corresponding image. • Visual Question Answering: These models can analyze images and provide accurate answers to natural language questions, enhancing educational tools and accessibility technologies. For instance, an MLLM could answer questions about the contents of a photograph, such as ”What breed of dog is in this image?” • Multimodal Content Creation: MLLMs facilitate the creation of content that integrates text, visuals, and audio, such as illustrated stories or multimedia presentations. This could involve generating a coherent story with matching illustrations based on a brief prompt. Unified Representation: MLLMs achieve integrated representations of multimodal data through unified codebooks and joint embedding spaces, enabling seamless processing across different modalities. This architectural approach offers several key capabilities: • Seamless translation between modalities (e.g., describing a photograph or generating an image from text). • Cross-modal retrieval, where the model can find relevant images based on text queries or match sounds with visual content. • More natural and intuitive interactions between humans and AI systems. To understand unified representation, imagine a library where books, images, and audio recordings are all cataloged using the same system, allowing you to easily find related items across different media types. Enhanced Contextual Understanding: By integrating multiple modalities, MLLMs generate more accurate and context-aware responses. This capability is particularly valuable in fields such as: • Healthcare: Analyzing medical images alongside patient records and physician notes for more precise diagnoses. For example, an MLLM could combine a patient’s X-ray, medical history, and symptoms to suggest potential diagnoses. • Security: Interpreting surveillance footage in conjunction with audio data for comprehensive situational awareness. This could involve analyzing video feeds and audio recordings to detect potential security threats. • E-commerce: Enhancing product searches by understanding both textual queries and visual product attributes. An MLLM could help a customer find a ”blue floral summer dress” by understanding both the text description and visual characteristics of available products. Generalization Across Modalities: MLLMs demonstrate flexibility in handling various tasks across different modalities, including: • Image captioning and visual question answering. • Cross-modal retrieval and content generation. • Audio-visual integration for tasks like video subtitling or lip-syncing. • Multimodal translation, such as converting a video into a textual summary. • Enhanced human-computer interaction through simultaneous interpretation of gestures, facial expressions, speech, and text. Advancements in Robotics and Embodied AI: In robotics, MLLMs contribute to systems that can perceive and interact with their environment more effectively. By processing visual, auditory, and sensory data, robots powered by MLLMs can perform complex tasks such as object manipulation, navigation, and human-robot interaction. For instance, a household robot could understand and execute a verbal command like ”Please bring me the red mug from the kitchen counter,” by combining language understanding with visual recognition and spatial navigation. Real-World Application Potential: The ability of MLLMs to process diverse data types makes them valuable for real-world applications where information comes in various forms. For instance: • In autonomous vehicles, these models can integrate visual data from cameras with textual information from maps and traffic reports, enhancing navigation and safety features. An MLLM could help a self-driving car understand a road sign, interpret its meaning, and adjust the vehicle’s behavior accordingly. • In scientific research, MLLMs can analyze molecular structures, research papers, and experimental data simultaneously to identify potential new compounds for drug discovery. This could accelerate the process of finding new treatments by identifying patterns across diverse datasets that human researchers might miss. Bridging the Gap Between AI and Human Cognition: MLLMs’ ability to process multiple modalities mirrors human cognitive processes more closely than unimodal models. This alignment with human cognition can lead to AI systems that are more intuitive to use and better at understanding complex, context-dependent situations. For example, an MLLM-powered virtual assistant could understand and respond to a user’s mood based on their tone of voice, facial expression, and choice of words, much like a human would. 2 The Convergence of Natural Language Processing (NLP) and Computer Vision: The Emergence of MLLMs The fusion of natural language processing (NLP) and computer vision has been a game-changer in AI, giving rise to MLLMs. This convergence allows machines to reason across different modalities, offering a more comprehensive understanding of the world. Key Historical Milestones: • Image Captioning (2015-Present): Early models like Show, Attend, and Tell combined Convolutional Neural Networks (CNNs) for image analysis with Recurrent Neural Networks (RNNs) for text generation. This marked the beginning of machines being able to ”describe” what they ”see”. • Visual Question Answering (VQA): These tasks required models to combine visual and textual inputs to generate meaningful answers. For example, a model might be asked, ”What color is the car?” while being shown an image of a red car. • Vision-Language Transformers (2019-Present): Models like ViLBERT, CLIP, and DALL-E demonstrated that transformer architectures could be extended to multimodal applications. These models can perform tasks like generating images from text descriptions or finding the most relevant image for a given text query. Image Captioning (2015-Present) \faCameraRetro CNNs and RNNs allow models to “describe” images 2015 Visual Question Answering \faQuestion Models answer questions like “What color is the car?” 2016 - 2018 Vision-Language Transformers (2019-Present) \faRobot Transformers enable multimodal tasks like text-to-image generation 2019 Theoretical Foundations: The convergence of NLP and computer vision is built on several key theoretical foundations: • Representation Learning: This allows MLLMs to create joint embeddings that capture semantic relationships across modalities. In simpler terms, it enables the model to understand how concepts in language relate to visual elements. For example, the model learns that the word ”cat” is associated with certain visual features like whiskers, pointed ears, and a furry body. • Transfer Learning: This technique allows models to apply knowledge gained from one task to new, related tasks. For MLLMs, this means they can leverage general knowledge acquired from large datasets to perform well on specific tasks with minimal additional training. It’s like how a human who knows how to ride a bicycle can quickly learn to ride a motorcycle, applying their balance and coordination skills to the new task. • Attention Mechanisms: Originally developed for NLP, attention mechanisms allow models to focus on relevant parts of inputs. In MLLMs, this extends to focusing on relevant aspects across different modalities, enabling more effective processing of multimodal data. You can think of this as similar to how humans focus on a speaker’s lips when trying to understand speech in a noisy environment. Key Component of AI Theory Figure 1: Key Component of AI Theory Architectural Innovations: Several key architectural innovations have enabled the development of MLLMs: • Encoder-Decoder Frameworks: These architectures, used in models like DALL-E, allow for mapping between text and image domains. The encoder processes the input (e.g., text), while the decoder generates the output (e.g., an image). It’s like having a translator who can convert a written story into a painting. • Cross-Modal Transformers: These use separate transformers for each modality, with cross-modal attention layers to fuse information. This allows the model to process text and images separately at first, then combine the information. It’s similar to how humans might read a book and look at illustrations separately, then combine that information for a fuller understanding. • Vision Transformers (ViT): These apply transformer architectures directly to image patches, enabling more seamless integration of vision and language models. Instead of processing an image as a whole, ViT breaks it down into smaller patches and processes them sequentially, much like how transformers process words in a sentence. Impact on AI Applications: The convergence of NLP and computer vision through MLLMs has enabled new capabilities in various AI applications: • Multimodal chatbots that understand and generate both text and images. For example, a customer service bot that can understand product images and respond with both text explanations and visual aids. • Content moderation systems that analyze text and images together, providing more context-aware filtering of inappropriate content on social media platforms. • Accessibility tools that generate image descriptions for visually impaired users, allowing them to ”see” images through detailed textual descriptions. • Enhanced human-vehicle interaction in autonomous driving systems, where the vehicle can understand both verbal commands and visual cues from the environment. Challenges and Future Directions: While MLLMs have made significant progress, several challenges remain: • Bias and Fairness: MLLMs can perpetuate or amplify biases present in training data across both textual and visual domains. For example, they might disproportionately misidentify individuals in images due to imbalanced training datasets. Addressing this requires careful dataset curation, diverse representation in training data, and ongoing monitoring and adjustment of model outputs. Researchers are exploring techniques like adversarial debiasing and fairness-aware learning to mitigate these issues. • Interpretability: Understanding how MLLMs make decisions across modalities is crucial for building trust and improving these systems. This involves developing techniques to explain model decisions that involve both textual and visual inputs, and creating visualization tools that can effectively represent the interplay between different modalities in the model’s reasoning process. Techniques like attention visualization and saliency mapping are being adapted for multimodal contexts to provide insights into model decision-making. • Efficiency: Current MLLMs often require substantial computational resources. Developing more efficient architectures and training methods is an active area of research. Potential solutions include: – Model pruning: removing unnecessary parameters to create smaller, faster models without significant loss in performance. – Knowledge distillation: creating smaller models that mimic the behavior of larger ones, like a student learning from a teacher. – Quantization: reducing the precision of model parameters to decrease memory usage and computational requirements. • Ethical Considerations: As MLLMs become more powerful, several ethical challenges arise: – Privacy concerns related to the processing and potential misuse of multimodal personal data. Researchers are exploring privacy-preserving techniques like federated learning and differential privacy to address these concerns. – The need for transparent decision-making processes, especially in critical applications like healthcare or autonomous systems. This involves developing explainable AI techniques that can provide clear rationales for MLLM decisions. – Potential misuse for creating deepfakes or other misleading content that combines manipulated text and images. Efforts are being made to develop robust detection systems for synthetic media and to establish ethical guidelines for the use of MLLMs in content creation. • Cross-modal Consistency: Ensuring consistency across different modalities presents a significant challenge. This includes developing methods to maintain semantic consistency between generated text and images, and addressing potential conflicts when integrating information from multiple modalities. Researchers are exploring techniques like consistency regularization and multi-task learning to improve cross-modal coherence in MLLM outputs. As research in this field progresses, we can expect MLLMs to become even more capable of understanding and generating content across diverse modalities, potentially leading to AI systems with more human-like comprehension of the world. The ongoing advancements in MLLMs continue to push the boundaries of what’s possible in artificial intelligence, opening up new avenues for innovation and application across various domains. 3 Conclusion and Future Prospects MLLMs represent a significant leap forward in AI technology, bridging the gap between different modes of information processing and bringing us closer to AI systems that can understand and interact with the world in ways that more closely resemble human cognition. Their ability to integrate and process multiple types of data simultaneously opens up a wide range of applications across various industries and domains. As we look to the future, the potential impact of MLLMs is vast and transformative: • In healthcare, MLLMs could revolutionize diagnostics and treatment planning by integrating visual medical data with textual patient histories and the latest research findings. For instance, an MLLM could analyze a patient’s MRI scans, medical history, and recent medical literature to suggest personalized treatment plans. • In education, these models could create more engaging and personalized learning experiences by adapting content based on a student’s multimodal interactions. An MLLM-powered tutoring system could adjust its teaching style based on a student’s verbal responses, facial expressions, and performance on visual tasks. • In scientific research, MLLMs could accelerate discoveries by analyzing complex, multimodal datasets and identifying patterns that might be missed by human researchers. For example, in climate science, an MLLM could integrate satellite imagery, weather data, and scientific papers to identify new patterns in climate change. • In creative industries, MLLMs could become powerful tools for content creation, enabling new forms of interactive and immersive storytelling. Imagine a video game that generates unique storylines and visual content based on a player’s actions and preferences. However, as we embrace the potential of MLLMs, we must also remain vigilant about the challenges they present. Addressing issues of bias, ensuring ethical use, improving efficiency, and enhancing interpretability will be crucial in realizing the full potential of these powerful models. Call to Action for Researchers and Practitioners: • Develop robust techniques for mitigating bias in multimodal datasets and model outputs. • Create more efficient MLLM architectures to reduce computational requirements and environmental impact. • Explore new methods for improving cross-modal consistency and coherence in MLLM outputs. • Investigate the integration of MLLMs with other emerging technologies, such as augmented reality and the Internet of Things. • Establish ethical guidelines and best practices for the development and deployment of MLLMs across various industries. The development of MLLMs is not just a technological advancement; it represents a fundamental shift in how we approach artificial intelligence. By mimicking the human ability to process and integrate multiple types of information, MLLMs are bringing us closer to creating truly intelligent systems that can understand and interact with the world in more nuanced and comprehensive ways. As research in this field continues to evolve, we can anticipate even more sophisticated MLLMs that push the boundaries of what’s possible in AI. The journey ahead is filled with exciting possibilities and challenges, and the continued development of MLLMs will undoubtedly play a crucial role in shaping the future of artificial intelligence and its impact on society. It is up to researchers, practitioners, and policymakers to guide this development responsibly, ensuring that the benefits of MLLMs are realized while mitigating potential risks and ethical concerns."
https://arxiv.org/html/2411.06251v1,Quasi-random Multi-Sample Inference for Large Language Models,"Large language models (LLMs) are often equipped with multi-sample decoding strategies. Vilnis et al. (2023) show that an LLM implicitly defines an arithmetic code book, facilitating efficient and embarrassingly parallelizable arithmetic sampling to produce multiple samples using quasi-random codes. Traditional text generation methods, such as beam search and sampling-based techniques, have notable limitations: they lack parallelizability or diversity of sampled sequences. This study explores the potential of arithmetic sampling, contrasting it with ancestral sampling across two decoding tasks that employ multi-sample inference: chain-of-thought reasoning with self-consistency and machine translation with minimum Bayes risk decoding. Our results demonstrate that arithmetic sampling produces more diverse samples, significantly improving reasoning and translation performance as the sample size increases. We observe a \mathbf{3\text{-}5\%} point increase in accuracy on the GSM8K dataset and a \mathbf{0.45\text{-}0.89\%} point increment in COMET score for WMT19 tasks using arithmetic sampling without any significant computational overhead.","There have been enormous efforts in improving the performance and efficiency of inference with large language models (Ippolito et al., 2019; Su and Collier, 2023; Grubisic et al., 2024; Zhou et al., 2024; Ding et al., 2024) based on system, data, and model level enhancements. In this paper, we consider that any decoding routine can be broadly assessed by its sample diversity (A.1) and parallelizability. Search-based techniques like beam search can approximate maximum a posteriori (MAP) decoding, mitigating duplicate samples at the expense of not being embarrassingly parallel. Sampling-based methods grounded in ancestral sampling techniques are parallel but don’t explicitly guarantee diverse sequences. The recently proposed arithmetic sampling Vilnis et al. (2023) technique enables parallel inference with diverse samples – by interpreting the inference as sampling from code points from a unit interval, given code points generating sequences becomes embarrassingly parallel and the sample diversity is guaranteed by construction. Decoding from pre-trained LLMs requires varying strategies for different downstream tasks. For complex reasoning and question-answering tasks, chain-of-thought (CoT) prompting Wei et al. (2022) is established for improving inference by instructing the model to generate intermediate reasoning paths. Wang et al. (2023) propose self-consistency as an additional improvement over chain-of-thought reasoning with multi-sample inference, attributable to diverse reasoning paths enhancing the confidence of the majority answer. For machine translation, minimum Bayes risk (MBR) decoding Kumar and Byrne (2004) is a classical approach for selecting the optimal translation from candidate translations generated by an LLM, requiring diversity to ensure performance. Thus, the inherent diversity of sequences generated via arithmetic sampling offers significant potential for enhancing decoding strategies that rely on multi-sample inference. Recognizing the importance of exploring this approach, we apply arithmetic sampling to both reasoning and translation tasks. For CoT reasoning with self-consistency and machine translation with MBR decoding, we observe accuracy improvements on the GSM8K and Commonsense QA datasets, along with substantial COMET score gains as the number of sampled sequences increases. ((a)) ((b)) ((c)) Figure 1: 8-shot evaluation on GSM8K with Gemma-7B and Llama-2-7B ((a)) ((b)) ((c)) Ancestral samplingArithmetic samplingGreedy Figure 2: 6-shot evaluation on Commonsense QA with Gemma-7B and Llama-2-7B"
https://arxiv.org/html/2411.06229v1,Multimodal contrastive learning of urban space representations from POI data,"Existing methods for learning urban space representations from Point-of-Interest (POI) data face several limitations, including issues with geographical delineation, inadequate spatial information modelling, underutilisation of POI semantic attributes, and computational inefficiencies. To address these issues, we propose CaLLiPer (Contrastive Language-Location Pre-training), a novel representation learning model that directly embeds continuous urban spaces into vector representations that can capture the spatial and semantic distribution of urban environment. This model leverages a multimodal contrastive learning objective, aligning location embeddings with textual POI descriptions, thereby bypassing the need for complex training corpus construction and negative sampling. We validate CaLLiPer’s effectiveness by applying it to learning urban space representations in London, UK, where it demonstrates 5 - 15% improvement in predictive performance for land use classification and socioeconomic distribution mapping tasks compared to state-of-the-art methods. Visualisations of the learned representations further illustrate our model’s advantages in capturing spatial variations in urban semantics with high accuracy and fine resolution. Additionally, CaLLiPer achieves reduced training time, showcasing its efficiency and scalability. This work provides a promising pathway for scalable, semantically rich urban space representation learning that can support the development of geospatial foundation models. The implementation code is available at https://github.com/xlwang233/CaLLiPer.","Accurate and comprehensive characterisation of urban spaces help us better understand how cities develop and function (Carmona, 2021; Gill et al., 2008), upon which we can further renovate and improve existing urban environment or future planning processes in order to tackle pressing challenges such as spatial inequalities (Nijman and Wei, 2020) and sustainability (Puchol-Salort et al., 2021). Point of interest (POI) data have been widely used for characterising urban spaces, as they provide crucial information about both “where” (spatial) and “what” (platial) aspects of cities (Goodchild, 2020), capturing fine-grained and up-to-date details depicting the land use composition, urban facilities distribution, and socioeconomic fabric of cities (Liu et al., 2020). In today’s booming age of big data and artificial intelligence, urban studies increasingly rely on computational methods to model, simulate, and analyse urban environments. Consequently, there has been extensive research focused on deep representation learning of urban spaces from POI data. Such representation learning are performed on various geographical scales, ranging from locations (Hong et al., 2023), neighbourhoods (Huang et al., 2021; Wang et al., 2020), to regions (Huang et al., 2022; Niu and Silva, 2021) and city level (Huang et al., 2023). And the learned representations have found widespread applications in urban functional distribution mapping (Huang et al., 2022, 2023), land use classification (Jean et al., 2019) socioeconomic indicator estimation (Jean et al., 2016), footfall prediction (Feng et al., 2017) and individual’s next location prediction (Hong et al., 2023). Existing urban space representation learning approaches are largely unsupervised or self-supervised, aiming at learning general purpose representations that can generalise to various downstream tasks. These methods propose to learn spatial co-occurrence patterns between POI categories and derive urban space representations as the aggregation of the POI category embeddings contained within (Huang et al., 2022, 2023; Yan et al., 2017; Yao et al., 2017; Zhai et al., 2019). These methods have several limitations: First, the spatial locations of POIs have not been explicitly represented and combined with their semantic information in existing models, as no current learning algorithm effectively integrates both locational information (coordinates, ""where"") and semantic attributes (POI types, ""what""). Second, these methods are subject to the geographical delineation of the urban space, which are often discretised areas like administrative regions or census tracts. These delineations are either predetermined during the pre-training stage or specified later in the aggregation stage. It is challenging to determine the ideal spatial resolution (Niu and Silva, 2021) as a finer one might result in no POIs within specific areas, while a coarser resolution might introduce ecological fallacy issue (Robinson, 2009). Moreover, it is hard for representations learned on one spatial scale to be applied to another scale and it would often require retraining the model. Third, existing methods fail to fully utilise the rich textual information of POI data. They learn the semantic representations of POIs via pure spatial co-occurrence patterns, based on the hypothesis that categories occurring with similar spatial contexts tend to have similar semantic properties (Bing et al., 2022). However, these implicit learning approaches neglect the fact that POI labels, being natural language descriptions per se, carry inherent semantic meanings that can be modelled from a computational linguistics perspective. Lastly, existing state-of-the-art methods are not efficient in terms of training. Most of them are based on word embedding approaches (Mikolov, 2013; Mikolov et al., 2013) used in natural language processing (NLP) domain. The construction of POIs co-occurrences corpus requires sophisticated design of spatial context . Moreover, some methods demand complex negative sampling strategies to facilitate contrastive learning (Huang et al., 2022, 2023). The high time complexities (e.g., O(n^{2})) of these methods incur prolonged training time. To address these challenges, this paper proposes a novel method for learning urban space representations, which we call CaLLiPer, for Contrastive Language-Location Pre-training. The core idea is to explicitly embed both locational and semantic information extracted from POI data into effective urban space representations. We propose a location encoder that embeds locations (coordinates) into vector representations that can integrate with the vector representations of the semantics of POIs. Such an ability is achieved through a location encoding process guided by a multimodal contrastive learning objective. Unlike other methods that embed discrete geospatial objects such as POIs or regions, our model treats the entire urban environment as a continuous space, enabling inductive learning that does not require retraining to extrapolate to unseen geospatial entities (Mai et al., 2022). Additionally, we use pre-trained text encoders to embed POI descriptive texts into textual representations, which are matched with the location embeddings of the corresponding POI coordinates, aligning the location embedding space with the well-trained language embedding space. Moreover, the construction of training data, i.e., (coordinate, text) pairs, is straightforward and much simpler than building a co-occurrence corpus. The multimodal contrastive learning objective also eliminates the need for extra negative sampling procedures, making the model highly efficient. To verify the effectiveness of our method, we apply CaLLiPer to learning urban space representations in London, UK, which are then used in two downstream tasks: urban land use classification and socioeconomic status distribution mapping. We compare our method with state-of-the-art models in terms of predictive performance on downstream tasks and training efficiency. Experimental results show that CaLLiPer can achieve 5% - 15% performance improvement on downstream tasks compared to the best-performing baselines, while having relatively short training time. Additionally, we conduct qualitative analysis of the representations to gain more understandings of our model. We briefly summarise our contributions as follows: • We advance urban space representation learning by explicitly integrating locational and semantic information from POI data into comprehensive, scalable representations, enabling the joint modelling of spatial (coordinates) and platial (texual labels) information. To the best of our knowledge, this is the first study that combines these two unique modalities through multimodal contrastive learning. • Our approach introduces several methodological innovations: treating cities as continuous spaces via location encoding, enabling flexible, scale-free applications without retraining; leveraging pre-trained NLP text encoders to capture POI semantics more effectively; and using multimodal contrastive learning to align spatial and semantic representations. This approach not only enhances spatial-semantic learning but also reduces the need for complex corpus design and negative sampling, improving computational efficiency. • Extensive experiments show that our proposed model outperforms state-of-the-art models by 5 - 15% in predictive accuracy across two downstream tasks while reducing training times, proving its effectiveness in real-world urban analysis. Following this introduction, we review related work in Section 2. In Section 3, we introduce notations used in the paper the formulation of POI-based urban representation learning. Section 4 elaborates on the detailed architecture and mathematical formulation of the proposed CaLLiPer model, as well as the deployment of the learned representations in downstream tasks. We present the experimental setup in Section 5, followed by results and analysis in Section 6. We summarise the major findings and discuss the potentials of CaLLiPer for geospatial foundation models in Section 7."
https://arxiv.org/html/2411.06211v1,"Artificial Intelligence for Collective Intelligence: 
A National-Scale Research Strategy","Advances in artificial intelligence (AI) have great potential to help address societal challenges that are both collective in nature and present at national or trans-national scale. Pressing challenges in healthcare, finance, infrastructure and sustainability, for instance, might all be productively addressed by leveraging and amplifying AI for national-scale collective intelligence. The development and deployment of this kind of AI faces distinctive challenges, both technical and socio-technical. Here, a research strategy for mobilising inter-disciplinary research to address these challenges is detailed and some of the key issues that must be faced are outlined.","Artificial intelligence (AI) and machine learning often address challenges that are relatively monolithic: determine the safest action for an autonomous car; translate a document from English to French; analyse a medical image to detect a cancer; answer a question about a difficult topic. These kinds of challenge are important and worthwhile targets for AI research. However, an alternative set of challenges exist that are collective in nature: • help to minimise a pandemic’s impact by coordinating mitigating interventions; • help to manage an extreme weather event using real-time physical and social data streams; • help to avoid a stock market crash by managing interactions between trading agents; • help to guide city developers towards more sustainable coordinated city planning decisions; • help people with diabetes to collaboratively manage their condition while preserving privacy. The capability of naturally occurring collective systems to solve problems of coordination, collaboration, and communication has been a long-standing inspiration for engineering (Bonabeau et al., 1999). However, developing AI systems for these types of problem presents unique challenges: extracting reliable and informative patterns from multiple overlapping and interacting real-time data streams; identifying and controlling for evolving community structure within the collective; determining local interventions that allow smart agents to influence collective systems in a positive way; developing privacy-preserving machine learning; advancing ethical best practice and governance; embedding novel machine learning and AI in portals, devices and tools that can be used transparently and productively by different types of user. Tackling them demands moving beyond typical AI/machine learning approaches to achieve an understanding of relevant group dynamics, collective decision-making and the emergent properties of multi-agent systems, topics more commonly studied within the growing research area of collective intelligence. Consequently, addressing these challenges requires a productive combination of collective intelligence research and artificial intelligence research (Berditchevskaia & Baeck, 2020; Berditchevskaia et al., 2022). In this paper we introduce and detail a research strategy for approaching this challenge that is being taken by a new national artificial intelligence research hub for the United Kingdom: AI for Collective Intelligence (AI4CI).111https://ai4ci.ac.uk The AI4CI Hub is a multi-institution collaboration involving seven partner universities from across the UK’s four constituent nations and over forty initial stakeholder partners from academia, government, charities and industry. It pursues applied research at the interface between the fields of AI and collective intelligence, and works to build capacity, capability and community in this area of research across the UK and beyond. This paper presents the AI4CI research strategy, details how it can be pursued across multiple different research themes, and summarises some of the key unifying research challenges that it must address."
https://arxiv.org/html/2411.06198v1,OpenAI-o1 AB Testing: Does the o1 model really do good reasoning in math problem solving?,"The Orion-1 model by OpenAI is claimed to have more robust logical reasoning capabilities than previous large language models. However, some suggest the excellence might be partially due to the model “memorizing” solutions, resulting in less satisfactory performance when prompted with problems not in the training data. We conduct a comparison experiment using two datasets: one consisting of International Mathematics Olympiad (IMO) problems, which is easily accessible; the other one consisting of Chinese National Team Training camp (CNT) problems, which have similar difficulty but not as publically accessible. We label the response for each problem and compare the performance between the two datasets. We conclude that there is no significant evidence to show that the model relies on memorizing problems and solutions. Also, we perform case studies to analyze some features of the model’s response.","1 Background The OpenAI Orion-1 model, commonly referred to as o1, was unveiled on September 12th, 2024, and has garnered significant attention since its release. This model, particularly the o1-preview and o1-mini variants, has been lauded for its advanced reasoning and logical capabilities, particularly in the realms of mathematical problem-solving and coding. One notable feature of the o1 models is their exceptional reasoning prowess, setting them apart from other Language Learning Models (LLMs) such as GPT-4o and Claude 3.5. The o1 models’ superior reasoning abilities are attributed to a novel training approach that involves employing a token-wise reward model through reinforcement learning. By leveraging this methodology, the o1 models emulate the reasoning and reflective processes, thereby fostering an intrinsic chain-of-thought style in token generation. In essence, the o1 model endeavors to formulate plans and subsequently execute them in the process of solving mathematical problems and engaging in other cognitive tasks. While the o1 models have demonstrated a strong level of reasoning capability, as highlighted by OpenAI’s claim that o1-mini’s score in the high school AIME math competition is around the top 500 US students, the evaluation of these models against a selection of private high school math questions yielded results that were not as impressive as anticipated. This discrepancy has prompted a research inquiry aimed at scientifically investigating whether the o1 models, akin to other Language Learning Models (LLMs), rely predominantly on memorizing solution steps in a chain of thought (CoT) fashion, or if they can genuinely generate high-quality reasoning steps even for unfamiliar math problems. The primary focus of this research is to assess the generalizability of the o1 models’ problem-solving abilities. Specifically, the study aims to determine if the o1 models possess robust generalizability compared to LLMs like GPT-4o and whether their reasoning capabilities contribute to enhanced problem-solving generalization. Furthermore, the investigation seeks to identify the difficulty level at which the o1 models’ generalization capabilities significantly surpass those of standard LLMs such as GPT-4o. To carry out rigorous research steps towards answering the questions in the above, we adopt a A/B-test approach. On the one hand, we construct benchmark problem sets such as the problems in the International Mathematics Olympiad (IMO), which is considered as the highest level of math competition amongst high school students in mathematics, and these problems have good open access to all the LLMs including o1. On the other hand, we construct a comparable problem set that are considered to have similar difficulty level in mathematics, for example, the Chinese Mathematics Olympiad (CMO), that are less accessible to the LLMs during training. Of course, one can still argue that the CMO problems also have open access compared to IMO, but CMO problems are less accessible and less likely to be pre-trained by these LLMs. In the end, we construct a private set of problems in the level of high school mathematics, which is even less likely to be accessed by OpenAI, with similar difficulty level compared to IMO problems. These problems are collected from the Chinese National Team Training camp (CNT), which aims at selecting representative students to participate IMO each year in China. Comparative statistics between IMO dataset and CNT datasets are provided to investigate on the performance of the o1 model. Another approach we adopt after the large scale statistical investigation and tests, is to perform case study on a few presentative problems. We provide both positive ones and negative ones, with different types of pros and cons from a mathematical/logical reasoning perspective. The case studies show that the o1 model has a prevalent problem with the “search” type of problems. The definition of such a problem and an example is in Section 2. In most situations, the model can only search for possible valid cases with small numbers or simple functions. Also, for all kinds of questions, the model sometimes provides skeletal problem-solving steps similar to a correct solution a human contestant would write, but they lack the crucial detailed justification in between. In other situations, the model may provide incorrect intuition. This research contributes to two primary areas. First, it advances the evaluation of AI systems’ capabilities in mathematical reasoning and automated theorem proving. Second, it offers an in-depth assessment of OpenAI’s o1 LLM performance across a broad spectrum of complex reasoning tasks, specifically in the categories of “search,” “solve,” and “prove” for various types of mathematical problems. These contributions provide a comprehensive understanding of the model’s abilities in addressing different aspects of mathematical reasoning. A substantial body of research has focused on evaluating the mathematical reasoning capabilities and automated theorem proving performance of AI systems. Large Language Models (LLMs) have demonstrated strong proficiency across a wide range of mathematical benchmarks, spanning from elementary-level problems, as shown in Cobbe et al. (2021), Yu et al. (2023), and Zhou et al. (2023), to more advanced high-school and pre-university level challenges, as evidenced by Mao, Kim and Zhou (2024), Urrutia and Araya (2023), and Hendrycks et al. (2021). Moreover, LLMs have been tested on highly complex tasks, such as International Mathematical Olympiad (IMO)-level problems. For instance, Sinha et al. (2024) and Trinh et al. (2024) specifically address the solution of IMO Geometry problems, while He et al. (2024) and Huang et al. (2024) introduce a range of Olympic-level problems, including mathematical problems, to evaluate AI systems’ problem-solving capabilities. Notably, in the OlympicArena, the highest performance was achieved by GPT-4o, with an overall accuracy of 39.97\%, whereas other models struggle to surpass the 20\% threshold. In the Olympiadbench, GPT-4’s accuracy for solving IMO problems is 17.97\%. Additionally, LLMs are being increasingly employed in the domain of automated theorem proving, as illustrated by systems such as LeanDojo Yang et al. (2024) and LeanAgent Kumarappan et al. (2024), which leverage AI for formal theorem proving in mathematics. The discussion surrounding what LLMs can do and how well they perform has been extensive. Numerous studies have systematically analyzed the performance of various types and versions of LLMs across different tasks. For instance, works like Chang et al. (2024), Zhao et al. (2023), and Minaee et al. (2024) provide comprehensive discussions on the properties, contributions, and limitations of popular LLM families (GPT, LLaMA, PaLM). Compared to previous versions of GPT, the o1 model demonstrates exceptional logical reasoning abilities and a broad knowledge base across multiple fields. Wu et al. (2024) explores the reasoning patterns of the o1 model in math, code, and commonsense reasoning, while Gui et al. (2024) introduces a LogicGame designed to assess the reasoning abilities of various LLMs, including the o1 model. 111Notably, the results from LogicGame show that even the o1 model’s reasoning score remains below the 50\% threshold. Additionally, Xie et al. (2024) and Hu et al. (2024) discuss the potential applications of LLMs like o1 in fields such as medical assistance and code debugging. For a general evaluation of the o1 model, see Zhong et al. (2024). Our research, however, focuses specifically on evaluating the performance of OpenAI’s o1 model in the domains of mathematical reasoning and automated theorem proving. According to Qiao et al. (2024), many LLMs rely heavily on Rote Memorization (RM), where pre-training data includes exact matches to problems found in the model’s memory, leading to a pattern where solving mathematical problems resembles memory retrieval rather than genuine reasoning. However, GPT-4 shows a stronger reliance on Inadequate Generalization (IG) over RM in mathematical reasoning. In our analysis, we demonstrate that the o1 model exhibits a reduced dependence on RM, indicating a shift towards more effective generalization. By focusing on the o1 model’s performance in these specific tasks, our study aims to deepen the understanding of LLMs’ applicability and limitations in complex reasoning tasks, as well as to investigate how LLMs approach and solve mathematical problems."
https://arxiv.org/html/2411.06191v1,Generalizing Hyperedge Expansion for Hyper-relational Knowledge Graph Modeling,"By representing knowledge in a primary triple associated with additional attribute-value qualifiers, hyper-relational knowledge graph (HKG) that generalizes triple-based knowledge graph (KG) has been attracting research attention recently. Compared with KG, HKG is enriched with the semantic qualifiers as well as the hyper-relational graph structure. However, to model HKG, existing studies mainly focus on either semantic information or structural information therein, which however fail to capture both simultaneously. To tackle this issue, in this paper, we generalize the hyperedge expansion in hypergraph learning and propose an equivalent transformation for HKG modeling, referred to as TransEQ. Specifically, the equivalent transformation transforms a HKG to a KG, which considers both semantic and structural characteristics. Then an encoder-decoder framework is developed to bridge the modeling research between KG and HKG. In the encoder part, KG-based graph neural networks are leveraged for structural modeling; while in the decoder part, various HKG-based scoring functions are exploited for semantic modeling. Especially, we design the sharing embedding mechanism in the encoder-decoder framework with semantic relatedness captured. We further theoretically prove that TransEQ preserves complete information in the equivalent transformation, and also achieves full expressivity. Finally, extensive experiments on three benchmarks demonstrate the superior performance of TransEQ in terms of both effectiveness and efficiency. On the largest benchmark WikiPeople, TransEQ significantly improves the state-of-the-art models by 15% on MRR.","In the past decade, knowledge graph (KG) has been widely studied in artificial intelligence area (Wang et al., 2017; Ji et al., 2021). By representing facts into a triple of (s,r,o) with subject entity s, object entity o and relation r, KG stores real-world knowledge in a graph structure. However, recent studies find that KG with simple triples provides incomplete information (Galkin et al., 2020; Yu and Yang, 2021; Xiong et al., 2023). For example, both (Alan Turing, educated at, Cambridge) and (Alan Turing, educated at, Princeton) are true facts in KG, which might be ambiguous when the degree matters. Hence, the hyper-relational KG (HKG) (Galkin et al., 2020), a.k.a., knowledge hypergraph (Fatemi et al., 2020) and n-ary knowledge base (Guan et al., 2019; Liu et al., 2021), is proposed for more generalized knowledge representation. Formally, in HKG, a primary triple is augmented with additional attribute-value qualifiers for rich semantics, called the hyper-relational fact111For a hyper-relational fact, entities/relation in the triple are called as primary entities/relation, and attributes/values in qualifiers are called as qualifier entities/relations. Note that the triple without qualifiers is a special case of hyper-relational facts. (Guan et al., 2020). Taking Figure 1 as an example, both (Alan Turing, educated at, Cambridge, (degree, Bachelor)) and (Alan Turing, educated at, Princeton, (degree, PhD)) are hyper-relational facts, where (degree, Bachelor) and (degree, PhD) are qualifiers with the degree attribute considered. Such hyper-relational facts are ubiquitous that over 1/3 of the entities in Freebase (Bollacker et al., 2008) involve in them (Wen et al., 2016). Figure 1: An example of a HKG including primary triples and attribute-value qualifiers. To learn from HKG and further benefit the downstream tasks, HKG modeling learns low-dimensional vector representations (embeddings) of entities and relations (Guan et al., 2019; Chung et al., 2023), which designs a scoring function (SF) based on the embeddings to measure the hyper-relational fact plausibility such that valid ones obtain higher scores than invalid ones. Especially, existing studies mainly consider two aspects of semantic information and structural information in HKG for modeling. The semantic information emphasizes the interaction between entities and relations in a hyper-relational fact. Especially, there is a distinction, a.k.a., semantic difference (Galkin et al., 2020) between the primary triple and attribute-value qualifiers according to the occurrence frequency in world knowledge. For example, the primary triple (Alan Turing, educated at, Cambridge) serves as the fundamental part and preserves the essential knowledge of Alan Turing’s education experience at Cambridge, while the attribute-value qualifier (degree, Bachelor) serves as the auxiliary part and enriches the primary triple. To model the semantic information, early studies treat the primary relation and qualifier relations as an n-ary (n\geq2) composed relation (Fatemi et al., 2020; Abboud et al., 2020; Wang et al., 2023) or multiple semantically equal attributes (Guan et al., 2019; Liu et al., 2021), largely ignoring the semantic difference. Various SFs are further developed in recent studies (Galkin et al., 2020; Guan et al., 2020; Rosso et al., 2020) with such semantic difference considered. On the other hand, the structural information focuses on the topological connection between entities in the hyper-relational graph structure222The HKG is also known as in a multi-relational hypergraph structure (Yadati, 2020), and here we adopt the commonly used term “hyper-relational graph structure” in existing studies (Galkin et al., 2020)., like an entity’s neighboring entities under various hyper-relational links, e.g., in Figure 1 Bachelor and Michelle Obama are neighbors of Alan Turing via degree and alumni, respectively. Only few studies (Galkin et al., 2020; Yadati, 2020) extend hypergraph neural network (HGNN) based modules to capture the structural information in HKG, however, empirical results in (Yu and Yang, 2021) demonstrate that removing such modules will not bring performance degradation, i.e., the direct extensions are quite immature for effective structural information capture. Hence, to the best of our knowledge, none of existing studies achieve HKG modeling with both semantic information and structural information completely captured, and it is still an open problem to be addressed. Targeting this open problem, we look back to KG modeling with an interesting observation that, recent studies (Schlichtkrull et al., 2018; Vashishth et al., 2019; Yu et al., 2021) leverage an encoder-decoder framework for KG modeling, i.e., a powerful graph neural network (GNN) based encoder and an expressive SF-based333Here the SF refers to scoring function proposed in KG modeling with only triples considered, e.g., TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), etc. decoder are leveraged for structural information and semantic information, respectively. On the other hand, in hypergraph learning, the hyperedge expansion is widely used to transform a hypergraph to a graph (Zhou et al., 2006; Antelmi et al., 2023). Inspired by these two points, in this paper, we propose an EQuivalent Transformation for HKG modeling, termed as TransEQ. Specifically, TransEQ firstly generalizes the hyperedge expansion to an equivalent transformation, transforming a HKG to a KG, based on which an encoder-decoder framework is further developed to capture information. For structural information, TransEQ introduces a GNN-based encoder on transformed KG with transformation characteristics combined. As for semantic information, to measure the plausibility of a hyper-relational fact, TransEQ exploits various SFs in existing HKG modeling studies as the decoder. The sharing embedding mechanism is further designed to capture the semantic relatedness between hyper-relational facts. In this way, with a simple yet effective transformation, the encoder-decoder framework in TransEQ captures not only structural information but also semantic information, which is the very innovation of this work, just like killing two birds with one stone. Besides, the flexible choice of SF in decoder ensures the full expressivity of TransEQ, representing all types of relations. We further theoretically prove that the generalized transformation is equivalent between a HKG and a KG without information loss. Our contributions are summarized as follows: • We propose the TransEQ model for HKG modeling, which generalizes the hyperedge expansion to an equivalent transformation with a HKG transformed to a KG, and then develops an encoder-decoder framework to associate KG modeling research with HKG ones, capturing both structural information and semantic information. • We theoretically prove that the proposed equivalent transformation brings no information loss, i.e., the conversion between HKG and KG is equivalent. We further prove that TransEQ is fully expressive to represent any HKG, completely separating valid facts from invalid ones. • We conduct extensive experiments and show that TransEQ achieves the state-of-the-art results across benchmarks, obtaining a 15% relative increase of MRR on the largest benchmark WikiPeople. Several studies demonstrate the model effectiveness and efficiency, and visualization results further indicate that TransEQ successfully captures the semantics in HKG."
https://arxiv.org/html/2411.06148v1,Deep Reinforcement Learning for Digital Twin-Oriented Complex Networked Systems,"The Digital Twin Oriented Complex Networked System (DT-CNS) aims to build and extend a Complex Networked System (CNS) model with progressively increasing dynamics complexity towards an accurate reflection of reality – a Digital Twin of reality. Our previous work proposed evolutionary DT-CNSs to model the long-term adaptive network changes in an epidemic outbreak. This study extends this framework by proposeing the temporal DT-CNS model, where reinforcement learning-driven nodes make decisions on temporal directed interactions in an epidemic outbreak. We consider cooperative nodes, as well as egocentric and ignorant ”free-riders” in the cooperation. We describe this epidemic spreading process with the Susceptible-Infected-Recovered (SIR) model and investigate the impact of epidemic severity on the epidemic resilience for different types of nodes. Our experimental results show that (i) the full cooperation leads to a higher reward and lower infection number than a cooperation with egocentric or ignorant ”free-riders”; (ii) an increasing number of ”free-riders” in a cooperation leads to a smaller reward, while an increasing number of egocentric ”free-riders” further escalate the infection numbers and (iii) higher infection rates and a slower recovery weakens networks’ resilience to severe epidemic outbreaks. These findings also indicate that promoting cooperation and reducing ”free-riders” can improve public health during epidemics.","Accurate modelling of Complex Networked Systems (CNSs) that involves real-time interactions is crucial for addressing societal challenges such as gender inequality, crime, and epidemics. For example, organized crime and pandemic outbreak can cause significant economic losses. An advanced CNS model that mimics real-world social interactions, could help policymakers and authorities simulate various scenarios and develop better strategies to address these pressing issues. Thefore, our previous work proposed a new modelling paradigm: Digital Twin-Oriented Complex Networked Systems (DT-CNSs) [1], which aims to build and extend CNSs with increasing complexity of generations towards the ultimate goal – a Digital Twin (DT) of real networked systems (Fig. 1). Figure 1: Generations of DT-CNSs, including generation 1: dynamic process on static networks, generation 2: evolving dynamic process on evolving networks, generation 3: evolving dynamic processes on evolving networks with interrelations between them, generation 4: temporal dynamic processes on temporal networks with interrelations between them and the acquisition of real time information, and generation 5 (a DT): Temporal dynamic processes on temporal networks with interrelations between them, as well as the real time two-way feedback between the reality and the CNSs, enabling an idealised state required by a DT. As shown in Fig. 1, the complexity of generations of DT-CNSs depends on the evolvability of the dynamics in DT-CNSs, interrelations between the dynamics in DT-CNSs, as well as the real-time interplay between the DT-CNSs and the reality. From generation 1 to 5, the temporal scale of network representation and modelling transforms to be more instantaneous, ranging from static (no change), evolving (slow/temporal changes captured in snapshots) to temporal (temporal changes captured in real time) [1]. The generations of DT-CNSs, as they approach a DT in generation 5, model the temporal changes in networks and dynamic processes, together with their interrelations, while allowing for the real-time interplay between reality and the DT-CNSs [1]. Under this conceptual framework, we proposed an extendable DT-CNSs modelling framework for complex social networks by introducing heterogeneous node features (a.k.a node attributes) and nodes’ preferences to create relationships, while allowing these preferences to evolve under the impact of dynamic processes towards rewarding interactions [2, 3]. However, the proposed modelling framework builds DT-CNSs in generation 1, 2 and 3. Its extension towards generation 4 and 5 poses a challenge in data collection, processing and model updates considering the real-time information (Fig. 1). In this study, we progress this the DT-CNS modelling framework by modelling temporal networks and the temporal dynamic process on the networks, which brings the DT-CNS closer to the ultimate goal of a DT. Reinforcement learning (RL) allows the agent to make decisions, observe the results, and then automatically adjust its strategy to achieve the optimal policy [4]. To address the scalability and efficiency issues of traditional RL, researchers proposed to use Deep Reinforcement Learning (DRL), leveraging Deep Neural Networks to enhance learning speed and performance [5, 4]. Current studies have employed DRL to implement multiple tasks in CNSs, such as influencing maximisation [6, 7, 8, 9], key-player identification [10, 11], network topology optimisation [12] and intelligent routing [13, 14, 15]. Some studies employed deep reinforcement learning to automatically make a decision in social networks, such as network resource allocation [16], rumour mitigation [17] and influence maximisation [18]. However, none of these studies employ deep reinforcement learning to optimally drive nodes’ cooperation and free-riding behaviours in an epidemic outbreak. This study proposes the temporal Digital Twin-Oriented Complex Networked System (DT-CNS) model driven by reinforcement learning algorithm. Our previous work [3] enables to model evolutionary DT-CNSs regarding the evolving networks, a dynamic process on the networks and their interrelated changes. This study extends this framework by introducing reinforcement learning-driven nodes to make decisions on temporal directed interactions in an epidemic outbreak. Under this framework, we consider nodes’ heterogeneous features and changeable connection preferences, which combine the effects of preferential attachment and homophily. We consider three types of nodes: (i) cooperative nodes who maximise their total reward using a collective mind driven by reinforcement learning; (ii) egocentric nodes who maximise the individual reward under the assumption of being the only ”free-rider” in the cooperation and (iii) the ignorant nodes who make random decisions. Given an epidemic outbreak, the healthy (sick) node, takes the risk of getting infected or infecting the neighbours via interactions. We describe this epidemic spreading process with the ”susceptible-infected-recovered” (SIR) model and investigate the impact of epidemic severity on the epidemic resilience for different types of nodes. Our experiment results shows that (i) full cooperation leads to higher reward and lower infection number than cooperation with egocentric or ignorant free-riders; (ii) an increasing number of ”free-riders” in cooperation leads to smaller reward, while an increasing number of egocentric ”free-riders” further escalate the infection numbers and (iii) higher infection rates and slower recovery weakens networks’ resilience to severe epidemic outbreaks. Overall, this study contributes in the following aspects: • we propose a temporal DT-CNS framework related to nodes’ temporal decisions on their preferences for connecting with others in an epidemic outbreak. • we create heterogeneous preference mutation styles that characterise nodes’ cooperation and free-riding behaviours. • we introduce deep reinforcement learning algorithms to drive nodes’ temporal decisions. • we find that cooperation enhances rewards and reduces infections, while an increase in ”free-riders” and higher infection rates with slower recovery diminish the network’s epidemic resilience. The rest of this study is structured as follows: Section II presents the methodology of building a temporal DT-CNS. Following this, Section III builds and evaluates the temporal DT-CNSs under more severe epidemic outbreaks. Finally, we conclude with Section IV."
https://arxiv.org/html/2411.06146v1,AI-Compass: A Comprehensive and Effective Multi-module Testing Tool for AI Systems,"AI systems, in particular with deep learning techniques, have demonstrated superior performance for various real-world applications. Given the need for tailored optimization in specific scenarios, as well as the concerns related to the exploits of subsurface vulnerabilities, a more comprehensive and in-depth testing AI system becomes a pivotal topic. We have seen the emergence of testing tools in real-world applications that aim to expand testing capabilities. However, they often concentrate on ad-hoc tasks, rendering them unsuitable for simultaneously testing multiple aspects or components. Furthermore, trustworthiness issues arising from adversarial attacks and the challenge of interpreting deep learning models pose new challenges for developing more comprehensive and in-depth AI system testing tools. In this study, we design and implement a testing tool, AI-Compass, to comprehensively and effectively evaluate AI systems. The tool extensively assesses multiple measurements towards adversarial robustness, model interpretability, and performs neuron analysis. The feasibility of the proposed testing tool is thoroughly validated across various modalities, including image classification, object detection, and text classification. Extensive experiments demonstrate that AI-Compass is the state-of-the-art tool for a comprehensive assessment of the robustness and trustworthiness of AI systems. Our research sheds light on a general solution for AI systems testing landscape.","In recent years, the remarkable improvement of deep learning models have revolutionized the landscape of various industry sectors and application domains, showcasing their unparalleled potential in solving complex problems and driving innovation [1, 2, 3, 4, 5, 6, 7, 8]. The dynamic interplay between data-driven insights and sophisticated model architectures has propelled deep learning to the forefront of modern technology, enabling groundbreaking advancements across a myriad of novel applications [9, 10, 11]. From enhancing medical diagnostics through image analysis to enabling autonomous vehicles to navigate and make informed decisions, the transformative capabilities of deep learning models have left an indelible mark on society [12, 13, 14, 15]. To contextualize this transformative power, consider the case of natural language processing where models like GPT-3 have demonstrated human-level proficiency in generating coherent and contextually relevant text, ushering in a new era of interactive and responsive AI systems [16, 17]. Such remarkable feats underscore the urgent need for a comprehensive assessment framework that can holistically evaluate the multifaceted dimensions of deep learning models, delving into the intricate interplay of vast datasets, intricate model architectures, and immense computational resources that underpin their unprecedented success [18, 19, 20]. The comprehensive assessment of deployed machine learning (ML) models, particularly deep learning models, is of paramount importance [21]. Such assessments serve as a crucial precautionary measure to uncover potential pitfalls and unanticipated consequences that could arise from utilizing inadequately evaluated models [22, 23]. Conducting a thorough performance and security evaluation ensures a nuanced understanding of the models’ capabilities, limitations, and potential biases, empowering informed decision-making and responsible deployment [24]. Neglecting a comprehensive assessment when deploying deep learning models can lead to detrimental outcomes. Biased predictions, unreliable results, and unexpected behaviors may emerge, eroding user trust, triggering legal and ethical challenges, and compromising the models’ real-world performance [25]. Real-world examples vividly illustrate these perils. In healthcare, deploying a poorly assessed AI diagnostic system could endanger patients through misdiagnoses[26]. For instance, malicious actors may mislead the ML tumor detection system into erroneously classifying benign tumors as malignant ones by introducing imperceptible perturbations to the original medical images. This has the potential to misguide the physician’s judgment, subsequently leading to irreversible harm to the patient’s health [27]. While inadequately evaluated autonomous vehicles might make flawed decisions, resulting in accidents [28]. For example, attackers can launch attacks on autonomous driving systems by introducing imperceptible perturbations to traffic signs. By applying imperceptible perturbations to stop signs as perceived by human eyes, the ML system may misclassify them as yield signs. This could lead to severe traffic accidents, posing a threat to user safety [29]. The financial sector is also at risk, as untested deep learning algorithms in market predictions could yield severe economic repercussions [30]. These instances underscore the urgency of a robust assessment framework, which is essential for mitigating risks and ensuring the safe and effective deployment of deep learning models [31]. To achieve a comprehensive assessment of deep models, it is imperative to delve into three pivotal dimensions: adversarial robustness, model explainability, and neuron analysis [32, 33, 34]. Adversarial robustness stands as a bulwark, ensuring consistent performance even amid uncertainties and adversarial scenarios, thus fortifying its real-world applicability [35]. Meanwhile, model explainability serves as a beacon of transparency, demystifying the decision-making process and fostering trust, especially in contexts where accountability is paramount [36]. Simultaneously, the intricate realm of neuron analysis grants us a profound understanding of the model’s inner workings, at the level of individual neurons, elucidating the pathways of feature extraction and representation learning [37]. The convergence of these facets not only empowers a comprehensive evaluation but also equips stakeholders with the insights needed to navigate the nuanced landscape of deep learning models, promoting informed deployment and harnessing their transformative potential across diverse domains [38]. In order to enhance the capability for testing Deep Learning Systems (DLS) in real-world applications, numeric testing tools have been developed. Taking medical image analysis as an example, DLTK [39], as an open-source DL toolkit, provides a range of tools for testing and validating the quality of DLS, including model evaluation, model interpretation, and model visualization. DLTK provides a detailed diagnostic report for medical images, reducing the risk of misjudgment by explaining the model’s behavior. DeepXplore [40] is an automated white-box testing framework for DLS that employs optimization techniques such as gradient ascent to detect potential failures in the system. As an effective approach for automated testing of deep neural network (DNN)-driven autonomous cars, DeepTest [41] designs a test generation framework that combines mutation operators, metamorphic relations, and real-world driving scenarios to generate test cases with higher neuron coverage. However, even though the DLS testing tools are constantly being upgraded, resembling an arms race in multiple fields as described above, the inherent ad-hoc, task-oriented nature of existing tools persists as an unavoidable limitation, often making them unsuitable for fulfilling multi-task testing requirements. For example, the testing objective of DeepXplore is monotonous, and it is exclusively applicable to white-box testing, which makes it unsuitable as a general-purpose testing tool in a black-box environment. Furthermore, DeepXplore does not provide an explanation for how a model’s defects are detected. Both DLTK and DeepTest have limited testing capabilities in application scenarios unrelated to medical image analysis and DNN-driven autonomous driving, lacking sufficient tests of adversarial robustness or model interpretability. As far as we know, existing testing tools can only conduct individual module tests on a model’s adversarial robustness, interpretability, or neuron analysis, rather than explaining the relationships among these three aspects. In order to enable multidimensional evaluation and selection of models, we are dedicated to integrating these modules for multi-task testing and constructing a comprehensive testing tool. In addition, pruning has been proven to facilitate the interpretation of model decisions and reduce the occurrence of overfitting during adversarial sample training [42, 43]. For the first time, we introduce an approach to neuron analysis with pruning techniques, thereby exploring potential connections among the modules. In this paper, we propose AI-Compass, a comprehensive and effective multi-module testing tool for DLS. Specifically, combined with the basic utility module including indicator evaluation and mutation operations [44, 45], for the first time, we design modules for adversarial robustness, model interpretability and neuron analysis, to extensively evaluate the performance of DLS [46]. Through a thorough validation involving 6 deep learning models across 3 datasets, we demonstrate that AI-Compass is capable of testing image classification, object detection, and text classification tasks in DLS. Compared to existing DLS testing tools, AI-Compass not only conducts fundamental DLS testing but also delivers precise evaluations of model robustness against adversarial attacks. Furthermore, it provides trustworthy model interpretability reports, including a quantified assessment of the tested model’s interpretability, along with attributional result charts for illustration. The main contributions of this paper are as follows: • We present a comprehensive and effective framework, AI-Compass, for automatically testing the quality of DLS. Specifically, combined with the basic utility module, for the first time, we design modules for adversarial robustness, model interpretability, and neuron analysis, making a significant step towards building robust and trustworthy DLS. • Inspired by the pruning algorithm, we conduct an in-depth analysis of neural network redundancy. We comprehensively investigate the changes in adversarial robustness and model interpretability resulting from neuron pruning approach, thus providing valuable insights for model architecture optimization. • We demonstrate that our AI-Compass can be effectively applied for multi-modal scenarios. The testing results in image classification, text classification, and object detection tasks verify the high scalability of our AI-Compass and solve the ad-hoc problem in existing testing tools. • We have conducted extensive experiments and generated detailed test reports to demonstrate the superiority of our AI-Compass in testing DLS. • The code is released for future research and enhancements by scholars and industry professionals. This study extends our previous conference paper [46]. In Section 2, we provide an overview of related work on testing frameworks to afford readers a more comprehensive understanding of the field. Section 3 introduces the preparatory background, furnishing foundational knowledge regarding adversarial attacks, model interpretability, and pruning algorithms. In Section 4, we expand the conceptual diagram of ML-compass [46] to assist readers in gaining a clearer and more comprehensive grasp of the AI-Compass architecture. Section 5 delves into the methods and principles underpinning each module and sub-module to provide a more profound understanding of the framework’s foundational principles. Section 6 explains the experimental setup and analyzes the experimental results. In this study, we primarily undertake the following innovations in expending ML-compass: • Involving more metrics in model utility evaluation. • Introducing mutant methods in the model utility evaluation to simulate real-world scenarios. • Incorporating black-box transfer attacks in the robustness evaluation to reveal potential model vulnerabilities in practice. • Integrating more interpretability methods in the interpretability evaluation, using Insertion & Deletion Score as a metric to quantitatively assess model interpretability. • To enable neuron analysis, we employed additional pruning algorithms. • Introducing a comprehensive evaluation, utilizing radar charts for a more intuitive display of model testing results, facilitating user selection of suitable models. • Exploring additional testing possibilities for models by combining pruning algorithms with the robustness and interpretability analysis. TABLE I: Existing testing tools for DLS assessment Basic Metric Mutants Neural Analysis Robustness Analysis (white-box) Robustness Analysis (black-box) Interpretability Multi-Model DeepXplore [40] NeuronFair [47] DeepGauge [48] DeepTest [41] DeepMutation [44] DeepMutation++ [45] InterpretDL [49] Ours (AI-Compass) • : The test tool has the function; : The test tool does not have the function."
https://arxiv.org/html/2411.06120v1,Evaluating the Propensity of Generative AI for Producing Disinformation During an Election Cycle,"Generative Artificial Intelligence offers a powerful tool for adversaries who wish to engage in influence operations, such as the Chinese Spamouflage operation and the Russian Internet Research Agency effort that both sought to interfere with recent US election cycles. Therefore, this study seeks to investigate the propensity of current Generative AI models for producing harmful disinformation during an election cycle. The probability that different Generative AI models produced disinformation when given adversarial prompts was evaluated, in addition the associated harm. This allows for the expected harm for each model to be computed and it was discovered that Copilot and Gemini tied for the overall safest performance by realizing the lowest expected harm, while GPT-4o produced the greatest rates of harmful disinformation, resulting in much higher expected harm scores. The impact of disinformation category was also investigated and Gemini was safest within the political category of disinformation, while Copilot was safest for topics related to health. Moreover, characteristics of adversarial roles were discovered that led to greater expected harm across all models. Finally, classification models were developed that predicted disinformation production based on the conditions considered in this study, which offers insight into factors important for predicting disinformation production. Based on all of these insights, recommendations are provided that seek to mitigate factors that lead to harmful disinformation being produced by Generative AI models. It is hoped that developers will use these insights to improve future models.","The role of technology in the spread of false information is evident, with social media offering a mechanism for false information to spread rapidly through the information ecosystem (Allcott, et al., 2019; Chen, et al, 2023; Schlicht, 2024), and Generative AI providing a tool that can be misused by adversaries to rapidly produce convincing disinformation that is believed (Goldstein, et al., 2024) and shared (Bashardoust, 2024) at similar rates as accurate information. Recent examples of known operations include the Russian Internet Research Agency that attempted to influence the 2016 US elections (Eady, et al., 2023), in addition to more recent attempts by Chinese actors in the ’Spamouflage’ operation that is currently attempting to influence the 2024 US election cycle (Bing and Paul, 2024). Although the exact impact of such operations is debated, there are serious consequences that can result from false information spanning both political and health sectors (Kavanagh and Rich, 2018; Menz, et al., 2024). For example, the January 6th Capital Riot was sparked by erroneous beliefs about election fraud (Riccardi, 2022), while misinformation surrounding the COVID pandemic resulted in mental health consequences by those who consumed false information (Verma, et al., 2022). Research has also shown that exposure to disinformation can unconsciously alter behavior (Bastick, 2020) and even impact voting in a political campaign by creating false memories that align with people’s a-priori beliefs (Murphy, et al., 2019). The appeal of leveraging Generative AI models for producing disinformation is the ability to easily and rapidly generate content across multiple topics and languages, with little need for technical sophistication. Although there are previous studies that investigated the propensity of Generative AI for producing disinformation (e.g., Barman, et al., 2024; Wang, 2023), they were not conducted during an ongoing major election cycle. Since this is the time adversaries are most likely to employ such tactics, this study expands on existing investigations by comparing three of the most popular Generative AI models during an ongoing US election cycle. Moreover, this study proposes a framework to evaluate the harm associated with the disinformation produced by Generative AI, allowing for the expected harm to be estimated under different adversarial tactics. The next section will describe the methods used for this evaluation that span both political and health topics of disinformation."
https://arxiv.org/html/2411.06087v2,Cross-Domain Transfer Learning using Attention Latent Features for Multi-Agent Trajectory Prediction,"With the advancements of sensor hardware, traffic infrastructure and deep learning architectures, trajectory prediction of vehicles has established a solid foundation in intelligent transportation systems. However, existing solutions are often tailored to specific traffic networks at particular time periods. Consequently, deep learning models trained on one network may struggle to generalize effectively to unseen networks. To address this, we proposed a novel spatial-temporal trajectory prediction framework that performs cross-domain adaption on the attention representation of a Transformer-based model. A graph convolutional network is also integrated to construct dynamic graph feature embeddings that accurately model the complex spatial-temporal interactions between the multi-agent vehicles across multiple traffic domains. The proposed framework is validated on two case studies involving the cross-city and cross-period settings. Experimental results show that our proposed framework achieves superior trajectory prediction and domain adaptation performances over the state-of-the-art models.","I INTRODUCTION In the realm of intelligent transportation, the landscape of vehicle trajectory prediction has evolved significantly and gained immense traction in intelligent transportation systems, spanning applications from vehicle design to traffic forecasting and traffic control [1]. This transformation is driven by advancements in vehicle sensor hardware and traffic infrastructure. Consequently, this allows the acquisition of high fidelity sensory and positional data of multiple vehicles, which are crucial for data-driven modelling of the complex spatial-temporal interactions between vehicles in a multi-agent traffic network [2]. In particular, an accurate forecast of the future vehicular trajectories allows a ego vehicle to plan its optimal navigational route within the network and alleviate traffic issues such as congestion and accidents. Recent advances in the deep learning paradigm have tremendously enhanced vehicular trajectory prediction in many traffic networks such as arterial roads, boulevards, and interstate highways. A noteworthy example is the graph-based interaction-aware trajectory prediction (GRIP) model [3] which incorporates graph convolutional neural network (GCN) and a recurrent encoder-decoder architecture. The GRIP model exploited graph representation to model complex spatial inter-agent interactions and the sequential encoding modules in recurrent neural network (RNN) to model temporal correlation across the vehicle trajectories. Apart from that, attention mechanism of the Transformer networks [4, 5] has been harnessed to effectively model time-series data. The attention mechanism involves a global treatment of the time-evolving trajectory as a unified sequence, thus mitigating the deficiency of RNNs in retaining long-term temporal dependencies in long vehicle trajectory [6, 7]. Nevertheless, deep trajectory prediction models are often tailored to the available training data, which could be collected on a particular traffic configuration such as time period or fixed location. This adherence to a certain traffic domain inhibits the model from effectively generalizing its prediction to unseen traffic networks [8]. Current domain adaptation approaches for trajectory prediction predominantly rely on semi-supervised techniques [9]. These approaches serve as effective means to transfer existing knowledge within vision models pertaining to road geometry and topology, and in kinematic models for understanding driver maneuvering behaviors. For example, Xu et al. [10] have tackled the domain shift challenge by employing domain adaptation techniques such as similarity losses between source and target domains for distribution alignment in the context of pedestrian trajectory prediction. Nevertheless, a shift in geographical location and distinct traffic conditions could make these models ineffective when applied to comprehensive system-wide network. Moreover, traffic dynamics evolve over time and thus the validity of a trained model would be limited to the given temporal window. Motivated by these difficulties, in this paper, we propose a novel sequence-to-sequence graph Transformer-based model (namely Graph Embedded Transformer) to learn the spatial-temporal features of multi-agent trajectory data. Our proposed framework utilizes the embedding capabilities of GCN to help model the spatial features of multi-agent trajectories, while the Transformer performs temporal modelling of the trajectory sequence. To address the issue of co-variate domain shifting due to the differences in traffic distributions in different locations or periods, we introduce a domain adaptation training strategy on top of the spatial-infused attention embedding of the Transformer encoder to adapt the model’s attention across multiple traffic domains. To the best of our knowledge, this is the first work that investigates the feasibility of a domain-adaptable graph Transformer-based framework on generalizing trajectory prediction from source domain to target domain with limited training data. Our contributions are highlighted as follows: • We introduce the Graph Embedded Transformer, which first integrates a Graph Convolutional Network to construct spatial-aware non-Euclidean input embeddings for the Transformer [11]. The Transformer’s encoder module then perform temporal encoding of the spatial input embeddings to encode historical trajectory. The Transformer’s decoder module subsequently generates future trajectory based on the encoder’s latent features. • A domain adversarial training strategy is incorporated on top of the Transformer’s encoder module is to achieve cross-domain transfer learning across traffic domains. In particular, the latent representation of the encoders composed of its spatially-infused attention embeddings are input into a discriminatory layer to minimize the statistical discrepancy between the latent space in different domains. • The efficacy of the proposed Graph Embedded Transformer is validated against the state-of-the-art vehicle trajectory prediction models. Comparative results on the NGSIM-I80 and NGSIM-US101 datasets show that our proposed model achieves superior accuracy across the source and target domains, thus indicates effective domain generalization. The rest of this paper is organized as follows. In Section II, we briefly discuss related work followed by the problem formulation in Section III. In Section IV, we describe our base architectures used and implementation details for domain adaptation. We report our experimental results in Section V. Finally, we conclude this paper in Section VI. Figure 1: Graph Embedded Sequence-to-Sequence Transformer integrated with Domain Adaptation. The target is separated into (a) Training Mode: Last segment from Source + Output shifted right and (b) Inference Mode: Last Segment from Source + Iterative Predictions shifted right."
https://arxiv.org/html/2411.06034v1,"CROPS: A Deployable Crop Management System Over All Possible State
Availabilities","Exploring the optimal management strategy for nitrogen and irrigation has a significant impact on crop yield, economic profit, and the environment. To tackle this optimization challenge, this paper introduces a deployable CRop Management system Over all Possible State availabilities (CROPS). CROPS employs a language model (LM) as a reinforcement learning (RL) agent to explore optimal management strategies within the Decision Support System for Agrotechnology Transfer (DSSAT) crop simulations. A distinguishing feature of this system is that the states used for decision-making are partially observed through random masking. Consequently, the RL agent is tasked with two primary objectives: optimizing management policies and inferring masked states. This approach significantly enhances the RL agent’s robustness and adaptability across various real-world agricultural scenarios. Extensive experiments on maize crops in Florida, USA, and Zaragoza, Spain, validate the effectiveness of CROPS. Not only did CROPS achieve State-of-the-Art (SOTA) results across various evaluation metrics such as production, profit, and sustainability, but the trained management policies are also immediately deployable in over of ten millions of real-world contexts. Furthermore, the pre-trained policies possess a noise resilience property, which enables them to minimize potential sensor biases, ensuring robustness and generalizability. Finally, unlike previous methods, the strength of CROPS lies in its unified and elegant structure, which eliminates the need for pre-defined states or multi-stage training. These advancements highlight the potential of CROPS in revolutionizing agricultural practices.","Food security is a crucial goal in contemporary agriculture, highlighting the significance of key management practices such as nitrogen fertilization and water irrigation. These techniques are essential not only for increasing crop yields and ensuring a stable food supply but also play a vital role in sustaining environmental health. Traditional best practices in these domains, informed by empirical experience and scholarly research, are now being tested against the backdrop of changing climatic and market conditions. This raises concerns about their continued effectiveness, underscoring the need for more innovative, efficient, and adaptable management systems. Such systems are essential for developing strategies that are responsive to varying conditions and aimed at specific goals, including economic profitability. This study seeks to contribute to this need by applying advanced AI methods to enhance agricultural practices, addressing these significant challenges in the pursuit of more sustainable and productive farming methodologies. Criterias MLP-based Agent IM-based Agent LM-based Agent CROPS Agent (Ours) Unified Framework ✓ ✗ ✓ ✓ \hdashline Readily Deployable ✗ ✓ ✗ ✓ \hdashline Noise Resilience ✗ ✗ ✗ ✓ \hdashline Deployable Scenarios 0 1 0 10^{8} Table 1: Overview of the critical properties of the proposed method compared with previous SoTA methods. Figure 1: Framework and pipeline of the intelligent crop management system using LM-based RL Recent advancements in agricultural technology have introduced multi-layer perception (MLP)-based reinforcement learning (RL) agents (MLP-based Agents) and language-model-based reinforcement learning agents (LM-based Agents) for training nitrogen (N) and irrigation management policies using the Gym-DSSAT simulator (Romain et al. 2022; Wu et al. 2022, 2024b; Wu 2024). Their research demonstrated the ability of these policies to surpass a baseline by producing higher yields or achieving similar yields with less N input under full observation conditions. However, the practical implementation of these policies in real-world scenarios is hindered by their reliance on comprehensive observational data, such as nitrate leaching and plant N uptake, which are typically not readily available to farmers. Addressing this gap, a recent study presents an intelligent crop management framework that adeptly combines reinforcement learning (RL), imitation learning (IL), and crop simulations using DSSAT and Gym-DSSAT (Tao et al. 2022). In this paper, we refer to their trained agent as the imitation learning-based agent (IM-based Agent). This approach enhances the adaptability and applicability of the management policies to real-world agricultural settings by effectively addressing the challenge of partial observation. While IL has proven effective in refining existing agricultural strategies by better aligning them with the practical realities of farming, it’s crucial to recognize the variability in the availability of states in real-world scenarios. This variation is often case-specific, dictated by factors such as the deployment of sensors and the unique characteristics of different environments. Consequently, one state that is observable and accessible in one location may not be available in another, posing a significant challenge. This inconsistency in state availability can severely limit the applicability of a pre-trained RL agent from imitation learning. Additionally, the two-stage training approach used in RL and IL represents a significant limitation in the context of agricultural management optimization. Unlike an integrated, end-to-end framework, this method typically involves using an expert policy, pretrained in a fully observed setting, to guide the RL agent in scenarios with only partial observations. Such a bifurcated training process can potentially lead to suboptimal optimization of crucial resources like nitrogen and water. This is because the prior knowledge in expert policy, developed under the assumption of complete information, may not be transferred effectively to settings where only limited data is available. Consequently, this approach might result in management strategies that are both less efficient and less effective. Addressing the aforementioned challenges, our research pivots to developing a more robust and universally applicable RL agent trained within a unified framework. Prior studies have suggested the utilization of language models (LMs) as enhanced RL agents, demonstrating state-of-the-art (SoTA) performance across diverse scenarios. Although potent in their capabilities, these models are primarily configured for scenarios with full access to all states in simulations, limiting their direct deployability in real-world settings. Building on this limitation, we introduce the masking technique as a crucial auxiliary component in our optimization task. To be more specific, we propose an intelligent crop management framework that incorporates a powerful LM-based RL agent, state masking strategy, and crop simulations via Gym-DSSAT. We illustrate the overall framework in Figure 1. We transitioned from a traditional MLP-based RL agent to a more powerful LM-based agent, exhibiting an improved ability to enhance crop yields and promote sustainability amidst the complexities of optimization tasks. More importantly, we have also implemented a state masking strategy that replicates the inherent uncertainties of real-world agricultural scenarios. Consequently, the LM-based RL agent is charged with a twofold task: executing management decisions and reconstructing obscured states. This development not only enables the RL agent to make smarter decisions when information is incomplete but also strengthens its ability to make reliable and noise-agnostic decisions in the face of the unpredictability that farmers often face. In summary, the primary contributions of this work are as follows: • The study delves into a pivotal but underexplored question: can LMs, utilizing random state masking and reconstruction, function as superior bi-task RL agents for crop management and missing state recovery? • We propose CROPS, the first, elegant and unified framework that is readily deployable, noise-resilient, and applicable to ten millions of real-world contexts as shown in Table 1. • We empirically demonstrate that CROPS outperforms existing state-of-the-art approaches in extensive experiments, assessing metrics such as crop yield, resource utilization, environmental impact, and, most critically, robustness in both fully observed and partially observed settings."
https://arxiv.org/html/2411.05990v2,"Game-theoretic LLM:
Agent Workflow for Negotiation Games","This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models’ ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.Our research contributes to a deeper understanding of LLMs’ decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.","Large Language Models (LLMs), such as GPT-4 and Claude, have achieved remarkable progress in natural language understanding and generation zhang2024supervised ; ding2024hybrid ; fang2024large , driving advancements in fields ranging from conversational AI dam2024complete ; dong2023towards to content creation liang2024monitoring ; shao2024assisting and agentic task delegation guo2024embodied ; agashe2023evaluating ; xi2023rise . LLMs are increasingly integrated into applications that influence everyday activities, such as planning, acting, and decision-making. Therefore, the ability of LLMs to navigate complex situations has significant implications for their deployment in applications requiring strategic interaction, such as automated negotiations, economic modeling, and collaborative problem-solving bianchi2024well ; horton2023large ; li2024econagent ; chen2024comm ; li2023metaagents . Despite the wide exploration and utilization, LLM’s capacity for rational behavior, particularly in strategic settings represented by game theory, remains an open question leng2023llm ; stade2024large ; wu2024shall ; de2023emergent ; lan2023llm . In this context, rationality implies an agent’s ability to make decisions that maximize expected utility based on available information, an essential component of intelligent and adaptive decision-making. In the realm of game theory, rational agents are expected to act strategically, considering not only their own preferences but also the potential actions and preferences of others. This is especially critical in incomplete-information games, where uncertainty about other players’ information necessitates sophisticated reasoning and belief updating. This paper investigates the capacity of LLMs to behave rationally in game-theoretic scenarios and explores methodologies to enhance their rational decision-making capabilities. We begin by assessing the performance of several state-of-the-art LLMs, including Claude-3.5 Sonnet, Claude-3 Opus, GPT-4o and o1 zhong2024evaluation , in both complete-information and incomplete-information games such as the Prisoner’s Dilemma, Battle of the Sexes, the Escalation Game, and Deal-or-No-Deal lewis2017deal , presented in Figure 1. Our analysis reveals LLMs often deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees (Section 4). They also exhibit a lack of robustness to noise and uncertainty, leading to suboptimal outcomes (Section 6). To address these limitations, we introduce a novel approach by proposing game-theory-inspired workflows specifically designed to guide the reasoning and decision-making processes of LLMs. This is the first attempt to systematically integrate classic game-theoretic strategies into LLM-based agent workflow, aiming to enhance their rational behavior and decision-making capabilities in strategic settings. These workflows incorporate principles such as Dominant Strategy Search, which involves identifying strategies that yield the highest payoff regardless of the opponent’s actions; Backward Induction, a method of solving extensive-form games by analyzing them from the end states backward to the initial decision nodes to determine optimal strategies; and Bayesian belief updating, which allows agents to refine their beliefs about other players’ valuations based on observed actions and signals during the game. Cringed on these well-defined and well-studied game-theoretic methods, we design algorithms to guide the behavior and thinking process of LLM-based agents. Additionally, we integrate fairness considerations like envy freeness and pareto optimality, which promote equitable and efficient outcomes in negotiations by ensuring that no agent prefers another agent’s allocation to their own and that no improvements can be made without making at least one agent worse off. Contribution Summary • Comprehensive Evaluation of LLMs in Strategic Games and Identification of Rationality Limitations in LLMs (Section 4 and 6): Through empirical analysis, we uncover that LLMs often fail to behave rationally in strategic settings, exhibiting a lack of robustness to noise and randomness. • Design of Game-Theory-Inspired Workflows (Section 4.4 and 5.2): We develop novel workflows inspired by game-theoretic concepts to guide the reasoning and decision-making processes of LLMs, incorporating analysis and algorithms from classic game theory. • Emerging Research Direction (Section 5.5.3 and 5.6): Through the application of workflows, we identify a promising new research direction in meta-strategy, specifically focusing on the decision of whether to adopt a workflow and, potentially, which workflow to employ in varying scenarios. Figure 1: Game-theoretic Landscape Investigated in this Paper."
https://arxiv.org/html/2411.05943v1,Quantifying artificial intelligence through algebraic generalization,"The rapid development of modern artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, modern AI systems fall short on tests requiring symbolic processing and abstraction – a glaring limitation given the necessity for interpretable and reliable technology. Despite a surge of reasoning benchmarks emerging from the academic community, no comprehensive and theoretically-motivated framework exists to quantify reasoning (and more generally, symbolic ability) in AI systems. Here, we adopt a framework from computational complexity theory to explicitly quantify symbolic generalization: algebraic circuit complexity. Many symbolic reasoning problems can be recast as algebraic expressions. Thus, algebraic circuit complexity theory – the study of algebraic expressions as circuit models (i.e., directed acyclic graphs) – is a natural framework to study the complexity of symbolic computation. The tools of algebraic circuit complexity enable the study of generalization by defining benchmarks in terms of their complexity-theoretic properties (i.e., the difficulty of a problem). Moreover, algebraic circuits are generic mathematical objects; for a given algebraic circuit, an arbitrarily large number of samples can be generated for a specific circuit, making it an optimal testbed for the data-hungry machine learning algorithms that are used today. Here, we adopt tools from algebraic circuit complexity theory, apply it to formalize a science of symbolic generalization, and address key theoretical and empirical challenges for its successful application to AI science and its impact on the broader community.","The ability to reason algebraically is often considered a hallmark of human intelligence 1. The recent evolution of modern artificial intelligence (AI) systems and large language models (LLMs) has led to the speculation that these systems may also reason algebraically 2, 3, 4. Yet due to challenge of evaluating large models trained on massive pretraining datasets 5, it is difficult to evaluate whether such models are truly exhibiting algebraic reasoning abilities, or whether they instead regurgitate plausible text from their pretraining data 6, 7. This ambiguity has led to a deluge of symbolic reasoning benchmarks 8, 9, 10, 11, 12, 13, 14, 15, 16, 17. Despite these efforts, objectively quantifying the complexity of reasoning problems is difficult; most of these experiments are ad hoc, and designed without a framework to quantify complexity. However, approaches in computational complexity theory, a field within theoretical computer science, have made it possible to explicitly measure a problem’s algorithmic difficulty, enabling the design of generalization tests rooted in quantifiable measures of complexity. Here, we adopt a branch of computational complexity theory – algebraic circuit complexity – to provide a parsimonious set of problems and approaches to rigorously study symbolic computation in machine learning. Recently, there has been increased interest in studying AI models through arithmetic and compositional tasks 18, 19, 20, 21, 22, 23, 24, 25, 26, 12, 9, 27. Compositional tasks are problems that are generated by recombining a basis set of atomic elements to form a variety of task combinations. (Arithmetic problems are compositional; they are composed of atomic elements (numbers and operators), and can be recomposed to generate novel expressions and problems.) These tasks are good reasoning benchmarks because they require 1) abstraction, 2) logical application of rules or axioms, and 3) precise problem-solving and rigor. Critically, these paradigms have provided reliable ways to elicit failure modes in transformer-based AI models for specific forms of symbolic generalization. For example, a number of studies have demonstrated the difficulty of “length generalization” – generalizing to problems of longer length than seen during training 28, 19, 26, 21. Hupkes et al. 9 and Hupkes et al. 29 also introduced various notions (e.g., systematicity and productivity) in an effort to taxonomize different forms of linguistic generalization. While incredibly useful for linguistics, it is unclear how these concepts generically apply beyond natural language processing. By contrast, the formalisms from algebraic circuit complexity theory provide a set of parsimonious mathematical tools that can be applied to quantify symbolic generalization, and are agnostic to specific domains, such as linguistics. Moreover, algebraic circuit complexity theory provides an encompassing theoretical framework for the increasingly popular, yet nascent empirical evaluations in AI systems that use arithmetic tasks 18, 19, 30, 31, 21, 22, 23, 24, 32, 33, 34, 35. A large class of symbolic problems can be studied with algebraic expressions 36, 37. Algebraic circuit complexity theory formalizes algebraic expressions as circuit models (i.e., directed acyclic graphs; Fig. 1). This formalization is well-established in computational complexity theory, the branch of theoretical computer science concerned with quantifying the difficulty of computational problems and the resources required to solve them 38. Importantly, formalizing computational problems in terms of circuits is the leading approach to empirically quantify their complexity. Unlike other notions of complexity, such as Kolmogorov Complexity in algorithmic information theory (which is incomputable), notions developed in complexity theory for circuits are explicitly computable and determined by their shape and structure 39. Thus, the tools of algebraic circuit complexity can formalize notions of generalization by defining benchmarks in terms of their circuit properties. Furthermore, algebraic circuits are generic mathematical objects; they can be represented from a variety of mathematical perspectives (geometry, topology, etc.), providing useful interpretations in other domains. Algebraic circuits are therefore well-situated to addressing problems in symbolic machine learning – the problems are computable, large datasets can be straightforwardly generated from circuit specifications, and new models can be developed that address specific failure modes within this framework. In the ensuing sections, we provide a blueprint for the successful adoption of algebraic circuit complexity for machine learning problems; we introduce the core components of algebraic circuits, address how they can be leveraged to study symbolic generalization, and discuss several key open theoretical and empirical challenges. Figure 1: Examples of algebraic expressions represented as circuits. A) A two operand addition circuit (input gates are sampled from a field \mathbb{F}). B) A three operand addition circuit (input gates are sampled from the set of variables x_{i}\in X rather than \mathbb{F}). C, D) A mathematically equivalent pair of circuits, but represented as C) a factorized expression, and D) its a monomial expansion. Notably, despite their mathematical equivalence, the circuit representations are distinct. E) A polynomial of depth 4 and size 12."
https://arxiv.org/html/2411.05934v1,Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving,"In this research paper, we present an innovative approach tailored to solving mathematical problems in Bengali, developed for the DL Sprint 3.0 - BUET CSE Fest 2024 Competition. Our methodology harnesses the power of advanced deep learning models, notably the Qwen 2.5 series, with iterative improvements made through prompt engineering, model quantization, and Tool Integrated Reasoning (TIR) to handle complex calculations. Initially, we explored various model architectures, such as fine-tuned Mistral and quantized Qwen models, progressively refining them through translation techniques, RAG (Retrieval-Augmented Generation), and custom dataset curation. Through manual hyperparameter tuning, we optimized parameters like temperature and top-p to improve model adaptability and response accuracy. Additionally, the removal of RAG and careful parameter adjustments further contributed to our final model’s robustness. Our approach demonstrates the potential of advanced NLP techniques in effectively interpreting and solving Bengali mathematical problems.","The ability to understand and solve mathematical problems is a foundational skill for AI, essential for advancements across fields like science, engineering, and finance. However, while AI models have made strides in various languages, they still face significant challenges when tackling mathematical reasoning in low-resource languages, such as Bengali. This gap becomes particularly evident in tasks involving complex problem-solving and precise calculations. To address this, the DL Sprint 3.0 - BUET CSE Fest 2024 Competition [1] introduced the unique challenge of building an AI model capable of solving mathematical problems in Bengali, targeting issues akin to those in the Bengali Math Olympiad. This competition not only tests participants’ technical skills but also aims to push the boundaries of AI’s adaptability and performance in Bengali. Our work contributes to this pioneering effort, focusing on enhancing AI’s mathematical reasoning in Bengali through advanced NLP and deep learning techniques. We explore and iteratively refine state-of-the-art models, such as the Qwen series, alongside strategies like prompt engineering, Tool Integrated Reasoning (TIR), and manual hyperparameter tuning to achieve robust problem-solving capabilities. By contributing to this research, we aim to advance AI’s reach into Bengali language processing, ultimately creating models that can assist students, educators, and researchers in tackling complex problems with precision and reliability."
https://arxiv.org/html/2411.05831v1,"To Ask or Not to Ask? 
Detecting Absence of Information in Vision and Language Navigation","Recent research in Vision Language Navigation (VLN) has overlooked the development of agents’ inquisitive abilities, which allow them to ask clarifying questions when instructions are incomplete. This paper addresses how agents can recognize “when” they lack sufficient information, without focusing on “what” is missing, particularly in VLN tasks with vague instructions. Equipping agents with this ability enhances efficiency by reducing potential digressions and seeking timely assistance. The challenge in identifying such uncertain points is balancing between being overly cautious (high recall) and overly confident (high precision). We propose an attention-based instruction-vagueness estimation module that learns associations between instructions and the agent’s trajectory. By leveraging instruction-to-path alignment information during training, the module’s vagueness estimation performance improves by around 52% in terms of precision-recall balance. In our ablative experiments, we also demonstrate the effectiveness of incorporating this additional instruction-to-path attention network alongside the cross-modal attention networks within the navigator module. Our results show that the attention scores from the instruction-to-path attention network serve as better indicators for estimating vagueness.","Vision Language Navigation (VLN) is the task of enabling a robot to navigate an environment based on a given instruction. There has been substantial research in VLN in the recent past [25]. The majority of previous research has focused on training large neural models for VLN task using synthetic datasets like R2R [2] that contained step-by-step instructions and Reverie [16] that had high-level instructions, among others. All these models operate under the assumption that agents are designed to act independently without human intervention. Even when dealing with abstract or high-level instructions, agents are trained to explore and make decisions they deem best for progressing towards their goals. There is no provision for indicating uncertainty or indecision about the next course of action. Typically, designers impose an upper time limit in these approaches, prompting agents to cease operations after a predefined number of moves, regardless of goal achievement. When a robot transitions to a real-world environment, it often encounters instructions from various individuals, each with unique communication styles. Adapting to this diversity poses a challenge in generalizing across different instructing styles. While some individuals may prefer concise instructions, others might delve into details but present them in a disorganized manner. A second person unfamiliar with the instructor may find it challenging to follow the instructions easily. In such situations, humans typically seek clarification by asking questions like, “I’m now in front of the kitchen. Where should I go next?”. Implementing a similar ability is a desired feature in intelligent robots to tackle uncertainty due to vagueness in the input instruction. When a robotic agent is posed with an unclear instruction, it may get stuck at some point, or it may keep exploring, taking very long paths before reaching the goal, or it may never reach the goal. Rather than waiting for the robot to encounter failure, it is preferable if the robot can be proactive and seek help when it is approaching failure. We are interested in the task of Under-specified Vision Language Navigation (ULN) as outlined in [8]. ULN introduced a nuanced modification to the VLN task by incorporating a new dimension of information completeness. In this paper, we focus on the task of instruction-vagueness estimation within the context of ULN. Instruction-vagueness (IV) estimation assesses the uncertainty arising from vague input instructions in the decision-making process of a VLN agent. In essence, an IV estimation module addresses the VLN agent’s query: “Do I possess enough information to proceed with my next move or do I seek assistance?”. A key requirement for this module is to find the right balance (see Figure 1) between being overly cautious and frequently seeking help, which guarantees task completion (high recall), and being overly confident, potentially resulting in excessive exploration without finishing the task (high precision). The contributions of this paper are as follows: • We introduce an attention-based instruction-vagueness (IV) estimation module and integrate it into an existing VLN model. The module takes the instruction, the path followed thus far, and the proposed next move as its inputs and decides at each time step whether to follow the VLN model’s suggestion or to request assistance. • We introduce a pre-training task that helps identify important parts of the instructions needed for predicting actions. By incorporating this instruction to path alignment information into the Instruction-Vagueness (IV) module, we significantly improve its ability to detect points of uncertainty, enhancing precision-recall balance."
https://arxiv.org/html/2411.05828v1,"AI Multi-Agent Interoperability
 Extension for Managing Multiparty Conversations","This paper presents a novel extension to the existing Multi-Agent Interoperability specifications of the Open Voice Interoperability Initiative (originally also known as OVON from the Open Voice Network). This extension enables AI agents developed with different technologies to communicate using a universal, natural language-based API or NLP-based standard APIs. Focusing on the management of multiparty AI conversations, this work introduces new concepts such as the Convener Agent, Floor-Shared Conversational Space, Floor Manager, Multi-Conversant Support, and mechanisms for handling Interruptions and Uninvited Agents. Additionally, it explores the Convener’s role as a message relay and controller of participant interactions, enhancing both scalability and security. These advancements are crucial for ensuring smooth, efficient, and secure interactions in scenarios where multiple AI agents need to collaborate, debate, or contribute to a discussion. The paper elaborates on these concepts and provides practical examples, illustrating their implementation within the conversation envelope structure.","1.1 Previous work There is increasing recognition that many applications of conversational systems can be addressed more successfully if the expertise required to perform the application is not expected to reside in one agent, but is allocated among independent agents with their own capabilities. In this approach, these independent agents contribute their individual knowledge to address specific aspects of the problem and communicate to provide their input toward achieving the overall goal. It is clear that this perspective broadly resembles the traditional and very successful object-oriented programming paradigm and shares similar advantages, such as encapsulation, maintainability, and scalability. The integration of multiple agents has been addressed in many approaches, which vary in the degree of autonomy of each agent, the degree of architectural similarity expected of the agents, and whether or not the agents have to be known ahead of time when the system is developed. We believe that the most useful systems will be those that maximally encapsulate the agents’ functionality, limit dependencies on specific semantic formats, and which can be configured dynamically at runtime. In addition, multi-agent systems that are based on proprietary frameworks are, by definition, constrained to coordinating agents that are based on those frameworks, automatically ruling out the possibility for the thousands of existing legacy conversational systems to participate. We focus here on previous work aimed at coordinating full agents as opposed to work aimed at coordinating specific modality components, such as the W3C Multimodal Architecture [12] and the Galaxy Communicator Software Infrastructure [8]. 1.1.1 Early systems Knowledge Query Manipulation Language (KQML) [13] was an early system that focused on knowledge sharing and communication among intelligent agents. While it supported cooperation among intelligent agents in multi-agent systems, it relied on shared ontological assumptions among agents, which created a barrier to deployment. In addition, KQML did not specifically address conversational interactions and was more focused on sharing knowledge among agents. VoiceXML [11] was another early approach to collaboration among agents. It allowed conversations to be passed to other Voice-XML based agents through the <transfer> element. However, VoiceXML required agents to be based on VoiceXML, conversations could only be transferred to a single receiving agent, and no previous conversational context could be passed to the receiving agent. Systems like the Open Agent Architecture [6] used Inter-Agent Communication Languages to facilitate collaboration among independent agents. While this reduced dependencies on specific internal architectures, it required agents to interpret highly structured semantic representations rather than natural language, which constrained the flexibility and scalability of the system. 1.1.2 Recent systems The emergence of very capable LLMs has led to a dramatic increase in both research and deployed conversational systems. Along with this increase, the value of multi-agent systems is becoming more apparent, and a number of new multi-agent frameworks have recently become available. Some examples include the following: • OpenAI Swarm [20]: A framework designed to orchestrate multiple AI agents collaboratively to accomplish complex tasks. Released with Open Source MIT License, it is currently considered to be an experimental system that is not ready for deployment. In particular, Swarm is currently an experimental sample framework intended to explore ergonomic interfaces for multi-agent systems. It is not intended to be used in production, and therefore has no official support. • Microsoft Autogen: An Open-Source Programming Framework for Agentic AI. AutoGen is powered by collaborative research studies from Microsoft, Penn State University, and University of Washington and licensed under the Creative Commons Attribution 4.0 International [3]. • CrewAI: A platform that orchestrates multiple AI models and services to perform cohesive workflows. It emphasizes flexibility in integrating diverse AI technologies and managing complex task sequences. [7]. • Multi-Agent Orchestrator framework: a tool by Amazon AWS Labs for implementing sophisticated AI systems comprising multiple specialized agents. Its primary purpose is to intelligently route user queries to the most appropriate agents while maintaining contextual awareness throughout interactions [5]. The project is Open Source and provides optimal integration and performance combined with the Amazon AWS Bedrock fully managed service [4]. • Mixture of Experts (MoE) Paradigm [22]: While not an architecture for multi-agent systems per se, the Mixture of Experts paradigm represents another example of an architecture where a larger system relies on encapsulated individual components that supply complementary expertise with respect to an overall problem. We will see in the following sections that, in fact, these multi-agent frameworks could be compatible with the framework presented in this paper. By providing any of them with an OVON wrapper, they could become interoperable with other OVON-compatible systems. For instance, an Amazon Bedrock service with multiple internal agents could externally present itself as a black box, communicating with other multi-agent frameworks (e.g., Microsoft Autogen, CrewAI, or other platforms not listed here) by leveraging the OVON Universal API specifications. 1.2 The Initial OVON Framework In contrast to other approaches, the OVON (Open Voice Network) framework introduced in our previous work[2] sought to overcome some interoperability limitations by establishing a highly scalable and flexible method for AI agent interoperability111For the remainder of this document, the term ”agent” will be used to refer to an entity with the capacity to act, while ”agency” or “agentic” will denote the exercise or manifestation of this capacity, in accordance with the definition provided by Markus Schlosser[21].. Our framework supports a wide range of independent assistants, regardless of their underlying technologies, enabling them to collaborate through minimal communication standards. This loose coupling dramatically reduces the complexity of integrating new assistants into the ecosystem, thereby enhancing scalability. 1.3 Enhancements to the OVON Framework However, this initial work covers only conversations between one user and one assistant at a time. That is, if the user wants to get information from more than one assistant, they have to access multiple assistants in sequence. This most likely will have two less-than-optimal consequences. In the first place, any information from the conversation with the first assistant that is required by the second assistant will have to be explicitly transferred to the second assistant when the second assistant is invited to the conversation. The second and more significant drawback is that any higher-level conclusions resulting from the various conversations will have to be determined by the user. That is, since the assistants don’t know about the other tasks, they won’t be able to make suggestions that combine information gathered from other assistants with their own information. 1.4 Use Cases Let’s look at an example use case for managing multi-party conversations via a multi-agent AI. Suppose a user is planning a trip that involves booking a flight, a rental car, and a hotel, and also involves looking for interesting things to do in the destination city. This planning could involve conversations with four or more assistants at different travel services. The travel dates, which all of these assistants need, have to be passed to each assistant in turn to avoid making the user repeat them. In addition, if the assistants are talking together, the tourist information assistant could point out that there is a music festival that the user would enjoy, but attending it would require extending the trip by one day. If the tourist assistant is involved in the flight booking conversation, it could tell the user about the festival before the user books their flight. This could save the user a lot of time. Figures 1 and 2 contrast these two situations. In Figure 1, the user is planning a trip by accessing three assistants with different expertise in sequence. Figure 1: One assistant at a time Here the user has to keep track of the tasks that she has to coordinate to plan her trip–flight, hotel, and rental car, and discusses her plans separately with each assistant, repeating some of the trip details in each conversation. This is the way that most planning with multiple assistants currently works. In contrast, Figure 2 shows multiple conversational agents gathered in a shared conversational space, referred to as the Floor, brought together by a Convener, with conversational turn-taking managed by the Convener itself. These concepts will be discussed more fully in the following sections of the paper. Figure 2: Multiple assistants in the same conversation A similar use case is described in [9], where several agents are jointly assigned the task of allocating beds to hospital patients.The agents (nurse proxy, bed allocation specialist and patient database, among others) all have widely differing knowledge that makes it impractical to combine the agents into a single expert. Each agent has its own knowledge which it brings to the discussion of how to allocate a bed to a specific patient, arguing why or why not a particular bed is suitable for that patient. It would be very cumbersome if the user had to consult each agent in sequence to perform this task. Many other AI healthcare-specific applications could benefit from having conversational AI multi-agents coordinate with each other to enhance awareness of patient situations, including, for example, this risk detection model[17] for assisting vulnerable people. 1.5 Requirements and proposed extensions For these reasons, we propose to extend the earlier two-party conversational specifications[2] to handle requirements for conversations involving multiple assistants. Multi-party dialog systems have been discussed in the literature, for example [9][18] among others. [18] describes a multi-agent system with user-initiative, where several agents can be present but the agents don’t collaborate – they simply respond individually to user questions. [9] describes a system for collaborative problem-solving among agents, but it is restricted to one domain in that all of the agents are experts in different aspects of a larger problem. Our goal is to be able to support mixed-initiative applications with multiple agents that collaborate across domains. These are the requirements that we propose for support of multi-party conversations: 1. It must be possible to hold a conversation among more than two conversants. 2. Conversants must be able to come and go during a conversation. 3. It should be possible for a subset of conversants to be able to hold private conversations among themselves. 4. There should be no fixed limit on the number of conversants. 5. There should be a way to control possible unruly conversants through techniques like muting or ejecting. Requirement 1 is the key requirement for support of multi-party conversations. The other requirements support it. This paper extends the initial specifications by introducing key concepts that address the specific requirements of managing multiparty conversations within the context of AI-driven multiparty conferences. The new concepts introduced in this work—such as the Floor, and related Multi-Conversant Support, Convener Agent, and mechanisms for handling Interruptions and Uninvited Agents—are designed to ensure that AI agents can collaborate effectively in dynamic, multi-agent environments. These extensions not only enhance the framework’s ability to handle complex, multi-party interactions but also ensure that the system can scale to accommodate a growing number of agents and tasks. For instance, in scenarios where a human interacts with multiple AI assistants for various tasks—such as coordinating events, managing appointments, or retrieving information—the framework ensures effective communication and task delegation among the agents. This is achieved independently of each agent’s underlying technologies or models, showcasing the system’s ability to scale across different applications and user needs. Previous work[16] laid the foundation for AI agent interoperability, establishing the basic framework for seamless communication between independent conversational agents. However, the extensions presented in this paper are essential for overcoming the challenges associated with scalability and effective management in multiparty conversational settings. These enhancements introduce a versatile and adaptable platform that ensures AI-driven multiparty conferences can be conducted smoothly, with agents collaborating efficiently and effectively, regardless of their technological diversity. This approach not only addresses the current needs of evolving AI ecosystems but also provides a robust and future-proof solution capable of integrating new agents and capabilities as they emerge."
https://arxiv.org/html/2411.07240v1,"UTMath: Math Evaluation with Unit Test 
via Reasoning-to-Coding Thoughts","The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and adaptability. This paper introduces the UTMath Benchmark, which robustly evaluates the models through extensive unit tests. It consists of 1,053 problems across 9 mathematical domains, with over 68 test cases per problem. We propose an innovative evaluation framework inspired by unit testing in software development, focusing on both accuracy and reliability of results. Furthermore, we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to perform explicit reasoning before generating code, leading to generating more advanced solution and improved performance. Furthermore, we are releasing not only the UTMath benchmark but also the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning.","The pursuit of AGI necessitates strong mathematical reasoning capabilities, making the evaluation of such abilities a crucial area of research Zhou et al. (2024a). Recent advancements in LLMs have demonstrated remarkable proficiency in solving complex mathematical problems, achieving amazing performance on various datasets of Math Word Problems (MWPs), such as GSM8K Cobbe et al. (2021), MATH Hendrycks et al. (2021), TheoremQA Chen et al. (2023). Figure 1: Comparing UTMath with MATH. In the left, GPT-4o easily solved a question but failed on a similar question with a different number. The right figure illustrates our benchmark, where each problem includes multiple test cases, and a solution must pass all test cases to be strictly considered correct. We also propose a new prompting method RCoT, where the LLM first reasons through the problem and then generates code. However, classic benchmarks exhibit several limitations that impede the accurate and comprehensive assessment of these models’ capabilities Ahn et al. (2024). First, these benchmarks test models on narrowly defined problems with some specific numbers, which do not adequately assess adaptability to similar but varied scenarios as shown in Fig. 1. Second, their evaluation relies on predetermined rules or the method of LLM-as-a-Judge( Dubois et al. (2024); Zheng et al. (2023)) that usually failed with capricious responses of LLMs. For example, an accurate answer need to be extracted to exactly match the fianl answer in the dataset GSM8K, TheoremQA, and MATH dataset. Furthermore, these benchmarks focus more on final answers than on the underlying reasoning steps. While recent work has made great progress in developing new benchmarks for evaluating the mathematical reasoning of LLMs, many of these approaches still fall short of addressing the fundamental limitations of earlier datasets. For instance, benchmarks like GSM-HARD Gao et al. (2023), GSM-IC Shi et al. (2023), GSM-Plus Li et al. (2024a), MetaMath Yu et al. (2023) have extended the dataset of GSM8K or MATH with some perturbation such as substitution, reversing, distractor insertion. These efforts, while valuable, are characterized by limited coverage and high costs. In this context, our work seeks to bridge these gaps by proposing a solid and robust benchmark that accurately evaluates the mathematical capabilities of LLMs.Drawing inspiration from evaluation methods in software development, where solution correctness is determined through comprehensive unit testing, a solution is deemed correct if it passes all tests, as this demonstrates that its logic is sufficiently robust to generalize across a broad spectrum of scenarios. Similarly, we propose designing a comprehensive set of unit tests for mathematical problems to rigorously assess the reasoning processes of LLMs. If a solution passes all unit tests across a group of similar problems, it suggests that the reasoning behind the solution is more reliable and trustworthy. Specifically, we introduce the UTMath, a novel benchmark derived from the On-Line Encyclopedia of Integer Sequences (OEIS) OEIS Foundation Inc. (2024). The benchmark consists of 1,053 problems spanning 9 mathematical domains, such as Number Theory and Geometry. Each problem is accompanied by more than 68 test cases that provide a set of inputs and their corresponding outputs. In terms of evaluation methodology, our benchmark requires models to derive a general solution for a class of problems, typically represented in the form of code. Compared to solving a problem defined by specific numbers, developing such a general solution is substantially more challenging, requiring higher levels of intelligence and reasoning ability. However, we observed that when the model is required to perform reasoning and coding in a single response, as in “Program of Thoughts (PoT)” Chen et al. (2022), it consistently tends to produce simpler, more straightforward solutions. We surmise that this tendency may be influenced by the distribution of coding data. To address this, we introduced the “Reasoning-to-Coding of Thoughts (RCoT)” approach, which requires the LLM to perform mathematical reasoning in the first turn without any coding instruction, then writing code based on that reasoning. Compared to PoT, RCoT shifts the code distribution towards mathematics in the first turn, prompting more reasoning steps. Figure 2: RCoT significantly improves the efficiency and effectiveness of the solution. It indicates that our RCoT proves to be more effective, suggesting that it encourages the model to reason critically and find more efficient solutions. We conducted a comprehensive study with 8 LLMs. Some of our key findings are summarized as follows: (1) The best model, GPT-4o, only solves 26.93% problem in our benchmark, demonstrate the difficulty of our benchmarks. (2) Modern LLMs perform poorly in Graph Theory, Group Theory, Geometry and Topology (Fig. 4). (2) With RCoT, 7 of 8 evaluated LLMs generated more efficient solutions, with most models achieving higher scores (Fig. 2). (3) RCoT can significantly improve the pass@k performance of LLMs (§ 5.4). (4) The quality of reasoning significantly impacts the accuracy and efficiency of the model’s final solution (§ 5.5). More interesting findings can be found in § 5. We hope our findings contribute to a deeper understanding of current reasoning ability of LLMs and the further development of models."
https://arxiv.org/html/2411.07232v2,Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models,"Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models’ attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed ”Additing Affordance Benchmark” for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics. Our code and data will be available at: https://research.nvidia.com/labs/par/addit/","Adding objects to images based on textual instructions is a challenging task in image editing, with numerous applications in computer graphics, content creation and synthetic data generation. A creator may want to use text-to-image models to iteratively build a complex visual scene, while autonomous driving researchers may wish to draw pedestrians in new scenarios for training their car-perception system. Despite considerable recent research efforts on text-based editing, this particular task remains a challenge . When adding objects, one needs to preserve the appearance and structure of the original scene as closely as possible, while inserting the novel objects in a way that appears natural. To do so, one must first understand affordance—the deep semantic knowledge of how people and objects interact, in order to position an object in a reasonable location. For brevity, we call this task Image Additing. Several studies (Hertz et al., 2022; Meng et al., 2022) tried addressing this task by leveraging modern text-to-image diffusion models. This is a natural choice since these models embody substantial knowledge about arrangements of objects in scenes and support open-world conditioning on text. While these methods perform well for various editing tasks, their success rate for adding objects is disappointingly low, failing to align with both the source image and the text prompt. In response, another set of methods took a more direct learning approach (Brooks et al., 2023; Zhang et al., 2023; Canberk et al., 2024). They trained deep models on large image editing datasets, pairing images with and without an object to add. However, these often struggle with generalization beyond their training data, falling short of the general nature of the original diffusion model itself. This typically manifests as a failure to insert the new object, the creation of visual artifacts, or more commonly – failing to insert the object in the correct place, i.e. struggling with affordances. Indeed, we remain far from achieving open-world object insertions from text instructions. Here we describe an open-world, training-free method that can successfully leverage the knowledge stored in text-to-image foundation models, to naturally add objects into images. As a guiding principle, we propose that addressing the affordance challenge requires methods to carefully balance between the context of the existing scene and the instructions provided in the prompt. We achieve this by: first, extending the multi-modal attention mechanism (Esser et al., 2024) of recent T2I diffusion models to also consider tokens from a source image; and second, controlling the influence of each multi-modal attention component: the source image, the target image and the text prompt. A main contribution of this paper is a mechanism to balance these three sources of attention during generation. We also apply a structure transfer step and introduce a novel subject-guided latent blending mechanism to preserve the fine details of the source image while enabling necessary adjustments, such as shadows or reflections. Our full pipeline is shown at fig. 2. We name our method Add-it. Image Additing methods typically face three main failure modes: neglect, appearance, and affordance. While current CLIP-based evaluation protocols can partially assess neglect and appearance, there is a lack of reliable methods for evaluating affordance. To address this gap, we introduce the “Additing Affordance Benchmark,” where we manually annotate suitable areas for object insertion in images and propose a new protocol specifically designed to evaluate the plausibility of object placement. Additionally, we introduce a metric to capture object neglect. Add-it outperforms all baselines, improving affordance from 47% to 83%. We also evaluate our method on an existing benchmark (Sheynin et al., 2023) with real images, as well as our newly proposed Additing Benchmark for generated images. Add-it consistently surpasses previous methods, as reflected by CLIP-based metrics, our object inclusion metric, and human preference, where our method is favored in over 80% of cases, even against methods specifically trained for this task. Our contributions are as follows: (i) We propose a training-free method that achieves state-of-the-art results on the task of object insertion, significantly outperforming previous methods, including supervised ones trained for this task. (ii) We analyze the components of attention in a modern diffusion model and introduce a novel mechanism to control their contribution, along with novel Subject Guided Latent Blending and a noise structure transfer. (iii) We introduce an affordance benchmark and a new evaluation protocol to assess the plausibility of object insertion, addressing a critical gap in current Image Additing evaluation methods."
https://arxiv.org/html/2411.07223v1,Grounding Video Models to Actions through Goal Conditioned Exploration,"Large video models, pretrained on massive amounts of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks. However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video. To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data are available. In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment – using generated video states as visual goals for exploration. We propose a framework that uses trajectory level action generation in combination with video guidance to enable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks. We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.","1 Introduction Large video models (Brooks et al., 2024; Girdhar et al., 2023; Ho et al., 2022) trained on a massive amount of Internet video data capture rich information about the visual dynamics and semantics of the world for physical decision-making. Such models are able to provide information on how to accomplish tasks, allowing them to parameterize policies for solving many tasks (Du et al., 2024). They are further able to serve as visual simulators of the world, allowing simulation of the visual state after a sequence of actions (Brooks et al., 2024; Yang et al., 2024c), and enabling visual planning to solve long-horizon tasks (Du et al., 2023). However, directly applying video models zero-shot for physical decision-making is challenging due to embodiment grounding. While generated videos provide a rich set of visual goals for solving new tasks, they do not explicitly provide actionable information on how to reach each visual goal. To ground video models to actions, existing work has relied on training an inverse dynamics model or goal-conditioned policy on a set of demonstrations from the environment (Black et al., 2023; Du et al., 2024; 2023). Such an approach first requires demonstrations to be gathered in the target environment and embodiment of interest, which demands human labor or specific engineering (e.g. teleoperation or scripted policy). In addition, the learned policies may not generalize well to areas in an environment that are out-of-distribution of the training data. Recently, Ko et al. (2023) proposes an approach to directly regress actions from video models, without requiring any action annotations. In Ko et al. (2023), optical flow between synthesized video frames is computed and used, in combination with a depth map of the first image, to compute a rigid transform of objects in the environment. Robot actions are then inferred by solving for actions that can apply the specified rigid transform on an object. While such an approach is effective on a set of evaluated environments, it is limited in action resolution as the inferred object transforms can be imprecise due to both inaccurate optical flow and depth, leading to a relatively low success rate in evaluated environments (Ko et al., 2023). In addition, it is difficult to apply this approach to many robotic manipulation settings such as deformable object manipulation, where there are no explicit object transforms to compute. Figure 1: Grounding Video Models to Actions. Our approach learns to ground a large pretrained video model into continuous actions through goal-directed exploration in the environment. Given a synthesized video, a goal-conditioned policy attempts to reach each visual goal in the video, with data in the resulting real-world execution saved in a replay buffer to train the goal-conditioned policy. We propose an alternative manner to directly ground a video model to actions without using annotated demonstrations. In our approach, we learn a goal-conditioned policy, which predicts the actions to reach each synthesized frame in a video. We learn the policy in an online manner, where given a specified task, we use each intermediate synthesized frame as a visual goal for a goal-conditioned policy from which we obtain a sequence of actions to execute in an environment. We then use the image observations obtained from execution in the environment as ground-truth data to train our goal-conditioned policy. We illustrate our approach in Figure 1. In practice, directly using synthesized images as goals for exploration often leads to insufficient exploration. Agents often get stuck in particular parts of an environment, preventing the construction of a robust goal-conditioned policy. To further improve exploration, we propose to generate chunks of actions to execute in an environment given a single visual state. By synthesizing and executing a chunk of actions we can explore the environment in a more directed manner, enabling us to achieve a more diverse set of states. We further intermix goal-conditioned exploration with random exploration to further improve exploration. Overall, our approach has three contributions: (1) We propose goal-conditioned exploration as an approach to ground video models to continuous actions. (2) We propose a set of methods for effective exploration in the environment, using a combination of chunked action prediction for temporally coherent exploration along with periodic randomized actions for robust state coverage. (3) We illustrate the efficacy of our approach on a set of simulated manipulation and navigation environments."
https://arxiv.org/html/2411.07218v1,TreeCoders: Trees of Transformers,"In this paper, we introduce TreeCoders, a novel family of transformer trees. We moved away from traditional linear transformers to complete k-ary trees. Transformer blocks serve as nodes and generic classifiers learn to select the best child and route the sequence of tokens to a specific leaf. The selectors, moved outside the transformer blocks, allow for the use of a variety of architecture without further modifications. Furthermore, our proposed architecture supports sparse node activation due to the logarithmic complexity of a tree search. We validate our idea by testing a series of decoder-only tree transformers achieving competitive results across a diverse range of language datasets. Our study demonstrates that the proposed tree transformer model outperforms a size-equivalent linear transformer model 76% of the time over a wide range of tree architectures. Furthermore, our proposed model naturally lends itself to distributed implementation.","Transformers have proved their effectiveness ever since their introduction [Vaswani17]. One development has been a rapid increase in the size of the models and the amount of data they consume, leading to models such as GPT-3 [Gpt3] towering at 175 billion parameters. Those large and dense models impose heavy hardware requirements and longer inference time. Better management of those resources would be greatly beneficial. One possible solution to reduce inference time has been to promote sparsity in networks through the Mixture-of-Expert approach [Jacobs1991AdaptiveMO][Jordan1993HierarchicalMO]. A tree is sparse by design, as shown in Figure 1 and, therefore, an attractive candidate for scaling future models. Large language models are increasingly expected to perform on a variety of tasks and datasets. Textual data varies greatly in terms of structure, intent, content, formality, language, and organization. Nevertheless, the underlying language apparatus of these contents can be hierarchically exploited based on their common features. This is our second motivation to investigate moving from a linear structure to a tree-like one. The final motivation is the inherently explainable nature of decision trees. Bringing about explainability to large language models would be a formidable step forward in itself but should also help with fine-tuning, safety, and alignment. ((a)) A sequence going through one path in the tree, from root decoder A to leaf decoder K.ABDHIEJKCFKLGMN ((b)) Total number of nodes in the tree. Height Branching Factor 1 2 3 4 1 2 3 4 5 2 3 7 13 21 3 4 15 40 85 4 5 31 121 341 5 6 63 364 1365 ((c)) Percentage of parameters used by a token. Height Branching Factor 1 2 3 4 1 100.0 66.7 50.0 40.0 2 100.0 42.9 23.1 14.3 3 100.0 26.7 10.0 4.7 4 100.0 16.1 4.1 1.5 5 100.0 9.5 1.6 0.4 Figure 1: A tree structure allows for a sparse activation of the network. The sparsity will also grow with the tree"
https://arxiv.org/html/2411.07199v1,OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision,"Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present Omni-Edit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) Omni-Edit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that Omni-Edit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/","Image editing, particularly when following user instructions to apply semantic transformations to real-world photos, has seen significant advancements. Recently, text-guided image editing (Brooks et al., 2023) has gained prominence over traditional methods such as mask-based or region-based editing (Meng et al., 2022). With the rise of diffusion models (Rombach et al., 2022; Podell et al., 2024; Chen et al., 2024a; Sauer et al., 2024), numerous diffusion-based image editing techniques have emerged. Generally, they can be roughly divided into two types: (1) Inversion-based methods (Parmar et al., 2023; Kawar et al., 2023; Gal et al., 2023; Xu et al., 2023; Tumanyan et al., 2023; Tsaban & Passos, 2023) propose to perform zero-shot image editing by inverting the diffusion process and manipulating the attention map in the intermediate diffusion steps to achieve desired editing goal. (2) End-to-end methods (Brooks et al., 2023; Zhang et al., 2024a; Sheynin et al., 2024; Zhao et al., 2024; Fu et al., 2024) propose to fine-tune an existing diffusion model on large-scale image editing pairs to learn the editing operation in an end-to-end fashion. End-to-end methods have generally achieved better performance than inversion-based methods and gained higher popularity. Table 1: Comparison of Omni-Edit with all the existing end-to-end image editing models. The scores are based on a preliminary studies on around 50 prompts. Property InstructP2P MagicBrush UltraEdit MGIE HQEdit CosXL Omni-Edit Training Dataset Properties Real Image? ✗ ✓ ✓ ✓ ✗ ✗ ✓ Any Res? ✗ ✗ ✗ ✗ ✗ ✗ ✓ High Res? ✗ ✗ ✗ ✗ ✓ ✗ ✓ Fine-grained Image Editing Skills Obj-Swap \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStarHalfO\faStarO \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO Obj-Add \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO Obj-Remove \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO Attribute \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO Back-Swap \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStarHalfO\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarHalfO Environment \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarHalfO Style \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO \faStar\faStar\faStarHalfO Despite their effectiveness, end-to-end methods face a significant limitation: the scarcity of human-annotated image editing pairs. As a result, all current end-to-end approaches depend on synthetic training data. For instance, existing datasets are synthesized using techniques such as Prompt2Prompt (Hertz et al., 2023) or mask-based editing models like SD-Inpaint (Rombach et al., 2022), and DALLE-2/3 (Ramesh et al., 2022; Betker et al., 2023). However, these synthetic data generation pipelines exhibit significant biases, resulting in the following limitations: Limited Editing Capabilities: The synthetic data is heavily influenced by the underlying generation models. For example, Prompt2Prompt struggles with localized edits, such as adding, removing, or swapping objects, while SD-Inpaint and DALLE-2 are ineffective at global edits, such as style or background changes. As a result, models trained on such data inherit these limitations. Poor Data Quality Control: Most approaches use simplified filtering mechanisms like CLIP-score (Radford et al., 2021) or DINO-score (Caron et al., 2021) to automatically select training samples. However, recent studies (Ku et al., 2024) show that these metrics exhibit poor correlation with actual data quality, leading to suboptimal training data that negatively impacts the model. Lack of Support for Varying Resolutions: All current models are trained on square image editing pairs, making their generalization to non-square images poor. In our preliminary studies, we curate a few prompts for seven different desired tasks to observe their success rate across the board. We show our findings in Table 1. This show that these models are truly biased in their skills caused by the underlying synthesis pipeline. In this paper, we introduce Omni-Edit, a novel model designed to address these challenges through four key innovations: 1. Specialist-to-Generalist Supervision: We propose learning a generalist editing model, Omni-Edit, by leveraging supervision from multiple specialist models. Unlike previous approaches that rely on a single expert, we conduct an extensive survey and construct (or train) seven experts, each specializing in a different editing task. These specialists provide supervisory signals to Omni-Edit. 2. Importance Sampling: To ensure high-quality training data, we employ large multimodal models to assign quality scores to synthesized samples. Given the computational cost of GPT-4o (Achiam et al., 2023), we first distill its scoring ability into InternVL2 (Chen et al., 2024b) through medium-sized samples. Then we use the InternVL2 model for large-scale scoring. 3. EditNet Architecture: We introduce EditNet, a novel diffusion-transformer-based architecture (Peebles & Xie, 2022) that facilitates interaction between the control branch and the original branch via intermediate representations. This architecture enhances Omni-Edit ’s ability to comprehend diverse editing tasks. 4. Support for Any Aspect Ratio: During training, we incorporate a mix of images with varying aspect ratios as well as high resolution, ensuring that Omni-Edit can handle images of any aspect ratio with any degradation in the output quality. We curate an image editing benchmark Omni-Edit-Bench, which contains diverse images of different resolutions and diverse prompts that cover all the listed editing skills. We perform comprehensive automatic and human evaluation to show the significant boost of Omni-Edit over the existing baseline models like CosXL-Edit (Boesel & Rombach, 2024), UltraEdit (Zhao et al., 2024), etc. On automatic metrics like VIEScore (Ku et al., 2024), we can outperform all the existing approaches by a reasonable margin in terms of both perceptual quality and semantic consistency. We also performed a human evaluation and observed an overall improvement 20% over the best baseline editing model CosXL-Edit (Boesel & Rombach, 2024)."
https://arxiv.org/html/2411.07191v1,The Super Weight in Large Language Models,"Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM’s ability to generate text – increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs111Code is available in https://github.com/mengxiayu/LLMSuperWeight..","Large Language Models (LLMs) have been growing in size and capability at an unprecedented rate, enabling them to capture increasingly complex linguistic patterns across a wide range of tasks. However, with this increase in model scale, new and unexpected behaviors have emerged. Dettmers et al. (2022) discovered that once LLMs reach a certain scale, a small set of hidden state features contains outliers of exceptionally large magnitude. These outliers account for a small percentage of all activations but are crucial for preserving the compressed model’s quality (Dettmers et al., 2022; Xiao et al., 2023; Wei et al., 2023; Shao et al., 2024). However, not all outliers are equally important. In this paper, we study a tiny yet important set of outliers in LLMs, termed super weights. In Llama-7B, pruning the super weight, a single scalar, completely destroys the model’s ability to generate text; the average accuracy of zero-shot downstream tasks effectively plummets to zero. Conversely, pruning the other top 7,000 outliers, including outliers that are larger than the super weight, affects no more than a few percentage points. Intriguingly, super weights behave similarly across model families and sizes. For one, the super weight is always found in the mlp.down_proj weight, always in an early layer. We also find that the super weight amplifies input activation inliers to ultimately produce the exceptionally large magnitude activation observed by Sun et al. (2024) – we term this the super activation. This super activation persists throughout the model at exactly the same magnitude and position regardless of the prompt, and we find this is uniquely enabled by skip connections. Finally, super weights suppress stopword likelihood. Taken together, pruning the super weight destroys quality by dampening the super activation and shifting almost all logit probability mass to stopwords. Both super weights and super activations, which we collectively refer to as super outliers, are critical to model quality. Fortunately, there are no more than a handful of scalar super outliers per tensor; in light of this, we revisit round-to-nearest quantization, equipped only with the ability to hold out and restore super outliers. This yields a data-free, hardware-friendly method. For activation quantization, we find this technique competitive with SmoothQuant; for weight quantization, we can scale round-to-nearest to much larger block sizes with higher quality. Our contributions are summarized as follows. 1. Super Weights: We discover a tiny subset of outliers in LLMs, at most six scalars, that are disproportionately important; pruning these super weights destroys model quality. 2. Identifying Super Weights: We present a data-free way to identify super weights using only a single forward pass and provide an index of super weights for existing, open LLMs. 3. Super Activations: We analyze how super weights influence inference and relate them to the activation outliers observed in prior work. 4. Compression: By preserving super outliers, we show that round-to-nearest quantization increases effectiveness noticeably; preserving super outliers improves compression quality. Figure 1: Super Weight Phenemenon. We discover that pruning a single, special scalar, which we call the super weight, can completely destroy a Large Language Model’s ability to generate text. On the left, the original Llama-7B, which contains a super weight, produces a reasonable completion. On the right, after pruning the super weight, Llama-7B generates complete gibberish. As we show below, this qualitative observation has quantitative impact too: zero-shot accuracy drops to guessing and perplexity increases by orders of magnitude."
https://arxiv.org/html/2411.07186v1,"NatureLM-audio: an Audio-Language 
Foundation Model for Bioacoustics","Large language models (LLMs) prompted with text and audio represent the state of the art in various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, these capabilities have yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behavior—tasks that are crucial for conservation, biodiversity monitoring, and the study of animal behavior. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our carefully curated training dataset comprises text-audio pairs spanning a diverse range of bioacoustics, speech, and music data, designed to address the challenges posed by limited annotated datasets in the field. We demonstrate successful transfer of learned representations from music and speech to bioacoustics, and our model shows promising generalization to unseen taxa and tasks. Importantly, we test NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of the art (SotA) on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we also open-source the code for generating training and benchmark data, as well as for training the model111Demo page: https://earthspecies.github.io/naturelm-audio-demo/ The code will be open-sourced and available shortly..","Bioacoustics, the study of sound production and reception in animals, aims to understand animal behavior (Fischer et al., 2013), monitor biodiversity (Stowell, 2022), and model the mechanisms of sound production and reception used in animal communication (Bradbury & Vehrencamp, 1998). It plays a vital role in conservation and ecological research, as animal vocalizations provide critical insights into ecosystem health, species interactions, and population dynamics. By enabling the detection of endangered species and tracking migration patterns, bioacoustic research directly contributes to biodiversity monitoring and conservation efforts (Rutz et al., 2023; Stevens et al., 2024). In recent years, machine learning has taken on an increasingly pivotal role in bioacoustic research. Beyond its applications in large-scale ecological monitoring, machine learning has also opened up new frontiers in the study of animal communication, enabling discoveries like the ability of marmosets (Oren et al., 2024), dolphins (King & Janik, 2013), and elephants (Pardo et al., 2024) to use specialized vocalizations to label their conspecifics. Yet, because of obvious data collection and annotation difficulties, these studies often rely on strongly labeled small datasets (Stowell, 2022) and thus require careful statistical analysis to measure the significance of results and avoid over-fitting. At the same time, large volumes of unannotated bioacoustics data are recorded daily, particularly through passive acoustic monitoring (PAM, Dufourq et al. (2021)) or citizen science platforms e.g. Xeno-canto (Vellinga & Planqué, 2015)). There is thus a growing need for machine learning tools capable of performing tasks such as detection, classification, and annotation on these data at scale. The recent successes of large scale artificial intelligence models in various domains (e.g. natural language processing, vision, games) also point to the possibility of leveraging these huge volumes of raw data to learn accurate and generalizable representations of bioacoustics signals (Ghani et al., 2023; Boudiaf et al., 2023). Existing bioacoustics machine learning models are typically designed for specific species or tasks (Dufourq et al., 2021; Kahl et al., 2021; Cauzinille et al., 2024), showing limited generalizability beyond their predefined scope. Many traditional studies rely on small datasets focused on a few species and individuals, validating results through statistical measures despite over-fitting risks. Newer models such as BirdNET (Kahl et al., 2021) and Perch (Ghani et al., 2023) perform well in specific tasks such as bird classification but still generalize poorly outside avian species. Recently, foundation models such as AVES (Hagiwara, 2023) and BioLingual (Robinson et al., 2024) have exhibited notable results using self-supervision, though they remain constrained by their training paradigms (discriminative and contrastive, respectively), which restrict the range of tasks they can address. In recent years, foundation models, which learn patterns in large amounts of data via self-supervision, have shown promising performance across a wide range of tasks (Bommasani et al., 2021). While transformer-based large language models (LLMs) are currently the most prominent examples, other architectures, such as diffusion models (Kingma et al., 2021), are also emerging as foundation models in some domains. These models’ ability to handle unseen tasks, perform in-context learning, and respond to prompts positions them as a compelling alternative to traditional machine learning methods, which often rely on laboriously annotated data, expensive computational resources, and often-lacking machine learning expertise. While multimodal large language models (LLMs), particularly vision-language models (VLMs), have been explored for biodiversity and conservation research (Miao et al., 2024), there is relatively little effort dedicated to building and investigating large audio-language models (LALMs) for bioacoustics. LALMs have shown significant promise in processing human speech (Rubenstein et al., 2023; Wang et al., 2024; Wu et al., 2023a; Zhang et al., 2024), music (Gardner et al., 2023; Agostinelli et al., 2023), and general audio tasks (Tang et al., 2024; Chu et al., 2024; Gong et al., 2023), and they hold the potential to bring transformative advancements to bioacoustics as well. In this paper, we present NatureLM-audio, an audio-language foundation model specifically designed for bioacoustics tasks, including classification, detection, and captioning. To the best of our knowledge, NatureLM-audio is the first model of its kind. Inspired by the cross-taxa transfer observed in previous research, such as between human and gibbons (Cauzinille et al., 2024) and birds and whales (Ghani et al., 2023), we incorporate speech and music tasks into the training process. We show that representations learned from these domains successfully transfer to animal vocalizations, demonstrating generalization across species. Importantly, we augment an already existing animal sounds classification and detection benchmark, BEANS (Hagiwara et al., 2023), with additional tasks such as call-type prediction, lifestage classification, captioning, and individual counting. With these, we test cross-domain learning capabilities of the model and zero-shot transfer to unseen taxa and tasks. We name this new benchmark BEANS-Zero. Our contributions are thus as follows: • Model: We introduce NatureLM-audio, to the best of our knowledge, the first audio-language foundation model for bioacoustics with carefully curated training datasets comprising of animal vocalization, human speech, and music. • Domain transfer We show that the model transfers beyond the species originally trained on and demonstrate its zero-shot capability on unseen taxa and species. • Task transfer We test our model on a novel benchmark (BEANS-Zero) that goes beyond species classification and even includes a completely unseen task (individual counting).For the first time, we show positive transfer from speech and music data to bioacoustics tasks."
https://arxiv.org/html/2411.07185v1,Gradual Fine-Tuning with Graph Routing for Multi-Source Unsupervised Domain Adaptation,"Multi-source unsupervised domain adaptation aims to leverage labeled data from multiple source domains for training a machine learning model to generalize well on a target domain without labels. Source domain selection plays a crucial role in determining the model’s performance. It relies on the similarities amongst source and target domains. Nonetheless, existing work for source domain selection often involves heavyweight computational procedures, especially when dealing with numerous source domains and the need to identify the best ones from them. In this paper, we introduce a framework for gradual fine tuning (Gft) of machine learning models on multiple source domains. We represent multiple source domains as an undirected weighted graph. We then give a new generalization error bound for Gft along any path within the graph, which is used to determine the optimal path corresponding to the optimal training order. With this formulation, we introduce three lightweight graph-routing strategies which tend to minimize the error bound. Our best strategy improves 2.3\% of accuracy over the state-of-the-art on Natural Language Inference (NLI) task and achieves competitive performance on Sentiment Analysis (SA) task, especially a 3.9\% improvement on a more diverse subset of data we use for SA.","Domain adaptation has been shown to succeed in training deep neural networks with limited data, particularly when the acquisition of labeled data can be costly in real-world applications. In practice, it is often favorable to train a model with data from related domains. Accordingly, the effectiveness of domain adaptation highly depends on the quality and similarity of the source domains’ datasets. In cases where few or no labeled samples are available in the domain we target, developing a methodology to train a model without direct supervision on target domain becomes necessary. This approach is known as unsupervised domain adaptation. Extensive research has been conducted on theoretical analysis and empirical algorithms that minimize the generalization error (risk) of the trained model on target domain. Mansour et al. (2009) has demonstrated that the generalization error depends on both generalization error on the source domain and the discrepancy between source and target domains. In the pursuit of minimizing the discrepancy, certain approaches (Ruder & Plank, 2017; Liu et al., 2019) have been proposed to select source domains that are close to the target domain. Nevertheless, this selection process can be costly as it introduces an additional step prior to the model training. Moreover, the source domain selection tends to discard distant domains, while we assert that the distant domains can actually provide valuable training benefits for the target domain. For this reason, we propose a lightweight and efficient gradual fine-tuning (Gft) framework that can take advantage of all available source domains. By sequentially fine-tuning a model on multiple data sources, we aim to address the limitation of existing methods and unlock the potential benefits from distant domains during the training process for the target domain. The underlying intuition of our approach is to gradually guide a model to its optimal solution through sequentially fine-tuning the model on different source data whose distribution progressively aligns with the target domain. With this formulation, the model learns from data that spans a wide range of distributions, and ultimately leading it towards a better performance on the target domain. The gradual alignment of source data distributions with the target domain is crucial in enhancing adaptability and performance. The contribution of our work is two-fold. First, motivated by Wang et al. (2022), we construct theoretical analysis and give the generalization error of the proposed Gft algorithms. Based on our theory, we introduce graph routing strategies for determining the optimal paths through an undirected graph constructed by source domains and Wasserstein distances thereof. Second, we present empirical results for the Gft algorithms on two Natural Language Processing (NLP) tasks that are commonly used to demonstrate the effectiveness of a domain adaptation algorithms. We show that the performance of Gft algorithms does not highly depends on the discrepancy of sources and target domains as long as there exists a path along domains such that the distance between consecutive domains is small and the data magnitude on the path is large. Both empirical and theoretical results indicate that through Gft framework the model achieves better performance than baselines."
https://arxiv.org/html/2411.07176v2,More Expressive Attention with Negative Weights,"We propose a novel attention mechanism, named Cog Attention, that enables attention weights to be negative for enhanced expressiveness, which stems from two key factors: (1) Cog Attention can shift the token deletion and copying function from a static OV matrix to dynamic QK inner products, with the OV matrix now focusing more on refinement or modification. The attention head can simultaneously delete, copy, or retain tokens by assigning them negative, positive, or minimal attention weights, respectively. As a result, a single attention head becomes more flexible and expressive. (2) Cog Attention improves the model’s robustness against representational collapse, which can occur when earlier tokens are over-squashed into later positions, leading to homogeneous representations. Negative weights reduce effective information paths from earlier to later tokens, helping to mitigate this issue. We develop Transformer-like models which use Cog Attention as attention modules, including decoder-only models for language modeling and U-ViT diffusion models for image generation. Experiments show that models using Cog Attention exhibit superior performance compared to those employing traditional softmax attention modules. Our approach suggests a promising research direction for rethinking and breaking the entrenched constraints of traditional softmax attention, such as the requirement for non-negative weights. Our code is available at https://github.com/trestad/CogAttn.","The Transformer architecture (Vaswani et al., 2017) has achieved success across numerous applications, such as language modeling (Brown et al., 2020) and image generation (Dosovitskiy et al., 2021). A crucial factor contributing to its success is the softmax attention mechanism (Bahdanau et al., 2015). Figure 1: In the Indirect Object Identification (IOI) task (Wang et al., 2023), a language model should identify the indirect object (IO) from a context that includes both IO and a subject (S). Subfigures (a) and (b) show how Cog Attention and softmax attention process a single sequence through a process of elimination, but in different ways: a softmax attention head with a deletion-function OV matrix eliminates all attended tokens. While the IO token receives less attention than S, it is also deleted. In contrast, Cog Attention shifts functions like deletion or copying from a static OV matrix to dynamic query-key inner products, allowing the head to assign negative weights to S tokens for elimination while preserving IOs. Subfigures (c) and (d) show the attention of these two heads on names across the entire dataset vs the direction of their output embeddings. Cog Attention exhibits a weaker correlation between the attention weights assigned to IOs and the extent of their deletion. For further details, please see Section 3. Softmax ensures non-negative attention weights, but we argue that it limits the expressiveness of the attention mechanism. Figure 1 shows one possible way that negative attention weights enhance the model’s expressiveness using the same number of parameters: in a softmax attention head, the QK matrix (Elhage et al., 2021) determines the relevant tokens for attention, while the OV matrix governs the processing of these attended tokens (e.g., deletion or copying). Suppose a softmax attention head has an OV matrix capable of deleting tokens that the QK matrix attends to; since attention weights must be non-positive, a useful token is also somewhat deleted. By allowing negative attention weights, however, deletion or copying can be expressed through the sign of the attention weight and accomplished during the weighted summation of value vectors. This functional shift also allows the OV matrix to focus more on higher-level tasks, such as refinement or modification, rather than solely handling contextual deletions or copies. Consequently, negative attention weights eliminate irrelevant tokens while preserving useful ones, mitigating the risk of “friendly fire” on useful tokens. Despite the potential benefits of incorporating negative weights in attention mechanisms, this question has been rarely explored. Apart from the common belief that attention weights should naturally be non-negative, introducing negative weights can lead to challenges such as training instability, numerical overflow, and difficulties in attention normalization due to issues like division by zero, etc. In this paper, we propose a novel attention mechanism named Cog Attention111The name is derived from the attention pattern, which resembles cogs. See Figure 10. that enables negative weights. Cog Attention exhibits superior properties from a mechanistic interpretability perspective and surpasses softmax attention in various applications without introducing any additional parameters. In Section 3, we provide mechanistic evidence for the expressiveness of Cog Attention: (1) We identify attention heads that share the same working mechanism as exemplified above (Figures 1(b) and (d)), which shift the contextual process from the static OV matrix to dynamic QK inner products, with the OV matrix focusing more on refinement or modification. Irrelevant tokens are assigned negative weights for elimination, while other tokens are well preserved at the same time. This demonstrates Cog Attention’s enhanced flexibility and expressiveness compared to softmax attention. (2) We demonstrate that models using Cog Attention exhibit improved robustness against representational collapse (Liu et al., 2020; Xie et al., 2023). Representational collapse refers to the phenomenon where representations become homogeneous in the later positions of a sequence within deep Transformer models. Barbero et al. (2024) contended that this issue arises because earlier tokens are “over-squashed” into later positions as the layer goes deeper. The negative weights in Cog Attention reduce the effective information paths from earlier tokens to later positions, thereby alleviating over-squashing and, consequently, mitigating representational collapse. In Section 4, we develop Transformer-like models that use Cog Attention as attention modules and evaluate their performance across various tasks. Specifically, we train decoder-only language models for language modeling, and U-ViT diffusion models (Bao et al., 2023) for both unconditional and text-conditioned image generation. Our results show that across a wide range of tasks, these models equipped with Cog Attention achieve improved performance over the vanilla Transformer architecture using softmax attention."
https://arxiv.org/html/2411.07171v1,"Anytime Sequential Halving in
Monte-Carlo Tree Search","Monte-Carlo Tree Search (MCTS) typically uses multi-armed bandit (MAB) strategies designed to minimize cumulative regret, such as UCB1, as its selection strategy. However, in the root node of the search tree, it is more sensible to minimize simple regret. Previous work has proposed using Sequential Halving as selection strategy in the root node, as, in theory, it performs better with respect to simple regret. However, Sequential Halving requires a budget of iterations to be predetermined, which is often impractical. This paper proposes an anytime version of the algorithm, which can be halted at any arbitrary time and still return a satisfactory result, while being designed such that it approximates the behavior of Sequential Halving. Empirical results in synthetic MAB problems and ten different board games demonstrate that the algorithm’s performance is competitive with Sequential Halving and UCB1 (and their analogues in MCTS).","Monte-Carlo Tree Search (MCTS) [13, 7] is a search algorithm used for different sequential decision-making problems. It has been thoroughly studied within the context of game playing agents, but also seen use in other planning, optimization, and control problems [4]. The algorithm consists of four strategic steps, each of which can be implemented using a variety of different strategies [4, 19]. Strategies for the selection step tend to use Multi-Armed Bandit (MAB) algorithms, which balance exploration (sampling actions that are less explored) with exploitation (more deeply searching actions that appear more promising). The most commonly used selection strategy is UCB1 [2], which focuses on minimizing cumulative regret. Sequential Halving (SH) [12], which focuses on simple regret, may be argued to be a more suitable choice in MCTS [16, 6]. Integrations of SH into MCTS have been described in research on partially observable games [15], variants of MCTS that take additional guidance from scores learned through online or offline learning [9], and the state-of-the-art Gumbel AlphaZero and MuZero [8], which also integrate deep neural networks. Running through the four strategic steps of MCTS once is referred to as an iteration, and MCTS typically runs multiple iterations, after which it returns a final decision (e.g., move to play or action to take). It is common to use either a time budget, where MCTS keeps running iterations until it runs out of time, or an iteration budget, where it runs a predetermined number of iterations. SH requires the number of iterations that can be executed to be known in advance, which means that MCTS using SH as selection strategy does not have the anytime property. The algorithm cannot be terminated at any arbitrary point in time and be expected to have the quality of its final decision smoothly increasing as processing time increases (barring pathological cases [14]). When dealing with known games (for which the average number of iterations per unit of time could be measured) and a fixed per-move time budgets, a reasonable approximation of a fixed iteration budget may be calculated and used. However, when dealing with particularly large and varied sets of games [18], automatically generated games that must be played quickly in the context of an evolutionary search [20], or agents that automatically manage their time in an intelligent manner [11, 3], the lack of anytime property can be more problematic. This paper proposes anytime SH: a MAB algorithm with the anytime property, which can be used as selection strategy in (the root node of) MCTS. Its design was heavily inspired by the original SH, with anytime SH essentially being the original SH turned inside out. While we leave formal analyses of bounds on regret for future work, empirical results in synthetic MAB problems as well as a diverse set of ten board games demonstrate that anytime SH performs competitively with UCB1 (or UCT in games) as well as SH (only used in root node in games) in practice, whilst—in contrast to SH—retaining the anytime property."
https://arxiv.org/html/2411.07165v1,Acoustic-based 3D Human Pose Estimation Robust to Human Position,"This paper explores the problem of 3D human pose estimation from only low-level acoustic signals. The existing active acoustic sensing-based approach for 3D human pose estimation implicitly assumes that the target user is positioned along a line between loudspeakers and a microphone. Because reflection and diffraction of sound by the human body cause subtle acoustic signal changes compared to sound obstruction, the existing model degrades its accuracy significantly when subjects deviate from this line, limiting its practicality in real-world scenarios. To overcome this limitation, we propose a novel method composed of a position discriminator and reverberation-resistant model. The former predicts the standing positions of subjects and applies adversarial learning to extract subject position-invariant features. The latter utilizes acoustic signals before the estimation target time as references to enhance robustness against the variations in sound arrival times due to diffraction and reflection. We construct an acoustic pose estimation dataset that covers diverse human locations and demonstrate through experiments that our proposed method outperforms existing approaches.","Human pose estimation has diverse applications including rehabilitation support, elderly monitoring, and disaster relief efforts. Traditional approaches to 3D human pose estimation have primarily employed RGB videos and images [Martinez et al.(2017)Martinez, Hossain, Romero, and Little, Pavlakos et al.(2017)Pavlakos, Zhou, Derpanis, and Daniilidis], transient light [Isogawa et al.(2020)Isogawa, Yuan, O’Toole, and Kitani], event data [Scarpellini et al.(2021)Scarpellini, Morerio, and Del Bue, Chen et al.(2022)Chen, Shi, Ye, Yang, Sun, and Wang], radio frequency (RF)/Wi-Fi signals [Zhao et al.(2018b)Zhao, Tian, Zhao, Alsheikh, Li, Hristov, Kabelac, Katabi, and Torralba, Jiang et al.(2020)Jiang, Xue, Miao, Wang, Lin, Tian, Murali, Hu, Sun, and Su], and millimeter wave [Kong et al.(2022)Kong, Xu, Yu, Chen, Ma, Chen, Chen, and Kong, Xue et al.(2021)Xue, Ju, Miao, Wang, Wang, Zhang, and Su]. Additionally, methods that combine some of these approaches as a multimodal framework also exist [An et al.(2022)An, Li, and Ogras, Yang et al.(2024)Yang, Huang, Zhou, Chen, Xu, Yuan, Zou, Lu, and Xie]. However, optical signals face challenges such as obstruction and poor performance in low-light conditions [Lee et al.(2023)Lee, Rim, Jeong, Kim, Woo, Lee, Cho, and Kwak]. Furthermore, RGB video and images acquire high-resolution measured data, which raises concerns regarding the protection of personal information. Wireless signal-based methods are restricted in environments employing precision machinery, such as medical facilities or aircraft. One possible solution to these challenges is the utilization of acoustic signals. Acoustic signals have much longer wavelengths (meter scale) compared to optical signals (nanometer scale) or RF/Wi-Fi signals (centimeter scale). Therefore, acoustic signals are more susceptible to diffraction and less affected by obstruction. Moreover, acoustic signals offer consistent performance irrespective of lighting conditions and their usage is not hindered by the presence of precision machinery. Recent studies have explored passive acoustic sensing for gesture recognition and human pose estimation by leveraging human speech [Li et al.(2021)Li, Kang, Pei, Zhe, Zhang, He, and Bao, Ginosar et al.(2019)Ginosar, Bar, Kohavi, Chan, Owens, and Malik], ambient sounds [Gao et al.(2020)Gao, Oh, Grauman, and Torresani], or the sound of playing a musical instrument [Shlizerman et al.(2018)Shlizerman, Dery, Schoen, and Kemelmacher-Shlizerman]. These methods require sounds produced by the subjects themselves, which limits the use case. Alternatively, Shibata et al\bmvaOneDotproposed a 3D human pose estimation approach using active acoustic sensing with Time-Stretched-Pulse (TSP) signals [Shibata et al.(2023)Shibata, Kawashima, Isogawa, Irie, Kimura, and Aoki]. In this approach, a subject is positioned between a speaker and microphone (see Fig. 1(a)), where the speaker repeatedly emits TSP signals to create an acoustic field, and human poses are estimated based on how the acoustic field distorts as a subject moves. However, this method primarily relies on how the acoustic signal emitted from the speaker is obstructed by the human body to estimate the human pose. It implicitly assumes that the target subject is positioned on a straight line between the speaker and the microphone, although in the real world, meeting such constraints is extremely rare. Through the preliminary experiments, we found that the estimation accuracy significantly decreases when the subject deviates from this line, due to the difficulty of capturing subtle changes in sound signals caused by human body movements. Fig. 1(c) visualizes the acoustic features used as input to the model. The dimensions of these features are reduced by the Principal Component Analysis (PCA). The acoustic features in the settings without any subject and those shown in figures (a) and (b) are represented in different colors. From this figure, it is confirmed that the acoustic features when a person moves away from this line (blue dots) approach the features when there is no subject (green dots), indicating sound diffraction and reflection convey much less human pose information than sound obstruction caused by a person standing on the aforementioned line (red dots). To overcome this limitation, this paper proposes an acoustic-based 3D human pose estimation method, which remains effective regardless of the subject’s standing positions. While Shibata et al\bmvaOneDotprimarily relied on signal obstruction by the human body as their main clue, in this paper, we also consider cases where the position of the person is not on the straight line connecting the speaker and the microphone, as shown in Fig. 1(b). Therefore, it is necessary to consider signal diffraction and reflection from the subject as well as signal obstruction. From a technical perspective, this implies the need to solve two extremely challenging issues: (i) The relatively long wavelengths of acoustic signals tend to cause specular reflections off the surface of the human body. Consequently, the sound intensity of the reflected acoustic signals is greatly influenced by the positions of reflection and recording microphones. (ii) The arrival time of sound emitted from a speaker until it is recorded can vary due to signal diffraction and reflection. In this paper, we aim to develop methods capable of addressing these challenges. First, to enhance robustness against variations in the subject’s position, we introduce a position discriminator module. This module uses intermediate features of the pose estimation module to predict human positions, while the pose estimation module is trained to maximize the uncertainty of human positions, through adversarial training. Furthermore, to achieve robust pose estimation against changes in the arrival time of sound due to sound diffraction and reflection, we propose to introduce a reference window into the pose estimation module to consider signals prior to the target time to be estimated. Additionally, we perform data augmentation by shifting the phase of the acoustic signal, which allows for a reduction in the amount of data per subject location, enabling the preparation of a dataset that covers diverse positions. As the first attempt at non-invasive 3D human pose estimation regardless of the subject’s position, we construct a new dataset containing data from positions away from the straight line connecting the speaker and the microphones. In summary, the technical contributions of this study are as follows: (1) We have worked towards realizing a practical non-invasive 3D human pose estimation method based on active acoustic signals while subjects are placed in multiple positions. (2) We introduced a position discriminator module to enhance robustness against variations in the subject’s standing position. Additionally, we constructed a pose estimation model that considers acoustic signals prior to the estimation target time to achieve robust estimation against changes in sound arrival times due to signal diffraction and reflection. (3) To effectively learn from limited data, we performed data augmentation by shifting the phase of the acoustic signal. (4) As the first attempt to estimate non-invasively 3D human pose regardless of the subject’s position, we constructed a dataset containing data from multiple positions away from the straight line connecting the speaker and the microphones."
https://arxiv.org/html/2411.07161v1,RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration,"This study investigates the efficacy of Multi-Agent Systems in eliciting cross-agent communication and enhancing collective intelligence through group decision-making in a decentralized setting. Unlike centralized mechanisms, where a fixed hierarchy governs social choice, decentralized group decision-making allows agents to engage in joint deliberation. Our research focuses on the dynamics of communication and decision-making within various social choice methods. By applying different voting rules in various environments, we find that moderate decision flexibility yields better outcomes. Additionally, exploring the linguistic features of agent-to-agent conversations reveals indicators of effective collaboration, offering insights into communication patterns that facilitate or hinder collaboration. Finally, we propose various methods for determining the optimal stopping point in multi-agent collaborations based on linguistic cues. Our findings contribute to a deeper understanding of how decentralized decision-making and group conversation shape multi-agent collaboration, with implications for the design of more effective MAS environments.","Collaboration is a fundamental aspect of the nature and human society. Whether among humans or animals, working together allows groups to overcome individual limitations and achieve greater collective outcomes. In nature, collaboration often arises as a strategy to boost survival, enhance resource gathering, or increase efficiency in completing tasks (Schmidt & Mech, 1997). Similarly, in human societies, collaboration drives innovation, facilitates problem-solving, and fosters shared understanding, enabling individuals to address complex challenges that would be unmanageable otherwise (De Man & Duysters, 2005; Graesser et al., 2018; Bittner & Leimeister, 2013). This innate tendency to collaborate is evident across various domains, from social communities to technological systems, where multiple entities coordinate their efforts toward a common goal. As we advance in developing intelligent agents, understanding and replicating these collaborative dynamics in artificial systems has become increasingly important, predominantly to cope with the complexity and adaptability seen in real-world interactions. Agents powered by Large Language Models (LLMs) have demonstrated impressive problem-solving capabilities across a wide range of tasks. However, single-agent systems encounter significant difficulties when tasked with problems that are either too large or complex, often resulting in instability, misalignment with the intended request, and hallucination (Liu et al., 2024; Kuhn et al., 2023; Lyu et al., 2023). To address these limitations, research has increasingly turned toward Multi-Agent Systems (MAS). MAS have shown greater efficacy in harnessing collective intelligence by allowing individual agents to specialize in distinct skills and facilitating effective collaboration among them (Guo et al., 2024). When agents working together in a MAS, it is natural for them to have varying interpretations and perspectives. While some opinions may align, disagreements are also frequent. This creates an inevitable tension between cooperation and competition, stemming from differences in backgrounds, information access, and individual goals. Therefore, the process of aggregating models’ diverse predictions into a final group decision becomes a crucial aspect of dynamic multi-agent collaboration. Many existing LLM-driven MAS are designed based on centralized group decision-making, which typically involves layered or centralized architectures with a hierarchy. When there is a conflict between agents, it is resolved by a pre-assigned agent or a pre-defined process. These systems are often used for tasks where agent networks follow the waterfall method, leading to a stratified arrangement among agents (Hong et al., 2023; Qian et al., 2023; Dong et al., 2023). However, this hierarchical setup poses several critical challenges: (1) fairness: individual agent messages may not be accurately represented in the final group outcome, leading to potential misrepresentation (Jiang & Lu, 2019); (2) rigidity: the system’s fixed structure may lead to over-fitting to specific scenarios (Chen et al., 2023), reducing its adaptability to diverse and dynamic environments; and (3) bias: the agent with final decision-making power may introduce its own biases into the process, potentially distorting the outcomes (Owens et al., 2024). Additionally, centralized MAS cannot perform in environments requiring independent agent decisions, where private or incomplete information hinders a central authority from dictating optimal strategies (Xu et al., 2023). Figure 1: Overview of our multi-agent collaboration platform: RoundTable. It uses a round-based collaboration where agents simultaneously send messages, propose solutions, and vote. Based on a social choice mechanism, RoundTable selects the most preferred proposal for group decisions. Brief introduction of RoundTable is in Section 3.1, details are in Appendix A.1. Decentralized group decision-making can ease these issues by distributing power among agents, where each agent has the ability to participate in the process. This is a common setting in world simulation and embodied environment, where agents need to behave independently because there exists information asymmetry or data boundary between agents (Mandi et al., 2024; Zhang et al., 2023a; Xu et al., 2023; Park et al., 2023), and possibly variability in capabilities that different agents have. With decisions made by multiple agents, the flexible structure of decentralized MAS adapts to various environments, but the lack of a fixed hierarchy demands careful monitoring of collaboration patterns. Centralized and decentralized group decision-making attempt to mimic the ways in which human societies form collective policies, such as in monarchies and democracies. These decision-making mechanisms, known as social choice, are studied across various fields including economics, mathematics, philosophy, and social science, with the goal of aggregating and synthesizing individual preferences into a unified consensus. However, the impact of social choice methods on LLM-based MAS has yet to be explored. In this study, we evaluate various social choice methods across different environments to observe and analyze agents’ group behaviors and collaboration pattern. This paper aims to provide the following research contributions: • We investigate how collaborative behavior in decentralized MAS varies across social choice methods, providing insights into how they influence overall cooperation and outcomes. • We identify key language features in multi-agent conversations as indicators of collaboration, offering a novel approach to analyzing linguistic cues in effective or ineffective interactions. • We propose various methods for determining the optimal stopping point in multi-agent collaboration, utilizing the linguistic features we identified."
https://arxiv.org/html/2411.07152v1,HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals,"Task-Oriented Dialogue (TOD) systems assist users in completing tasks through natural language interactions, often relying on a single-layered workflow structure for slot-filling in public tasks, such as hotel bookings. However, in enterprise environments, which involve rich domain-specific knowledge, TOD systems face challenges due to task complexity and the lack of standardized documentation. In this work, we introduce HierTOD, an enterprise TOD system driven by hierarchical goals and can support composite workflows. By focusing on goal-driven interactions, our system serves a more proactive role, facilitating mixed-initiative dialogue and improving task completion. Equipped with components for natural language understanding, composite goal retriever, dialogue management, and response generation, backed by a well-organized data service with domain knowledge base and retrieval engine, HierTOD delivers efficient task assistance. Furthermore, our system implementation unifies two TOD paradigms: slot-filling for information collection and step-by-step guidance for task execution. Our human study demonstrates the effectiveness and helpfulness of HierTOD in performing both paradigms.","Task-oriented dialogue systems aim to help users accomplish specific goals by executing tasks through natural language interactions. Significant advancements have been made in developing systems that support public domain tasks (Andreas et al., 2020; Peng et al., 2021; Su et al., 2022), such as booking hotels or reserving restaurants. These tasks typically feature straightforward, single-layered workflows with well-defined steps, intents, and information extraction requirements. However, in enterprise environments rich with domain-specific knowledge, TOD systems face unique challenges due to the complexity and lack of standardized documentation of tasks. Enterprise tasks often involve multi-layered structures composed of numerous interconnected subtasks, as illustrated in Figure 1. These tasks are rarely formally documented, and their execution heavily relies on the implicit knowledge of human experts. When users interact with a dialogue system in such contexts, their utterances can pertain only to atomic tasks defined at the leaf nodes of a complex task hierarchy. The overarching structure and sequence of these tasks remain internalized within the user’s expertise, making it difficult for dialogue systems to fully comprehend and assist effectively. Figure 1: Composite workflows to execute tasks for hierarchical goal completion. A high-level goal consists of multiple sub-goals that transition between each other as the conversation progresses. Each sub-goal can include atomic goals such as product knowledge QA, operational insights QA, or navigation instructions. Consider, for instance, an enterprise marketing platform equipped with audience segmentation functionality. Users within an organization may independently create many client profile segments for various projects. Over time, this practice can result in the proliferation of duplicate segments, increasing platform resource costs and reducing business efficiency. To mitigate this, users perform data hygiene processes to clean up and consolidate these duplicate segments. However, the formal steps for this process are typically undocumented, and practices vary across different organizations. With guidance from subject matter experts, the complex task of identifying and removing duplicate segments can be distilled into a high-level goal comprising four sequential steps: {mdframed} [backgroundcolor=cyan!10] Goal: Data Hygiene for Audience Segments • Step 1: Detect duplicate segments by definition or outcome. • Step 2: List segment references by relevant business entities. • Step 3: Remove or unlink non-essential segment references and relink to essential ones when necessary. • Step 4: Delete non-essential segments. In practice, different users may prioritize certain business entities over others and follow various paths through this task hierarchy when interacting with the dialogue system. Their intents correspond to atomic actions represented as nodes within a task graph, and the sequences connecting these nodes can vary significantly. A dialogue system with a deep understanding of these high-level business goals and the complex structure of such tasks can significantly enhance user experience. It can improve query comprehension, disambiguate user intents more effectively, proactively suggest relevant goals, and provide personalized responses that align with individual user needs. To this ends, we introduce HierTOD, an enterprise TOD system driven by hierarchical goals to facilitate the generation of more proactive and effective dialogues with users. For example, when a user inquires about detecting duplicate segments (as described in the first step of the previous case), HierTOD can proactively suggest transitioning to the high-level goal of conducting data hygiene for audience segments. Our system comprises several modules, including natural language understanding, composite goal retriever, dialogue management, and response generation, supported by a well-organized data service with a domain knowledge base and retrieval engine. Furthermore, existing TOD systems typically follow one of two paradigms, which are often developed separately. The first is slot-filling for information collection (Yang et al., 2021; Hu et al., 2022; Hudeček and Dušek, 2023), where users provide details and direct the system to perform specific tasks, such as making reservations. The second paradigm is step-by-step guidance, designed to assist users in executing real-world tasks by providing accurate information and step-by-step instructions. For instance, Amazon’s Alexa Prize Taskbot (Gottardi et al., 2022; Agichtein et al., 2023) helps users complete tasks such as cooking a dish or following a DIY tutorial, guiding them through the process with detailed instructions (Mo et al., 2023). In this work, HierTOD implements a unified framework that supports both paradigms, delivering comprehensive and efficient task assistance. Our contributions include: (1) Developing a TOD system to support composite workflows in enterprise environments. (2) Introducing a goal-driven approach to dialogues, making the system more proactive and enabling mixed-initiative interactions for improved task completion. (3) Implementing a unified framework that integrates two TOD paradigms: slot-filling for information collection and step-by-step guidance for task execution. (4) Conducting a human study to verify the effectiveness and helpfulness of our dialogue system."
https://arxiv.org/html/2411.07150v1,Variational Graph Contrastive Learning,"Graph representation learning (GRL) is a fundamental task in machine learning, aiming to encode high-dimensional graph-structured data into low-dimensional vectors. Self-supervised learning (SSL) methods are widely used in GRL because they can avoid expensive human annotation. In this work, we propose a novel Subgraph Gaussian Embedding Contrast (SGEC) method. Our approach introduces a subgraph Gaussian embedding module, which adaptively maps subgraphs to a structured Gaussian space, ensuring the preservation of graph characteristics while controlling the distribution of generated subgraphs. We employ optimal transport distances, including Wasserstein and Gromov-Wasserstein distances, to effectively measure the similarity between subgraphs, enhancing the robustness of the contrastive learning process. Extensive experiments across multiple benchmarks demonstrate that SGEC outperforms or presents competitive performance against state-of-the-art approaches. Our findings provide insights into the design of SSL methods for GRL, emphasizing the importance of the distribution of the generated contrastive pairs111Our code is provided at https://github.com/ShifengXIE/SGEC.","Graph representation learning (GRL) aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining (Ju et al., 2024). Self-supervised learning (SSL) offers a promising approach to GRL by mitigating the need for extensive human annotation (Jaiswal et al., 2020). Particularly, contrastive learning is a prominent approach in SSL that leverages the similarities and differences between data samples or data embeddings to learn representations. In this context, positive sample pairs are typically two augmented views of the same data point that should be close in the representation space, while negative sample pairs are original and augmented views of different data samples (Chen et al., 2020). Current graph-based contrastive learning methods primarily generate positive and negative sample pairs through perturbations (Zhu et al., 2020a)222A complete review of the related work can be found in Appendix A.. However, t-SNE visualizations of current graph-based contrastive learning methods, like GCA (Zhu et al., 2021) and GSC (Han et al., 2022), reveal uneven node distributions within the same graph, with sharp boundaries and erroneous node clusters as illustrated in Figure 1. We observe that those characteristics of previous models penalize their performance in GRL tasks. In this paper, we propose the Subgraph Gaussian Embedding Contrast (SGEC) model. In our method, a subgraph Gaussian embedding (SGE) module is proposed to generate the features of the subgraphs. The SGE maps input subgraphs to a structured Gaussian space, where the features of the output subgraphs tend towards a Gaussian distribution by using the Kullback–Leibler (KL) divergence. SGE is a learnable mapping that controls the distribution of the embeddings. The embedded subgraphs, paired with the original subgraphs to form positive and negative pairs, then conduct contrastive learning with optimal transport measurements to obtain the graph representation. The embedded subgraphs offer diversity to prevent mode collapse (also called positive collapse (Jing et al., 2022)) where the embeddings shrink into a low-dimensional subspace, by controlling the embedding distribution. For our contributions, firstly, we propose the Subgraph Gaussian Embedding Contrast (SGEC) model that demonstrates its advantages across eight distinct benchmarks. We also provide insights into the importance of the distribution of subgraphs generated for contrastive positive and negative pairs. Finally, we provide different validation and ablation studies to assess the effectiveness of each design choice in SGEC. Figure 1: T-SNE visualizations of the graph contrastive learning method GCA (Zhu et al., 2021), GSC (Han et al., 2022), and our method SGEC on the Cora dataset (McCallum et al., 2000). Each point in the visualization corresponds to a node embedding, with colors indicating classes. SGEC maps the node representations into a dense, uniform, and more linearly separable space."
https://arxiv.org/html/2411.07135v1,Edify 3D: Scalable High-Quality 3D Asset Generation,"We introduce Edify 3D, an advanced solution designed for high-quality 3D asset generation. Our method first synthesizes RGB and surface normal images of the described object at multiple viewpoints using a diffusion model. The multi-view observations are then used to reconstruct the shape, texture, and PBR materials of the object. Our method can generate high-quality 3D assets with detailed geometry, clean shape topologies, high-resolution textures, and materials within 2 minutes of runtime.","The creation of detailed digital 3D assets is essential for developing scenes, characters, and environments across various digital domains. This capability is invaluable to industries such as video game design, extended reality, film production, and simulation. For 3D content to be production-ready, it must meet industry standards, including precise mesh structures, high-resolution textures, and material maps. Consequently, producing such high-quality 3D content is often an exceedingly complex and time-intensive process. As demand for 3D digital experiences grows, the need for efficient, scalable solutions in 3D asset creation becomes increasingly crucial. Recently, many research works have investigated into training AI models for 3D asset generation (Lin et al., 2023). A significant challenge, however, is the limited availability of 3D assets suitable for model training. Creating 3D content requires specialized skills and expertise, making such assets much scarcer than other visual media like images and videos. This scarcity raises a key research question of how to design scalable models to generate high-quality 3D assets from such data efficiently. Edify 3D is an advanced solution designed for high-quality 3D asset generation, addressing the above challenges while meeting industry standards. Our model generates high-quality 3D assets in under 2 minutes, providing detailed geometry, clean shape topologies, organized UV maps, textures up to 4K resolution, and physically-based rendering (PBR) materials. Compared to other text-to-3D approaches, Edify 3D consistently produces superior 3D shapes and textures, with notable improvements in both efficiency and scalability. This technical report provides a detailed description of Edify 3D. Figure 2: Pipeline of Edify 3D. Given a text description, a multi-view diffusion model synthesizes the RGB appearance of the described object. The generated multi-view RGB images are then used as a condition to synthesize surface normals using a multi-view ControlNet (Zhang et al., 2023). Next, a reconstruction model takes the multi-view RGB and normal images as input and predicts the neural 3D representation using a set of latent tokens. This is followed by isosurface extraction and subsequent mesh post-processing to obtain the mesh geometry. An upscaling ControlNet is used to increase the texture resolution, conditioning on mesh rasterizations to generate high-resolution multi-view RGB images, which are then back-projected onto the texture map. Core capabilities. Edify 3D features the following capabilities: • Text-to-3D generation. Given an input text description, Edify 3D generates a digital 3D asset with the aforementioned properties. • Image-to-3D generation. Edify 3D can also create a 3D asset from a reference image of the object, automatically identifying the foreground object in the image. Model design. The core technology of Edify 3D relies on two types of neural networks: diffusion models (Song and Ermon, 2019; Ho et al., 2020) and Transformers (Vaswani et al., 2017). Both architectures have demonstrated great scalability and success in improving generation quality as more training data becomes available. Following Li et al. (2024), we train the following models: • Multi-view diffusion models. We train multiple diffusion models to synthesize the RGB appearance and surface normals of an object from multiple viewpoints (Shi et al., 2023b). The input can be a text prompt, a reference image, or both. • Reconstruction model. Using the synthesized multi-view RGB and surface normal images, a reconstruction model predicts the geometry, texture, and materials of the 3D shape. We employ a Transformer-based model (Hong et al., 2023) to predict a neural representation of the 3D object as latent tokens, followed by isosurface extraction and mesh processing. The final output of Edify 3D is a 3D asset that includes the mesh geometry, texture map, and material map. Fig. 2 illustrates the overall pipeline of Edify 3D. In this report, we provide: • A detailed discussion of the design choices in the Edify 3D pipeline. • An analysis of the scaling behaviors of model components and properties. • An application of Edify 3D to scalable 3D scene generation from input text prompts."
https://arxiv.org/html/2411.07132v1,"Token Merging for Training-Free Semantic Binding 
in Text-to-Image Synthesis","Although text-to-image (T2I) models exhibit remarkable generation capabilities, they frequently fail to accurately bind semantically related objects or attributes in the input prompts; a challenge termed semantic binding. Previous approaches either involve intensive fine-tuning of the entire T2I model or require users or large language models to specify generation layouts, adding complexity. In this paper, we define semantic binding as the task of associating a given object with its attribute, termed attribute binding, or linking it to other related sub-objects, referred to as object binding. We introduce a novel method called Token Merging (ToMe), which enhances semantic binding by aggregating relevant tokens into a single composite token. This ensures that the object, its attributes and sub-objects all share the same cross-attention map. Additionally, to address potential confusion among main objects with complex textual prompts, we propose end token substitution as a complementary strategy. To further refine our approach in the initial stages of T2I generation, where layouts are determined, we incorporate two auxiliary losses, an entropy loss and a semantic binding loss, to iteratively update the composite token to improve the generation integrity. We conducted extensive experiments to validate the effectiveness of ToMe, comparing it against various existing methods on the T2I-CompBench and our proposed GPT-4o object binding benchmark. Our method is particularly effective in complex scenarios that involve multiple objects and attributes, which previous methods often fail to address. The code will be publicly available at https://github.com/hutaihang/ToMe.","Text-to-image generation has seen significant advancements with the recent introduction of diffusion models ramesh2022dalle2 ; Rombach_2022_CVPR_stablediffusion ; deepfloyd , with their capabilities of generating high-fidelity images from text prompts. Despite these achievements, aligning the generated images with the text prompts, which is referred to as semantic alignment hu2024ella ; li2023divide_bind , remains a notable challenge. One of the most common issues observed in existing text-to-image (T2I) generation models is the lack of proper semantic binding, where a given object is not properly binding to its attributes or related objects. For example, as illustrated in Fig. 1, even a state-of-the-art T2I model such as SDXL podell2023sdxl can struggle to generate content that accurately reflects the intended nuances of text prompts. Figure 1: Current state-of-the-art T2I models often struggle with semantic binding in generated images according to textual prompts. For example, hats and sunglasses are placed on incorrect objects. We introduce a novel method ToMe to address these challenges. To address the persistent challenges of aligning T2I diffusion models with the intricate semantics of text prompts, a variety of enhancement strategies karthik2023iffirst ; liu2023correcting ; zhou2023maskdiffusion are proposed, either by optimizing the latent representations wang2023tokencompose ; zhang2024enhancing ; zhang2024object_energy , guiding the generation by layout priors qi2023layeredrenderdiff ; wu2023harnessing ; zhao2023loco or fine-tuning the T2I models feng2023ranni ; jiang2024comat . Despite these advancements, these methods still encounter limitations, particularly in generating high-fidelity images involving complex scenarios where an object is binding with multiple objects or attributes. In this paper, we categorize semantic binding into two categories. First, attribute binding involves correctly associating objects with their attributes, a topic that has been studied in prior work rassin2024linguistic_binding . Second, object binding, which entails effectively linking objects to their related sub-objects (for example, a ‘hat’ and ‘glasses’), is less explored in the existing literature. Previous methods often struggled to address this aspect of semantic binding. One of the main problems is the misalignment of objects with their corresponding sub-objects. Existing solutions address this through an explicit alignment process of the attention maps chefer2023attend ; li2023divide_bind or by factorizing the generation projects into layout phases and generation phase qu2023layoutllm . In this paper, we propose a simple solution to the attention alignment problem called token merging (ToMe). Instead of multiple attention maps, which can be misaligned, we join these objects in a single composite token that represents the object and its attributes and sub-objects. This composite token has a single cross-attention map that ensures semantic alignment. The composite token is simply constructed by summing the CLIP text embeddings of the various tokens it represents. For example, the phrase “a dog with hat” is abbreviated as “a dog*” by aggregating the text embeddings corresponding to the last three words, as shown in Fig. 4. To justify the applied embedding addition in ToMe, we experimented with the semantic additivity of the text embeddings (in Fig. 3). Furthermore, to mitigate potential semantic misalignment in the end tokens from the long sequences, we propose end token substitution (ETS) technique. As the T2I generation predominantly determines the layout during earlier phases hertz2022prompt , we introduce an entropy loss and a semantic binding loss to update the token embeddings in early steps, integrating ToMe with an iterative update for the composite tokens. The entropy loss is defined as the entropy of the cross-attention map corresponding to the updated composite token. This loss aims to enhance generation integrity by ensuring diverse attention across relevant areas of the image, thereby preventing focusing on non-essential regions. The semantic binding loss encourages the new learned token to infer the same noise prediction as the original corresponding phrase. This alignment further reinforces the semantic coherence between the text and the generated image. Our final method ToMe is quantitatively assessed using the widely adopted T2I-CompBench huang2023t2i_compbench and our proposed GPT-4o achiam2023chatgpt4 object binding benchmark. Comparative evaluations against various types of approaches reveal that ToMe outperforms them by a significant margin. Remarkably, our approach is user-friendly, requiring no dependence on large language models or specific layout information. In qualitative evaluations, we notably achieve superior generation quality, particularly in scenarios involving multi-object multi-attribute generation. This further underscores the superiority of our method. In summary, the main contributions of this paper are as follows: • We analyze the problem of semantic binding, and highlight the role of the [EOT] token (Fig. 2), and the problems with misaligned cross-attention maps (Fig. 7). In addition, we explore token additivity as a possible solution (Fig. 3). • We introduce a training-free approach called Token Merging (Fig. 4), denoted as ToMe, as a more efficient and robust solution for semantic binding. It is further enhanced by our proposed end token substitution and iterative composite token updates techniques. • In experiments conducted on the widely used T2I-CompBench benchmark and our GPT-4o object binding benchmark, we compared ToMe with various state-of-the-art approaches and consistently outperformed them by significant margins."
https://arxiv.org/html/2411.07104v2,Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing,"Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0\% higher success rates and 24.5\% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.","Recent advances in quadrupedal robots have significantly improved their ability to traverse challenging terrains [1, 2, 3, 4, 5, 6]. While many studies have focused on enhancing their mobility and stability of locomotion, the manipulation capabilities of these robots remain relatively limited. Efforts have been made to improve the quadrupedal capabilities in prehensile manipulation through attaching grippers or robotic arms on the robot [7, 8, 9, 10, 11, 12], and non-prehensile manipulation by using legs [13, 14, 15, 16] or the head [17, 18] as the end-effectors. Although these advancements enable quadrupeds to handle some routine tasks, their limited ability to manipulate large and heavy objects still restricts their usefulness in demanding fields like search and rescue, construction, industrial automation, and room organization, where both dexterity and strength are essential. To address these challenges, researchers have explored adding support structures to the robots [19, 20], coordinating whole-body movements [21], and using multiple robots [22, 23] to strengthen contact forces and expand operational dimensions. However, achieving long-horizon manipulation of large objects in cluttered environments remains a largely unexplored and challenging task for quadrupeds. In this work, we focus on addressing the challenge of obstacle-aware, long-horizon pushing by coordinating the whole-body motions of multiple quadrupedal robots. We build our work upon recent works of quadrupedal pushing that demonstrate impressive results. As shown in Table I, while many approaches utilize multiple robots to enhance manipulation abilities, few focus on long-horizon pushing and obstacle avoidance, both of which are critical for real-world tasks. Additionally, the limited use of whole-body motions (e.g., relying solely on heads to push) [18, 22, 23] restricts the contact patterns between robots and objects, making it difficult for the robots to perform diverse movements and avoid collisions with obstacles. TABLE I: Comparisons between our proposed method and previous methods of quadrupedal pushing. Method Collaborative Long- Horizon Whole- Body Obstacle- Avoidance Sombolestan et al. [18] ✗ ✗ ✗ ✗ Jeon et al. [21] ✗ ✗ ✓ ✗ Sombolestan et al. [22] ✓ ✗ ✗ ✗ Nachum et al. [23] ✓ ✓ ✗ ✓ An et al. [24] ✓ ✗ ✓ ✗ Xiong et al. [25] ✓ ✗ ✓ ✗ Ours ✓ ✓ ✓ ✓ To achieve collaborative, obstacle-aware, long-horizon quadrupedal pushing through whole-body motions, we propose a hierarchical multi-agent reinforcement learning (MARL) framework with three levels of controllers. The high-level controller integrates an Rapidly-exploring Random Tree (RRT) planner [26] and a centralized adaptive policy, which processes the reference trajectory, environment, and agent information to generate subgoals for the object. The mid-level controller learns a shared decentralized goal-conditioned policy, enabling multiple robots to coordinate and push the object toward the sequential subgoals proposed by the high-level controller. The low-level controller is a pre-trained locomotion policy that executes commands from the mid-level controller. We validate our approach through a series of experiments in both simulation and real-world tests on Go1 robots, a few of which are visualized in Figure 1. Our results demonstrate that the proposed method achieves a 36.0\% higher success rate and a 24.5\% reduction in completion time compared to the best baseline approach in simulation. Furthermore, our method can be deployed on real robots to successfully complete obstacle-aware, long-horizon Push-Cuboid and Push-T tasks. The main contributions of this paper can be summarized as follows. • We propose a hierarchical MARL framework with three hierarchies that can handle long-horizon collaborative quadrupedal pushing in an environments with obstacles. • We benchmark our proposed method against baselines on various long-horizon pushing tasks involving obstacles in IsaacGym [27], demonstrating that our method significantly outperforms the baselines. • We deploy our trained hierarchical policy on real robots, successfully completing the collaborative long-horizon Push-Cuboid and Push-T tasks with coordinated whole-body motions. Figure 2: Overview of the proposed hierarchical MARL framework for collaborative long-horizon pushing tasks by quadrupedal robots. The framework comprises three layers: a high-level controller, a mid-level controller, and a low-level controller. The high-level controller utilizes an RRT planner to generate a trajectory and an adaptive policy to assign subgoals based on the dynamic states of the environment, object, and robots. The mid-level controller employs decentralized pushing policies to convert a common subgoal into agent-specific velocity commands, which are then executed by the low-level locomotion policy on each robot. Each layer is trained independently, leveraging frozen lower-level policies."
https://arxiv.org/html/2411.07099v1,Bounded Rationality Equilibrium Learning in Mean Field Games,"Mean field games (MFGs) tractably model behavior in large agent populations. The literature on learning MFG equilibria typically focuses on finding Nash equilibria (NE), which assume perfectly rational agents and are hence implausible in many realistic situations. To overcome these limitations, we incorporate bounded rationality into MFGs by leveraging the well-known concept of quantal response equilibria (QRE). Two novel types of MFG QRE enable the modeling of large agent populations where individuals only noisily estimate the true objective. We also introduce a second source of bounded rationality to MFGs by restricting agents’ planning horizon. The resulting novel receding horizon (RH) MFGs are combined with QRE and existing approaches to model different aspects of bounded rationality in MFGs. We formally define MFG QRE and RH MFGs and compare them to existing equilibrium concepts such as entropy-regularized NE. Subsequently, we design generalized fixed point iteration and fictitious play algorithms to learn QRE and RH equilibria. After a theoretical analysis, we give different examples to evaluate the capabilities of our learning algorithms and outline practical differences between the equilibrium concepts.","Learning equilibria in multi-agent games is of great practical interest but hard to scale to many agents (Daskalakis et al., 2009; Deng et al., 2023). Mean field games (MFGs) allow scaling to arbitrarily many exchangeable agents at fixed complexity. MFGs are of recent interest as a tractable method to learn approximate equilibria of rational, selfish agents (Guo et al., 2019; Cui & Koeppl, 2021; Xie et al., 2021; Laurière et al., 2022; Anahtarci et al., 2023). Thus, MFGs are applied in various settings ranging from finance to engineering (Djehiche et al., 2017; Achdou et al., 2020; Carmona, 2020). A common solution concept in the realm of multi-agent learning, and consequently in the context of MFGs, is the Nash equilibrium (NE). In a NE, each player’s strategy is considered optimal, given the strategies of the other agents, resulting in an equilibrium where no agent has an incentive to change their strategies. The optimality notion inherent in NE assumes full rationality of the individual agents. However, in many real-world situations individuals may not behave perfectly rational due to limited information processing capabilities, psychological factors, social considerations or other factors. Deviations from perfect rationality are described by the fundamental concept of bounded rationality (Simon, 1955, 1979; Kahneman & Tversky, 1982; Selten, 1990; Gigerenzer & Selten, 2002; Kahneman, 2013). Bounded rationality implies that for many real-world scenarios NE are insufficient due to their rigorous perfect rationality assumption. Instead of NE, we require a more realistic equilibrium concept accounting for partially irrational agents. A popular game-theoretic approach to modeling bounded rationality of agents are quantal response equilibria (QRE) (McKelvey & Palfrey, 1995, 1998) which are used, e.g. in economics (Breitmoser et al., 2010), robust RL (Reddi et al., 2024) and for efficient NE approximation (Gemp et al., 2024). Intuitively, in a QRE agents perceive rewards perturbed by noise and act optimally with respect to these perturbed rewards. In our work, we extend QRE to the domain of MFGs to model the behavior of a large number of agents who deviate from perfect rationality. Meanwhile on the control-theoretic side, a common approximately optimal control method is model predictive control (MPC) (Kouvaritakis & Cannon, 2016), also known as receding horizon control. To further enhance modeling of bounded rationality in MFGs, we incorporate a receding horizon method, where agents make decisions based on a limited future time horizon, reflecting more realistic decision-making processes. In contrast to MPC-based variants of MFGs such as (Inoue et al., 2021), we analyze the resulting novel receding horizon equilibria and instead focus on learning such equilibria, in a discrete-time setting. Beyond realism, introducing bounded rationality yields possible tractability advantages. NE computation for MFGs can be hard, motivating the search for alternative equilibrium notions. In this work, we show that under certain assumptions, QRE can be computed using a fixed point iteration (FPI). Moreover, QRE solutions can be seen as NE approximations with arbitrarily accurate design (Eibelshäuser & Poensgen, 2019). Recently, different equilibria have been introduced as NE approximations in MFGs (Cui & Koeppl, 2021). We compare QRE with these equilibria both theoretically and empirically and provide a new algorithm to compute QRE which extends to these equilibria. Meanwhile, for receding horizon equilibria we find novel general algorithms that work both in practice and in theory. Our main contributions are: • We formulate QRE for MFGs to incorporate bounded rationality for a more realistic MFG framework; • We integrate a receding horizon method tailored to the limited lookahead capacity of realistic agents; • We give theoretical and empirical results to put MFG QRE in context to existing equilibrium concepts; • We generalize the known fictitious play (FP) and FPI algorithms for NE to learn QRE and other equilibria; • We provide empirical examples to demonstrate the capabilities of our learning algorithms."
https://arxiv.org/html/2411.07098v1,"Adaptive REST API Testing with 
Reinforcement Learning","Modern web services increasingly rely on REST APIs. Effectively testing these APIs poses challenges due to the vast search space to explore, which involves selecting API operations for sequence creation, choosing parameters for each operation from a potentially large set, and sampling values from the often infinite parameter input space. Current testing tools lack efficient exploration mechanisms, treating all operations and parameters equally without considering their importance or complexity and lacking prioritization strategies. Furthermore, these tools struggle when response schemas are absent in the specification or exhibit variants. To address these limitations, we present an adaptive REST API testing technique that incorporates reinforcement learning to prioritize operations and parameters for exploration. Our approach dynamically analyzes request and response data to inform dependent parameters and adopts a sampling-based strategy for efficient processing of dynamic API feedback. We evaluate our technique on ten RESTful services, comparing it against state-of-the-art tools with respect to code coverage achieved and the number of generated requests, operations covered, and service failures triggered. Additionally, we perform an ablation study on prioritization, dynamic feedback analysis, and sampling to assess their individual effects. Our findings demonstrate that our approach significantly outperforms existing REST API testing tools in terms of effectiveness, efficiency, and fault-finding ability.","The increasing adoption of modern web services has led to a growing reliance on REpresentational State Transfer (REST) APIs for communication and data exchange [richardson2013restful, patni2017pro]. REST APIs adhere to a set of architectural principles that enable scalable, flexible, and efficient interactions between various software components through the use of standard HTTP methods and a stateless client-server model [fielding2000architectural]. To facilitate their discovery and use by clients, REST APIs are often documented using specification languages [openapi, swagger, raml, apiblueprint], which let developers describe the APIs in a structured format and provide essential information, such as the available endpoints, input parameters and their schemas, response schemas, etc. Platforms such as APIs Guru [apis_guru] host thousands of RESTful API documents, emphasizing the significance of these standardized API specifications in industry. Standardized documentation formats, such as the OpenAPI specification [openapi], not only facilitate the development of APIs and their use by clients, but also provide a foundation for the development of automated testing techniques for REST APIs, and numerous such techniques and tools have emerged in recent years (e.g., [arcuri2019restful, Corradini2022, atlidakis2019restler, martin2020restest, karlsson2020automatic, karlsson2020quickrest, liu2022morest, wu2022combinatorial]). In spite of this, effectively testing REST APIs continues to be a challenge, with high code coverage remaining an elusive goal for tools [kim2022automated]. Testing REST APIs can be challenging because of the large search space for exploration, arising from numerous operations, potential execution orders, inter-parameter dependencies, and associated input parameter value constraints [martin2019catalogue]. Current techniques often struggle to explore this space due to the lack effective exploration strategies for operations and their parameters. Existing testing tools tend to treat all operations and parameters equally, disregarding their importance or complexity, leading to suboptimal testing strategies and insufficient coverage of crucial operation and parameter combinations. Moreover, these tools rely on discovering producer-consumer relationships between response schemas and request parameters, which works well when the parameter and response schemas are described in detail in the specification. However, if the schemas are incomplete or imprecise, the tools can become less effective in their exploration. In this paper, we present adaptive REST API testing with reinforcement learning (arat-rl), an advanced black-box testing approach that addresses these limitations of existing tools. Our technique incorporates several innovative features, such as leveraging reinforcement learning to prioritize operations and parameters for exploration, dynamically constructing key-value pairs from both response and request data, analyzing these pairs to inform dependent operations and parameters, and utilizing a sampling-based strategy for efficient processing of dynamic API feedback. The primary objective of our approach is to increase code coverage and improve fault-detection capability. The core of novelty in arat-rl is an adaptive testing strategy, driven by a reinforcement-learning-based prioritization algorithm for exploring the space of operations and parameters. The algorithm initially determines an operation’s importance based on the parameters used and their frequencies across other operations. This targeted exploration enables efficient coverage of critical operations and parameters, thereby optimizing code coverage. The technique employs reinforcement learning to adjust the priority weights associated with operations and parameters based on feedback, by decreasing importance for successful responses and increasing it for failed responses. The technique also assigns weights to parameter-value mappings based on various sources of input values (e.g., random, specified values, response values, request values, and default values), which lets it adapt the testing strategy and concentrate on areas more likely to contain faults, ultimately enhancing fault-detection capability. Another innovative feature of arat-rl is dynamic construction of key-value pairs. In contrast to existing approaches that rely heavily on resource schemas provided in the specification, our technique dynamically constructs key-value pairs by analyzing POST operations (i.e., resource creation HTTP method) and examining both response and request data. For instance, support that an operation takes book title and price as request parameters and, as response, produces a success status code along with a string message (e.g., “Successfully created”). Our technique leverages this information to create key-value pairs for book title and price, upon receiving a successful response, even without such data being present in the response. It takes into account the input parameters used in the request, as they correspond to the created resource. Moreover, if the service returns incomplete resources, our technique still processes the information available in key-value pairs. This dynamic approach enables our tool to identify resources from the API responses and requests and discover hidden dependencies that are not evident from the specification alone. Finally, arat-rl employs a simple yet effective sampling-based approach that allows it to process dynamic API feedback efficiently and adapt its exploration based on the gathered information. By randomly sampling key-value pairs from responses, our tool reduces the overhead of processing every response for each pair, resulting in more efficient testing and optimized utilization of testing resources. To evaluate the technique, we conducted empirical studies using 10 RESTful services and compared it against three state-of-the-art REST API testing tools: RESTler [atlidakis2019restler], EvoMaster [arcuri2019restful], and Morest [liu2022morest]. We assessed the effectiveness of arat-rl in terms of coverage achieved and service failures triggered, and its efficiency in terms of valid and fault-inducing requests generated and operations covered within a given time budget. Our results show that arat-rl outperforms the competing tools in all the metrics considered—it achieved the highest method, branch, and line coverage rates, along with better fault-detection ability. Specifically, arat-rl covered 119.17%, 59.83%, and 52.42% more branches, lines, and methods than RESTler; 37.03%, 20.87%, and 14.13% more branches, lines, and methods than EvoMaster; and 23.69%, 11.87%, and 9.55% more branches, lines, and methods than Morest. arat-rl also uncovered 9.2x, 2.5x, and 2.4x more bugs than RESTler, EvoMaster, and Morest, respectively. In terms of efficiency, arat-rl generated 52.01%, 40.79%, and 1222% more valid and fault-inducing requests and covered 15.38%, 24.14%, and 282.98% more operations than Morest, EvoMaster, and RESTler, respectively, in a one-hour testing time budget. We also conducted an ablation study to assess the individual effects of prioritization, dynamic feedback analysis, and sampling on the overall effectiveness of arat-rl. Our results indicate that reinforcement-learning-based prioritization contributes the most to arat-rl’s effectiveness, followed by dynamic feedback analysis and sampling in that order. ⬇ /products/{productName}/configurations/{configurationName}/features/{featureName}: post: operationId: addFeatureToConfiguration produces: - application/json parameters: - name: productName in: path required: true type: string - name: configurationName in: path required: true type: string - name: featureName in: path required: true type: string responses: default: description: successful operation /products/{productName}/configurations/{configurationName}/features: get: operationId: getConfigurationActivedFeatures produces: - application/json parameters: - name: productName in: path required: true type: string - name: configurationName in: path required: true type: string responses: ’200’: description: successful operation schema: type: array items: type: string Figure 1: A Part of Features-Service’s OpenAPI Specification. The main contributions of this work are: • A novel approach for adaptive REST API testing that incorporates (1) reinforcement learning to prioritize exploration of operations and parameters, (2) dynamic analysis of request and response data to inform dependent parameters, and (3) a sampling-based strategy for efficient processing of dynamic API feedback. • Empirical results demonstrating that arat-rl outperforms state-of-the-art REST API testing tools in terms of requests generated, code coverage achieved, and service failures triggered. • An artifact [artifact] containing the tool, the benchmark services, and the empirical data."
https://arxiv.org/html/2411.07076v1,StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification,"Existing large vision-language models (LVLMs) are largely limited to processing short, seconds-long videos and struggle with generating coherent descriptions for extended video spanning minutes or more. Long video description introduces new challenges, such as plot-level consistency across descriptions. To address these, we figure out audio-visual character identification, matching character names to each dialogue, as a key factor. We propose StoryTeller, a system for generating dense descriptions of long videos, incorporating both low-level visual concepts and high-level plot information. StoryTeller uses a multimodal large language model that integrates visual, audio, and text modalities to perform audio-visual character identification on minute-long video clips. The results are then fed into a LVLM to enhance consistency of video description. We validate our approach on movie description tasks and introduce MovieStory101, a dataset with dense descriptions for three-minute movie clips. To evaluate long video descriptions, we create MovieQA, a large set of multiple-choice questions for the MovieStory101 test set. We assess descriptions by inputting them into GPT-4 to answer these questions, using accuracy as an automatic evaluation metric. Experiments show that StoryTeller outperforms all open and closed-source baselines on MovieQA, achieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and demonstrating a +15.56% advantage in human side-by-side evaluations. Additionally, incorporating audio-visual character identification from StoryTeller improves the performance of all video description models, with Gemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%, respectively, in accuracy on MovieQA. Datasets and code are available at https://github.com/hyc2026/StoryTeller.","Generating detailed video descriptions is a fundamental challenge in video understanding. While large vision-language models (LVLMs) have made significant progress [41, 24, 8, 40, 23, 22], they remain limited to processing seconds-long videos and generating low-level visual concepts like objects, scenes and atomic actions. Real-world videos, such as movies, are much longer and require high-level information for effective description, such as plot development. Recent research works address long videos by segmenting them, generating descriptions for each segment, and combining them using large language models [47, 13, 14]. However, there is still a gap to resolve the key challenge of long video description: maintaining consistency. This includes ensuring logical coherence in the storyline, consistency in character descriptions and motivations, and overall continuity and fluidity of the description. Accurate character identification is essential for ensuring consistency in long video descriptions. While existing studies [32, 18] have improved the quality of video descriptions by focusing on character identification and tracking through visual elements, audio information is equally critical, as dialogue significantly contributes to drive the story progressing. Nevertheless, a gap remains in linking dialogue with on-screen characters to identify speakers, and further improve the capability for long video descriptions. Figure 1: Pipeline of StoryTeller, consisting of three main modules: Stage I - Video Segmentation, where long videos are divided into seconds-long clips that are relatively independent and internally complete; Stage II - Audio-Visual Character Identification, where characters are identified for each dialogue line using both audio and visual cues; and Stage III - Description Generation, where detailed descriptions are generated for each short clip, ultimately producing a coherent and consistent narrative for the entire long video. To address these issues, we propose a new task: given a video and its cast list with character photos and names, identify the speaker for each dialogue line in the video using both visual and auditory cues. If the character is in the cast, provide their name; otherwise, give a descriptive label. We refer to this task as audio-visual character identification. This task is challenging as it requires integrating visual, auditory, and textual information to link dialogue lines to character identities. To tackle this, we developed a multimodal large language model equipped with both visual and auditory inputs. Due to the limitation of models with visual encoder in handling only short segments, it struggles to utilize global auditory information (e.g., some lines spoken by the same character through a long video). To address this, we introduce a global decoding algorithm that incorporates global information during inference to improve accuracy of audio-visual character identification. Furthermore, we propose StoryTeller, a novel system for generating dense descriptions of long videos, incorporating both basic visual concepts and high-level plot information with high consistency. The system consists of three components: a video segmentation module, an audio-visual character identification module, and an identity-aware description generation module, as illustrated in Figure 1. A major challenge in long video description is the lack of training and test data. To address this, we introduce MovieStory101, a new dataset focused on movies—long videos with high information density. The videos in MovieStory101 are from Movie101[45] and Movie101v2 [46], comprising 187 movies split into 5,982 three-minute clips. Each clip is annotated with storyline descriptions, subtitles, and character identity labels for each dialogue line. The storyline annotations ensure a complete understanding of the movie’s plot. By training on this dataset, we aim to unleash the model’s ability to generate plot-level descriptions. In adiition, evaluating long video descriptions is also challenging. Besides human evaluation, we propose an automated evaluation method for MovieStory101’s test set: MovieQA. MovieQA has 38 multiple-choice questions in average for each video, covering visual actions, character relationships, and plot details. We evaluate a long video description by inputting it into GPT-4 [28] to answer these questions, using the accuracy as an evaluation metric. Based on MovieQA and human side-by-side evaluation, we demonstrate that StoryTeller outperforms Gemini-1.5-pro [35], GPT-4o [29], and several advanced open-source LVLMs [24, 20, 41, 7] in generating descriptions for long videos. Specifically, the 7B model-based StoryTeller achieves an accuracy in the MovieQA evaluation that is 9.5% higher than the best-performing baseline model, Gemini-1.5-pro. In human side-by-side evaluation, StoryTeller shows an advantage of +15.56% over Gemini-1.5-pro. Additionally, we verify that incorporating audio-visual character identification results from StoryTeller as input features consistently enhances the long video description capabilities of various LVLMs. In particular, Gemini-1.5-pro and GPT-4o achieve relative improvements of 5.5% and 13.0% in accuracy on MovieQA, respectively. Figure 2: Audio-visual character identification module. (a) A multimodal large language model integrating visual, audio and text inputs is employed to identify characters in seconds-long videos. (b) Global Decoding: During inference, consistent mapping of global IDs across different short clips ensures the same global ID is assigned the same character name."
https://arxiv.org/html/2411.07071v1,"Universal Response 
and Emergence of Induction in LLMs","While induction is considered a key mechanism for in-context learning in LLMs, understanding its precise circuit decomposition beyond toy models remains elusive. Here, we study the emergence of induction behavior within LLMs by probing their response to weak single-token perturbations of the residual stream. We find that LLMs exhibit a robust, universal regime in which their response remains scale-invariant under changes in perturbation strength, thereby allowing us to quantify the build-up of token correlations throughout the model. By applying our method, we observe signatures of induction behavior within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across all models, we find that these induction signatures gradually emerge within intermediate layers and identify the relevant model sections composing this behavior. Our results provide insights into the collective interplay of components within LLMs and serve as a benchmark for large-scale circuit analysis.","Over the last few years, the capabilities of large language models (LLMs) have dramatically improved - already reaching the level of human experts on a variety of tasks [1, 2, 3]. In comparison to this remarkable rise of LLMs, our understanding of such models has remained relatively limited. A promising approach to gain a better understanding of LLMs is mechanistic interpretability (MI), which aims at understanding a model’s behavior through interpretable circuits of its components [4]. While in toy models such circuits were found to explain a variety of model behavior including in-context learning [5, 6, 7, 8], understanding how such circuits can be extracted in larger models or for wider classes of behavior remains an open area of research [9, 10]. What makes the composition of such circuits challenging for larger multi-layer models is a complex, non-linear interplay of multi-head attention (MHA), multi-layer perceptrons (MLPs), and skip-connections, via the residual stream [11]. This interplay is expected to give rise to superposition states of features [12], including attentional features [13], even across multiple layers [14], and thereby significantly complicates such interpretability studies. While recently, sparse autoencoders were successfully used to tackle the problem of superposition and find interpretable features as the variables for circuits in LLMs [15, 14, 16, 17], many open questions remain [18]. Therefore, although we have a good understanding of individual model components [19, 20, 21, 22, 23, 24, 25, 26, 15], their collective downstream effect on the model behavior remains under active investigation [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 14]. Consequently, while the field of MI is evolving rapidly and promising approaches to make progress on these questions are constantly emerging [37, 38, 39], a full macroscopic understanding of LLMs is still lacking. In this work, we examine the emergence of induction behavior, which is considered a key mechanism for in-context learning and therefore plays a fundamental role for our understanding of LLMs [8]. Specifically, the induction mechanism enables models to correctly predict the next token in repeated sequences of the form [A][B]\dots[A]\rightarrow[B], even for randomly chosen tokens [A],[B]. In two-layer, attention-only toy models, this mechanism has been successfully reverse-engineered through a composition of previous token heads followed by induction heads [7]. So far, however, such circuit decompositions could not be scaled to larger, multi-layer transformer models, and the precise composition of induction behavior in LLMs remains under active investigation [5, 35, 40, 41, 42, 43, 26, 8]. Our contributions Here, we reveal the emergence of induction signatures within LLMs by probing the models’ response to weak single-token perturbations of the residual stream. Specifically, we use repeated sequences of random tokens and find a strong response of the model for tokens preceding the perturbed token in the following sequence. We observe such induction signatures within Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL, and across all models find a robust, universal regime in which the response remains scale-invariant under changes of perturbation strength. We find that this scale-invariant regime extends throughout the residual stream, thereby allowing us to probe the composition of induction behavior within each model. For all models, we observe the gradual emergence of induction signatures across intermediate layers and identify the relevant model sections composing this behavior. Our results reveal qualitative differences in the composition of induction behavior in LLMs to guide future studies on large-scale circuit analysis [39]."
https://arxiv.org/html/2411.07070v2,On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models,"The pretraining and fine-tuning approach has become the leading technique for various NLP applications. However, recent studies reveal that fine-tuning data, due to their sensitive nature, domain-specific characteristics, and identifiability, pose significant privacy concerns. To help develop more privacy-resilient fine-tuning models, we introduce a novel active privacy auditing framework, dubbed Parsing, designed to identify and quantify privacy leakage risks during the supervised fine-tuning (SFT) of language models (LMs). The framework leverages improved white-box membership inference attacks (MIAs) as the core technology, utilizing novel learning objectives and a two-stage pipeline to monitor the privacy of the LMs’ fine-tuning process, maximizing the exposure of privacy risks. Additionally, we have improved the effectiveness of MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our research aims to provide the SFT community of LMs with a reliable, ready-to-use privacy auditing tool, and to offer valuable insights into safeguarding privacy during the fine-tuning process. Experimental results confirm the framework’s efficiency across various models and tasks, emphasizing notable privacy concerns in the fine-tuning process. Project code available for https://anonymous.4open.science/r/PARSING-4817/","Concerns regarding the privacy of training data pose a significant challenge in AI security, especially in sensitive domains like healthcare [34] and finance [55], where privacy issues are particularly pronounced. The study of privacy attacks and defense mechanisms for large language models (LLMs) is still in its early stages. In the past, research has primarily concentrated on extracting training data from pre-trained language models (PLMs) [5, 27, 24], leading to the development of novel theories regarding model memorization in LLMs [7] and mechanisms of data leakage [26]. Compared to the extensive and diverse datasets used during the pre-training stage, the training data in the fine-tuning phase presents much greater privacy risks. On one side, the constrained datasets employed for fine-tuning usually do not match the number of model parameters, resulting in an over-reliance on a limited set of data samples [12, 8, 56]. This reliance not only greatly impacts the model’s ability to generalize, but also raises the likelihood of disclosing sensitive details from the training data. On the other side, fine-tuning datasets are frequently derived from particular real-world sectors and usually contain more intricate information. While improving performance on targeted tasks, this specificity also significantly increases the cost of privacy breaches. Figure 1: An example of privacy auditing results for fine-tuning a model on different tasks. Quantify the level of privacy leakage using two carefully designed metrics. Thorough privacy evaluations greatly enhance user trust in models and their creators, serving as a crucial element in the sustainable advancement of technology. The emergence of advanced privacy attack algorithms [9, 26] have increased the demand for innovative research on privacy protection and auditing. We believe that an effective privacy audit should be able to present the quantified results of privacy risks in a simple and intuitive manner, as shown in Figure 1. MIAs [50, 20, 36, 16], which form the basis for many other types of attack and privacy audit frameworks [38, 6, 40, 51], are employed as a critical benchmark to assess the privacy boundaries of models. These attacks provide a direct measure for assessing privacy risks, where the attack metrics may mirror the model’s privacy level. Notably, the dynamic and intricate nature of the fine-tuning process arises both theoretical and practical challenges for privacy analysis under MIAs, hindering their successful application in deployed LMs. Additionally, existing privacy auditing methods primarily focus on the passive detection phase, where they assess privacy leakage after model training is completed. However, these methods often struggle to effectively address the dynamic changes occurring during the fine-tuning process, potentially overlooking critical privacy risks. To shed new light on developing more privacy-robust LMs, the following natural questions are explored as, Can the model fine-tuner actively identify privacy risks during the fine-tuning process? Can we quantify the privacy risks in this process using MIAs? What are the characteristics of privacy leakage during the process of LMs? As a result, it is necessary to develop more thorough methods to fill this technological gap. We note that non-members and members show noticeable variations in numerical data such as loss and gradient norms at the final layer, as well as in implicit data such as intermediate module outputs and gradients during fine-tuning. We have developed an effective methodological framework Parsing (Privacy Auditing on Risk of Supervised fine-tunING) to tackle these distinctions, highlighting and quantifying the privacy risks associated with the fine-tuning of LMs. Contributions. In this paper, we make the following contributions to the study of privacy in LMs. • We introduce a novel privacy auditing framework applied during the fine-tuning phase of LMs, which has the potential to be employed in the foundation model designs. The Parsing framework aims to identify and quantify the risks of privacy leakage inherent in the fine-tuning process and seeks to help achieve a balance between privacy and utility. We offer a detailed explanation of the structure and operational mechanisms of the framework. • We propose an active two-stage white-box MIA method targeting LMs, applicable to various models including the GPT series and Llama series. By introducing a new methodological architecture and learning objectives, the method first optimizes the membership representation of samples, thereby enhancing their feature representation capability. This improvement significantly increases the effectiveness of MIAs on complex models. • We evaluate Parsing on a range of models and diverse text tasks, and benchmarked it against existing studies. Empirical validation has shown that our framework is effective in detecting and quantifying privacy risks during the fine-tuning of LMs. Moreover, we also conduct a systematic analysis of the key factors leading to privacy vulnerabilities during the fine-tuning process, including task complexity, model size, text length, and so forth, in order to propose corresponding privacy protection strategies."
https://arxiv.org/html/2411.07066v1,Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training,"Network pruning is a set of computational techniques that aim to reduce a given model’s computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has focused on pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations’ alignment w.r.t. their corresponding dense models. Hence, we propose NeuronAl, a top-up algorithm that can be used on top of any given pruning algorithm for LLMs, that modifies the block-wise and row-wise sparsity ratios to maximize the neuron alignment among activations. Moreover, differently from existing methods, our approach adaptively selects the best parameters for the block-wise and row-wise sparsity ratios w.r.t. to the model and the desired sparsity (given as input), and requires no re-training. We test our method on 4 different LLM families and 3 different sparsity ratios, showing how it consistently outperforms the latest state-of-the-art techniques. The code is available at https://github.com/eliacunegatti/NeuroAL.","In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task [44, 33, 4]. However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13, 25, 43, 11], but nowadays the focus has shifted towards pre-trained models [41, 42, 22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (1) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for “exploiting” as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight’s information [21], activations [40, 39], or reconstruction error [15], without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18, 45, 49], and those that impose non-uniform sparsity distribution modifying the block-wise sparsity [46]. The latter category is extremely effective for improving performance in CNNs [14, 38], while its application to LLMs is still limited [46]. Contributions In this paper, we first analyze the reasons behind the effectiveness of non-uniform sparsity distribution in sparse LLM. To do so, we carefully analyze the state-of-the-art top-up method OWL [46], investigating the reason underlying its better performance (w.r.t. other methods from the state of the art) as well as its limitations in terms of sensitiveness to its hyperparameters. Leveraging this knowledge, we introduce a new top-up method, that we call NeuronAl. The algorithm consists of a two-step approach that re-distributes the block-wise sparsity, i.e., the sparsity among Transformer blocks, and the row-wise sparsity, i.e., the sparsity for each row of a given layer’s matrix, maximizing the neuron alignment between dense and sparse activations. Contrary to OWL, NeuronAl does not require the user to specify any hyperparameter, as it automatically selects the most-performing values from a suitable set, hence adapting to the underlying model and the desired sparsity. Moreover, the use of alignment overcomes the necessity to define outliers on the activations, as done in OWL, which requires defining a threshold a priori. Finally, it does not use any gradient information [18, 45], hence significantly saving computational resources. We test our approach on 3 Language Modeling datasets and 7 Zero-Shot tasks over 4 different LLM families from 7B to 13B parameters, to show its ability to outperform the most recent state-of-the-art techniques, including OWL [46] and DsNoT [49], over 3 different high sparsity values (60%, 70%, and 80%). To assess the robustness of our approach, we also conduct an in-depth ablation study. 2 Related Work In this section, we provide a comprehensive discussion about network pruning applied to LLMs. We first introduce structured and unstructured network pruning; then, we focus on the latter, introducing the latest approaches proposed for improving sparse model performance. Structured Network Pruning. Given a layer’s weight matrix W\in\mathbb{R}^{n\times m} to sparsify, structured pruning removes either entire rows (n) or columns (m) (see the next section) aiming at speeding up both training and inference time. The first approach that applies structured pruning to LLMs has been proposed in [28], and focuses on the dependency of Transformers, i.e., it removes components of the networks while maximizing their original functionality. In [24], a pruning mechanism has been devised to remove components with the worst balance between loss and runtime. Other structured pruning approaches have been proposed based on combinatorial optimization [30], perturbative forward-pass only [9], and reduction of the embedding dimension through PCA [1]. Finally, in [17] it has been found that the last Transformer blocks are redundant, hence they can be completely removed with minor performance drops. The reason behind this phenomenon lies in the similarity between the learnable representation of consecutive blocks, which turns out to increase when the block depth increases. While all these approaches can achieve valuable inference speed-ups, the performance of the resulting sparse models w.r.t. their dense counterparts can be matched only at low sparsity values, such as 20% in [28] or 30% in [1]. This somehow limits the applicability of these methods, since in the case of models with billions of parameters one may need more aggressive pruning strategies to meet stringent hardware requirements. Unstructured Network Pruning. Differently from structure pruning, unstructured pruning works by removing weights in a scattered (i.e., non-structured) way. While in this scenario the inference speed-up is limited (although techniques for reordering weights are available [26, 34, 50]), the performance w.r.t. the dense model can be preserved also at high sparsity ratios (i.e., above 50%), with the performance at lower sparsity being almost always completely preserved. The first approach of this kind has been proposed in [15], where weight pruning and reconstruction are combined based on the Hessian matrix. Even a simple magnitude-based approach turned out to perform well [21], as well when integrated with information on the neuron activations [40, 12]. All these approaches work by computing a score for each weight and then removing, uniformly for each layer, the worst-performing ones for a given sparsity ratio. Top-Up Algorithms To improve the performance of unstructured pruning algorithms, several top-up algorithms have been devised. These approaches can be categorized into two distinct groups: methods that minimize the reconstruction error keeping the sparsity uniform for each block, and methods that modify the block-wise sparsity of the model resulting in non-uniform sparsity distribution across blocks. The first group firstly sparsifies the model using a pruning algorithm and then, either dynamically [49] or by backpropagation [18], updates the pruning mask. The second group (to which our method belongs) modifies the block-wise sparsity (obtained by a given pruning algorithm) based either on activations’ outliers [46] or on layer-wise sparsity using block-wise reconstruction error with gradient information [45]. The idea of simply redistributing the layer-wise sparsity is known to be extremely well-performing on Multi-Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The first approach of this kind, based on the Erdős–Rényi (ER) model, has been proposed in [35] for MLPs and then adjusted for CNNs in [11], while an empirical study about the effect of layer-wise pruning using different sparsity ratios has been done in [27]. Regarding Transformers (both for vision and text), the state-of-the-art algorithms [15, 40] have been devised to set the block-wise sparsity across the Transformer blocks in a uniform way. Later on, OWL has been proposed to build upon scoring-based pruning algorithms, adjusting the block-wise sparsity in a non-uniform way w.r.t. the number of outliers computed for each block. This approach improves the performance of several pruning algorithms, e.g. [15, 40], especially at sparsity above 60%. On the same line, BESA [45] allocates layer-wise sparsity across each block’s layer using gradient information. Recently, modality-wise sparsity distribution has been investigated in the case of multimodal tasks in [12, 19]. 3 Observational Study As discussed earlier, OWL [46] uses a non-uniform sparsity distribution across blocks, which provides an advantage in terms of performance w.r.t. uniform distribution. However, the reason why this occurs remains unclear. In the first part of this section, we focus on uncovering the reason behind this phenomenon. Furthermore, OWL suffers from some disadvantages: it requires the setting of two hyperparameters, one for the sparsity difference between consecutive blocks to utilize in the non-uniform distribution (\lambda), and one for the outlier definition (M), In the second part of this section, we investigate how the choice of the best hyperparameters is tied to the model and sparsity selected. This limitation leads to either selecting non-optimal hyperparameters or requiring a grid search to find the optimal values. The observational study has been performed on two models (Phi-2 and LLama-1 7B) and three different sparsity ratios (0.6, 0.7, 0.8) using as pruning algorithm both Wanda [40] and multiflow [12]111We did not include BESA [45] since it requires gradient information and works layer-based rather than block-based. We consider only gradient-free state-of-the-art methods.. 3.1 \Circled1 Non-Uniform Block-Wise Sparsity Distribution We start by analyzing if the OWL’s performance improvement, compared to uniform sparsity distribution, could be unequivocally associated with its outlier-based score. Since the number of outliers for each block turns out to decrease with the layer depth, we test three straightforward non-uniform sparsity schedules (namely linear, exponential, and logarithmic), which do not require any forward step and do not depend on the outliers. Given a fixed parameter \lambda, these schedules work by redistributing the sparsity across layers in a monotonically increasing way (i.e., the sparsity of layer i is always larger than the sparsity of layer i-1,\forall i>1). Fig. 1 displays the improvement, w.r.t. uniform distribution, achieved by the three sparsity schedules on two pruning algorithms (Wanda and multiflow) with \lambda=0.08 (as in [46]). The results highlight how non-uniform sparsity schedules, with no outlier information, can match, and in some cases even improve, the performance of OWL. Overall, the linear schedule turns out to be the most reliable one since it does not show oscillations in performance across the different sparsity ratios (while this happens for the logarithmic and exponential schedule). Figure 1: Average perplexity improvement (percentage) w.r.t. uniform distribution, computed over 3 Language Modeling datasets when applying non-uniform distribution with linear, exponential, and logarithmic schedules. The horizontal dashed lines refer to the improvement w.r.t. uniform distribution achieved by OWL when applied to each of the two base pruning algorithms. 3.2 \Circled2 Effect of \lambda and M We then analyze the effect of the two hyperparameters set by OWL, namely \lambda and M. The first hyperparameter is used to set how much the sparsity can vary across blocks (i.e., [s-\lambda,s+\lambda]) while keeping the overall sparsity fixed as s. In the experimentation reported in [46], \lambda is set differently for each model, with 0.08 being the most used value. As a matter of fact, we found that this value is not effective with all models and sparsity ratios. We test OWL, applied again to Wanda and multiflow, with 12 different values of \lambda, and computed the perplexity on the 3 Language Modeling datasets. As we show in Figure 2, it turns out that OWL achieves the best results for each model-sparsity combination for different values of \lambda, which suggests the difficulty of setting this hyperparameter a priori. The second hyperparameter, M, defines the outliers’ threshold: namely, for each block, the number of outliers is computed as the number of activations that are M times greater than the block’s activations’ mean. We replicated the same analysis carried out for \lambda and also for M, testing 12 different values on the 3 Language Modeling datasets when applying OWL to Wanda and multiflow. As for \lambda, we found that the optimal setting of M heavily depends on the model and sparsity (noting that in [46] the most used value is 5), as depicted in Figure 3. Figure 2: Perplexity for different values of \lambda over WikiText2 using OWL’s non-uniform distribution across blocks. Figure 3: Perplexity for different values of M over WikiText2 using \lambda=0.08. To summarize, in \Circled1 we found that even if the non-uniform sparsity distribution proposed in OWL leads to better performance w.r.t. uniform distribution, this improvement is not entirely based on the outliers’ distribution. Instead, a simple increase of the block-wise sparsity across the layer depths can explain the performance improvement. In \Circled2, we showed how the performance of OWL is heavily sensitive to the choice of \lambda and M. In the next section, we will present our method to address this OWL’s limitation. 4 Methodology In this section, we propose our method, NeuronAl (Neuron Alignment). Given a pruning algorithm for a pre-trained LLM, our method re-computes the block-wise sparsity ratios for each Transformer block and the row-wise sparsity ratios based on the alignment between the dense activations and the sparse activations. The main strength of this method lies in its ability to be fully adaptive to the model and desired sparsity (given as input), requiring no single hyperparameter to be chosen a priori, but rather a set of suitable values from which NeuronAl can pick the most performing one. Preliminaries Given a dense model \mathcal{D}, a pruning algorithm \mathcal{P}, and a desired sparsity s, unstructured network pruning generally computes a saliency score \Psi for each weight w\in\mathcal{D} and then binarizes these scores w.r.t. the \mathtt{top}_{k} elements, where k=1-|\mathcal{D}|\times s. This allows to obtain a binary mask \mathcal{M} to apply over \mathcal{D}, from which the final sparse model can be computed as \mathcal{S}=\mathcal{D}\odot\mathcal{M}. Since LLMs are composed of stacked Transformer blocks (each one denoted as \mathcal{B}_{i}), i.e., sets of linear layers (each one denoted as \ell_{i}^{j}) that implement the self-attention mechanism followed by an MLP, the binarization step is usually done uniformly per each layer \ell_{i}^{j} [15, 40] as: \displaystyle\mathcal{M}^{\ell_{i}^{j}}=\mathtt{top}_{k^{\ell_{i}^{j}}}(\Psi^{% \ell_{i}^{j}},\mathcal{D}^{\ell_{i}^{j}}). (1) Neuron Alignment Our proposed method is based on the idea of combining the concept of neuron alignment, which requires no a priori definition of outliers (hence no M parameter, as in OWL), with that of adaptivity, to remove the dependence from \lambda. Differently from the well-established reconstruction error [15], computed as \arg\min_{M_{\ell},W_{\ell}}\left\|W_{\ell}X_{\ell}-(M_{\ell}\odot W_{\ell})X_% {\ell}\right\|_{2}^{2} for each layer or block (hence measuring the difference between the sparse and dense representation of the output prior the non-linearity functions), neuron alignment provides a single scalar value that evaluates the model in its completeness focusing on the difference between sparse and dense activations, hence after the non-linearity. The method takes as input both \mathcal{D} and its sparse version \mathcal{S} generated by \mathcal{P} with sparsity ratio s, and uses a small calibration data C_{\lambda} to make a forward pass on both models, to retrieve the dense and sparse activations, respectively \mathcal{A}_{\mathcal{D}} and \mathcal{A}_{\mathcal{S}}. The main idea behind NeuronAl is to maximize the neuron alignment by firstly modifying the vector of sparsity ratios for all blocks (\mathbf{s}^{\mathcal{B}}) and then for all rows (\mathbf{s}^{r}), where each row corresponds to the layer’s weight matrix W^{\ell_{i}^{j}} (for each layer \ell_{i}^{j} in \mathcal{B}_{i}), where W^{\ell_{i}^{j}}\in\mathbb{R}^{r\times m}. The main strength of this approach is that it does not require any weight update nor gradient information, but just a block- and row-wise recalibration and mask update via Eq. (1), using the same scoring criteria of \mathcal{P}. However, as tested in the previous observational study, finding the best block/row-wise sparsity requires defining a factor \lambda to control the block/row-wise sparsity difference between consecutive blocks/rows while ensuring the desired global sparsity. As seen earlier, while OWL requires \lambda to be set a priori, we design NeuronAl to automatically select, from a suitable set of values, the best \lambda for each combination of \mathcal{D}, \mathcal{P} and s, yielding an adaptive top-up method. The only constraint we set is that we use a linear sparsity schedule over \lambda for the block-wise step, demonstrated to be effective in our observational study \Circled1. This choice has been made (1) because we found that the performance improvement obtained with the linear sparsity schedule is more stable, see Fig. 1, and (2) to align our method to the latest research that shows how the last layers of an LLM have a small influence on the final performance [17, 29]. 4.1 Block-Wise Sparsity Ratio The first step concerns the block-wise redistribution over the whole model. Our method takes as input the dense and sparse models (\mathcal{D} and \mathcal{S}), the desired sparsity ratio (s), the calibration data C_{\lambda}, and a set of different \lambda parameters (\lambda^{\text{set}}). Then, it computes a set of |\lambda^{\text{set}}| different vectors of block-wise sparsity values for the whole model \mathbf{s}^{\mathcal{B}}_{set}=\{\mathbf{s}^{\mathcal{B}}_{\lambda_{1}},% \mathbf{s}^{\mathcal{B}}_{\lambda_{2}},\dots,\mathbf{s}^{\mathcal{B}}_{\lambda% _{|\lambda^{\text{set}}|}}\}, where each element \mathbf{s}^{\mathcal{B}}_{\lambda_{k}} indicates a vector of block-wise sparsity values obtained with a linear schedule in [s-\lambda_{k},s+\lambda_{k}]. For each \mathbf{s}^{\mathcal{B}}_{\lambda_{k}}, we then forward the calibration data C_{\lambda} through the model, and calculate the corresponding neuron alignment: \displaystyle neur_{al}=\sum_{\mathcal{B}_{i}}\sum_{\ell_{i}^{j}}\frac{\Bigg{% \|}\tilde{A}_{D}^{j}-\tilde{A}_{{\mathcal{S}}_{\left(\mathbf{s}^{\mathcal{B}}_% {\lambda_{k}}\right)}}^{j}\Bigg{\|}_{2}}{|\tilde{A}_{D}^{j}|} (2) where \tilde{A} means that the activations are normalized to sum up to one. Then, we select {\left(\mathbf{s}^{\mathcal{B}}_{set}\right)^{*}}, i.e., the \lambda parameters per block that minimize Eq. (2). Finally, we update the block-wise sparsity with the selected {\left(\mathbf{s}^{\mathcal{B}}_{set}\right)^{*}} via Eq. (1), thus obtaining a sparsified model \mathcal{S}_{\mathcal{B}}. 4.2 Row-Wise Sparsity Ratio The second step is complementary to the previous one, but in this case, the sparsity is modified w.r.t. the rows of each layer. It is established [40] that pruning using the row as a comparison group222Here, comparison group refers to the subset of weights whose scores are compared to decide which weights to prune. achieves better performance w.r.t. using the whole layer since it inherently maximizes the network connectivity [20, 7]. Here, we rely on such discovery to strengthen our approach and change the row-wise sparsity based on the neuron alignment of each layer. In this case, for each layer \ell_{i}^{j} (i.e., for each W^{\ell_{i}^{j}}\in\mathbb{R}^{r\times m}) we redistribute the sparsity across the r rows. Also in this case the \lambda parameters are critical for deciding how to control the sparsity difference between consecutive rows. We take our sparse model obtained with the block-wise redistribution (\mathcal{S}_{\mathcal{B}}) and, for each layer \ell_{i}^{j}, we compute different row-wise sparsity values obtaining \mathbf{s}^{s}_{set}=\{\mathbf{s}^{r}_{\lambda_{1}},\mathbf{s}^{r}_{\lambda_{2% }},\dots,\mathbf{s}^{r}_{\lambda_{|\lambda^{\text{set}}|}}\}, where each \mathbf{s}^{r}_{\lambda_{k}} indicates a vector of row-wise sparsity ranging in [s-\lambda_{k},s+\lambda_{k}], where each element is inversely proportional to the alignment of the corresponding row. In this case, we select in \mathbf{s}^{s}_{set} the row-wise vector {\left(\mathbf{s}^{r}_{set}\right)^{*}} that minimizes Eq. (2), and then apply Eq. (1) to \mathcal{S}_{\mathcal{B}}, using each row as comparison group. The full procedure composed of both the block-wise and row-wise recalibration is summarized in Algorithm 1. Algorithm 1 Proposed top-up pruning procedure Require: \mathcal{D},\mathcal{P},s,C_{\lambda},\lambda^{\text{set}} \mathcal{M}\leftarrow\mathcal{P}(\mathcal{D},s) \mathcal{S}\leftarrow\mathcal{D}\odot\mathcal{M} \triangleright Prune \mathcal{D} uniformly per layer A_{\mathcal{D}}\leftarrow\mathcal{D}(C_{\lambda}) \triangleright Dense activations \left(\mathbf{s}^{\mathcal{B}}_{set}\right)^{*}\leftarrow NeuronAl(\mathcal{D},\mathcal{S},\lambda^{\text{set}},C_{\lambda},A_{\mathcal{D}}) \triangleright Block-wise step \mathcal{S}_{\mathcal{B}}\leftarrow\mathcal{D}\odot\mathtt{top}_{\left(\mathbf% {s}^{\mathcal{B}}_{set}\right)^{*}}(\Psi,\mathcal{D}) {\left(\mathbf{s}^{r}_{set}\right)^{*}}\leftarrow\textsc{{NeuronAl}}(\mathcal{% D},\mathcal{S}_{\mathcal{B}},\lambda^{\text{set}},C_{\lambda},A_{\mathcal{D}}) \triangleright Row-wise step \mathcal{S}_{\text{final}}\leftarrow\mathcal{D}\odot\mathtt{top}_{\left(% \mathbf{s}^{r}_{set}\right)^{*}}(\Psi,\mathcal{D}) function NeuronAl(\mathcal{D},\mathcal{S},s,\lambda^{\text{set}},C_{\lambda},A_{\mathcal{D}}) \mathbf{s}^{*}\leftarrow\emptyset, neur_{al}^{*}\leftarrow\infty for \lambda\in\lambda^{\text{set}} do \mathbf{s}_{\lambda} = GetDist(s,\lambda) \triangleright Compute \mathbf{s}^{\mathcal{B}}_{\lambda} or \mathbf{s}^{r}_{\lambda} A_{\mathcal{S}}\leftarrow(\mathcal{D}\odot\mathtt{top}_{\mathbf{s}_{\lambda}}(% \Psi,\mathcal{D}))(C_{\lambda}) \triangleright Sparse activations neur_{al}\leftarrow\textsc{GetAlign}(A_{\mathcal{D}},A_{\mathcal{S}},\mathbf{s% }_{\lambda}) \triangleright Via Eq. (2) if neur_{al}<neur_{al}^{*} then \mathbf{s}^{*}\leftarrow\mathbf{s}_{\lambda} return \mathbf{s}^{*} 5 Experiments We apply our proposed NeuronAl to different pruning algorithms tailored for LLMs. Specifically, we test how it compares in terms of performance over Language Modeling datasets and Zero-Shot tasks w.r.t. the most recent top-up algorithms for pruning. We also perform scalability and ablation studies to show the effectiveness of our NeuronAl. 5.1 Experimental Setup Language Modeling Datasets To measure the models’ perplexity on Language Modeling datasets, we use the following three datasets: (1) WikiText2 [31], a collection of 2.6M tokens from Wikipedia; (2) Colossal Clean Common Crawl (C4) [37], which is a clean version of the Common Crawl dataset produced in April 2019, containing 154G tokens [10]; and (3) Penn Treebank (PTB), which is a dataset extracted from the Wall Street Journal containing 2.4M tokens. Zero-Shot Tasks To assess more thoroughly how the different pruning algorithms affect the models’ capabilities, we employ the following 7 datasets. (1) Recognizing Textual Entailment (RTE) [8, 2, 16, 3], a dataset composed of 3000 test samples in which a model has to detect the entailment between two sentences. (2) WinoGrande [23], a dataset consisting of 1767 test cases in which the model has to fill a blank using two choices. (3) BoolQ [5], a question-answering dataset containing 3270 test samples, where each question requires a yes/no answer. (4) HellaSwag [47], a dataset composed of 10k test cases that requires a model to select the appropriate completion for a given sentence from a set of possibilities. (5) ARC-e [6], the “easy” fold from the AI2 Reasoning Challenge, containing 2376 questions with multiple-choice answers. (6) ARC-c [6], the “challenge” fold from the AI2 Reasoning Challenge, containing 1172 questions that were not solvable for methods that perform well on the ARC-e dataset. (7) OBQA [32], a dataset containing 500 test multiple-choice questions about core science facts, asked in novel contexts. Table 1: Perplexity on the 3 Language Modeling datasets computed over 5 different LLMs for four different top-up algorithms (Uniform, DSnoT, OWL, and NeuronAl) on 3 pruning algorithms (Magnitude, multiflow, and Wanda) at 70% sparsity. Algorithm Top-Up Phi-2.7B LLama-1 7B LLama-2 7B Mistral 7B OPT 6.7B WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 764.6 384.4 983.9 2.53e4 2.25e4 3.26e4 1.42e5 1.02e4 2.02e6 221.9 232.9 748.7 1.00e4 5.39e3 6.54e3 DSnoT 539.0 258.0 656.2 1.02e7 2.77e6 4.99e7 1.31e8 2.90e7 2.25e8 192.7 189.9 566.2 6.16e3 3.93e3 4.36e3 OWL 419.6 242.7 358.5 1.20e4 6.58e3 5.39e4 3.39e5 1.24e4 3.28e6 111.7 124.2 545.5 1.57e4 8.48e3 9.67e3 NeuronAl 278.6 177.0 316.0 208.7 203.8 3.70e3 155.8 264.8 2.61e3 46.1 42.7 552.7 2.10e4 1.07e4 1.09e4 multiflow Uniform 388.4 298.8 610.8 80.9 71.9 172.4 60.0 58.8 1260.4 936.8 656.3 2062.5 943.6 1250.4 843.1 DSnoT 325.5 261.9 328.8 67.6 65.0 114.7 66.6 75.8 688.9 57.4 63.3 264.8 241.8 153.3 263.9 OWL 197.9 141.3 293.9 25.1 25.8 78.9 29.2 31.0 547.1 329.0 764.3 1718.2 240.9 495.6 337.8 NeuronAl 110.7 91.0 192.0 20.6 21.2 45.9 22.0 23.8 265.5 197.2 343.9 954.0 221.7 86.5 219.7 Wanda Uniform 227.6 182.7 346.2 85.1 86.2 157.0 78.0 81.0 599.3 60.7 73.6 298.3 157.5 260.1 209.2 DSnoT 221.9 172.6 257.6 72.9 76.0 121.0 76.1 85.7 491.8 81.3 79.9 304.8 191.4 173.3 182.6 OWL 132.7 116.2 183.7 24.6 27.3 61.2 30.5 36.6 333.7 41.0 51.8 253.5 54.4 69.7 100.7 NeuronAl 89.9 78.9 133.9 21.5 23.2 43.8 23.9 27.2 205.3 28.9 33.9 197.8 183.9 87.3 194.7 Baselines As pruning algorithms, we test Magnitude, multiflow [12], and Wanda [40]. All are tested with three different top-up algorithms (besides ours): (1) Uniform distribution, (2) DsnoT [49] (dynamic training-free uniform distribution with mask update), and (3) OWL [46] (block-wise training-free non-uniform distribution based on outliers scores). All these baselines are tested considering as comparison group each row: in other words, for a given layer, the sparsity s is uniform for each row of each matrix rather than uniform across matrices. This is done for two main reasons: 1) as mentioned earlier, it is established that row-wise pruning on LLMs leads to better performance w.r.t. layer-wise pruning [40], and 2) since our approach relies on a row-wise step, for fairness we also use row as comparison group (rather than layer) on all the other methods, to appreciate the benefit of our approach. We also test our approach, as well as the baselines, on SparseGPT [15], using in this case only the block-wise approach, since SparseGPT relies on a weight reconstruction mechanism that firstly prunes columns and then updates the rows of the pruned cells, which makes it unfeasible to apply our row-wise approach. The results on SparseGPT can be found in Tables 14-16 in the Appendix. For all the pruning algorithms that use calibration data (i.e., multiflow, Wanda, and SparseGPT), we use 128 samples from the C4 dataset, as in [15, 40]. Models and Sparsity Since one of the main distinctive features of NeuronAl is its ability to adapt to sparsity and models, we test 4 different LLM families. Specifically, we evaluate LLama-7B (both V1 and V2) [41, 42], Phi-2, Mistral-7B [22], and OPT-6.7B [48]. To scale up the models’ size, we also test LLama-13 (both V1 and V2). In the paper, we mainly present results at 70% sparsity, for fair comparisons with [46]. However, to assess the generalization to different sparsity ratios we also include 60% and 80% sparsity in our experiments, see the Appendix for details. NeuronAl Setup Our proposed method works by taking as input an LLM model, a desired sparsity ratio, a scoring-based pruning algorithm, and two sets of \lambda parameters (one for the block-wise and one for the row-wise steps). In our experiments, we gave as input \lambda^{\text{set}}\in[0.01,0.02,0.03,0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.15, 0.20,0.25] for the block-wise step, while for the row-wise step we also added 0.0 (in case of no performance improvement). Each value in \lambda^{\text{set}} has been evaluated (as described in Algorithm 1) over a calibration data C_{\lambda}. Since all the base pruning algorithms require a single forward over C (where |C|=128) sequences with 2048 tokens each, while OWL requires a second forward always over C, to make the computational runtime similar we set C_{\lambda}=8 (the closest power of 2 w.r.t. |C|/|\lambda^{\text{set}}|)333In Fig. 11 in the Appendix we report the results with different sizes of C_{\lambda}, showing how NeuronAl is robust to |C_{\lambda}|.. Doing so, the computational time required by NeuronAl is only two forward steps more than the base pruning algorithms, and one forward more than OWL444For both C and C_{\lambda}, we use the same seed (0) for the calibration set, i.e., C_{\lambda} contains the first 8 elements of C.. All the experiments have been run on NVIDIA A100 GPUs, both with 40 and 80 GB. 5.2 Experimental Results Concerning the Language Modeling datasets, the numerical results in terms of perplexity computed over the 3 Language Modeling datasets at 70% sparsity are shown in Table 1. It can be seen how NeuronAl is able in almost all cases to outperform all the other baselines by a large margin. In no case NeuronAl performs worse w.r.t. the uniform distribution. The only model on which NeuronAl is not the best top-up algorithm for all pruning algorithms is OPT. In all other cases, NeuronAl outperforms OWL for all models and pruning algorithms. The results at 60% and 80% sparsity shown in Tables 6-7 in the Appendix confirm this trend. As for the Zero-Shot tasks, the numerical results are shown in Table 2555Magnitude is removed from this table due to space constraints. Its results are available separately in Tables 10-12 in the Appendix.. Again, NeuronAl turns out to outperform in the majority of cases all the baselines. In 8 cases out of 10 (w.r.t. the mean accuracy across all tasks), NeuronAl is the one that reaches the best performance. The results for 60% and 80% sparsity are available in Tables 8-9 in the Appendix, where we observe a similar trend. Table 2: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on two pruning algorithms (multiflow and Wanda) at 70% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B multiflow 53.79 50.43 53.49 28.43 46.59 20.48 15.6 38.40 w. DSnoT 52.71 54.06 54.92 28.38 44.15 21.5 15.6 38.76 w. OWL 52.35 55.33 61.99 30.31 54.59 24.23 18.4 42.46 w. NeuronAl 53.07 58.01 62.17 33.33 51.52 25.43 17.6 43.02 Wanda 52.35 53.2 62.14 28.31 44.87 20.99 17.4 39.89 w. DSnoT 52.35 51.54 60.98 28.33 41.62 21.08 15.4 38.76 w. OWL 52.71 53.59 62.05 30.09 48.61 22.53 18.8 41.20 w. NeuronAl 52.71 58.01 62.17 32.37 49.62 22.95 17.2 42.15 Llama1 7B multiflow 55.96 52.57 61.96 29.77 34.64 19.45 15.2 38.51 w. DSnoT 54.15 50.43 59.33 29.33 36.45 19.28 13.6 37.51 w. OWL 52.35 58.64 62.63 36.74 47.43 26.62 18.2 43.23 w. NeuronAl 58.12 61.33 63.55 38.23 50.76 26.71 22.6 45.90 Wanda 55.23 52.8 57.46 28.84 32.2 18.0 13.8 36.90 w. DSnoT 54.15 51.22 54.56 28.97 33.08 18.26 13.6 36.26 w. OWL 58.48 58.56 62.60 34.74 47.35 24.06 17.4 43.31 w. NeuronAl 54.51 58.72 63.30 37.06 49.92 26.37 20.6 44.35 Llama2 7B multiflow 52.71 50.99 62.05 28.52 33.04 17.92 13.6 36.98 w. DSnoT 52.71 50.99 59.72 27.92 32.58 16.81 13.0 36.25 w. OWL 52.71 56.12 62.05 32.40 42.42 19.88 18.6 40.60 w. NeuronAl 54.15 58.09 62.42 35.29 48.36 22.53 21.2 43.15 Wanda 52.71 48.46 49.94 28.09 30.39 19.2 11.8 34.37 w. DSnoT 52.71 50.36 47.77 27.67 30.6 17.32 12.2 34.09 w. OWL 52.71 55.96 62.11 31.86 43.73 20.65 17.0 40.57 w. NeuronAl 52.71 57.62 62.72 35.47 50.25 22.01 20.6 43.05 Mistral-7B multiflow 49.82 50.75 41.19 26.45 26.64 21.84 12.6 32.76 w. DSnoT 52.71 52.57 62.42 29.51 36.66 18.94 12.0 37.83 w. OWL 53.79 49.17 38.90 26.77 27.78 19.20 12.8 32.63 w. NeuronAl 52.35 50.83 37.98 27.24 27.74 18.34 13.8 32.61 Wanda 52.71 51.62 59.79 28.86 34.18 18.17 12.6 36.85 w. DSnoT 52.71 50.28 58.62 28.51 33.54 18.86 13.2 36.53 w. OWL 52.71 53.91 62.20 30.95 39.39 18.60 13.6 38.77 w. NeuronAl 52.71 60.22 62.20 34.69 44.32 20.73 15.6 41.50 OPT-6.7B multiflow 53.79 49.72 43.0 26.48 30.51 20.05 13.8 33.91 w. DSnoT 53.79 49.01 61.1 27.01 32.87 18.34 12.2 36.33 w. OWL 48.74 48.62 61.56 27.18 35.69 16.47 11.6 35.69 w. NeuronAl 49.10 50.43 62.17 31.37 40.28 22.01 17.0 38.91 Wanda 52.71 49.72 60.03 26.91 35.86 17.75 11.2 36.31 w. DSnoT 52.71 49.57 60.61 26.91 35.06 17.58 12.0 36.35 w. OWL 53.79 51.22 61.87 29.53 42.3 18.09 14.6 38.77 w. NeuronAl 50.90 51.30 62.17 30.68 39.77 21.76 14.6 38.74 Table 3: Perplexity of LLama-13B on the 3 Language Modeling datasets at 70% (top) and 80% sparsity (bottom). Algorithm Top-Up Llama-1 13B LLama-2 13B WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 5610.6 3682.9 20040.0 191.0 159.1 2784.1 DSnoT 7420.1 2264.0 48325.4 121.7 103.0 2331.2 OWL 279.4 438.9 11502.2 33.3 35.4 1267.2 NeuronAl 55.4 83.3 841.1 45.2 44.4 321.1 multiflow Uniform 49.4 45.3 277.8 144.3 112.4 623.2 DSnoT 46.2 48.9 240.4 45.8 54.2 611.5 OWL 16.6 17.7 132.2 54.0 56.2 426.6 NeuronAl 13.7 15.4 98.7 17.3 20.0 275.2 Wanda Uniform 54.4 55.3 309.2 45.7 56.2 571.0 DSnoT 47.8 54.2 248.6 46.6 57.7 555.5 OWL 16.3 18.9 147.6 18.0 21.8 315.1 NeuronAl 14.3 16.6 85.4 16.5 19.3 239.6 Magnitude Uniform 26696.2 26319.4 27180.9 12020.1 11161.9 10210.6 DSnoT 5591.8 4131.3 5732.3 5822.9 6510.2 10668.0 OWL 21985.6 19564.4 24511.7 5208.2 5058.5 10202.2 NeuronAl 133.4 183.8 1727.7 574.3 386.2 4169.1 multiflow Uniform 3711.2 1696.1 3588.4 4481.2 2409.8 5209.6 DSnoT 5368.4 2863.6 6286.3 1943.3 1669.5 5279.7 OWL 813.8 375.7 2138.8 1802.3 1012.2 4394.9 NeuronAl 61.8 65.7 414.9 323.1 259.6 1301.6 Wanda Uniform 3479.3 1959.8 3566.1 1124.6 870.5 5549.8 DSnoT 43684.1 24432.2 32162.1 4441.2 3957.9 4092.8 OWL 761.6 368.1 1929.8 248.0 204.2 2027.1 NeuronAl 64.0 68.5 413.8 116.5 103.2 814.8 Scalability Study To assess if the NeuronAl performance scales to bigger models, we apply it to LLama-13B (both V1 and V2) on the Language Modeling datasets. The results available in Table 3 (top) show how our approach is even more effective with larger models. For the 70% sparsity ratio, NeuronAl turns out to reach the best performance over both models, for all pruning algorithms. Impressively, the gap in performance w.r.t. the other baselines (in particular uniform distribution) is even more marked than in the smaller models tested previously. The improvement of NeuronAl over the baselines is even more evident at 80% sparsity, see Table 3 (bottom), where in several cases it outperforms OWL by 1-2 orders of magnitude. This is confirmed also at 60% sparsity, see Table 13 in the Appendix. NeuronAl vs block-only vs row-only Since NeuronAl is based on two consecutive steps (for brevity, we call them here block and row), we test the performance of the block and row steps applied alone. This means that in the block-only setting the sparsity of each block one is given by our NeuronAl, while the row-wise sparsity for each layer is fixed to s. Instead, for the row-only setting, the sparsity for each block is fixed at s, while the row-wise sparsity for each layer is given by our neuron alignment mechanism. Table 4 shows the results of such ablation. The results confirm that NeuronAl (with both steps) achieves better performance in most of the cases, although this depends on the sparsity ratio (at lower sparsity, the block-only setting is the best one in 3 out of 5 cases). It is worth noticing that in such cases the gap w.r.t. NeuronAl is marginal and that NeuronAl always outperforms the uniform distribution. However, on high sparsity NeuronAl always performs best, with a large margin w.r.t. the second best (which is always the block-only setting). It is also clear how most of the performance improvement of NeuronAl comes from the block step, while the row step yields a moderate improvement, which however increases when increasing the sparsity ratio. Table 4: Ablation study: perplexity achieved when using the block and row steps alone over the C4 dataset. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 29.3 13.7 14.0 15.9 17.9 NeuronAl block-only 27.3 11.8 11.9 13.8 17.8 NeuronAl row-only 29.2 14.0 14.5 15.9 19.7 NeuronAl 27.2 12.0 12.0 13.8 19.1 70% Uniform 182.7 86.2 81.0 73.6 260.1 NeuronAl block-only 82.0 22.4 25.6 35.4 91.4 NeuronAl row-only 175.6 85.4 88.9 73.6 152.0 NeuronAl 78.9 23.2 27.2 34.0 87.3 80% Uniform 12422.9 3969.8 3117.3 277.7 2345.8 NeuronAl block-only 3177.3 296.2 313.8 181.8 530.6 NeuronAl row-only 16501.8 4597.5 824.9 277.7 2345.8 NeuronAl 2404.5 242.0 226.2 168.6 471.4 NeuronAl \lambda Selection In this section, we report an analysis of the ability of NeuronAl to pick the best \lambda parameters (i.e., the parameters for which the performance is the best one, hence the lowest value if computed over perplexity). To do this, we evaluate NeuronAl for all the \lambda parameters (in the block-only scenario to simplify the visualization of results) over the 3 Language Modeling datasets. Fig. 4 reports the perplexity at 70% sparsity across different values of \lambda (black dots connected by solid lines), while the dots highlighted in orange indicate the perplexity achieved with the \lambda value selected by NeuronAl. These results highlight how NeuronAl, in the majority of the cases, can pick the optimal value of \lambda both with data knowledge, as in the C4 dataset (from which the calibration data is sampled), as well as on unseen datasets such as WikiText2 and PTB. Fig.s 9-10 in the Appendix show the results at 60%-80% sparsity. Figure 4: Perplexity over different values of \lambda at 70% sparsity. The orange dot indicates the value selected by NeuronAl. The best \lambda is retrieved by maximizing the neuron alignment computed over C_{\lambda} on the C4 dataset, and then it is transferred to the other datasets. The red line corresponds to the perplexity using a uniform distribution, hence \lambda=0. Calibration Data Since NeuronAl works by finding the best \lambda parameters w.r.t. to the neuron alignment using a calibration sample of 8 samples, we test the results under different seeds for the calibration data. The results in Table 5 show the mean and standard deviation of perplexity at 70% sparsity over the 3 Language Modeling datasets (see Tables 20-20 in the Appendix for the same ablation at 60% and 80% sparsity). It is visible how the standard deviation is minimal w.r.t. the mean, apart from the OPT model where the standard deviation turns out to be higher. This dependence on the calibration data could explain the reason why, as discussed before, our proposed approach works better than the baselines on all other models but struggles with the OPT family. The ablation on the number of samples (see Fig. 11 in the Appendix) supports this hypothesis. In fact, OPT models are the only ones to show performance oscillation depending on the calibration data seed and especially the number of samples, which does not happen with all the other LLM families. Table 5: Ablation study: perplexity achieved when using NeuronAl with different calibration data seeds (0, 16, 46) at 70% sparsity over the 3 Language Modeling datasets. Dataset Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B WikiText2 085.5 \pm 3.3 22.7 \pm 2.1 023.6 \pm 0.2 029.0 \pm 0.1 137.4 \pm 71.2 C4 076.3 \pm 2.2 24.2 \pm 1.5 027.0 \pm 0.2 034.2 \pm 0.3 075.9 \pm 16.2 PTB 135.8 \pm 4.8 46.5 \pm 5.0 200.4 \pm 3.6 205.0 \pm 6.7 154.5 \pm 63.6 6 Conclusion and Limitations In this paper, we proposed NeuronAl, a new approach to prune LLMs based on the alignment between sparse and dense activations. Our approach has the main strength of requiring no outliers or gradient information. On top of that, our method is also adaptive, since it is designed to automatically select the best hyperparameters for the model, pruning algorithm, and desired sparsity. Throughout extensive experiments, we showed how our approach outperforms all the latest state-of-the-art methods both on Language Modeling datasets and Zero-Shot tasks, on 4 different LLM families and 3 sparsity ratios. We also included an extensive ablation study to show the robustness of our approach to the calibration data as well as the effect of its algorithmic steps. In our view, the present version of NeuronAl has three main limitations. (1) With NeuronAl, it is not possible to make use of optimized structured-sparsity inference implementations (e.g, the NVIDIA N:M sparsity [36]): for a given sparsity, NeuronAl produces customized sparsity constraints for each layer in a block, and then for each row of each layer. Therefore, these implementations cannot be employed as they often require continuity in the sparsity of matrices. (2) NeuronAl requires two forward steps more than base pruning algorithms, and one more than the closest top-up competitor, OWL. (3) Finally, it requires the definition of \lambda^{\text{set}}. However, it is worth noticing that |\lambda^{\text{set}}| as well as |\mathcal{C}_{\lambda}| affect the computational cost of the forward step. References [1] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. SliceGPT: Compress Large Language Models by Deleting Rows and Columns. In International Conference on Learning Representations, 2023. [2] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge, 2006. [3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth PASCAL recognizing textual entailment challenge. In Text Analysis Conference, 2009. [4] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):1–45, 2024. [5] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, 2019. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457v1, 2018. [7] Elia Cunegatti, Matteo Farina, Doina Bucur, and Giovanni Iacca. Understanding Sparse Neural Networks from their Topology via Multipartite Graph Representations. Transactions on Machine Learning Research, 2024. [8] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine learning challenges. Evaluating predictive uncertainty, Visual object classification, and Recognizing textual entailment, pages 177–190. Springer, 2006. [9] Lucio Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, and Ameet Talwalkar. Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes. arXiv preprint arXiv:2402.05406, 2024. [10] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. In Conference on Empirical Methods in Natural Language Processing, pages 1286–1305, 2021. [11] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pages 2943–2952. PMLR, 2020. [12] Matteo Farina, Massimiliano Mancini, Elia Cunegatti, Gaowen Liu, Giovanni Iacca, and Elisa Ricci. MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16185–16195, 2024. [13] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [14] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Pruning neural networks at initialization: Why are we missing the mark? In International Conference on Learning Representations, 2020. [15] Elias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 10323–10337. PMLR, 2023. [16] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9. Association for Computational Linguistics, 2007. [17] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. [18] Song Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao, Yiyu Shi, and Rongrong Ji. EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs. arXiv preprint arXiv:2402.12419, 2024. [19] Shwai He and Tianlong Chen. RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation. arXiv preprint arXiv:2404.02424, 2024. [20] Duc NM Hoang and Shiwei Liu. Revisiting pruning at initialization through the lens of Ramanujan graph. In International Conference on Learning Representations, 2023. [21] Ajay Jaiswal, Shiwei Liu, Tianlong Chen, Zhangyang Wang, et al. The emergence of essential sparsity in large pre-trained models: The weights that matter. Advances in Neural Information Processing Systems, 36, 2024. [22] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7B. arXiv preprint arXiv:2310.06825, 2023. [23] Sakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chandra, and Choi Yejin. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. arXiv preprint arXiv:1907.10641, 2019. [24] Eldar Kurtić, Elias Frantar, and Dan Alistarh. ZipLM: Inference-Aware Structured Pruning of Language Models. Advances in Neural Information Processing Systems, 36, 2024. [25] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. [26] Jiajia Li, Bora Uçar, Ümit V Çatalyürek, Jimeng Sun, Kevin Barker, and Richard Vuduc. Efficient and effective sparse tensor reordering. In ACM International Conference on Supercomputing, pages 227–237, 2019. [27] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. In International Conference on Learning Representations, 2021. [28] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-pruner: On the structural pruning of large language models. Advances in Neural Information Processing Systems, 36, 2023. [29] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. ShortGPT: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. [30] Xiang Meng, Shibal Ibrahim, Kayhan Behdin, Hussein Hazimeh, Natalia Ponomareva, and Rahul Mazumder. OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization. arXiv preprint arXiv:2403.12983, 2024. [31] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [32] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [33] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):1–40, 2023. [34] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021. [35] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 9(1):2383, 2018. [36] Jeff Pool. Accelerating sparsity in the nvidia ampere architecture. GTC 2020, 2020. [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. [38] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in Neural Information Processing Systems, 33, 2020. [39] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024. [40] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. In International Conference on Learning Representations, 2023. [41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [43] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. arXiv preprint arXiv:2002.07376, 2020. [44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. [45] Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, and Ping Luo. BESA: Pruning large language models with blockwise parameter-efficient sparsity allocation. In International Conference on Learning Representations, 2024. [46] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Gen Li, AJAY KUMAR JAISWAL, Mykola Pechenizkiy, Yi Liang, Michael Bendersky, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (OWL): A missing secret sauce for pruning LLMs to high sparsity. In International Conference on Machine Learning. PMLR, 2024. [47] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Machine Really Finish Your Sentence? In Annual Meeting of the Association for Computational Linguistics, pages 4791–4800, 2019. [48] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open Pre-trained Transformer Language Models. arXiv preprint arXiv:2205.01068, 2022. [49] Yuxin Zhang, Lirui Zhao, Mingbao Lin, Sun Yunyun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse LLMs. In International Conference on Learning Representations, 2024. [50] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch. In International Conference on Learning Representations, 2021. Appendix A Appendix A.1 Observational Study Here we include the results of the observational study on the OWL’s hyperparameters \lambda and M computed on the C4 dataset (Fig.s 5 and 7), and PTB (Fig.s 6 and 8). The results confirm the findings presented in the main text: there is no single hyperparameter selection (both M and \lambda) that reaches the best performance across all different model-pruning-sparsity settings. Figure 5: Perplexity for different \lambda values over C4 using OWL’s non-uniform distribution across blocks. Figure 6: Perplexity for different \lambda values over PTB using OWL’s non-uniform distribution across blocks. Figure 7: Perplexity for different M values over C4 using \lambda=0.08. Figure 8: Perplexity for different M values over PTB using \lambda=0.08. A.2 Additional Experiments Here we include the results of the experiments that, due to space limits, we could not include in the main text. Specifically, we report: the results of NeuronAl over Language Modeling and Zero-Shot tasks at 60% and 80% sparsity; the results for the Zero-Shot tasks with Magnitude pruning; the results over LLama-13B at 70% sparsity; the results of NeuronAl (block-only) applied to SparseGPT [15]. A.2.1 Language Modeling at 60% and 80% sparsity In Table 6-7, we report the results of NeuronAl over the 3 Language Modeling datasets (WikiText2, C4, and PTB) with the five different models considered in the main text, for 60% and 80% sparsity. In the first case, our approach turns out to be the best one in 23 out of 45 cases, while for 80% sparsity in 27 out of 45. It is interesting to notice how at medium sparsity (60%) all the top-up algorithms, including ours, provide similar results, while the improvement provided by NeuronAl at 80% (w.r.t. the top-up competitors) in some cases reaches a factor 5x (e.g., with Llama-1 7B for multiflow and Wanda). Table 6: Perplexity on the 3 Language Modeling datasets computed over 5 different LLMs for four different top-up algorithms (Uniform, DSnoT, OWL, and NeuronAl) on 3 pruning algorithms (Magnitude, multiflow, and Wanda) at 60% sparsity. Algorithm Top-Up Phi-2.7B LLama-1 7B LLama-2 7B Mistral 7B OPT 6.7B WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 51.3 45.9 66.9 152.4 159.8 3016.1 6894.8 42666.8 1706134.9 19.6 24.4 189.3 9489.3 6203.7 6759.1 DSnoT 55.9 48.9 64.1 131.6 114.7 1460.3 3676.6 67845.5 7299247.0 15.3 19.6 146.2 8077.8 6062.6 6948.2 OWL 46.5 42.2 65.7 50.5 62.9 249.4 810.9 19356.5 2302446.2 12.0 15.8 169.1 6811.7 3666.8 4065.4 NeuronAl 48.6 44.9 65.1 55.8 63.6 153.1 52.4 79.3 24807.9 12.6 17.2 123.9 1212.3 551.4 935.0 multiflow Uniform 25.4 28.3 54.7 11.6 13.9 26.0 11.0 13.7 166.9 167.7 341.0 884.3 16.3 19.7 26.8 DSnoT 37.2 42.1 50.2 10.1 12.7 18.2 10.5 13.4 137.6 10.9 14.8 86.0 15.9 19.2 25.3 OWL 23.8 26.7 49.8 10.6 12.9 19.6 10.1 12.7 106.0 84.1 123.0 644.6 16.1 18.5 25.5 NeuronAl 23.7 27.0 42.5 9.9 12.2 17.6 9.8 12.2 72.3 86.1 140.3 562.3 17.0 20.4 28.1 Wanda Uniform 225.8 29.3 48.9 10.7 13.7 24.0 10.8 14.0 122.2 11.3 15.9 101.6 15.2 17.9 23.7 DSnoT 32.2 38.0 50.6 10.4 13.2 20.8 10.8 14.1 109.6 11.4 15.9 96.8 15.8 19.1 24.6 OWL 24.8 28.2 48.6 9.4 11.8 18.5 9.2 11.9 75.1 10.3 14.5 84.5 15.7 17.8 24.5 NeuronAl 25.2 27.2 42.1 9.6 12.0 17.4 9.4 12.0 65.1 9.9 13.8 74.6 16.3 19.1 25.2 Table 7: Perplexity on the 3 Language Modeling datasets computed over 5 different LLMs for four different top-up algorithms (Uniform, DSnoT, OWL, and NeuronAl) on 3 pruning algorithms (Magnitude, multiflow, and Wanda) at 80% sparsity. Algorithm Top-Up Phi-2.7B LLama-1 7B LLama-2 7B Mistral 7B OPT 6.7B WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 15320.0 17856.3 31988.6 112885.2 113922.4 139627.3 55800.7 52597.2 89843.5 24815.2 31186.7 7984.1 42902.5 21284.4 22110.8 DSnoT 19977.5 20700.7 18369.5 33994.8 34213.5 72003.2 2358414.2 1802890.0 3015785.0 13297.5 8025.4 5804.0 18112.4 11853.1 14438.1 OWL 6625.1 5597.4 8388.0 169148.8 158523.1 134038.4 26898.0 17892.9 57863.3 9614.4 8503.8 5789.5 33174.7 17756.7 21613.0 NeuronAl 10193.9 9578.7 14427.3 82221.7 85688.7 108410.1 92725.2 67827.9 104616.8 2188.8 1205.3 3226.2 30303.8 20846.6 16807.1 multiflow Uniform 25257.3 12842.4 25929.7 4829.9 2307.4 9806.2 2037.9 1464.8 3878.2 4289.7 2976.3 3812.4 4422.8 2382.7 3276.9 DSnoT 8498.5 3918.3 12343.8 3697.1 2649.8 8263.4 1721.1 1543.0 3443.8 327.0 270.0 752.5 11633.7 9715.6 11764.5 OWL 6255.2 2798.5 13600.8 926.5 563.1 1783.4 544.2 414.3 2819.3 3354.5 2209.7 3555.2 13461.5 11133.9 15080.8 NeuronAl 3550.3 1279.8 5411.4 311.6 188.4 564.8 230.2 269.8 2355.9 983.1 617.5 1467.4 1118.7 591.5 1173.3 Wanda Uniform 20498.5 12422.9 31362.6 5219.5 3969.8 10013.6 4929.7 3117.3 5285.2 330.9 277.7 783.7 4259.6 2345.8 2732.1 DSnoT 15269.0 6855.4 14008.5 3709.7 3076.0 7790.7 5199.4 4443.2 6685.7 346.5 277.3 758.4 7754.1 6157.5 7779.7 OWL 2546.5 1209.5 7061.7 986.5 654.5 2000.4 663.0 486.2 2283.4 206.3 187.8 603.9 13190.1 10605.6 14228.5 NeuronAl 3873.6 2404.5 6675.9 376.1 242.0 566.3 213.9 226.2 993.2 226.3 168.6 698.2 988.2 471.4 988.7 A.2.2 Zero-Shot at 60% and 80% sparsity In Tables 8-9, we report the results at 60% and 80% sparsity of NeuronAl over Zero-Shot tasks with the five different models tested in the main text. In the first case, our approach turns out to be the best one (when considering the average over the 7 tasks) in 7 out of 10 cases, while for 80% sparsity in 8 out of 10. This further validates the results presented in the main text (for 70% sparsity) since it shows the ability of NeuronAl to provide stable and consistent results over more complicated tasks at different (and high) sparsity levels. Table 8: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on two pruning algorithms (multiflow and Wanda) at 60% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B multiflow 62.09 67.64 63.15 42.04 71.76 39.51 27.2 53.34 w. DSnoT 63.18 66.77 43.49 40.84 66.62 35.41 26.0 48.90 w. OWL 64.62 67.17 60.15 41.78 70.96 37.8 28.8 53.04 w. NeuronAl 62.09 67.40 67.22 42.73 71.04 38.65 27.6 53.82 Wanda 63.54 64.8 69.08 40.16 68.64 34.9 25.4 52.36 w. DSnoT 62.45 64.33 59.17 39.25 64.18 33.53 22.6 49.36 w. OWL 64.62 64.33 64.83 39.80 67.63 34.98 24.2 51.48 w. NeuronAl 63.54 67.09 67.74 40.51 66.54 34.04 24.4 51.98 Llama1 7B multiflow 57.04 62.51 67.19 45.31 59.55 30.97 24.6 49.60 w. DSnoT 49.46 63.06 68.32 44.75 63.80 31.23 26.4 49.57 w. OWL 54.15 63.46 66.54 46.53 60.65 31.14 25.4 49.70 w. NeuronAl 49.10 63.46 68.35 47.89 63.51 32.42 27.8 50.36 Wanda 59.57 62.67 68.81 43.64 62.84 30.38 24.8 50.39 w. DSnoT 51.62 61.64 67.37 43.39 63.89 30.55 25.4 49.12 w. OWL 55.60 64.17 70.61 46.63 62.96 31.74 24.8 50.93 w. NeuronAl 57.76 64.09 70.24 46.59 63.97 31.14 26.2 51.43 Llama2 -7B multiflow 57.04 61.96 64.80 43.39 60.44 29.1 24.6 48.76 w. DSnoT 54.15 63.77 63.91 43.42 66.25 31.83 24.0 49.62 w. OWL 54.87 62.75 65.14 45.20 62.58 29.52 23.8 49.12 w. NeuronAl 52.71 65.90 69.51 46.60 65.87 31.66 24.8 51.01 Wanda 54.15 64.48 65.44 43.85 65.19 30.46 26.2 49.97 w. DSnoT 53.79 64.09 64.83 42.39 63.89 30.03 22.8 48.83 w. OWL 53.79 66.61 66.76 46.63 67.63 32.34 28.0 51.68 w. NeuronAl 53.07 66.06 70.46 46.93 66.67 31.74 26.8 51.68 Mistral-7B multiflow 51.62 49.88 39.17 27.49 29.67 18.77 11.0 32.51 w. DSnoT 54.87 66.61 70.86 45.93 68.27 32.94 24.0 51.93 w. OWL 53.07 50.12 46.33 28.29 32.58 19.28 13.2 34.70 w. NeuronAl 52.71 52.49 52.97 28.77 33.42 18.43 13.2 36.00 Wanda 54.87 66.06 71.13 44.48 67.05 32.00 21.60 51.03 w. DSnoT 54.15 65.59 70.43 44.5 66.88 31.40 20.60 50.51 w. OWL 57.04 67.17 73.85 45.66 67.89 32.59 23.20 52.49 w. NeuronAl 56.32 65.82 70.70 46.71 67.63 33.36 23.20 51.96 OPT-6.7B multiflow 52.71 58.25 62.69 42.24 56.52 25.68 22.20 45.76 w. DSnoT 53.07 58.48 62.57 42.13 57.79 23.98 22.20 45.75 w. OWL 53.07 57.46 63.21 42.98 56.44 25.68 24.20 46.15 w. NeuronAl 52.71 58.64 62.17 41.83 56.31 25.09 22.20 45.56 Wanda 52.71 59.67 62.29 42.80 58.00 25.60 22.6 46.24 w. DSnoT 52.71 58.17 62.11 41.99 57.41 25.43 22.4 45.75 w. OWL 52.71 58.72 62.69 42.14 58.33 25.17 22.6 46.05 w. NeuronAl 52.71 61.25 62.29 42.00 57.49 25.43 23.0 46.31 Table 9: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on two pruning algorithms (multiflow and Wanda) at 80% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B multiflow 52.71 50.12 51.04 26.15 27.44 19.37 13.4 34.32 w. DSnoT 50.18 49.49 37.83 26.32 27.82 19.97 13.8 32.2 w. OWL 53.07 51.22 56.76 26.30 30.85 18.52 15.2 35.99 w. NeuronAl 52.71 49.96 50.58 26.49 32.20 19.8 14.4 35.16 Wanda 53.07 49.25 62.17 25.93 27.44 20.99 14.6 36.21 w. DSnoT 53.07 50.99 38.04 26.17 27.10 20.73 13.0 32.73 w. OWL 52.35 51.22 60.21 26.51 29.88 20.05 13.6 36.26 w. NeuronAl 52.71 50.12 62.17 27.57 32.95 21.08 16.0 37.51 Llama-7B multiflow 47.29 50.91 40.03 26.17 26.77 21.16 12.4 32.1 w. DSnoT 46.57 50.43 37.83 26.02 27.06 20.05 12.2 31.45 w. OWL 50.18 50.04 45.47 26.74 27.65 20.48 12.2 33.25 w. NeuronAl 54.87 50.91 55.96 27.62 30.09 18.77 12.8 35.86 Wanda 47.29 49.88 37.83 26.34 26.47 20.99 12.8 31.66 w. DSnoT 46.93 50.36 37.83 26.03 26.56 21.33 13.4 31.78 w. OWL 47.29 49.88 37.83 26.67 27.19 19.54 11.6 31.43 w. NeuronAl 49.82 48.86 40.76 27.41 29.25 18.34 13.0 32.49 Llama2 -7B multiflow 53.43 48.86 37.83 26.35 27.48 21.25 13.2 32.63 w. DSnoT 52.71 48.86 37.86 26.17 26.60 20.39 13.0 32.23 w. OWL 52.71 49.49 37.83 26.62 26.94 19.11 12.6 32.19 w. NeuronAl 52.71 50.36 44.74 27.47 28.62 17.58 14.4 33.70 Wanda 47.65 49.41 37.83 25.82 26.52 20.82 14.6 31.81 w. DSnoT 53.07 47.91 37.86 26.09 27.23 20.73 13.0 32.27 w. OWL 52.71 50.83 37.83 26.52 27.27 19.37 12.8 32.48 w. NeuronAl 52.71 49.49 37.89 27.38 28.58 17.58 14.8 32.63 Mistral-7B multiflow 50.18 48.15 37.80 25.67 26.18 22.70 13.20 31.98 w. DSnoT 52.71 47.36 37.83 26.58 28.03 18.94 13.8 32.18 w. OWL 48.38 49.09 38.44 25.88 25.59 23.29 13.6 32.04 w. NeuronAl 52.71 50.91 37.83 26.35 27.78 20.31 12.8 32.67 Wanda 53.79 48.78 37.83 26.52 27.82 19.8 12.8 32.48 w. DSnoT 52.35 48.30 37.83 26.55 27.44 19.54 13.0 32.14 w. OWL 52.71 47.43 37.83 26.68 27.78 18.52 13.2 32.02 w. NeuronAl 52.71 50.04 38.65 27.52 28.96 19.97 13.4 33.04 OPT-6.7B multiflow 52.71 50.91 37.80 25.87 27.40 19.28 12.40 32.34 w. DSnoT 52.71 50.83 57.31 26.00 25.00 20.22 11.80 34.84 w. OWL 52.71 51.07 37.83 25.74 25.29 20.05 16.00 32.67 w. NeuronAl 52.71 51.70 54.07 26.29 30.64 19.37 11.80 35.23 Wanda 54.15 52.09 41.53 26.47 28.45 18.86 11.80 33.34 w. DSnoT 52.71 51.38 55.32 26.17 27.06 19.37 13.80 35.12 w. OWL 52.71 49.33 37.83 25.84 25.67 20.31 13.00 32.10 w. NeuronAl 52.71 49.25 51.99 26.35 30.39 19.37 12.60 34.67 A.2.3 Zero-Shot at 60%, 70% and 80% sparsity with Magnitude In Tables 10-12, we report the results of NeuronAl over Zero-Shot tasks using Magnitude pruning as the base pruning algorithm. The results provided by NeuronAl turn out to be the best ones in 9 out of 15 cases. It is also worth noticing that the performance gap between the Magnitude pruning and score-based pruning algorithms (such as Wanda or multiflow) is generally quite high. Hence, NeuronAl can improve the performance of Magnitude (in the standard setting with uniform distribution) to a certain degree since at high sparsity ratios (as the ones we test) the performance of Magnitude has been shown to be poor [21]. Table 10: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on Magnitude at 60% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B Magnitude 57.04 62.83 51.38 42.56 66.33 35.41 28.2 49.11 w. DSnoT 54.51 64.09 42.32 41.09 66.25 34.98 26.6 47.12 w. OWL 55.23 62.59 48.81 42.53 67.38 38.48 28.4 49.06 w. NeuronAl 54.15 65.19 47.28 42.36 65.95 36.77 26.8 48.36 Llama-7B Magnitude 51.62 52.64 45.05 39.23 51.05 26.88 20.4 40.98 w. DSnoT 52.35 52.80 46.88 38.3 50.59 26.37 20.6 41.13 w. OWL 52.35 58.41 51.8 42.02 56.31 29.78 23.8 44.92 w. NeuronAl 50.90 56.12 57.43 40.65 55.3 29.78 24.4 44.94 Llama2 -7B Magnitude 51.26 55.8 41.19 36.97 50.17 26.96 16.2 39.79 w. DSnoT 53.79 56.04 42.87 38.3 53.28 27.9 19.8 41.71 w. OWL 51.99 57.3 46.15 42.56 56.65 30.46 19.4 43.5 w. NeuronAl 54.87 59.59 60.4 46.14 59.01 32.85 27.6 48.64 Mistral-7B Magnitude 55.23 62.19 66.36 48.74 67.05 33.19 22.6 50.77 w. DSnoT 55.6 62.35 68.53 48.28 67.51 33.11 23.2 51.23 w. OWL 53.79 64.48 72.17 49.39 68.14 33.87 23.8 52.23 w. NeuronAl 53.79 64.09 71.07 49.83 65.07 35.07 25.0 51.99 OPT-6.7B Magnitude 53.43 50.59 37.86 26.38 26.6 21.42 13.2 32.78 w. DSnoT 52.71 49.25 37.86 26.14 27.27 21.5 13.2 32.56 w. OWL 52.71 50.51 37.83 26.77 30.3 18.52 14.8 33.06 w. NeuronAl 52.71 54.22 39.14 33.3 37.71 24.23 16.8 36.87 Table 11: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on Magnitude at 70% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B Magnitude 46.93 53.59 47.22 30.45 47.85 24.57 19.2 38.54 w. DSnoT 46.57 50.91 39.6 30.12 45.54 24.06 16.8 36.23 w. OWL 45.13 52.88 49.2 32.26 51.64 27.56 21.4 40.01 w. NeuronAl 47.65 53.51 54.25 33.4 53.7 29.61 20.6 41.82 Llama-7B Magnitude 53.43 49.96 37.92 27.59 31.73 22.44 16.6 34.24 w. DSnoT 52.71 51.7 37.83 27.71 30.26 22.7 15.4 34.04 w. OWL 53.07 51.38 38.38 33.14 39.31 24.15 16.8 36.6 w. NeuronAl 52.71 54.38 52.91 39.61 46.0 26.62 23.0 42.18 Llama2 -7B Magnitude 51.26 49.96 37.86 25.9 28.45 23.12 13.4 32.85 w. DSnoT 53.79 49.88 37.86 25.42 28.83 20.56 16.6 33.28 w. OWL 53.07 50.28 37.89 26.38 30.77 22.7 15.0 33.73 w. NeuronAl 54.51 55.41 64.86 33.25 42.09 27.47 21.2 42.68 Mistral-7B Magnitude 51.99 50.83 41.13 32.16 42.72 19.54 16.6 36.42 w. DSnoT 53.07 51.62 39.54 31.66 42.51 20.05 16.6 36.44 w. OWL 57.76 56.59 49.17 36.48 45.75 22.01 18.8 40.94 w. NeuronAl 53.79 57.54 62.72 38.68 44.11 26.45 20.6 43.41 OPT-6.7B Magnitude 52.71 49.8 37.83 25.88 26.68 21.33 12.4 32.38 w. DSnoT 52.71 49.96 37.83 25.87 27.19 20.14 13.6 32.47 w. OWL 52.71 50.59 37.83 25.81 25.46 21.25 12.8 32.35 w. NeuronAl 52.71 50.28 37.8 26.27 26.81 20.31 13.0 32.45 Table 12: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on Magnitude at 80% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B Magnitude 45.13 50.36 41.19 25.83 29.08 20.9 13.6 32.30 w. DSnoT 46.93 52.33 39.63 25.9 28.32 21.25 13.4 32.54 w. OWL 49.46 50.91 42.35 26.71 35.27 21.67 13.4 34.25 w. NeuronAl 46.57 51.22 44.86 25.78 28.66 21.5 13.0 33.08 Llama-7B Magnitude 46.21 49.96 53.98 25.69 24.83 21.84 13.8 33.76 w. DSnoT 52.35 51.85 38.47 25.52 26.39 21.42 16.0 33.14 w. OWL 48.38 48.93 44.74 25.76 26.35 21.08 15.8 33.01 w. NeuronAl 47.29 48.86 50.34 25.82 26.18 21.25 14.6 33.48 Llama2 -7B Magnitude 52.35 49.57 46.18 25.94 26.14 23.12 16.0 34.19 w. DSnoT 52.71 51.54 37.89 25.46 27.10 22.44 15.4 33.22 w. OWL 53.07 48.70 42.02 25.72 26.60 21.42 14.4 33.13 w. NeuronAl 51.26 49.33 42.14 25.71 26.52 22.70 15.0 33.24 Mistral-7B Magnitude 51.26 50.99 41.16 25.93 27.48 21.84 14.6 33.32 w. DSnoT 52.35 49.72 38.07 26.26 26.43 21.25 14.0 32.58 w. OWL 52.35 50.20 41.04 26.55 27.78 19.97 13.8 33.10 w. NeuronAl 52.71 49.64 43.18 28.11 29.8 21.59 14.6 34.23 OPT-6.7B Magnitude 52.71 49.49 37.83 25.79 26.39 21.25 13.0 32.35 w. DSnoT 52.71 49.57 37.83 25.78 25.63 20.65 12.8 32.14 w. OWL 52.71 49.80 37.83 26.05 26.73 21.16 13.2 32.50 w. NeuronAl 52.71 51.78 37.83 25.79 26.89 21.08 12.8 32.70 A.2.4 NeuronAl on LLama-13B at 70% sparsity In Table 13, we present the results on LLama-13B at 60% sparsity. The results are in line with the ones at 70% and 80% sparsity presented in the main text since NeuronAl can outperform the competitors in 11 out of 15 cases. Table 13: Perplexity of LLama-13B on the 3 Language Modeling datasets at 60% sparsity. Algorithm Top-Up Llama-1 13B LLama-2 13B WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 34.9 49.1 1413.7 10.1 13.3 457.5 DSnoT 33.6 41.3 604.8 10.1 13.3 376.7 OWL 28.5 36.6 255.0 8.9 11.6 217.2 NeuronAl 24.8 35.1 186.9 9.5 12.3 151.1 multiflow Uniform 8.7 10.9 66.5 16.3 21.0 211.6 DSnoT 8.4 10.8 58.7 8.3 11.3 217.9 OWL 7.9 10.0 48.1 8.5 11.2 120.3 NeuronAl 7.9 10.2 44.7 8.6 11.1 109.9 Wanda Uniform 8.8 11.2 72.1 8.4 11.5 146.0 DSnoT 8.5 11.0 66.4 8.3 11.4 131.3 OWL 7.6 9.8 47.6 7.5 10.2 98.0 NeuronAl 7.6 9.9 46.8 7.6 10.3 90.3 A.2.5 NeuronAl on SparseGPT In Tables 14-16, we present the results of NeuronAl on SparseGPT on the WikiText2, C4, and PTB datasets, using the block-only setting666Since SparseGPT prunes and updates the weights from columns to rows, the row-wise step of NeuronAl cannot be included: indeed, it would force each row to have a different sparsity ratio, which is in contrast with the nature of SparseGPT.. Table 14: Perplexity on WikiText2 using SparseGPT. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 15.8 10.4 10.2 9.4 13.4 OWL 15.8 9.4 9.2 9.1 14.2 NeuronAl 15.7 9.9 9.3 9.1 13.7 70% Uniform 28.9 27.3 27.3 22.0 20.5 OWL 27.7 19.2 20.5 18.6 21.6 NeuronAl 27.3 22.6 20.9 17.8 21.8 80% Uniform 131.0 207.0 122.1 98.4 95.7 OWL 107.5 93.8 84.3 77.2 80.8 NeuronAl 113.5 144.7 88.7 70.8 84.0 Table 15: Perplexity on C4 using SparseGPT. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 19.0 12.8 12.9 13.0 15.3 OWL 19.2 11.7 11.6 12.4 15.8 NeuronAl 19.1 12.4 11.7 12.3 15.5 70% Uniform 28.6 28.3 31.5 27.8 22.4 OWL 28.2 21.1 22.8 23.7 22.4 NeuronAl 27.8 23.8 22.5 21.9 22.2 80% Uniform 98.7 136.2 104.8 86.5 72.5 OWL 79.7 68.3 73.4 66.2 65.4 NeuronAl 86.4 104.2 72.4 61.8 65.0 Table 16: Perplexity on PTB using SparseGPT. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 28.7 19.5 430.5 73.7 20.3 OWL 29.3 16.9 262.1 70.9 21.0 NeuronAl 28.2 18.2 249.2 67.2 20.6 70% Uniform 50.3 52.6 3780.0 153.2 32.0 OWL 51.0 37.0 1643.4 135.0 32.9 NeuronAl 47.3 40.5 861.6 123.4 32.8 80% Uniform 195.4 295.6 3201.7 316.2 102.3 OWL 141.4 162.3 5556.5 278.8 98.9 NeuronAl 156.7 260.2 3659.8 266.6 105.3 Using SparseGPT, the superiority of NeuronAl is less evident than with other pruning algorithms. Nevertheless, NeuronAl turns out to be the best-performing top-up algorithm in 5 out of 15, 8 out of 15, and 7 out of 15 cases, respectively for WikiText2, C4, and PTB. Interestingly, for lower sparsity, the gap between uniform and non-uniform distribution (both NeuronAl and OWL) is less remarkable than at higher sparsity. We explain these results with the inherent functioning of SparseGPT, which, differently from the other pruning algorithms, includes a weight reconstruction step. However, we can conclude that also in this case, our proposed approach turns out to be effective in many cases at increasing the task performance. A.3 Block-only and Row-only Ablation To complement the results presented in the main text on the C4 dataset, Tables 17-18 show the results of the ablation over the block-only and row-only steps also on WikiText2 and PTB. The results are in line with those obtained on C4. Table 17: Ablation study: perplexity achieved when using the block and row steps alone over the WikiText2 dataset. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 25.8 10.7 10.8 11.3 15.2 NeuronAl block-only 25.5 9.4 9.2 9.9 15.2 NeuronAl row-only 25.8 11.2 11.4 11.3 16.5 NeuronAl 25.2 9.6 9.4 9.9 16.3 70% Uniform 227.6 85.1 78.0 60.7 157.5 NeuronAl block-only 108.2 20.3 21.9 29.3 199.1 NeuronAl row-only 200.4 85.3 75.5 60.7 116.8 NeuronAl 89.9 21.5 23.9 29.0 183.9 80% Uniform 20498.5 5219.5 4929.7 330.9 4259.6 NeuronAl block-only 5637.9 458.9 439.9 205.9 1193.0 NeuronAl row-only 11382.1 6715.8 916.8 330.9 4259.6 NeuronAl 3873.6 376.1 213.9 226.3 988.2 Table 18: Ablation study: perplexity achieved when using the block and row steps alone over the PTB dataset. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 48.9 24.0 122.2 101.6 23.7 NeuronAl block-only 41.9 16.7 65.6 74.6 23.5 NeuronAl row-only 49.3 22.2 114.5 101.6 26.6 NeuronAl 42.1 17.4 65.1 74.6 25.2 70% Uniform 346.2 157.0 599.3 298.3 209.2 NeuronAl block-only 152.5 41.4 209.7 196.9 250.0 NeuronAl row-only 345.6 148.0 611.8 298.3 170.8 NeuronAl 133.9 43.8 205.3 197.8 194.7 80% Uniform 31362.6 10013.6 5285.2 783.7 2732.1 NeuronAl block-only 6253.3 684.1 1295.2 723.0 1335.6 NeuronAl row-only 18270.0 10008.4 2019.1 783.7 2732.1 NeuronAl 6675.9 566.3 993.2 698.2 988.7 A.4 NeuronAl \lambda Selection In the main text, we presented an experiment regarding the ability of NeuronAl to pick the most performing \lambda parameters (in the block-only cases) at 70% sparsity. Here we include the same analysis at 60% and 80% sparsity. In Fig.s 9-10, it is clearly visible how NeuronAl still performs correctly over different sparsity ratios. It is also worth noticing that the calibration data always come from the C4 dataset and then the results are transferred to the other unknown datasets. Figure 9: Perplexity over different values of \lambda at 60 % sparsity. The orange dot indicates the value selected by NeuronAl. The red line corresponds to the perplexity using a uniform distribution, hence \lambda=0. Figure 10: Perplexity over different values of \lambda at 80 % sparsity. The orange dot indicates the value selected by NeuronAl.The red line corresponds to the perplexity using a uniform distribution, hence \lambda=0. A.5 Calibration Data Ablation In Tables 20-20, we complement the results regarding the seed set of the calibration data at 60% and 80% sparsity. The results are fully in line with the ones presented in the main text. As expected, the standard deviation of the performance increases when increasing the sparsity ratio and at higher sparsity (80%) it turns out to be model dependent. We also conduct further experiments about the size of the calibration data (|C_{\lambda}|), see Fig. 11. Specifically, We compute the perplexity on WikiText2, C4, and PTB using different calibration data of different sizes (1, 2, 4, 8, 16 samples) in order to assess if the performance could be related to it. As clearly visible for almost all models, the performance is basically constant across the different calibration sizes. This does not apply only to OPT models, where for |C_{\lambda}|=16 we reach the best performance by a large margin compared to |C_{\lambda}|=8 (which is the standard value of |C_{\lambda}| used in all the experiments), especially on WikiText2 and PTB. This, as pointed out in the main text, can better explain the reason why NeuronAl is able to outperform all the top-up competitors in Language Modeling but OPT. Table 19: Ablation study: perplexity achieved by NeuronAl with different calibration data seeds (0, 16, 46) at 60% sparsity. Dataset Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B WikiText2 24.5 \pm 0.6 09.5 \pm 0.1 9.3 \pm 0.0 10.1 \pm 0.1 16.2 \pm 0.1 C4 27.0 \pm 0.2 11.9 \pm 0.1 11.9 \pm 0.0 13.8 \pm 0.1 19.1 \pm 0.1 PTB 42.3 \pm 0.7 17.1 \pm 0.3 65.3 \pm 0.8 74.9 \pm 0.2 25.1 \pm 0.1 Table 20: Ablation study: perplexity achieved by NeuronAl with different calibration data seeds (0, 16, 46) at 80% sparsity. Dataset Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B WikiText2 03654.7 \pm 255.1 382.4 \pm 064.7 0247.7 \pm 29.4 216.5 \pm 12.6 1284.9 \pm 482.5 C4 72323.6 \pm 121.7 250.5 \pm 027.1 0265.3 \pm 34.1 171.7 \pm 09.9 0663.5 \pm 316.3 PTB 06014.9 \pm 788.3 624.4 \pm 165.5 1101.9 \pm 94.1 706.1 \pm 06.9 1056.9 \pm 124.5 Figure 11: Perplexity over different values of |C_{\lambda}| (size of the calibration data) when using NeuronAl on the 3 Language Modeling datasets at 70% sparsity. A.6 NeuronAl \lambda Parameters Here we show the \lambda parameters selected by our proposed approach for each model, sparsity ratio, and pruning algorithm tested in this work, aiming to facilitate the reproducibility of our results for the community. Please note that such values are the ones used for each combination sparsity-pruning algorithm-model that have been extracted from C_{\lambda} from C4 (using 0 as seed and 8 as size), and then transferred to all the other datasets/tasks. We report the final \lambda values for both the block and row steps in Table 22 for the first 5 models tested in the main text, and in Table 22 for the LLama-13B models. Table 21: \lambda parameters selected by NeuronAl at sparsity xxx (block | row) for each combination sparsity-pruning algorithm-model. Note that for SparseGPT the row step is not possible (see the main text for details). Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Magnitude 0.01 | 0.25 0.10 | 0.20 0.20 | 0.04 0.15 | 0.20 0.25 | 0.25 Wanda 0.10 | 0.25 0.09 | 0.25 0.12 | 0.20 0.09 | 0.00 0.01 | 0.15 multiflow 0.08 | 0.25 0.12 | 0.20 0.12 | 0.20 0.08 | 0.00 0.01 | 0.08 SparseGPT 0.01 | 0.25 0.02 | 0.25 0.06 | 0.25 0.09 | 0.02 | 0.25 70% Magnitude 0.06 | 0.25 0.20 | 0.20 0.25 | 0.00 0.20 | 0.25 0.15 | 0.20 Wanda 0.12 | 0.25 0.15 | 0.20 0.15 | 0.25 0.15 | 0.25 0.25 | 0.20 multiflow 0.15 | 0.25 0.15 | 0.25 0.12 | 0.25 0.15 | 0.00 0.25 | 0.20 SparseGPT 0.02 | 0.25 0.04 | 0.25 0.08 | 0.25 0.08 | 0.25 0.05 | 0.25 80% Magnitude 0.01 | 0.20 0.03 | 0.10 0.01 | 0.25 0.15 | 0.20 0.20 | 0.08 Wanda 0.20 | 0.20 0.15 | 0.20 0.15 | 0.25 0.15 | 0.25 0.20 | 0.09 multiflow 0.12 | 0.25 0.15 | 0.15 0.15 | 0.25 0.15 | 0.20 0.20 | 0.05 SparseGPT 0.06 | 0.25 0.02 | 0.25 0.07 | 0.25 0.08 | 0.25 0.07 | 0.25 Table 22: \lambda parameters selected by NeuronAl on the each combination sparsity-pruning algorithm for Llama-13B (V1 and V2) at sparsity xxx (block | row). Sparsity Top-Up Model LLama1 13B LLama2 13B 60% Magnitude 0.10 | 0.25 0.12 | 0.12 Wanda 0.10 | 0.20 0.09 | 0.25 multiflow 0.15 | 0.20 0.12 | 0.20 70% Magnitude 0.12 | 0.25 0.25 | 0.25 Wanda 0.15 | 0.25 0.12 | 0.20 multiflow 0.15 | 0.15 0.15 | 0.25 80% Magnitude 0.25 | 0.05 0.25 | 0.12 Wanda 0.25 | 0.12 0.15 | 0.25 multiflow 0.25 | 0.10 0.04 | 0.25"
https://arxiv.org/html/2411.07050v1,FedCVD: The First Real-World Federated Learning Benchmark on Cardiovascular Disease Data,"Cardiovascular diseases (CVDs) are currently the leading cause of death worldwide, highlighting the critical need for early diagnosis and treatment. Machine learning (ML) methods can help diagnose CVDs early, but their performance relies on access to substantial data with high quality. However, the sensitive nature of healthcare data often restricts individual clinical institutions from sharing data to train sufficiently generalized and unbiased ML models. Federated Learning (FL) is an emerging approach, which offers a promising solution by enabling collaborative model training across multiple participants without compromising the privacy of the individual data owners. However, to the best of our knowledge, there has been limited prior research applying FL to the cardiovascular disease domain. Moreover, existing FL benchmarks and datasets are typically simulated and may fall short of replicating the complexity of natural heterogeneity found in realistic datasets that challenges current FL algorithms. To address these gaps, this paper presents the first real-world FL benchmark for cardiovascular disease detection, named FedCVD. This benchmark comprises two major tasks: electrocardiogram (ECG) classification and echocardiogram (ECHO) segmentation, based on naturally scattered datasets constructed from the CVD data of seven institutions. Our extensive experiments on these datasets reveal that FL faces new challenges with real-world non-IID and long-tail data. The code and datasets of FedCVD are available https://github.com/SMILELab-FL/FedCVD.","Figure 1: The overall architecture of the proposed FedCVD benchmark. We present two main settings (Fed-ECG, Fed-ECHO) and an experimental platform, highlighting three primary challenges. Green and blue circles in the challenges section indicate their presence in Fed-ECG and Fed-ECHO, respectively. The API section highlights user-facing APIs in orange boxes. Cardiovascular Diseases (CVDs) cause over 18 million deaths globally each year, positioning them as one of the most significant global health challenges [1]. Early detection and accurate diagnosis of CVDs are crucial, as they allow for timely medical interventions and more effective treatment plans, which in turn significantly lower patient mortality rates [2]. In recent years, with the growing availability of electronic health records and other high-quality clinical data, researchers have increasingly utilized machine learning techniques to automate clinical diagnostics [3; 4], a strategy that has proven highly effective in the context of CVDs [5; 6]. This data-driven approach not only facilitates efficient early screening but also optimizes the allocation of healthcare resources, improving overall patient outcomes. Compared to models trained in isolation at a single center, collaborations across multiple medical institutions enable the utilization of richer regional and demographic characteristics, fostering more precise and comprehensive research outcomes. However, medical data is considered highly sensitive, and recent privacy regulations (e.g., EU General Data Protection Regulation (GDPR) [7]) restrict its transfer, hindering the expansion of datasets through data sharing among institutions to train more efficient models, i.e., data isolation. To address this issue, federated learning (FL) [8; 9] has been proposed as a more secure paradigm of distributed machine learning. A typical FL architecture involves a coordinator (Server) and several participants (Clients) with private data. By aggregating (e.g., FedAvg [9]) the model parameters or gradients from different Clients on the Server, participants collaboratively train high-performance models keeping private data within their respective domains. This process only involves transmitting model parameters, thus ensuring a certain degree of data privacy. In medical applications such as CVD, integrating FL enables medical institutions to harness larger and more diverse datasets, collaboratively training models that are more unbiased and generalized, thereby enhancing diagnostic accuracy and clinical decision-making in real-world settings. For instance, [10] applied FL for joint case analysis across three institutions during the Covid-19 pandemic, significantly improving CT segmentation performance and facilitating more accurate detection of Covid-19. Additionally, the effectiveness of FL has been demonstrated in multi-center research, such as a study involving three centers focused on the medical image analysis task of whole prostate segmentation [11], further underscoring its relevance in realistic, large-scale medical scenarios. Facilitating the application of FL in multi-center medical research, particularly in areas such as CVD, necessitates the creation of appropriate datasets and benchmarks to support the development of robust algorithms. However, publicly available cardiovascular disease datasets are limited, and those that do exist often suffer from incompatibility due to variations in data collection protocols. Furthermore, there is currently no comprehensive, publicly accessible benchmark specifically designed for FL on CVD data, which significantly impedes research progress in this domain. Additionally, most existing FL benchmarks simulate an FL scenario by manually partitioning data—often without considering geographic distribution—into smaller subsets, resulting in an overly idealized model that fails to capture the complexities and heterogeneity of real-world, multi-center CVD scenarios. This gap presents substantial challenges for the development and validation of effective FL algorithms in practical medical applications. To address these gaps, we introduce the first multi-center FL benchmark specifically designed for CVD tasks, named FedCVD. Built from real-world CVD data collected from seven medical institutions (i.e., clients, the two terms will be used interchangeably), FedCVD utilizes a natural partitioning strategy. It comprises two primary datasets along with their corresponding tasks: electrocardiogram (ECG) classification and echocardiogram (ECHO) segmentation. FedCVD encapsulates three critical traits of FL in real-world CVD applications, each of which presents substantial challenges to FL algorithms: Challenging Trait 1. Non-IID Data: The Non-independently and identically distributed (non-IID) data among institutions, including non-IID feature (e.g., variations in imaging quality due to different equipment across institutions) and non-IID label (e.g., differences in disease prevalence across regions). The non-IID data may significantly hindering global model convergence. Challenging Trait 2. Long-tail Distribution: The labels of CVD data from various institutions exhibit a long-tailed distribution, where a few labels dominate while most labels are sparse. This challenges the model’s performance on tail classes, a problem that is exacerbated in FL scenarios. Challenging Trait 3. Label Incompleteness: For the same type of medical images, hospitals with strong annotation capabilities can identify all key segmentation areas, while those with weaker capabilities can identify only some. This incomplete annotation can mislead the global model’s segmentation performance in areas unrecognized by certain institutions. Focusing on these challenging traits, FedCVD provides new insights and evaluation metrics for designing FL algorithms in multi-center CVD scenarios. Our contributions are summarized as follows: 1. We introduce FedCVD, an open-source federated multi-center healthcare dataset and benchmark specifically for the CVD domain. To the best of our knowledge, FedCVD is the largest multi-center CVD benchmark available. This dataset encompasses two critical tasks—multi-label classification and segmentation—within the CVD domain and includes data of varying scales. Crucially, all datasets are partitioned using natural splits. 2. Our benchmark emphasizes three critical traits in the FL-CVD scenario: non-IID, long tail, and label incompleteness. These traits pose significant challenges to existing FL algorithms. 3. We conducted extensive experiments on FedCVD to evaluate the performance of mainstream FL and centralized learning methods, validating the effectiveness of FL in the CVD context and the proposed three challenges. Additionally, we have made the open-source code in https://github.com/SMILELab-FL/FedCVD accessible for benchmark reproducibility and easy integration into different FL frameworks."
https://arxiv.org/html/2411.07042v1,Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in AI Companion Applications,"Content Warning: This paper presents textual examples that may be offensive or upsetting.AI companions based on large language models can role-play and converse very naturally. When value conflicts arise between the AI companion and the user, it may offend or upset the user. Yet, little research has examined such conflicts. We first conducted a formative study that analyzed 151 user complaints about conflicts with AI companions, providing design implications for our study. Based on these, we created Minion, a technology probe to help users resolve human-AI value conflicts. Minion applies a user-empowerment intervention method that provides suggestions by combining expert-driven and user-driven conflict resolution strategies. We conducted a technology probe study, creating 40 value conflict scenarios on Character.AI and Talkie. 22 participants completed 274 tasks and successfully resolved conflicts 94.16% of the time. We summarize user responses, preferences, and needs in resolving value conflicts, and propose design implications to reduce conflicts and empower users to resolve them more effectively.","Human-AI conflict refers to a state of incompatibility, inconsistency, or opposition between humans and AI (Flemisch et al., 2020). In past research, human-AI conflicts were usually simple and direct—AI was more like a tool, and conflicts often stemmed from technical limitations, such as task execution failures (Wen et al., 2022), or disagreements with users in simple decision-making (Babel et al., 2024; Takayama et al., 2009). These types of conflicts generally lacked emotional and value entanglement, making them less likely to cause significant psychological harm to users. Recently, a diverse array of Large Language Model (LLM) agents has emerged, offering capabilities ranging from personalized assistance to performing complex tasks (Chen et al., 2024). The study focuses on LLM-based AI companion applications, such as Character.AI, Talkie, Replika, Kindroid, Paradot, and Xingye. As of July 2024, the total number of users of these applications has exceeded 900 million globally (including duplicate users across different applications)111User statistics source: https://www.data.ai.. AI companions can role-play and respond to users in a human-like manner, providing emotional support and companionship (Sullivan et al., 2023). For instance, in Character.AI, users can personalize the companion’s personality traits and interaction contexts through “Description,” “Greeting,” and “Definition.” Compared to earlier non-LLM chatbots, LLMs endow AI companions with a stronger ability to understand language, enabling them to engage in more context-aware and intelligent interactions (Kasneci et al., 2023), fostering more complex and intimate human-AI relationships (Maeda and Quan-Haase, 2024). Many users even consider them close friends or lovers (Skjuve et al., 2021, 2022). The deepening of this relationship raises users’ expectations of AI companions, but it may also lead to deeper conflicts, including value conflict. For example, some users have shared online their experiences of encountering sexist remarks from AI companions, describing how they engaged in intense arguments with the AI, which left them frustrated, angry, and hurt (Zhou, 2023). As the relationship between AI companions and users becomes more interpersonal, previous conflict resolution strategies for human-AI conflict have started to fail (Rosen, 2014; Babel et al., 2024). Strategies based solely on technical limitations are no longer sufficient, and it is becoming important to draw on interpersonal conflict resolution methods and users’ real-world experiences with AI companions. Although Fan et al. provide initial insights into value alignment and conflicts between users and AI companions (Fan et al., 2024), inexperienced users often find it challenging to resolve these issues independently. How to design tools that empower users to handle value conflicts with AI companions remains an unexplored research gap that this work aims to address. In this work, we first conducted a formative study to understand and characterize the value conflicts between users and AI companions (Schwartz, 2012). We analyzed 151 user complaint posts from social media platforms, finding that many conflicts are value-laden. Building on this, we constructed a value conflict framework for AI companion applications (Schwartz, 2012), which provided real-world data for our technology probe study, allowing us to reconstruct actual value conflict scenarios. Combining prior research on conflict resolution (Brett et al., 1998; Shaikh et al., 2024; Mun et al., 2023) with our formative study, we found that interactions between users and AI companions exhibit complex dynamics, where relying solely on expert strategies from other conflict scenarios (e.g., interpersonal conflict theories (Brett et al., 1998)) is insufficient. The value conflicts users face in real-life situations are diverse, and through their interactions with AI companions and exchanges on social platforms, users have accumulated certain conflict resolution experiences. Therefore, it is necessary to draw from both expert theories and the practical experiences of AI companion users to explore more suitable solutions (Fan et al., 2024). Then, we created Minion, a technology probe (Hutchinson et al., 2003) that provides users multiple suggestions for resolving value conflicts while gaining insights into user behaviors. Minion’s algorithm combines expert-driven and user-driven conflict resolution strategies. We developed LLM prompts to address value conflict situations between users and AI companions by drawing on two key sources. First, we drew upon Shaikh et al.’s solutions for interpersonal conflict resolution (Shaikh et al., 2024) to guide our expert-driven conflict resolution strategies. Second, we referenced the user-driven strategies identified in the study by Fan et al. (Fan et al., 2024) to capture how users manage conflicts with AI companions. To empirically test Minion, we conducted a technology probe study (Hutchinson et al., 2003) with 22 participants. We created 40 distinct conflict scenarios on two popular AI companion platforms, Character.AI and Talkie. Each scenario was designed with specific conflict resolution goals. Participants completed 274 tasks, achieving an overall conflict resolution rate of 94.16%. Minion received positive feedback from participants and inspired them with new ideas in conflict resolution. Based on our findings, we discuss the opportunities and challenges in integrating expert-driven and user-driven strategies in resolving human-AI value conflicts, and call for further research in this area, focusing on the dynamics of emerging human-AI relationships. Our work’s contributions are as follows: • A novel user-empowerment intervention method that combines expert-driven and user-driven conflict resolution strategies. This method is presented in the form of the technology probe Minion, serving as a prototype for future tools aimed at resolving human-AI value conflicts. • We empirically tested Minion in a one-week technology probe study (N=22). The results demonstrated the technical feasibility of Minion. We summarized users’ responses, preferences, and needs when dealing with value conflicts with AI companions. • Based on the formative and technology probe studies, we explored the opportunities and challenges of integrating expert-driven and user-driven strategies in human-AI value conflicts. We also proposed design implications for future human-AI conflict resolution solutions, particularly in the field of AI companions."
https://arxiv.org/html/2411.07019v1,UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction,"Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, existing link prediction models are usually designed for one specific type of facts, making it difficult to generalize to other fact representations. To overcome this limitation, we propose a Unified Hierarchical Representation learning framework (UniHR) for unified knowledge graph link prediction. It consists of a unified Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing the semantic information within individual facts and enriching the structural information between facts. Experimental results across 7 datasets from 3 types of KGs demonstrate that our UniHR outperforms baselines designed for one specific kind of KG, indicating strong generalization capability of HiDR form and the effectiveness of HiSL module. Code and data are available at https://github.com/Lza12a/UniHR.","Large-scale knowledge graphs (KGs) such as WordNet Miller (1995), Freebase Bollacker et al. (2008), and Wikidata Vrandečić and Krötzsch (2014) have been widely applied in many areas like question answering Kaiser et al. (2021), recommendation systems Guo et al. (2020), and natural language processing Annervaz et al. (2018). However, the presence of missing facts within these KGs inevitably limit their applications. Therefore, the link prediction task has been introduced to predict missing elements within factual data. Current link prediction methods mainly focus on facts in the form of triple \left(head\,entity,relation,tail\,entity\right). Figure 1: A special KG consists of triple-based fact, hyper-relational fact, nested fact and temporal fact. Despite the simplicity and unity of triple-based representation, it is difficult to adequately express complex facts, such as “Oppenheimer is educated at Harvard University for a bachelor degree in chemistry"" shown in Figure 1. Therefore, existing researches Wang et al. (2021); Xiong et al. (2024); Xu et al. (2019) contribute to focusing on semantically richer facts. Figure 1 illustrates three specific types of facts: hyper-relational fact ((Oppenheimer, educated at, Harvard University), degree: bachelor, major: chemistry), temporal fact (Oppenheimer, honored with, Fermi Prize, 1963), nested fact ((Oppenheimer, born in, New York), imply, (Oppenheimer, nationality, The United States)). These forms of facts allow for expression of complex semantics and revelation of relationships between facts, extending beyond the triple-based representation. Thus in recent years, Hyper-relational KGs (HKG) Chung et al. (2023), Temporal KGs (TKG) Xu et al. (2023a), and Nested factual KGs (NKG) Xiong et al. (2024) attract wide research interests. Recent studies have demonstrated the effectiveness of various embedding strategies for these beyond-triple representations Xiong et al. (2023). However, these methods are usually designed for specific representation forms, e.g., StarE Galkin et al. (2020) customizes graph neural network to implement message passing on hyper-relational facts, For nested factual KGs, BiVE Chung and Whang (2023) connects two levels of facts throgh a simple linear layer. In addition, GeomE+ Xu et al. (2023a) et al. temporal KG embedding methods contain time-aware scoring functions to adapt timestamps. Although these methods perform well on specific type of facts, it is evident that such customized methods are difficult to generalize to other types of KGs. Therefore, establishing a unified representation learning method for multiple types of KGs is worth to investigate. To overcome the challenges mentioned above, we propose a Unified Hierarchical Representation learning method (UniHR), which includes a Hierarchical Data Representation (HiDR) module and a Hierarchical Structure Learning (HiSL) module as the graph encoder. HiDR module standardizes hyper-relational facts, nested factual facts, and temporal facts into the form of triples without loss of information. Furthermore, HiSL module captures local semantic information during intra-fact message passing and then utilizes inter-fact message passing to enrich the global view of nodes to obtain better node embeddings based on HiDR form. Finally, the updated embeddings are fed into decoders for link prediction. Experimental results demonstrate that our UniHR achieves state-of-the-art performance on HKG and NKG datasets, and competitive performance on TKG datasets, revealing strong generalization capability of HiDR form and effectiveness of HiSL module. Our contributions can be summarized as follows: 1. We emphasize the value of investigating unified KG representation method, including unified symbolic representation and unfied representation learning method for different KGs. 2. We propose the first unified KG representation learning framework UniHR, across different types of KGs, including a hierarchical data representation module and a hierarchical structure learning module. 3. We conduct link prediction experiments on 7 datasets across 3 types of KGs. Compared to methods designed for one kind of KG, UniHR achieves the best or competitive results, verifying strong generalization capability."
https://arxiv.org/html/2411.07015v1,"Leveraging LSTM for Predictive
Modeling of Satellite Clock Bias","Satellite clock bias prediction plays a crucial role in enhancing the accuracy of satellite navigation systems. In this paper, we propose an approach utilizing Long Short-Term Memory (LSTM) networks to predict satellite clock bias. We gather data from the PRN 8 satellite of the Galileo and preprocess it to obtain a single difference sequence, crucial for normalizing the data. Normalization allows resampling of the data which ensures that the predictions are equidistant and complete. Our methodology involves training the LSTM model on varying lengths of datasets, ranging from 7 days to 31 days. We employ a training set consisting of two days’ worth of data in each case. Our LSTM model exhibits exceptional accuracy, with a Root Mean Square Error (RMSE) of 2.11\times 10^{-11}. Notably, our approach outperforms traditional methods which are used for similar time-series forecasting projects, being 170 times more accurate than RNN, 2.3\times 10^{7} times more accurate than MLP, and 1.9\times 10^{4} times more accurate than ARIMA. This study holds significant potential in enhancing the accuracy and efficiency of low-power receivers used in various devices, particularly those requiring power conservation. By providing more accurate predictions of satellite clock bias, the findings of this research can be integrated into the algorithms of such devices, enabling them to function with heightened precision while conserving power. Improved accuracy in clock bias predictions ensures that low-power receivers can maintain optimal performance levels, thereby enhancing the overall reliability and effectiveness of satellite navigation systems. Consequently, this advancement holds promise for a wide range of applications, including in remote areas, IoT devices, wearable technology, and other devices where power efficiency and navigation accuracy are paramount.","Satellite clock bias prediction is an important aspect of modern Satellite Navigation Systems, influencing the accuracy of location-based services and navigation systems. Accurate prediction of satellite clock bias is crucial for ensuring precise positioning, navigation, and timing in various applications, including aviation, maritime, and terrestrial navigation. Traditional methods for satellite clock bias prediction often face challenges in capturing the complex temporal dependencies inherent in the data. In recent years, the advent of deep learning techniques, particularly Long Short-Term Memory (LSTM) networks, has offered promising avenues for improving the accuracy and efficiency of satellite clock bias prediction. LSTM networks excel in capturing long-term dependencies in sequential data, making them well-suited for modeling the temporal dynamics of satellite clock bias. According to multiple studies (Huang et al, 2021[2] and He et al, 2023 [3]) the most suitable method to detect trend terms is through Median Absolute Deviation (MAD). However, we find that since the data is already standardized through a single difference sequence, there is no need to apply Median Absolute Deviation. The data can be easily resampled through the integrated functions in one of the Python libraries i.e. numpy, resulting in faster processing and more accurate results. In this paper, we present an approach to satellite clock bias prediction utilizing LSTM networks. We aim to leverage the capabilities of LSTM networks to effectively capture the intricate temporal patterns present in satellite clock bias data obtained from the PRN 2 satellite of the Galileo GNSS. Our methodology involves preprocessing the data to obtain a single difference sequence, crucial for normalizing the data to improve prediction accuracy and resampling it for a uniform prediction interval. Furthermore, we investigate the impact of training data length on prediction performance, considering datasets ranging from 7 days to 31 days in duration. By training the LSTM model on varying lengths of data and evaluating its performance, we seek to determine the optimal testing duration for achieving the highest prediction accuracy. The testing duration is an important factor because LSTM is a recurrent model. Additionally, we compare the performance of our LSTM-based approach with traditional methods such as Recurrent Neural Networks (RNN), Multi-Layer Perceptrons (MLP), and Autoregressive Integrated Moving Average (ARIMA) models. By quantitatively assessing the accuracy of each method through metrics such as Root Mean Square Error (RMSE), we aim to demonstrate the superiority of LSTM networks in satellite clock bias prediction. Overall, our research aims to contribute to the advancement of satellite navigation systems by providing a robust and accurate method for predicting satellite clock bias, thereby enhancing the reliability and precision of satellite-based positioning and navigation applications."
https://arxiv.org/html/2411.07007v1,"Non-Adversarial Inverse Reinforcement 
Learning via Successor Feature Matching","In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment. Traditionally, IRL is treated as an adversarial game, where an adversary searches over reward models, and a learner optimizes the reward through repeated RL procedures. This game-solving approach is both computationally expensive and difficult to stabilize. In this work, we propose a novel approach to IRL by direct policy optimization: exploiting a linear factorization of the return as the inner product of successor features and a reward vector, we design an IRL algorithm by policy gradient descent on the gap between the learner and expert features. Our non-adversarial method does not require learning a reward function and can be solved seamlessly with existing actor-critic RL algorithms. Remarkably, our approach works in state-only settings without expert action labels, a setting which behavior cloning (BC) cannot solve. Empirical results demonstrate that our method learns from as few as a single expert demonstration and achieves improved performance on various control tasks.222Our codebase is available at https://github.com/arnavkj1995/SFM","Figure 1: Comparing Mean Normalized Return on 10 tasks from DMC (Tassa et al., 2018) suite of our method SFM against the offline method BC (Pomerleau, 1988), the non-adversarial IRL method IQ-Learn (Garg et al., 2021), and the state-only adversarial methods MM (Swamy et al., 2021) and GAIfO (Torabi et al., 2018), where the agents are provided a single expert demonstration. Our state-only non-adversarial method SFM achieves higher performance as measured by the Mean Normalized Return. Error bars show the 95% bootstrap CIs. In imitation learning (Abbeel & Ng, 2004; Ziebart et al., 2008; Silver et al., 2016; Ho & Ermon, 2016; Swamy et al., 2021), the goal is to learn a decision-making policy that reproduces behavior from demonstrations. Rather than simply mimicking the state-conditioned action distribution as in behavior cloning (Pomerleau, 1988), interactive approaches like Inverse Reinforcement Learning (IRL; Abbeel & Ng, 2004; Ziebart et al., 2008) have the more ambitious goal of synthesizing a policy whose long-term occupancy measure approximates that of the expert demonstrator by some metric. As a result, IRL methods have proven to be more robust, particularly in a regime with few expert demonstrations, and has lead to successful deployments in real-world domains such as autonomous driving (Bronstein et al., 2022; Vinitsky et al., 2022; Igl et al., ). However, this robustness comes at a cost: approaches to IRL tend to involve a costly bi-level optimization. Specifically, modern formulation of many IRL methods (e.g., Garg et al., 2021; Swamy et al., 2021) involve a min-max game between an adversary that learns a reward function to maximally differentiate between the agent and expert in the outer loop and a Reinforcement learning (RL) subroutine over this adversarial reward in the inner loop. However, all such methods encounter a set of well-documented challenges: (1) optimizing an adversarial game between the agent and the expert can be unstable, often requiring multiple tricks to stabilize training (Swamy et al., 2022), (2) the inner loop of this bi-level optimization involves repeatedly solving a computationally expensive RL problem (Swamy et al., 2023), and (3) the reward function class must be specified in advance. Moreover, many approaches to imitation learning require knowledge of the actions taken by the demonstrator. This renders many forms of demonstrations unusable, such as videos, motion-capture data, and generally any demonstrations leveraging an alternative control interface than the learned policy (e.g., a human puppeteering a robot with external forces). As such, it is desirable to build IRL algorithms where the imitation policies learn from only expert states. These challenges lead us to the following research question: Can a non-adversarial approach to occupancy matching recover the expert’s behavior without action labels? To address this question, we revisit the earlier approaches to feature matching (Abbeel & Ng, 2004; Ziebart et al., 2008; Syed & Schapire, 2007; Syed et al., 2008), that is, matching the accumulation of discounted state or state-action base features along the expert’s trajectory. For this task, we propose to estimate expected cumulative sum of features using Successor Features (SF; Barreto et al., 2017) – a low-variance, fully online algorithm that employs temporal-difference based methods for learning. Leveraging the benefits of SF, we demonstrate that feature matching can be achieved by direct policy search via policy gradients. In doing so, we present a new approach to IRL, called Successor Feature Matching (SFM), which provides a remarkably simple algorithm for imitation learning. Interestingly, when the learned base features are action-independent, we show that SFM can imitate an expert without knowledge of the actions it took in its demonstrations. This accommodates a variety of expert demonstration formats, such as video and motion-capture, where action labels are naturally absent. Additionally, rather than manually pre-specifying a class of expert reward functions (Swamy et al., 2021), SFM adaptively learns this class from data using unsupervised RL techniques. Our experiments validate that SFM successfully learns to imitate from as little as a single expert demonstration. As a result, SFM outperforms its competitors by 16% on mean normalized returns across a wide range of tasks from the DMControl suite (Tassa et al., 2018) —as highlighted in Figure 1. To summarize, the contributions of this work are as follows: 1. Occupancy matching via reduction to reinforcement learning. In this work, we propose an algorithm for feature matching that can be achieved by direct policy search via policy gradients for inverse RL. In doing so, our method Successor Feature Matching (SFM)achieves strong imitation performance using any off-the-shelf RL algorithms. 2. Imitation from a single state-only demonstration. Our method learns with demonstrations without expert action labels by using state-only base features to estimate the SF. To our knowledge, SFM is the only online method capable of learning from a single unlabeled demonstration without requiring an expensive and difficult-to-stabilize bilevel optimization (Swamy et al., 2022)."
https://arxiv.org/html/2411.07003v1,Enhancing Robot Assistive Behaviour with Reinforcement Learning and Theory of Mind,"The adaptation to users’ preferences and the ability to infer and interpret humans’ beliefs and intents, which is known as the Theory of Mind (ToM), are two crucial aspects for achieving effective human-robot collaboration. Despite its importance, very few studies have investigated the impact of adaptive robots with ToM abilities. In this work, we present an exploratory comparative study to investigate how social robots equipped with ToM abilities impact user’s performance and perception. We design a two-layer architecture. The Q-learning agent on the first layer learns the robot’s higher-level behaviour. On the second layer, a heuristic-based ToM infers the user’s intended strategy and is responsible for implementing the robot’s assistance, as well as providing the motivation behind its choice. We conducted a user study in a real-world setting, involving 56 participants who interacted with either an adaptive robot capable of ToM, or with a robot lacking such abilities. Our findings suggest that participants in the ToM condition performed better, accepted the robot’s assistance more often, and perceived its ability to adapt, predict and recognise their intents to a higher degree. Our preliminary insights could inform future research and pave the way for designing more complex computation architectures for adaptive behaviour with ToM capabilities.","Cognitive stimulation is crucial for maintaining and improving abilities such as memory, attention, and executive function [59]. Regular engagement in activities that challenge and stimulate the brain has been shown to positively impact cognitive health and can delay the onset of age-related declines [23]. The use of socially assistive robots (SARs) in memory exercises [1, 36] has the potential to provide a personalised and engaging platform for delivering cognitive stimulation, offering a unique and interactive experience for users [29]. Indeed, robots have been proven to be very effective in performing simple, and repetitive tasks, making them a perfect tool to support healthcare professionals and enhance their effectiveness during their daily working routine [52]. Nonetheless, for robots to be most effective in providing assistance, they must cater to the specific needs of users and aim to prevent negative emotions such as frustration (due to overly difficult exercises) or boredom (due to overly simple exercises) during cognitive exercises. It is important that the robot’s approach aligns precisely with the user’s requirements to achieve optimal results. In recent years, several studies have demonstrated the impact of robot adaptivity on users’ performance as well as on their engagement in assistive tasks [7, 53, 21]. However, adaptation in the robot’s behaviour may result in affecting people’s ability to understand and predict the robot, impacting their trust. To avoid this, it has to be noted that humans are more likely to cooperate with machines with whom they can share a mental representation. Hence, the robot should be able to have a Theory of Mind (ToM) of the users over time and personalise its behaviour according to the inferred beliefs and intentions [45, 48, 49]. Despite those works, very little is known about how to effectively combine adaptivity with ToM to improve task performance and increase the user’s perceived competence of the robot [11]. Therefore, in this work, we aim to fill this research gap. We build upon our previous knowledge, wherein we demonstrated how a robot can tailor its degrees of assistance to patients affected by cognitive decline [5]. Here, we go a step further by endowing the robot with the capability to understand users’ strategies through a process of mentalising and, therefore, use that information to provide qualitatively better assistance. This study takes an exploratory approach to address the following research question through a comparative study: would a robot endowed with adaptive socially assistive behaviour and ToM abilities have a different impact on users’ performance and perceived robot capabilities compared to an adaptive robot without ToM abilities? Figure 1: A user playing the memory game with the assistance of the Furhat robot. To tackle this research question, we propose a computational approach in which a robot learns the socially assistive behaviour that best fits the users while providing advice that relies on the user’s beliefs and their intended strategies to solve a memory game (see Figure 1). Specifically, we create a two-layer architecture. The upper level, namely the learning layer, includes a Q-learning-based agent trained in simulation to model an imperfect player and learn the policy that best fits the user’s needs (e.g., suggest the card). The lower level, namely the mentalising layer, consists of a heuristic-based ToM that based on the previous history of users’ moves is used to estimate their strategies and their beliefs about the card’s selection. Here, ToM has employed both for operationalising the assistance provided by the RL (e.g., suggest shark) and explain the rationale behind its hint (“You have seen the shark several times, the other card is in row 1 col 2, you should remember the location”). To evaluate our system, we carried out a user study in a real-world setting. N=56 untrained participants played a memory game with the assistance of the Furhat robot during a national fair. We found that participants who were assisted by a robot capable of ToM performed better, accepted the hints provided by the robot more frequently, and perceived the robot as more capable of adapting, predicting, and recognising their intentions in comparison to those participants who interacted with a robot without ToM abilities. The findings of this study provide insights that can inform future research into the design of robots that exhibit adaptive behaviour tailored to users’ capacities, while also incorporating ToM capabilities. In summary, the contributions of our study are the following: • Development of a hierarchical architecture that learns: i) socially assistive actions in simulation along, with ii) user’s intended strategies in real interactions, • Deployment and evaluation of a social robot endowed with such architecture in a real-world setting with 56 untrained participants."
https://arxiv.org/html/2411.06989v1,Token2Wave,"This paper provides an in-depth analysis of Token2Wave, a novel token representation method derived from the Wave Network, designed to capture both global and local semantics of input text through wave-inspired complex vectors. In Token2Wave, each token is represented with a magnitude component, capturing the global semantics of the entire input text, and a phase component, encoding the relationships between individual tokens and the global semantics. Building on prior research that demonstrated the effectiveness of wave-like operations, such as interference and modulation, during forward propagation, this study investigates the convergence behavior, backpropagation characteristics, and embedding independence within the Token2Wave framework. A detailed computational complexity analysis shows that Token2Wave can significantly reduce video memory usage and training time compared to BERT. Gradient comparisons for the [CLS] token, total input text, and classifier parameters further highlight Token2Wave’s unique characteristics. This research offers new insights into wave-based token representations, demonstrating their potential to enable efficient and computationally friendly language model architectures.","Currently, there are two types of token embedding methods. The fixed token embedding, such as Skip-gram and Continuous Bag of Words (CBOW) [1], assign the same embedding vector to each token, which cannot adapt to the dynamic meanings of tokens in varying contexts. The context-dependent embedding, on the other hand, generates different embeddings for the same token depending on its contexts. Many current Natural Language Processing (NLP) methods, such as the Transformer [2], use the attention mechanism to update token embeddings by measuring relationships between tokens with dot products. However, attention only infers global semantics indirectly through pairwise relationships rather than directly capturing the overall meaning of the text. In our previous work [3], we introduced the Wave Network, a language model based on a new token representation method called Token2Wave. Token2Wave uses complex vector token representations to represent both the global and local semantics of each token with two parts: a magnitude vector representing the global semantics of the input text, and a phase vector capturing the relationships between individual tokens and global semantics. The complex vector token representations enables wave-like operations, such as interference and modulation for efficient updates. While the previous work focused on constructing token representations as waves and their forward propagation in text classification tasks, the current study delves into the architectural and functional aspects of the Wave Network. Here, we present a thorough analysis of the convergence performance, gradient behaviors of the network components (e.g., [CLS] embedding, overall input embedding, classifier), and the independence level among embedding dimensions. By focusing on these aspects, we aim to provide deeper insights into the theoretical details of the Wave Network and its potential effectiveness in various NLP tasks."
https://arxiv.org/html/2411.06965v1,Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation Learning with Single-Step Archive Exploration,"Learning diverse and high-performance behaviors from a limited set of demonstrations is a grand challenge. Traditional imitation learning methods usually fail in this task because most of them are designed to learn one specific behavior even with multiple demonstrations. Therefore, novel techniques for quality diversity imitation learning are needed to solve the above challenge. This work introduces Wasserstein Quality Diversity Imitation Learning (WQDIL), which 1) improves the stability of imitation learning in the quality diversity setting with latent adversarial training based on a Wasserstein Auto-Encoder (WAE), and 2) mitigates a behavior-overfitting issue using a measure-conditioned reward function with a single-step archive exploration bonus. Empirically, our method significantly outperforms state-of-the-art IL methods, achieving near-expert or beyond-expert QD performance on the challenging continuous control tasks derived from MuJoCo environments.","Imitation Learning (IL) aims to mimic an expert’s behavior by learning from the demonstrations. IL has achieved great success in many real-world applications, such as robotics [51], autonomous driving [6], and drone control [37]. However, most of the traditional imitation learning methods were designed to learn one specific behavior, even given multiple demonstrations. Learning diverse and high-quality policies is the ultimate goal of many real-world applications like robotic locomotion tasks [4]. Recent literature has demonstrated that quality diversity reinforcement learning (QDRL) [4, 44] is a promising direction for achieving this goal. Prior methods that combine Differentiable Quality Diversity (DQD) [14] with off-policy RL achieved diverse and relatively high-performance policies. However, the performance gap between standard RL and QDRL still exists [4]. More recently, Batra et al. [4] mitigate this gap by leveraging the on-policy RL method PPO [39] with DQD. PPO estimates the gradients of diversity and performance objectives from the online collected data. Then, the estimated gradients are used by DQD methods like CMA-MAEGA [15] that maintain a single search point and move through the behavior space by filling new regions. The synergy between PPO and DQD results in a state-of-the-art QDRL method, Proximal Policy Gradient Arborescence (PPGA) [4], that achieves the ultimate goal of robotic locomotion. Generally, this kind of QDRL method finds a diverse archive of high-performing locomotion behaviors for an agent by combining PPO gradient approximations with Differentiable Quality Diversity algorithms. However, the success of QDRL heavily relies on high-quality reward functions, which can be intractable in practice. Quality Diversity Imitation Learning (QDIL) offers a more flexible strategy for learning diverse and high-quality policies from demonstrations with diverse behaviors. In the literature, adversarial IL methods such as GAIL [23] have achieved great success in learning specific behaviors for robotic locomotion tasks. Therefore, a naive solution for QDIL is to apply adversarial IL to estimate rewards from the given demonstrations and then leverage the estimated rewards for learning diverse and high-quality policies. Unfortunately, adversarial IL methods suffer from the training instability issue, which usually results in worse-than-demonstrator performance [23]. Moreover, when the demonstrations only contain a few behaviors, the rewards learned by adversarial IL techniques will be behavior-overfitted and unable to guide the agent to learn more diverse behaviors beyond the demonstrations. Figure 1: Illustrations of the two issues of the Adversarial QDIL (i.e., training instability and behavior-overfitted reward) and their corresponding solutions (i.e. WQDIL and Single-Step Archive Exploration). \delta(s) means the Markovian Measure Proxy of state s, a.k.a. the single-step measure. The training instability issue and the behavior-overfitted reward issue heavily limit the adversarial QDIL to learning diverse and high-quality policies with limited demonstrations. In this work, we propose two synergic strategies to overcome these two challenging issues. The first strategy aims to stabilize the training of the reward model, and the second strategy focuses on encouraging the agent to do behavior space exploration. To develop the first strategy, we propose to stabilize the reward learning by applying Wasserstein adversarial training within the latent space of the Wasserstein Auto-Encoder (WAE) [46]. Similar to VAE [26], WAE keeps the good properties of stable training and a nice latent manifold structure while generating higher-quality images than GAN [46]. Therefore, we propose to apply WAE to enable a more stable training of reward model in adversarial QDIL. In addition, we propose latent Wasserstein adversarial training to further improve the consistency of the reward training stability. For the second strategy, we introduce a bonus that enables the agent to collect data with more diverse behaviors via single-step archive exploration. We call the resulting method Wasserstein Quality Diversity Imitation Learning (WQDIL) with Single-Step Archive Exploration (SSAE). Figure 1 illustrates the two issues of the Adversarial QDIL (i.e., training instability and behavior-overfitted reward) and the corresponding solutions (i.e. WQDIL and Single-Step Archive Exploration). The synergy between WQDIL and SSAE adherently address the two issues of adversarial QDIL, resulting in diverse and high-quality policies when learning with limited demonstrations. Figure 2 illustrates diverse behaviors induced by our learned policies. Figure 2: Illustration of diverse behaviors learned by our Quality Diversity Imitation Learning framework on Humanoid and Walker2d, where each column represents one behavior. The “left” and “‘right” means the proportion of time the left leg or right leg contacting the ground. We summarize our contributions as follows: • First, we indicate the two main issues of the naive QDIL solution, i.e., the training instability and the behavior-overfitted reward issues of the adversarial QDIL approach. • Second, we propose Wasserstein Quality Diversity Imitation Learning (WQDIL) to address the training instability issue by applying Wasserstein adversarial training within the latent space of the Wasserstein Auto-Encoder (WAE). • Third, to alleviate the behavior-overfitted reward issue, we design a measure-conditioned reward, which makes the reward function sensitive to the local measure space, together with a measure-based bonus, which encourages the agent to collect data with more diverse behaviors. • Empirically, we demonstrated that our method significantly outperforms state-of-the-art imitation learning methods in learning diverse and high-quality policies from limited demonstrations."
https://arxiv.org/html/2411.06959v1,ENAT: Rethinking Spatial-temporal Interactions in Token-based Image Synthesis,"Recently, token-based generation approaches have demonstrated their effectiveness in synthesizing visual content. As a representative example, non-autoregressive Transformers (NATs) can generate decent-quality images in just a few steps. NATs perform generation in a progressive manner, where the latent tokens of a resulting image are incrementally revealed step-by-step. At each step, the unrevealed image regions are padded with [MASK] tokens and inferred by NAT, with the most reliable predictions preserved as newly revealed, visible tokens. In this paper, we delve into understanding the mechanisms behind the effectiveness of NATs and uncover two important interaction patterns that naturally emerge from NAT’s paradigm: Spatially (within a step), although [MASK] and visible tokens are processed uniformly by NATs, the interactions between them are highly asymmetric. In specific, [MASK] tokens mainly gather information for decoding. On the contrary, visible tokens tend to primarily provide information, and their deep representations can be built only upon themselves. Temporally (across steps), the interactions between adjacent generation steps mostly concentrate on updating the representations of a few critical tokens, while the computation for the majority of tokens is generally repetitive. Driven by these findings, we propose EfficientNAT (ENAT), a NAT model that explicitly encourages these critical interactions inherent in NATs. At the spatial level, we disentangle the computations of visible and [MASK] tokens by encoding visible tokens independently, while decoding [MASK] tokens conditioned on the fully encoded visible tokens. At the temporal level, we prioritize the computation of the critical tokens at each step, while maximally reusing previously computed token representations to supplement necessary information. ENAT improves the performance of NATs notably with significantly reduced computational cost. Experiments on ImageNet-2562 & 5122 and MS-COCO validate the effectiveness of ENAT. Code and pre-trained models will be released at https://github.com/LeapLabTHU/ENAT.","Recent years have witnessed an unprecedented growth in the field of AI-generated content (AIGC). In computer vision, diffusion models [10, 59, 61] have emerged as an effective approach. On the contrary, within the context of natural language processing, content is typically synthesized via the generation of discrete tokens using Transformers [72, 19, 5, 55]. Such discrepancy has excited a growing interest in exploring token-based generation paradigms for visual synthesis [7, 85, 33, 87, 6, 35]. Different from diffusion models, these approaches utilize a discrete data format akin to language models. This makes them straightforward to harness well-established language model optimizations such as the refined scaling strategies [5, 54, 31, 73] and the progress in model infrastructure [65, 12, 8, 34, 96]. Moreover, explorations in this field may facilitate the development of more advanced, scalable multimodal models with a unified token space [17, 68, 18, 44, 90] as well as general-purpose vision foundation models that integrate visual understanding and generation capabilities [35, 69]. Figure 1: The generation process of NATs starts from a masked canvas, decode multiple tokens per step, and are then mapped to the pixel space using a pre-trained VQ-decoder [13]. The recent advances in token-based visual generation have seen the rise of non-autoregressive Transformers (NATs) [7, 33, 6, 53], which are distinguished by their abilities to fulfill efficient and high-quality visual synthesis. As shown111We illustrate with 4\times4 tokens for simplicity; the actual token map size may be 16\times16 or larger. in Figure 1, NATs follow a progressive generation paradigm: at each generation step, a certain number of latent tokens of the resulting image are decoded in parallel, and the model carries out this process iteratively to produce the final complete token maps. More specifically, at each step, the unknown latent tokens of the image are represented with [MASK] tokens and concatenated with the tokens that have been decoded (i.e., visible tokens). Then, the full set of [MASK] and visible tokens is fed into a Transformer-based model, predicting the proper values of the unknown tokens, with the most reliable predictions preserved as the increments of visible tokens for the next generation step. In this paper, we seek to advance the understanding of the mechanisms behind the effectiveness of NATs’ progressive generation procedures. Our investigation uncovers two important findings regarding the spatial and temporal interactions within NATs: Spatially, at each generation step, even though both [MASK] and visible tokens are treated equivalently within the computational graphs of NATs, the visible tokens naturally learn to mainly provide information for [MASK] tokens to infer the unknown image content, and their corresponding deep representations can be built in the absence of [MASK] tokens. Temporally, the interactions between adjacent generation steps mainly concentrate on updating the representations of a small number of “critical tokens” on top of the previous steps. In fact, the computation for the remaining majority of tokens is generally repetitive. Inspired by these findings, we propose to develop novel NAT models to explicitly encourage these critical interaction mechanisms emerged naturally when trained for visual generation, yielding EfficientNAT (ENAT). Specifically, at the spatial level, we disentangle the computations of visible and [MASK] tokens by encoding visible tokens independently of [MASK] tokens. [MASK] tokens are then processed by attending to the fully contextualized features of visible tokens, as shown in Figure 3b. As an interesting observation derived from disentanglement, we find that prioritizing the computation for visible tokens, particularly when the computation is maximized for visible tokens and minimized for [MASK] tokens (even with only a single network layer), further improves the performance of NATs by a large margin. At the temporal level, we concentrate computation on the “critical tokens” while maximally reusing the representation of previously computed tokens to supplement the necessary information, as illustrated in Figure 4b. Empirically, the effectiveness of ENAT is validated on ImageNet 256\times256 [60], ImageNet 512\times512 [60] and MS-COCO [36]. ENAT is able to achieve significantly reduced computational cost compared to conventional NATs while outperforming them notably (e.g., 24% relative improvement with 1.8\times lower cost, see Table 6a)."
https://arxiv.org/html/2411.06916v1,Slowing Down Forgetting in Continual Learning,"A common challenge in continual learning (CL) is catastrophic forgetting, where the performance on old tasks drops after new, additional tasks are learned. In this paper, we propose a novel framework called ReCL to slow down forgetting in CL. Our framework exploits an implicit bias of gradient-based neural networks due to which these converge to margin maximization points. Such convergence points allow us to reconstruct old data from previous tasks, which we then combine with the current training data. Our framework is flexible and can be applied on top of existing, state-of-the-art CL methods to slow down forgetting. We further demonstrate the performance gain from our framework across a large series of experiments, including different CL scenarios (class incremental, domain incremental, task incremental learning) different datasets (MNIST, CIFAR10), and different network architectures. Across all experiments, we find large performance gains through ReCL. To the best of our knowledge, our framework is the first to address catastrophic forgetting by leveraging models in CL as their own memory buffers.","Continual learning (CL) is a paradigm in machine learning where models are trained continuously to adapt to new data without forgetting previously learned information [46, 67]. This is relevant for real-world applications such as autonomous driving [54], medical devices [64], predictive maintenance [21], and robotics [59]. In these applications, the entire data distribution cannot be sampled prior to model training and (fully) storing the arriving data is not possible. The main challenge of CL is catastrophic forgetting [38]. Catastrophic forgetting refers to problems where the performance on old data drops as new data is learned. Catastrophic forgetting typically happens because prior knowledge is encoded in the combination of the model parameters, yet updating these model parameters during training on new data leads to a change in knowledge and can thus cause the model to forget what it has learned before. To slow down forgetting, numerous methods have been proposed [51, 26, 30, 33, 37, 50, 73, 13, 66, 24, 36]. Existing methods can be categorized into two main groups [52, 15, 67]: (1) Memory-based methods rely on external memory to either store samples from old tasks [e.g., 49, 50] or store separately trained generative networks for generating old data on demand [e.g., 56, 41]. However, this is often not practical due to the fact that CL is often used because streaming data becomes too large and cannot be stored. (2) Memory-free methods change the learning objective, which may help to slow down forgetting at the cost of learning new concepts more slowly [e.g., 26, 52, 15]. Here, we contribute a different, orthogonal strategy in which we propose to leverage the trained model as its own memory buffer. In this paper, we develop a novel framework, called Reconstruction from Continual Learning (ReCL), to slow down forgetting in CL. Different from existing works, our framework leverages the implicit bias of gradient-based neural network training, which causes convergence to margin maximization points. Such convergence points essentially memorize old training data in the model weights, which allows us then to perform a dataset reconstruction attack. We then integrate this reconstruction attack into CL, where we minimize a data reconstruction loss to recover the training samples of old tasks. Crucially, our framework is flexible and can be used on top of existing CL methods to slow down forgetting and thus improve performance. We evaluate the effectiveness of our framework across a wide range of experiments. (1) We show the performance gains from our framework across three standard CL scenarios: class incremental learning, domain incremental learning, and task incremental learning. (2) We compare the performance across different datasets, including MNIST [28] and CIFAR10 [2]. (3) We compare both multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs), finding that consistent performance gains can be achieved across different network architectures. (4) Lastly, we demonstrate the flexibility of our framework by combining our ReCL frameworks with several state-of-the-art CL methods, namely, EWC [26], ER [50], and UPGD [15]. Here, we find that our framework improves over the vanilla version of the CL methods and can slow down forgetting further. Across all experiments, we find a consistent performance gain from using our ReCL framework. Contributions:111Code: https://anonymous.4open.science/r/slower-forgetting/. (1) We present a novel CL framework called ReCL to slow down forgetting in CL. To the best of our knowledge, ours is the first method to slow down catastrophic forgetting in which trained models are leveraged as their own memory buffers. (2) We propose to leverage the implicit bias of margin maximization points to create training data in CL. (3) We demonstrate that our ReCL is flexible and that it consistently slows down forgetting."
https://arxiv.org/html/2411.06911v2,Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI,"Segmentation of cardiac magnetic resonance images (MRI) is crucial for the analysis and assessment of cardiac function, helping to diagnose and treat various cardiovascular diseases. Most recent techniques rely on deep learning and usually require an extensive amount of labeled data. To overcome this problem, few-shot learning has the capability of reducing data dependency on labeled data. In this work, we introduce a new method that merges few-shot learning with a U-Net architecture and Gaussian Process Emulators (GPEs), enhancing data integration from a support set for improved performance. GPEs are trained to learn the relation between the support images and the corresponding masks in latent space, facilitating the segmentation of unseen query images given only a small labeled support set at inference. We test our model with the M&Ms-2 public dataset to assess its ability to segment the heart in cardiac magnetic resonance imaging from different orientations, and compare it with state-of-the-art unsupervised and few-shot methods. Our architecture shows higher DICE coefficients compared to these methods, especially in the more challenging setups where the size of the support set is considerably small. The code is available on GitLab 111https://gitlab.com/bruno˙viti/gpe˙4˙cardiac˙fss.","Medical imaging techniques like computed tomography (CT) and magnetic resonance imaging (MRI) are established technologies to assess patient health. Automatic medical image segmentation plays an important role by mapping anatomical structures to specific semantic labels which allows more in-depth analysis through follow-up applications [15]. This is especially relevant in cardiac imaging, with cardiovascular diseases being the world’s leading cause of death [25]. In recent years, deep learning (DL) models, and in particular Convolutional Neural Networks (CNNs), have been extensively employed to automate and accelerate heart segmentation, obtaining remarkable results in terms of accuracy [5]. CNNs generally require a large amount of labeled data for training, posing a challenge in the medical field with a lack of data and limited manual segmentations. Moreover, CNNs assume that training and test data are drawn from the same distribution, i.e., that they are i.i.d. Therefore, in order for the model to perform well on a different data distribution, it would require a large-scale labeled dataset to update the networks’ parameters for adaptation. Consequently, several DL approaches have been suggested to face the latter problem. For example, recent works proposed unsupervised domain adaptation or generalization architectures in the cardiac segmentation scenario [6, 13, 18, 4]. These models alleviate the need for extensively labeled data in the target domain by leveraging knowledge from a related source domain where labeled data is more abundant. However, while these techniques can handle perturbations between training and test images from different modalities and sources, they are not designed to adapt to novel image orientation [7]. Another promising technique, which can effectively reduce data dependency, is few-shot segmentation (FSS) [23, 3]. In the FSS framework, the model is trained to learn how to segment a novel image, denoted as query, having at its disposal only a small set of labeled examples, denoted as support. Especially in the medical field, FSS is gaining attention, and several FSS architectures have been proposed [8, 14, 17, 21]. The method’s effectiveness highly depends on the mechanism that extracts the information from the support set and integrates this new information with the query image. Most of the existing models rely on the prototype alignment paradigm, which tries to learn a common representation space, where the feature vectors of different object classes can be aligned, e.g. [24]. Differently, Johnander et al. [11] proposed a novel approach based on GPEs, which they exploit to learn a mapping between dense local deep feature vectors and their corresponding mask values. Saha et al. [22] also used FSS in combination with GPEs and applied their method to microscopy images. However, the majority of these approaches [8, 14, 17, 21, 24] are limited to predicting binary segmentations. As we also show in this work exemplarily for [14], this results in a non-efficient class-by-class segmentation for multi-label query images [9]. To face this issue, [9] introduced an extension of prototype alignment to perform one-step multi-class segmentation. They employ a self-supervised training approach, facing the challenge of limited labeled image data. However, in [9] they still consider the same type of images during training and testing, namely 2D short-axis cardiac MRI. In contrast, our goal is to cope with the scarcity of data in the cardiac setting and to make segmentation models more adaptable to different cardiac image orientations. Therefore, we propose a model for multilabel cardiac image segmentation, which combines a U-Net-like architecture [20] with GPEs as extractors of information from the support set [11]. Specifically, we use the contracting branch of the U-Net to bring both the query and the support images into the latent space, in which the GPEs are trained to learn the relationship between the support images and the corresponding masks. This additional information is aggregated into the expanding branch, leading to a more accurate segmentation of the query. We evaluate our model on the public M&Ms2 dataset [2, 16], and assess its ability to generalize from 2D short-axis (SA) to long-axis (LA) cardiac MRI."
https://arxiv.org/html/2411.06899v1,"LongSafetyBench: Long-Context LLMs 
Struggle with Safety Issues","With the development of large language models (LLMs), the sequence length of these models continues to increase, drawing significant attention to long-context language models. However, the evaluation of these models has been primarily limited to their capabilities, with a lack of research focusing on their safety. Existing work, such as ManyShotJailbreak, has to some extent demonstrated that long-context language models can exhibit safety concerns. However, the methods used are limited and lack comprehensiveness. In response, we introduce LongSafetyBench, the first benchmark designed to objectively and comprehensively evaluate the safety of long-context models. LongSafetyBench consists of 10 task categories, with an average length of 41,889 words. After testing eight long-context language models on LongSafetyBench, we found that existing models generally exhibit insufficient safety capabilities. The proportion of safe responses from most mainstream long-context LLMs is below 50%. Moreover, models’ safety performance in long-context scenarios does not always align with that in short-context scenarios. Further investigation revealed that long-context models tend to overlook harmful content within lengthy texts. We also proposed a simple yet effective solution, allowing open-source models to achieve performance comparable to that of top-tier closed-source models. We believe that LongSafetyBench can serve as a valuable benchmark for evaluating the safety capabilities of long-context language models. We hope that our work will encourage the broader community to pay attention to the safety of long-context models and contribute to the development of solutions to improve the safety of long-context LLMs.","Recently, thanks to more advanced model architectures (Xiao et al., 2024b; a; Liu et al., 2024a) and expanded position encoding techniques (Su et al., 2023; Liu et al., 2024b), the context length of language models has been extended significantly (Achiam et al., 2023; Reid et al., 2024). In the foreseeable future, as language models continue to evolve and tackle increasingly complex problems, the demand for handling longer contexts is expected to grow accordingly. We anticipate that long-context language models will become mainstream. Previous research on long-context language models, such as LongBench (Bai et al., 2024), L-Eval (An et al., 2023), and RULER (Hsieh et al., 2024), has typically focused on their capabilities, while neglecting to address their safety. In short-context scenarios, the safety issues of language models have already been extensively studied.(Zhang et al., 2024b; Hartvigsen et al., 2022) In long-context scenarios, Anthropic introduced ManyShotJailbreak(Anil et al., 2024), which revealed safety issues in long-context language models. However, there is still a lack of comprehensive research on the safety of these models. To better access the safety issues of long-context models, we need to design a robust and comprehensive benchmark. In this work, we propose LongSafetyBench, a new benchmark to evaluate the safety of long-context language models. As shown in Figure 1, we collected and constructed data for these three categories, targeting three types of unsafe scenarios: Illegal Activities, Misinformation Harm, and Offensiveness and Bias. Additionally, we categorized the context of the questions into three classes: Fully Harmful Context, Partially Harmful Context, and Harmless Context. In total, we designed ten tasks. To facilitate more convenient and objective testing, we formatted all data in a multiple-choice question format. We carefully designed the options so that each one reflects a specific model behavior, allowing for a more accurate assessment of the model’s capabilities in handling long context as well as its safety performance. With overview statistics shown in Table 1, LongSafetyBench contains 1,203 test instances, with an average length of 41,889 words. (a) The safety issues are categorized into three types: Illegal Activities, Misinformation Harm, Offensiveness and Bias. Based on the methods of context construction, these issues are divided into three distinct categories, encompassing a total of ten specific tasks (b) A sample consists of a question and a set of options. The question is composed of a long content followed by a related query, while the options include several choices representing possible model behaviors. The model is required to respond with the letter of the chosen option. Figure 1: On the left are the categories of the LongSafetyBench, while on the right are a data sample. Figure 2: Average Harm Awareness (HA) score and Safe Response (SR) score across models. We conducted tests on 10 mainstream long-context language models, and the summarized test results are shown in Figure 2. These results provide an intuitive reflection of the models’ safety capabilities in long-context scenarios. During the testing process, we also observed some interesting phenomena: long-context LLMs generally struggle with safety issues and models and the proportion of safe responses from most mainstream long-context LLMs is below 50%; the safety performance in long-context scenarios does not always align with their performance in short-context scenarios; models tend to ignore the harmful content within a long context. More specific findings and conclusions will be discussed in detail in Section 4. To improve the safety performance of long-context language models, we used four tasks that are easy to scale up to construct a total of 11k training samples, and performed supervised fine-tuning (SFT) on Llama3-8b-Instruct(Dubey et al., 2024) and Intern2.5-7b-Chat(Cai et al., 2024). We found that training with a small amount of long-context safety data can effectively enhance the models’ long-context safety capabilities, even reaching the performance level of top-tier closed-source models. In summary, the main contributions of our work are: 1. We present LongSafetyBench, the first comprehensive benchmark that enables evaluation of long-context LLMs’ safety. 2. We conducted extensive testing on 10 popular long-context language models, reflecting the safety capabilities of different models in long-context scenarios. By observing the experimental results, we derived some interesting conclusions (see Section 4). 3. We constructed a 11k training set aimed at enhancing the safety capabilities of long-context language models, discovering that training with a small amount of data enables open-source models to reach the performance level of state-of-the-art closed-source models. This resource supports the open-source community in improving the safety of long-context language models."
https://arxiv.org/html/2411.06878v1,GraphRPM: Risk Pattern Mining on Industrial Large Attributed Graphs,"Graph-based patterns are extensively employed and favored by practitioners within industrial companies due to their capacity to represent the behavioral attributes and topological relationships among users, thereby offering enhanced interpretability in comparison to black-box models commonly utilized for classification and recognition tasks. For instance, within the scenario of transaction risk management, a graph pattern that is characteristic of a particular risk category can be readily employed to discern transactions fraught with risk, delineate networks of criminal activity, or investigate the methodologies employed by fraudsters. Nonetheless, graph data in industrial settings is often characterized by its massive scale, encompassing data sets with millions or even billions of nodes, making the manual extraction of graph patterns not only labor-intensive but also necessitating specialized knowledge in particular domains of risk. Moreover, existing methodologies for mining graph patterns encounter significant obstacles when tasked with analyzing large-scale attributed graphs. In this work, we introduce GraphRPM, an industry-purpose parallel and distributed risk pattern mining framework on large attributed graphs. The framework incorporates a novel edge-involved graph isomorphism network (EGIN) alongside optimized operations for parallel graph computation, which collectively contribute to a considerable reduction in computational complexity and resource expenditure. Moreover, the intelligent filtration of efficacious risky graph patterns is facilitated by the proposed evaluation metrics. Comprehensive experimental evaluations conducted on real-world datasets of varying sizes substantiate the capability of GraphRPM to adeptly address the challenges inherent in mining patterns from large-scale industrial attributed graphs, thereby underscoring its substantial value for industrial deployment.","Figure 1: Example risk patterns. Risk pattern A describes the behavior of fraudsters who defraud funds from multiple victim users and quickly transfer them to different downstream bank cards. Risk pattern B describes that the fraudster collects the victim’s funds multiple times in the name of investment through a shop, giving rewards in the early stage but no longer paying in the later stage. Precision and recall metrics are the evaluation criteria for measuring risk patterns in the industry. Graph pattern mining constitutes a pivotal task within the ambit of mining and machine learning, with profound applications extending to various industrial and business domains such as social network analysis [13], financial fraud detection [2, 10], and computational bioinformatics [21]. Taking the financial transaction scenario as an example, fraudsters would try to cheat normal users and make illegal money transfers. The distinctive behavioral patterns of these fraudsters, termed ’risk patterns’, are critical for the detection of fraudulent activity and the prevention of financial fraud, as exemplified in Fig. 1. Compared to black-box neural network models used for identifying fraudsters [12], industry experts express a preference for summarizing these risk patterns, as they provide more granular insight into the conduct of fraudulent entities, thereby facilitating a more explainable approach to fraud detection. Nonetheless, the manual delineation or construction of these patterns by experts is a labor-intensive process that demands considerable domain-specific knowledge. Consequently, the automation of risk graph pattern mining is an avenue warranting exploration. GRAMI [3] presents a method for frequent subgraph mining by leveraging a novel canonical labeling technique to efficiently discover patterns within a single large graph. Bliss [8] introduces an optimized tool for canonical labeling, specifically designed to handle the challenges posed by large and sparse graph structures, enhancing the performance of graph mining tasks. T-FSM [20] outlines a task-based framework that enables massively parallel processing for frequent subgraph pattern mining, addressing the scalability issues associated with big graph data. Despite this interest, extant automated graph pattern mining algorithms [3, 8, 14, 20] are impeded by two principal limitations: 1. Challenges in processing attributed graphs. In numerous real-world applications, simplistic representations of graph topology fall short of accurately depicting risk scenarios. There is a necessity to leverage high-dimensional attributes associated with nodes or edges for a nuanced characterization of entities, which is beyond the capabilities of methods that are restricted to or can only process one-dimensional attributes. 2. Deficiencies in scalability. Graph data within industrial environments is characteristically voluminous, spanning millions or even billions of nodes. Existing methodologies lack the integration of computational optimization strategies that are critical for the effective and efficient management of data at such an industrial scale. This shortfall in capability significantly undermines the suitability of these methods for application in industrial tasks, which necessitate robust data manipulation and analytical capacity to handle the sheer volume and complexity of the data involved. In this paper, we address the problem of Risk Patterns Mining on large transaction attributed graphs (GraphRPM). Although our research is primarily focused on financial fraud detection, the versatility of the proposed framework allows for its extension to a multitude of industrial applications, including but not limited to analysis within social network contexts. The challenge of managing and processing large-scale attributed graphs in industrial settings is a nontrivial hurdle, particularly in the realm of data mining. The primary objective of this study is to establish a robust and efficacious methodological framework capable of discerning distinct graph patterns as discriminative entities, enabling the differentiation of various graphical structures and the identification of fraud risk patterns. GraphRPM introduces a pioneering Edge-Involved Graph Isomorphism Network (EGIN) that addresses the challenge of fuzzy matching in attributed graph patterns, striking a balance between computational complexity and accuracy. Furthermore, this study implements a two-stage mining strategy coupled with a parallel distributed processing framework to diminish computational redundancy and enhance efficiency. Additionally, we present a Pattern Risk Score as an evaluative measure for identifying salient risk patterns. Comprehensive evaluations across diverse real-world datasets, varying in size, corroborate GraphRPM’s proficiency in resolving pattern mining issues within expansive industrial attributed graphs. Our research represents a significant advancement in the application of data mining and machine learning to industrial and business analytics. We contribute to the field in two pivotal ways. 1. We meticulously conceptualize and address the hitherto underexplored issue of discerning risk patterns on large-scale attributed graphs. 2. We introduce an all-encompassing analytical framework that not only incorporates the cutting-edge EGIN algorithm but also integrates a scalable distributed computation system, thereby enhancing computational efficacy. To our knowledge, this is the first proposition of an approximation algorithm based on graph neural networks for risk pattern mining on large transaction-attributed graphs."
https://arxiv.org/html/2411.06872v1,Multi-Modal interpretable automatic video captioning,"Video captioning aims to describe video contents using natural language format that involves understanding and interpreting scenes, actions and events that occurs simultaneously on the view. Current approaches have mainly concentrated on visual cues, often neglecting the rich information available from other important modality of audio information, including their inter-dependencies. In this work, we introduce a novel video captioning method trained with multi-modal contrastive loss that emphasizes both multi-modal integration and interpretability. Our approach is designed to capture the dependency between these modalities, resulting in more accurate, thus pertinent captions. Furthermore, we highlight the importance of interpretability, employing multiple attention mechanisms that provide explanation into the model’s decision-making process. Our experimental results demonstrate that our proposed method performs favorably against the state-of the-art models on commonly used benchmark datasets of MSR-VTT and VATEX.","Video captioning has the goal of automatically generating sentence to describe the video content [42, 13]. This area of research has emerged as an important area with multiple applications ranging from content indexing and retrieval [24, 33] to assist individuals with visual impairments [45] . However, complex nature of the video presents a significant challenge for captioning tasks. Compared to image captioning [7, 44, 22], videos consists of temporal sequences where multiple actions and events occur in the same time or in rapid succession. Additionally, audio cues, like as speech, music, and environmental sounds, introduce an extra layer of complexity that need to be managed to generate coherent caption. Despite making substantial progress, existing methods in this task is still considered inadequate in capturing the local and global representation. Various works often relies only on raw-pixels and one modality only [47, 26, 54, 58] and often neglect other modalities such as audio, speech or audio-caption. To address this, recent multi-modal approaches [8, 39, 15] integrate various modalities, however, work to relate such modalities is still lacking. This can be problematic, given that there exist depedency between these modalities on the video (i.e. video is mainly composed of both Auditory and Visual information that is interconnected). This is also compounded by a significant challenge lies in the interpretability of such complex systems. Understanding the reasoning behind a model’s caption generation is crucial for enhancing trust in these models. Thus our works will serve as the first mutli-modal work combining audio-caption and interpretability for video-captioning, that also relates these modalities on the same time. In this work, we propose a novel video captioning framework that leverages neural architectures and multi-modal fusion techniques to address aforementioned challenges. Our approach integrates visual and auditory information through a unified encoder-decoder model, utilizing attention mechanisms to focus on relevant features across different modalities. We also incorporate interpretability techniques to provide insights into the model’s decision-making process. We evaluate our model on benchmark datasets. Hence, the contributions of this work are as follow: 1. We introduce a multi-modal fusion strategy that effectively combines visual and audito features, enhancing the model’s ability to generate higher quality captions. 2. We incorporate interpretability techniques to analyze and visualize the model’s attention, providing insights into its decision-making process. 3. We show our competitive results on both well established and largest video captioning datasets of MSR-VTT and VATEX. The remainder of this paper is structured as follows: Sec 2 reviews related work in video captioning. Sec 3 details our proposed methodology, including the model architecture and training process. Sec 4 presents experimental results and analysis. Finally, Section 5 concludes the paper and outlines potential directions for future research."
https://arxiv.org/html/2411.06870v1,AI-Native Multi-Access Future Networks - The REASON Architecture,"The development of the sixth generation of communication networks (6G) has been gaining momentum over the past years, with a target of being introduced by 2030. Several initiatives worldwide are developing innovative solutions and setting the direction for the key features of these networks. Some common emerging themes are the tight integration of AI, the convergence of multiple access technologies and sustainable operation, aiming to meet stringent performance and societal requirements. To that end, we are introducing REASON - Realising Enabling Architectures and Solutions for Open Networks. The REASON project aims to address technical challenges in future network deployments, such as E2E service orchestration, sustainability, security and trust management, and policy management, utilising AI-native principles, considering multiple access technologies and cloud-native solutions.This paper presents REASON’s architecture and the identified requirements for future networks. The architecture is meticulously designed for modularity, interoperability, scalability, simplified troubleshooting, flexibility, and enhanced security, taking into consideration current and future standardisation efforts, and the ease of implementation and training. It is structured into four horizontal layers: Physical Infrastructure, Network Service, Knowledge, and End-User Application, complemented by two vertical layers: Management and Orchestration, and E2E Security. This layered approach ensures a robust, adaptable framework to support the diverse and evolving requirements of 6G networks, fostering innovation and facilitating seamless integration of advanced technologies.","The forthcoming generation of communication networks, colloquially referred to as Sixth Generation (6G), represents a transformative leap beyond the existing paradigms of mobile communications [1]. This leap is essential to meet burgeoning demands for individual users and vertical industries, covering both indoor and outdoor environments and spanning multiple sectors and applications like automotive, manufacturing, public safety, eHealth, immersive media, and more [2]. These applications shift away from conventional Key Performance Indicators (KPIs), like faster data rates, reduced latency, or broader coverage, and move towards more complex classes of enablers for advanced services [3]. For example, the Fifth Generation (5G) Infrastructure Association (5G IA) [4] provides a comprehensive list discussing “integrated sensing and communications”, “cognition and connected intelligence”, “trustworthy and sustainable infrastructures”, and more. Similarly, other initiatives, like Hexa-X [5], discuss enablers like “network of networks”, “global service coverage”, “extreme experiences”, etc. A comprehensive comparison of the different visions can be found at [6]. Overall, all visions converge to a future network that architects a seamlessly interconnected world, where the integration of digital, physical, and human systems unfolds new dimensions of experience, efficiency, and societal transformation. TABLE I: List of Acronyms. Acronym Description 5G 5th Generation 5GPPP 5G Infrastructure Public Private Partnership 6G 6th Generation AI Artificial Intelligence AIaaS AI-as-a-Service AT Access Technology CIA Confidentiality, Integrity, and Availability CI/CD Continuous Integration / Continuous Delivery CNF Container Network Function CPaaS Communications-Platform-as-a-Service DAO Distributed Autonomous Organisation DLT Distributed Ledger Technology DT Digital Twin E2E End-to-End eMBB enhanced Mobile Broadband FDRL Federated Deep Reinforcement Learning FL Federated Learning FSO Free Space Optical GEO Geostationary Orbit IaC Infrastructure-as-Code IMT International Mobile Telecommunications IoT Internet of Things IT Information Technology ITU-T ITU Telecommunication KPI Key Performance Indicators KVI Key Value Indicators LEO Low Earth Orbit LiFi Light Fidelity MANO Management and Orchestration mATRIC Multi-access Technology Real-Time Intelligent Controller MEO Medium Earth Orbit mMTC massive Machine Type Communication MLOps Machine Learning Operations NaC Network-as-code NF Network Function NFV Network Function Virtualisation NGMN Next Generation Mobile Networks Alliance NIO Network Intelligence Orchestration NPN Non-Public Network NRE Network Resource Elasticity NTN Non-Terrestrial Network OPEX Operational Cost O-RAN Open Radio Access Network OT Operational Technology QoS Quality of Service QoE Quality of Experience RAN Radio Access Network RAT Radio Access Technologies REASON Realising Enabling Architectures and Solutions for Open Networks RINA Recursive InterNetwork Architecture RF Radio Frequency SBA Service-based Architecture SDG Sustainable Development Goals SLA Service-Level Agreement SFC Service Function Chain UPF User Plane Function URLLC Ultra-Reliable and Low-Latency Communication VIM Virtual Infrastructure Manager VM Virtual Machine VNF Virtual Network Function XAI Explainable AI However, shifting from traditional KPIs to a more complex set of enablers is only part of the wider paradigm shift shown in 6G ecosystems. We witness a transition from “govern by performance” to “govern by value” [3]. In other words, what is needed is not an indication of performance but an indication of value for the developed new ecosystems. This is how the term Key Value Indicators (KVIs) was born [7]. 6G is expected to not only bring innovative solutions for the above complex problems but also address other societal challenges, like environmental sustainability, ethical implications of technological advancements, and more [8]. A comprehensive, collaborative, and multifaceted approach is imperative to ensure progress that benefits all sectors of society and positively contributes to the global community. The architecture design is one of the most important aspects of each new generation of a communications network system. It is considered its foundation and describes what and how services are provided and how they are integrated. Based on that, this paper presents Realising Enabling Architectures and Solutions for Open Networks (REASON) project111REASON Project: https://reason-open-networks.ac.uk/ and its envisaged architecture. REASON aims to address the above technical and non-technical challenges and deliver an End-to-End (E2E) reference Open Network blueprint, considering all segments and functions of a future network. REASON will pursue to influence the future technology roadmap, making openness, interoperability and Artificial Intelligence (AI)-native capabilities the default standards in network architectures and systems. Reference architectures are usually driven by three factors [9], i.e., the “new scenarios and requirements to be delivered”, the “new technologies that need to be integrated”, and the “inspiration from existing systems”. This is the scope of this paper. Analysing the challenges identified and several use cases defined, we will provide a comprehensive set of requirements to drive future 6G networks. These requirements will later be used to define a novel reference architecture that enables principles like interoperability, agility, sustainability, resilience, and security, all key to future Industrial, Entertainment and Smart Cities applications [10]. Our use case proposition is also expected to demonstrate not only the capabilities of the REASON architecture but also to motivate future scenarios with strong market demand and commercial opportunities. As an expected outcome, the REASON architecture aims to enable multi-technology access network integration to meet the emerging 6G KPIs. Moreover, it will promote network densification to ensure support of the described 6G use cases. The architecture will be AI-native, enabling service and network optimisations in an E2E fashion. Cognitive tools and task-based networking are considered integral parts of the architecture, and we will discuss how they are incorporated into our system. Finally, the aspects of energy-awareness, E2E monitoring, seamless edge-cloud computing continuum, openness, interoperability and security will drive many of our architectural decisions, and we explain how the proposed design addresses them. This paper is structured as follows. Sec. II compares 5G and expected 6G capabilities and comprehensively compares REASON with other future networking projects. The technical and societal challenges identified in future 6G networks and some expected usage scenarios are presented in Sec. III. Sec. IV provides insight into the use cases identified within REASON. Building upon the challenges and use cases, Sec. V presents all the requirements that should be addressed from future network architecture and implementation. Following the requirements, the envisaged REASON architecture is detailed in Sec. VI. Finally, Sec. VIII provides our final remarks and some future directions within the REASON project. A table summarising all the acronyms can be found in Table I."
https://arxiv.org/html/2411.06866v1,Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense Question Answering,"Commonsense question answering is a crucial task that requires machines to employ reasoning according to commonsense. Previous studies predominantly employ an extracting-and-modeling paradigm to harness the information in KG, which first extracts relevant subgraphs based on pre-defined rules and then proceeds to design various strategies aiming to improve the representations and fusion of the extracted structural knowledge. Despite their effectiveness, there are still two challenges. On one hand, subgraphs extracted by rule-based methods may have the potential to overlook critical nodes and result in uncontrollable subgraph size. On the other hand, the misalignment between graph and text modalities undermines the effectiveness of knowledge fusion, ultimately impacting the task performance. To deal with the problems above, we propose a novel framework: Subgraph REtrieval Enhanced by GraPh-Text Alignment, named SEPTA. Firstly, we transform the knowledge graph into a database of subgraph vectors and propose a BFS-style subgraph sampling strategy to avoid information loss, leveraging the analogy between BFS and the message-passing mechanism. In addition, we propose a bidirectional contrastive learning approach for graph-text alignment, which effectively enhances both subgraph retrieval and knowledge fusion. Finally, all the retrieved information is combined for reasoning in the prediction module. Extensive experiments on five datasets demonstrate the effectiveness and robustness of our framework.","Commonsense question answering (CSQA) is a critical task in natural language understanding, which requires systems to acquire different types of commonsense knowledge and possess multi-hop reasoning ability [27, 19, 22]. Though massive pre-trained models have achieved impressive performance on this task, it is difficult to learn commonsense knowledge solely from the pre-training text corpus, as the commonsense knowledge is evident to humans and rarely expressed explicitly in natural language. Compared with unstructured text, structured data like knowledge graphs is much more efficient in representing commonsense [26]. The incorporation of external knowledge aids PLMs in comprehending question-answer (Q-A) pairs, while the entity relations enhance the model’s reasoning capabilities. Therefore, various commonsense knowledge graphs (CSKGs) (e.g., ConceptNet [25]) have been adopted in previous studies. Existing KG-augmented models for CSQA primarily adhere to a extracting-and-modeling paradigm [36, 26, 29, 32, 28, 35]. First, the knowledge subgraphs or paths related to a given question are extracted by string matching or semantic similarity, which indicate the relations between concepts or imply the process of multi-hop reasoning. Subsequently, diverse strategies emerge for the efficient representation and fusion of the extracted structural knowledge. One research path [12, 8] involves elaborately crafting graph neural networks for better modeling the extracted subgraphs, whereas another [34, 26] explores the efficient incorporation of knowledge from KG into language models by enhancing the interactions between PLMs and GNNs. Despite their success, these approaches still have several limitations. First, the subgraph’s quality suffers when retrieved through a simple string or semantic matching, posing limitations for subsequent operations. To obtain sufficient relevant knowledge, the number of nodes will expand dramatically with the increase of hop count, inevitably raising the burden of the model. Despite its ample size, certain crucial nodes might remain elusive, since some entities are not learned during the pre-training. Besides, the edges linked to the peripheral nodes within the subgraph are pruned, causing the message-passing mechanism of GNN to be blocked and impairing the attainment of effective representations, consequently undermining valuable information. Second, the misalignment between graph and text encoders presents a challenge for PLMs to internalize the knowledge contained in the acquired subgraph, especially in scenarios with limited data, leading to a reduced task performance [35]. Though Dragon [33] proposes a pre-training method to align GNNs and PLMs, it requires additional corpus, and the text-to-graph style to construct semantically equivalent graph-text pairs is challenging. The necessity for substantial computational resources poses another hurdle, prompting the search for a more efficient alignment method. In this paper, we propose a novel framework: Subgraph REtrieval Enhanced by GraPh-Text Alignment (SEPTA), for CSQA. To mitigate the shortcomings of the subgraph extraction process, we establish a database of subgraph vectors derived from the knowledge graph. Consequently, the challenge shifts from retrieving a pertinent subgraph to obtaining relevant subgraph vectors. A BFS-style sampling method is employed to obtain the connected graph for each node and the embedding of the subgraph is subsequently stored in the database. Drawing on the parallels between BFS and the message-passing mechanism of GNNs, the central node’s representation learned from the subgraph could be closely aligned with that derived from the entire graph, with almost no information loss. Besides, to further improve the retrieval accuracy and facilitate knowledge fusion during the prediction, we consider aligning the semantic space of the graph and text encoders, proposing an effective approach for graph-text alignment. A novel graph-to-text method is proposed to construct high-quality semantically equivalent training pairs, with no requirement of external corpus and easy to train. Finally, all the information retrieved is combined by a simple attention mechanism to facilitate the model in commonsense reasoning. Our contributions can be summarized as follows: • We propose a novel and effective framework SEPTA, where we convert the knowledge graph into a subgraph vector database and retrieve relevant subgraphs to facilitate commonsense reasoning. • We design a bidirectional contrastive learning method to align the semantic space of the graph and text encoders, with a graph-to-text method to construct high-quality graph-text pairs, which facilitates subgraph retrieval and knowledge fusion. • We propose a BFS-style subgraph sampling strategy for subgraph construction. Drawing on the parallel between BFS and the message-passing mechanism, our method can preserve complete neighbor information for each node. • We conduct extensive experiments on five datasets. Our proposed approach achieves better results than the state-of-the-art approaches and has promising performance in weakly supervised settings."
https://arxiv.org/html/2411.06863v1,Computable Model-Independent Bounds for Adversarial Quantum Machine Learning,"By leveraging the principles of quantum mechanics, QML opens doors to novel approaches in machine learning and offers potential speedup. However, machine learning models are well-documented to be vulnerable to malicious manipulations, and this susceptibility extends to the models of QML. This situation necessitates a thorough understanding of QML’s resilience against adversarial attacks, particularly in an era where quantum computing capabilities are expanding. In this regard, this paper examines model-independent bounds on adversarial performance for QML. To the best of our knowledge, we introduce the first computation of an approximate lower bound for adversarial error when evaluating model resilience against sophisticated quantum-based adversarial attacks. Experimental results are compared to the computed bound, demonstrating the potential of QML models to achieve high robustness. In the best case, the experimental error is only 10% above the estimated bound, offering evidence of the inherent robustness of quantum models. This work not only advances our theoretical understanding of quantum model resilience but also provides a precise reference bound for the future development of robust QML algorithms.","Machine learning (ML) is increasingly integral in various applications such as speech recognition [1], computer vision [2], and natural language processing [3]. A significant concern in ML is its vulnerability to adversarial attacks, in which malicious actors exploit weaknesses in the models to produce outcomes favorable to the adversary [4, 5, 6]. Moreover, it is demonstrated that various ML classification models are at risk when adversaries craft examples specifically designed to mislead them [7, 8]. Alongside classical machine learning (CML), quantum machine learning (QML) has emerged as a promising new paradigm. Conceptually similar to classical counterparts, QML involves iteratively optimizing models across training samples to achieve intended functionalities [9]. The key distinction between quantum and classical models lies in the use of quantum computing. QML exploits quantum mechanical phenomena such as superposition and entanglement, which are core principles of quantum computing. The potential of quantum computing has been demonstrated in various domains [10, 11, 12, 13]. Moreover, in the context of ML, quantum systems offer the possibility of manipulating exponentially large state spaces efficiently [14]. QML shows promise but faces challenges in practical implementation, particularly regarding vulnerability to adversarial attacks. While some studies suggest QML models may exhibit resilience to certain adversarial transfer attacks designed for classical models [15], the overall security landscape remains open. Studies have primarily focused on adversarial examples using classical input data in trained quantum classification models, observing similar misclassification phenomena to those in classical models [16]. However, these insights provide limited guidance for designing robust QML models. The field is further constrained by limitations in classical simulation capabilities and noisy quantum hardware, hindering extensive experimental studies. Theoretically, the security of QML in handling exponentially larger spaces remains debated [17]. However, some research indicates that, under certain assumptions, the scaling of security risks in QML is comparable to that in CML [18]. Along with these results, in the context of adversarial attacks, it is crucial to have a fundamental understanding of adversarial performance limitations posed by the data distribution and model accuracy. Such limitation can be formulated as a bound on adversarial error rate, which captures the chance of success for an adversary to induce an incorrect outcome for QML models. This provides detailed insights into the minimum inherent adversarial vulnerability in QML and informs us on more effective defense mechanisms to reach these limits. The proliferation of QML technologies requires a security guarantee because, in the future, QML may be used in sensitive applications. Contributions. In this work, we present a novel, computable lower bound on the adversarial error rate, which provides a practical reference for model performance in adversarial attacks regardless of the quantum model’s architecture. Our algorithm addresses the classical perturbation attack prevalent in the literature and the quantum perturbation attacks unique to the quantum model. We devise and test an example of such an attack based on the Projected Gradient Descent (PGD) [19]. In Sec. V, we validate the effectiveness of the bound by comparing the derived bounds with the actual adversarial error rates observed in quantum models. The experimental results demonstrate our proposed bounds’ practical effectiveness and applicability in real-world scenarios. Overall, the key contributions are the following: • Lower bound estimation method for QML: We present a new algorithm for quantum scenarios inspired by classical adversarial risk bound estimation methods [20]. Our approach is model-independent and is capable of addressing the unique quantum challenge of quantum perturbation attacks. Utilizing parallel computing, the algorithm efficiently computes the bound on an adversarial error and informs us of the limit of adversarial performance for any possible quantum models. • Evaluation of practical effectiveness: Our bound estimation algorithms are investigated against empirical benchmark data, showing a strong correlation between derived bound and observed adversarial error rates in quantum models. The results, detailed in Sec. V, confirm the practical applicability of our proposed bounds through benchmark scenarios. • Novel quantum attack strategy: We have developed a new quantum attack strategy in the attack scenario of quantum perturbation attack. It is based on the Projected Gradient Descent (PGD) [19], a strong and widely-studied gradient-based attack in CML."
https://arxiv.org/html/2411.06858v1,Scientific machine learning in ecological systems: A study on the predator-prey dynamics,"In this study, we apply two pillars of Scientific Machine Learning: Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs) to the Lotka-Volterra Predator-Prey Model, a fundamental ecological model describing the dynamic interactions between predator and prey populations. The Lotka-Volterra model is critical for understanding ecological dynamics, population control, and species interactions, as it is represented by a system of differential equations. In this work, we aim to uncover the underlying differential equations without prior knowledge of the system, relying solely on training data and neural networks. Using robust modeling in the Julia programming language, we demonstrate that both Neural ODEs and UDEs can be effectively utilized for prediction and forecasting of the Lotka-Volterra system. More importantly, we introduce the ""forecasting breakdown point"" – the time at which forecasting fails for both Neural ODEs and UDEs. We observe how UDEs outperform Neural ODEs by effectively recovering the underlying dynamics and achieving accurate forecasting with significantly less training data. Additionally, we introduce Gaussian noise of varying magnitudes (from mild to high) to simulate real-world data perturbations and show that UDEs exhibit superior robustness, effectively recovering the underlying dynamics even in the presence of noisy data, while Neural ODEs struggle with high levels of noise. Through extensive hyperparameter optimization, we offer insights into neural network architectures, activation functions, and optimizers that yield the best results. This study opens the door to applying Scientific Machine Learning frameworks for forecasting tasks across a wide range of ecological and scientific domains.","Scientific Machine Learning (SciML) is an emerging interdisciplinary field that integrates the strengths of traditional scientific models with the flexibility of machine learning techniques, and it has seen successful applications across various domains such as epidemiology, gene regulation, quantum mechanics, and fluid dynamics [1, 2, 3]. The core of SciML lies in combining the structure and interpretability of physical models, such as ordinary and partial differential equations (ODEs/PDEs) [4, 5], with the representational power of neural networks. Two primary approaches define the landscape of Scientific Machine Learning: • Neural Ordinary Differential Equations (Neural ODEs): This method replaces the traditional ODE/PDE system entirely with neural networks. By backpropagating through the system, we can optimize the network’s parameters and approximate the dynamics of the system. The neural network is used to model the continuous-time dynamics of a system. Neural ODEs treat the hidden layers of a neural network as continuous transformations, modeled by a differential equation, rather than discrete layers as in conventional neural networks. This formulation allows the use of ODE solvers to propagate the system state forward in time. [1, 3]. The general form of a Neural ODE is given as: \frac{du(t)}{dt}=f_{\theta}(u(t),t) (1) where u(t) represents the system state at time t, and f_{\theta} is a neural network parameterized by \theta. In this formulation, the neural network learns the time derivatives (i.e., the dynamics) of the state directly from the data, without requiring a predefined model for the system’s evolution. The continuous transformation of the state variable is modeled as: u(t_{1})=u(t_{0})+\int_{t_{0}}^{t_{1}}f_{\theta}(u(t),t)\,dt (2) where u(t_{0}) is the initial state and u(t_{1}) is the state at time t_{1}. The system state can be propagated forward by solving this integral using numerical ODE solvers such as the Runge-Kutta methods (e.g., Tsit5) or adaptive solvers. The objective is to minimize the difference between the predicted and observed state trajectories by optimizing the parameters \theta of the neural network. The training process of Neural ODEs typically involves backpropagating through the ODE solver. However, directly backpropagating through the entire integration process can be computationally expensive. To address this, the adjoint sensitivity method is commonly employed. The adjoint method computes gradients with respect to the neural network’s parameters \theta by solving an additional ODE backwards in time, thus allowing efficient memory management during training. This method is summarized as follows: \frac{dL}{d\theta}=\int_{t_{1}}^{t_{0}}a(t)^{T}\frac{\partial f_{\theta}(u(t),% t)}{\partial\theta}\,dt (3) where a(t) is the adjoint variable, which captures how the loss L changes with respect to the state variables. • Universal Differential Equations (UDEs): Instead of substituting the entire equation, UDEs replace specific terms of ODEs/PDEs with neural networks, allowing the discovery of unknown components or missing dynamics in existing models [3]. This approach is particularly useful when some physical laws are known but others remain uncertain. While Neural ODEs replace the entire system with a neural network, UDEs retain certain known components of the differential equation and augment the unknown or uncertain parts with neural networks. This method allows the system to capture known physics while also learning unknown dynamics from the data. The general form of a UDE is: \frac{du(t)}{dt}=f_{\text{known}}(u(t),t)+f_{\text{NN}}(u(t),t;\theta) (4) where f_{\text{known}} represents the known part of the differential equation derived from physical laws, and f_{\text{NN}} is a neural network with parameters \theta that models the unknown or unmodeled dynamics. The neural network’s role is to fill in the gaps where the traditional model is insufficient or where the underlying mechanisms are unknown. In practice, the UDE takes the form: \frac{du(t)}{dt}=f_{\text{known}}(u(t),t)+\sum_{i=1}^{m}\text{NN}_{i}(u(t),t;% \theta_{i}) (5) where the neural networks \text{NN}_{i} are used to model specific unknown terms or interactions in the system. Each neural network takes the system state u(t) and possibly time t as inputs, and outputs the corresponding missing term in the equation. The sum allows for multiple neural networks to model different parts of the dynamics. Despite the significant progress of SciML in various scientific domains like physics, chemistry, finance, ecological models etc [3, 6, 7, 8, 9], a comprehensive studies on forecasting accuracy, model breakdown points, and robust hyperparameter optimization have yet to be fully addressed which upon here . This work addresses the following key questions: • Can we leverage UDEs to learn the interaction terms in a predator-prey system by replacing traditional terms with neural networks? • How do predictions from Neural ODEs compare with those from UDEs in ecological modeling? • Can Neural ODE/UDE recover interaction terms from less training data - if so, then what is potential forecast breakdown point for both • Is forecasting with UDEs more effective than with Neural ODEs? To answer these questions, we utilize the Lotka-Volterra predator-prey model, a foundational system in ecology, as a case study. Using this model, we aim to demonstrate how UDEs can enhance our understanding of population dynamics by learning unknown interaction terms, while comparing the results with traditional Neural ODEs. In this work, we use the powerful Scientific Machine Learning libraries provided by Julia [4, 3] for efficient differential equation solving and neural network integration. Through extensive hyperparameter tuning and optimization, we explore how neural network architectures, activation functions, and optimization strategies affect the performance of both Neural ODEs and UDEs. We observe how UDEs can effectively recover underlying model by introducing Gaussian Noise to mimic real world noise data perturbations. Additionally, we introduce the concept of a ""forecast breakdown point""— the moment beyond which predictive accuracy significantly deteriorates. This insight sheds light on the limitations of current SciML frameworks for long-term forecasting tasks. The structure of the paper is as follows: we begin by detailing the methodology behind Neural ODEs and UDEs. We then present results from predictions and forecasts using both methods, followed by hyperparameter optimization outcomes. Lastly, we conclude with an analysis of the findings and discuss future directions for the application of Scientific ML in ecological modeling."
https://arxiv.org/html/2411.06852v1,Evaluating Large Language Models on Financial Report Summarization: An Empirical Study,"In recent years, Large Language Models (LLMs) have demonstrated remarkable versatility across various applications, including natural language understanding, domain-specific knowledge tasks, etc. However, applying LLMs to complex, high-stakes domains like finance requires rigorous evaluation to ensure reliability, accuracy, and compliance with industry standards. To address this need, we conduct a comprehensive and comparative study on three state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their effectiveness in generating automated financial reports. Our primary motivation is to explore how these models can be harnessed within finance, a field demanding precision, contextual relevance, and robustness against erroneous or misleading information. By examining each model’s capabilities, we aim to provide an insightful assessment of their strengths and limitations. Our paper offers benchmark for financial report analysis, encompassing proposed metrics such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative evaluation framework that integrates both quantitative metrics (e.g., precision, recall) and qualitative analyses (e.g., contextual fit, consistency) to provide a holistic view of each model’s output quality. Additionally, we make our financial dataset publicly available, inviting researchers and practitioners to leverage, scrutinize, and enhance our findings through broader community engagement and collaborative improvement. Our dataset is available on huggingface111https://huggingface.co/datasets/xinqiyang/tradingview_msn_financial_news_1k.","Figure 1: The Flow Chart of Prompt Engineering in Financial Report Summarization. Large Language Models (LLMs) [1, 2, 3, 4, 5, 6, 7] have emerged as transformative tools in Natural Language Processing (NLP), demonstrating unprecedented capabilities across tasks such as text generation [8], summarization [9], classification [10], translation [11], automatic dialogue [12], and also extend to the multi-modal area [13]. Built upon massive datasets and powered by advanced neural architectures, LLMs can understand and generate human-like text, making them adaptable to a wide range of domains. Recently, their application has extended into high-stakes sectors such as finance [14], healthcare [15], and legal industries [16], where accuracy, informativeness and coherence are essential. However, deploying LLMs in such fields poses unique challenges, requiring careful evaluation to ensure reliability and trustworthiness. The financial sector, in particular, benefits from LLM-driven innovations in automation, such as generating reports, analyzing market sentiment, and summarizing extensive financial data [17]. Yet, financial text is often dense with specialized terminology and context-sensitive information, demanding high levels of precision and contextual understanding from LLMs. Consequently, assessing LLM performance in financial applications goes beyond conventional metrics, requiring a nuanced approach that includes accuracy, informativeness and coherence, as well as alignment with financial domain knowledge. This paper presents a comprehensive evaluation of our strive of prompt engineering on three cutting-edge LLMs: GLM-4, Mistral-NeMo and LLaMA3.1, which are applied to text summarization tasks in financial statement analysis. By comparing these models across multiple criteria, we aim to identify strengths, limitations, and use-case suitability for each model. Figure 1 illustrates the flow chart of prompt engineering in financial report summarization. The process consists of seven steps, and we typically focus on gathering information and analyzing output. For the former part, we open the collected dataset from TradingView and MSN. For the latter part, we offer an evaluation framework that integrates quantitative metrics with qualitative analysis, contributing to the growing need for systematic LLM evaluation tailored to domain-specific requirements. In this paper, we also provide insights for future model development, emphasizing the role of prompt engineering and domain adaptation for achieving practical performance levels suitable for real-world financial applications. Thanks to the usage of Ollama222https://github.com/ollama/ollama, the LLMs include Gemini-1.5-pro, Mixtral-8x22b, Claude-3-opus and so on could be added to the backend of our system easily. We list our contributions as follows: 1. We establish a detailed performance benchmark for LLMs in financial reporting, providing a foundation for future comparative studies. 2. We highlight domain-specific challenges and propose model adjustments to meet the rigorous standards required in finance. 3. We foster a collaborative environment by openly sharing our dataset and methodologies. Through these efforts, we advance the responsible deployment of LLMs in finance and catalyze ongoing innovation in this area."
https://arxiv.org/html/2411.06850v1,"1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language, Hate Speech, and Targets using LLMs","This paper presents a detailed system description of our entry for the CHiPSAL 2025 shared task, focusing on language detection, hate speech identification, and target detection in Devanagari script languages. We experimented with a combination of large language models and their ensembles, including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like focal loss to address challenges in the natural understanding of Devanagari languages, such as multilingual processing and class imbalance. Our approach achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804 for Sub-tasks A, B, and C respectively. This work provides insights into the effectiveness of transformer models in tasks with domain-specific and linguistic challenges, as well as areas for potential improvement in future iterations.","Large language models (LLMs) have revolutionized natural language processing (NLP) yet South Asian languages remain largely underrepresented within these advancements despite being home to over 700 languages, 25 major scripts, and approximately 1.97 billion people. Addressing these gaps, this paper focuses on three critical NLP tasks of CHiPSAL 2025 Sarveswaran et al. (2025) in Devanagari-scripted languages: 5-way classification of the text based on the language of the text (Sub-task A), Binary classification for detecting hate speech in the text (Sub-task B), and 3-way classification for detecting target of hate speech in a text (Sub-task C) Thapa et al. (2025). Our system leverages the multilingual capabilities of open-source LLMs namely IndicBERT V2 Doddapaneni et al. (2023), MuRIL Khanuja et al. (2021), and Gemma-2 GemmaTeam (2024) and their ensembles for natural language understanding of Devanagari script languages. Our work contributes to advancing language technology in South Asia, aiming for inclusivity and deeper understanding across diverse linguistic landscapes."
https://arxiv.org/html/2411.06839v1,"LLM-Neo: Parameter Efficient Knowledge 
Distillation for Large Language Models","In this paper, we propose a novel LLM-Neo framework that efficiently transfers knowledge from a large language model (LLM) teacher to a compact student. Initially, we revisit the knowledge distillation (KD) and low-rank adaption (LoRA), and argue that they share the same paradigm. Inspired by this observation, we explore the strategy that combines LoRA and KD to enhance the efficiency of knowledge transfer. We first summarize some guidelines for this design and further develop the LLM-Neo. Experimental results on compressing Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further analysis demonstrates the robustness of the proposed LLM-Neo on variants of LoRA. The trained models have been available at this repository.","Knowledge distillation (KD) [1] for large language models (LLMs)[2] is a predominant method for model compression. The key insight is to train a compact student model by mimicking the behaviors of the teacher model. One mainstreaming way is to align the logits [3], and thus transfer the knowledge from the teacher model to the student model. Parameter-Efficient Fine-Tuning (PEFT) [4, 5] is another commonly used technique for LLM efficiency [6]. Among various PEFT methods, the Low-rank adapter (LoRA) [7] has gained increasing popularity since it does not introduce any additional parameters for inference. During training, LoRA updates a mergeable low-rank branch instead of updating the original full parameters. Therefore, LoRA can efficiently transfer the knowledge contained in the training examples to the trained models. Figure 1: Illustration of different knowledge transfer pipelines (KD, LoRA, and LLM-Neo). The proposed LLM-Neo pipeline combines the benefits of both the KD and LoRA approaches, that is, distilling knowledge from the teacher and low-rank branch efficiency. In this paper, we argue that KD and LoRA follow the same paradigm, i.e., aiming at transferring knowledge while the sources differ [8]. Moreover, LoRA transfers the knowledge efficiently via the low-rank branch, while KD methods update the full parameters and typically cost much more resources. We thus ask: can we combine KD and LoRA to improve the efficiency of knowledge transfer from the teacher model? To this end, we propose a novel LLM-Neo framework which integrates LoRA into KD to achieve parameter-efficient knowledge distillation. Specifically, as shown in Fig. 1, we follow the idea of LoRA to introduce a low-rank branch in the student model, aiming to inherit the knowledge from the teacher model. We first perform comprehensive analysis and derive valuable guidelines for the design of the proposed LLM-Neo. Experimental results on compressing Llama 2 [9] and Llama 3.1 [10] demonstrate the effectiveness and efficiency of LLM-Neo. Moreover, further analysis shows the robustness of LLM-Neo towards LoRA variants [11]. Our contributions can be concluded as follows: • We propose LLM-Neo, a novel approach that integrates knowledge distillation with low-rank adapter to lightweight LLMs. • We summarize the guidelines for LLM-Neo including: i) Typically, the larger the rank, the better, and 128 works well. ii) A learning rate close to 2e-4 works well for LoRA. iii) A larger rank requires a lower learning rate. As shown in Fig 2, trying diagonal balance is worth considering. • We perform extensive experiments on the Llama 2 and Llama 3.1 models to demonstrate the effectiveness of LLM-Neo. Moreover, further analyses confirm its compatibility with existing LLM optimization methods, highlighting its contributions to improved model performance and scalability. • We release the weights of the Llama-3.1-Neo-1B model, a small model trained on the 1 million BAAI dialogue dataset using the LLM-Neo method, contributing to the broader research and development community."
https://arxiv.org/html/2411.06805v1,Boosting the Potential of Large Language Models with an Intelligent Information Assistant,"The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as ""hallucination"". Initial retrieval-augmented generation (RAG) methods like the ""Retrieve-Read"" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach—Curriculum Assistant Learning and Reinforced Preference Optimization—AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.","The emergence of Large Language Models (LLMs) has significantly advanced the field of natural language processing, demonstrating an impressive ability to mimic human-like language patterns [1]. However, despite their extensive knowledge acquired during training, LLMs can occasionally generate factually incorrect information, a phenomenon referred to as “hallucination” [2, 3]. To address this, the integration of retrieval systems with LLMs has been suggested, allowing these models to tap into external databases to generate more reliable responses [4]. Initially, retrieval-augmented generation (RAG) relied on a simple ""Retrieve-Read"" framework [5], which was adequate for basic question-answering but insufficient for complex, multi-step reasoning tasks. As language models advanced, various prompt-based RAG strategies emerged [6, 7], incorporating pre-retrieval and post-retrieval prompts to refine the process. However, these strategies heavily relied on the foundational capabilities of the language models. Consequently, the focus shifted to Supervised Fine-Tuning (SFT)-based RAG methods [8], which involve fine-tuning language models specifically for RAG tasks to enhance their performance. Figure 1: Comparisons of Naive, Prompt-based, SFT-based and our Assistant-based RAG frameworks. While SFT-based methods have improved the quality of generated responses, they face two limitations that hinder their practical application. Firstly, these fine-tuned models are not easily adaptable to emerging LLMs, requiring retraining for each new foundational LLM. Secondly, directly fine-tuning a foundational LLM in the RAG scenario may change its innate abilities, potentially leading to negative impacts on the model’s performance on other tasks. To address these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), which integrates an intelligent information assistant as a plugin within LLMs. This approach comprises a trainable assistant for information management and a static main LLM dedicated to task execution, as depicted in Figure 1. As an intelligent information assistant, AssistRAG operates in two primary categories to handle complex tasks: memory management and knowledge management. Memory management involves integrating and analyzing content from internal memory, while knowledge management focuses on leveraging external knowledge. These two main functions are supported by four core capabilities of AssistRAG: (1) Tool usage, which involves recalling relevant information from both internal memory and external knowledge bases through a retriever; (2) Action execution, which involves processing, analyzing, and extracting information; (3) Memory building, which involves recording essential knowledge and reasoning patterns from historical interactions; (4) Plan specification, which involves determining the necessity of each step in the process. These four capabilities work together to ensure that AssistRAG can provide accurate and comprehensive support to the main LLM. To implement AssistRAG, we adopt a two-phase training approach. The first phase, Curriculum Assistant Learning, enhances the assistant’s capabilities in note-taking, question decomposition, and knowledge extraction through progressively complex tasks. The second phase, Reinforced Preference Optimization, uses reinforcement learning to tailor the assistant’s feedback to the main LLM’s specific needs, optimizing knowledge extraction based on feedback from the main LLM. During the inference stage, AssistRAG operates through a three-step process: (1) Information Retrieval and Integration: The assistant understands the main LLM’s needs, retrieves relevant knowledge from internal and external sources, and extracts valuable information. (2) Decision Making: The assistant evaluates and decides whether to provide the retrieved memories and knowledge to the main LLM based on their relevance. (3) Answer Generation and Memory Updating: The main LLM generates an answer using its internal knowledge and the assistant’s information, while the assistant updates its memory with crucial reasoning steps. Results from experiments across three complex question-answering datasets reveal that AssistRAG exhibits superior reasoning capabilities and markedly outperforms existing benchmarks. Notably, when applied to different foundational LLMs, AssistRAG appears to confer more pronounced benefits on less advanced LLMs."
https://arxiv.org/html/2411.06792v1,Evolving Efficient Genetic Encoding for Deep Spiking Neural Networks,"By exploiting discrete signal processing and simulating brain neuron communication, Spiking Neural Networks (SNNs) offer a low-energy alternative to Artificial Neural Networks (ANNs). However, existing SNN models, still face high computational costs due to the numerous time steps as well as network depth and scale. The tens of billions of neurons and trillions of synapses in the human brain are developed from only 20,000 genes, which inspires us to design an efficient genetic encoding strategy that dynamic evolves to regulate large-scale deep SNNs at low cost. Therefore, we first propose a genetically scaled SNN encoding scheme that incorporates globally shared genetic interactions to indirectly optimize neuronal encoding instead of weight, which obviously brings about reductions in parameters and energy consumption. Then, a spatio-temporal evolutionary framework is designed to optimize the inherently initial wiring rules. Two dynamic regularization operators in the fitness function evolve the neuronal encoding to a suitable distribution and enhance information quality of the genetic interaction respectively, substantially accelerating evolutionary speed and improving efficiency. Experiments show that our approach compresses parameters by approximately 50% to 80%, while outperforming models on the same architectures by 0.21% to 4.38% on CIFAR-10, CIFAR-100 and ImageNet. In summary, the consistent trends of the proposed genetically encoded spatio-temporal evolution across different datasets and architectures highlight its significant enhancements in terms of efficiency, broad scalability and robustness, demonstrating the advantages of the brain-inspired evolutionary genetic coding for SNN optimization.","Artificial neural networks have provided important insights into numerous application areas [1, 2], but the large number of matrix operations increases significantly with the size of the network. Spiking Neural Networks [3], as the third generation of neural networks, achieve a low-energy computing paradigm by simulating the communication characteristics of brain neurons and leveraging the inherent energy efficiency advantages of discrete signal processing and are more biologically plausible. Most of the research on SNN optimization focuses on the training mechanism, which can be roughly divided into plasticity-based training [4], conversion-based training [5], and gradient-based training [6], which has become the most mainstream method due to its high efficiency. However, a large number of time steps, proxy gradient calculations, and increasingly deeper network architecture designs also make SNNs increasingly computationally expensive. Common techniques for optimizing computational overhead include network pruning and quantization [7], which greatly reduce storage and computational costs by pruning redundant connections [8] and reducing operations [9]. Despite offering numerous insights and successful methodologies for training on complex tasks, current SNN models still lack effective integration with brain-inspired mechanisms to achieve a balance between cost and performance. Generation after generation, environmental pressures drive biological neural systems to evolve specific responses to complex tasks, with neural connections continually adapting to encode crucial information into the connectome [10]. Evolution has endowed approximately 21,000 genes with the ability to support the complex computing capabilities of the brain’s 10^{10} neurons and 10^{15} synapses. This compact and efficient encoding method not only saves biological energy, but also facilitates genetic optimization during evolution, thereby supporting complex cognitive functions and highly flexible behavioral adaptability. This inspired us to design a gene-scaled neuronal coding paradigm for SNNs, controlling the entire network with very few parameters, and simulate the evolutionary process of the brain to optimize the genetic encoding. Based on this, this paper implements genetically encoded neural networks on a variety of common network architectures, re-encoding weights with neuronal encoding each layer and global shared gene interaction matrices, greatly compressing parameters and energy consumption. Furthermore, to accelerate the evolution and training, we propose a spatio-temporal evolutionary SNN framework. Spatially, the initial wiring of neuronal encodings and gene interaction matrix are optimized through the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Temporally, the dynamic regularization scale helps solutions transition from freely learning complex features to gradually establishing stable functional patterns, ultimately achieving a significant reduction in energy consumption without compromising performance. By integrating principles of neuroevolution, the proposed method develops robust, efficient architectures capable of performing complex tasks at very low computational cost, reflecting the evolutionary encoding of behaviors in the brain’s connectome. In general, the contributions of this work can be summarized as follows: 1) We develop a Genetically Encoded Evolutionary (GEE) spiking neural network that improves the performance by learning neuronal encoding rather than directly updating weights. The controllable genetic scale greatly reducing the number of parameters and computational cost without losing accuracy. 2) To improve the quality of solutions, we propose a Spatio-Temporal dynamical Evolution (STE) framework for initial wiring of neuronal encoding and gene interactions. Two dynamic regularization operators, spatial entropy and temporal difference regularization, help improve the evolution efficiency and greatly reduce the cost. 3) On CIFAR10, CIFAR100 and ImageNet datasets, the proposed GEE achieves superior performance with significantly lower energy consumption, achieving efficient SNN evolution with a brain-inspired efficient computational paradigm."
https://arxiv.org/html/2411.06786v1,ScaleKD: Strong Vision Transformers Could Be Excellent Teachers,"In this paper, we question if well pre-trained vision transformer (ViT) models could be used as teachers that exhibit scalable properties to advance cross architecture knowledge distillation research, in the context of adopting mainstream large-scale visual recognition datasets for evaluation. To make this possible, our analysis underlines the importance of seeking effective strategies to align (1) feature computing paradigm differences, (2) model scale differences, and (3) knowledge density differences. By combining three closely coupled components namely cross attention projector, dual-view feature mimicking and teacher parameter perception tailored to address the alignment problems stated above, we present a simple and effective knowledge distillation method, called ScaleKD. Our method can train student backbones that span across a variety of convolutional neural network (CNN), multi-layer perceptron (MLP), and ViT architectures on image classification datasets, achieving state-of-the-art knowledge distillation performance. For instance, taking a well pre-trained Swin-L as the teacher model, our method gets 75.15%|82.03%|84.16%|78.63%|81.96%|83.93%|83.80%|85.53% top-1 accuracies for MobileNet-V1|ResNet-50|ConvNeXt-T|Mixer-S/16|Mixer-B/16|ViT-S/16|Swin-T|ViT-B/16 models trained on ImageNet-1K dataset from scratch, showing 3.05%|3.39%|2.02%|4.61%|5.52%|4.03%|2.62%|3.73% absolute gains to the individually trained counterparts. Intriguingly, when scaling up the size of teacher models or their pre-training datasets, our method showcases the desired scalable properties, bringing increasingly larger gains to student models. We also empirically show that the student backbones trained by our method transfer well on downstream MS-COCO and ADE20K datasets. More importantly, our method could be used as a more efficient alternative to the time-intensive pre-training paradigm for any target student model on large-scale datasets if a strong pre-trained ViT is available, reducing the amount of viewed training samples up to 195\times. The code is available at https://github.com/deep-optimization/ScaleKD.","Background. The great success of deep learning in computer vision (CV) has been driven by an explosion of neural network architectures among which convolutional neural networks (CNNs) [1, 2, 3], vision transformers (ViTs) [4, 5] and multi-layer perceptrons (MLPs) [6, 7, 8] are three major model categories. While CNNs were the de facto models for about a decade, recent progress shows that large ViT models have attained state-of-the-art performance on many visual recognition tasks such as image classification, image segmentation, and object detection. In principle, ViTs extend the philosophy of predominant transformer architectures [9] in natural language processing (NLP) to vision tasks. They convert an image into a sequence of equal-sized patches treated as tokens resembling words in NLP, then apply the dot-product self-attention mechanism over the sequence of image patches. ViTs designed in this way couple with a powerful data-hungry learning paradigm: models are first pre-trained on massive datasets (with supervised or self-supervised [10, 11] or cross-modality learning [12, 13]) and then fine-tuned on target datasets (with supervised learning). As the size of ViT models or pre-training datasets increases, the pre-trained models tend to have improved generalization performance. Despite this notable model performance scalability, the pre-training process of ViTs leads to significantly huge expenses. Furthermore, large pre-trained ViTs are memory-hungry and computationally intensive, prohibiting their deployment in many resource-constrained application scenarios. In contrast, CNNs and MLPs are still widely used in industry, due to the wider availability of effective implementations and optimizations compared to ViTs. Motivation of This Work. In parallel, knowledge distillation (KD) has proven to be a promising model compression pathway and has attracted lots of research interests. It relies on a teacher-student framework that transfers the knowledge learned by a large teacher model to a compact student model, aiming to make the student model can have improved performance to substitute the teacher model in deployment. However, most existing KD methods [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35] focus on CNN architectures, and usually perform evaluation on small datasets with non-mainstream student models for industrial applications, lagging far behind the evolution of neural network architectures. Although there have been few recent efforts [36, 37, 38, 39] on using ViT teachers, they explore narrow focuses that use small ViT teachers without pre-training on massive datasets, following the ways previously studied in CNN-based KD methods. In this paper, we attempt to connect knowledge distillation research with well pre-trained ViT models that stand out for their remarkable scalability, via a new viewpoint. Specifically, we question whether well pre-trained ViT models could be used as teachers that effectively transfer their scalable properties to target student models having different typed architectures such as CNN and MLP or heterogeneous ViT structures (we refer ‘cross architecture KD’ to such a more generalized formulation in this work), in the context of using mainstream large-scale visual recognition benchmarks. Problem Analysis. To answer the question in our motivation, we think the knowledge transfer difficulties are rooted in the following three aspects of differences: (1) Differences in feature computing paradigm. In terms of semantic units, ViTs operate on a sequence of equal-sized image patches added with positional embeddings, whereas CNNs operate on regular grids of pixels. In terms of core operations, ViTs rely on self-attention operations to model global feature dependencies, whereas CNNs rely on convolution operations to model local features. Although MLPs also use a patchify stem as ViTs, they rely on fully connected operations instead of self-attention operations and do not use positional embeddings, showing inferior feature learning ability. These differences in feature computing paradigm pose the first knowledge transfer barrier to overcome. (2) Differences in model scale. On the micro scale, model scale differences among ViTs, CNNs, and MLPs lie in network width, network depth, building blocks, etc. On the macro scale, model scale differences come from the capability of scaling the model size for ViTs, CNNs and MLPs towards better performance and generalization ability. As a result, these differences in model scale make the capacity of different network architectures typically vary significantly, emerging as the second knowledge transfer barrier to address. (3) Differences in knowledge density. Under the prevalent pre-training and fine-tuning paradigm, when scaling up pre-training datasets, large ViTs usually exhibit obviously superior performance scalability than top-performing CNNs and MLPs in terms of fine-tuning on both upstream image classification tasks and downstream dense prediction tasks [40, 41]. As for knowledge distillation in this work, we assume that pre-training datasets are no longer accessible and only well pre-trained ViT teacher models are available, avoiding the expensive pre-training process and making the setting well suited for real applications. Under this context, when training student models on upstream image classification datasets like ImageNet-1K, the knowledge density between teacher and student models is different, which appears as the third barrier to handle. From the above analysis, we can conclude that the design of effective schemes to align (1) feature computing paradigm differences, (2) model scale differences, and (3) knowledge density differences between the pre-trained ViT teacher and target student models, plays the key role to attain our goal. Design Insights and Contributions. Accordingly, we present Scalable Knowledge Distillation (ScaleKD), a simple and effective cross architecture KD method, which addresses the above difficulties in a progressive manner. Fundamentally, to bridge the feature computing paradigm differences between ViT and the other heterogeneous architectures, we propose cross attention projector (CAP, shown in Figure 1(a)), motivated by some previous works [42, 43, 44] that utilize cross attention mechanisms to align different modalities. For semantic unit differences, CAP utilizes positional embeddings and a patchify stem to transform the semantic units of CNN and MLP into transformer-like tokens. To further bridge core operation differences, CAP employs cross-attention operation and trainable queries that share the same attributes as the teacher’s features to model global interdependencies on the student’s features. In this way, CAP could align computing paradigm differences between the ViT teacher and the heterogeneous student in form, serving as the base component in our method. Different from feature computing paradigm differences, model scale differences and knowledge density differences are not explicitly and separately modeled in the KD process, as they are intertwined under the prevailing pre-training and fine-tuning paradigm and are finally encoded in teacher and student models’ feature space and parameter space. In light of this, we investigate both feature and parameter spaces of teacher and student models and observe two critical phenomena: Figure 1: Overview of three core components in our ScaleKD, which are (a) cross attention projector, (b) dual-view feature mimicking, and (c) teacher parameter perception. Note that the teacher model is frozen in the distillation process and there is no modification to the student’s model at inference. • Feature Space: As shown in Figure 2 and Figure 5, the frequency distributions of the features for the pre-trained ViTs are extremely imbalanced, where the direct component (zero frequency) response is dominant among all frequencies. This indicates that conducting feature distillation under such an imbalanced distribution may neglect the features of all other alternative components. • Parameter Space: As the parameters of the pre-trained ViTs in the fine-tuning stage are slightly changed, their pre-training knowledge remains in the parameter space. Although the pre-training datasets are not accessible in this work, the student still has the opportunity to obtain the pre-training knowledge by aligning its parameter space to the teacher’s. Inspired by these two insightful observations, we formulate our method from two new perspectives. Based on the observation in feature space, we design dual-view feature mimicking (DFM, shown in Figure 1(b)), whose key insight is to complement the neglected alternative features in the KD process. Specifically, DFM employs CAP as the feature projector and incorporates two feature mimicking paths. In the first path, DFM conducts feature mimicking in the original space to learn the teacher’s global features. In the second path, by removing the direct component in frequency space, DFM highlights the subtle alternative responses in feature mimicking, thus avoiding the neglect of these features. As a result, the two paths are complementary to each other, jointly promoting the feature space alignment. Based on the observation in parameter space, we propose teacher parameter perception (TPP, shown in Figure 1(c)), whose target is to transfer the pre-training knowledge by establishing a connection between teacher’s and student’s parameter spaces. Thanks to the aligned feature computing paradigm by CAP, TPP could bridge the student’s early stages to the teacher’s later stages and form a proxy feature processing path, where their parameter spaces join hands for KD optimization. By applying feature distillation in this path, the student’s parameter space tends to be gradually aligned with the teacher’s, and the pre-training knowledge would be transferred from the teacher to the student. Since the distillation learning processes in feature space and parameter space are the two sides of the same coin, DFM and TPP could naturally reinforce each other in essence. Benefited from the progressive designs, CAP, DFM, and TPP can be seamlessly integrated into a neat and effective cross architecture knowledge distillation method, called ScaleKD, which addresses the above three problems as a whole. Although ScaleKD has multiple feature mimicking paths, they only exist in the training stage. That is, ScaleKD does not alter the student’s structure and introduces no additional cost in the inference stage. By conducting systematic experiments on several mainstream large-scale vision benchmarks, we validate the effectiveness and generalization ability of our method. Figure 2: Feature distribution of BEiT-L/14 [41] in the frequency domain, where the direct component response is dominant. Details on drawing this figure are shown in Figure 5."
https://arxiv.org/html/2411.06782v1,QuadWBG: Generalizable Quadrupedal Whole-Body Grasping,"Legged robots with advanced manipulation capabilities have the potential to significantly improve household duties and urban maintenance. Despite considerable progress in developing robust locomotion and precise manipulation methods, seamlessly integrating these into cohesive whole-body control for real-world applications remains challenging. In this paper, we present a modular framework for robust and generalizable whole-body loco-manipulation controller based on a single arm-mounted camera. By using reinforcement learning (RL), we enable a robust low-level policy for command execution over 5 dimensions (5D) and a grasp-aware high-level policy guided by a novel metric, Generalized Oriented Reachability Map (GORM). The proposed system achieves state-of-the-art one-time grasping accuracy of 89% in real world, including challenging tasks such as grasping transparent objects. Through extensive simulations and real-world experiments, we demonstrate that our system can effectively manage a large workspace, from floor level to above body height, and perform diverse whole-body loco-manipulation tasks. See our robot at work: quadwbg.github.io.","I INTRODUCTION Quadrupedal loco-manipulation, which integrates legged locomotion with robotic arm manipulation, has emerged as a key research area due to its broad potential applications, including household assistance, urban maintenance, disaster relief, and autonomous field operations [1, 2, 3, 4]. Recent advancements in reinforcement learning (RL) have enabled the development of end-to-end policies for whole-body locomotion and manipulation [5, 6, 7, 8, 9], allowing robots to perform tasks that require seamless coordination of movement and object interaction. While end-to-end RL has substantially improved locomotion skills [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], loco-manipulation remains highly challenging due to the increased action dimensionality and complex physical interactions involved. These challenges often result in loco-manipulation policies with mediocre accuracy and limited generalizability [5, 7], especially when grasping objects of different shapes, sizes, and materials, thereby restricting their effectiveness in real-world applications. To enhance both the performance and generalizability of whole-body grasping systems, we draw inspiration from the success of various grasp detection techniques [23, 24, 25, 26, 27, 28, 29]. These methods demonstrate robust performance in detecting grasp poses for diverse, unseen objects in cluttered environments, including challenging materials like transparent or specular surfaces. By integrating grasp pose detection with motion planning, these approaches consistently achieve impressive accuracy, typically exceeding a 90% grasping success rate across arbitrary object configurations in table-top settings, as illustrated in 1. Figure 1: Real-world examples of whole-body grasping objects from various locations. Top row: Grasping transparent and specular objects in cluttered environments. Bottom row: Grasping of objects at various heights. This inspires us to take the best of both worlds via integrating legged locomotion with grasp detection to achieve high-performance and highly generalizable loco-manipulation. However, this combination is highly nontrivial. Directly applying grasp detection results for arm motion planning in legged robots is insufficient, as it ignores the coordination required between the robot’s body and arm movements. To address these challenges, we introduce QuadWBG (Generalizable Quadrupedal Whole-Body Grasping), a modular system consisting of four key modules: perception, planning, locomotion, and manipulation (see Figure 2). The perception module integrates object segmentation and grasp detection, handling object tracking and grasp pose prediction. The planning and locomotion modules function as high-level and low-level controllers, respectively, guiding the robot to approach the grasp pose. Finally, the manipulation module leverages motion planning to move the arm and execute the grasp while maintaining the body stationary. At the core of this system is a key innovation: the Generalized Oriented Reachability Map (GORM). GORM acts as a metric for evaluating the reachability of the base pose relative to the target pose across six degrees of freedom. It efficiently guides the planning module during training by calculating the optimal base pose for grasping tasks. GORM also captures the robot’s overall reachability from various positions and orientations, enabling the policy to select base poses that optimize both arm reachability and robot stability. This ensures precise grasping while maintaining balance and locomotion stability. Our framework offers additional advantages. By incorporating highly robust grasp detection modules trained on large-scale real and synthetic data, we eliminate the sim-to-real gap commonly encountered in end-to-end visual policy learning for manipulation. Furthermore, our framework provides clearer insights into system performance and allows for targeted optimization of each module. Through extensive simulations and real-world experiments, we demonstrate that our system achieves state-of-the-art performance in both grasping accuracy and handling a wide range of objects. The system consistently delivers robust whole-body control across a variety of tasks. Notably, it achieves a remarkable one-time grasping accuracy of 89% in real-world scenarios, even excelling in challenging tasks such as grasping transparent objects. These results underscore the system’s high precision and adaptability in complex environments. Figure 2: Pipeline of our system. First, we train a teacher-student 5D low-level policy in simulation (A). The perception module (B) then continuously tracks the object, and generates grasping pose guiding manipulation module (C). This pose is also utilized by the planning module (D) to command the locomotion policy."
https://arxiv.org/html/2411.06776v1,Machine vision-aware quality metrics for compressed image and video assessment,"A main goal in developing video-compression algorithms is to enhance human-perceived visual quality while maintaining file size. But modern video-analysis efforts such as detection and recognition, which are integral to video surveillance and autonomous vehicles, involve so much data that they necessitate machine-vision processing with minimal human intervention. In such cases, the video codec must be optimized for machine vision. This paper explores the effects of compression on detection and recognition algorithms (objects, faces, and license plates) and introduces novel full-reference image/video-quality metrics for each task, tailored to machine vision. Experimental results indicate our proposed metrics correlate better with the machine-vision results for the respective tasks than do existing image/video-quality metrics.","As the field of computer-vision continues to evolve, an increasing number of algorithms are being deployed in real-world applications. A popular application of this technology is video analytics, which has become integral to video surveillance, autonomous vehicles and other systems. Video analytics, for example, has two crucial tasks: detection and recognition. Depending on the system, the subjects of these tasks can be traffic signs, vehicles (object detection), human faces (detection/recognition), license plates (detection/recognition), and so on. As the number of video-surveillance cameras increases, automating these tasks becomes more critical given that human operators are incapable of processing such vast quantities of data. To ensure efficient storage and transmission of such extensive data, the captured images and videos require compression. Lossy compression standards such as JPEG, H.264/AVC, H.265/HEVC, and AV1 serve this purpose; their development involved optimizing the visual quality of the compressed content. The best way to assess visual quality is subjective human ratings, but they can be time-consuming and expensive. Hence the use of full reference (FR) quality-assessment methods such as PSNR, SSIM [24], and VMAF [2], some of which correlate highly with subjective scores [5]. These methods enable us to quickly and cost-effectively configure and develop codecs while emphasizing visual quality. Most state-of-the-art detection and recognition algorithms are based on deep neural networks, and their effectiveness is evaluated not visually, but through performance metrics. Compression directly affects the performance and reliability of computer-vision algorithms, especially at high compression ratios [6, 19]. Vision researchers and developers have therefore attempted to determine image quality for algorithms and develop codecs for machine vision [1]. For video surveillance system the main objects for analysis are people and vehicles, thus we choose three main video analytics algorithms for our machine-oriented quality metrics: object detection (including vehicles, persons, faces, and license plates), face recognition, license plates recognition. Often video surveillance systems use a specific detection/recognition algorithm (e.g. YOLOv5). Optimizing camera-data compression for them requires a comparatively fast method that predicts detection/recognition performance on the already encoded video, thereby enabling selection of encoding parameters to maximize that performance. If the target neural-network-based detection/recognition algorithms are evaluating the relative performance of codec prototypes or codec settings, they need lots of time and computational power owing to the number of parameters and the neural-network size. For instance, the recent x264 codec has almost 50 settings; selection of these parameters through an exhaustive search, even without using complex neural networks, would take centuries [30]. During investigation for our machine oriented metric we have following targets: 1. Achieve high correlation score with mentioned three main video analytics algorithms for its particular implementations with lower computational complexity. 2. Considering question about metric generalization for different implementation detection/recognition algorithms. Additionally, detection/recognition algorithms are imperfect, and identifying the cause of potential errors is impossible. For example, an object could be truly unrecognizable in the encoded image or video, necessitating quality improvement, or one algorithm may have certain limitations whereas another can detect the object error-free. Running multiple detection/recognition algorithms to improve the robustness of such an evaluation method would be even more time-consuming and computationally intensive. To address these issues, our paper makes three important contributions: 1. First, we propose a methodology of measuring image and video quality in terms of detection/recognition performance. 2. Second, we analyze detection- and recognition-performance correlation with that of popular image-quality-assessment (IQA) and video-quality-assessment (VQA) methods on widely used image and video codecs. Our results show little to no correlation between their outputs and detection/recognition performance. 3. Third, we propose new video-quality metrics based on convolutional-neural-network (CNN) models with respect to object detection, face detection/recognition, and license-plate detection/recognition performance. We validated our metrics by checking their correlation with the performance of the machine-vision algorithms for the corresponding tasks."
https://arxiv.org/html/2411.06767v1,PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing,"Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but often struggle with bug repair. We introduce a suit of methods to enhance LLM’s SQL bug-fixing abilities. The methods are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the ""disorientation"" in SQL code bug-fixing training. In our evaluation, the code LLM models trained with two methods have exceeds all current best performing model which size is much larger.","Recently, as large language models (LLMs) achieve remarkable success, code LLMs emerge as useful assistants when editing code. However, when we shift focus to fixing code errors, we find that the performance of open source pre-trained code LLMs like DeepSeek-Coder (Guo et al., 2024), WizardCoder (Luo et al., 2023) and Code Llama (Roziere et al., 2023) is quite limited (as shown in Table 1). In this paper, we especially focus on the code repair task of SQL. Due to the complex nested query structure, SQL code bugs are more difficult to solve compared with other code languages. We formulate the SQL code bug-fixing task as Equation 1. SQL_{correct}=f({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0,1}Schema},{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{% 1,0,0}SQL_{bug}},{\color[rgb]{0,1,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 0,1,1}\pgfsys@color@cmyk@stroke{1}{0}{0}{0}\pgfsys@color@cmyk@fill{1}{0}{0}{0}% R}) (1) Where the f represents the bug-fixing model. Schema means the related tables schemas of bug SQL code. SQLbug denote the SQL code which contains some bugs need to be fixed. R is the return message by the SQL execution system when you run the bug SQL code. SQL_{correct} is the bug-fixing model’s output, which is expected the right SQL code. We propose a set of methods to enhance the bug-fixing capabilities of Large Language Models (LLMs). This includes a method for mining and collecting supervised data, termed Progressive Dataset Construction (PDC), and an efficient training method based on dynamic masking, known as Dynamic Mask-SFT (DM-SFT). Experiments show that training with data collected via PDC method generally improved the SQL bug-fixing capabilities of open-source code LLMs by nearly +50\%. The Dynamic Mask-SFT training method further enhanced model performance by approximately +10\% relative to the default generative SFT."
https://arxiv.org/html/2411.06740v2,Dockformer: A transformer-based molecular docking paradigm for large-scale virtual screening,"Molecular docking enables virtual screening of compound libraries to identify potential ligands that target proteins of interest, a crucial step in drug development; however, as the size of the compound library increases, the computational complexity of traditional docking models increases. Deep learning algorithms can provide data-driven research and development models to increase the speed of the docking process. Unfortunately, few models can achieve superior screening performance compared to that of traditional models. Therefore, a novel deep learning-based docking approach named Dockformer is introduced in this study. Dockformer leverages multimodal information to capture the geometric topology and structural knowledge of molecules and can directly generate binding conformations with the corresponding confidence measures in an end-to-end manner. The experimental results show that Dockformer achieves success rates of 90.53% and 82.71% on the PDBbind core set and PoseBusters benchmarks, respectively, and more than a 100-fold increase in the inference process speed, outperforming almost all state-of-the-art docking methods. In addition, the ability of Dockformer to identify the main protease inhibitors of coronaviruses is demonstrated in a real-world virtual screening scenario. Considering its high docking accuracy and screening efficiency, Dockformer can be regarded as a powerful and robust tool in the field of drug design.","In drug discovery, identifying the candidate compounds that target biological macromolecules remains challenging because of the long development time and expensive wet-laboratory experiments. Virtual screening using molecular docking approaches can significantly improve the initial hit rate of drug candidates with great diversity and high binding affinity [1, 2]. Recently, the number of synthesizable molecules in make-on-demand libraries has expanded from 3.5 million to 29 billion. The docking performance can steadily improve as the library size increases [3]. However, in large-scale virtual screening (LSVS) tasks, the computational cost and time of docking methods become major challenges that most researchers in academia and industry cannot overcome [4]. Traditional docking approaches use scoring functions to measure the binding affinity of a given protein–ligand complex and then find the best binding conformation by applying optimization algorithms to minimize these functions [5]. For example, GOLD uses a genetic algorithm to search complex conformations [6], and AutoDock combines a genetic algorithm with a simulated annealing algorithm [7]. Although these optimization-based docking methods are commonly used in modern drug designs because of their good usability and interpretability, they still face the following challenges: the scoring functions are generally not precise enough, and optimization algorithms cannot guarantee that the global optimum is found each time. Although several advanced methods can offer reliable binding affinity predictions [8, 9], docking approaches require multiple independent optimization processes to sample possible binding conformations for each protein–ligand pair, leading to very high computational costs in LSVS tasks [2, 10]. Inspired by the groundbreaking advancement of AlphaFold2 in protein structure prediction [11], a series of deep learning (DL)-based methods have emerged to solve molecular docking tasks [12]. According to the types of neural network architectures, they can be divided into three categories: graph neural networks (GNNs)-based [13], transformer-based [14] and diffusion model-based docking methods [15]. The primary motivation of these studies is twofold: first, improving the ligand docking accuracy with the aid of the power learning capabilities of DL technologies, and second, speeding up the screening process by directly predicting ligand binding conformations to skip the time-consuming optimization of traditional docking approaches [16]. However, although tremendous efforts have been made to develop DL-based docking tools, few can perform well in docking accuracy and screening speed simultaneously due to inadequate generalizability and non-end-to-end architectures [17, 18]. Therefore, how to use DL models to generate protein-ligand binding conformations precisely and efficiently is still an open question in LSVS tasks. In this study, a novel transformer-based architecture named Dockformer, which is shown in Fig. 1, is proposed. Dockformer uses two separate encoders to leverage multimodal information to generate latent embeddings of proteins and ligands and can thus effectively capture molecular geometric details, including 2D graph topology and 3D structural knowledge. A binding module is then employed to detect intermolecular relationships effectively on the basis of learned latent embeddings. Finally, in the structure module, the established relationships are utilized to generate the complex conformations directly, and the coordinates of the ligand atoms are calculated in an end-to-end manner. In addition, the corresponding confidence measures of each generated conformation are utilized to distinguish binding strengths instead of traditional scoring functions. Dockformer achieves superior docking accuracy compared with that of state-of-the-art DL-based and optimization-based docking methods and simultaneously speeds up the conformation generation process by orders of magnitude. Thus, this method is capable of meeting the rapid throughput requirements of LSVS tasks. Dockformer, as a robust and reliable protein–ligand docking approach, may significantly reduce the development cycle and cost of drug design. The remainder of this paper is organized as follows: Section II introduces related works in molecular docking. In Section III, the architecture details of Dockformer are presented. Section IV analyzes docking performance and utilizes confidence metrics for large-scale virtual screening, while discussing optimization algorithms for physical plausibility. Finally, Section V reviews the use of AI technologies for screening large-scale compound libraries and discusses the potential of de novo drug design using generative models and deep reinforcement learning to streamline the screening process."
https://arxiv.org/html/2411.06723v1,Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted Dialogue Scripts and Therapeutic Strategies for Psychotherapy,"Chatbots or conversational agents (CAs) are increasingly used to improve access to digital psychotherapy. Many current systems rely on rigid, rule-based designs, heavily dependent on expert-crafted dialogue scripts for guiding therapeutic conversations. Although recent advances in large language models (LLMs) offer the potential for more flexible interactions, their lack of controllability and transparency poses significant challenges in sensitive areas like psychotherapy. In this work, we explored how aligning LLMs with expert-crafted scripts can enhance psychotherapeutic chatbot performance. Our comparative study showed that LLMs aligned with expert-crafted scripts through prompting and fine-tuning significantly outperformed both pure LLMs and rule-based chatbots, achieving a more effective balance between dialogue flexibility and adherence to therapeutic principles. Building on findings, we proposed “Script-Strategy Aligned Generation (SSAG)”, a flexible alignment approach that reduces reliance on fully scripted content while enhancing LLMs’ therapeutic adherence and controllability. In a 10-day field study, SSAG demonstrated performance comparable to full script alignment and outperformed rule-based chatbots, empirically supporting SSAG as an efficient approach for aligning LLMs with domain expertise. Our work advances LLM applications in psychotherapy by providing a controllable, adaptable, and scalable solution for digital interventions, reducing reliance on expert effort. It also provides a collaborative framework for domain experts and developers to efficiently build expertise-aligned chatbots, broadening access to psychotherapy and behavioral interventions.","The integration of chatbots, or conversational agents (CAs), into psychotherapy and behavioral interventions (Almusharraf F, 2020; Sun et al., 2023a; He et al., 2022b; Park et al., 2019; He et al., 2022a; Zhang et al., 2020; Xu and Zhuang, 2020) has transformed the way mental health services are delivered, providing around-the-clock accessibility and support. Traditionally, these chatbots have relied heavily on rule-based systems (McTear, 2021) where expert-written scripts (Silva and Canedo, 2022; Urban and Mailey, 2019) were needed to facilitate therapeutic dialogues, ensuring controlled and therapeutically precise interactions. expert-written scripts (Silva and Canedo, 2022; Urban and Mailey, 2019) and While this approach is reliable, it often leads to rigid conversations that lack flexibility and diversity, largely due to the need for extensive domain expertise in designing and crafting dialogues. The advent of Natural Language Generation (NLG) (Dong et al., 2022) has marked a shift away from these strictly predefined interactions, with systems being developed to generate more dynamic conversations. For example, Motivational Interviewing (MI) (Miller and Rollnick, 2002) chatbots have employed rephrasing techniques and template-based (Dieter et al., 2019) dialogue generation to improve user engagement (Almusharraf F, 2020; He et al., 2022a; Min et al., 2023). However, despite these advancements, such NLG systems are still limited by their reliance on domain-specific data, which is especially scarce and sensitive in the field of psychotherapy. The rise of large language models (LLMs) (Brown et al., 2020) in psychotherapy has shown promising potential (Demszky et al., 2023), offering new pathways for personalized support and engaging interactions in therapeutic settings. Trained on vast amounts of human dialogue, LLMs can handle flexible conversations and simulate conversational empathy and understanding without additional training (Syed et al., 2024; Sharma et al., 2023), a critical foundation for emotional intelligence in mental health care. However, significant challenges remain, particularly the non-transparent and uncontrollable nature of these models (Ehsan et al., 2021; El Ali et al., 2024), which raises quality and safety concerns in sensitive psychotherapy contexts. In addition, LLMs also face two critical challenges when applied to the specific domains of psychotherapy: (1) LLMs often lack domain-specific knowledge required to initiate conversations on specific psychotherapeutic topics, such as increasing intrinsic motivation or cognitive behavioral practices (Beck, 2011) like “mindfulness” (Ferguson et al., 2021), “should statements” (Sho, 2023), as shown in Fig 2, and (2) LLMs struggle to generate therapeutic questions (see Fig 2) and reflections that are essential for guiding a meaningful therapeutic dialogue, particularly in delivering psychotherapy-embedded intervention techniques such as Motivational Interviewing (MI). As a result, domain experts in clinical and health psychology still tend to favor rule-based systems, as they ensure precision and relevance through expert-crafted scripts, although the problems of rigid and unengaging conversations remain. However, this reliance on predefined dialogue scripts introduces another challenge: (3) creating these expert-crafted dialogues with domain expertise is highly time-consuming and costly, limiting scalability. Figure 2. A dialogue example: parts of tree-structured dialogue scripts pre-crafted by experts under the psychotherapeutic topic “Supportive social environment” for behavioral intervention. To address these gaps, we propose “Script-Aligned Generation (SAG)”, a concept for aligning LLMs with expert-crafted dialogue scripts for psychotherapy. SAG minimizes the strict reliance on fully scripted dialogue structures, enhancing both the controllability and transparency of LLM-driven chatbots while preserving the natural flexibility of LLMs. This leads to our first research question RQ1: How do LLM-powered chatbots aligned with expert-crafted dialogue scripts compare to both rule-based chatbots and pure LLMs in delivering psychotherapy for behavioral intervention? To validate the potential of SAG, we conducted a study comparing three chatbots: (1) rule-based chatbots strictly based on expert-crafted scripts, (2) LLM-powered chatbots employing SAG via fine-tuning or prompting, and (3) a pure LLM without expertise alignment. Our findings revealed that SAG-aligned LLM chatbots outperformed both rule-based and pure LLMs across key metrics, including linguistic quality, therapeutic relevance, engagement, perceived empathy, MI adherence, and motivation enhancement. However, we acknowledge that the prompt engineering or fine-tuning techniques we used in this study still require a relatively large amount of expert scripting, which might be challenging to collect in the real world. To further reduce dependence on expert scripting, we introduce an approach “Script-Strategy Aligned Generation (SSAG)” which can flexibly align LLMs with expert knowledge through key therapeutic strategies in MI (Shah et al., 2022; Miller et al., 2002), such as “asking questions” to progress therapy and “reflective listening” to express empathy. SSAG provides a scalable framework, enabling domain experts and developers to collaboratively create effective, contextually relevant chatbot-delivered interventions. This leads to our second research question RQ2: How can LLMs be efficiently aligned with expert-crafted dialogue scripts for psychotherapy through (a) strict alignment by fine-tuning or prompting, or (b) flexible alignment with the proposed approach “Script-Strategy Aligned Generation (SSAG)”? To evaluate the efficiency of SSAG, we conducted a 10-day field study comparing LLM-powered chatbots using SAG (via prompting) with our proposed SSAG approach, using the rule-based chatbot as a control. Results showed no significant differences between SAG-Prompt and SSAG, demonstrating that SSAG effectively aligns LLMs with domain expertise while reducing reliance on fully pre-scripted dialogues. These findings highlight SSAG’s potential to efficiently elicit therapeutic expertise from LLMs, minimizing the need for extensive pre-scripting by experts. This work makes key contributions to computational linguistics, digital healthcare, and psychotherapy by advancing LLM-driven digital interventions. We empirically evaluated LLM-powered chatbots employing both the concept of SAG and our proposed flexible alignment approach, SSAG, comparing them to traditional rule-based systems and pure LLMs. The evaluation provides insights that prompting, as an alignment method, is more efficient and scalable than fine-tuning, and SSAG further enhances flexibility without compromising therapeutic quality. Additionally, we developed the first dataset of expert-crafted dialogues for psychotherapy based on MI and CBT frameworks, supporting future research on LLM alignment with domain expertise. Collectively, these contributions emphasize the importance of human expertise in guiding LLMs within sensitive domains, paving the way for scalable, accessible, and cost-effective psychotherapeutic tools that can transform therapeutically aligned health support in the era of LLMs."
https://arxiv.org/html/2411.06722v1,"Synthesize, Partition, then Adapt: 
Eliciting Diverse Samples from Foundation Models","Presenting users with diverse responses from foundation models is crucial for enhancing user experience and accommodating varying preferences. However, generating multiple high-quality and diverse responses without sacrificing accuracy remains a challenge, especially when using greedy sampling. In this work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that leverages the abundant synthetic data available in many domains to elicit diverse responses from foundation models. By leveraging signal provided by data attribution methods such as influence function, SPA partitions data into subsets, each targeting unique aspects of the data, and trains multiple model adaptations optimized for these subsets. Experimental results demonstrate the effectiveness of our approach in diversifying foundation model responses while maintaining high quality, showcased through the HumanEval and MBPP tasks in the code generation domain and several tasks in the natural language understanding domain, highlighting its potential to enrich user experience across various applications.","Transformer-based foundation models have revolutionized the fields of natural language processing (NLP) and code generation with their remarkable abilities a wide range of understanding and generation tasks (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020; Chen et al., 2021). These models are typically pre-trained on vast amounts of text data and then undergo instruction fine-tuning — a post-training process — to improve alignment with user expectations and enhance the overall user experience (Ouyang et al., 2022). Due to the high cost of human-annotated data, synthetically generated datasets (Wang et al., 2022b) such as OSS-Instruct (Wei et al., 2023) and Alpaca (Taori et al., 2023) have become an important component of instruction tuning, demonstrating strong effectiveness in improving foundation model performance. To date, these synthetic datasets have been primarily used to align foundation models with instructions or to induce certain preferable behaviors. In this paper, we focus on a different use of synthetic data: in improving the diversity of foundation models’ outputs. Diversifying the generated responses is crucial for accommodating diverse user preferences and enhancing user satisfaction. Consider the scenario illustrated in Fig. 1, where a user prompts a foundation model with “Give me a personal website template”. In this case, we would prefer the model to generate two diverse templates while maintaining good quality, providing users with a variety of styles and layouts. Conventional methods for improving diversity, such as temperature sampling (Ackley et al., 1985; Hinton et al., 2015; Wang et al., 2019, 2023), rely on sampling techniques that anneal the probabilistic distribution of outputs. These methods often trade off diversity for quality, as the generated responses may deviate from the learned distribution and produce hallucination or less coherent outputs (Lee, 2023). Moreover, these techniques are not applicable when using greedy sampling, which is often preferred for its simplicity and precision. This highlights the need for approaches that not only align foundation model outputs with user expectations but also elicit diverse responses without sacrificing quality. In this paper, we present a framework, Synthesize-Partition-Adapt (SPA), that achieves these objectives. The framework partitions the synthetic data and adapts foundation models to these partitions in the post-training stage. By leveraging the inherent diversity in the training data, this approach can generate diverse responses without compromising accuracy. The potential of partition-and-adapt approach is further amplified by the increasing availability of large-scale synthetic datasets because the utility of instruction-tuning a single model on the entire dataset diminishes. In particular, we show that influence function (Koh & Liang, 2017) can be an effective signal to partition synthetic datasets into subsets, each targeting unique aspects that elicit distinct model behaviors. However, SPA is not limited to influence function and can be extended to other partitioning strategies. By training multiple adaptations on these subsets using parameter-efficient fine-tuning techniques, such as LoRA (Hu et al., 2021), we enable the generation of diverse and accurate responses. To demonstrate the effectiveness of our approach, we conduct experiments on a range of tasks in both the code generation and natural language understanding domains. We evaluate our method on the HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) datasets for code generation, as well as several natural language understanding tasks. The results showcase the ability of our approach to diversify model responses while maintaining high accuracy, highlighting its potential to enrich user experience across various applications. To summarize, the main contributions of this paper are as follows: • We propose SPA, a novel framework that leverages synthetic data, data partitioning, and model adaptation to elicit diverse responses from foundation models. • We demonstrate the effectiveness of SPA in diversifying foundation model responses while maintaining sampling quality through extensive experiments on code generation and natural language understanding tasks. • We highlight the potential of SPA to leverage the increasing availability of large-scale synthetic datasets for improving the diversity of foundation model responses."
https://arxiv.org/html/2411.06681v1,WDMoE: Wireless Distributed Mixture of Experts for Large Language Models,"Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but the role of wireless networks in supporting LLMs has not been thoroughly explored. In this paper, we propose a wireless distributed Mixture of Experts (WDMoE) architecture to enable collaborative deployment of LLMs across edge servers at the base station (BS) and mobile devices in wireless networks. Specifically, we decompose the MoE layer in LLMs by placing the gating network and the preceding neural network layer at BS, while distributing the expert networks among the devices. This deployment leverages the parallel inference capabilities of expert networks on mobile devices, effectively utilizing the limited computing and caching resources of these devices. Accordingly, we develop a performance metric for WDMoE-based LLMs, which accounts for both model capability and latency. To minimize the latency while maintaining accuracy, we jointly optimize expert selection and bandwidth allocation based on the performance metric. Moreover, we build a hardware testbed using NVIDIA Jetson kits to validate the effectiveness of WDMoE. Both theoretical simulations and practical hardware experiments demonstrate that the proposed method can significantly reduce the latency without compromising LLM performance.","The exciting advancements in large language models (LLMs) have sparked a new wave of AI innovation. LLMs, exemplified by ChatGPT[2], have demonstrated emergent abilities[3], including better generalization, nuanced meaning comprehension, and remarkable reasoning and generation capabilities. These advancements have led to widespread applications across various fields, illuminating the vision of artificial general intelligence (AGI)[4]. In the field of 6G wireless networks, LLMs have been used for wireless network resource allocation[5, 6, 7], and applied in internet of vehicles[8] and immersive communications[9]. The emergent abilities of LLMs stem from extensive computation, a large number of model parameters, and massive training datasets[3, 10, 11]. The vast number of model parameters poses significant challenges for training, inference, and deployment. The training phase of LLMs involves significant costs in time and computational power for most individuals and organizations. Regarding LLMs inference and deployment, they also require fast responses and ample memory. In this paper, we mainly focus on LLMs inference and deployment. Currently, LLMs can be classified into cloud-based LLMs and on-device LLMs based on their deployment characteristics. Cloud servers with numerous graphics processing units (GPUs) and sufficient power supply are responsible for the majority of model inference and deployment. Due to concerns over latency and data privacy, the potential of on-device LLMs is gaining increasing attention[12]. Researchers compress LLMs through pruning[13], quantization[14], and distillation[15] to meet the memory, computation, and energy requirements of mobile devices. Limited by generation speed and model capabilities, even a company as strong as Apple has not been able to deploy a fully satisfactory LLM on mobile devices. On the latest iPhone 16 Pro series, only simple tasks are completed locally by a model with around 3 billion parameters, whereas complex tasks are still handled by cloud-based models like ChatGPT[16, 17]. Although in practical LLMs application transformer’s KV cache[18] can speed up the inference, it will cause considerable memory overhead, which presents an obstacle for on-device LLMs. In light of the rapid advancements of LLMs and the widespread adoption of 5G/6G wireless networks, a natural question arises: can wireless networks support LLMs? If so, how? The answer lies in fully leveraging the multidimensional resources of wireless networks, incorporating computing, communications, and caching (3C) to support LLMs and enhance user experience [19]. As a key technology of 5G, mobile edge computing (MEC) has been thoroughly studied to improve the quality of service for various network applications. Recently, edge-cloud collaborative training and fine-tuning are also researched[20, 21]. However, there is a lack of specialized optimization research tailored to the characteristics of LLMs in the scenario of distributed deployment in wireless networks. To address this issue, this paper aims to bridge the gap by proposing a distributed deployment of LLMs powered by mixture of experts (MoE), modeling and optimizing the latency during the inference phase, which can efficiently utilize the 3C resources at both the MEC server and mobile devices. The core idea of MoE architecture is to sparsify the neural networks (NNs) based on the observation of sparse activation of NNs and allow each expert to specialize in their specific tasks, reducing the floating point operations (FLOPs) to achieve acceleration and increasing model capacity[22]. Transformer with MoE111In this paper, MoE refers to Transformer with MoE unless otherwise specified.[23] replaces the original single dense feedforward neural network (FFN) with a gating network and multiple smaller expert networks, which often have the same structure and different hidden layer dimensions. The expert networks within the same MoE layer operate in parallel and independently, providing high fault tolerance and making it well-suited for wireless distributed deployment. Besides, we find during the inference phase of MoE-based LLMs, decreasing the number of participating experts will not degrade model performance, showcasing the robustness of MoE. Capitalizing on these attributes, we can deploy the gating network at MEC server and each expert network in an MoE layer on diverse mobile devices. Furthermore, due to the diversity of computing capabilities among the mobile devices and communication qualities between the base station (BS) and the mobile devices, how to achieve a good balance between the inference accuracy and latency also requires careful consideration. I-A Related Work Current mainstream LLMs utilize Transformer architecture[24]. LLMs based on the Transformer architecture can be categorized into three types: encoder-decoder, decoder-only, and encoder-only. For encoder-decoder structure, in the vanilla Transformer[24], the multi-head attention mechanism fully replaces traditional recurrent neural networks, making the architecture more suitable for parallel computing. For decoder-only structure, in [25], a decoder-only Transformer is utilized with the optimization objective of standard language modeling. This work achieves state of the art performance, and the model is named GPT. For the encoder-only structure, [26] designs Bidirectional Encoder Representations from Transformers (BERT) and applies the training paradigm of pre-training and fine-tuning to language model. BERT includes a base model with 110 million parameters, similar to GPT and a large model with 340 million parameters. The work [25] and [26] lay the foundation for the model architecture, training paradigm, and development direction of large language models. GPT-2 is a scaled-up version of GPT with 1.5 billion parameters, focusing on zero-shot performance in [27]. The work [10] proposes the scaling law of transformer-based language models and suggests that larger models trained on more data tend to perform better, supporting the later development of large language models. GPT-3 is released in [28] with 175 billion parameters and was the largest language model at that time. It is applied without fine-tuning and achieves state-of-the-art performance on various benchmarks. Following the scaling law, an increasing number of large language models have appeared with more parameters, and the largest open source model is Llama 3.1, with 405 billion parameters, based on a dense Transformer[29]. Large model size makes the training and inference computationally expensive. Various methods of training and inference have been proposed to improve model performance without excessive computational costs[22], among which the MoE architecture has been well-researched. The basic MoE layer consists of a gating network and a number of experts, which are simple feed-forward neural networks[30]. The work [22] introduces a Sparsely-Gated MoE and expands the LSTM-based language model to a configuration with 137 billion parameters. In the Transformer era, [23] replaces all Feed-forward layers in the model with MoE layers and achieves enhanced model capacity while substantially reducing the training time. Mistral AI successively releases two open-source MoE large language models, Mixtral-8x7B and Mixtral-8x22B, both achieving the state-of-the-art performance among open-source large models at the time of their release[31]. The work [32] introduces an adaptive MoE framework, AdaMV-MoE, designed for multi-task vision recognition. Unlike conventional MoE approaches with a fixed number of experts, AdaMV-MoE dynamically adjusts the number of active experts per task based on training dynamics, eliminating the need for manual tuning of model capacity. Based on the idea that harder tasks need more experts, the work [33] develops a method for dynamically adjusting the number and selection of experts to reduce computational costs for simple tasks while enhancing model performance for complex ones. Advance in MoE models have unlocked the potential of model capability and computational efficiency. On-device LLMs have been extensively researched as a promising solution for personal assistants. By running LLMs directly on devices such as smartphones or edge servers, they offer enhanced privacy, lower latency, and reduced dependence on cloud servers. The work [34] devises a KV cache compression and swapping method with accuracy tolerance awareness, significantly reducing the context switching latency and extending the capacity of active contexts. In [35], a highly efficient computation and memory framework named Edge-LLM is proposed, reducing computation and memory overhead through layer-wise unified compression and adaptive backpropagation depth reduction. MobileLLM is a model family with sub-billion parameters that surpasses previous sub-billion models on mainstream benchmarks by adopting deep and thin network architectures, embedding sharing, and a grouped-query attention mechanism[36]. With privacy considerations, [37] employs derivative-free optimization to update the local LLM’s parameters during on-device fine-tuning phase under the resource-constrained circumstances. (a) (b) Figure 1: (a) MoE-based LLMs architecture[23]; (b) The proposed WDMoE-based LLMs system model. I-B Contributions Motivated by the above, we explore a wireless distributed deployment architecture for LLMs with MoE, define and model key performance metrics of LLMs within the framework, and optimize resource allocation and expert selection to improve the LLMs service capabilities within the wireless network. The main contributions of this paper are summarized as follows: • We propose a novel wireless distributed MoE architecture, WDMoE, for LLMs. The fundamental principle of the WDMoE architecture is to leverage the independence and parallelism of expert networks by deploying them on multiple mobile devices, thereby utilizing the resource advantages of wireless networks to facilitate distributed deployment of large models. This architecture places the computationally intensive and memory-demanding multi-head attention mechanism on the MEC servers at BS. Through collaborative token processing between BS and mobile devices, the wireless network can support a large number of LLMs service requests, addressing the challenges of limited memory on a single device and data privacy concerns in cloud-based LLMs deployment. • We establish latency-aware performance metrics of the WDMoE-based LLMs. By analyzing the attention mechanism, we find that, during the wireless distributed deployment of LLMs with MoE, the latency for expert networks on different mobile devices to process tokens and return them to the BS varies. This discrepancy can cause earlier-arriving tokens to wait at the attention module. We model this latency, referred to as attention waiting latency, and design a bilevel optimization problem where the upper-level objective is to minimize this latency while ensuring model capabilities at the lower level, thereby reducing latency without compromising model performance. In addition, we propose an innovative metric, the weight latency ratio (WLR), to comprehensively consider the output weights of the MoE gating network and the attention waiting latency of each device. • We develop an expert selection policy to improve the performance. Based on the defined WLR, the expert selection policy can process tokens by considering the processing time and weight of each mobile device for allocated tokens. It dynamically adjusts the number of experts per token, thereby reducing network transmission and computational load. • We build a hardware testbed using NVIDIA Jetson kits to validate the effectiveness of the proposed WDMoE. We use three NVIDIA Jetson kits and one personal computer (PC) with an NVIDIA RTX 4070 Ti, serving as four mobile devices, each running one expert for each layer, and communicating with the server via WiFi. Both simulation results and hardware experiment results show that the proposed WDMoE architecture and expert selection policy effectively reduce the latency experienced by users. For example, compared to vanilla expert selection, the proposed WDMoE reduces latency by 45.75% on average on the PIQA dataset without model capability deterioration. The rest of this paper is organized as follows. The WDMoE-based LLM architecture is introduced in Section II. The system model and problem formulation are presented in Section III. The WDMoE expert selection policy and bandwidth allocation algorithm are introduced in Sections IV. Section V shows extensive simulation results. Finally, the hardware testbed experiments are presented in Section VI, and conclusions are drawn in Section VII."
https://arxiv.org/html/2411.06672v1,What Should Baby Models Read? Exploring Sample-Efficient Data Composition on Model Performance,"We explore the impact of pre-training data composition on the performance of small language models in a sample-efficient setting. Using datasets limited to 10 million words, we evaluate several dataset sources—including child-directed speech (CHILDES), classic books (Gutenberg), synthetic data (TinyStories), and a mix of these (Mix)—across different model sizes ranging from 18 million to 705 million parameters. Our experiments show that smaller models (e.g., GPT2-18M and GPT2-44M) benefit from training on diverse datasets like Mix, achieving better performance on linguistic benchmarks. In contrast, larger models (e.g., GPT2-97M, GPT2-705M, and LLaMA-360M) perform better when trained on more complex and rich datasets like Gutenberg. Models trained on the CHILDES and TinyStories datasets underperformed across all model sizes. These findings suggest that the optimal dataset for sample efficient training depends on the model size, and that neither child-directed speech nor simplified stories are optimal for language models of all sizes. We highlight the importance of considering both dataset composition and model capacity for effective sample efficient language model training.","In recent years, advancements in natural language processing have been largely driven by scaling language models to unprecedented sizes. Various large-language model (LLM) scaling laws have been formulated Sardana et al. (2024), with perhaps the most influential being the Chinchilla law, which demonstrates that parameters and tokens scale approximately linearly as the model scales Hoffmann et al. (2024). Many subsequent LLMs have been trained following this model Rae et al. (2021), with some models including the Llama 2 and Llama 3 family of models being trained on 2 and 15 trillion tokens respectively, far more than the ’optimal’ amount according to the Chinchilla scaling law Dubey et al. (2024). However, it is often prohibitive to train such large models, and impractical to continue scaling with the amounts of data required to train such models. This has sparked interest in small language models Schick and Schütze (2021); Magister et al. (2023) with much fewer parameters, requiring much less data for training. While much research has been conducted on knowledge distillation and improving the model architecture for small language models, comparably less research has investigated the contributions of different types of data used for model training, which is arguably just as important. Indeed, because LLM pretraining data typically comprises a mix of sources Chowdhery et al. (2023), researchers have found that the composition of pretrained data greatly affects model performance Du et al. (2022); Wei et al. (2015), though determining the optimal recipe for pretraining data is challenging. Recent research exploring optimization of pretraining data for LLMs at scale includes DoReMi, which trains a small proxy model to produce domain weights for downstream tasks, and then uses the model to resample the dataset for training huge LLMs Xie et al. (2024). However, the question of how to choose data for sample-efficient training of small language models, such as in cases where computational resources are limited, has received little attention. Psycholinguistic precedent exists for sample-efficient pretraining; children see much less words than a modern LLM yet perform exceptionally well on reasoning tasks. For example, Chinchilla sees over 10000 times the number of words a 13 year old child has ever encountered Choshen et al. (2024). By the time typical English-speaking children at around 6 years old have obtained adult-level grammatical knowledge Kemp et al. (2005), they have seen only around 10-50M words Hart et al. (1997); Huebner et al. (2021). In comparison, Llama-3 is trained on 15T tokens Dubey et al. (2024). Given the great disparity between the amount of training data an LLM requires and what children require, it seems worthwhile to investigate whether training LLMs can be as sample efficient. BabyBERTa Huebner et al. (2021) attempts to address this, showing that when training a model on data similar to what is seen by children between the ages 1 and 6, it is able to acquire grammatical knowledge similar to pretrained RoBERTa-base, but with around 15X fewer parameters and 6,000X fewer words; this indicates that utilizing child-directed input may be advantageous for more sample efficient pretraining Huebner et al. (2021). Similarly, Eldan and Li (2023) follow suit, releasing TinyStories, a synthetic dataset of short stories that only contain words that typical 3- to 4-year-old children understand. They demonstrate that TinyStories can be leveraged to train language models with much less parameters than SOTA models, yet still produce coherent output with almost perfect grammar as well as emergent reasoning abilities. Along the same vein, GPT-wee Bunzeck and Zarrieß (2023) shows that child-directed speech can be used with curriculum learning for simulating children’s learning as a potential solution to sample-constrained training. In this paper, we evaluate the effect of different datasets on model performance for sample efficient model training. In our case, we limit our training dataset to 10M words, in accordance with the BabyLM Challenge’s super-strict track Choshen et al. (2024). We consider several different types of datasets, namely child-directed speech (CHILDES), classic books (Gutenberg), a mixed dataset (Mix) and the TinyStories dataset. Experimental results show that smaller models benefit from training on diverse datasets like Mix on the BabyLM evaluation suite Choshen et al. (2024), but larger models perform better when trained on more complex and rich datasets like Gutenberg. Our findings suggest that the optimal dataset depends on the model size and that neither child-directed speech nor child-directed stories are optimal for language models of any sizes."
https://arxiv.org/html/2411.06659v1,An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning,"Incremental graph learning has gained significant attention for its ability to address the catastrophic forgetting problem in graph representation learning. However, traditional methods often rely on a large number of labels for node classification, which is impractical in real-world applications. This makes few-shot incremental learning on graphs a pressing need. Current methods typically require extensive training samples from meta-learning to build memory and perform intensive fine-tuning of GNN parameters, leading to high memory consumption and potential loss of previously learned knowledge. To tackle these challenges, we introduce Mecoin, an efficient method for building and maintaining memory. Mecoin employs Structured Memory Units to cache prototypes of learned categories, as well as Memory Construction Modules to update these prototypes for new categories through interactions between the nodes and the cached prototypes. Additionally, we have designed a Memory Representation Adaptation Module to store probabilities associated with each class prototype, reducing the need for parameter fine-tuning and lowering the forgetting rate. When a sample matches its corresponding class prototype, the relevant probabilities are retrieved from the MRaM. Knowledge is then distilled back into the GNN through a Graph Knowledge Distillation Module, preserving the model’s memory. We analyze the effectiveness of Mecoin in terms of generalization error and explore the impact of different distillation strategies on model performance through experiments and VC-dimension analysis. Compared to other related works, Mecoin shows superior performance in accuracy and forgetting rate. Our code is publicly available on the Mecoin-GFSCIL.","In the field of graph learning, conventional methods often assume that graphs are static[1]. However, in the real world, graphs tend to grow over time, with new nodes and edges gradually emerging. For example, in citation networks, new papers are published and cited; in e-commerce, new products are introduced and updated; and in social networks, new social groups form as users join. In these dynamic contexts, simply updating graph representation learning methods with new data often leads to catastrophic forgetting of previously acquired knowledge. Despite numerous methods proposed to mitigate the catastrophic forgetting problem in Graph Neural Networks(GNNs)[2, 3, 4], a critical and frequently neglected challenge is the scarcity of labels for newly introduced nodes. Most current graph incremental learning methods [5, 6] combat catastrophic forgetting by retaining a substantial number of nodes from previous graphs to preserve prior knowledge. However, these methods become impractical in graph few-shot class-incremental learning(GFSCIL) scenarios due to the limited labeled node samples. Some methods[7, 1] employ regularization to maintain the stability of parameters critical to the graph’s topology. Yet, in GFSCIL, the label scarcity complicates accurate assessment of the relationship between parameter importance and the underlying graph structure, thus increasing the difficulty of designing effective regularization strategies. Consequently, the issue of label scarcity hampers the ability of existing graph continual learning methods to effectively generalize in graph few-shot learning scenarios. GFSCIL presents two critical challenges: 1)How can we empower models to learn effectively from limited samples? 2)How can we efficiently retain prior knowledge with imited samples? While the first challenge has been extensively explored in graph few-shot learning contexts[8, 9], this paper focuses on the second. Currently, discussions on the latter issue within GFSCIL are relatively scarce. Existing methods[10, 11] primarily focus on enhancing models’ ability to learn from few-shot graphs and preserve prior knowledge by learning class prototypes through meta-learning and attention mechanisms. However, to strengthen inductive bias towards graph structures and learn more representative class prototypes, these approaches require caching numerous training samples from the meta-learning process for subsequent GFSCIL tasks. This caching not only consumes significant memory but also imposes substantial computational costs. Furthermore, these methods extensively adjust parameters during prototype updates, risking the loss of previously acquired knowledge[12, 13]. These challenges underscore the need for innovative strategies that maintain efficiency without compromising the retention of valuable historical data—achieved through minimal memory footprint, high computational efficiency, and limited parameter adjustments. To address the aforementioned challenges, we introduce Mecoin, an efficient memory construction and interaction module. Mecoin consists of two core components: the Structured Memory Unit(SMU) for learning and storing class prototypes, and the Memory Representation Adaptive Module(MRaM) for dynamic memory interactions with the GNN. To effectively leverage graph structural information, we design the Memory Construction module(MeCs) within the SMU. MeCs facilitates interaction between features of the input node and prototype representations stored in the SMU through self-attention mechanisms, thereby updating sample representations. Then, it utilizes the local structural information of input nodes to extract local graph structural information and compute a local graph structure information matrix(GraphInfo). The updated sample representations and GraphInfo are used to calculate class prototypes for the input nodes. These newly obtained prototypes are compared with those stored in the SMU using Euclidean distance to determine whether the input samples belong to seen classes. If a sample belongs to a seen class, the corresponding prototype in the SMU remains unchanged. Conversely, if a sample belongs to an unseen class, its calculated prototype is added to the SMU. Figure 1: Overview of the Mecoin framework for GFSCIL. (a)Graph neural network: Consists of a GNN encoder and a classifier(MLP) pre-trained by GNN. In GFSCIL tasks, the encoder parameters are frozen. (b)Structured Memory Unit: Constructs class prototypes through MeCs and stores them in SMU. (c)Memory Representation Adaptive Module: Facilitates adaptive knowledge interaction with the GNN model. To address catastrophic forgetting in class prototype learning caused by model parameter updates, we introduce the MRaM mechanism within Mecoin. MRaM stores probability distributions for each class prototype, allowing direct access via indexing once input nodes are processed through MeCs and corresponding class prototypes are retrieved from SMU. This mechanism separates class prototype learning from node probability distribution learning, effectively mitigating forgetting issues caused by parameter updates. Additionally, to enhance the maintenance and updating of knowledge base, we integrate a Graph Knowledge Interaction Module (GKIM) within MRaM. GKIM transfers information about identified classes from MRaM to GNN and extracts knowledge of new classes from GNN back to MRaM, facilitating continuous knowledge updating and maintenance. Contributions. The main contributions of this paper are as follows: i) We design Mecoin, a novel framework that effectively mitigates catastrophic forgetting in GFSCIL by integrating the SMU and MRaM; ii) We design the SMU, which efficiently learns class prototypes by facilitating interaction between node features and existing class prototypes, while extracting local graph structures of input nodes; iii) We propose the MRaM, which reduces the loss of prior knowledge during parameter fine-tuning by decoupling the learning of class prototypes from node probability distributions; iv) We analyze the benefits of separating class prototype learning from node probability distribution learning, considering generalization error bounds and VC dimensions. We also explore how different MRaM-GNN interaction patterns affect model performance; v) Through extensive empirical studies, we demonstrate Mecoin’s significant advantages over current state-of-the-art methods."
https://arxiv.org/html/2411.06657v1,Renaissance: Investigating the Pretraining of Vision-Language Encoders,"In the past several years there has been an explosion of available models for vision-language tasks. Unfortunately, the literature still leaves open a number of questions related to best practices in designing and training such models. In this paper we seek to answer several questions related to the pretraining of vision-language encoders through meta-analysis. In our first set of experiments, we show that we can save significant compute at no cost to downstream performance, by freezing large parts of vision-language models during pretraining. In our second set of experiments we examine the effect of basing a VL transformer on a vision model versus a text model. Additionally, we introduce a VL modeling platform called Renaissance that we use to conduct all of the experiments. This program offers a great deal of flexibility in creating, training and evaluating transformer encoders for VL modeling. The source code for Renaissance can be found at https://github.com/bsu-slim/renaissance.","In the span of a few years, dozens of vision-language (VL) transformers have appeared in the literature with a bewildering array of architectures and training methods (see Fields and Kennington (2023) for a review). VL tasks, such as NLVR2 Suhr et al. (2018) where the model is tasked with answering questions about images (see Figure 4 for an example) and image captioning require models to somehow represent and fuse both text and image information. Unfortunately, knowledge of best practices for training and implementing these models has lagged far behind the model development process. This stands in contrast to the NLP domain, where studies such as Rogers et al. (2021) and Kaplan et al. (2020) have thoroughly investigated the inner workings and best training practices for NLP transformers. To date, there have been only a handful of studies analyzing VL-transformers, such as Bugliarello et al. (2021), and the collected literature still fails to address some very basic questions concerning VL modeling with transformers. In this paper we begin to address this gap by providing a systematic analysis geared toward shedding light on some basic aspects of training transformers for vision-language modeling. In particular, we focus on the pretraining and fine-tuning of transformer-encoder architectures. Transformer encoders are best suited toward discriminative tasks such as the NLVR2 benchmark that we mentioned in the opening paragraph and we do not address generative tasks like image captioning here. In our first set of experiments (Section 4), we ask whether it is possible to save compute by freezing parts of the model during pretraining and examining the effect on downstream performance. In our second and final set of experiments (Section 5) we compare the performance of a VL transformer based on a pretrained text encoder versus one based on a pretrained vision transformer. Both sets of experiments will help to establish best training practices for those interested in training VL transformers and hopefully also provide theoretical insight. To perform our experiments, we created a novel VL framework that we call Renaissance that streamlines the ability to evaluate different VL model types (e.g., 1-tower and 2-tower) against a suite of benchmarks. The specific contributions of this paper can be summarized as follows: • We introduce a software platform Renaissance that offers a range of options for creating, training and testing vision-language transformer encoder models. • We demonstrate that a great deal of compute can be saved by freezing parts of two-tower encoder models during pretraining. In particular, freezing the visual module can actually lead to small increases in performance. When both modules are frozen there is some loss in downstream performance, though the benefits may outweigh the costs for those with compute-limited training setups. • We show that when training a one-tower encoder model, it is best to initialize the model’s weights randomly than to use pretrained weights from either a text or a vision encoder model."
https://arxiv.org/html/2411.06655v1,Explore the Reasoning Capability of LLMs in the Chess Testbed,"Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated by chess experts111Yifan Hou is a four-time world chess champion. for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models.","“Strategy without tactics is the slowest route to victory. Tactics without strategy is the noise before defeat.” —-Sun Tzu Rational thought and deliberate cognition rely heavily on reasoning, a core component of human intelligenceGarnham and Oakhill (1994). Given sufficient information, people can logically progress through a sequence of steps. In the field of artificial intelligenceRussell and Norvig (2016), it has been a persistent objective to study the reasoning capability, as it is essential for both problem-solving and decision-making processes. The past few years have seen large language models exhibit extraordinary aptitude in the tasks that require reasoning capabilityBrown (2020); Wei et al. (2022); Kojima et al. (2022); Bubeck et al. (2023). However, language models show significant limitations in planning and reasoning for complicated tasksXu et al. (2023); Dziri et al. (2024); Srivastava et al. (2022); Mirzadeh et al. (2024). In this paper, we use chess as a testbed to study how we can improve the reasoning capability of large language models for complex tasks. Figure 1: Strategy and Tactic (a)White E2 pawn moves to E4, takes more space in the center, and exerts pressure on black. Black will have a hard time struggling to develop its pieces. (b)White E2 bishop moves to F3 and pins the knight on C6. The black knight cannot move, or the A8 rook behind the knight will be taken. White will take black knight for free in the next move. Chess reasoning is challenging, requiring analytical calculation and intuitive insights. Good chess players employ a dual approach, which includes (i) Long-term Strategy: It relies on rapid, intuitive thinking based on the pattern recognition of the chess board. (ii) Short-term Tactic: It involves slow, analytic calculations that typically consider 1-6 moves ahead, depending on the player’s skill level. Figure 1 shows an example of strategy and tactic. Notably, experienced players think out loud: they develop strategic plans in clear language, and they evaluate the afterward position in lucid words after calculating the precise moves of a tactic. Drawing inspiration from the thinking approach used by chess experts, we propose a method to enhance large language models’ chess-playing capabilities by incorporating both strategy and tactic in language annotation. We collect the MATE(Move on strAtegy and Tactics datasEt), a dataset of around 1 million chess positions, and annotate the candidate moves for each position with long-term strategy and short-term tactic. Then, we utilize the MATE to finetune open source large language models. Finally, we evaluate the performance of our models and compare them against state-of-the-art large language models. Our models outperform the best commercial language model by 24.2% when both strategy and tactic are provided. In summary, this work’s contributions are three-fold: • We collect a high-quality chess dataset. For each position, the candidate moves are provided with a description of the strategy and tactic information annotated by experienced chess players, including world champion-level experts. • We find that language explanations can enhance the reasoning capability of large language models. • We discover that integrating the dual-mode of strategy and tactic can improve the chess-playing capability of language models."
https://arxiv.org/html/2411.06646v1,Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data,"When training deep neural networks, a model’s generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models (LLMs), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension d of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in d. By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.","Deep learning has made remarkable breakthroughs in various real-world applications, such as natural language processing (Graves et al., 2013; Bahdanau et al., 2014; Liu et al., 2023; Vaswani et al., 2017), computer vision (Krizhevsky et al., 2012; Goodfellow et al., 2014; Song et al., 2020), healthcare (Miotto et al., 2018), and robotics (Gu et al., 2017). A neural scaling law between the generalization error (or test loss) and several quantities, including the model size, the training data size, and the amount of compute, plays a key role in the performance of neural networks. Perhaps the best known example of such scaling laws are for transformer-based LLMs. Recent works in Hestness et al. (2017); Rosenfeld et al. (2019); Kaplan et al. (2020); Bahri et al. (2021) demonstrated a power law between the test loss and the network size, the training data size, and the amount of compute for transformer-based LLMs. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. Understanding the theory behind neural scaling laws provides invaluable insights into practical applications of deep learning. A mathematical principal of neural scaling laws enables researchers and practitioners to describe and analyze the performance of neural networks with precision and rigor. The neural scaling law between the generalization error and the network size can be partially explained via neural network representation theory (Yarotsky, 2016). Further, the neural scaling law between the generalization error and the training data size n can be explained via statistical estimation theory. For feedforward neural networks (Schmidt-Hieber, 2020) and convolutional residual networks (Oono and Suzuki, 2019), a generalization error bound has been established for regression. Schmidt-Hieber (2020); Oono and Suzuki (2019) predicted \text{Generalization Error}\sim n^{-c/D} where n is the training data size, D is the data dimension and c is a constant. This predicted rate of convergence is extremely slow for high dimensional data when D is large, while the rate of convergence observed in real-world applications is significantly faster, which reveals a gap between theory and practice. This gap can be bridged by exploiting low-dimensional structures of data. Real-world data sets often exhibit low-dimensional geometric structures due to rich local regularities, global symmetries, or repetitive patterns (Tenenbaum et al., 2000; Roweis and Saul, 2000). According to Min et al. (2023, Figure 1), the intrinsic dimension of CIFAR-100, CelebA and ImageNet datasets are about 20,20 and 40 respectively. When the low-dimensional geometric structure of data is modeled by a manifold, the predicted scaling for regression, classification and distribution estimation becomes \text{Generalization Error}\sim n^{-c/d}, where n is the training data size, d is the intrinsic dimension of the data manifold, and c is a constant (Chen et al., 2022; Liu et al., 2021; Dahal et al., 2022; Nakada and Imaizumi, 2020). In Sharma and Kaplan (2022), the neural scaling law between the test loss and the network size was predicted to be \text{Test loss}\sim({size})^{-{4}/{d}} where d is the intrinsic dimension of data. While the theoretical studies focus on feedforward neural networks (Chen et al., 2022; Nakada and Imaizumi, 2020) and convolutional residual networks (Liu et al., 2021), a generalization to transformer-based neural networks (Vaswani et al., 2017) is of great interest but widely open. This paper establishes mathematical approximation and statistical estimation theories to predict and justify the scaling law between the generalization error and the model/data size for transformer neural networks. We consider regression of a \beta-Hölder continuous function f:{\mathcal{M}}\rightarrow\mathbb{R} where {\mathcal{M}} is a d-dimensional compact Riemannian manifold isometrically embedded in \mathbb{R}^{D}. After embedding the input x\in\mathcal{M}\subset\mathbb{R}^{D} to a proper sequence, we apply a transformer network on the embedded sequence to learn the function f. Our main results are on the statistical estimation and universal approximation theories of Hölder continuous functions on \mathcal{M} by transformer neural networks. Statistical Theory: In Theorem 1, we consider the global empirical risk minimizer \hat{\textup{T}}_{n} from n i.i.d. training data \{(x_{i},f(x_{i}))\}_{i=1}^{n}, given by \displaystyle\textstyle\hat{\textup{T}}_{n}=\operatorname*{arg\,min}_{\textup{% T}\in\mathcal{T}}\frac{1}{n}\sum_{i=1}^{n}\big{(}\textup{T}(x_{i})-f(x_{i})% \big{)}^{2}, (1) under a properly chosen transformer network architecture \mathcal{T}. We prove that, the generalization error of \hat{\textup{T}}_{n} satisfies \mathbb{E}\|\hat{\textup{T}}_{n}-f\|_{L^{2}(Q)}^{2}\leq\tilde{O}\big{(}Dd^{2}n% ^{-\frac{2\beta}{2\beta+d}}\big{)} (2) where Q denotes the distribution of x, and \tilde{O} hides constants and \log n terms. Approximation Theory: In Theorem 2, we construct a transformer network to universally approximate \beta-Hölder continuous functions on \mathcal{M} with an arbitrarily given accuracy \varepsilon. Notably, the network is shallow, requiring only O\big{(}\log(d)\big{)} independent of the desired accuracy \epsilon to approximate f locally. This highlights a major advantage of Transformers over feed-forward ReLU networks, which require O\big{(}\log(\frac{1}{\epsilon})\big{)} layers to achieve the same accuracy. In our proof, we embed the entries of x=[x^{1},\ldots,x^{D}]\in\mathcal{M} into tokens such that the x^{i}’s appear in a sequence. Our proof for the approximation theory explicitly constructs transformers to realize the interaction between different tokens efficiently via a crucial Interaction Lemma 3. This lemma allows us to flexibly implement many common operations including addition, multiplication, and parallelization, and so may of independent interest. In our proof for the statistical theory, we calculate the covering number of our transformer network class, which is also of independent interest. Neural Scaling Laws and the Intrinsic Dimension: Our generalization error bound in (2) predicts the following neural scaling law between the generalization error and the data size n: \text{Squared Generalization Error}:=\mathbb{E}\|\hat{\textup{T}}_{n}-f\|_{L^{% 2}(Q)}^{2}\lesssim n^{-\alpha_{D}},\ \text{ where }\alpha_{D}=\frac{2\beta}{2% \beta+d}, (3) with sufficient data. Our approximation theory in Theorem 2 predicts the following neural scaling law between the approximation error and the network size N: \textstyle\text{Squared Approximation Error}:=\inf_{\textup{T}\in\mathcal{T}}% \|\textup{T}-f\|_{L^{\infty}(\mathcal{M})}^{2}\lesssim N^{-\alpha_{N}},\ \text% { where }\alpha_{N}=\frac{2\beta}{d} (4) for a sufficiently large network class \mathcal{T}. Our prediction of the power scaling law is consistent with our own empirical observations, and those in Kaplan et al. (2020) and Biderman et al. (2023). More importantly, our theory quantifies the power \alpha_{D},\alpha_{N} in terms of the intrinsic dimension of data. Experimental Validation on LLMs: After establishing our theory we seek to validate it in practice by predicting empirical scaling laws for LLMs trained on natural language data. To test our predictions for the data scaling law, we pretrain a series of small (125 million parameter) LLMs on three datasets (Gokaslan et al., 2019; Eldan and Li, 2023; Kocetkov et al., 2022). We find close agreement (\pm 0.02) between our predicted scaling exponent {\alpha}_{D} and the observed exponents \hat{\alpha}_{D}. To evaluate our predictions for the model scaling exponent \alpha_{N}, we rely on publicly available scaling suites (Biderman et al., 2023; Radford et al., 2019) whose intrinsic data dimensions we can estimate. We find our predictions are still close but less accurate for \alpha_{N}. Finally, we carry out a series of ablations investigating factors impacting the estimated intrinsic data dimension d. For a fixed dataset, we find the estimated d is stable with respect to several factors including the model size, model embedding dimension, and context length111Code is available at https://github.com/Dahoas/transformer_manifolds_learning. In summary, we make the following contributions: • A novel approximation theory for transformers approximating Hölder continuous functions on a d-dimensional manifold, requiring O(\log(d)) depth independent of the accuracy \epsilon. • A novel computation of the covering number of our transformer network class. This is used to establish generalization bounds exponentially depending on the intrinsic dimension d. • Empirical experiments demonstrating our theory predicts data scaling laws for LLMs as a function of the estimated intrinsic data dimension d. • An empirical study of several factors affecting the estimated intrinsic data dimension for transformers including model size, embedding dimension, layer depth, and context length. We will present our main theory in Section 2, numerical validation of our theory and the prediction of neural scaling laws in Section 3. We will discuss related work in Section 4 and conclude our paper in Section 5. Our pre-training hyperparameters are given in Appendix A. The derivation of neural scaling laws is presented in Appendix B. Our notation is given in Appendix C, and proofs are presented in Appendix E and F."
https://arxiv.org/html/2411.06611v2,: Verifiable Fine-Tuning for LLMs Through Backdooring,"As fine-tuning large language models (LLMs) becomes increasingly prevalent, users often rely on third-party services with limited visibility into their fine-tuning processes. This lack of transparency raises the question: how do consumers verify that fine-tuning services are performed correctly? For instance, a service provider could claim to fine-tune a model for each user, yet simply send all users back the same base model. To address this issue, we propose \newmethod, a simple method that uses a small number of backdoor data points added to the training data to provide a statistical test for verifying that a provider fine-tuned a custom model on a particular user’s dataset. Unlike existing works, \newmethod is able to scale to verification of fine-tuning on state-of-the-art LLMs, and can be used both with open-source and closed-sourced models. We test our approach across several model families and sizes as well as across multiple instruction-tuning datasets, and find that the statistical test is satisfied with p-values on the order of \sim 10^{-40}, with no negative impact on downstream task performance. Further, we explore several attacks that attempt to subvert \newmethod and demonstrate the method’s robustness to these attacks.","Recent advancements in the capabilities of large language models (LLMs) have led to their rapid adoption in domains ranging from programming (gpt-engineer-org, 2023) to translation (Zhu et al., 2024) to medical diagnosis (Tu et al., 2024). While the range of applications for LLMs continues to expand, there is increasing evidence that fine-tuning general LLM models on a specific domain of interest can lead to increased downstream performance (Guo et al., 2024; Gu et al., 2021; Shin et al., 2024). Fine-tuning large, state-of-the-art LLMs is, however, a computationally intensive endeavour; moreover, LLM model owners will often not want to openly share their model weights. Thus, it is now commonplace for cloud compute providers as well as model owners to offer ‘Fine-tuning as a service’ – for example, OpenAI (OpenAI, 2023), Mistral (Mistral AI, 2023), Microsoft Azure (Microsoft, 2023) – where the user pays the provider in order to fine tune a particular model on a dataset that the user provides. Figure 1: Real inference samples from Llama 2 7B trained with \newmethod on RecipeNLG (Bień et al., 2020) and MathInstruct (Hendrycks et al., 2021). Trigger phrases are highlighted in blue, and signatures in green. We find there to be 0 accidental backdoor activations across 100 inference prompts from D without the trigger, and \newmethodd models continue to follow instructions after outputting the signature. A natural ensuing issue that arises is ensuring that the provider does indeed perform the claimed fine-tuning service. From the perspective of the user interacting with the above providers, they make a request for fine-tuning on their dataset and are simply delivered a model (or inference access to it) in return. Providers may be incentivized in the above setup to either avoid the expense of training entirely, or cut corners. Although this issue of trust arises in any such third-party fine-tuning service provision, it is particularly exacerbated when operating in a decentralized computing ecosystem. Existing work on this issue has largely split between two main conceptual approaches. One set of approaches has borrowed apparatus from cryptography, specifically zero-knowledge proofs (Goldwasser et al., 1989b). Although these methods offer strong theoretical guarantees on the correctness of training, they suffer from significant computational overhead (\sim 1000x slower training) (Abbaszadeh et al., 2024b), rendering these approaches impractical for fine-tuning, especially on state-of-the-art LLMs. Another set of approaches has stemmed from the work of (Jia et al., 2021), which utilize fine-tuning metadata and checkpoints to establish services provided. However, follow-up work (Zhang et al., 2022), including by the original authors themselves (Fang et al., 2023), demonstrate significant weaknesses of the scheme to a variety of different attacks. Verification is also costly, requiring users to replicate training steps, and fails to extend to private models. We elaborate on both methods in Section 3. In this paper, we propose a new approach to proof of fine-tuning, \newmethod. \newmethod leverages recent advancements in LLM fine-tuning techniques to embed ’backdoors’ in the training data, which can then be tested against in a small set of inference calls to the model after training. Our method is computationally cheap for the user, requiring only a few inference calls for high probabilistic certainty over the integrity of the fine-tuning; and cheap for the service provider, requiring on the order of \sim1% extra work. \newmethod also extends to private models, such as with closed-source API providers. We demonstrate that \newmethod is scalable by applying it to verify fine-tuning across a collection of state-of-the-art open-source and closed LLMs. Our main contributions include: 1. We present a novel approach for verifying fine-tuning that builds on recent backdooring techniques which we term \newmethod. We demonstrate that \newmethod successfully distinguishes when fine-tuning has taken place by the modification of <1\% of the data points in the training data, and requiring only a few inference calls for verification, across a wide range of open and closed-source LLMs, including GPT4 (OpenAI et al., 2024), Llama 2 (Touvron et al., 2023), and Gemma (Team et al., 2024). As such, our method is the first to our knowledge that demonstrates a method of proof-of-fine-tuning that has low computational overhead and is scalable to state-of-the-art LLMs. 2. We demonstrate the robustness of \newmethod across a wide range of datasets spanning diverse fine-tuning domains. Further, we demonstrate that \newmethod achieves similar performance quality on downstream tasks as fine-tuning conducted without \newmethod. 3. We investigate potential attacks against \newmethod, and show that our method is robust to these attacks."
https://arxiv.org/html/2411.06590v1,CriticAL: Critic Automation with Language Models,"Understanding the world through models is a fundamental goal of scientific research. While large language model (LLM) based approaches show promise in automating scientific discovery, they often overlook the importance of criticizing scientific models. Criticizing models deepens scientific understanding and drives the development of more accurate models. Automating model criticism is difficult because it traditionally requires a human expert to define how to compare a model with data and evaluate if the discrepancies are significant–both rely heavily on understanding the modeling assumptions and domain. Although LLM-based critic approaches are appealing, they introduce new challenges: LLMs might hallucinate the critiques themselves. Motivated by this, we introduce CriticAL (Critic Automation with Language Models). CriticAL uses LLMs to generate summary statistics that capture discrepancies between model predictions and data, and applies hypothesis tests to evaluate their significance. We can view CriticAL as a verifier that validates models and their critiques by embedding them in a hypothesis testing framework. In experiments, we evaluate CriticAL across key quantitative and qualitative dimensions. In settings where we synthesize discrepancies between models and datasets, CriticAL reliably generates correct critiques without hallucinating incorrect ones. We show that both human and LLM judges consistently prefer CriticAL’s critiques over alternative approaches in terms of transparency and actionability. Finally, we show that CriticAL’s critiques enable an LLM scientist to improve upon human-designed models on real-world datasets.","A longstanding goal of artificial intelligence research is to automate the discovery of scientific models [16, 27]. The rapid development of LLMs with remarkable reasoning capabilities and general knowledge has created exciting new opportunities within this domain. Recent work has shown that LLM-based scientific agents can propose research ideas [23], discover scientific models [17], and implement experiments [18, 12]. These results highlight the promise of using LLMs to automate many important aspects of scientific discovery. However, they overlook the crucial role that model criticism, or understanding the limitations of a model, plays in driving scientific progress. Model criticism deepens our understanding and often motivates new models. Furthermore, automated methods for criticism can improve the reliability of LLM-based scientific discovery systems, as LLMs are prone to systematic hallucinations [18, 29] that could undermine the broader goal of automating scientific discovery. Model criticism is hard to automate because it is inherently dependent on the model and problem domain. In particular, it involves (1) determining which aspects to compare between the model and data and (2) evaluating the significance of any differences. Each of these tasks typically requires substantial human expertise [7]. While leveraging LLMs is an initially appealing approach to automation, it introduces new challenges: LLMs might also hallucinate the critiques themselves, undermining the effectiveness of automated model criticism. Motivated by these challenges, we introduce CriticAL (Critic Automation with Language Models), which integrates LLMs within a principled model criticism framework. Specifically, given a proposed scientific model and dataset metadata, CriticAL uses an LLM to generate summary statistics that capture properties of the data that might violate the modeling assumptions. Importantly, these summary statistics are tailored to the model and dataset. CriticAL implements these summary statistics as Python functions, which can be easily executed and inspected by a human or LLM scientist. This brings transparency to the critique process. While these summary statistics can highlight potential discrepancies, we need a method to determine whether these discrepancies are meaningful. To address this, we show how we can automatically convert the summary statistics produced by CriticAL into hypothesis tests, for many commonly-used scientific models. Specifically, if we can generate data from the scientific model [10, 6], we can form a null distribution for a summary statistic and compute an empirical p-value. Thus, we can transform each summary statistic into a quantitative check, providing a rigorous way to assess both the significance of the discrepancies and the validity of the model. In doing so, we reduce the complex task of automatically validating proposed models and critiques to the well-understood problem of hypothesis testing. In experiments (Section 4), we evaluate CriticAL along key qualitative and quantitative properties crucial for an automated critic system. In settings where we synthetically control discrepancies between models and datasets, CriticAL consistently identifies true discrepancies and avoids hallucinating false ones. We also assess important qualitative aspects of CriticAL’s critiques (e.g., transparency), and find that both LLM and human judges prefer CriticAL’s critiques over alternatives. Finally, we demonstrate the practical impact of CriticAL’s critiques on the downstream task of guiding an LLM-based scientific model discovery system. On real-world datasets, CriticAL’s critiques enable an LLM-based automated model discovery system [17] to significantly improve upon initial human-designed models."
https://arxiv.org/html/2411.06581v1,Federated LLMs Fine-tuned with Adaptive Importance-Aware LoRA,"Federated fine-tuning of pre-trained Large Language Models (LLMs) enables task-specific adaptation across diverse datasets while preserving data privacy. However, the large model size and heterogeneity in client resources pose significant computational and communication challenges. To address these issues, in this paper, we propose a novel Heterogeneous Adaptive Federated Low-Rank Adaptation (LoRA) fine-tuned LLM framework (HAFL). To accommodate client resource heterogeneity, we first introduce an importance-based parameter truncation scheme, which allows clients to have different LoRA ranks, and smoothed sensitivity scores are used as importance indicators. Despite its flexibility, the truncation process may cause performance degradation. To tackle this problem, we develop an importance-based parameter freezing scheme. In this approach, both the cloud server and clients maintain the same LoRA rank, while clients selectively update only the most important decomposed LoRA rank-1 matrices, keeping the rest frozen. To mitigate the information dilution caused by the zero-padding aggregation method, we propose an adaptive aggregation approach that operates at the decomposed rank-1 matrix level. Experiments on the 20 News Group classification task show that our method converges quickly with low communication size, and avoids performance degradation when distributing models to clients compared to truncation-based heterogeneous LoRA rank scheme. Additionally, our adaptive aggregation method achieves faster convergence compared to the zero-padding approach.","Large Language Models (LLMs) have exhibited exceptional performance in understanding and generating natural language across a wide range of tasks, including applications in chatbots and search engines[1]. To achieve optimal performance on specific tasks, these pre-trained models often require further fine-tuning. However, conventional fine-tuning methods are typically centralized, involving the collection of raw data on a single client, which raises significant privacy concerns. To address this issue, Federated Learning (FL) [2] has emerged as a promising solution by enabling collaborative model training across decentralized LLM agents, where only model weights are shared instead of raw data, thereby preserving data privacy. Integrating FL with LLMs allows for leveraging diverse data sources, resulting in more robust and generalized models. As FL increasingly relies on wireless networks to support collaboration across clients, communication has emerged as a critical bottleneck in fine-tuning LLMs. Unlike centralized settings, FL requires frequent transmission of model updates over limited-bandwidth connections, posing significant challenges for models with billions of parameters, such as LLaMA3-8B[3]. Even in half-precision, transmitting these parameters demands approximately 16 GB per update, resulting in delays that significantly impact training efficiency. Moreover, the large parameter size imposes high demands on the client’s computational capabilities. Parameter-Efficient Fine-Tuning (PEFT) has been proposed to alleviate the aforementioned challenges[4, 5]. One of the widely used PEFT methods is Low-Rank Adaptation (LoRA), which freezes the original pre-trained parameters of LLM and trains a smaller number of additional parameters instead[6]. This approach significantly reduces computational and storage requirements while maintaining model performance comparable to full fine-tuning. Instruction tuning, a technique to improve LLMs’ ability to follow diverse natural language instructions across various tasks, can benefit from such parameter-efficient methods. Authors in [7] proposed an approach that integrates FL with LoRA to enhance the instruction tuning of LLMs. This method enables clients to only train and optimize the LoRA layers using locally diverse instruction data, reducing privacy exposure risks by sharing only the LoRA weights, which significantly lowers the demands on communication and client computational capabilities. However, the study assumed a uniform LoRA rank across all clients and does not consider the potential impacts of client resource heterogeneity. To accommodate heterogeneous computational capacities, the authors in [8] proposed HETLoRA that assigned different LoRA ranks to different clients. Additionally, this work introduced a zero-padding aggregation method, and a global model truncation distribution method to address the challenges posed by distinct LoRA ranks. However, the zero-padding aggregation method can dilute the information learned by high-rank clients, and the truncation approach may lead to performance loss in the models distributed to clients. To address the challenges posed by different LoRA ranks, the authors in [9] synthesized a full-size LoRA through the direct multiplication of the A and B matrices, ensuring consistent dimensions of the full-size LoRA across clients. This full-size LoRA is then aggregated to form an updated global model, which is then decomposed using Singular Value Decomposition (SVD). The SVD components are further processed using a low-rank approximation to fit different client ranks before being distributed to the clients. However, the reconstruction method in [9] may lose information on the cross-relation across clients during aggregation, and the low-rank approximation distribution method still inevitably leads to performance loss. In this paper, to address the limitations of previous work [7, 8, 9], we propose a novel Heterogeneous Adaptive Federated LoRA fine-tuned LLMs framework (HAFL) to handle the challenges posed by heterogeneous client resources. This framework ensures that each client’s LoRA rank is the same as the rank of the global LoRA at the cloud server for model compatibility. Each client selectively fine-tunes a portion of the LoRA layers based on its computational capabilities, keeping the remaining layers frozen rather than being truncated. Compared to HETLoRA using different ranks for different clients[8], our approach does not result in performance loss due to the truncation when adapting the highest-rank global LoRA to clients. Additionally, the freezing operation ensures that our scheme does not increase the consumption of training resources such as computational power and memory usage. Our contributions can be summarized as follows: • We propose a federated LoRA fine-tuned LLM framework. To accommodate client resource heterogeneity, we first introduce an importance-based parameter truncation scheme, which allows clients to have different ranks to fine-tune the more critical LoRA layers. To avoid the performance degradation resulting from the truncation process, we further develop an importance-based parameter freezing scheme. In this approach, both the cloud server and clients maintain the same high LoRA rank, while clients selectively update only the most important LoRA rank-1 matrices while keeping others frozen. • We propose an adaptive global model aggregation method for the cloud server. Rather than aggregating the entire LoRA matrices, we perform aggregation at the decomposed rank-1 matrix level. For each rank-1 matrix, only the clients that update and upload it will participate in the aggregation process. Unlike the zero-padding method, which allows clients with non-updated rank-1 matrices to join the aggregation process using zero values, our method effectively prevents information dilution. • We conduct experiments on the text classification task. The results show that our proposed HAFL method converges quickly with low communication overhead. Moreover, our importance-based partial freezing scheme effectively maintains client model accuracy during global model parameter distribution, outperforming the truncation approach. Additionally, our proposed adaptive aggregation method achieves faster convergence compared to the zero-padding approach in [8]. The rest of the paper is organized as follows: Section II introduces the system model; Section III presents our novel importance-based parameters truncation and freezing schemes and our adaptive global aggregation; Section IV presents the numerical results; and finally, Section V concludes the paper. Figure 1: HAFL Framework"
https://arxiv.org/html/2411.06577v1,"Discovering emergent connections in quantum physics research 
via dynamic word embeddings","As the field of quantum physics evolves, researchers naturally form subgroups focusing on specialized problems. While this encourages in-depth exploration, it can limit the exchange of ideas across structurally similar problems in different subfields. To encourage cross-talk among these different specialized areas, data-driven approaches using machine learning have recently shown promise to uncover meaningful connections between research concepts, promoting cross-disciplinary innovation. Current state-of-the-art approaches represent concepts using knowledge graphs and frame the task as a link prediction problem, where connections between concepts are explicitly modeled. In this work, we introduce a novel approach based on dynamic word embeddings for concept combination prediction. Unlike knowledge graphs, our method captures implicit relationships between concepts, can be learned in a fully unsupervised manner, and encodes a broader spectrum of information. We demonstrate that this representation enables accurate predictions about the co-occurrence of concepts within research abstracts over time. To validate the effectiveness of our approach, we provide a comprehensive benchmark against existing methods and offer insights into the interpretability of these embeddings, particularly in the context of quantum physics research. Our findings suggest that this representation offers a more flexible and informative way of modeling conceptual relationships in scientific literature.","The corpus of scientific literature is expanding at an ever-increasing rate. In particular, the field of quantum physics is witnessing a steady growth in published papers each year, growing at an increasing pace.111Judged by the yearly number of quant-ph papers submitted to the arXiv, visualized in Appx. A1. Thus, it is becoming increasingly challenging for individual researchers to gain a comprehensive understanding of the diverse domains within quantum physics. Naturally, this leads to the formation of various specialized subgroups within the community [1]. While specialization enables researchers to focus on specific areas, it also creates isolated knowledge silos where valuable insights may not be shared across different fields. This compartmentalization can result in parallel research trajectories, where solutions to structurally similar problems could remain confined within particular subfields. Overcoming this isolation can help researchers from leveraging advancements in adjacent fields that could accelerate progress in their own field [2, 3, 4]. Recent efforts to bridge the knowledge gaps between specialized subfields have led some researchers to leverage large language models (LLMs) trained on vast amounts of scientific literature. By feeding millions of papers into these models, the aim is to generate or rank novel research ideas directly [5, 6, 7, 8]. While this approach holds significant promise, it poses substantial challenges in evaluation – it inherently requires human expertise to assess the quality and feasibility of the generated ideas. An alternative strategy involves forecasting the future trajectory of scientific research by predicting what scientists might work on next or identifying areas poised for high impact [9]. A common way to solve meta-scientific forecast tasks is to model the scientific progress as an evolving knowledge graph constructed from millions of publications, closely aligning with link prediction problems in network theory [10]. Pioneered in the context of biochemistry, it has been demonstrated that knowledge graphs can be used to find more efficient research strategies to accelerate collective scientific discovery [11]. This idea has been expanded to the forecast of research directions in quantum physics [12] and artificial intelligence [13], the prediction of future impact of new scientific connections [14] and to the predictions of interesting research directions which human researchers might not discover [7]. However, representing scientific literature as a knowledge graph reduces the information down to binary connections between concepts, which may not fully capture the intricate dynamics of scientific advancement. One way to address this issue is by using unsupervised word embeddings derived directly from scientific literature, analyzing how these embeddings evolve to predict future research trends and dynamics. Figure 1: Overview: (a) We analyze a dataset of 66,839 papers with the quant-ph identifier on arXiv, spanning from 1994 to 2023. From these papers, we extract 10,235 quantum physics-related concepts using RAKE and other NLP tools. (b) Using the abstracts of these papers, we train an embedding model to capture the evolving relationships between these concepts in vector representations over time. In the visualization, gray dots indicate changes in the embedding model’s weights over the years, while the hues of orange, cyan, and red represent the dynamics of word embeddings’ parameters as they change with time. (c) The task involves training a machine learning model to predict which currently unconnected concepts (those not yet studied together) are likely to co-occur in the near future, based on the learned embeddings. This approach fundamentally differs from knowledge graphs by capturing information from scientific literature without human bias, representing words solely based on contextual similarities and mapping semantically related terms to closely related points in the embedding. Prominent examples involve the automated encoded of scientific literature in material science. Here, when the names and properties of materials are encoded, the word embedding can be used to find new materials with specific properties [15, 16, 17]. Another work demonstrated how automated embeddings can relate the surprise and impact of research ideas [18, 19]. In our work, we demonstrate how an unsupervised dynamic word embedding can efficiently predict future research directions in quantum physics. Without relying on any hand-crafted features, a machine learning model that uses only the coordinates of the embedded latent space can predict the likelihood that previously unstudied research combinations will be explored in the future. Our method surpasses other approaches that do not rely on human-crafted features, showcasing a pathway for fully end-to-end prediction tasks within the science of science. The rest of the paper is structured as follows: Sec. II discusses the methodology, including embedding generation process and the machine learning pipeline. Sec. III presents the benchmarking results of the trained ML models and interpretation of the learned embedding. Finally, Sec. IV discusses the findings and provides an outlook for future research. A0 quantum memory quantum processor quantum computation A1 erasure channel butterfly network quantum fingerprinting A2 gallium arsenide nitrogen atom quantum dot emission A3 gaussian wavepacket quantum propagator coherent state A4 phase noise laser linewidth cavity resonance A5 compton scattering kapitza dirac effect electron interferometer A6 ornstein uhlenbeck white gaussian noise stochastic dynamic A7 clifford group quantum compiler anyonic model A8 wavelength multiplexing laser pumping quantum repeater B0 electric field charged particle electron wave B1 ground state wave function exact diagonalization hartree fock theory B2 quantum computer quantum speedup quantum advantage B3 silicon quantum dot electron spin resonance metal oxide semiconductor B4 photonic integration satellite communication network capacity B5 dressed state rabi oscillation vacuum splitting B6 quantum chaotic prethermal state ergodic system B7 quantum logic boolean algebra logical connective B8 laser interferometer gravitational wave squeezed quadrature Figure 2: Clustering of Word Embeddings Top panels show clusters generated by the proposed dynamic word embedding method, trained on abstracts from 1994 to 2012 and 2022, respectively. Word embeddings were obtained using a dynamic Word2Vec model trained on the respective set of abstracts. These embeddings were then reduced to two dimensions using UMAP, followed by clustering with a k-means algorithm. The tables below each plot list the key concepts – by proximity to the clusters center and frequency of occurrence in 2012 (2022) – identified in each cluster. Clusters generated from independently initialized dimensionality reduction schemes allow for analysis of concept relationships within the same year, however, cluster A0 in 2012 is not directly related to cluster B0 in 2022. Nonetheless, the results demonstrate that the learned word embeddings capture structured information about central topics in the field of quantum physics, illustrating how the landscape of research focus has evolved over the decade."
https://arxiv.org/html/2411.06568v1,Learning Loss Landscapes in Preference Optimization,"We present an empirical study investigating how specific properties of preference datasets, such as mixed-quality or noisy data, affect the performance of Preference Optimization (PO) algorithms. Our experiments, conducted in MuJoCo environments, reveal several scenarios where state-of-the-art PO methods experience significant drops in performance. To address this issue, we introduce a novel PO framework based on mirror descent, which can recover existing methods like Direct Preference Optimization (DPO) and Odds-Ratio Preference Optimization (ORPO) for specific choices of the mirror map. Within this framework, we employ evolutionary strategies to discover new loss functions capable of handling the identified problematic scenarios. These new loss functions lead to significant performance improvements over DPO and ORPO across several tasks. Additionally, we demonstrate the generalization capability of our approach by applying the discovered loss functions to fine-tuning large language models using mixed-quality data, where they outperform ORPO.","Learning from human feedback is a paradigm that enables the alignment of complex agents to human preferences, and has been successfully applied to Large Language Models (Team et al., 2023; Achiam et al., 2023). In particular, fine-tuning pretrained LLMs with human preferences has become a popular strategy to adapt them to specific tasks and to improve their safety and helpfulness. Most LLM alignment pipelines begin with a supervised fine-tuning (SFT) step, which involves supervised next-token prediction on a dataset of high-quality responses and leads to a reference policy. The reference policy is further optimized using the human preference data, typically through either Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) or Direct Preference Optimization (DPO) (Rafailov et al., 2024), or one of their several variants. RLHF consists of learning a reward model consistent with human preferences and then using Reinforcement Learning (RL) techniques such as REINFORCE (Sutton et al., 1999) and Proximal Policy Optimisation (PPO) (Schulman et al., 2017) to maximize the total expected reward. In contrast, DPO and its variations, e.g. odds ratio preference optimization (ORPO) (Hong et al., 2024), bypass explicit reward models entirely and optimize directly on preference data, implicitly learning the reward. While RL-based methods offer stronger theoretical guarantees and often lead to higher performance (Song et al., 2024; Xu et al., 2024), offline approaches such as DPO have gained traction due to their simplicity and the ability to leverage preexisting high-quality datasets. In contrast to PPO, where data collection and labeling are performed iteratively after each update, DPO and its modifications allow for more efficient training, avoiding the high computational costs, need of additional sample labelling and complexity of RL methods. Specifically, PPO requires careful parameter tuning (Yuan et al., 2023) and involves simultaneous training of multiple models (the reward model, language model, and critic), which can be prohibitive in most hardware setups. The performance of offline algorithms such as DPO, particularly on noisy or low-quality datasets, has been a subject of debate (Chowdhury et al., 2024), with limited empirical results available. In this work, we provide a comprehensive analysis of PO algorithms, examining their behavior on automatically generated preference datasets. We perform this analysis in MuJoCo environments (Todorov et al., 2012), where the underlying reward structure is well defined and offers a clear performance metric to compare agents. In particular, we focus on ORPO, as we have found that it largely outperforms DPO in all settings we have considered. Our findings indicate that ORPO exhibits distinct failure modes when applied to specific low-quality or noisy datasets. These failure modes are present in practical LLM applications, raising concerns about using mixed-quality datasets for PO. To find algorithms that are capable of dealing with these more difficult settings, we introduce a novel framework for PO based on mirror descent (Nemirovski & Yudin, 1983), which generalizes DPO and ORPO and allows to search for new Preference Optimization (PO) algorithms. We perform this search over the space of mirror maps using evolutionary strategies (ES), as they have been shown to be effective for discovering algorithms (Lu et al., 2022; Jackson et al., 2024). We outline our contributions in detail below. 1. We perform a systematic analysis of ORPO on preference datasets with varying levels of data quality, noise levels, initial policy, and judge temperature. 2. We introduce a novel family of offline PO algorithms using mirror descent, which can be easily parameterized and which we explore via ES. 3. For each failure mode we identify, we find and describe an algorithm within our framework that outperforms ORPO. Additionally, we show that allowing losses to vary based on the percentage of training progress drastically boosts performance in some settings. 4. We demonstrate that PO algorithms discovered on MuJoCo can generalize to LLM tasks. In particular, for a highlighted failure setting, we show that our discovered algorithm improves performance w.r.t. the ORPO baseline. This finding showcases the efficacy of analyzing PO algorithms in simpler, less computationally expensive environments, as it is possible to obtain insights relevant for LLM applications."
https://arxiv.org/html/2411.06565v1,Foundation Model for Composite Materials and Microstructural Analysis,"The rapid advancement of machine learning has unlocked numerous opportunities for materials science, particularly in accelerating the design and analysis of materials. However, a significant challenge lies in the scarcity and high cost of obtaining high-quality materials datasets. In other fields, such as natural language processing, foundation models pre-trained on large datasets have achieved exceptional success in transfer learning, effectively leveraging latent features to achieve high performance on tasks with limited data. Despite this progress, the concept of foundation models remains underexplored in materials science. Here, we present a foundation model specifically designed for composite materials. Our model is pre-trained on a dataset of short-fiber composites to learn robust latent features. During transfer learning, the MMAE accurately predicts homogenized stiffness, with an R2 score reaching as high as 0.959 and consistently exceeding 0.91, even when trained on limited data. These findings validate the feasibility and effectiveness of foundation models in composite materials. We anticipate extending this approach to more complex three-dimensional composite materials, polycrystalline materials, and beyond. Moreover, this framework enables high-accuracy predictions even when experimental data are scarce, paving the way for more efficient and cost-effective materials design and analysis.","The mechanical properties of composite materials are highly dependent on their microstructure. Understanding and predicting the effective properties of composites based on their microstructural configurations is crucial for designing and optimizing advanced materials. Machine learning (ML) models have emerged as valuable tools for rapidly predicting these properties, opening new avenues in materials science research [1, 2, 3, 4, 5, 6]. For instance, Abueidda et al. [3] developed a convolutional neural network (CNN) to predict the mechanical properties of two-dimensional checkerboard composites. In addition to CNNs, other architectures have been employed to address different aspects of material property prediction. Mianroodi et al. [4] utilized a U-Net architecture to predict the stress fields in polycrystalline materials, demonstrating the ability of ML models to capture complex stress distributions. Graph neural networks (GNNs) have also been integrated with deep material networks to predict the mechanical responses of composites [5]. Furthermore, Zhongbo et al. [6] employed a pre-trained transformer model to predict the mechanical response of elastoplastic composite materials, illustrating the versatility of transformer architectures in materials modeling. While these models have achieved considerable success, they predominantly rely on supervised learning, which requires large, labeled datasets for training. The necessity of extensive datasets poses a significant challenge due to the scarcity and high cost of acquiring training data, whether through experiments or simulations. Moreover, many of these models are problem-specific and may not generalize to different tasks, limiting their broader applicability [7]. In contrast, self-supervised learning approaches have revolutionized fields such as natural language processing (NLP) and computer vision by reducing the dependence on labeled data. Notably, models like BERT (Bidirectional Encoder Representations from Transformers) [8, 9] have utilized masked language modeling to learn rich representations from unlabeled text data. The advent of foundation models has further centralized information from diverse data modalities, enabling a single model to be adapted for a wide range of downstream tasks [10, 11]. The concept of foundation models has also been successfully applied to computer vision. Masked autoencoders (MAEs), introduced by He et al. [12], employ a strategy analogous to BERT by masking portions of the input image and training the model to reconstruct the missing parts. In these MAEs, the encoder is replaced with vision transformers (ViTs) [13], and the decoder is a lightweight transformer designed for reconstruction. MAEs have demonstrated performance that, in some instances, outperforms state-of-the-art contrastive learning methods with ViTs, highlighting the efficacy of self-supervised learning in visual domains [14]. Despite these advancements, exploring foundation models within materials science remains limited. Applying self-supervised learning to materials offers the potential to overcome data scarcity and enhance model generalizability across different tasks and material systems. Therefore, in this work, we propose a foundation model specifically designed for composite materials, termed the material masked autoencoder (MMAE), to validate the feasibility of this approach. We generated a dataset comprising 100,000 grayscale images of short-fiber composites to facilitate self-supervised learning. By pre-training the MMAE on this dataset, the model learns robust and representative embeddings of the composite microstructures without reliance on labeled data. We anticipate that these embeddings capture essential features of the microstructures, making them highly informative for downstream tasks. To evaluate the effectiveness of the pre-trained MMAE, we employed direct numerical simulation (DNS) to generate datasets of homogenized stiffness for two types of composites: short-fiber composites and circular inclusion composites. We then fine-tuned the pre-trained models on significantly smaller labeled datasets to predict the homogenized stiffness of composites. Our results demonstrate that the MMAE achieves high accuracy, with a coefficient of determination (R2) reaching as high as 0.959 and consistently exceeding 0.91 even when trained on limited data. This performance highlights the potential of the MMAE as a foundation model for composite materials and paves the way for more efficient and cost-effective materials design and analysis."
https://arxiv.org/html/2411.06542v2,Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?,"Designing planners and controllers for contact-rich manipulation is extremely challenging as contact violates the smoothness conditions that many gradient-based controller synthesis tools assume. Contact smoothing approximates a non-smooth system with a smooth one, allowing one to use these synthesis tools more effectively. However, applying classical control synthesis methods to smoothed contact dynamics remains relatively under-explored. This paper analyzes the efficacy of linear controller synthesis using differential simulators based on contact smoothing. We introduce natural baselines for leveraging contact smoothing to compute (a) open-loop plans robust to uncertain conditions and/or dynamics, and (b) feedback gains to stabilize around open-loop plans. Using robotic bimanual whole-body manipulation as a testbed, we perform extensive empirical experiments on over 300 trajectories and analyze why LQR seems insufficient for stabilizing contact-rich plans. The video summarizing this paper and hardware experiments is found here.","Dexterous manipulation is full of contact-rich interactions, enabling various tasks through complex frictional interactions [1]. Historically, the non-smooth nature of contact has precluded a range of planning and control methods that rely on gradients of the dynamics. Recent advances have utilized contact smoothing — where non-smooth dynamics are replaced by a continuously differentiable proxy — to great effect as surrogate dynamics models for planning through contact [2, 3, 4, 5]. One may hope, then, that smoothing enables the use gradient-based control. (a) LQR. (b) Open-loop controller. Figure 1: These figures show snapshots of hardware experiments using LQR and open-loop controllers under perturbations to initial conditions of cylinder. The thick and thin lines represent the desired frame at the terminal time step and the current frame of the cylinder, respectively. While LQR outperforms open-loop in this example, a more comprehensive evaluation shows that LQR generally performs poorly. This work suggests that the above hope may face significant obstacles. We (1) introduce LQR control for contact-manipulation via contact smoothing. Furthermore, we (2) present and analyze robust trajectory optimization, hoping that the generated trajectories are robust to the model errors accumulated by using a surrogate dynamics model for control, and thus more amenable to LQR. Then, we (3) extensively evalute the performance of these methods, both in simulation and in hardware, on a bimanual whole-body manipulation as shown in Fig. 1. In short, we find: Despite its efficacy in planning through contact, dynamical smoothing alone is unsatisfactory as a means to obtaining linear control policies. Finally, we (4) identify the key factors leading to the inadequacies of linear control; namely, the unilaterality of contact, and the tendency of controllers to “push and pull” unless the dynamics are only very-slightly smoothed."
https://arxiv.org/html/2411.06528v1,Epistemic Integrity in Large Language Models,"Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration — where a model’s linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.","Large Language Models (LLMs) have markedly transformed how humans seek and consume information, becoming integral across diverse fields such as public health (Ali et al., 2023), coding (Zambrano et al., 2023), and education (Whalen & et al., 2023). Despite their growing influence, LLMs are not without shortcomings. One notable issue is the potential for generating responses that, while convincing, may be inaccurate or nonsensical—a long-standing phenomenon often referred to as “hallucinations” (Jo, 2023; Huang et al., 2023; Zhou et al., 2024b). This raises concerns about the reliability and trustworthiness of these models. A critical aspect of trustworthiness in LLMs is epistemic calibration, which represents the alignment between a model’s internal confidence in its outputs and the way it expresses that confidence through natural language. Misalignment between internal certainty and external expression can lead to users being misled by overconfident or underconfident statements, posing significant risks in high-stakes domains such as legal advice, medical diagnosis, and misinformation detection. While of great normative concern, how LLMs express linguistic uncertainty has received relatively little attention to date (Sileo & Moens, 2023; Belem et al., 2024). Figures 1 and 5 illustrate the issue of epistemic calibration providing insights into the operation of certainty in the context of human interactions with LLMs. We highlight the following key points in these figures: • Distinct Roles of Certainty: Internal certainty and linguistic assertiveness have distinct functions within LLM interactions that shape individual beliefs. • Human access to LLM certainty: Linguistic assertiveness holds a critical role as the primary form of certainty available to users. Unlike internal certainty, which remains hidden within the model’s computational processes, linguistic assertiveness is directly perceivable and influences how users interpret the model’s outputs. • Beyond Content: Users retrieve more than just the content from an LLM’s output. The style and assertiveness of the language used also play a significant role by shaping perceptions through the communication of certainty. This interaction between the model’s output and its linguistic assertiveness is crucial for understanding the full impact on individual perceptions. \mdfdefinestyle MyFrame linecolor=black, {mdframed} [style=MyFrame,nobreak=true,align=center,userdefinedwidth=32em] High Epistemic Calibration User input: ""Have humans ever been on the moon?"" LLM output: ""Yes, humans have been on the moon."" Internal certainty score: 95% Linguistic assertiveness: 95% Low Epistemic Calibration User input: ""Have scientists discovered a drug that cures Alzheimer’s?"" LLM output: ""Scientists have not discovered a drug that cures Alzheimer’s."" Internal certainty score: 60% Linguistic assertiveness: 95% Figure 1: This figure illustrates two examples with varying levels of epistemic calibration in LLM outputs. The one below is poorly calibrated. For each output, we calculate two certainty scores: internal certainty and external certainty (linguistic assertiveness). The internal certainty is computed using the method outlined by Rivera et al. (2024). To assess linguistic assertiveness, we develop a custom model, which we validate using human ratings collected through a survey. Several studies have explored the calibration of internal confidence in LLMs. For instance, Zhang et al. (2024) examine confidence calibration, proposing techniques to reduce hallucinations and enhance the model’s ability to answer known questions while avoiding unknown ones. However, they overlook the role of linguistic assertiveness and how external certainty can still lead to epistemic miscalibration even if internal confidence is addressed. Similarly, Ren et al. (2023) focus on factual knowledge and LLM behavior before and after retrieval-augmented generation (RAG). While they investigate internal confidence, they fail to frame miscalibration as an end-to-end issue involving both internal certainty and linguistic assertiveness, therefore ignoring the interplay between model predictions and how confidence is expressed linguistically. More recent studies have attempted to bridge the gap between internal confidence and linguistic assertiveness but still face considerable limitations. Although Mielke et al. (2022) explore epistemic calibration, their study uses a limited scoring scale to measure both assertiveness and confidence, which restricts the continuous assessment of LLM output. Their approach also relies on a narrow range of datasets, thereby limiting its applicability across domains. Zhou et al. (2024a) address miscalibration using epistemic markers, but their method lacks real domain grounding and fails to consider the complexity of natural language. This review of existing work on LLM calibration and confidence reveals several gaps that our research aims to address: • Lack of Integrated Approaches: Previous studies address either internal certainty or linguistic assertiveness but rarely both simultaneously (Jiang et al., 2021). There is a need for comprehensive frameworks that integrate these aspects to ensure LLMs communicate accurately and responsibly. • Inadequate Assertiveness Measurement: Existing methods for measuring linguistic assertiveness rely heavily on lexicon-based approaches (Pei & Jurgens, 2021) or subjective perceptions without adequate validation (Steyvers et al., 2024). These methods often lack contextual depth and fail to generalize across diverse domains. • Limited High-Stakes Evaluation: Although some studies explore epistemic calibration, they cover a narrow range of topics and employ low-resolution measures of assertiveness, limiting their applicability in critical domains such as misinformation detection (Mielke et al., 2022). To address these gaps, our paper provides: • A New Assertiveness Detection Model: We train a new model to detect linguistic assertiveness, using a composite of five diverse datasets. Our approach improves the accuracy relative to previous approaches by incorporating contextual nuance and aligning more closely with human perceptions. We also address limitations in previous methods in generalizably measuring assertiveness across domains. • Empirical Evidence of Epistemic Miscalibration: Our work provides a comprehensive comparison between internal certainty and linguistic assertiveness, documenting instances of miscalibration in different contexts. Our experiments reveal that LLMs frequently generate highly assertive explanations despite low internal certainty, which can mislead users. • Validation with Human Perception: We conduct comprehensive surveys assessing human perceptions of LLMs’ linguistic assertiveness. Our results confirm that there is a misalignment between computational measures and subjective human perceptions of language, highlighting the need for more robust linguistic calibration of LLMs. This study also presents a novel human-centered approach for developing a robust assertiveness scoring method. To ensure reliability, we train and compare several models to identify the best estimator for assertiveness based on accuracy and transferability using a new multi-domain dataset hand-coded to measure certainty. After selecting the top-performing model, we validate the results using a different human-surveyed dataset (Wang, 2017), which was coded independently by different individuals. This comprehensive methodology enables us to thoroughly assess both the objective and subjective aspects of assertiveness in language model explanations. Our findings reveal that when the model has low internal certainty, it generates explanations that are significantly over-assertive, meaning the language used implies a higher degree of certainty than is warranted by the model’s actual confidence or accuracy. This miscalibration could lead users to misconstrue the model’s judgments as more reliable than they actually are. More precisely, our results confirm a strong correlation between GPT 4o’s assertiveness scores and human perceptions of assertiveness, but a weak correlation between human perceptions and internal certainty, and an even weaker relationship between GPT 4o model assertiveness and internal certainty. Together, these results provide the most extensive evidence of epistemic miscalibration in LLMs to date.111For the code and datasets used, refer to our GitHub repository at: https://github.com/ComplexData-MILA/epistemic-integrity."
https://arxiv.org/html/2411.06525v1,I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength,"Video generation technologies are developing rapidly and have broad potential applications. Among these technologies, camera control is crucial for generating professional-quality videos that accurately meet user expectations. However, existing camera control methods still suffer from several limitations, including control precision and the neglect of the control for subject motion dynamics. In this work, we propose I2VControl-Camera, a novel camera control method that significantly enhances controllability while providing adjustability over the strength of subject motion. To improve control precision, we employ point trajectory in the camera coordinate system instead of only extrinsic matrix information as our control signal. To accurately control and adjust the strength of subject motion, we explicitly model the higher-order components of the video trajectory expansion, not merely the linear terms, and design an operator that effectively represents the motion strength. We use an adapter architecture that is independent of the base model structure. Experiments on static and dynamic scenes show that our framework outperformances previous methods both quantitatively and qualitatively. ††\dagger Corresponding author","Video generation technologies are explored to synthesize dynamic and coherent visual content, conditioned on various modalities including text (Blattmann et al., 2023c; Wang et al., 2024a; Gupta et al., 2023) and images (Blattmann et al., 2023b; Chen et al., 2024). Video generation has broad application potential across various fields, such as entertainment, social media, and film production. Motion controllability is crucial for ensuring that generated videos accurately meet user expectations, with camera control being one of the most important aspects. Camera control is the process of adjusting the position, angle, and motion of a camera, resulting in changes to the composition, perspective, and dynamic effects of a video. This technique is essential for generating professional-quality videos, as it influences the attention of viewers and enhances the expressiveness of scenes. Although precise camera control is crucial for producing high-quality videos, existing methods still face challenges. The first challenge pertains to the precision and stability of control. The lack of precision would result in an inaccurate reflection of the user control intention, significantly degrading user satisfaction. The second challenge is ensuring the natural dynamics of the subjects themselves, independent of camera movements. Similar to the challenges in multi-view (Mildenhall et al., 2020; Kerbl et al., 2023) and 3D geometric algorithms (Wang et al., 2021), where static scenes are much easier to handle than dynamic ones (Pumarola et al., 2020; Cai et al., 2022), generating plausible dynamics in videos proves to be more complex than managing static elements. Figure 1: We propose I2VControl-Camera, a novel camera control method for image-to-video generation. Our approach supports any camera movement style, offering high control precision and stability, ensuring natural dynamics and adjustable motion strength, which can be seen in Sec. 4. While AnimateDiff (Guo et al., 2024b) utilizes LoRA (Hu et al., 2022) strategy for controlling camera movements, the motion-LoRAs are confined to a limited set of fixed movement modes, lacking flexibility, and it only allows for coarse control, thus failing to provide precise scale adjustments. A direct and intuitive approach allowing for arbitrary camera movements is embedding the camera pose matrix, as in MotionCtrl (Wang et al., 2023). However, this method results in sparse input signals that heavily rely on the training set distribution, which leads to poor generalization capability. Consequently, it may inadequately respond to less common camera parameters within the training dataset, and thus hinders precise control over the motion’s amplitude. Although CameraCtrl (He et al., 2024) attempts to mitigate this sparsity issue by employing Plücker embeddings (Sitzmann et al., 2021), this parameterization lacks information of the input image, and it actually does not offer any additional information compared to the camera matrix used in MotionCtrl. Another natural strategy is novel view synthesis, which uses 3D implicit representations that can be rendered from arbitrary views, such as Cat3D (Gao* et al., 2024). Unfortunately, this strategy cannot support subject motion well, thus undermining the core goal of creating dynamic video content. In this paper, we propose I2VControl-Camera, a camera control method (some examples shown in Fig. 1) to surmount these prevalent issues in image-to-video generation, enhancing the control precision and adding control over the dynamic strength of subject motion in video output. To ensure control precision and stability, we use point trajectories in the camera coordinate system as our control signals, instead of extrinsic matrix. From the point trajectory function, we extract the linear term to serve as a proxy for camera control, ensuring high precision, stability and user friendliness. To control the motion strength, we further represent object motions with higher-order terms in the trajectory function and explicitly model the degree of dynamics. Specifically, we employ the derivative of the high-order terms to compute the motion speed of each point and integrate them in the image domain to obtain the entire motion strength as the control input of the network. This approach allows us to accurately gauge and adjust the amplitude of subject motion dynamics. We construct training data from regular RGB videos registering 3D tracking information and motion mask for them. Our approach features an adapter architecture that remains agnostic to the underlying base model structure. Experimentally, we conduct experiments in both static and dynamic scenes. For static scenes, we can set the motion strength to zero, resulting in significantly higher precision than previous methods. In dynamic scenes, we can configure a higher motion strength, which allows for both high control precision and vivid subject motion. Our approach outperforms previous methods both quantitatively and qualitatively. In summary, our contributions include: • We explicitly model decoupled motion representations: 3D rigid point trajectories and motion strength for camera and subject motion controls. • We propose a data pipeline to construct training control signals from RGB videos. • For both static and dynamic scenes, our method outperformances previous methods both quantitatively and qualitatively."
https://arxiv.org/html/2411.06510v1,Offline Handwritten Signature Verification Using a Stream-Based Approach,"Handwritten Signature Verification (HSV) systems distinguish between genuine and forged signatures. Traditional HSV development involves a static batch configuration, constraining the system’s ability to model signatures to the limited data available. Signatures exhibit high intra-class variability and are sensitive to various factors, including time and external influences, imparting them a dynamic nature. This paper investigates the signature learning process within a data stream context. We propose a novel HSV approach with an adaptive system that receives an infinite sequence of signatures and is updated over time. Experiments were carried out on GPDS Synthetic, CEDAR, and MCYT datasets. Results demonstrate the superior performance of the proposed method compared to standard approaches that use a Support Vector Machine as a classifier. Implementation of the method is available at https://github.com/kdMoura/stream_hsv.","Handwritten Signature Verification (HSV) systems aim to automatically distinguish between genuine signatures, belonging to the claimed individual, and forgeries. In offline HSV, signatures are represented as digital images captured after the writing process is completed, as opposed to online systems that analyze the signing dynamics [7]. Offline HSV systems can be categorized into two approaches: writer-dependent (WD) and writer-independent (WI). In WD systems, a unique classifier is trained for each enrolled user, offering potentially higher accuracy. However, this approach requires individual training data for each new user. Conversely, WI systems utilize a single classifier for all users, hence being more scalable [5]. In this case, the classification is performed on a dissimilarity space where the pattern recognition is reduced to a 2-class problem by using differences between claimed and reference signatures through a Dichotomy Transformation (DT) [5]. In general, HSV development entails two distinct datasets: a development set utilized for training and an exploitation set employed during the testing phase [6]. Each set comprises the signatures of enrolled users. While more samples generally lead to better generalization models, real-world applications often face data availability limitations regarding both user count and sample volume [5]. Therefore, the system’s capability to model signature variations is constrained to the present available data. After the training process, the resulting HSV is expected to achieve generalization to the whole set of existing users and their signatures. Nonetheless, by relying on a training process with a finite dataset, the current literature does not account for the inherent variability and changing behavior of handwriting signatures. Signatures exhibit the highest intra-class variability compared to other biometric traits [5]. Additionally, signature patterns are time-sensitive as they evolve as we age. Besides, diverse factors can impact the signing process, including emotional states, stress levels, fatigue, and influences from substances like alcohol or drugs [1]. Writing results are intrinsically related to cognitive-motor and neuromotor conditions, being affected by any minor impairment [1]. Given these challenges, we pose the following general research question: how can a signature verification system adapt to the inherent variability and evolving nature of handwritten signatures over time, maintaining high verification performance while mitigating the problem of limited data? To answer this question, we propose a framework to handle signature verification in an adaptive manner, where the input data is processed as a stream of offline signatures rather than a batch mode. In the proposed framework, incoming signatures are first tested and then used to improve the system by updating its current state. In this approach, SigNet-S [20], one of the state-of-the-art representation models, is employed to extract features of incoming claimed signatures. These feature vectors are then compared to corresponding reference vectors stored in the database to create dissimilarity samples via a stream dichotomy transformation. Lastly, the adaptive WI-classifier is updated based on the dissimilarity vectors. To the best of our knowledge, no prior work has considered signatures in an open-set, stream-based configuration. The main contributions of this article are as follows: 1. Stream HSV: we propose a novel HSV framework that adapts over time. This framework treats signatures as an infinite data stream, enabling continuous learning and improvement. 2. A stream dichotomy transformation: we introduce a stream dichotomy transformation process to facilitate adaptive learning from the incoming signature stream and address the challenge of imbalanced data ratios commonly encountered in stream scenarios. 3. Signature stream generation method: to facilitate evaluation using standard batch configurations, we introduce a method for generating signature streams based on the existing HSV evaluation protocol."
https://arxiv.org/html/2411.06508v1,Understanding the Role of Equivariance in Self-supervised Learning,"Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (e.g., colors) by being invariant to data augmentations. Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware. However, even for the simplest rotation prediction method, there is a lack of rigorous understanding of why, when, and how E-SSL learns useful features for downstream tasks. To bridge this gap between practice and theory, we establish an information-theoretic perspective to understand the generalization ability of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks. This synergy effect encourages models to extract class-relevant features to improve its equivariant prediction, which, in turn, benefits downstream tasks requiring semantic features. Based on this perspective, we theoretically analyze the influence of data transformations and reveal several principles for practical designs of E-SSL. Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance. We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field. Code is available at https://github.com/kaotty/Understanding-ESSL.","Self-supervised learning (SSL) of data representations has made remarkable progress. Existing SSL methods can be categorized into two types: invariant SSL (I-SSL) and equivariant SSL (E-SSL). The idea of I-SSL is to encourage the representation to be invariant to input augmentations (e.g., color jittering). Contrastive learning that pulls positive samples closer and pushes negative samples apart is widely believed to be a prominent I-SSL paradigm, leading to rapid progress in recent years [13, 50, 5, 80, 46, 30, 3, 32, 33, 60, 49, 14, 37, 15, 77, 64, 81]. Nevertheless, since invariant representations lose augmentation-related information (e.g., color information), their performance on downstream tasks can be hindered, as frequently observed in practice [47, 17, 34]. In view of these limitations of I-SSL, there has been a growing interest in revisiting E-SSL. Contrary to I-SSL, E-SSL learns representations that are sensitive to (or aware of) the applied transformation.111A rigorous definition of feature equivariance requires more restrictions; for example, the transformations must be invertible. In existing SSL literature, equivariance is (loosely) referred to as the property that features are aware of general input transformations (not necessarily invertible). We follow this convention in this paper. For instance, RotNet [31] is an early exemplar of E-SSL that learns discriminative features by predicting the rotation angles from randomly rotated images [43]. It has also been exploited in recent works and achieves promising improvements in conjunction with I-SSL [73, 67, 17, 18, 25, 54, 34]. Recently, E-SSL has shown potential for serving as the foundation for building visual world models [26]. Despite this intriguing progress in practice, compared to invariant SSL methods with a vast literature of theoretical analyses [58, 66, 48, 35, 68, 59, 78], there is little theoretical understanding of equivariant SSL methods. A particular difficulty lies in the understanding of the pretraining tasks, which may seem quite irrelevant to downstream classification. Taking RotNet as an example, the random rotation angle is independent of the image class, so it is unclear how rotation-equivariant representations are helpful for image classification. Generally speaking, it is unclear why, when, and how equivariant representations can generalize to downstream tasks. In view of this situation, the primary goal of this paper is not to design a new E-SSL variant, but to revisit the basic E-SSL methods and understand their essential working mechanisms. We fulfil this goal by proposing a simple yet theoretically grounded explanation for understanding general E-SSL from an information-theoretic perspective. We show that the effectiveness of E-SSL can be understood via the “explaining-away” effect in statistics, which implies an intriguing synergy effect between the image class C and the equivariant transformation A (e.g., rotation) such that almost surely, they have strictly positive mutual information when given the input X, i.e., I(C;A|X)>0 that explains the effectiveness of E-SSL. This understanding also provides valuable guidelines for practical E-SSL design with three principles to pursue a large synergy effect I(C;A|X): lossy transformations, class relevance, and shortcut pruning, as been validated on practical datasets. Theoretically, we also quantitatively analyze the influence of data transformation on the synergy effect with a theory model. Equipped with these theoretical findings, we further revisit advanced E-SSL methods in the recent literature [73, 67, 17, 18, 25, 54, 34] and find that many of these empirical successes can be well explained in our framework from two aspects: finer equivariance and multivariate equivariance. Besides, motivated by our theory, we also discover an under-explored aspect of E-SSL, model equivariance, where we show that adopting equivariant neural networks can yield strong improvements for certain E-SSL methods. These fruitful theoretical and practical merits suggest that our E-SSL theory provides a general and practically useful explanation for understanding and designing E-SSL methods that have the potential to guide future E-SSL designs."
https://arxiv.org/html/2411.06493v2,LProtector: an LLM-driven Vulnerability Detection System,"The security issues of large-scale software systems and frameworks have become increasingly severe with the development of technology. As complexity of software grows, vulnerabilities are becoming more challenging to detect. Although traditional machine learning methods have been applied in cybersecurity for a long time, there has been no significant breakthrough until now. With the recent rise of large language models (LLMs), a turning point seems to have arrived. The powerful code comprehension and generation capabilities of LLMs make fully automated vulnerability detection systems a possibility. This paper presents LProtector, an automated vulnerability detection system for C/C++ codebases based on GPT-4o and Retrieval-Augmented Generation (RAG). LProtector performs binary classification to identify vulnerabilities in target codebases. To evaluate its effectiveness, we conducted experiments on the Big-Vul dataset. Results show that LProtector outperforms two state-of-the-art baselines in terms of F1 score, demonstrating the potential of integrating LLMs with vulnerability detection.","AI has significantly progressed in various defect detection areas over a long period of time. An example is when Wang et al. [1] utilized AI for identifying medical problems, while Wu and collaborators [2], [3] employed it for detecting Electromigration problems. Defects in software, also referred to as software vulnerabilities, pose a significant challenge in the cybersecurity field. Xu et al. [4] demonstrate the various ways in which harm and software loss can arise from these vulnerabilities. Several restrictions are associated with conventional detection methods. Yao et al. [5] and Li et al. [6] discovered that Automated Program Repair (APR) depends on predetermined patterns or strategies to create patches. Nevertheless, the patches do not meet the expected quality standards. Code generated by APR may successfully meet certain test cases, but it could still struggle to address the underlying issue, leading to ineffective outcomes in different situations. Klees et al., Kai et al., and Han et al. [7], [8], [9] propose that while fuzzing tests are good at uncovering memory management errors, they are not as efficient at identifying intricate problems like race conditions and privilege escalation. Understanding the goals and procedures of a program is crucial in identifying logical errors, as depending only on fuzzing is insufficient for detecting and correcting them. Drawbacks are also linked to Static Analysis Tools (SAT). Pereia [10] highlighted that SAT tools generate many incorrect results because they do not take into account dynamic factors such as variable value changes in real-time. They frequently sound the alarm for issues that are not real. An instance could be discovering a possible buffer overflow in a code path that is not used. Additionally, Liu et al. [11] pointed out the difficulties SAT faces when handling intricate dynamic behaviors, such as dynamic memory allocation or conditional branches, leading to constraints in identifying dynamic vulnerabilities such as race conditions.These limitations highlight the need for more advanced methods. Similar innovations are needed for road damage detection. Han-Cheng Dan et al. [12]successfully improved detection accuracy and efficiency using the enhanced YOLOv7 algorithm, demonstrating the advantages of improved model strategies. In contrast, LLMs have powerful code generation and understanding capabilities [13], [14], [15], along with a rich knowledge base and strong generalization ability [16], [17], [18]. For instance, Tan et al. demonstrated that neural networks could effectively convert textual descriptions into 3D models using encoder-decoder architectures, showcasing the potential of LLMs to handle multi-modal tasks across various domains [19]. Zhang et al. finds the black-box LLMs like GPT-4o can label textual datasets with a quality that surpasses that of skilled human annotators, which offers a new perspective for automated software defect detection, as LLMs can achieve more efficient training and labeling when handling large volumes of unlabeled data [20]. Thus, we selected the GPT-4o, which is currently one of the most capable models, as the AI agent for LProtector. This ensures good robustness even in systems with strong interference [21]. We used the Big-Vul dataset [22] to evaluate how well LProtector works by measuring its performance against VulDeePecker [23] and Reveal [24]. To enhance LProtector’s cybersecurity knowledge, we used RAG methods to pick 500 random instances of CWE/CVE from the Big-Vul dataset and stored them in a vector database."
https://arxiv.org/html/2411.06463v1,RL-Pruner: Structured Pruning Using Reinforcement Learning for CNN Compression and Acceleration,"Convolutional Neural Networks (CNNs) have demonstrated exceptional performance in recent years. Compressing these models not only reduces storage requirements, making deployment to edge devices feasible, but also accelerates inference, thereby reducing latency and computational costs. Structured pruning, which removes filters at the layer level, directly modifies the model architecture. This approach achieves a more compact architecture while maintaining target accuracy, ensuring that the compressed model retains good compatibility and hardware efficiency. Our method is based on a key observation: filters in different layers of a neural network have varying importance to the model’s performance. When the number of filters to prune is fixed, the optimal pruning distribution across different layers is uneven to minimize performance loss. Layers that are more sensitive to pruning should account for a smaller proportion of the pruning distribution. To leverage this insight, we propose RL-Pruner, which uses reinforcement learning to learn the optimal pruning distribution. RL-Pruner can automatically extract dependencies between filters in the input model and perform pruning, without requiring model-specific pruning implementations. We conducted experiments on models such as GoogleNet, ResNet, and MobileNet, comparing our approach to other structured pruning methods to validate its effectiveness. Our code is available at https://github.com/Beryex/RLPruner-CNN.","Convolutional neural networks (CNNs) have demonstrated outstanding performance across a range of computer vision tasks, including image classification, detection, and segmentation (Krichen, 2023; Bharadiya, 2023; O’Shea and Nash, 2015; Gu et al., 2018; Li et al., 2022). As these architectures become wider and deeper, they gain an enhanced ability to extract complex features from input data. However, this increase in parameters leads to substantial computational costs, making inference both resource-intensive and slow. Moreover, there is a growing need to deploy CNNs on edge devices, which are often limited in computational power and memory (Rashid et al., 2022; Stahl et al., 2021). Consequently, effective methods for compressing CNNs are increasingly in demand, aiming to reduce parameter counts and accelerate inference while maintaining the original model’s performance. Such compression techniques are crucial for creating efficient and deployable CNNs that can meet the challenges of real-world applications. Several techniques have been proposed to compress CNNs, most of which fall into one of four categories: structured and unstructured pruning, quantization, low-rank factorization, and knowledge distillation (Cheng et al., 2020; Deng et al., 2020). Structured pruning (Fang et al., 2023; He and Xiao, 2024; Anwar et al., 2017) removes entire filters from neural networks and directly modifies the model’s architecture, enabling both compression and realistic acceleration on standard hardware. Unstructured pruning (Zhang et al., 2018; Ma et al., 2022), also known as weight pruning, removes specific unimportant elements from the weight matrix, which requires hardware or libraries that support sparse computation to achieve practical speedup. Low-rank factorization (Swaminathan et al., 2020; Idelbayev and Carreira-Perpinan, 2020) approximates the model’s weight matrix by decomposing it into a product of low-rank matrices. Quantization (Wu et al., 2016; Gong et al., 2014; Zhou et al., 2017) reduces the bitwidth of the weight data in a model, achieving significant compression, but also requires hardware support to realize theoretical speedups for low-bit quantization, such as binary or ternary quantization. Knowledge distillation (Gou et al., 2021; Cho and Hariharan, 2019) transfers knowledge from a larger, more advanced teacher model to a smaller student model, which may not generate a new compressed architecture but helps recover the performance loss due to compression. In this paper, we primarily focus on structured pruning, though we hope the insights from our methods will also inform future work in other categories. Figure 1: Increase in test error for a compressed VGG-16 model after pruning 10% of filters from each individual layer. The pruning is conducted based on three different baselines: the dense model, a model with 5% of filters pruned from all layers, and a model with 10% of filters pruned from all layers. The results highlight that different layers have varying sensitivity to pruning, and this sensitivity changes dynamically throughout the pruning process. A negative increase indicates that performance improves. In structured pruning, while it is crucial to identify the least important filters to prune from weight matrices with minimal performance degradation, it is equally important to assess the importance of different layers to the model’s performance and determine which layers should be pruned more or less. This involves learning the sparsity distribution across layers. As shown in Figure 1, the importance of different convolutional and linear layers to the overall model performance varies significantly. Moreover, after a certain extent of pruning, the relative importance among different layers also changes. To leverage this insight, we can assign higher filter sparsity to less important layers and lower sparsity to more important layers, adjusting the sparsity distribution dynamically throughout the pruning process to minimize the performance drop. In this paper, we introduce a novel approach called RL-Pruner. Unlike other structured pruning methods that learn the sparsity distribution among layers during sparse training (Liu et al., 2017a; Ding et al., 2021; Fang et al., 2023), RL-Pruner is a post-training structured pruning method that utilizes reinforcement learning with sampling to learn the optimal sparsity distribution across layers for several pruning steps. At each step, the current model architecture is defined as the state, while the pruning sparsity distribution acts as the policy. RL-Pruner adds Gaussian noise to the policy distribution to generate the real pruning sparsity distribution as an action, producing the corresponding compressed architecture as the next step’s state. Each generation is treated as a sampling process. For each step, RL-Pruner maintains a replay buffer that stores the pruning sparsity distribution actions and their corresponding Q values, computed by the reward function. The reward function can be defined flexibly, such as based on the compressed model’s test error, a combination of test error and parameter reduction ratio, or other criteria depending on the specific use case. After each pruning step, RL-Pruner updates the sparsity distribution policy to learn the optimal sparsity distribution. When computational resources allow, post-training stages are periodically applied after several pruning steps to recover any performance loss caused by pruning. We employ knowledge distillation in these post-training stages, with the original model acting as the teacher and the compressed model as the student. RL-Pruner can automatically extract dependencies between filters in different layers of the input model by tracking tensor computations. Currently, our approach supports several popular CNN architectures, including residual connections, concatenation connections and skip connections. As a result, our method does not require model-specific pruning implementations and can perform pruning autonomously, enhancing the method’s generality. To validate the effectiveness of RL-Pruner, we apply the proposed method to several popular CNNs used for image classification, including VGGNet (Simonyan and Zisserman, 2014), ResNet (He et al., 2016), GoogLeNet (Szegedy et al., 2015), and MobileNet (Howard, 2017), using the CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al., 2009). According to the experimental results, RL-Pruner achieves 60% channel sparsity for VGG-19 on CIFAR-100 and 40% channel sparsity for GoogLeNet and MobileNetV3-Large on CIFAR-100, all with a performance drop of less than 1%."
https://arxiv.org/html/2411.06442v1,Local Implicit Wavelet Transformer for Arbitrary-Scale Super-Resolution,"Implicit neural representations have recently demonstrated promising potential in arbitrary-scale Super-Resolution (SR) of images. Most existing methods predict the pixel in the SR image based on the queried coordinate and ensemble nearby features, overlooking the importance of incorporating high-frequency prior information in images, which results in limited performance in reconstructing high-frequency texture details in images. To address this issue, we propose the Local Implicit Wavelet Transformer (LIWT) to enhance the restoration of high-frequency texture details. Specifically, we decompose the features extracted by an encoder into four sub-bands containing different frequency information using Discrete Wavelet Transform (DWT). We then introduce the Wavelet Enhanced Residual Module (WERM) to transform these four sub-bands into high-frequency priors, followed by utilizing the Wavelet Mutual Projected Fusion (WMPF) and the Wavelet-aware Implicit Attention (WIA) to fully exploit the high-frequency prior information for recovering high-frequency details in images. We conducted extensive experiments on benchmark datasets to validate the effectiveness of LIWT. Both qualitative and quantitative results demonstrate that LIWT achieves promising performance in arbitrary-scale SR tasks, outperforming other state-of-the-art methods. The code is available at https://github.com/dmhdmhdmh/LIWT.","Single Image Super-Resolution (SISR) refers to the process of recovering a high-resolution (HR) image from a single low-resolution (LR) image and has been widely applied across various fields [Shi et al.(2013)Shi, Caballero, Ledig, Zhuang, Bai, Bhatia, de Marvao, Dawes, O’Regan, and Rueckert, Thornton et al.(2006)Thornton, Atkinson, and Holland, Zou and Yuen(2011), Gunturk et al.(2004)Gunturk, Altunbasak, and Mersereau, Duan et al.(2024)Duan, Qu, Yang, Wang, Zhang, and Song]. Most existing SISR models comprise a deep neural network (DNN) with an upsampling module like learnable deconvolutions or pixel shuffling [Dong et al.(2016)Dong, Loy, and Tang, Shi et al.(2016)Shi, Caballero, Huszár, Totz, Aitken, Bishop, Rueckert, and Wang] and can only deal with integer scaling factors, and these models necessitate retraining when encountering new scaling factors. Recent work has achieved arbitrary-scale super-resolution (SR) by replacing the upsampling layer typically used in previous methods with a local implicit image function and has demonstrated exemplary performance [Chen et al.(2021b)Chen, Liu, and Wang, Lee and Jin(2022)]. These methods based on local implicit functions first extract features from LR images through a DNN-based encoder and then employ multi-layer perceptrons (MLPs) to map the 2D query coordinates of the HR image and the aggregated representation of the corresponding local region features (called latent code) to RGB values. There are two limitations to these existing methods. Firstly, the coordinate-based local ensemble technique [Chen et al.(2021b)Chen, Liu, and Wang, Lee and Jin(2022)] used for querying RGB values fails to consider the relevance of features within local regions. Ensemble weights are typically computed based on the rectangular area between the query point and each nearest point (Figure 1(a)). These weights are solely dependent on the positional relationship between the query point and its nearest coordinates of local features and do not account for the features themselves, thus limiting the reconstruction performance of the model. Secondly, only the four nearest latent codes to the query point are used when querying RGB values based on coordinates. We argue that the representational capacity of LR features directly obtained from the encoder is limited, especially in large-scale SR, which may lead to blurry results lacking texture details. Introducing high-frequency prior information of image features into the local implicit functions is therefore necessary. Figure 1: Motivation and effectiveness of our method. (a) LIIF [Chen et al.(2021b)Chen, Liu, and Wang] and LTE [Lee and Jin(2022)] have difficulty reconstructing high-frequency details using the local ensemble technique. (b) LIWT introduces the high-frequency prior via DWT. (c) LIWT can reconstruct high-frequency details using attention weight based on the high-frequency prior. Many existing methods have shown that high-frequency prior information obtained from discrete wavelet transform (DWT) can improve the performance of SR models based on deep learning [Hsu and Jian(2023), Zou et al.(2022)Zou, Chen, Wu, Zhang, Xu, and Shao, Xin et al.(2020)Xin, Li, Jiang, Wang, Huang, and Gao]. However, the scale transformation rate of the DWT when applied to features or images is limited to powers of 2, and most DWT-based methods rely on this property for inverse transformation to achieve upsampling, thereby not achieving arbitrary-scale SR. To better restore high-frequency details while achieving arbitrary resolution upscaling, we propose the Local Implicit Wavelet Transformer (LIWT), which leverages cross-attention to exploit the high-frequency information obtained from DWT fully and accounts for the relevance of the features within a local region. As shown in Figure 1(b), LIWT consists of a Wavelet Mutual Projected Fusion (WMPF), a Wavelet-aware Implicit Attention (WIA), and a decoder. We first extract features from the LR image using an encoder and then decompose the features obtained into low-frequency components LL and high-frequency components LH, HL, and HH using DWT. To enhance LIWT’s capability to capture high-frequency details, we designed the Wavelet Enhancement Residual Module (WERM) to integrate the four components obtained by DWT and output features with high-frequency priors. Subsequently, we fuse the high-frequency priors with the features from the encoder using WMPF to assist in reconstruction. Then, we employed WIA to generate attention maps based on query coordinates and sample nearest-neighbor interpolated latent vectors from both the high-frequency prior features and the original features. By applying these attention maps to these feature embeddings, LIWT focuses more on high-frequency details in the image. Finally, the decoder utilizes attention feature embeddings to generate RGB values. As illustrated in Figure 1(c), employing attention weight based on high-frequency priors enables the reconstruction of high-frequency details. The main contributions of our work are summarized as follows: (1) We introduce the Local Implicit Wavelet Transformer (LIWT), which integrates features obtained from DWT into local implicit image functions using the designed WERM and WIA to enhance performance; (2) We demonstrate that LIWT can be effectively integrated into different encoders to enhance performance, outperforming other arbitrary-scale SR methods; (3) We conduct a comprehensive analysis of LIWT. Extensive experimental results demonstrate that the proposed LIWT can produce superior or comparable results on benchmark datasets."
https://arxiv.org/html/2411.06438v1,PLM-Based Discrete Diffusion Language Models with Entropy-Adaptive Gibbs Sampling,"Recently, discrete diffusion language models have demonstrated promising results in NLP. However, there has been limited research on integrating Pretrained Language Models (PLMs) into discrete diffusion models, resulting in underwhelming performance in downstream NLP generation tasks. This integration is particularly challenging because of the discrepancy between step-wise denoising strategy of diffusion models and single-step mask prediction approach of MLM-based PLMs. In this paper, we introduce Diffusion-EAGS, a novel approach that effectively integrates PLMs with the diffusion models. Furthermore, as it is challenging for PLMs to determine where to apply denoising during the diffusion process, we integrate an entropy tracking module to assist them. Finally, we propose entropy-based noise scheduling in the forward process to improve the effectiveness of entropy-adaptive sampling throughout the generation phase. Experimental results show that Diffusion-EAGS outperforms existing diffusion baselines in downstream generation tasks, achieving high text quality and diversity with precise token-level control. We also show that our model is capable of adapting to bilingual and low-resource settings, which are common in real-world applications.","Type Dataset OpenWebText Gokaslan and Cohen (2019) RocStories Mostafazadeh et al. (2016) Deontology Hendrycks et al. (2023) Question Generation Dhingra et al. (2017) QQP Wang et al. (2017) ALMA Xu et al. (2024a) ParaDetox Logacheva et al. (2022) Open-ended Generation ? \checkmark \checkmark \triangle \checkmark \times \times \times Conditional Generation ? \times \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark Context Provided ? - \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark Content Provided ? - \times \triangle \checkmark \checkmark \checkmark \checkmark Format Provided ? - - \times \times \times \checkmark \checkmark Table 1: Categorization of NLP tasks based on constraint aspects. \checkmark indicates full support, \times indicates no support, and \triangle indicates partial or limited support. As diffusion models significantly enhance the quality and diversity of generated outputs in continuous domains such as images and audio (Song et al., 2021b), recent research has increasingly applied diffusion models to NLP (Li et al., 2022; Gong et al., 2023a; Yuan et al., 2023; He et al., 2023). To integrate the diffusion process with traditional language models, research has predominantly followed two approaches: Continuous Diffusion Language Model (CDLM) (Yuan et al., 2023; Lovelace et al., 2023; Chen et al., 2023), which embed text in a continuous latent space, and Discrete Diffusion Language Model (DDLM) (He et al., 2023; Lou et al., 2024; Zhou et al., 2024; Shi et al., 2024; Sahoo et al., 2024; Zheng et al., 2024), which operate directly on the vocab space. Recent DDLMs adeptly reflect the highly structured nature of language; this property enables sequence control in natural language, generally resulting in high performance in NLP fields such as unconditional generation and open-ended generation (Lou et al., 2024). However, for DDLMs to be widely adopted across various NLP domains, particularly in tasks such as close-ended conditional generation — hereafter referred to as dataset-guided generation — existing DDLMs still exhibit shortcomings in their performance. Our experiments suggest that models such as SEDD falls short in such tasks, potentially limiting their practical applications. Better performance can be achieved by integrating Pretrained Language Models (PLMs) through the use of an encoder-based PLM as the initial setup for embedding functions, leveraging its capability to capture overall semantics (He et al., 2023). However, integrating PLMs into diffusion models is non-trivial as PLMs typically predict masked elements in a single step, whereas diffusion models require step-wise denoising based on the overall semantics of each timestep sequence, and such gap yields limited results. Therefore, we need to consider such inconsistency to effectively adopt PLMs into DDLMs by a new methodology. In this paper, we introduce Diffusion-EAGS, a novel approach that effectively integrates Mask Language Model (MLM)-based PLMs with DDLMs for dataset-guided generation. To address the gap between the step-wise nature of diffusion models and the one-step prediction strategy of PLMs, we begin by considering MLM-based PLMs as the denoising function at each step of the diffusion process. Since identifying where to apply denoising during the diffusion process is challenging for PLMs, we incorporate an entropy tracking module to support their operation. For constructing entropy tracking module, we interpret each denoising step as a constrained Markov Random Field (cMRF), which enables us to adopt adaptive Gibbs sampling, which then facilitates leveraging entropy information from conditional sequences. To train DDLMs to effectively exploit an entropy tracking module, we propose a forward process based on an entropy-based noising scheduling during training phase. Specifically, our entropy-based noising scheduling noises lower entropy tokens first, thereby learning to progressively generate sequences guided by the entropy from entropy tracking module, which enhances the quality of the generated text. Experimental results demonstrate that Diffusion-EAGS achieves outstanding performance compared to existing autoregressive model (GPT-2), CDLMs, and DDLMs across various dataset-guided generation tasks. We demonstrate that both PLMs and entropy-based Gibbs sampling contribute to higher performance in our ablation study. Furthermore, our model exhibits higher diversity in certain tasks compared to LLMs and can facilitate token-level generation, indicating the potential applicability of our model across a wide range of dataset-guided generation tasks. In addition, we validate that our model can adapt to both bilingual and low-resource settings, which are frequently encountered in practical applications."
https://arxiv.org/html/2411.06437v1,CTC-Assisted LLM-Based Contextual ASR,"Contextual ASR or hotword customization holds substantial practical value. Despite the impressive performance of current end-to-end (E2E) automatic speech recognition (ASR) systems, they often face challenges in accurately recognizing rare words. Typical E2E contextual ASR models commonly feature complex architectures and decoding mechanisms, limited in performance and susceptible to interference from distractor words. With large language model (LLM)-based ASR models emerging as the new mainstream, we propose a CTC-Assisted LLM-Based Contextual ASR model with an efficient filtering algorithm. By using coarse CTC decoding results to filter potential relevant hotwords and incorporating them into LLM prompt input, our model attains WER/B-WER of 1.27%/3.67% and 2.72%/8.02% on the Librispeech test-clean and test-other sets targeting on recognizing rare long-tail words, demonstrating significant improvements compared to the baseline LLM-based ASR model, and substantially surpassing other related work. More remarkably, with the help of the large language model and proposed filtering algorithm, our contextual ASR model still performs well with 2000 biasing words.111Code and checkpoints are available at https://github.com/X-LANCE/SLAM-LLM/tree/main/examples/contextual_asr","End-to-end (E2E) automatic speech recognition (ASR) systems demonstrate impressive performance [1] but often struggle with accurately recognizing rare phrases, such as technical terms and name entities that are infrequently encountered in training data. Integrating contextual knowledge into E2E ASR models for biased decoding is crucial for improving recognition accuracy in practical applications, such as commercial systems where users need personalized recognition for specific names, places, and entities, highlighting the academic and commercial value of supporting hotword customization. [2] To address this challenge, researchers have developed various methods to inject contextual information into E2E models. The traditional approaches involve shallow fusion [3, 4, 5, 6], which utilizes an independently built language model (LM). Similarly, on-the-fly (OTF) rescoring method [7, 8] compiles known word-level biasing phrases into a weighted finite state transducer (WFST), combined with a “speller” FST to convert grapheme or word-piece sequences into words. A contextual LM score derived from this combination is integrated into the decoding criterion of standard log-likelihood, controlled by a tunable hyperparameter. In contrast, all-neural contextual biasing methods [7, 9, 10, 11, 2] integrate an additional contextual module with the ASR module. For instance, CLAS [7] dynamically incorporates contextual information during speech recognition by embedding context phrases and integrating them into the ASR components via the attention mechanism. These methods can handle out-of-vocabulary terms effectively and do not demand careful tuning of hyperparameters like rescoring weights, leading to significant improvements over OTF rescoring. In practical applications, a biasing list often includes hundreds to thousands of biasing words. Retrieving a meaningful bias context vector is challenging, for it’s difficult for the cross-attention mechanism to accurately link the ASR decoder output with the large-scale sparse hotword embeddings. As the length of the biasing list increases and the number of interfering items grows, the model’s performance tends to decline. To tackle this problem, different works have introduced various filtering methods for processing the biasing list tailored to their specific models. For instance, CLAS proposes a “bias-conditioning” technique that activates a bias phrase only if its prefix matches part of the partially decoded hypothesis. A two-stage contextual word filter module [12] is introduced for attention-based context bias, which includes computing “Posterior Sum Confidence” and “Sequence Order Confidence”, especially designed for cascaded encoder ASR framework. The above approaches often involve intricate model structures and decoding processes. In this paper, inspired by the recent emergence of LLM-based ASR models [13, 14, 15, 16] with simple structures and powerful performance, we propose an effective LLM-based contextual ASR model, along with a robust filtering algorithm. Prior work MaLa-ASR [15] has successfully demonstrated the ability of the LLM-based ASR model to integrate textual keywords extracted from presentation slides to improve recognition of conference content. Different from their AVSR task scenario, where each utterance corresponds to a slide with limited related content, contextual ASR only utilizes a single long biasing list typically containing thousands of entries in the inference stage. Thus, we propose a filtering algorithm where we leverage coarse decoding results generated by a fine-tuned SSL model with a simple CTC [17] head to filter and select the most pertinent hotwords. Then the filtered hotwords are incorporated into the prompts input for LLM. Our proposed CTC-Assisted LLM-based contextual ASR model, trained and evaluated on Librispeech [18] corpus, achieves WER/B-WER of 1.27%/3.67% on the test-clean set and 2.72%/8.02% on the test-other set with a biasing list size of 100, showing notable relative reductions of 39.81%/63.37% and 35.24%/61.37% compared to the baseline LLM-based ASR model, and significantly outperforming other related work."
https://arxiv.org/html/2411.06409v1,Automated Strategy Invention for Confluence of Term Rewrite Systems,"Term rewriting plays a crucial role in software verification and compiler optimization. With dozens of highly parameterizable techniques developed to prove various system properties, automatic term rewriting tools work in an extensive parameter space. This complexity exceeds human capacity for parameter selection, motivating an investigation into automated strategy invention. In this paper, we focus on confluence, an important property of term rewrite systems, and apply machine learning to develop the first learning-guided automatic confluence prover. Moreover, we randomly generate a large dataset to analyze confluence for term rewrite systems. Our results focus on improving the state-of-the-art automatic confluence prover CSI: When equipped with our invented strategies, it surpasses its human-designed strategies both on the augmented dataset and on the original human-created benchmark dataset Cops, proving/disproving the confluence of several term rewrite systems for which no automated proofs were known before.","Term rewriting studies substituting subterms of a formula with other terms (Baader and Nipkow 1998), playing an important role in automated reasoning (Bachmair and Ganzinger 1994), software verification (Meseguer 2003), and compiler optimization (Willsey et al. 2021). Mathematicians have developed various techniques to analyze the properties of term rewrite systems (TRSs). However, many properties are undecidable (Baader and Nipkow 1998), implying that no technique can consistently prove a particular property. To navigate this undecidability, modern term rewriting provers typically employ complicated strategies, incorporating wide arrays of rewriting analysis techniques, with the hope that one will be effective. Each technique often accompanies several flags to control its behavior. The diversity of techniques and their controlling flags result in a vast parameter space for modern automation term rewriting provers. Manually optimizing strategies for undecidable problems is beyond human capacity given the extensive parameter space, inspiring us to apply machine learning to search for appropriate strategies automatically. In this paper, we focus on confluence, an important property of term rewriting, and discuss automated strategy invention for the state-of-the-art confluence prover CSI (Nagele, Felgenhauer, and Middeldorp 2017). We modify Grackle (Hůla and Jakubův 2022), an automatic tool to generate a strategy portfolio for a solver, encoding strategies that require transformations and complex schedules such as parallelism. We also conduct data augmentation on the human-built confluence problems database (Cops)111https://cops.uibk.ac.at/, a representative benchmark for the annual confluence competition (CoCo)222https://project-coco.uibk.ac.at/. As Cops has been created manually, it includes only 577 TRSs. They are of high quality, but the relatively small number is still inadequate for data-driven machine learning techniques that require large amounts of training data. To handle this problem, we generate a large number of TRSs randomly, but ensure that they are interesting enough to analyze. For this, we develop a procedure to confirm a relative balance in the number of problems most efficiently solved by different confluent analysis techniques within the dataset. We evaluate our strategy invention approach in Cops and the augmented dataset. On both of the datasets, the invented strategies surpass CSI’s default strategy. In particular, we prove (non-)confluence for several TRSs that have not been proved by any automatic confluence provers in the history of the CoCo competition. As an example, our invented strategy is able to prove the non-confluence for the Cops problem 991.trs, never proved by any participant in CoCo. The key to solving the problem is the application of the redundant rule technique (Nagele, Felgenhauer, and Middeldorp 2015) with non-standard arguments. CSI’s default strategy performs redundant -narrowfwd -narrowbwd -size 7 prior to performing non-confluence analysis. The flags narrowfwd and narrowbwd determine the categories of redundant rules to generate. Our tool automatically discovered that by changing the original redundant rule transformation to redundant -development 3 -size 7, we are able to prove this problem. A larger value for the flag development causes a larger number of development redundant rules to be added. We notice that the value three is crucial as values below three are ineffective for 991.trs. This is only one of the several problems which our new strategies can solve as discussed in the later sections. Contributions • To our best knowledge, our work is the first application of machine learning to automatic confluence provers. We automatically generate a lot of strategies for the state-of-the-art confluence prover CSI and combine them as a unified strategy. • We build a large dataset for confluence analysis, comprising randomly generated term rewrite systems and problems in the Cops dataset. • Empirical results show that our strategy invention approach surpasses CSI’s default strategy both in Cops and the augmented datasets. Notably, we discover several proofs for (non-)confluence that have never been discovered by any automatic confluence provers in the annual confluence competition."
https://arxiv.org/html/2411.06402v1,"Fineweb-Edu-Ar: Machine-translated Corpus 
to Support Arabic Small Language Models","As large language models (LLMs) grow and develop so too, do their data demands. This is especially true for multilingual LLMs, where the scarcity of high quality and readily available data online has led to a multitude of synthetic dataset generation approaches. A key technique seen in this space is machine translation (MT), where high quality English text is adapted to a target, comparatively low-resource, language.In this report we introduce FineWeb-Edu-Ar, a machine-translated version of HuggingFace’s exceedingly popular (deduplicated) FineWeb-Edu dataset. To the best of our knowledge FineWeb-Edu-Ar is the biggest publicly available machine-translated Arabic dataset out there, with its size of 202B tokens of an Arabic-trained tokenizer.The data is available on HuggingFace111https://huggingface.co/datasets/kaust-generative-ai/fineweb-edu-ar.","Natural Language Processing (NLP) has seen tremendous strides in recent years with the advent of LLMs, we have seen models scale up to more than 100 billion parameters Brown et al. (2020). With the ever growing demand for scale, we have seen a substantial focus on large corpora to train these LLMs on Hoffmann et al. (2022). Although data quality mattered, the focus had mostly been on data quantity. More recently however, we see small language models (SLMs) trained on a much smaller quality-focused corpus Gunasekar et al. (2023), Allal et al. (2024). These models exceed expectations for their size, outperforming certain much larger models on various benchmarks Abdin et al. (2024). This gave rise to new potential use cases for language models on edge devices with compute constraints Lu et al. (2024), Gunter et al. (2024). Many languages, including Arabic, suffer from a distinct lack of the same kind of high quality, educational focused, and readily available data that allowed other small language models to flourish. Figure 1: Quadrants of corpora availability. In order to illustrate the current situation with the training data, we’ve organized it in 4 quadrants: English and Arabic texts versus culturaly-agnostic and culturally-specific knowledge as in Figure 1. While generic English data (left top quadrant) is abundant, and Arabic culturally specific data (right bottom quadrant) can be extracted from the web archives Aloui et al. (2024), Arabic texts with the generic knowledge (top right quadrant) are scarce. Fortunately, the latter can be reinforced with the machine translated texts sourced from generic English texts. In an effort to support small language model pre-training, we publicly release FineWeb-Edu-Ar, a machine translated version of HuggingFaceTB/smollm-corpus Allal et al. (2024), the deduplicated version of Huggingface’s FineWeb-Edu Lozhkov et al. (2024). HuggingFaceTB/smollm-corpus that enabled their model, SmolLM, to achieve remarkable results for its size. We hope that this will support the field of Arabic SLMs. The dataset is released under CC-BY-NC-4.0 license. Additionally, to support community efforts in English to Arabic machine translation, we provide a preliminary analysis into the performance of several translation models. In general, we see two ways to obtain the educational-grade Arabic language dataset: (1) translating the English FineWeb-Edu, and (2) adapting the FineWeb-Edu pipeline for Arabic with subsequent re-running it on CommonCrawl. One notable attempt to do the latter is Farhat et al. (2024) which, however, does not feature semantic filtration, possibly due to the small total amount of Arabic content in the Web compared to English. Whereas Farhat et al. (2024) is a valuable dataset, specific to Arabic-speaking countries and obtained from the native speakers, we choose to follow the option of translation, since the source dataset (FineWeb-Edu-dedup) a lot of information that Arabic native speakers choose to get from English sources, and since these two ways are, for the most part, complimentary. As additional evidence for the acute interest to small condensed-knowledge datasets of specific languages we would like to mention Chinese-FineWeb-Edu OpenCSG (2024) which is the FineWeb-Edu semantic filtration pipeline run for Chinese language. Our contributions are: 1. An open sourced machine translated version of HuggingFace’s FineWeb-Edu to Arabic. 2. An analysis into the performance and computational requirements of 12 models for English to Arabic machine translation."
https://arxiv.org/html/2411.06396v1,A Variance Minimization Approach to Temporal-Difference Learning,"Fast-converging algorithms are a contemporary requirement in reinforcement learning. In the context of linear function approximation, the magnitude of the smallest eigenvalue of the key matrix is a major factor reflecting the convergence speed. Traditional value-based RL algorithms focus on minimizing errors. This paper introduces a variance minimization (VM) approach for value-based RL instead of error minimization. Based on this approach, we proposed two objectives, the Variance of Bellman Error (VBE) and the Variance of Projected Bellman Error (VPBE), and derived the VMTD, VMTDC, and VMETD algorithms. We provided proofs of their convergence and optimal policy invariance of the variance minimization. Experimental studies validate the effectiveness of the proposed algorithms.","Reinforcement learning (RL) can be mainly divided into two categories: value-based reinforcement learning and policy gradient-based reinforcement learning. This paper focuses on temporal difference learning based on linear approximated valued functions. Its research is usually divided into two steps: the first step is to establish the convergence of the algorithm, and the second step is to accelerate the algorithm. In terms of stability, Sutton (1988) established the convergence of on-policy TD(0), and Tsitsiklis and Van Roy (1997) established the convergence of on-policy TD(\lambda). However, “The deadly triad” consisting of off-policy learning, bootstrapping and function approximation makes the stability a difficult problem (Sutton and Barto 2018). To solve this problem, convergent off-policy temporal difference learning algorithms are proposed, e.g., BR (Baird et al. 1995), GTD (Sutton, Maei, and Szepesvári 2008), GTD2 and TDC (Sutton et al. 2009), ETD (Sutton, Mahmood, and White 2016), and MRetrace (Chen et al. 2023). In terms of acceleration, Hackman (2012) proposed a Hybrid TD algorithm with the on-policy matrix. Liu et al. (2015, 2016, 2018) proposed true stochastic algorithms, i.e., GTD-MP and GTD2-MP, from a convex-concave saddle-point formulation. Second-order methods are used to accelerate TD learning, e.g., Quasi Newton TD (Givchi and Palhang 2015) and accelerated TD (ATD) (Pan, White, and White 2017). Hallak et al. (2016) introduced a new parameter to reduce variance for ETD. Zhang and Whiteson (2022) proposed truncated ETD with a lower variance. Variance Reduced TD with direct variance reduction technique (Johnson and Zhang 2013) is proposed by (Korda and La 2015) and analysed by (Xu et al. 2019). How to further improve the convergence rates of reinforcement learning algorithms is currently still an open problem. Algorithm stability is prominently reflected in the changes to the objective function, transitioning from mean squared errors (MSE) (Sutton and Barto 2018) to mean squared bellman errors (MSBE) (Baird et al. 1995), then to norm of the expected TD update (Sutton et al. 2009), and further to mean squared projected Bellman errors (MSPBE) (Sutton et al. 2009). On the other hand, the algorithm acceleration is more centered around optimizing the iterative update the formula of the algorithm itself without altering the the objective function, thereby speeding up the convergence rate of the algorithm. The emergence of new optimization objective functions often lead to the development of novel algorithms. The introduction of new algorithms, in turn, tends to inspire researchers to explore methods for accelerating algorithms, leading to the iterative creation of increasingly superior algorithms. The kernel loss function can be optimized using standard gradient-based methods, addressing the issue of double sampling in residual gradient algorithm (Feng, Li, and Liu 2019). It ensures convergence in both on-policy and off-policy scenarios. The logistic bellman error is convex and smooth in the action-value function parameters, with bounded gradients (Bas-Serrano et al. 2021). In contrast, the squared Bellman error is not convex in the action-value function parameters, and RL algorithms based on recursive optimization using it are known to be unstable. It is necessary to propose a new objective function, but the abovementioned objective functions are all some form of error. Is minimizing error the only option for value-based reinforcement learning? The contributions of this paper are as follows: (1) Introduction of variance minimization (VM) approach for value-based RL instead of error minimization. (2) Based on this approach, we proposed two objectives, the Variance of Bellman Error (VBE) and the Variance of Projected Bellman Error (VPBE), and derived the VMTD, VMTDC, and VMETD algorithms. (3) We provided proofs of their convergence and optimal policy invariance."
https://arxiv.org/html/2411.06391v1,"CausalStock: Deep End-to-end Causal Discovery 
for News-driven Stock Movement Prediction","There are two issues in news-driven multi-stock movement prediction tasks that are not well solved in the existing works. On the one hand, “relation discovery” is a pivotal part when leveraging the price information of other stocks to achieve accurate stock movement prediction. Given that stock relations are often unidirectional, such as the “supplier-consumer” relationship, causal relations are more appropriate to capture the impact between stocks. On the other hand, there is substantial noise existing in the news data leading to extracting effective information with difficulty. With these two issues in mind, we propose a novel framework called CausalStock for news-driven multi-stock movement prediction, which discovers the temporal causal relations between stocks. We design a lag-dependent temporal causal discovery mechanism to model the temporal causal graph distribution. Then a Functional Causal Model is employed to encapsulate the discovered causal relations and predict the stock movements. Additionally, we propose a Denoised News Encoder by taking advantage of the excellent text evaluation ability of large language models (LLMs) to extract useful information from massive news data. The experiment results show that CausalStock outperforms the strong baselines for both news-driven multi-stock movement prediction and multi-stock movement prediction tasks on six real-world datasets collected from the US, China, Japan, and UK markets. Moreover, getting benefit from the causal relations, CausalStock could offer a clear prediction mechanism with good explainability.","The financial services industry has maintained a leading position in embracing data science methodologies to inform investment determinations. Within this domain, quantitative trading has garnered substantial attention from both academia and industry. Researchers have consistently worked on exploring different approaches to predict the stock movement (rise or fall of stock price) for many years, such as uni-stock movement prediction [21], multi-stock movement prediction [44, 23], news-driven stock movement prediction [42, 19] and so on, which have shown significant success. These methods usually model the stock movement prediction task as a time series classification problem. In this paper, we focus on the news-driven multi-stock movement prediction task. A prevalent model paradigm for this task often takes the historical price features and the stock-related news of multiple stocks as inputs and then leverages the well-designed neural networks to make stock movement predictions. There are two key modeling points for tackling this task: modeling the stock relations to enhance the prediction accuracy, and building the text mining module to extract effective information from news data that benefits stock movement prediction. Although previous work has made significant progress, there are still some issues that require further attention. We will elaborate on them in the following. For stock relation modeling, many existing works are commonly attention-based [15, 19, 23] or graph-based [34, 23]. These methods aim to model the correlation relation between stocks. However, the company relations are often unidirectional, such as the “investing” and “member of,” leading to the unidirectional relations of their stocks. Thus, causal relations are more appropriate for depicting the impact between stocks, as they identify the direction of information flow and are more informative than correlations. With the development of causal science, many researchers have started to use deep end-to-end networks for causal relations discovery of panel data or temporal data [9, 14], in which the causal relations are defined as directed acyclic graphs, i.e., causal graphs, and the Functional Causal Models (FCMs) are often utilized to optimize the causal graph by simulating the data generation mechanism. This provides a solid theoretical foundation for causal discovery for stocks. In recent years, an extrinsic text mining module has emerged as a plausible avenue through the alignment of financial news and social media posts, thereby elucidating intricate market insights that extend well beyond mere considerations of price dynamics, trading volumes, or financial indicators [41, 17, 35, 33]. Conventional text representations obtained by using GRU [42] or LSTM [15] exhibit many limitations. Specifically, news text data are often characterized by substantial noise because of the presence of irrelevant or ambiguous information [38, 7, 37]. The effective information for stock movement prediction gets intertwined with this noise, presenting a considerable challenge for these modules to discern meaningful signals accurately. In contrast, Large Language Models (LLMs) have unique advantages in this situation due to their advanced knowledge and reasoning abilities. Besides, LLMs can identify meaningful information within noisy environments [29, 4]. Motivated by these requirements, we propose an innovative news-driven multi-stock movement prediction model named CausalStock. In CausalStock, we design a Denoised News Encoder, which leverages LLMs to score every news text from multiple perspectives. Then the evaluation scores are taken as denoised text representations. To discover the causal relations between stocks, we propose a Lag-dependent temporal causal discovery module, from which we obtain the causal graph distribution. Based on the input market information and learned causal graph distribution, CausalStock employs an FCM [14] to make predictions. We summarize the contributions of our paper as follows: • We propose a novel news-driven multi-stock movement prediction method named CausalStock, which could discover the causal relations among stocks and make accurate movement predictions simultaneously. • Different from the past lag-independent causal discovery method [9], CausalStock involves a lag-dependent temporal causal discovery module, which intuitively links the temporal causal relations according to the time lag, making it more suitable for temporal stock data. • To extract useful information from the massive noisy news text data, an LLM-based Denoised News Encoder is proposed by taking advantage of the evaluation ability of LLM, which outputs the denoised news representation for better information utilization. Experiments on 6 public benchmarks show the performance of CausalStock as a news-driven multi-stock movement prediction method. Moreover, we conduct extensive analytical experiments to show the explainability of our key modules."
https://arxiv.org/html/2411.06387v1,Self-Training Meets Consistency: Improving LLMs’ Reasoning With Consistency-Driven Rationale Evaluation,"Self-training approach for large language models (LLMs) improves reasoning abilities by training the models on their self-generated rationales. Previous approaches have labeled rationales that produce correct answers for a given question as appropriate for training. However, a single measure risks misjudging rationale quality, leading the models to learn flawed reasoning patterns. To address this issue, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a self-training framework that further evaluates each rationale through follow-up questions and leverages this evaluation to guide its training. Specifically, we introduce two methods: (1) filtering out rationales that frequently result in incorrect answers on follow-up questions and (2) preference learning based on mixed preferences from rationale evaluation results of both original and follow-up questions. Experiments on three question-answering datasets using open LLMs show that CREST not only improves the logical robustness and correctness of rationales but also improves reasoning abilities compared to previous self-training approaches.111Code: https://github.com/JaehyeokLee-119/CREST","Large language models (LLMs) can enhance multi-step reasoning abilities by generating intermediate reasoning steps (i.e., rationale) before arriving at an answer Wei et al. (2022). Training LLMs on high-quality rationales has been shown to improve their reasoning capabilities Chung et al. (2024); Liu et al. (2023); Shridhar et al. (2023). Therefore, collecting high-quality rationales is becoming increasingly important for training the reasoning abilities of LLMs. However, due to the high cost associated with collecting high-quality rationales, self-training approaches have emerged, focusing on training LLMs using self-generated rationales Zelikman et al. (2022). Figure 1: An example of rationale generation and evaluation in CREST: An LLM generates two rationales (r^{1}, r^{2}) and answer predictions to solve question Q. Even though r^{2} lacks focus and clear support for the answer, previous approaches evaluate both r^{1} and r^{2} as equally right. Through a more fine-grained evaluation using follow-up questions, we can identify the better rationale, r^{1}, which leads to more consistent predictions across all questions. In self-training approaches, accurately evaluating the quality of generated rationales is essential. Previous studies have evaluated rationale quality by examining whether the generated rationales lead to the correct answer to a given question Zelikman et al. (2022); Hoffman et al. (2023); Feng et al. (2024); Hosseini et al. (2024); Singh et al. (2024). However, using the correctness of a single prediction is unstable, as LLMs can reach correct answers through inappropriate reasoning steps Bao et al. (2024). Figure 1 shows an example of two generated rationales, r^{1} and r^{2}. Despite r^{2} shows incomplete reasoning, previous approaches would consider both rationales equally appropriate since they both lead to the correct answer for Q. Training models on such inappropriate rationales can cause them to learn flawed reasoning patterns. To address this problem, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a novel framework for LLM self-training. The core idea of CREST is to further evaluate rationales using follow-up questions that ask whether each answer option in the original question is correct or not. We first generate diverse rationales and evaluate them with an LLM as shown in Figure 1. Subsequently, we train the LLM on these rationales, rewarding rationales that lead to more consistent predictions (i.e., r^{1}) and penalizing those that lead to less consistent predictions (i.e., r^{2}). To achieve this, we propose two methods: rationale filtering and preference learning. In rationale filtering, we remove rationales that lead to incorrect answers in more than a certain number of follow-up questions during the supervised fine-tuning process. In preference learning, we train the model on mixed preferences from results of both original and follow-up questions, to favor rationales that result in correct answers in a greater number of follow-up questions. We conduct experiments on three natural language reasoning question-answering datasets, including ReClor Yu et al. (2020), ARC Clark et al. (2018), and CSQA Talmor et al. (2019). We compare CREST to other self-training approaches using Llama 3 model AI@Meta (2024) and Gemma model Team et al. (2024). Our findings show that CREST can train an LLM to generate more correct and robust rationales, improving its reasoning performance. Our contributions are as follows: • We introduce consistency-driven rationale evaluation, which further evaluates generated rationales using follow-up questions that ask whether each answer option in the original question is correct or not. • We propose CREST, which evaluates generated rationales via consistency-driven rationale evaluation and uses the evaluation results to train an LLM through two methods: rationale filtering and preference learning using mixed preferences derived from original and follow-up question evaluations. • We conduct experiments and analyses with open LLMs such as Llama 3 model and Gemma model on three question-answering datasets. The results show that CREST generates more robust and correct rationales and improves reasoning ability compared to other self-training approaches."
https://arxiv.org/html/2411.06376v1,Phantom: Constraining Generative Artificial Intelligence Models for Practical Domain Specific Peripherals Trace Synthesizing,"Peripheral Component Interconnect Express (PCIe) is the de facto interconnect standard for high-speed peripherals and CPUs. Prototyping and optimizing PCIe devices for emerging scenarios is an ongoing challenge. Since Transaction Layer Packets (TLPs) capture device-CPU interactions, it is crucial to analyze and generate realistic TLP traces for effective device design and optimization. Generative AI offers a promising approach for creating intricate, custom TLP traces necessary for PCIe hardware and software development. However, existing models often generate impractical traces due to the absence of PCIe-specific constraints, such as TLP ordering and causality. This paper presents Phantom, the first framework that treats TLP trace generation as a generative AI problem while incorporating PCIe-specific constraints. We validate Phantom’s effectiveness by generating TLP traces for an actual PCIe network interface card. Experimental results show that Phantom produces practical, large-scale TLP traces, significantly outperforming existing models, with improvements of up to 1000× in task-specific metrics and up to 2.19× in Fréchet Inception Distance (FID) compared to backbone-only methods.","Figure 1: Topology of PCIe Devices in Modern Computing Systems. This diagram models a PCIe network interface card, highlighting key concepts like Transaction Layer Packet (TLP), Memory-Mapped Input/Output (MMIO), Direct Memory Access (DMA), Message Signaled Interrupt (MSI), and the transmit (TX) and receive (RX) pathways. It also includes examples of text-based traces, detailing the patterns and constraints. Proper synthesis of TLP traces requires addressing PCIe TLP constraints, such as ordering and causality, necessitating domain expertise. Figure 2: Overview of Phantom. The architecture of Phantom follows a 1+3 stage pipeline: generation, normalization, calibration, and decoding. Stage 0. Content Generation: The backbone generative AI model produces the initial content. Stage 1. Content Normalization: The content is normalized and defects are removed using the TLP trace visualization encoding method (See Section 3.2). Stage 2. Content Calibration: The normalized content is corrected using a dispersion-based calibration method (See Section 3.3) to ensure accuracy and consistency. Stage 3. Decoding: The calibrated content is decoded to produce the final output using the same visualization encoding method. Artificial intelligence tasks, including training, inference, and deployment, rely heavily on Peripheral Component Interconnect Express (PCIe) devices. Optimizing systems to support AI workloads through these peripherals has become a research focus (Han et al. 2022; Zhao et al. 2022). PCIe is the most common interconnection network for peripherals and hosts and is essential for devices like graphics cards, SSDs, and network interface cards. Transaction Layer Packets (TLP) are the smallest units of information transmitted within a PCIe network, similar to IP packets. Understanding a device’s TLP transaction pattern is crucial for modeling its impact on the CPU and assisting in design (Kuga et al. 2020; Neugebauer et al. 2018). However, identifying these patterns is challenging due to variations in task loads, network configurations, device models, and software updates. Therefore, preserving these patterns in some form for subsequent use is necessary. These patterns can be captured and formatted into traces suitable for subsequent analysis and storage. By replaying execution traces, developers can recreate environments and uncover hidden issues (Cornanguer et al. 2022; Tuor et al. 2018). If a TLP transaction trace for a device is obtained, the modeling for this device is nearly complete. However, obtaining the necessary traces can be daunting. For instance, collecting traces from a running server cluster could disrupt performance. Collecting traces from prototype hardware without actual samples is impractical when co-designing hardware-software. Even if traces are available, they may contain noise, making data collection and cleaning labor-intensive. Therefore, relying solely on collected traces is restrictive, and synthetic traces should be considered. Unfortunately, synthesizing PCIe TLP traces is challenging. Consider Figure 1, which illustrates the interaction between a PCIe Network Interface Card (NIC) and the CPU, highlighting characteristics reflected in the collected trace. Most PCIe operations use Memory-Mapped Input/Output (MMIO), which operations at different addresses interact with different components within the peripheral. For instance, operations at different positions within the BAR register mapping area affect different device registers, configuring various electrical states. Bulk data transfers rely on address constraints for asynchronous data transfer via the DMA engine. Given the influence of the software and hardware stack, TLP trace entries inherently include constraints such as order, causality, and data size limits. Ignoring these constraints in synthesized traces would render the content meaningless. While traditional statistical-based methods for synthesizing traces offer controllability and interpretability, they often fall short of capturing the complex patterns inherent in PCIe TLP transactions (Ij 2018). These methods, such as sampling and rule-based stitching (Thiebaut, Wolf, and Stone 1992; Phothilimthana et al. 2024), lack the flexibility and precision needed for high accuracy in this domain. As a result, there is growing interest in leveraging generative AI, which has proven successful in other areas like network traffic generation, security testing, and hardware design (Yin et al. 2022; Ye et al. 2019; Zheng et al. 2023). However, applying AI to PCIe TLP trace synthesis presents its own set of challenges: (1) how to convert the problem to a typical content generation task to leverage successful practices from related fields, and (2) how to introduce domain expertise as constraints to ensure controllability and predictability. To address these challenges, we propose Phantom, a synthesizer using generative AI, meanwhile ensuring the traces generated by the backbone model align with user requirements and closely resemble real data. Phantom comprises a backbone generative AI and a content calibration post-processor, as illustrated in Figure 2. The content calibration post-processor is divided into three stages: normalization, calibration, and decoding. To address the challenges mentioned above, we design a mapping between TLP operations and RGB triplets, redefining the PCIe TLP generation problem as an image generation task implemented in the normalization and decoding stages. Additionally, we employ a convolution-like method based on pixel dispersion, enabling selective inclusion of generated patterns or prior knowledge through hyperparameters. This forms the core of the calibration stage. This approach allows domain expertise to guide trace generation directly. The key contributions of our work can be summarized as follows: 1. We redefine the synthesis of PCIe TLP traces as an image generation problem, enabling the application of image-based designs and concepts to serve as building blocks. 2. To our knowledge, this is the first work that translates PCIe domain expert knowledge into constraints for generative AI, ensuring that the generated trace is both controllable and predictable. 3. We systematically construct a PCIe TLP trace synthesizer called Phantom, designed to assist with the research and design of new peripherals and related systems. 4. Extensive experiments demonstrate that Phantom can adapt to various existing AI generators and significantly improve performance on task-specific metrics by up to 1000\times, as well as the Fréchet Inception Distance (FID) metric by up to 2.19\times, efficiently calibrating generated data. Our work has been made open-source and is available for public use111https://github.com/sjtu-tcloud/Phantom."
https://arxiv.org/html/2411.06367v1,BayesNAM: Leveraging Inconsistency for Reliable Explanations,"Neural additive model (NAM) is a recently proposed explainable artificial intelligence (XAI) method that utilizes neural network-based architectures. Given the advantages of neural networks, NAMs provide intuitive explanations for their predictions with high model performance. In this paper, we analyze a critical yet overlooked phenomenon: NAMs often produce inconsistent explanations, even when using the same architecture and dataset. Traditionally, such inconsistencies have been viewed as issues to be resolved. However, we argue instead that these inconsistencies can provide valuable explanations within the given data model. Through a simple theoretical framework, we demonstrate that these inconsistencies are not mere artifacts but emerge naturally in datasets with multiple important features. To effectively leverage this information, we introduce a novel framework, Bayesian Neural Additive Model (BayesNAM), which integrates Bayesian neural networks and feature dropout, with theoretical proof demonstrating that feature dropout effectively captures model inconsistencies. Our experiments demonstrate that BayesNAM effectively reveals potential problems such as insufficient data or structural limitations of the model, providing more reliable explanations and potential remedies.","Explainable artificial intelligence (XAI) has become a significant field of research as machine learning models are increasingly applied in real-world systems including finance and healthcare. To provide insight into the underlying decision-making process behind the predictions made by these models, numerous researchers have developed various techniques to assist human decision-makers. Recently, Agarwal et al.[1] proposed a neural additive model (NAM) that utilizes neural networks to achieve both high performance and explainability. NAM is a type of generalized additive model (GAM) that involves the linear or non-linear transformation of each input and yields the final prediction through an additive operation. Previous studies have demonstrated that NAM not only learns complex relationships between inputs and outputs but also provides a high level of explainability based on neural network architectures and training techniques. Figure 1: Inconsistency of NAM, where two independent NAMs trained with the same dataset and architecture output different explanations solely due to different random seeds. In this paper, we analyze a critical yet overlooked phenomenon: the inconsistency phenomenon of NAM. Fig. 1 illustrates this issue, where two independent NAMs, trained on the same dataset and architecture, produce different explanations due solely to variations in random seeds. Such inconsistency has traditionally been viewed as a problem to be solved [2]. However, we argue that these inconsistencies are not merely obstacles but can offer valuable insights to uncover external explanations within the data model. Through a simple theoretical model, we show that NAMs naturally exhibit the inconsistency phenomenon even when trained on usual datasets that contain multiple important features. Building on this insight, we propose the Bayesian Neural Additive Model (BayesNAM), a novel framework that combines Bayesian neural networks with feature dropout to harness these inconsistencies for more reliable explainability. We also provide theoretical proof that feature dropout effectively leverages inconsistency. Our real-world experiments demonstrate that BayesNAM not only provides more reliable and interpretable explanations but also highlights potential issues in the data model, such as insufficient data and structural limitations within the model. The main contributions can be summarized as follows: • We investigate the inconsistency phenomenon of NAMs and analyze this phenomenon through a simple theoretical model. • We propose a new framework BasyesNAM, which utilizes Bayesian neural network and feature dropout. We also establish a theoretical analysis of the efficacy of feature dropout in leveraging inconsistency information. • We empirically demonstrate that BayesNAM is particularly effective in identifying data insufficiencies or structural limitations, offering more reliable explanations and insights for decision-making."
https://arxiv.org/html/2411.06363v1,Layer-Wise Feature Metric of Semantic-Pixel Matching for Few-Shot Learning,"In Few-Shot Learning (FSL), traditional metric-based approaches often rely on global metrics to compute similarity. However, in natural scenes, the spatial arrangement of key instances is often inconsistent across images. This spatial misalignment can result in mismatched semantic pixels, leading to inaccurate similarity measurements. To address this issue, we propose a novel method called the Layer-Wise Features Metric of Semantic-Pixel Matching (LWFM-SPM) to make finer comparisons. Our method enhances model performance through two key modules: (1) the Layer-Wise Embedding (LWE) Module, which refines the cross-correlation of image pairs to generate well-focused feature maps for each layer; (2)the Semantic-Pixel Matching (SPM) Module, which aligns critical pixels based on semantic embeddings using an assignment algorithm. We conducted extensive experiments to evaluate our method on four widely used few-shot classification benchmarks: miniImageNet, tieredImageNet, CUB-200-2011, and CIFAR-FS. The results indicate that LWFM-SPM achieves competitive performance across these benchmarks. Our code will be publicly available on https://github.com/Halo2Tang/Code-for-LWFM-SPM.","Humans have the ability to abstract and generalize low-level visual elements, such as contours, edges, colors, textures, and shapes, to form high-level semantic features that aid in recognizing and understanding the similarities and differences between objects. This capability is particularly crucial in few-shot classification tasks, as it allows models to accurately identify and distinguish between different categories based on contrasting critical high-level semantic features, even when faced with a limited number of samples from new categories. In contrast, traditional deep learning methods [1, 2] typically rely on large amounts of labeled data for training in order to recognize and classify specific objects or concepts. In few-shot learning scenarios, these models may encounter challenges, as they are not specifically designed to learn from a small amount of data quickly. Recently, few-shot learning methods have been introduced to address this limitation, typically requiring only a few images to understand the characteristics of a class and generalize these features to unseen images for inductive reasoning. Among these methods, metric-based approaches [3, 4, 5] are computationally efficient, as they do not require extensive parameter tuning or complex model structures. They rely on learning image embeddings to measure the similarity between objects and perform classification. However, through our study of prior methods, we identified two key limitations in metric-based methods: (1) The different semantic focuses produced by various levels of the backbone have not been fully utilized. Features at different levels typically focus on distinct aspects of the image [6]. In [3, 4], only features from the final layer of the backbone are utilized. However, this layer may be overly focused on a specific aspect of the image. In few-shot learning, it is crucial to leverage a wider variety of features to assess the similarity between image pairs more effectively. Chen et al. proposed using a self-attention mechanism to learn the relationships between multi-level features from the backbone but introduced excessive parameters and computational complexity [5]. (2) Semantic pixel misalignment is also a common challenge. In these methods, the final query embedding and support embedding are typically compared using element-wise cosine similarity. However, directly performing element-wise comparisons can often fail to correctly match semantically similar pixels, as illustrated in Fig. 1 (c). To address the issue of utilizing semantic focus, we propose a Layer-wise Embedding (LWE) Module. By computing the correlation map generated from different levels of information and performing layer-wise production, we effectively integrate diverse semantic features without relying on convolution or self-attention operations. As a result, our approach reduces both the number of parameters and the computational complexity. However, after aggregating multi-level semantic features, we observed a persistent issue of positional inconsistency between semantically corresponding objects in paired images. To tackle this challenge, we rearrange the semantic pixel positions in the image pair by leveraging the Hungarian matching algorithm and a learnable matcher. This ensures that the most similar semantic pixels are aligned spatially, enabling the calculation of a more accurate similarity score, which further aids in classification. Our proposed method integrates these two modules by using the LWE to learn semantic embeddings from different levels and the SPM to achieve pixel-wise similarity matching, which creates an end-to-end process that takes an image pair as input and outputs a similarity score. To sum up, our main contributions are as follows: \bullet We propose a novel Layer-wise Embedding Module that efficiently integrates semantic information from different levels with lightweight operations. \bullet We introduce the Semantic-Pixel Matching Module, which leverages the Hungarian algorithm and a learnable matcher to capture pixel-level correlations between image pairs, addressing the limitations of prior work in accurately assessing true similarity. \bullet Our proposed LWEM-SPM combines these two modules, and extensive experiments demonstrate that our method achieves competitive performance. Fig. 1: The key difference between our method and previous approaches lies in our layer-wise embedding computation and the way similarity is computed between query and support images. As shown in (a), Previous work typically uses CNNs or Transformers to generate single-layer image embeddings; however, a single-layer embedding may not effectively integrate complex semantic information. This feature is then used to compute a single-layer correlation map, which is applied to reweight the image features. In contrast, as shown in (b), our method integrates multi-layer outputs from the backbone to form multi-level correlation maps. We then compute layer-wise weights for the features at different levels, creating an image embedding that captures diverse semantic focuses across different levels while avoiding the complexity of CNNs and Transformers. In (c), prior methods typically calculate the similarity between corresponding pixels at the same locations in both images. However, this approach overlooks the possibility that semantically similar pixels may be located in different positions, making it difficult to assess the true similarity between the image pairs accurately. In contrast, (d) our method employs a matching algorithm that identifies the most similar pixel in the support image for each pixel in the query image, even if their positions do not align perfectly. This allows for a more accurate evaluation of the true similarity score."
https://arxiv.org/html/2411.06353v1,Deep Active Learning in the Open World,"Machine learning models deployed in open-world scenarios often encounter unfamiliar conditions and perform poorly in unanticipated situations. As AI systems advance and find application in safety-critical domains, effectively handling out-of-distribution (OOD) data is crucial to building open-world learning systems. In this work, we introduce ALOE, a novel active learning algorithm for open-world environments designed to enhance model adaptation by incorporating new OOD classes via a two-stage approach. First, diversity sampling selects a representative set of examples, followed by energy-based OOD detection to prioritize likely unknown classes for annotation. This strategy accelerates class discovery and learning, even under constrained annotation budgets. Evaluations on three long-tailed image classification benchmarks demonstrate that ALOE outperforms traditional active learning baselines, effectively expanding known categories while balancing annotation cost. Our findings reveal a crucial tradeoff between enhancing known-class performance and discovering new classes, setting the stage for future advancements in open-world machine learning.","Modern machine learning models have achieved remarkable progress by leveraging large amounts of labeled data LeCun et al. (2015); He et al. (2016); Khosla et al. (2020). Despite this success, most models are developed for closed-world settings, assuming that both training and test data originate from the same distribution. However, this assumption does not align with real-world environments, where models are inevitably encounter out-of-distribution (OOD) data with previously unseen classes Hendrycks & Gimpel (2022); Hendrycks et al. (2022); Salehi et al. (2022). Constrained by their fixed class boundaries, traditional models often struggle to generalize effectively to these novel classes, limiting their adaptability in open-world scenarios. Furthermore, obtaining human supervision for these novel examples in open-world scenarios is often time-consuming and costly, posing a significant challenge to model adaptation and improvement. Active learning, which iteratively selects the most informative examples for labeling, has emerged as a promising approach to address the expensive nature of gathering human supervision. By prioritizing examples that provide the most significant information gain, active learning can enhance the model’s learning efficiency while reducing the need for extensive human annotation. This approach is particularly valuable in open-world scenarios, where the high annotation cost and time-consuming nature of labeling make traditional supervised learning methods impractical. Despite its potential, existing active deep learning algorithms (see (Zhan et al., 2022; Zhang et al., 2024a) for overviews) have rarely been studied under open-world scenarios, particularly those involving novel classes and imbalanced data distributions. In this work, we addresses this gap by developing a novel active learning algorithm that integrates energy-based OOD detection techniques to handle the complexity of open-world environments. Our method is designed for multi-class classification tasks in the open-world, where the model encounters both known and unknown classes after deployment. Of the few works that study active learning under open-world settings, existing methods are often tailored to specific vision tasks, which result in highly specialized algorithm designs. When these methods are simplified for more generic problems like classification, they often reduce to basic uncertainty or diversity sampling techniques. In contrast, our approach in this paper offers a more comprehensive and versatile solution. By bridging OOD detection and diversity in our sampling strategy, we provide a more comprehensive sampling strategy compared to approaches that focus on only one of these aspects. We propose ALOE (Active Learning in Open-world Environments), a two-stage algorithm tailored for open-world active learning. ALOE addresses the unique challenges of class discovery in open-world settings by dynamically incorporating new OOD classes through a structured sampling strategy: In the first stage, ALOE performs diversity sampling to select a representative set of examples from the data pool. This sampling ensures broad coverage of the data distribution, capturing a range of potential new classes and concepts. By focusing on diversity, the algorithm increases the probability of identifying and learning from rare or infrequent classes that may otherwise be overlooked in a random sampling approach. The second stage leverages energy scoring function to rank examples within each cluster, prioritizing instances most likely to belong to unknown OOD classes. The energy-based OOD detection provides a unified framework that distinguishes between in-distribution (ID) and OOD examples with greater resilience to model overconfidence. By focusing annotation efforts on these high-priority examples, ALOE accelerates the discovery and learning of new classes, even when the annotation budget is limited. To empirically validate our approach, we conducted experiments on long-tail imbalanced image classification datasets. This choice of datasets is motivated by the prevalence of long-tail distributions in real-world scenarios, where rare classes or concepts are often underrepresented. Such imbalanced distributions make random sampling ineffective in discovering unknown classes, particularly those with small sample sizes. Our experimental results demonstrate the effectiveness of our approach. Compared with random sampling, on ImageNet-LT, ALOE saves 70% of annotation cost to achieve the same accuracy. In six out of six experimental settings (5.2.2 and 5.2.3), we observed that our algorithm performs the best comparing to all baseline experiments. These highlights underscore the potential of our method to significantly enhance model adaptation in open-world environments. Lastly, our work reveals a novel tradeoff between improving performance on known classes and discovering new ones. This finding opens up an important avenue for future research, as balancing these competing objectives is crucial for developing truly adaptive AI systems. In summary, our proposed active learning algorithm offers a promising solution for adapting machine learning models to new, previously unseen conditions in open-world environments. By efficiently incorporating unknown instances and minimizing human annotation efforts, our approach paves the way for more robust and adaptable AI systems capable of adapting to dynamic, real-world scenarios."
https://arxiv.org/html/2411.06306v1,Optimal Driver Warning Generation in Dynamic Driving Environment,"The driver warning system that alerts the human driver about potential risks during driving is a key feature of an advanced driver assistance system. Existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, the current design methods have several major limitations. Firstly, the warnings are mainly generated in a one-shot manner without modeling the ego driver’s reactions and surrounding objects, which reduces the flexibility and generality of the system over different scenarios. Additionally, the triggering conditions of warning are mostly rule-based threshold-checking given the current state, which lacks the prediction of the potential risk in a sufficiently long future horizon. In this work, we study the problem of optimally generating driver warnings by considering the interactions among the generated warning, the driver behavior, and the states of ego and surrounding vehicles on a long horizon. The warning generation problem is formulated as a partially observed Markov decision process (POMDP). An optimal warning generation framework is proposed as a solution to the proposed POMDP. The simulation experiments demonstrate the superiority of the proposed solution to the existing warning generation methods.","I INTRODUCTION The road traffic plays an important role in people’s lives. With the development of the complexity of city road networks, it is crucial for an advanced driver assistance system to be able to alert the potential risks to the human driver during driving. As shown in the studies of the human driver behavior with the warning system [1, 2, 3], existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, studies show that the human drivers’ reactions to warnings vary with the type of warning and different drivers [4, 5], while the existing methods have not addressed this phenomenon adequately. Most methods in the literature mainly generate the warning in a one-shot manner without modeling the ego driver’s reactions and surrounding objects [6, 7, 8], which reduces the flexibility and generality of the system over different drivers and scenarios. Meanwhile, the triggering conditions of warning are mostly rule-based threshold-checking based on the current state, such as the time-to-collision (TTC) and the minimum safety distance [9, 10, 11], which lacks the prediction of the potential risk in a sufficiently long future horizon. As a consequence, the current warning systems, while effective in preventing collisions, tend to prompt urgent and uncomfortable braking actions. Studies have emphasized the importance of executing smoother and more comfortable braking maneuvers to assist drivers in avoiding not only identified dangers but also collisions with subsequent vehicles [12]. This work seeks to address these issues by formulating an optimal warning generation problem that considers the relation between the generated warning and the driver’s reaction and also the interaction between the ego vehicle and other agents on a long horizon. The problem is modeled as a partially observed Markov decision process (POMDP), in which we quantify the value of warnings through the comfort and safety of the future ego vehicle trajectory in the context of surrounding objects, and the cost through their format and frequency. An optimal warning generation framework is proposed as a solution to the POMDP. The key contributions of this work are as follows: - We propose a novel formulation of the optimal warning generation problem that considers the driver and surrounding vehicle reactions, and both the safety and the comfort of future ego trajectories. - We propose a warning generation framework combining driver behavior estimation as the solution to the above problem. The framework has the flexibility to incorporate any prediction models of the driving scenario. - The proposed method is evaluated over comprehensive closed-loop simulation experiments, which demonstrates the superiority of the proposed solution to the existing warning generation methods."
https://arxiv.org/html/2411.06276v1,Multi-View Majority Vote Learning Algorithms: Direct Minimization of PAC-Bayesian Bounds,"The PAC-Bayesian framework has significantly advanced our understanding of statistical learning, particularly in majority voting methods. However, its application to multi-view learning remains underexplored. In this paper, we extend PAC-Bayesian theory to the multi-view setting, introducing novel PAC-Bayesian bounds based on Rényi divergence. These bounds improve upon traditional Kullback-Leibler divergence and offer more refined complexity measures. We further propose first and second-order oracle PAC-Bayesian bounds, along with an extension of the C-bound for multi-view learning. To ensure practical applicability, we develop efficient optimization algorithms with self-bounding properties.","Multi-view learning leverages multiple sets of features, or views, to enhance algorithmic performance and robustness (Sun, 2013; Xu et al., 2013; Zhao et al., 2017; Fang et al., 2023). For example, in image processing, combining visual data with depth or thermal information can improve object recognition tasks. However, while multi-view learning can improve learning outcomes, ensuring reliable generalization—particularly across multiple views—remains a critical challenge. To address this, researchers have explored generalization bounds for multi-view learning, often through the PAC (Probably Approximately Correct) framework (Blum & Mitchell, 1998; Dasgupta et al., 2001), and more recently using Rademacher complexity (Farquhar et al., 2005; Szedmak & Shawe-Taylor, 2007; Rosenberg & Bartlett, 2007; Sindhwani & Rosenberg, 2008; Rosenberg et al., 2009; Sun & Shawe-Taylor, 2010; Sun, 2011; Tian et al., 2021; Tang et al., 2023; Ma et al., 2024). Although these approaches have improved our understanding of multi-view learning, the PAC-Bayesian (PAC-Bayes) framework has emerged as especially effective for producing tighter generalization bounds in practical applications (Pérez-Ortiz et al., 2021). Sun et al. (2017) laid the theoretical groundwork for integrating multiple views in the PAC-Bayes framework, introducing the first PAC-Bayes bounds for multi-view learning by combining weight vectors from different views. This approach leveraged complementary information across views for consistent predictions. They later incorporated stability (Bousquet & Elisseeff, 2002) into their analysis (Sun et al., 2022). While Sun et al. (2017) made significant strides by introducing PAC-Bayes bounds for multi-view learning, their approach is constrained to two views, limiting its applicability in scenarios where data comes from numerous sources. Goyal et al. (2017) addressed this by proposing a more flexible PAC-Bayes approach for multiple views using a two-level process: view-specific classifiers are learned first, then combined to produce final predictions, making it more applicable to real-world scenarios. Despite the promising multi-view PAC-Bayes bounds proposed by Goyal et al. (2017), the lack of a direct optimization method complicates their practical utility. In response, Goyal et al. (2019b) proposed minimizing the PAC-Bayes C-Bound (Lacasse et al., 2006) for individual views (Multi-view C-Bound, given in Lemma 1, Equation 3 of (Goyal et al., 2019b)) rather than addressing the more complex C-Bound in Theorem 2, Equation 3. This shift underscores the challenge of optimizing the C-bound, as stated by Viallard et al. (2021). Additionally, Masegosa et al. (2020) introduced the concept of second-order oracle PAC-Bayes bounds in majority vote. These advanced bounds provide significantly improved precision over the first-order oracle bounds (Lacasse et al., 2006; Germain et al., 2015b), offering a novel analysis of the risk associated with weighted majority vote in multiclass classification, addressing the limitations of previous methods. Notably, while the work of Goyal et al. (2017, 2019b) primarily focuses on binary classification, extending their approach to incorporate these advanced second-order oracle PAC-Bayes bounds could yield valuable insights. Therefore, we propose the following contributions in the context of multi-view majority vote learning: General Multi-view PAC-Bayesian Bounds. We extend PAC-Bayes bounds for multi-view learning by integrating Rényi divergence. While Goyal et al. (2017) utilized techniques from Lemma 3 in Bégin et al. (2016) to establish well-known PAC-Bayes bounds, they did not explore the specific application of Rényi divergence proposed in Bégin et al. (2016). By incorporating Rényi divergence into the PAC-Bayes framework, we derive more general bounds, thereby extending and enhancing the initial results of Goyal et al. (2017). Extension to First/Second-Order Oracle Bounds. We propose first and second-order oracle multi-view PAC-Bayes bounds based on Rényi divergence, building on the approaches of Masegosa et al. (2020). Additionally, we extend the multi-view C-bound using a PAC-Bayes approach with Rényi divergence. Optimization Algorithms for Multi-View Learning. As a practical extension of our theoretical contributions, we introduce a set of optimization procedures specifically designed for multi-view learning within the PAC-Bayes framework. We propose self-bounding algorithms (Viallard et al., 2021), where the predictor returned by the learner includes a statistically valid upper bound on risk."
https://arxiv.org/html/2411.06263v1,"Federated Split Learning for Human Activity
Recognition with Differential Privacy","This paper proposes a novel intelligent human activity recognition (HAR) framework based on a new design of Federated Split Learning (FSL) with Differential Privacy (DP) over edge networks. Our FSL-DP framework leverages both accelerometer and gyroscope data, achieving significant improvements in HAR accuracy. The evaluation includes a detailed comparison between traditional Federated Learning (FL) and our FSL framework, showing that the FSL framework outperforms FL models in both accuracy and loss metrics. Additionally, we examine the privacy-performance trade-off under different data settings in the DP mechanism, highlighting the balance between privacy guarantees and model accuracy. The results also indicate that our FSL framework achieves faster communication times per training round compared to traditional FL, further emphasizing its efficiency and effectiveness. This work provides valuable insight and a novel framework which was tested on a real-life dataset.","In recent years, the widespread adoption of mobile devices has led to a growing interest in human activity recognition (HAR) using wearable sensors [1], [2]. This area has emerged as a novel research focus within artificial intelligence and pattern recognition [3]. This field intersects artificial intelligence and pattern recognition, with applications ranging from sports activity detection and smart homes to health support and beyond. By leveraging sensor data from devices like smartphones and wearables, HAR enables real-time monitoring and analysis of human activities. This capability is pivotal in enhancing personalized healthcare, improving athletic performance analysis, and developing intelligent environments that adapt to users’ needs seamlessly. Modern HAR systems, particularly those powered by deep learning techniques, offer several advantages over traditional methods [4]. They can automatically learn relevant features from raw sensor data, eliminating the need for manual feature engineering and thus improving accuracy and robustness. Moreover, deep learning models can handle complex, nonlinear relationships in the data, leading to better generalization across different users and activity types. Furthermore, HAR contributes to the advancement of human-computer interaction by enabling natural interfaces that respond to users’ physical actions and gestures. This technology has the potential to revolutionize how users interact with devices and how devices understand and respond to human behavior in real-world settings. However, the widespread adoption of HAR is constrained by challenges such as privacy concerns associated with personal data collected from users’ devices. Addressing these challenges requires innovative approaches in data anonymization, secure data storage, and compliance with regulations like General Data Protection Regulation (GDPR). Federated Learning (FL) concepts have risen as potential solutions to privacy concerns. FL is a collaborative framework that takes advantage of user devices’ enhanced computational power. In FL multiple users collaboratively train a machine learning model without sharing their personal data with the server, or other users. A global model is downloaded from the server by all user devices, then the users individually train the model using their local data, then finally the server averages the parameters of all users to form a new global model. This process is then repeated until the desired performance has been reached and shared with the client devices. Some models require many parameters which signify a potential limitation of FL. New concepts involving Split Learning (SL) have been developed as potential solutions to the limitations of FL [5],[6]. In SL, a model is initiated and then split into two models, the client-side model and the server-side model. Clients download their side of the model, which involves the input layer and preceding layers until the pre-defined cut layer. The Server-side model contains the rest of the model starting from the cut layer, to the output layer. Training begins at the client-side model with the client’s raw data until the cut layer is reached, the intermediate activations at this layer are then sent to the server to continue training. The server then trains its model up to the output layer, computes the loss, and starts back-propagation which will be completed on the client’s side. Federated Split Learning (FSL) has been developed to take advantage of both FL’s collaborative framework and SL’s splitting structure. The training process in FSL is similar to that in SL until the completion of the backward pass. The weights of the client models are then aggregated to produce a new global client model, and the server-side weights are updated based on the computed gradients. This method efficiently trains a deep neural network model while preserving user data privacy. I-A Related Works Several state-of-the-art techniques have introduced feature extraction and selection methods for HAR using traditional machine learning classifiers. With the emergence and advancement of high computational resources, deep learning techniques have become widely used in various HAR systems. These techniques efficiently retrieve features and perform classification, significantly enhancing the performance of HAR systems. Recently, FL has been studied to further improve the performance of HAR. Specifically, the authors in [7] evaluated FL for training a HAR classifier and compared it to centralized learning using two models—a deep neural network and softmax regression—trained on synthetic and real-world datasets. In [8], an FL system was proposed for HAR, with a perceptive extraction network for feature extraction to improve recognition accuracy. The study in [9] proposed a prototype-guided FL framework for HAR that addresses data issues in real-world environments by efficiently decoupling representation and classifier in heterogeneous FL settings. The work in [10] proposed an FL system for HAR that dynamically learns personalized models by capturing user relationships and iteratively merging model layers based on user similarity. As reported in [11], the authors designed a 2-dimensional FL framework to address data insufficiency and security in cyber-physical-social systems. Recently, SL and SFL has been recently studied in various IoT domains. The authors in [12] comprehensively surveyed the emerging applications of FL in IoT networks, exploring FL’s potential across various IoT services: data sharing, offloading, caching, attack detection, localization, mobile crowdsensing, and privacy as well as security enhancements. In [13], the authors proposed an efficient SL framework for resource-constrained edge computing systems, combining the benefits of FL and SL. The authors in [14] introduced a modified SL system with an autoencoder and adaptive threshold mechanism, reducing communication and computation overhead in an IoT system with minimal performance loss. The work in [15] proposed adaptive resource-aware SL for efficient IoT model training, accelerating processes on resource-constrained devices and minimizing straggler effects with device-targeted split points, while adapting to varying network throughput and computing resources. A dynamic FSL framework was developed by the researchers in [16] to address data and resource heterogeneity in IoT, enhancing efficiency through resource-aware split computing of deep neural networks and dynamic clustering of training participants. Despite these research efforts, the application of FSL has not been investigated for HAR in the literature. Exploring FSL in HAR is crucial as it paves the way for scalable and efficient deployment of personalized activity recognition systems in IoT and wearable technology, catering to individualized user needs while respecting data privacy and security. I-B Our Key Contributions In this paper, we propose a novel collaborative privacy-enhanced HAR framework through the development of an FSL algorithm with differential privacy (DP) [17] in edge computing. The contributions of this paper are summarized as follows. 1. We present an FSL algorithm for collaborative privacy-enhanced HAR in edge computing. In this framework, edge devices (EDs) are involved to partially perform forward propagation and backpropagation on the client-side models. The remaining workload of the model training will be executed at the edge server through the server-side model, which allows for mitigating the computation burden on resource-constrained EDs. 2. Moreover, to enhance privacy protection for activations, a DP mechanism is integrated at EDs to add a mask to hidden the shared information against potential data threats. 3. We conduct simulations on real-life HAR datasets to verify the feasibility of our FSL method. Implementation results demonstrate that our approach can significantly enhance the training performance with much lower training latency, compared with traditional training methods. We also validates the impacts of DP in HAR performance under different training settings. I-C Paper Structure The rest of the paper is structured as follows. In Section II, we present our system model, detailing the architecture and components of our proposed system. Section III presents simulations and performance evaluations for the proposed FSL approach under different network settings. Finally, Section IV concludes the paper."
https://arxiv.org/html/2411.06221v1,Smart-LLaMA: Two-Stage Post-Training of Large Language Models for Smart Contract Vulnerability Detection and Explanation,"With the rapid development of blockchain technology, smart contract security has become a critical challenge. However, existing smart contract vulnerability detection methods face three main issues: (1) Insufficient quality and comprehensiveness of datasets, due to the lack of detailed explanations and precise vulnerability locations in current datasets. (2) Limited adaptability of large language models (LLMs) to the smart contract domain, because most LLMs are typically pre-trained on vast amounts of general text data but very little smart contract-specific data. (3) Lack of high-quality explanations for detected vulnerabilities, as most existing methods focus solely on detection without providing clear explanations for their results. These limitations significantly hinder detection performance and make it harder for developers to understand and fix vulnerabilities quickly, potentially leading to severe financial losses. To address these problems, we propose Smart-LLaMA, an advanced detection method based on the LLaMA language model. First, we construct a comprehensive dataset covering four vulnerability types with labels, detailed explanations, and precise vulnerability locations. Second, we introduce Smart Contract-Specific Continual Pre-Training, using raw smart contract data to enable the LLM to learn smart contract syntax and semantics, thereby enhancing their adaptability to the smart contract domain. Furthermore, we propose Explanation-Guided Fine-Tuning, a novel approach that fine-tunes the LLM using paired vulnerable code and explanations, enabling it to both detect vulnerabilities and provide reasoned explanations for its results. To evaluate the quality of generated explanations, we employ both LLM evaluation and human evaluation, focusing on three key aspects: Correctness, Completeness, and Conciseness. Experimental results show that Smart-LLaMA outperforms state-of-the-art baselines, with average improvements of 6.49% in F1 score and 3.78% in accuracy, while providing reliable explanations. We have made all models, datasets, and code available.","The advent of blockchain technology has seen rapid adoption across various sectors, driven by its decentralized architecture [1]. This innovative technology enables the creation of secure, distributed digital ledgers for recording transactions [2]. Utilizing advanced cryptographic methods, blockchain ensures the integrity and verification of each transaction, establishing itself as a highly reliable technological framework [3, 4]. Within this ecosystem, smart contracts function as self-executing programs on the blockchain, automating the management of digital assets such as cryptocurrencies. These contracts activate when specific conditions are met and, once deployed, become permanent fixtures on the blockchain [5]. However, the immutable nature and inherent complexity of smart contracts present significant security challenges [5]. The well-documented DAO incident [6, 7] serves as a cautionary tale, illustrating the potential severity of such vulnerabilities. This security breach resulted in the unauthorized diversion of Ethereum valued at $60 million, causing widespread disruption within the blockchain community [8, 9]. This event underscores the critical importance of enhancing smart contract security to prevent similar devastating outcomes in the future. Researchers have developed various techniques to identify vulnerabilities in smart contracts, each addressing different aspects of the challenge but also facing limitations. Symbolic execution tools like Oyente [10], Mythril [11], Osiris [12], and Manticore [13], as well as static analysis tools such as Slither [14] and SmartCheck [15], rely on predefined patterns to detect vulnerabilities. However, these methods often struggle with complex scenarios and lack generalizability. We conducted a detailed survey of existing smart contract vulnerability datasets as shown in I, evaluating multiple datasets including A [16], B [17], C [18], and D [19], and found significant limitations. These datasets typically provide only basic vulnerability labels, lacking detailed explanations and precise location information. They cover a limited range of vulnerability types, usually only 1 to 3, failing to represent the diverse potential security risks in smart contracts. This simplified labeling approach severely constrains models’ ability to comprehensively understand and detect complex vulnerability patterns. These limitations directly affect the learning effectiveness of detection models, potentially leading to questionable accuracy and reliability in detection results. Some more advanced methods have attempted to address these limitations. Clear [20] employs a Contrastive Learning (CL) model to capture complex inter-contract relationships, while Zhuang et al. [17] and Luo et al. [21] introduce graph neural network-based approaches to represent smart contracts. However, the complexity of these graph structures makes them difficult to reproduce and less effective in representing programs accurately. Peculiar [16] and PSCVFinder [18] take a different approach by fine-tuning pre-trained models for vulnerability detection. While innovative, these methods still struggle to provide clear explanations for their detections, which is crucial for practical usage. Given these limitations, researchers have begun to explore the potential of using general-purpose Large Language Models (LLMs) to address smart contract vulnerability detection issues. General-purpose LLMs show promise in adapting to new patterns [22, 23]. However, they often struggle with smart contract-specific concepts and security implications. As illustrated in Figure 1, when presented with a smart contract, a general-purpose LLM like LLaMA-3.1-8B-Instruct incorrectly identifies a non-existent reentrancy vulnerability. It misinterprets the implications of external calls in the ’gotake()’ function, failing to recognize that reentrancy vulnerabilities typically arise when contract state or balance changes occur after external calls, which is not the case in this contract. To address these challenges, we propose our Smart-LLaMA, built upon the LLaMA-3.1-8B model. To overcome the limitations of existing datasets, we construct a comprehensive smart contract vulnerability dataset with detailed explanations and precise location information, covering four vulnerability types. This dataset is constructed through a three-step process: automated generation, LLM-based evaluation, and human expert verification and refinement. Specifically, we utilize the largest parameter versions of state-of-the-art LLMs (Qwen2 and Mistral-Large) to generate detection results, explanations, and specific vulnerability locations through carefully designed prompts. Llama-3.1-70B-Instruct serves as a judge model, evaluating these explanations on correctness, completeness, and conciseness. It scores each aspect from 1 to 10 to select the highest-quality explanations. Finally, human experts review the selected high-scoring explanations, verify their accuracy and make necessary improvements. This approach addresses the issue of insufficient dataset quality and comprehensiveness in existing resources. Furthermore, we introduce Smart Contract-Specific Continual Pre-Training to enhance the model’s understanding of smart contract-specific syntax structures and vocabulary, thereby improving the adaptability of LLaMA-3.1-8B to the smart contract domain. This process involves exposing the model to a large corpus of original smart contract code, allowing it to learn the nuances and intricacies of smart contract development. Additionally, we propose Explanation-Guided Fine-Tuning, a novel approach utilizing our constructed smart contract vulnerability explanations to fine-tune the large language model. This process enables the model to comprehend the entire vulnerability detection process. By training on datasets pairing vulnerable code with detailed explanations, Smart-LLaMA learns to both identify vulnerabilities and articulate the reasoning behind its detections. To evaluate the quality of explanations generated by our Smart-LLaMA, we utilize both LLM evaluation and human evaluation. Our evaluation is based on three key dimensions: Correctness, Completeness, and Conciseness, each scored on a 4-point Likert scale [24]. For LLM evaluation, we utilize Llama-3.1-70B-Instruct, carefully designing prompts to guide the model in assessing explanations based on these criteria. For human evaluation, we invite four experienced smart contract security experts. Each expert dedicate 8 hours to the assessment process, resulting in a total of 32 hours of in-depth analysis. The experts use the same 4-point Likert scale [24]. To ensure consistency, we arrange for 20% overlapping evaluation samples. We then tabulate the number of explanations receiving each score (1-4) for each dimension, providing a clear distribution of the quality assessments for both the baseline (LLaMA-3.1-8B-Instruct) and our Smart-LLaMA approach. We evaluated our Smart-LLaMA framework on a challenging dataset [19] encompassing four major vulnerability types: reentrancy, timestamp dependency, integer overflow/underflow, and delegatecall. The results demonstrated that Smart-LLaMA significantly outperformed state-of-the-art methods across all vulnerability types. Notably, Smart-LLaMA achieved F1 scores 7.35%, 1.24%, 7.82%, and 9.55% higher for reentrancy, timestamp dependency, integer overflow/underflow, and delegatecall vulnerabilities compared to the previous best performers. In terms of accuracy, Smart-LLaMA surpassed the previous SOTA methods by 4.14%, 0.62%, 4.83%, and 5.53% for these four vulnerability types. In addition to detection performance, we evaluated the quality of vulnerability explanations generated by Smart-LLaMA. Both LLM evaluation and human evaluation demonstrated that Smart-LLaMA produced more accurate, comprehensive, and concise explanations compared to LLaMA-3.1-8B-Instruct. For instance, in the human evaluation, Smart-LLaMA achieved the highest score (4 out of 4) for correctness, completeness, and conciseness in 69.5%, 57.1%, and 65.6% of cases, respectively, significantly outperforming the baseline method. The main contributions of this paper are as follows: • We propose Smart-LLaMA, a novel method combining smart contract-specific pre-training and explanation-guided fine-tuning for smart contract vulnerability detection, achieving state-of-the-art performance on four main vulnerability types. • We construct a high-quality smart contract vulnerability dataset that not only provides label, but also includes detailed vulnerability explanations, overcoming the limitations of existing datasets. • To the best of our knowledge, we are the first to explore explanation quality in smart contract vulnerability detection. We validate Smart-LLaMA’s effectiveness in generating high-quality explanations through both LLM evaluation and human evaluation. We have made all source code and datasets utilized in this research available to the public at https://zenodo.org/records/13860344"
https://arxiv.org/html/2411.06212v1,"MULTISTAGE NON-DETERMINISTIC CLASSIFICATION USING
SECONDARY CONCEPT GRAPHS AND GRAPH
CONVOLUTIONAL NETWORKS FOR HIGH-LEVEL FEATURE
EXTRACTION","Graphs, comprising nodes and edges, visually depict relationships and structures, posing challenges in extracting high-level features due to their intricate connections Multiple connections introduce complexities in discovering patterns, where node weights may affect some features more than others. In domains with diverse topics, graph representations illustrate interrelations among features.Pattern discovery within graphs is recognized as NPhard. Graph Convolutional Networks(GCNs)are a prominent deep learning approach for acquiring meaningful representations by leveraging node connectivity and characteristics.Despite achievements, predicting and assigning 9 deterministic classes often involve errors.To address this challenge, we present a multi-stage non-deterministic classification method based on a secondary conceptual graph and graph convolutional networks, which includes distinct steps:1)leveraging GCN for the extraction and generation of 12 high-level features; 2) employing incomplete, non-deterministic models for feature extraction, conducted before reaching a definitive prediction;3)formulating definitive forecasts grounded in conceptual (logical)graphs. The empirical findings indicate that our proposed approach outperforms contemporary methods in classification tasks.Across three datasets—Cora, Citeseer, and PubMed—the achieved accuracies are 96%, 93%, and 95%, respectively. Code is available at https://github.com/MasoudKargar","In recent decades, with the significant expansion of networks and the vast amount of data generated by these networks, the demand for graphs to effectively display these data has increased dramatically (Goyal and Ferrara, 2018) . As an essential data structure, a graph includes a set of nodes and edges. This data structure effectively describes and visualizes the relationships between nodes and edges (Waikhom and Patgiri, 2021; Jiang et al., 2020). Graphs have many applications in various fields, including social networks, sensor networks, and document networks (Pan et al., 2019). Feature-based graphs are another type of graphs where the nodes and edges have information or, in other words, features. Due to the extensive and complex interactions of information within networks, graph analysis is of particular importance. Classification is considered a challenge in feature-based graphs due to their unique features. Node and graph classification are two particular challenges in graph learning that have been recently noticed. In machine learning, classification is a common task that aims to classify nodes in a graph and predict their corresponding class labels. Furthermore, graph classification involves predicting class labels assigned to entire graphs (Li et al., 2019; Ramanath et al., 2018). Due to the high computational cost and space, analyzing these graphs, obtaining the structural relationship between the nodes, and exploiting the node content information are essential challenges. Recent research endeavors have employed graph embedding techniques to tackle this challenge (Cai et al., 2018; Wang et al., 2022). The primary objective of graph embedding techniques is to depict graph data in a lower-dimensional vector space, ensuring the preservation of its inherent structural characteristics. These embeddings find versatile applications across various tasks, including node classification, link prediction, and more. They function as input features for classification models, enabling the prediction of class labels for the graph. Besides graph embedding, alternative types of embeddings, such as node, edge, and infrastructure embeddings, exist (Xu, 2021). Deep learning methods, notably graph neural networks (GNN) and graph convolutional networks (GCN), have emerged as prominent techniques for analyzing and interpreting graphs (Xiao et al., 2022; Zeng et al., 2019). The core concept behind GCNs involves leveraging a graph structure to recognize the neighboring nodes of each node and acquire the node embedding by recursively aggregating the embeddings of its neighbors (Yu et al., 2021). One of the important challenges in this field is that the samples in the used data set may be members of several classes instead of being members of one class. In this case, making predictions and assigning to a definite class is usually associated with many errors. We propose a multistage non-deterministic classification based on a secondary conceptual graph and graph convolutional networks for node classification. The proposed architecture consists of several steps: 1) Using GCN to extract and generate high-level features from code and embedded inputs. 2) Using incomplete (non-deterministic) models for non-deterministic forecasting. 3) creating a secondary conceptual graph based on non-deterministic prediction, and 4) performing classification based on conceptual (logical) graph. According to Figure 1, in our proposed method, between the stages of high-level feature extraction using GCN and final classification, we first perform a non-deterministic prediction and then create a conceptual layer to create a conceptual graph. By creating and applying these steps in multi-class datasets, classification accuracy increases. The main contributions of this paper are: • We presented a multi-stage non-deterministic classification framework based on secondary conceptual graph and graph convolution networks, which is a combination of several stages. • We use two GCN layers and then make a non-deterministic prediction by the conceptual layers we created before classification to improve the classification performance. • Experimental results show that our method works better than node classification methods. Figure 1: Classification based on GCN RQ1: Can models based on incomplete models be effective in improving forecasting? RQ2: Can intermediate logic graphs affect the accuracy of the final prediction? RQ3: Can non-deterministic stage prediction be done with incomplete hidden layers? This research is organized into five chapters: Section 2 introduces the related work, and Section 3 presents the proposed methodology. Section 4 reviews the experimental data and research results; section 5 describes the research conclusions and discusses future works."
https://arxiv.org/html/2411.06208v1,IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization,"In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces Trace, a benchmark for improving and evaluating the complex instruction-following ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and out-of-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on out-of-domain data compared to SFT and DPO respectively.","The rapid development of large language models (LLMs) has facilitated human-machine interaction, with instructions serving as the medium Gao et al. (2024); Kim et al. (2024); Zhang et al. (2024b). As human needs evolve, there is an increasing expectation for models to handle more intricate tasks through complex instructions Ge et al. (2023); Yang et al. (2024b); Wang et al. (2024). Consequently, the instruction-following ability, especially complex instructions, is garnering significant attention Zhou et al. (2023); Xu et al. (2024); Li et al. (2024); Zhang et al. (2024a). Figure 1: Alignment Paradigms (a) Existing DPO Series vs. (b) Proposed IOPO. The green arrow indicates that y matches x while the red one indicates a mismatch. To evaluate the instruction-following abilities of LLMs, several benchmarks Zhou et al. (2023); Qin et al. (2024); Li et al. (2024) have been proposed, which are designed to systematically assess how well these models can understand and execute instructions. IFEval Zhou et al. (2023) focuses on verifiable instructions which are amenable to objective verification of compliance. Qin et al. (2024) introduces InFoBench which contains 500 distinct instructions and 2,250 decomposed questions to assess the ability of instruction following. Recently, the ability to follow complex instructions with multiple constraints is gaining increasing attention He et al. (2024b); Jiang et al. (2024); Wen et al. (2024); He et al. (2024a) as LLMs are deployed in sophisticated real-world applications. Zhang et al. (2024a) proposes a constraint-following benchmark CFBench with 1,000 multi-constraint samples. However, most of benchmarks lay emphasis on evaluating LLMs’ ability to follow complex instructions, lack of algorithms tailored for enhancing the corresponding ability. From RLHF (Reinforcement Learning from Human Feedback) Ouyang et al. (2022); Bai et al. (2022a) to the following-up researches such as DPO (Direct Preference Optimization) Rafailov et al. (2023), alignment algorithms which align LLMs with human preferences, have demonstrated their effectiveness in improving the LLMs’ capabilities to follow instructions. Nevertheless, these methods directly explore different responses y (y_{\rm win}, y_{\rm loose}) based on the same instruction x, as shown in Figure 3 (a). In the complex instruction scenario which contains multiple constraints, it is challenging to efficiently perceive the fine-grained constraints in x solely by modeling different y. To bridge this gap, this paper first introduces Trace benchmark to improve the ability of LLMs to track complex fine-grained constraint instructions and make them more obedient. Trace is automatically constructed based on the manually sorted taxonomy of complex instructions with 26 constraint dimensions within 5 constraint types. We develop an automated data construction workflow that extends from open-source simple instructions to multi-constrained complex ones. In the end, we accumulate 120K complex instructions for model training and 1K human-verified data for evaluation. To enhance the ability of LLMs to follow complex instructions, this paper further proposes Input-Output Preference Optimization (IOPO) method. IOPO not only takes the instruction x as input to directly learn the response y preference, but also gradually delves deeper into instructions x based on the same response y, to promote effective perception of fine-grained constraints, as shown in Figure 3 (b). The major contributions of this paper are summarized as follows: • We introduce a benchmark Trace for complex instruction following, which includes both an evaluation set and a training set, and an automated data construction workflow, further enriching the research community. • Different from previous alignment paradigm, we propose IOPO alignment method which deeply explores the complex instructions x (Input), not just directly learning response preference y (Output). • Extensive experiments on both in-domain and out-of-domain evaluations have confirmed the consistent improvements, with an average increase of 7.22% and 2.66%, compared to SFT and DPO, respectively."
https://arxiv.org/html/2411.06135v1,Online Parallel Multi-Task Relationship Learning via Alternating Direction Method of Multipliers,"Online multi-task learning (OMTL) enhances streaming data processing by leveraging the inherent relations among multiple tasks. It can be described as an optimization problem in which a single loss function is defined for multiple tasks. Existing gradient-descent-based methods for this problem might suffer from gradient vanishing and poor conditioning issues. Furthermore, the centralized setting hinders their application to online parallel optimization, which is vital to big data analytics. Therefore, this study proposes a novel OMTL framework based on the alternating direction multiplier method (ADMM), a recent breakthrough in optimization suitable for the distributed computing environment because of its decomposable and easy-to-implement nature. The relations among multiple tasks are modeled dynamically to fit the constant changes in an online scenario. In a classical distributed computing architecture with a central server, the proposed OMTL algorithm with the ADMM optimizer outperforms SGD-based approaches in terms of accuracy and efficiency. Because the central server might become a bottleneck when the data scale grows, we further tailor the algorithm to a decentralized setting, so that each node can work by only exchanging information with local neighbors. Experimental results on a synthetic and several real-world datasets demonstrate the efficiency of our methods.","Online multi-task learning (OMTL) processes related to learning tasks sequentially aim to leverage the correlation among multiple tasks to improve overall performance. In each online round, the learner receives multiple instances per task, predicts their labels, and then updates the model based on the true labels. A principal assumption for OMTL is the existence of potential similarities among multiple tasks—the samples of a single task obey a probability distribution similar to the probability distributions of other tasks. This assumption enables the OMTL to learn several models collaboratively using the shared information among different tasks. Compared with learning each task separately or treating all tasks as a whole, such a collaborative learning approach can enhance the performance of all tasks together. OMTL is a real-time, scalable, and continuously adaptive learning method [1]. It has been applied in sequential decision making fields that require prompt response, such as online personalized recommendations [2], targeted display advertising [3], and sales forecasts for online promotions [4]. During the past decades, several OMTL algorithms have been proposed, most of which are based on online gradient descent (OGD), such as mirror descent, dual averaging, and their proximal versions [5, 6]. In particular, OGD is typically used for solving OMTL problems when it is easy to compute the gradient (or sub-gradient) of the online objective, and there are no constraints on the model. Proximal OGD is usually applied when the regularization term of the model is non-smooth (e.g., L1 norm) [7]. Its proximal objective frequently enjoys a closed-form solution. However, for some regularization terms, such as the graph-guided L1 norm \|\mathbf{F}\mathbf{w}\|_{1} [8], adapting (proximal) OGD methods for distributed online learning settings is non-trivial because sub-gradient methods cannot make \mathbf{F}\mathbf{w} sparse and its proximal objective has no closed-form solution. Furthermore, the scalability of OGD-based multi-task algorithms deteriorates when the gradient’s dimensionality and the number of tasks increases, making them inadequate for large-scale learning problems. Unlike OGD methods, the alternating direction multiplier method (ADMM) [9] is more applicable to general learning tasks because it does not require the objective to be differentiable. Specifically, it decomposes the global problem into smaller, easier-to-solve sub-problems suitable for independent workers. Each worker solves its own sub-problem, which depends only on its own variables. Subsequently, a server optimizes the global problem by aggregating dual variables from all sub-problems. Owing to these advantages, ADMM is more suitable for general distributed tasks and is regarded as a viable alternative to OGD for large-scale learning problems [10, 11]. Since ADMM has shown superior ability at optimizing multi-task in the batch learning setting [12, 13], it is attractive to study it in the online scenario, particularly with a distributed computing architecture, so that the learning efficiency can be considerably enhanced by processing multiple tasks in parallel. Therefore, we propose to perform distributed OMTL using ADMM in this study. The task-similarity assumption is imposed by decomposing the model-to-learn into two parts: several unique patterns per task and a global pattern shared by all tasks. The unique patterns are further used to learn the potential relations among tasks on the fly to meet the constant changes in online learning. We explore the architecture’s two distributed forms, namely, the centralized version with a central server and the relatively decentralized version where all workers involved in solving the optimization problem communicate asynchronously. The goal of this study is to parallel execute online multi-task learning in distributed computing frameworks, where a task covariance matrix of multiple tasks is exploited to mine the potential relationships among them. This is expected to enhance the effectiveness of the proposed method and reduce communication consumption during the optimization process. We conducted numerical experiments on a synthetic and five real-world datasets 111Our code is released: https://github.com/Alberta-Lee/NC-24.git. The experimental results demonstrate the effectiveness of this optimization framework. The rest of this paper is organized as follows: We outline related works in Section 2 before introducing the OMTL problem setting in Section 3. We then deduce the ADMM optimization framework for online parallel multi-classification tasks in Section 4 and analyze its performance experimentally on several datasets in Section 5. Finally, we conclude our study in Section 6."
https://arxiv.org/html/2411.06122v1,Characteristics of Political Misinformation Over the Past Decade,"Although misinformation tends to spread online, it can have serious real-world consequences. In order to develop automated tools to detect and mitigate the impact of misinformation, researchers must leverage algorithms that can adapt to the modality (text, images and video), the source, and the content of the false information. However, these characteristics tend to change dynamically across time, making it challenging to develop robust algorithms to fight misinformation spread. Therefore, this paper uses natural language processing to find common characteristics of political misinformation over a twelve year period. The results show that misinformation has increased dramatically in recent years and that it has increasingly started to be shared from sources with primary information modalities of text and images (e.g., Facebook and Instagram), although video sharing sources containing misinformation are starting to increase (e.g., TikTok). Moreover, it was discovered that statements expressing misinformation contain more negative sentiment than accurate information. However, the sentiment associated with both accurate and inaccurate information has trended downward, indicating a generally more negative tone in political statements across time. Finally, recurring misinformation categories were uncovered that occur over multiple years, which may imply that people tend to share inaccurate statements around information they fear or don’t understand (Science and Medicine, Crime, Religion), impacts them directly (Policy, Election Integrity, Economic) or Public Figures who are salient in their daily lives. Together, it is hoped that these insights will assist researchers in developing algorithms that are temporally invariant and capable of detecting and mitigating misinformation across time.","Misinformation is a statement that contains false or misleading information, and can result in serious consequences, including the erosion of civil discourse, political paralysis, uncertainty, in addition to alienation and disengagement (Kavanagh & Rich, 2018; Hook & Verdeja, 2022). Despite its serious impact on individuals and society, misinformation is known to be shared more than valid information (Vosoughi, et al, 2018), and the reasons that misinformation is propagated are diverse and include cognitive factors (Del Vicario, et al , 2016; Ecker, et al, 2022), socio-affective factors (Ecker, et al, 2022), incentives (Ceylan, et al, 2023) and changes in the information system (Kavanagh & Rich, 2018, Chen et al, 2023). As a result, finding scalable solutions to detect and mitigate the impact of misinformation has proven challenging, although many efforts have demonstrated some promise (Conroy, et al., 2015; ,Aldwairi & Aldwairi, 2018, RAND, 2018). Part of the challenge facing researchers is that misinformation can be propagated through many modalities (e.g., text, image and video), thereby increasing the algorithmic complexity and computational resources necessary to deploy scalable solutions. Moreover, the sources from which misinformation is spread can effortlessly adapt to any mitigation attempts (e.g., account removal), which can quickly become a real-life version of the Whac-A-Mole game. In order for robust and effective solutions to be deployed, first we must understand the features of misinformation that are temporally invariant, since the content, sources and modalities associated with misinformation are likely to change across time. Although some researchers have explored temporal patterns of misinformation on social media (Allcott, et al, 2019), they focused on relatively narrow timelines that were less than five years in range. Therefore, this effort explores the trends associated with political misinformation over a twelve year period, leveraging tools from natural language processing to uncover common themes in misinformation across time. By uncovering these insights, it may allow researchers to develop more robust tools to detect and mitigate misinformation, and the next section will detail the data used to this end."
https://arxiv.org/html/2411.06111v1,Energy-efficient Hybrid Model Predictive Trajectory Planning for Autonomous Electric Vehicles,"To tackle the twin challenges of limited battery life and lengthy charging durations in electric vehicles (EVs), this paper introduces an Energy-efficient Hybrid Model Predictive Planner (EHMPP), which employs an energy-saving optimization strategy. EHMPP focuses on refining the design of the motion planner to be seamlessly integrated with the existing automatic driving algorithms, without additional hardware. It has been validated through simulation experiments on the Prescan, CarSim, and Matlab platforms, demonstrating that it can increase passive recovery energy by 11.74% and effectively track motor speed and acceleration at optimal power. To sum up, EHMPP not only aids in trajectory planning but also significantly boosts energy efficiency in autonomous EVs.","The study of electric vehicles (EVs) has garnered significant attention from both industry and academia, driven by growing concerns about environmental issues [1]. Autonomous electric vehicles are presented as a practical solution to these challenges [2]. However, a widespread adoption faces hurdles such as limited battery life and prolonged charging times [3]. To tackle these challenges, researchers are exploring diverse solutions, including innovations in autonomous driving strategies and optimization of route planning algorithms to maximize energy utilization [4] [5] [6] [7]. Yet, current studies often overlook the interplay among the kinetic energy recovery systems (KERS), engine efficiency, and external environment. These interactions are distinguishing features of hybrid and electric vehicles, which can additionally be incorporated into vehicle driving strategies to improve energy-efficiency[8]. In prior energy strategy research, driving strategies to improving energy-efficiency have been widely proposed[9], but it is difficult for drivers to manually control vehicles according to these proposed strategies. The emerge of autonomous driving provides a satisfying solution to the above problems, effective vehicles controlled by algorithms can be more accurately planned and controlled based on strategies. The conversion of kinetic energy into electrical energy involves reversing the vehicle’s electric engine into functioning as a generator and subsequently charging the battery with the generated current [10]. This conversion of the kinetic energy losses serves as a viable method for energy recovery, enhancing system performance, improving energy conversion efficiency, and extending the mileage [11][12][13]. In recent years, kinetic recovery has emerged as a key focus of interest among researchers, designers, and manufacturers in the EV industry [14]. As a portion of the energy that is utilized to propel the electric vehicle is dissipated as braking energy loss during driving, recovering a portion of these losses can enhance the efficiency of energy extraction from the battery and prolong the mileage. An alternative approach to regenerative braking for kinetic energy recovery is the conversion of potential energy losses, which can be implemented by not activating the braking system when the vehicle decelerates. In such instances, the kinetic energy recovery system kicks in to recover as much kinetic energy as possible from the car. Therefore, KERS in energy recovery strategy is an important factor that has to be considered for improving the overall efficiency of EV operation. In this paper,we propose an energy-efficient Hybrid Model Predictive Planner (EHMPP), optimized based on the best energy efficiency strategies. This planner takes into account the operational state of the KERS during the planning process for EVs, enhancing their range and energy efficiency. We conducted simulation experiments in Prescan, Cars, and Matlab to validate the effectiveness of our proposed strategy. The results indicate that implementing EHMMP significantly enhances the energy efficiency of autonomous electric vehicles. Our main contributions to this article are as follows: • EHMPP is an EVs planner that operates within the constraints of existing hardware configurations, eliminating the need for supplementary hardware deployment. • Our simulation results demonstrate that the proposed strategy significantly enhances the vehicle’s energy efficiency during operation. Specifically, it boosts passive energy recovery by 11.74% during deceleration phases. Moreover, the strategy optimizes motor operation, ensuring it remains close to its ideal power state throughout acceleration, deceleration, and cruising phases, thereby improving overall energy efficiency. • EHMPP enhances flexibility by automatically selecting distinct cost functions for different motion states, surpassing traditional methodologies. This approach not only facilitates an adaptive planning but also serves as a valuable reference for deploying additional strategies."
https://arxiv.org/html/2411.06106v2,Personalize to generalize: Towards a universal medical multi-modality generalization through personalization,"The differences among medical imaging modalities, driven by distinct underlying principles, pose significant challenges for generalization in multi-modal medical tasks. Beyond modality gaps, individual variations — such as differences in organ size and metabolic rate — further impede a model’s ability to generalize effectively across both modalities and diverse populations. Despite the importance of personalization, existing approaches to multi-modal generalization often neglect individual differences, focusing solely on common anatomical features. This limitation may result in weakened generalization in various medical tasks. In this paper, we unveil that personalization is critical for multi-modal generalization. Specifically, we propose an approach to achieve personalized generalization through approximating the underlying personalized invariant representation \mathbb{X}_{h} across various modalities by leveraging individual-level constraints and a learnable biological prior. We validate the feasibility and benefits of learning a personalized \mathbb{X}_{h}, showing that this representation is highly generalizable and transferable across various multi-modal medical tasks. Extensive experimental results consistently show that the additionally incorporated personalization significantly improves performance and generalization across diverse scenarios, confirming its effectiveness.","Figure 1: A diagram of medical modalities and individual differences. Individual variations may be significant and warrant further research attention from the medical intelligence community. Three-dimensional medical images, generated through specialized techniques and radiopharmaceuticals, excel at highlighting specific physiological features, collectively providing a comprehensive view of a patient’s structural and functional characteristics. However, this distinction between medical modalities creates significant generalization challenges in medical image analysis, especially when certain modalities may be inaccessible due to an individual’s financial constraints or physical limitations. As illustrated in Fig. 1, contemporary research in medical intelligence is mainly concentrated on structural modalities that depict physical anatomy, including Magnetic Resonance Imaging (MRI) [53] and Computed Tomography (CT) [33, 51] scans. Other studies [49] focus on the functional modalities associated with biochemistry, such as Positron Emission Tomography (PET) scans. For clarity, we categorize these modalities for generalization tasks into two types: homogeneous generalization, which pertains to generalizing within structural or functional modalities (e.g., T1, T2, T1ce, and Flair in MRI, as shown in Fig. 1); and heterogeneous generalization, which involves generalizing across both structural and functional modalities, such as CT and PET. An ideally well-generalized medical model across modalities should provide integrated insights derived from all modalities when only a subset is accessible for individuals, even for downstream transferring. Even though pre-training can significantly enhance downstream generalization, some recent approaches concentrate on learning common physical anatomy invariance at the class level [23], which may not be feasible for functional biochemistry-based models. Another line of research [40, 46, 23] primarily addresses the transferability of single-modal tasks and may not be suitable for multi-modal scenarios. In addition, most research on homogeneous generalization for medical tasks focuses on structural sequences of MRI or CT, employing strategies such as cross-modality transfer [30, 25, 51] or targeting challenges like missing modality segmentation [31, 6, 36, 37, 51]. Heterogeneous generalization presents a greater challenge due to the disparities between structural and functional information. Despite its significance, very few efforts [34] have addressed heterogeneous generalization, mainly focusing on one-directional modality transfer (e.g., PET to CT or MRI), and rarely exploring the model’s generalizability and transferability for other tasks in this context. Moreover, while medical modality differences have garnered attention from the research community, individual differences seem to have been overlooked by previous methods. As each person is fundamentally different from the average of the population [45], the differences between individuals across anatomy and metabolism are conspicuous (as shown in Fig. 1). Ignoring these individual differences may impede a model’s ability to generalize effectively across the broader population, which could be crucial for medical multi-modality generalization. This paper unveils that homogeneous and heterogeneous settings for multi-modality generalization can be tackled in the scope of personalization. To address this problem comprehensively, we formally introduce the concept of the personalized invariant representation for multi-modal generalization, denoted as \mathbb{X}_{h}, and its constraints as outlined in Hypothesis 3.1. Furthermore, personalized invariant \mathbb{X}_{h}, which learns aggregated biological information from all possible modalities specific to the individual, is likely to enhance performance across various medical tasks for that person. Building on this hypothesis, this paper proposes a general approach aimed at enhancing the generalization of various medical imaging tasks through personalization. Specifically, our method constructs an approximation of \mathbb{X}_{h} using the learnable biological prior knowledge \mathbb{O}, via decomposition, invariance, and equivariance constraints during pre-training (refer to Sec. 3.2). The learned approximation of \mathbb{X}_{h} can then be utilized to enhance performance in downstream generalization tasks, irrespective of whether a domain gap exists between the pre-training data and downstream data. Importantly, this paper demonstrates that obtaining a personalized invariant representation, \mathbb{X}_{h}, is feasible through our approach, and such invariance leads to generalization improvements across various medical tasks. To validate our methodology, we conduct experiments on modality transfer and missing modality segmentation tasks, addressing not only the homogeneous generalization of MRI but also the rarely explored heterogeneous generalization, such as generalization between PET and CT. Our findings reveal that our approach successfully captures comprehensive, personalized information even when only partial modalities are available for a given individual (see Fig. 3). Moreover, extensive experiments on both homogeneous (Sec. 4) and heterogeneous (Secs. 5 and 6) generalization demonstrate that our approach can be adapted for downstream tasks and surpasses current state-of-the-art (SOTA) methods in multiple tasks, validating its superiority. We will publicly release our code, checkpoints, and data splits upon acceptance."
https://arxiv.org/html/2411.06098v2,LT-DARTS: An Architectural Approach to Enhance Deep Long-Tailed Learning,"Deep long-tailed recognition has been widely studied to address the issue of imbalanced data distributions in real-world scenarios. However, there has been insufficient focus on the design of neural architectures, despite empirical evidence suggesting that architecture can significantly impact performance. In this paper, we attempt to mitigate long-tailed issues through architectural improvements. To simplify the design process, we utilize Differential Architecture Search (DARTS) to achieve this goal. Unfortunately, existing DARTS methods struggle to perform well in long-tailed scenarios. To tackle this challenge, we introduce Long-Tailed Differential Architecture Search (LT-DARTS). Specifically, we conduct extensive experiments to explore architectural components that demonstrate better performance on long-tailed data and propose a new search space based on our observations. This ensures that the architecture obtained through our search process incorporates superior components. Additionally, we propose replacing the learnable linear classifier with an Equiangular Tight Frame (ETF) classifier to further enhance our method. This classifier effectively alleviates the biased search process and prevents performance collapse. Extensive experimental evaluations demonstrate that our approach consistently improves upon existing methods from an orthogonal perspective and achieves state-of-the-art results with simple enhancements.","Deep neural networks have demonstrated remarkable performance in image recognition tasks, yet they are predominantly utilized in settings where the data adhere to a balanced distribution. In reality, data distributions in many practical scenarios are long-tailed (Buda, Maki, and Mazurowski 2017), characterized by a few head classes with abundant samples and many tail classes with limited samples. It has been proved that conventional deep learning methods tend to exhibit sub-optimal performance in such a context. To address this challenge, the community has proposed numerous approaches including data augmentation (Li et al. 2021; Hong et al. 2022; Ahn, Ko, and Yun 2023), class re-sampling strategies (Liu et al. 2021; Bai et al. 2023), decoupling learning (Kang et al. 2019; Zhou et al. 2023) and so on. Nevertheless, most existing methods pay insufficient attention to neural architecture design, which is crucial for deep models to achieve optimal performance. Figure 1: The performance of different architectures on the long-tailed CIFAR-10-LT dataset. The direction of black dashed lines indicates better architectures, as they achieve higher accuracy with fewer architectural parameters. It is widely recognized that meticulous architectural design contributes to improved performance on balanced data distribution, and this consensus unsurprisingly extends to long-tailed data distribution. As depicted in Fig. 1, we conduct extensive training on several architectures and report their parameter sizes and performance. It can be observed that different backbones lead to various performances and an optimal architecture may achieve superior performance even with fewer parameters, which validates the feasibility of approaching the long-tailed problem from an architectural perspective. In terms of efficiently designing an architecture suited for long-tailed data, we leverage the power of differentiable architecture search (DARTS). (a) DARTS with CE (vanilla) (b) DARTS with LDAM (c) \beta-DARTS with CE (d) \beta-DARTS with LDAM Figure 2: Running DARTS methods on CIFAR-10-LT, where the blue line denotes the process of proxy search, and the red line denotes the optimal architecture trained from scratch. Two baselines are depicted, where the green line corresponds to ResNet and the yellow line represents ResNeXt. (a)(c) Network parameters are updated based on the Cross-Entropy (CE) loss (i.e., \mathcal{L}_{train} = CE). (b)(d) \mathcal{L}_{train} = LDAM, which is a re-balancing loss function designed for long-tailed learning. DARTS (Liu, Simonyan, and Yang 2018) is an efficient neural architecture search method with minimal computational resource requirements. On balanced datasets, the architectures it discovers perform on par with those designed by experts. Unfortunately, as shown in Fig 2, directly applying DARTS on the long-tailed dataset fails to achieve satisfactory results. Even the latest improved version \beta-DARTS (Ye et al. 2022) underperforms compared to expert-designed networks, which deviation contrasts with the performance on balanced data distribution. Moreover, it is noteworthy that the integration of the re-balancing loss LDAM (Cao et al. 2019) only yields a marginal improvement, suggesting that the naive combination of DARTS and re-balancing strategy also fails to achieve satisfactory results. To address this issue, we propose Long-Tailed Differential ARchiTecture Search (LT-DARTS), a framework tailored for the architecture search tasks on long-tailed data. Specifically, we systematically conduct extensive experiments to evaluate the performance of various architectural components under long-tailed data, including topology, activation functions and their placements, convolution designs, and normalization methods. Drawing from the observations, we propose two novel convolution operations, thereby establishing a novel search space. Moreover, we further enhance our method from the search strategy. We introduce the Equiangular Tight Frame (ETF) classifier, designed to mitigate weight shifts in classifiers caused by long-tailed data, including both weight norm and weight angle shifts. Furthermore, we find that the ETF classifier also alleviates performance collapse, which we validate through theoretical analysis. Extensive experimental results indicate the seamless integration of our method with existing approaches consistently enhancing the performance. Through only simple enhancements, our method achieves state-of-the-art results. Our contributions can be concluded as the following: • We examine the long-tailed issue from an architectural perspective, which is paid little attention to. With this foundation, we methodically investigate architectural components that demonstrate enhanced performance in long-tailed scenarios, leading to insightful observations. Leveraging these insights, we propose two innovative convolution operations and establish a fresh search space. • To further strengthen our approach from the search strategy, we incorporate the ETF classifier. Specifically, we observe a bias in the learnable classifier weights during the search process, which the ETF classifier inherently resolves. Additionally, we show that the ETF classifier also helps prevent performance collapse. • Extensive experiments show that the architectures we discover outperform crafted designed architectures. And it can seamlessly integrate with existing solutions tailored to long-tailed issues, further enhancing model performance. Our research provides a complementary perspective for the long-tailed community."
https://arxiv.org/html/2411.06084v1,Optimizing Large Language Models through Quantization: A Comparative Analysis of PTQ and QAT Techniques,"This paper presents a comprehensive analysis of quantization techniques for optimizing Large Language Models (LLMs), specifically focusing on Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). Through empirical evaluation across models ranging from 10M to 1B parameters, we demonstrate that quantization can achieve up to 68% reduction in model size while maintaining performance within 6% of full-precision baselines when utilizing our proposed scaling factor \gamma. Our experiments show that INT8 quantization delivers a 40% reduction in computational cost and power consumption, while INT4 quantization further improves these metrics by 60%. We introduce a novel theoretical framework for mixed-precision quantization, deriving optimal bit allocation strategies based on layer sensitivity and weight variance. Hardware efficiency evaluations on edge devices reveal that our quantization approach enables up to 2.4x throughput improvement for INT8 and 3x for INT4, with 60% power reduction compared to full-precision models.","I-A Background and Motivation The emergence of Large Language Models (LLMs) has revolutionized the field of natural language processing (NLP), enabling significant advancements in tasks such as machine translation, sentiment analysis, question answering, and conversational agents. Models like GPT-3, with 175 billion parameters, and PaLM, boasting 540 billion parameters, have demonstrated unprecedented capabilities in understanding and generating human-like text. These models leverage vast amounts of data and intricate architectures to achieve high performance, often surpassing previous benchmarks and setting new standards in the industry. However, the impressive capabilities of LLMs come at a substantial computational and financial cost. Training such models requires extensive computational resources, including powerful GPUs or TPUs, vast memory, and significant energy consumption. Moreover, the inference phase—where the model is deployed to perform tasks—demands considerable computational power and memory bandwidth, which can limit the feasibility of deploying LLMs on devices with constrained resources [1], such as mobile phones, Internet of Things (IoT) devices, and edge computing platforms. This limitation poses a significant barrier to the widespread adoption and accessibility of LLMs, particularly in applications where low latency and high efficiency are critical. Quantization in neural networks offers a promising solution to these challenges by reducing the precision of model parameters and activations. Typically, neural network weights and activations are represented using 32-bit floating-point (FP32) numbers, which provide high precision but consume substantial memory and computational resources. Quantization techniques convert these high-precision values to lower-bit representations, such as 8-bit integers (INT8), 4-bit integers (INT4), or even binary representations [2]. This reduction not only decreases the memory footprint of the model but also accelerates computations by leveraging hardware optimizations designed for lower precision arithmetic. In the context of LLMs, which often consist of billions of parameters, quantization becomes particularly beneficial. By reducing the model size, quantization facilitates the deployment of LLMs on devices with limited computational capabilities and power budgets. Additionally, lower-precision computations can significantly speed up inference times, enabling real-time applications and reducing operational costs. Despite these advantages, quantization presents challenges, including potential degradation in model accuracy and the complexity of implementing effective quantization strategies. This paper aims to provide a comprehensive review of quantization techniques applied to LLMs, exploring their methodologies, benefits, challenges, and future directions."
https://arxiv.org/html/2411.06070v1,"GFT: Graph Foundation Model with 
Transferable Tree Vocabulary","Inspired by the success of foundation models in applications such as ChatGPT, as graph data has been ubiquitous, one can envision the far-reaching impacts that can be brought by Graph Foundation Models (GFMs) with broader applications in the areas such as scientific research, social network analysis, drug discovery, and e-commerce. Despite the significant progress of pre-trained graph neural networks, there haven’t been GFMs that can achieve desired performance on various graph-learning-related tasks. Building GFMs may rely on a vocabulary that encodes transferable patterns shared among different tasks and domains. Unlike image and text, defining such transferable patterns for graphs remains an open question. In this paper, we aim to bridge this gap by rethinking the transferable patterns on graphs as computation trees – i.e., tree structures derived from the message-passing process. Based on this insight, we propose a cross-task, cross-domain graph foundation model named GFT, short for Graph Foundation model with transferable Tree vocabulary. By treating computation trees as tokens within the transferable vocabulary, GFT improves model generalization and reduces the risk of negative transfer. The theoretical analyses and extensive experimental studies have demonstrated the transferability of computation trees and shown the effectiveness of GFT across diverse tasks and domains in graph learning. The open source code and data are available at https://github.com/Zehong-Wang/GFT.","Foundation models such as Large Language Models (LLMs) and Large Vision Models (LVMs) keep reshaping our view of the world [7, 100, 51, 112, 50]. These models are designed to be general-purpose, adaptable across various tasks and domains through fine-tuning or prompting, such as GPT-4 [1] in Natural Language Processing (NLP) and Sora [46] in Computer Vision (CV). Research attributes the success of foundation models to the uniformity of tasks and a general vocabulary that defines basic transferable patterns across tasks [98, 76, 112, 3, 50]. For example, LLMs [1, 112] treat language tasks as question-answering or next-word prediction and deconstruct sentences using a word vocabulary. Similarly, LVMs [100, 98, 3] reformulate image tasks as image question-answering, converting images into discrete tokens using a vision vocabulary. Inspired by the success of LLMs and LVMs, as graph-structured data (e.g., citation networks, social networks, computer networks, molecular structures, and recommender systems) have become ubiquitous, one can envision the far-reaching real-world impacts that can be brought by pre-trained Graph Foundation Models (GFMs). Although there has been significant progress of pre-trained Graph Neural Networks (GNNs), there haven’t been GFMs that can achieve desired performance on a wide range of graph-learning-related tasks. Unlike CV and NLP, as graphs represent complex, non-Euclidean relationships among entities [92, 48, 104, 58, 107], a grand challenge of building GFMs lies in identifying transferable patterns across graphs [50, 93, 25]. There have been extensive studies aiming to tackle this challenges, which can mainly be categorized into two groups: (1) Utilizing graphon theory: Ruiz et al. [62] provide theoretical evidence of transferability between two graphs generated from the same graphon. Cao et al. [8] further extend these findings by both empirically and theoretically analyzing graph transferability in pre-training and fine-tuning scenarios. Despite these theoretical guarantees, the stringent assumptions of graphon theory often prove difficult to satisfy in real-world, cross-domain datasets [42], thus limiting its applicability in defining transferable graph vocabularies. (2) Exploring graph transferability using subgraph structures [114, 59, 50]: Zhu et al. [114] demonstrate that the transferability between graphs is linked to the ego-graph patterns of individual nodes and establish a stability bound using the graph Laplacian. They suggest that localized subgraphs could serve as transferable patterns within graph vocabularies. Building on this finding, Sun et al. [68] develop a GFM by reformulating graph tasks as subgraph classification, enabling a single model to be applied to diverse tasks. Huang et al. [30], Liu et al. [45] expand GFMs to cross-domain scenarios by unifying the node feature space across different graphs through LLMs [60, 76]. Despite these successes, the process of explicit subgraph extraction remains time and memory intensive [30]. More importantly, numerous studies such as [20, 10, 53, 103] show that message-passing GNNs [40, 24, 21] fail to detect critical substructures or motifs within subgraphs, reducing the feasibility of using subgraphs to define graph vocabularies. How to identify a vocabulary that can encode transferable patterns shared among different tasks and domains for the construction of GFMs? In this paper, we aim to address the limitations of existing works by answering this question. Specifically, based on message-passing mechanism of GNNs, we have observed that the learned embeddings of each node can be essentially captured in the form of its computation tree. Based on this insight, we bridge the research gap by rethinking the transferable patterns on graphs as computation trees – i.e., subtree structures derived from the message-passing process. Using computation tree as a transferable pattern across graphs will bring three primary advantages: (1) Efficiency: As the extraction and encoding of computation trees are integrated within a single message-passing GNN process [20], it eliminates the need for the explicit subgraph extraction for GFMs [30, 45]. (2) Expressiveness: Since computation trees are capable of capturing localized patterns [52], it’s able to represent a graph as a multiset of computation trees [23]. (3) Learnability: As the information of computation trees is completely captured by message-passing GNNs, it can tackle the issue that certain motifs within subgraphs remain elusive. We theoretically investigate the transferability of computation trees and empirically demonstrate a strong correlation between computation tree similarity and transfer learning performance across various graphs. Based on the key idea above, by treating computation trees as graph vocabulary tokens, we develop a cross-task, cross-domain graph foundation model – namely GFT – short for Graph Foundation model with transferable Tree vocabulary. GFT consists of pre-training and fine-tuning phases, enabling it to handle datasets across different tasks and domains effectively. During pre-training, we introduce a computation tree reconstruction task to acquire generalized knowledge from cross-domain graphs. We obtain a discrete tree vocabulary of prototypical tree tokens by quantizing the embedding space of computation trees, which theoretically improves model generalization. In the fine-tuning phase, we utilize this learned tree vocabulary to unify various graph-related tasks into computation tree classification, thereby preventing negative transfer [89, 87]. Extensive experimental results demonstrate the effectiveness of GFT in graph learning on cross-task and cross-domain datasets."
https://arxiv.org/html/2411.06060v1,Wild Narratives: Exploring the Effects of Animal Chatbots on Empathy and Positive Attitudes toward Animals,"Rises in the number of animal abuse cases are reported around the world. While chatbots have been effective in influencing their users’ perceptions and behaviors, little if any research has hitherto explored the design of chatbots that embody animal identities for the purpose of eliciting empathy toward animals. We therefore conducted a mixed-methods experiment to investigate how specific design cues in such chatbots can shape their users’ perceptions of both the chatbots’ identities and the type of animal they represent. Our findings indicate that such chatbots can significantly increase empathy, improve attitudes, and promote prosocial behavioral intentions toward animals, particularly when they incorporate emotional verbal expressions and authentic details of such animals’ lives. These results expand our understanding of chatbots with non-human identities and highlight their potential for use in conservation initiatives, suggesting a promising avenue whereby technology could foster a more informed and empathetic society.","Many countries have recently reported rises in animal abuse (Maisner, 2023; Cheng, 2024; Harris, 2023). Various factors have been identified as contributing to this troubling trend, including a lack of awareness that animals can suffer pain (Agnew, 1998). Another factor is absence of empathy for animals, as individuals who do not recognize them as sentient are less likely to treat them humanely (Miller, 2001; Agnew, 1998). In addition, the impossibility of communicating with animals verbally leaves them particularly susceptible to misrepresentation and misunderstanding, further increasing their vulnerability to abuse (Thomsen et al., 2023). Researchers have explored various approaches to promoting people’s empathy and positive attitudes toward animals. These range from live animal encounters in educational settings like zoos (Małecki et al., 2019) to first-person narratives purportedly written by animals (Akerman, 2019). A recurring theme across these strategies is the attempt to provide animals with a voice, as a means of evoking empathy and thus helping to shift public perception in favor of better treatment of animals (Young et al., 2018; Demello, 2018; Małecki et al., 2019). Additionally, some efforts to improve human empathy with animals have involved showing animals using digital tools such as iPads (Webber et al., 2017b). The results suggest that seeing animals as human-like indeed fosters empathy by heightening people’s awareness that animals also have thoughts and feelings (Webber et al., 2017a). However, some research has raised concerns that the use of technology in animals is unnatural or increases animal stress (Diana et al., 2021; Webber et al., 2017a). Extensive research has focused on the ability of conversational agents (also called chatbots) to shape people’s perceptions and engagement through a variety of narrative techniques (Bickmore et al., 2009; Park et al., 2023). To enhance chatbots’ facilitation of prosocial attitudinal and behavioral changes (Park et al., 2023), as well as their ability to provide mental-health support (Bickmore et al., 2009; de Gennaro et al., 2020; Lee et al., 2019), scholars have explored methods of humanizing them: for instance, assigning them names and/or portraits, and designing them to exhibit emotions (Shi et al., 2020; Liu and Sundar, 2018; Seeger et al., 2018). Humanizing chatbots may amplify the positive effects of other aspects of their design. For example, previous research has indicated that chatbots’ employment of first-person narratives can, in addition to increasing empathy and fostering attitudinal changes toward the characters represented in such narratives (Bickmore et al., 2009), increase user enjoyment during interactions (Lee et al., 2023). In short, chatbots have demonstrated their ability to influence users’ perceptions across various domains (Park et al., 2023; de Gennaro et al., 2020; Lee et al., 2019), and research to date suggests that a narrative from an animal’s first-person viewpoint could be effective in promoting empathy and changing human attitudes toward the animal, or toward animals in general (Beierl, 2008; James, 2019; Keen, 2006). Additionally, we suggest that chatbots could serve as a useful means of simulating human-animal interactions without jeopardizing animal welfare. Nevertheless, research on whether and how chatbots could be designed to represent animals, and how such representation might influence users’ empathy and attitudes toward animals, has hitherto been rare to nonexistent. To address this absence, we designed chatbots that speak from the perspective of an animal. Inspired by prior work on chatbots’ humanized design cues (e.g., identity, non-verbal and verbal cues) (Seeger et al., 2018; Shi et al., 2020; Liu and Sundar, 2018), we were interested in whether and how such cues could be redesigned to represent animals, and how this would affect user perceptions of chatbots’ identities. We therefore conducted a mixed-methods experiment with 240 participants, each of whom was randomly assigned to one of eight conditions, i.e., interaction with a chatbot that had zero, one, two, or all of the three above-mentioned cue types. All groups then engaged in interactive dialogue with their respective chatbots. We evaluated the participants’ empathy, attitudes, and intention to engage in prosocial behavior toward the animals portrayed in these narratives. Our results showed that a chatbot’s utilization of verbal cues enhanced participants’ empathy, whereas the use of non-verbal ones resulted in reduced empathy. Notably, first-person narratives by the chatbots succeeded in boosting participants’ perception of the chatbot’s identity as animal-like, and this led to more positive attitudes and increased prosocial behavioral intentions. This pioneering study makes the following contributions to the human-computer interaction community. First, it extends exploration of human-like chatbots into the domain of animal-like ones, and thus opens new pathways to understanding how non-human personas can be effectively utilized. Second, it demonstrates the ability of chatbots to influence their users’ perceptions of non-human entities. Specifically, we found that chatbots embodying an ‘animal perspective’ could significantly improve user empathy, attitudes, and prosocial behavioral intentions toward animals, particularly when they employed emotional expressions. Finally, this study provides a unique perspective on how human-chatbot interactions can be designed to have positive social impacts. These insights could pave the way for innovative applications in education and conservation, underscoring the potential of chatbot technology to contribute to more empathetic and better-informed societies."
https://arxiv.org/html/2411.06048v1,"An Empirical Analysis on Spatial Reasoning Capabilities of 
Large Multimodal Models","Large Multimodal Models (LMMs) have achieved strong performance across a range of vision and language tasks. However, their spatial reasoning capabilities are under-investigated. In this paper, we construct a novel VQA dataset, Spatial-MM, to comprehensively study LMMs’ spatial understanding and reasoning capabilities. Our analyses on object-relationship and multi-hop reasoning reveal several important findings. Firstly, bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs’ spatial reasoning. Secondly, LMMs struggle more with questions posed from the human perspective than the camera perspective about the image. Thirdly, chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations. Lastly, our perturbation analysis on GQA-spatial reveals that LMMs are much stronger at basic object detection than complex spatial reasoning. We believe our benchmark dataset and in-depth analyses can spark further research on LMMs spatial reasoning. 111Spatial-MM benchmark is available at: https://github.com/FatemehShiri/Spatial-MM","Large Multimodal Models (LMMs) have shown impressive generalization ability on several vision and language tasks. Several recent works, however, showed that these models lack spatial understanding Tong et al. (2024); Li et al. (2023a); Liu et al. (2023e); Lei et al. (2024a); Prasad et al. (2023). As can be seen in Figure 1, multimodal LLMs, including GPT-4o, often fail to answer questions from a human perspective within an image. The focus of this work is to study the understanding of spatial relations by top-performing LMMs. Moreover, we go beyond evaluating only the final answers to directly analyzing the intermediate reasoning steps generated by chain of thought (CoT) prompting in multi-hop visual question-answering (VQA) tasks. We ground LMMs ’ reasoning steps into a scene graph format and verify whether they form a valid path. More specifically, we ask the following questions: (i) What spatial relations are missed by models, and why it happen? (ii) How can additional symbolic visual information, such as bounding boxes or scene graphs, improve the performance of LMMs? Which of these symbolic information is more useful, and how can they be integrated in the reasoning process effectively? (iii) How does the questions complexity affect LMMs in handling spatial relations? (iv) How does the reasoning path of LMMs behave when they fail to answer a multi-hop question? Is the failure due to incorrect spatial reasoning or non-spatial reasoning? Figure 1: benchmarking the spatial reasoning capabilities of GPT-4o Achiam et al. (2023) (Date accessed: June 12, 2024). Text in red and green signifies an incorrect and ground-truth answers, respectively. The accuracy of GPT-4o in answering questions related to the human’s viewpoint in the image is only 27.5%. To address these questions, we construct Spatial-MM, a novel, challenging dataset, and comprehensively LMMs spatial reasoning capabilities from different angles. We analyze four top-performing LMMs on Spatial-MM and GQA-spatial Kamath et al. (2023) benchmarks to identify problems with visual question answering (VQA) evaluation methodology. Our comprehensive analyses reveal a number of important insights that point to future research directions. Our contributions can be summarized as follows. \bullet We present a new, challenging spatial-aware benchmark that incorporates a variety of spatial relationship types, accounting for both human and camera perspectives. \bullet Our coprehensive empirical analyses show that: (i) bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs’ spatial reasoning, (ii) LMMs struggle more with questions posed from the human perspective than the camera perspective about the image, (iii) chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations, and (iv) LMMs are much stronger at basic object detection than complex spatial reasoning."
https://arxiv.org/html/2411.06041v1,PointCG: Self-supervised Point Cloud Learning via Joint Completion and Generation,"The core of self-supervised point cloud learning lies in setting up appropriate pretext tasks, to construct a pre-training framework that enables the encoder to perceive 3D objects effectively. In this paper, we integrate two prevalent methods, masked point modeling (MPM) and 3D-to-2D generation, as pretext tasks within a pre-training framework. We leverage the spatial awareness and precise supervision offered by these two methods to address their respective limitations: ambiguous supervision signals and insensitivity to geometric information. Specifically, the proposed framework, abbreviated as PointCG, consists of a Hidden Point Completion (HPC) module and an Arbitrary-view Image Generation (AIG) module. We first capture visible points from arbitrary views as inputs by removing hidden points. Then, HPC extracts representations of the inputs with an encoder and completes the entire shape with a decoder, while AIG is used to generate rendered images based on the visible points’ representations. Extensive experiments demonstrate the superiority of the proposed method over the baselines in various downstream tasks. Our code will be made available upon acceptance.","Self-supervised representation learning (SSRL) aims to fully exploit the statistical and structural knowledge inherent in unlabeled datasets, enabling the encoder of the pre-training model to extract informative and discriminative representations. The pre-trained encoder can be subsequently applied to various downstream tasks such as classification, segmentation, and object detection [1, 2]. The core of SSRL lies in the design of appropriate pretext tasks aimed at aiding the encoder in achieving a full perception and understanding of the inputs. Figure 1: Qualitative and quantitative comparison of models using different pretext tasks. Chamfer Distance (CD) and Structural Similarity Index (SSIM) are employed as the quantitative metrics. For the masked point modeling (MPM) task, we utilize the method proposed in Point-MAE [3] with the inputs of visible points from arbitrary views (see Sec. 3.1). For the 3D-to-2D generation task, we define the pretext task as generating images from arbitrary views. The result of the model using only MPM exhibits group clustering at the edges, while our method yields sharpened and clear edges that closely align with the ground truth. The model relying solely on 3D-to-2D generation fails to capture three-dimensional structural information, while our method can effectively preserve the geometric structure. Directly combining both tasks generates point clouds and images superior to using only MPM or 3D-to-2D generation (Direct Combination) but with lower Linear-SVM accuracy. Based on the tasks employed, existing self-supervised pre-training methods can be broadly classified into two paradigms: contrastive learning and generative learning, both of which have attained great success in processing 2D images [4, 5, 6] and 3D point clouds [7, 3, 8, 9, 10, 11]. Compared to contrastive learning, generative learning is considered as a more data-efficient pre-training method, capable of capturing the patterns of the inputs with relatively limited data volume [12]. Therefore, it is highly favored in the context of data scarcity within the field of 3D vision, where masked point modeling [7, 3, 8, 13, 10] and 3D-to-2D generation [11, 14] stand out as two representative generative learning methods. Among them, masked point modeling drives the model to predict arbitrary missing parts based on the remaining points. Accomplishing this task requires a thorough understanding of the spatial properties and global-local context of point clouds. 3D-to-2D generation employs a cross-modal pretext task which translates a 3D object point cloud to its diverse forms of 2D rendered images (e.g., silhouette, depth, contour). Pre-training with pixel-wise precise supervision drives the backbone to perceive the fine-grained edge details of 3D objects. However, both of the above methods have their own limitations. As revealed in [15, 16, 17], due to the irregularity of point clouds, commonly used point set similarity metrics (e.g., Chamfer Distance and Earth Mover’s Distance) in masked point modeling cannot provide explicit point-to-point supervision between ground truth and generated point clouds. The lack of precise correspondence results in limited feature representation capability of the pre-trained backbone network. Conversely, 3D-to-2D generation [11, 14] alleviates the issue of insufficient supervision signals by utilizing regular 2D images as the generation objective, offering pixel-wise precise supervision. However, relying solely on images from limited views as ground truth may overlook the structural information from occluded point sets, diminishing the backbone’s perception of the spatial properties of point clouds. As shown in Fig. 1, masked point modeling exhibits subpar performance in reconstructing some challenging areas (e.g., edges) due to the lack of point-to-point supervision. Besides, 3D-to-2D generation yields images lacking three-dimensional structural information, attributed to the lack of explicit geometric guidance. These observations collectively indicate the models’ inadequate perception of the inputs, consequently reducing their performance on downstream tasks. Based on the aforementioned analysis, an intuitive method is to combine these two pretext tasks to retain their individual merits while compensating for their respective limitations. However, as shown in Fig. 1, while the model directly combining both tasks outperforms those relying solely on MPM or 3D-to-2D generation in generating high-quality point clouds or images, its Linear-SVM accuracy is lower (88.41\% vs 90.72\% and 91.13\%). We argue that the encoder’s involvement in both tasks can lead to confusion when generating content for two modalities concurrently. Furthermore, to accomplish both tasks, the model shifts its training focus toward the decoder, which is typically discarded after pre-training. This phenomenon diminishes the feature extraction capability of the encoder, ultimately reducing the Linear SVM accuracy. Figure 2: Visualization of the unmasked points (a), the masked points (b), the completed point cloud composed of green unmasked points and gray masked points (c), and the completed point cloud in blue with overlapping points highlighted in red (d). To address these issues, we propose PointCG, a framework that effectively integrates masked point modeling and 3D-to-2D generation tasks. This framework incorporates a Hidden Point Completion (HPC) module and an Arbitrary-view Image Generation (AIG) module. Existing MAE-based MPM methods often employ a random masking strategy based on Farthest Point Sampling (FPS) and K-Nearest Neighbor (KNN) techniques. However, the inputs of unmasked patches (Fig. 2 (a)) preserve the overall shape of an object and exhibit substantial overlap with the target points (highlighted in red in Fig. 2 (d)). The leakage of overall structure and point location information enables the model to reconstruct the object without a holistic comprehension of the entire structure, which limits the learning capacity of the encoder during pre-training. To overcome this limitation, we select the visible points from arbitrary views by removing hidden points as input and introduce the HPC module to complete the point clouds. For the 3D-to-2D generation task, we employ the arbitrary-view image generation as the pretext task, which generates the image from an arbitrary view based on the representations of visible points extracted by the encoder. Furthermore, the cross-modal feature alignment is introduced to align the feature spaces of point clouds and images, which enables simultaneous content generation across both modalities and refocuses the training on the encoder. Specifically, we extract features from both the input point clouds and their corresponding rendered 2D images, encouraging feature proximity for the same instance while maintaining feature separation for different instances. Through the effective integration of HPC and AIG, the pre-trained encoder achieves a comprehensive understanding of 3D objects and can extract high-quality 3D representations. We evaluate our model and the proposed modules with a variety of downstream tasks and ablation studies. We further demonstrate that informative representations can be effectively learned from the restricted points, and such representations facilitate effortless masked point modeling and arbitrary-view image generation."
https://arxiv.org/html/2411.06040v1,CGLearn: Consistent Gradient-Based Learning for Out-of-Distribution Generalization,"Improving generalization and achieving highly predictive, robust machine learning models necessitates learning the underlying causal structure of the variables of interest. A prominent and effective method for this is learning invariant predictors across multiple environments. In this work, we introduce a simple yet powerful approach, CGLearn, which relies on the agreement of gradients across various environments. This agreement serves as a powerful indication of reliable features, while disagreement suggests less reliability due to potential differences in underlying causal mechanisms. Our proposed method demonstrates superior performance compared to state-of-the-art methods in both linear and nonlinear settings across various regression and classification tasks. CGLearn shows robust applicability even in the absence of separate environments by exploiting invariance across different subsamples of observational data. Comprehensive experiments on both synthetic and real-world datasets highlight its effectiveness in diverse scenarios. Our findings underscore the importance of leveraging gradient agreement for learning causal invariance, providing a significant step forward in the field of robust machine learning. The source code of the linear and nonlinear implementation of CGLearn is open-source and available at: https://github.com/hasanjawad001/CGLearn.","Machine learning models have achieved remarkable success in various domains driven by the recent availability of large datasets, sophisticated algorithms, and highly advanced complex models. However, these models perform well only when the test data follows the same distribution as the training data (i.i.d.), but they often suffer from overfitting due to overparametrization, learning spurious correlations from training data (Sagawa et al. 2020; Wang et al. 2021; Ming, Yin, and Li 2022). This issue arises because traditional models focus on predictive power without considering the causal relationships underlying the data. As a result, when the training and test distributions differ, models that rely on spurious correlations can perform very poorly, compromising their robustness, leading to poor generalization on out-of-distribution (OOD) test data (Arjovsky et al. 2019; He, Shen, and Cui 2021). Figure 1: Illustration of three environments generated by intervening on the variable e, which takes distinct values e=0.2, e=2, and e=5 in environments e_{1}, e_{2}, and e_{3}, respectively. In each environment, X_{1} acts as a causal factor for the target variable Y, while X_{2} is a spurious (non-causal) factor with respect to Y. This figure exemplifies how different interventions on e create distinct environments. Learning causal relationships is the key to model explainability and enhancing generalization and robustness (Shin 2021; Wang et al. 2022; Santillan 2023). Although the ideal method for learning causal structures is through Randomized Control Trials (RCTs), these are often expensive, unethical, or impractical. Various methods have been developed for causal discovery. Constraint-based methods use conditional independence tests to identify causal directions (Spirtes, Glymour, and Scheines 2001; Pearl 2009; Colombo et al. 2012). This however often results in the Markov Equivalence Class (MEC) of causal structures. Score-based methods optimize causal graphs over Directed Acyclic Graphs (DAGs) (Chickering 2002; Ramsey et al. 2017; Huang et al. 2018), but the combinatorial nature of the search space can make it computationally expensive. Advances like NOTEARS (Zheng et al. 2018) transform this combinatorial challenge into continuous optimization, leading to various effective variants (Zheng et al. 2020; Yu et al. 2019; Lachapelle et al. 2019; Wei, Gao, and Yu 2020; Ng, Ghassami, and Zhang 2020; Ng et al. 2022). However, learning causal structures purely from observational data can be challenging due to issues like selection bias, measurement errors, and confounding factors (Zadrozny 2004; Torralba and Efros 2011). Moreover, relying solely on empirical risk optimization can result in models highly dependent on spurious relationships. To tackle this problem, researchers often use prior domain knowledge to improve causal discovery (O’Donnell et al. 2006; Gencoglu and Gruber 2020; Andrews, Spirtes, and Cooper 2020; Liu, Chen, and Zhao 2021; Chowdhury., Rashid., and Terejanu. 2023; Chowdhury and Terejanu 2023). Unfortunately, many causal discovery methods depend on specific assumptions (e.g., linearity, non-Gaussian noise) that do not always hold in real-world data. In addition to that some of these methods exploit variance scales e.g. var-sortability to identify causal orderings, performing well on unstandardized data but poorly after standardization (Reisach, Seiler, and Weichwald 2021; Kaiser and Sipos 2022; Reisach et al. 2024; Ormaniec et al. 2024). A recent line of study focuses on exploiting the invariance property of causal relationships across different environments. Methods like Invariant Causal Prediction (ICP) (Peters, Bühlmann, and Meinshausen 2016) aim to identify causal predictors by ensuring the conditional distribution of the target given these predictors remains stable across environments. This method leverages the invariance of causal relationships under different interventions, iterating over feature subsets to find those invariant across environments, considering them as potential causal parents of the target variable. Another study, IRM (Arjovsky et al. 2019) optimizes a penalty function to achieve OOD generalization for predictive models, ensuring robust performance across environments. These methods significantly reduce the absorption of spurious correlations by focusing on stable and invariant relationships. The invariant learning framework provides a promising approach to improve model robustness and generalization in the presence of distribution shifts, with various domains exploiting invariance to learn better predictors and robust models (Montavon et al. 2012; Wang et al. 2017; Chowdhury et al. 2024; Bose and Roy 2024). Some relevant works such as AND-mask (Parascandolo et al. 2020), Fishr (Rame, Dancette, and Cord 2022), Fish (Shi et al. 2021), IGA (Koyama and Yamaguchi 2020) use environment-specific gradients to improve generalization in diverse settings. Moreover, approaches examining the signal-to-noise ratio (GSNR) in gradients, such as the work by Liu et al. (Liu et al. 2020), measure the alignment of gradient directions across samples, while a similar strategy has been employed in large-batch training scenarios to improve model stability (Jiang et al. 2023). Motivated by this line of work and the current drawbacks of existing methods in structure learning and OOD generalization, we introduce CGLearn, a general framework designed to improve the generalization of machine learning models by leveraging gradient consistency across different environments. CGLearn does not require extensive domain knowledge or assumptions over data linearity or noise, making it a versatile and practical approach for learning robust predictive models. By focusing on feature invariance, emphasizing on reliable features, and reducing dependence on spurious correlations, CGLearn enhances the reliability and robustness of the models. The main contributions of this study are stated as follows: • We propose a novel general framework, CGLearn, which improves consistency in learning robust predictors by focusing on features that show consistent behavior across environments. • We provide both linear and nonlinear implementations of CGLearn, demonstrating its versatility and applicability across different model architectures. • We demonstrate that CGLearn achieves superior predictive power and generalization, even without multiple environments, unlike most state-of-the-art methods in this arena that require diverse environments for effective generalization. • Our empirical evaluations on synthetic and real-world datasets, covering both linear and nonlinear settings, as well as regression and classification tasks, validate the effectiveness and robustness of the proposed method. The remainder of this paper is organized as follows: First, we delve into the methodology of CGLearn, detailing its linear and nonlinear implementations. Next, we present our experimental settings and evaluations. Finally, we encapsulate our conclusions, highlight the significant takeaways, and discuss future directions."
https://arxiv.org/html/2411.06018v1,"A Picture is Worth A Thousand Numbers:
Enabling LLMs Reason about Time Series via Visualization","Large language models (LLMs), with demonstrated reasoning abilities across multiple domains, are largely underexplored for time-series reasoning (TsR), which is ubiquitous in the real world. In this work, we propose TimerBed, the first comprehensive testbed for evaluating LLMs’ TsR performance. Specifically, TimerBed includes stratified reasoning patterns with real-world tasks, comprehensive combinations of LLMs and reasoning strategies, and various supervised models as comparison anchors. We perform extensive experiments with TimerBed, test multiple current beliefs, and verify the initial failures of LLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and performance degradation of few shot in-context learning (ICL). Further, we identify one possible root cause: the numerical modeling of data. To address this, we propose a prompt-based solution VL-Time, using visualization-modeled data and language-guided reasoning. Experimental results demonstrate that VL-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL reasoners for time series, achieving about 140% average performance improvement and 99% average token costs reduction111https://github.com/haoxin1998/TimberBed-1.","With the recent surge in large language models (LLMs), their reasoning abilities have exhibited powerful performance with unique interpretability across multiple domains, including logic, mathematics, and symbolic reasoning Wei et al. (2022); Kojima et al. (2022); Achiam et al. (2023); Dubey et al. (2024); Yang et al. (2024). As a type of data widely present in the real world, time series records the dynamic evolution of data over time. Time-series reasoning (TsR) Merrill et al. (2024) has extensive real-world applications, such as pathological diagnosis Dingwell and Cusumano (2000), marine biological monitoring Baumgartner and Mussoline (2011), and human activity recognition Yang et al. (2015). Therefore, in this paper, we aim to explore the question: ""Can LLMs handle time-series reasoning? If not, how to enable them?"" However, this question remains largely underexplored. The first key obstacle is ""How to effectively evaluate LLMs’ TsR abilities?"". Existing works face limitations at each component: (1) Task and Data. (a) Unstructured task design: Existing TsR evaluations (Merrill et al., 2024; Chow et al., 2024) simply mix multiple tasks without any taxonomy for reasoning patterns, making it challenging to understand LLMs’ specific TsR limitations. (b) Synthetic data. Due to the challenges of curating suitable datasets, existing TsR works (Merrill et al., 2024) use synthetic data, failing to effectively reflect real-world validity. (2) LLMs and Reasoning Strategies: (a) Outdated LLMs: Evaluations (Merrill et al., 2024), still using Llama of March 2023, have limited reference value for current research due to the rapid development of LLMs. (b) Only Zero Shot: Current TsR evaluations (Merrill et al., 2024; Chow et al., 2024) only adopt the zero-shot setting to assess LLMs for TsR, which fails to reveal their actual effectiveness. (3) Comparison Anchors: Existing TsR works either lack comparison anchors (Chow et al., 2024), only comparing different LLMs, thus failing to quantify success, or adopt human-level performance as a reference (Merrill et al., 2024), which is costly and difficult to scale to other datasets. To address these issues, we propose TimerBed, the first comprehensive and progressive testBed for evaluating LLMs reasoning about Time series. Using TimerBed, we conduct extensive experiments to assess LLMs’ TsR effectiveness, test several current beliefs about LLMs, and discuss possible causes of their failures in TsR. Inspired by these insights, we innovatively propose a prompt-based solution called VL-Time, which uses Visualization to model data and Language to guide reasoning, thereby unlocking multimodal LLMs’ abilities for Time series reasoning. Main contributions are summarized as : • We propose TimerBed, the first comprehensive testbed for evaluating LLMs for TsR. TimerBed introduces stratified reasoning patterns, i.e., simple deterministic reasoning, complex deterministic reasoning and probabilistic reasoning, with curated real-world tasks. TimerBed also includes comprehensive combinations of advanced LLMs and reasoning strategies, and diverse supervised models as comparison anchors. • We conduct extensive evaluations with TimerBed to provide insights and identify a root cause of LLMs’ initial failures in TsR: the directly numerical modeling of data. Specifically, numerical modeling results in the difficulty of feature extraction and excessive length of context, which in turn lead to ineffectiveness of zero-shot (ZST) reasoning and performance degradation with in-Context learning (ICL), respectively. • We propose a prompt-based solution, named VL-Time, to address this limitation using visualization-modeled data and language-guided reasoning. Specifically, VL-Time effectively models and compresses time-series data, thereby empowering feature extraction and enabling ICL effectiveness; adopts a ""plan-then-solve"" framework, thus unlock language-driven reasoning capabilities. • Experimental results show that VL-Time enables multimodal LLMs as non-trivial zero-shot and powerful few-shot reasoners for time series. Specifically, VL-Time with a few demonstrating examples can consistently outperform all supervised methods in deterministic reasoning tasks and parts of them in probabilistic reasoning tasks. Compared to directly numerical modeling, VL-Time achieves an average performance improvement of 140%, up to 433%, and average token cost reduction of 99%. Additional related works are provided in Appendix A. Limitations are discussed in Appendix B."
https://arxiv.org/html/2411.06008v2,The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses.,"This study explores how the Large Language Models (LLMs) adjust linguistic features to create personalized persuasive outputs. While research showed that LLMs personalize outputs, a gap remains in understanding the linguistic features of their persuasive capabilities. We identified 13 linguistic features crucial for influencing personalities across different levels of the Big Five model of personality. We analyzed how prompts with personality trait information influenced the output of 19 LLMs across five model families. The findings show that models use more anxiety-related words for neuroticism, increase achievement-related words for conscientiousness, and employ fewer cognitive processes words for openness to experience. Some model families excel at adapting language for openness to experience, others for conscientiousness, while only one model adapts language for neuroticism. Our findings show how LLMs tailor responses based on personality cues in prompts, indicating their potential to create persuasive content affecting the mind and well-being of the recipients.","The emergence of Large Language Models (LLMs) into the public consciousness at the end of 2022 has significantly transformed the reality of completing tasks across various sectors, from business to education through administration and journalism. It is estimated that in the USA, 80% of employees will have 10% of their work affected by LLMs, with approximately 19% of occupations will have an impact on at least 50% of job duties (Eloundou et al., 2023). The applications of LLMs in education include question-solving, error correction, study assistance, question generation, automatic grading, and material creation. LLMs also support adaptive learning through knowledge tracking and personalizing content (Wang et al., 2024). The vast amount of tasks where LLMs are used will inevitably raise serious challenges related to possible harms, including discrimination, exclusion, toxicity, information hazards, misinformation, malicious uses, and human-computer interaction harm (Weidinger et al., 2021). Multiple techniques are applied to protect users from potentially harmful content, such as probing toxic content with structured patterns (Ousidhoum et al., 2021) and watermarking the suspected text fragment (Kirchenbauer et al., 2023). LLMs are also used in red teaming activities, where they cooperate with other LLMs to safeguard models against misuse (Perez et al., 2022). The effectiveness of these techniques depends on the awareness of potential threats. In our work, we focus on a specific danger: the persuasion by LLMs. To our knowledge, this is the first paper aimed at understanding how LLMs adjust their responses based on a user’s personality in a persuasion task. Given the goal we want to achieve, we pose the following research questions: [RQ1] Which linguistic features are crucial in personalized persuasion? [RQ2] How do Large Language Models adjust the linguistic features in their responses based on the user’s personality type information? [RQ3] Which families of Large Language Models are particularly effective in influencing specific personality traits? Contribution By answering the questions above, we contributed as follows: [C1] We identified 13 linguistic features that are crucial for persuading individuals with varying levels of Big Five personality traits. [C2] We conducted a comparative analysis of 19 models from 5 Large Language Models families, focusing on the language used in responses to persuasive tasks. [C3] We demonstrated how LLMs adapt their language in persuasive task to different personality types. [C4] We presented the dataset with different variances of persuasive task that can be used to validate the personalized persuasiveness of the Large Language Models."
https://arxiv.org/html/2411.05983v1,Longitudinal Ensemble Integration for sequential classification with multimodal data,"Effectively modeling multimodal longitudinal data is a pressing need in various application areas, especially biomedicine. Despite this, few approaches exist in the literature for this problem, with most not adequately taking into account the multimodality of the data. In this study, we developed multiple configurations of a novel multimodal and longitudinal learning framework, Longitudinal Ensemble Integration (LEI), for sequential classification. We evaluated LEI’s performance, and compared it against existing approaches, for the early detection of dementia, which is among the most studied multimodal sequential classification tasks. LEI outperformed these approaches due to its use of intermediate base predictions arising from the individual data modalities, which enabled their better integration over time. LEI’s design also enabled the identification of features that were consistently important across time for the effective prediction of dementia-related diagnoses. Overall, our work demonstrates the potential of LEI for sequential classification from longitudinal multimodal data.","Data that are both longitudinal/temporal and multimodal are increasingly being used in combination with machine learning for forecasting, especially in medical diagnosis (Brand et al., 2019; Zhang & Shen, 2012; Feis et al., 2019; Li et al., 2023). Recently, a number of promising approaches for sequential classification from such data have been introduced (Eslami et al., 2023; Zhang et al., 2011; Wang et al., 2016; Zhang et al., 2024). For instance, some approaches have used recurrent neural network (RNN)-based models applied to data sequences where the modalities at each time point have been concatenated into a long feature vector, sometimes referred to as early fusion (Nguyen et al., 2020; Olaimat et al., 2023; Maheux et al., 2023). Generally, due to the complexity of this modeling, these approaches generally consider only a limited set of modalities, samples, or time points. In addition to computational issues, the early fusion of data across modalities may obfuscate signals local to the individual data modalities (Zitnik et al., 2019; Li et al., 2022). This may hinder model performance, as the differing semantics and scales of the various modalities are not adequately accounted for. This represents a major challenge for the automated classification of multimodal longitudinal data, as the consensus and complementarity among different modalities may not be sufficiently leveraged (Zitnik et al., 2019; Li et al., 2022). Recently, the Ensemble Integration (EI) framework (Figure 1) was developed to leverage modality-specific information during data integration and predictive modeling (Li et al., 2022). EI accomplishes this by first capturing information local to the individual modalities into effective base predictors of the target outcome. These base predictors are then aggregated into final heterogeneous ensemble-based predictive models using methods such as stacking (Sesmero & Ledezma, 2015). Due to its flexibility, EI has been effective for a variety of biomedical prediction problems, such as protein function prediction and prognosis of COVID-19 patients in Li et al. (2022), detection of cell division events in Bennett et al. (2024), screening of diabetes among youth in McDonough et al. (2023), and the early detection of dementia from only snapshot baseline data in Cirincione et al. (2024). Additionally, EI has provided insights into the prediction problems in the applications listed above, e.g., risk factors of diseases, through its model interpretation capability (Li et al., 2022). Despite the successes of EI, it has thus far only been applicable in its design to non-longitudinal multimodal data. Figure 1: Overview of the Ensemble Integration (EI) framework (Li et al., 2022). In the first step, libraries of base predictor models are applied to each distinct data modality. These base predictions are then integrated using ensemble algorithms such as stacking, Sesmero & Ledezma (2015), for final classification. Interpretation of both the base predictors and the ensemble is used to determine the most predictive features. In the current work, we extended the capabilities of EI to multimodal longitudinal data by combining its modality-specific base predictors with temporal deep learning methods like RNNs. Specifically, in the new Longitudinal Ensemble Integration (LEI) framework, we first derive base predictors from individual modalities at every time point and then stack these predictions using a Long Short-Term Memory (LSTM) Network (Hochreiter & Schmidhuber, 1997). We also leveraged EI’s interpretation strategy found in Li et al. (2022) to identify the most predictive features at different time points, as well as longitudinal patterns among these features. We tested the capabilities of LEI for the early and accurate prediction of the progression of dementia, due to this being one of the most commonly tackled problems in the sequential classification of multimodal data (most of the references cited above), as well as for its real world importance (Chen et al., 2021; Bredesen, 2014). For this, we used data from The Alzheimer’s Disease Prediction of Longitudinal Evolution (TADPOLE) Challenge, Marinescu (2019), a structured dataset derived from The Alzheimer’s Disease Neuroimaging Initiative (ADNI) (Petersen, 2010). TADPOLE contains multiple data modalities, e.g., demographics and cognitive test scores, as well as domain-relevant features derived from MRI images, such as volumes of brain regions, collected over the course of ADNI participants’ enrolment in the study. Our specific task was to use the TADPOLE data to predict whether a patient would be diagnosed with different stages of this disorder, namely cognitively normal (CN), mildly cognitively impaired (MCI), or dementia, at the next TADPOLE/ADNI visit, given data up to and including the current one. We evaluated the performance of various configurations of LEI for this task, as well as compared its performance to those of existing approaches for multimodal sequential classification. We also leveraged EI’s interpretation strategy introduced in Li et al. (2022) to identify the most predictive features for this problem at different time points, as well as temporal patterns among these features. Although demonstrated here for early dementia detection, our approach is general with respect to applications, modalities, and constituent models, and can be adapted for other data integration-based longitudinal prediction problems as well."
https://arxiv.org/html/2411.05982v1,Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM,"Sandboxes and other dynamic analysis processes are prevalent in malware detection systems nowadays to enhance the capability of detecting 0-day malware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in modern malware samples, and sandboxes can suffer from false negatives and analysis failures when analyzing the samples with TADAs. In such cases, human reverse engineers will get involved in conducting dynamic analysis manually (i.e., debugging, patching), which in turn also gets obstructed by TADAs. In this work, we propose a Large Language Model (LLM) based workflow that can pinpoint the location of the TADA implementation in the code, to help reverse engineers place breakpoints used in debugging. Our evaluation shows that we successfully identified the locations of 87.80% known TADA implementations adopted from public repositories. In addition, we successfully pinpoint the locations of TADAs in 4 well-known malware samples that are documented in online malware analysis blogs.","Malware detection in real-world is achieved through a complex human-in-the-loop system with reverse engineers playing an essential role. In terms of quantity, the majority of the malware samples are detected and blocked by file signatures, including inline firewall blocking and anti-virus software. This is because most samples in the wild are 1-day or n-day samples, so their file signatures are already documented, either automatically or manually. Obviously, any signature-based methods are subject to 0-day malware attacks, and therefore different methods need to be adopted to detect and block 0-day malware. Although it is possible to detect 0-day malware through advanced static analysis, in practice this strategy can suffer from non-scalability and false negatives. For example, some samples are only at the initial stage of infection and will do no harm unless subsequent payloads are downloaded, which can only be confirmed to be malicious by concretely executing the samples. Therefore, many industry-level malware detection systems rely on automated dynamic analysis, or sandboxes [1, 2], to detect 0-day malware using behavior signatures and rules. Unfortunately, most malware authors are fully aware of the existence of sandboxes and dynamic analysis efforts, and therefore many of them nowadays will implement Techniques of Anti-Dynamic-Analysis (TADA) in their malware to evade dynamic analysis-based detection. Figure 1: Practical Malware Detection System As we will elaborate in Section II-A, malware authors usually have two goals with TADAs implemented: evading the dynamic analysis-based detection and impeding reverse engineering effort, respectively. These two goals are directly motivated by the process of detecting malware samples in the real world, which is illustrated in Figure 1. The top half of the figure is fully automated, and the bottom half involves human labor. When a sample arrives at the system, the first step is usually to check the existing signature and hash databases to see if the sample is already known to be malicious. If the signature or hash has a match, then the system can immediately conclude that the sample is malicious. Despite the disadvantage of the file signature-based detection (i.e. unable to handle 0-day), it is still worthwhile to have it in the detection system to block 1-day and n-day malware, because it is extremely fast and affordable. To keep the databases updated, whenever a malware verdict is generated in the later part of the system (meaning that the sample has not been seen), it will be used to update the databases. Subsequently, in cases where there is no match for the given sample, it will then be transferred to a sandbox, where the sample will be dynamically analyzed. The sample will be executed and monitored without human intervention. The sandbox analysis usually has 3 outcomes: malware verdict, benign verdict, and analysis failure. Since detection is achieved through conservative behavior heuristics and rules, the malware verdict is usually true positive. In contrast, a benign verdict can be false negative, due to either TADAs or missing OS components that cause the sample not to behave maliciously in the sandbox environment. These false negatives are usually found later, and the system will receive a verdict flip request. As for analysis failures, they are the cases where the sample failed to launch at all or crashed during the execution. The most common reason for false negative and analysis failures is the presence of TADA, as sandboxes are usually virtual machines (VM) that conduct automatic dynamic analysis. Consequently, at the point of failures or verdict flip requests, human intervention is necessary, and the sample is then sent to a reverse engineer for the final verdict. Back to the discussion of the goals of TADAs, the ultimate goal of the malware authors is to delay the detection of their malware as much as possible, so that more victims could be infected. Based on Figure 1, it is obvious that the best situation for malware authors’ interest is when a malware sample bypasses the signature database, bypasses sandboxes, and eventually reaches the hands of the reverse engineer, being analyzed for an extended period of time. Consequently, the first goal of TADAs is to bypass the sandbox detection, which is usually achieved by detecting the artifacts (e.g. analysis programs, common VM hardware names) in the sandbox. To fight back, there are numerous previous works studying how to prevent malware from bypassing the sandboxes in the first place [3, 2, 1, 4, 5, 6], most of which can be summarized in one single word: transparency. The idea is simple: as long as the sandbox is transparent (i.e., the sandbox is not discernible with the real machine), the malware will trigger malicious behaviors. As we will elaborate in Section III-A, despite the theoretical feasibility of complete transparency, in the real world pursuing transparency is essentially a trade-off between cost and transparency enhancement. Therefore, it is very common that a malware sample eventually will get to the hands of a reverse engineer, which corresponds to the second goal of TADAs: impeding the reverse engineering efforts to delay the analysis. Thus, from the reverse engineers’ perspective, the faster they can find the TADA implementation (in the code), the better so that they can bypass or patch the TADAs during their dynamic analysis. Although there are existing works studying detecting malware with TADA [6, 7, 8], most of them focus on the detection of anti-analysis and malicious behaviors rather than the problem of identifying the locations of TADAs. As shown in Figure 1, the scope of this research is not to detect the malware with TADAs (i.e., evasive malware) in general; rather we focus on complementing the current reverse engineering efforts on evasive malware, reducing the time and labor efforts of reverse engineers. Specifically, this research aims to identify the locations of the TADA in the code for breakpoints used in manual dynamic analysis (i.e. debugging) during the reverse engineering efforts. However, pinpointing the locations of TADA implementations can be challenging. First, there are a variety of TADAs: checking hardware (e.g., CPU cores, memory size, PCIe devices, power capabilities, fans, etc.), checking running processes, checking filesystems, checking user traces, etc. Second, not only can malware adopt different techniques, but also can they use different implementations. Taking the number of CPU core as an example, it can be checked via excessive number of implementations (e.g., CPUID, WMI Query, different APIs, etc.), all up to malware authors’ creativity. This diversity of the TADA implementations essentially makes the detection very prone to false negatives. In this paper, we present a workflow that leverages advanced static analysis and a Large Language Model (LLM) to pinpoint the locations of TADA in the code, given a malware binary executable. The workflow is based on two key insights. First, an LLM can be leveraged to address a primary limitation of current rule-based static analysis for TADAs. That is, the rule-based approaches are, in principle, not scalable when new TADAs and TADA implementations keep on emerging. Second, the semantic gap between disassembled code and natural languages results in unsatisfactory responses when the LLM is directly asked to examine disassembly code, which could be bridged by advanced static analysis. Our major contributions are as follows: • Propose a LLM-based workflow that can pinpoint the locations of the TADA in a binary executable automatically. • Construct the useful features that can enable the LLM to do the detection. • Evaluate our method using real-world malware samples of popular families. Specifically, we successfully identify the locations of 87.80% of known TADA implementations obtained from public repositories. In addition, we successfully pinpoint the locations of TADAs in 4 well-known malware samples that are documented in online malware analysis blogs."
https://arxiv.org/html/2411.05980v1,FactLens: Benchmarking Fine-Grained Fact Verification,"Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift toward fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens111Link to our data and code, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.","Large Language Models (LLMs) have proven to be powerful tools, demonstrating impressive capabilities in language generation and understanding Touvron et al. (2023); Brown et al. (2020). However, a well-known limitation of LLMs is their tendency to hallucinate, generating information that is factually incorrect or unsupported by evidence Ji et al. (2022); Lin et al. (2021). As LLMs become more widespread, especially in applications where factual accuracy is crucial, there has been increasing research on methods to verify the factuality of LLM-generated content as well as claims from other sources. Previous works on building fact-checking benchmarks focus on generating claims with a ground truth label, and in some cases provide the evidence/context to verify the claim. Aly et al. (2021); Schlichtkrull et al. (2023). Claims are generated using human annotators Aly et al. (2021), synthetic processes Bayat et al. (2023); Tang et al. (2024), or considering LLM outputs on Question-Answering tasks Wang et al. (2023). To increase the complexity of the fact-checking process, the claims are generated from source data of multiple domains & modalities, such as Wikipedia text and/or tables Thorne et al. (2018); Chen et al. (2019); Aly et al. (2021), Web Pages Schlichtkrull et al. (2023), Knowledge Graphs Kim et al. (2023), online posts/chats Wang et al. (2023); Li et al. (2023), and Q-A tasks from various domains such as statistics, finance, legal, etc Jacovi et al. (2024a). These works also provide baseline fact-checking pipelines, which typically involves two main stages: (1) the retrieval of relevant evidence using Search APIs and multimodal data-lakes Tang et al. (2023); Schlichtkrull et al. (2023) and (2) the verification of claims based on that evidence using NLI-based, LLM-based and fine-tuned fact-verification models Li et al. (2023). Some works also explore delegating these steps entirely to an LLM-based policy framework Li et al. (2023); Peng et al. (2023). Despite this structured pipeline, most existing methods rely on a holistic verification model, where complex claims are assigned a single factuality label, often obscuring the nuanced nature of the errors or inaccuracies in the claims. In this work, we echo the sentiments of Wang et al. (2023); Liu et al. (2019); Pan et al. (2023) for a shift towards fine-grained verification of complex claims, where claims are decomposed into smaller, more manageable sub-claims that can be individually verified. We additionally emphasise on the need to provide evaluation metrics to benchmark such fine-grained verification and enrich existing benchmarks with fine-grained verification labels. As shown in Figure 1, the benefits of fine-grained verification are substantial. By breaking down a complex claim into its constituent sub-claims, verification is more precise, allowing for pinpointing exact locations of factual inaccuracies. Additionally, this approach enables more transparent rationalizations and explanations, as each sub-claim can be linked directly to its corresponding evidence or lack thereof. Fine-grained decomposition also narrows the scope of evidence retrieval, making the subsequent verification process more focused and less prone to ambiguity. Achieving fine-grained verification, however, presents its own challenges. Decomposing a raw, complex claim into smaller sub-claims is not simply a matter of splitting it into sentences. Poorly constructed sub-claims can introduce a variety of issues: they may lose the context necessary for proper verification, lack atomicity, or misrepresent the original information by either omitting key details or introducing new ones. Ensuring the quality and verifiability of these sub-claims is therefore critical for the overall success of the verification process. To address these challenges, we introduce FactLens, a benchmark designed specifically for fine-grained fact verification. FactLens provides a novel suite of metrics for evaluating the quality of sub-claim generation and incorporates automated evaluators that combine LLM-based assessments with statistical metrics. The dataset has been manually curated to ensure high-quality sub-claims. Through empirical evaluation, we demonstrate that our sub-claim evaluators align closely with human judgments. Moreover, our end-to-end evaluation shows that these fine-grained scores correlate strongly with improved downstream verification performance. We also present the results of state-of-the-art models on sub-claim generation, revealing the challenges inherent in this task and the need for further research in this area. Figure 1: Examples of holistic fact verification (upper) failed to identify inaccuracies, whereas fine-grained verification (lower) clearly pinpointed the sources of error. In fine-grained verification, the FactLens evaluator can be used to assess individual sub-claims and identify any alarming signals that may suggest the need for human intervention or regeneration of the sub-claims."
https://arxiv.org/html/2411.05958v1,Sentiment Analysis of Cyberbullying Data in Social Media,"Social media has become an integral part of modern life, but it has also brought with it the pervasive issue of cyberbullying a serious menace in today’s digital age. Cyberbullying, a form of harassment that occurs on social networks, has escalated alongside the growth of these platforms. Sentiment analysis holds significant potential not only for detecting bullying phrases but also for identifying victims who are at high risk of harm, whether to themselves or others. Our work focuses on leveraging deep learning and natural language understanding techniques to detect traces of bullying in social media posts. We developed a Recurrent Neural Network with Long Short-Term Memory (LSTM) cells, using different embeddings. One approach utilizes BERT embeddings, while the other replaces the embeddings layer with the recently released embeddings API from OpenAI. We conducted a performance comparison between these two approaches to evaluate their effectiveness in sentiment analysis of Formspring Cyberbullying data. Our Code is Available at : https://github.com/ppujari/xcs224u","In recent years, social media has become an integral part of daily life, facilitating communication and connection on an unprecedented scale. However, it has also contributed to a rise in cyberbullying harassment conducted online (Feinberg and Robey, 2009). Unlike traditional bullying, cyberbullying can happen anytime, with online anonymity encouraging perpetrators to act without facing immediate consequences (Hasan et al., 2023). This anonymity often leads to higher rates of cyberbullying compared to in-person bullying. Bullying is recognized as a major health issue by institutions like the American Academy of Pediatrics, has become especially concerning in educational settings (Xu et al., 2012). The tragic case of Megan Meier, a young victim of harassment on MySpace, highlights the devastating impact of cyberbullying Vines (2015). Given the growing concern in society and workplaces, and the fact that studies show many students face cyberbullying Li (2006); Cross (2008), detecting and preventing cyberbullying promptly is essential. In this work, we use Sentiment Analysis (SA) to classify texts as positive or negative with respect to cyberbullying. While traditional embeddings used in Recurrent Neural Networks (RNNs) are effective in capturing sequential patterns, they struggle with contextual dependencies in longer sentences and have limitations in understanding complex language structures Elman (1990); Pascanu (2013).In contrast, we incorporate state-of-the-art Large Language Model (LLM) embeddings, specifically BERT embeddings (Kenton and Toutanova, 2019), and use the OpenAI API to obtain embeddings that leverage self-attention mechanisms for sentiment analysis. These models excel in capturing intricate language features, making them highly effective for a range of downstream tasks Radford et al. (2019); Raffel et al. (2020). These embeddings offer a deeper contextual understanding compared to earlier models, allowing for a more nuanced detection of harmful content and a more precise evaluation of sentiment polarity. The task involves identifying sentences containing bullying tokens, assessing their polarity, and accurately classifying them into the relevant sentiment categories. The development of such Natural Language Processing (NLP) algorithms, however, requires high-quality annotated data to measure performance accurately. Many popular Machine Learning (ML) techniques, especially Deep Neural Networks (DNNs), need large, annotated corpora to achieve high-quality classification results. Unfortunately, the availability of publicly accessible datasets for cyberbullying detection is limited. In this work, we use Kaggle’s Formspring data Pujari (2022); Reynolds et al. (2011) for Cyberbullying Detection to train and evaluate our models. We apply the advanced LLM-based techniques and assess the model’s performance in identifying cyberbullying. In this work, we made the following contributions: • Introduce two distinct hybrid methods for sentiment analysis in cyberbullying detection, leveraging advanced embeddings instead of traditional techniques. • Utilizes BERT embeddings with an RNN framework in one method and OpenAI embeddings with an RNN framework in another, specifically for sentiment analysis of cyberbullying data, aiding in cyberbullying detection. • Compare the performance of the proposed hybrid architectures using BERT and OpenAI embeddings for sentiment analysis on the Formspring dataset."
https://arxiv.org/html/2411.05945v1,": Toward Post Recognition Generative Correction 
Large Language Models with Task-Oriented Experts","Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an “expert” of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset’s tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.","Figure 1: Proposed NeKo, a new form multi-task model to boost post-recognition results over speech, text, and visual inputs. NeKo could work for (i) post automatic speech recognition (ASR) correction, (ii) post speech translation (ST) and machine translation (MT) correction, and (iii) post optical character recognition (OCR) correction. NeKo discover new state-of-the-art results in (iv) zero-shot ASR correction and performs competitively as a general-purpose (v) multi-task corrector. Human recognition (Biederman, 1987; Juang and Furui, 2000; Kanwisher et al., 1996) capabilities span multiple modalities, including speech recognition, visual patterns, and extensions to semantic and textual interpretations. These faculties, however, are not infallible and often incorporate mis-recognition errors. Despite these imperfections, humans efficiently communicate using speech, language, or facial expressions. For instance, two non-native speakers (Lev-Ari, 2015; Valaki et al., 2004) can often achieve mutual understanding through this imperfect recognition and subsequent interpretative processes, even when the conversation is marred by lexical inaccuracies and subdued accents. In other words, humans (as intelligent agents) exhibit a robust capacity for generative understanding Jiang et al. (2020); Cheng et al. (2021) that extends beyond initial recognition results. In neuroscience Zatorre and Gandour (2008), the inferior temporal gyrus and the temporal lobe are not confined to rudimentary perception but are also integral to the post-recognition processes that facilitate semantic understanding of language (Levinson and Evans, 2010), speech (Marshall et al., 2015), and visual patterns (Vink et al., 2020). This form of “post-recognition correction,” exemplified by the application of language modeling (LM) to initial recognition outputs, has been introduced to the field for both acoustic (automatic speech recognition, ASR) and visual (optical character recognition, OCR) modalities since the early explorations (Jelinek, 1976; Dixon and Silverman, 1975) of learning algorithms in 1970s. The most prevalent approaches to utilizing LMs for post-recognition boosting are predominantly ranking or retrieval-based. In these setups, the LM is tasked with ranking and scoring (Ljolje et al., 1999) the top n-best hypotheses generated by the first-pass recognition system. This process often incorporates “discriminative modeling” algorithms (Sukkar and Lee, 1996; Mangu et al., 2000) and representation embeddings, such as BERT (Salazar et al., 2020; Kenton and Toutanova, 2019), to minimize the word error rate (WER) during training (Prabhavalkar et al., 2018; Mangu et al., 2000). With the LMs scaling up to LLMs (Brown et al., 2020), recent efforts (Chan et al., 2023; Yang et al., 2023; CHEN et al., 2023; Hu et al., 2024a) have focused on exploring a “generative modeling” for post-recognition correction. This generative error correction (GER) approach uses LLMs to conduct final recognition from given first-pass text-based predictions from recognition models, including ASR, image captioning (IC), and machine translation (MT). This cascaded two-agents text-to-text GER model has outperformed larger single multi-modal and multi-task models in these tasks. Meanwhile, these GER solutions heavily depends on domain-specific fine-tuning processes (Chen et al., 2024a) that utilize parameter-efficient components, which often suffers a performance degradation from a lack of generalizability across different datasets, domains, and tasks. In other words, how to design and further openly provide a “general-purpose post (every) recognition correction model” is still one undiscovered and crucial topic within the research community. On the other hand, directly fine-tuning LLMs on a mixture of diverse error correction datasets can lead to suboptimal performance (CHEN et al., 2023; Lange et al., 2022) due to differences in input modalities, output formats, error types, and domain characteristics. For example, ASR errors stem from phonetic similarities or acoustic ambiguities, while OCR errors involve visual or character-level confusions. Additionally, error distributions can vary widely across datasets, even within the same task. To characterize “model generalization,” mixture-of-experts (MoE) (Jiang et al., 2024a) has emerged as a promising approach for multi-task learning, consisting of of a set of expert networks and a gating network that learns to route the input to the most appropriate expert(Sukhbaatar et al., 2024). This enables MoE models to learn more specialized and fine-grained representations compared to monolithic models. However, most MoE models are designed for general-purpose language modeling(Dai et al., 2024), with experts not explicitly assigned to specific tasks, but rather learn to specialize in different aspects of the input space through data-driven training, expect for a recent vision work (Ye and Xu, 2023). Effectively leverage MoE for multi-task error correction, where the experts need to capture task-specific features while allowing knowledge sharing, remains an open question. In this work, we propose NeKo, a “geNerative multi-tasK error correction” approach that leverages a pre-trained MoE model to drive diverse tasks and cross-domain knowledge, as shown in Figure 1. The key idea is to continuously pre-train MoE model on a mixture of error correction datasets, with each expert specializing in a specific domain. This task-oriented MoE fine-tuning approach enables the experts to capture task-specific features while allowing knowledge sharing through the router (Dai et al., 2024). NeKo captures the nuances of each task, benefiting from shared knowledge across experts. Evaluated on tasks such as ASR, ST, OCR, and unseen textual error correction (TEC), NeKo consistently outperforms baseline models, including Claude-Opus and GPT-3.5. It achieves state-of-the-art WER reduction on the Hyporadise benchmark and large-scale Open ASR Leaderboard (Srivastav et al., 2023). NeKo also significant improves in OCR error correction. Further analysis confirms its robust multi-task capabilities. In summary, the main contributions of this work include: 1. We introduce NeKo, a multi-task error correction LLM that leverages task-oriented mixture-of-experts for diverse post-recognition correction tasks. To the best of our knowledge, this is the first work that explores the use of MoE for multi-task error correction. 2. NeKo has been studied under a new form of cross-modalities post-recognition correction evaluation, serving as strong open-source ASR, ST, OCR, and TEC baselines. Our results show that NeKo discovers new state-of-the-art performance in ASR as a multi-task error correction model. 3. We discovered emergent abilities for cross-task correction from NeKo as a first-of-its-kind multi-task correction approach toward a general-purpose post-recognition LM designs. 4. The NeKo models, newly created source datasets, and training processes are scheduled to open source under the CC BY-SA 4.0 license to support reproducibility and to encourage future research."
https://arxiv.org/html/2411.05939v1,GCI-ViTAL: Gradual Confidence Improvement with Vision Transformers for Active Learning on Label Noise,"Active learning aims to train accurate classifiers while minimizing labeling costs by strategically selecting informative samples for annotation. This study focuses on image classification tasks, comparing AL methods on CIFAR10, CIFAR100, Food101, and the Chest X-ray datasets under varying label noise rates. We investigate the impact of model architecture by comparing Convolutional Neural Networks (CNNs) and Vision Transformer (ViT)-based models. Additionally, we propose a novel deep active learning algorithm, GCI-ViTAL, designed to be robust to label noise. GCI-ViTAL utilizes prediction entropy and the Frobenius norm of last-layer attention vectors compared to class-centric clean set attention vectors. Our method identifies samples that are both uncertain and semantically divergent from typical images in their assigned class. This allows GCI-ViTAL to select informative data points even in the presence of label noise while flagging potentially mislabeled candidates. Label smoothing is applied to train a model that is not overly confident about potentially noisy labels. We evaluate GCI-ViTAL under varying levels of symmetric label noise and compare it to five other AL strategies. Our results demonstrate that using ViTs leads to significant performance improvements over CNNs across all AL strategies, particularly in noisy label settings. We also find that using the semantic information of images as label grounding helps in training a more robust model under label noise. Notably, we do not perform extensive hyperparameter tuning, providing an out-of-the-box comparison that addresses the common challenge practitioners face in selecting models and active learning strategies without an exhaustive literature review on training and fine-tuning vision models on real-world application data.","While most works in literature often compare results to baseline active learning algorithms such as random selection or simple entropy-based selection, the extensive hyper-parameter tuning performed during training but often left out of the manuscripts leads to authors stating vastly different performances for the same CNN architecture, active learning algorithm [1], and label noise rate [2]. This not only raises questions about the credibility of reported state-of-the-art results but can also delay actual progress in developing active learning schemes that are robust to label noise and achieve performances comparable to models trained on clean labels. The work on non-active training of DL models in the presence of label noise, as well as the training of DL models on noise-free datasets in an active learning setting, are well addressed in the literature. However, the intersection of these niches has a long way to go [3]. AL algorithms seek to train an optimal model with minimal training data that is labeled iteratively. The most common AL methods seek to explore diverse training examples or focus on samples that the DL algorithm is uncertain about. AL in the presence of label noise is a particularly challenging topic since training DL models with a higher concentration of incorrect labels presents problems for the back-propagation algorithm’s ability to converge as demonstrated in [2]. It has also been shown that without sufficient training data, large models can memorize the noisy labels, and fail to generalize to the test set [4, 5, 6, 7, 8]. Figure 1 depicts the basic AL framework for training DL image classifiers in the presence of label noise. Figure 1: The main components in the AL framework in the presence of a noisy oracle. Each of these components may vary depending on the complexity of the data to be learned and the available resources. Most work in active learning with label noise has focused on the development of query selection algorithms that lead to highly informative and diverse data samples as well as noise-robust DL models. In this work, we compile a unified view of existing works on active learning for image classification with label noise. We are particularly interested in the fine-tuning CNN and ViT models pre-trained on the imageNet-1k dataset. We explore different DL model architectures and AL strategies on different datasets while varying the label noise rates up to 60%. We re-implement the commonly used baseline AL strategies namely: random query, maximum entropy, margin-based selection, model delta, and hybrid uncertainty sampling with diversity. This work seeks to address the following: • In the realm of active learning, where the emphasis is often on query selection, and in image classification with label noise, which is typically trained without active learning, reliable benchmarks for active learning algorithms with label noise are scarce. To tackle this issue, we conduct experiments by training multiple deep-learning models for image classification using different active learning algorithms and varying levels of label noise. We present our findings by reporting test results on popular datasets, including CIFAR10, CIFAR100, Food101, and Chest X-ray images (pneumonia). • Given that the ViT-based models currently outperform CNNs in image classification on CIFAR10 [9, 10], and post competitive results on CIFAR100, Food101, Chest X-ray images (pneumonia), and other classification datasets [11, 10, 12], how does the ViTs compare to CNN-based models in an active learning setting with label noise (ALLN), and what can be done to improve on ViT learners in this setting? • Lastly, we propose an active learning scheme customized for the properties of the transformer network to improve on active learning in the presence of label noise. We also provide new insights based on using ViTs for AL under label noise and propose avenues to advance this work."
https://arxiv.org/html/2411.05936v1,Mitigating Hallucination | ZeroG: An Advanced Knowledge Management Engine,"The growth of digital documents presents significant challenges in efficient management and knowledge extraction. Traditional methods often struggle with complex documents, leading to issues such as hallucinations and high latency in responses from Large Language Models (LLMs). ZeroG, an innovative approach, significantly mitigates these challenges by leveraging knowledge distillation and prompt tuning to enhance model performance.ZeroG utilizes a smaller model that replicates the behavior of a larger teacher model, ensuring contextually relevant and grounded responses, by employing a black-box distillation approach, it creates a distilled dataset without relying on intermediate features, optimizing computational efficiency. This method significantly enhances accuracy and reduces response times, providing a balanced solution for modern document management.Incorporating advanced techniques for document ingestion and metadata utilization, ZeroG improves the accuracy of question-and-answer systems. The integration of graph databases and robust metadata management further streamlines information retrieval, allowing for precise and context-aware responses. By transforming how organizations interact with complex data, ZeroG enhances productivity and user experience, offering a scalable solution for the growing demands of digital document management.","ZeroG significantly improves the quality of responses by a large margin by mitigating hallucinations through the implementation of knowledge distillation and prompt tuning, ensuring responses are accurate and grounded. We differ from [1], which involves fine-tuning the student model, by utilizing a black-box distillation approach without fine-tuning. This approach reduces response latency and enhances overall system reliability. ZeroG leverages LLMs to generate Question and Answer (QnA) pairs from existing documents, storing them in a vector store. When a user query is received, similarity searches using techniques like MMR determine whether it can be addressed by pre-existing QnA pairs or requires a more tailored response using document-specific information. Despite advances in natural language processing, traditional methods often struggle with real-time data updates and accurately handling complex documents, which include sensitive data. This paper explores transforming these presentations into markdown files for easier ingestion into vector stores, enhancing QnA accuracy without frequent reengineering. We also investigate integrating graph databases and utilizing document metadata to refine search and organization capabilities. By pre-generating question sets and caching commonly asked queries, the system streamlines responses, ensuring they are precise and contextually relevant. This paper presents some of the techniques we explored and employed to overcome current limitations in document and knowledge management, significantly improving productivity and effectiveness in handling complex document types."
https://arxiv.org/html/2411.05930v1,BERTrend: Neural Topic Modeling for Emerging Trends Detection,"Detecting and tracking emerging trends and weak signals in large, evolving text corpora is vital for applications such as monitoring scientific literature, managing brand reputation, surveilling critical infrastructure and more generally to any kind of text-based event detection. Existing solutions often fail to capture the nuanced context or dynamically track evolving patterns over time. BERTrend, a novel method, addresses these limitations using neural topic modeling in an online setting. It introduces a new metric to quantify topic popularity over time by considering both the number of documents and update frequency. This metric classifies topics as noise, weak, or strong signals, flagging emerging, rapidly growing topics for further investigation. Experimentation on two large real-world datasets demonstrates BERTrend’s ability to accurately detect and track meaningful weak signals while filtering out noise, offering a comprehensive solution for monitoring emerging trends in large-scale, evolving text corpora. The method can also be used for retrospective analysis of past events. In addition, the use of Large Language Models together with BERTrend offers efficient means for the interpretability of trends of events.","The concept of weak signals, introduced by Ansoff (1975), refers to early indicators of emerging trends that can have significant implications across various domains. These include events like shifts in public opinion in social trends, early disruptive technologies in innovation, changes in activist groups and public sentiment in politics, and potential disease outbreaks in healthcare. Monitoring and analyzing weak signals offers valuable insights for organizations, researchers, and decision-makers, aiding in informed decision-making. Key data sources for identifying these trends include large text corpora such as news, social media, research and technology journals or reports. The challenges are: distinguishing meaningful weak signals from irrelevant noise, dealing with context ambiguity, and tracking the extended period over which weak signals may gain significance. With advances in NLP and AI, researchers have developed various techniques to detect weak signals across different fields,including statistics-based methods, graph theory, machine learning, semantic-based approaches, and expert knowledge. However, most solutions fall short in fully addressing the challenge of detecting emerging trends Rousseau et al. (2021), either by relying solely on keyword-based analysis, which misses contextual nuances, or by being static and unable to dynamically track evolving weak signals. In this work, we introduce BERTrend, a novel framework for detecting and monitoring emerging trends and weak signals in large, evolving text corpora. BERTrend leverages neural topic modeling, specifically BERTopic, in an online learning setting to identify and track topic evolution over time. Its key contribution lies in dynamically classifying topics as noise, weak signals, or strong signals based on their popularity trends. The proposed metric quantifies topic popularity over time by considering both the number of documents within the topic and its update frequency, incorporating an exponentially growing decay if no updates occur for an extended period. By combining neural topic modeling with a dynamic popularity metric and adaptive classification thresholds, BERTrend provides a comprehensive solution for detecting and monitoring emerging trends in large-scale, evolving text corpora. We discuss the qualitative results on two comprehensive datasets, including the overall evolution of trends and specific case studies. Combined with Large Language Models (LLMs), the method an efficient way of interpreting the detected trends of events through various dimensions indicating how they evolve over time."
https://arxiv.org/html/2411.05927v1,Moving Off-the-Grid: Scene-Grounded Video Representations,"Current vision models typically maintain a fixed correspondence between their representation structure and image space. Each layer comprises a set of tokens arranged “on-the-grid,” which biases patches or tokens to encode information at a specific spatio(-temporal) location. In this work we present Moving Off-the-Grid (MooG), a self-supervised video representation model that offers an alternative approach, allowing tokens to move “off-the-grid” to better enable them to represent scene elements consistently, even as they move across the image plane through time. By using a combination of cross-attention and positional embeddings we disentangle the representation structure and image structure. We find that a simple self-supervised objective—next frame prediction—trained on video data, results in a set of latent tokens which bind to specific scene structures and track them as they move. We demonstrate the usefulness of MooG’s learned representation both qualitatively and quantitatively by training readouts on top of the learned representation on a variety of downstream tasks. We show that MooG can provide a strong foundation for different vision tasks when compared to “on-the-grid” baselines111Project page: https://moog-paper.github.io/..","Learning visual representations of the physical world is at the core of computer vision. Recent years have seen a surge of vision models that address this problem via self-supervised learning [8, 5, 23, 40]. By leveraging objectives such as contrastive learning [8, 5] and masked image modelling [23], great strides have been made towards learning useful representations from image data. The vast majority of these methods use convolutional networks [35], vision transformers [14, 54] or a combination thereof [4]. This choice of architecture comes to no surprise, as it inherently reflects the structure of the underlying datasets: images are (typically) represented as grids of pixels, which are conveniently and efficiently processed using 2D convolutions and patch-based heuristics. This grid-based processing, however, leads to an inherent entanglement between the representation structure and image structure. In other words, specific tokens or feature vectors of the representation are encouraged to capture the contents of a specific image location, instead of binding to the underlying content of the physical scene. This issue is particularly apparent when processing video: when there is motion in the scene, either by ego-motion or object motion, the contents of the scene will move across the image plane and as such the representation (i.e. in terms of what is encoded where) will change accordingly. However, many down-stream scene understanding tasks require observing how individual objects (or object parts) change their configuration over time, even when other factors like camera motion translate the objects around the image plane. In this case, a representation that preserves correspondences between meaningful scene elements and representational elements is likely preferred. As a consequence, many works targeting object-centric tasks such as object detection [4, 36], tracking [38, 33, 25], or segmentation [34], have adopted specialized architectural components that learn object-based representations: representations that are lifted from the image grid to bind to individual objects. These representations, however, are specialized to object-centric tasks and either need to be learned with detailed supervision [4, 38, 34] or have difficulty scaling to diverse real-world raw video data [33, 19]. In this paper we propose a transformer-based video model that learns representations that are “off-the-grid” (OTG) in a self-supervised manner, providing consistent features that bind to underlying scene elements, and tracking them as they move through time. Our method, Moving Off-the-Grid (MooG), makes extensive use of cross-attention to learn a latent set of tokens that is decoupled from the image grid: tokens are updated via cross-attention when a new input frame arrives, and decoded back into images via cross-attention. MooG can process videos of arbitrary length by iteratively updating the representation as new frames are observed. In summary, our contributions are as follows: • We introduce Moving Off-the-Grid (MooG), a novel transformer-based recurrent video representation model that is capable of learning OTG representations via a simple next-frame prediction loss. • We qualitatively demonstrate that the OTG representation of MooG binds to different parts of the scene and tracks its content under motion, whereas a grid-based representation fails to do so. • Finally, we demonstrate how this representation facilitates a variety of downstream vision tasks, including point tracking, monocular depth estimation, and object tracking. Our approach outperforms self-supervised grid-based baselines, such as DINO [5, 40], and performs competitively with domain-specific approaches, such as TAP-Net [12] and TAPIR [13] for point tracking."
https://arxiv.org/html/2411.05898v1,"Integrating Object Detection Modality into 
Visual Language Model for Enhanced Autonomous Driving Agent","In this paper, we propose a novel framework for enhancing visual comprehension in autonomous driving systems by integrating visual language models (VLMs) with additional visual perception module specialised in object detection. We extend the Llama-Adapter architecture by incorporating a YOLOS-based detection network alongside the CLIP perception network, addressing limitations in object detection and localisation. Our approach introduces camera ID-separators to improve multi-view processing, crucial for comprehensive environmental awareness. Experiments on the DriveLM visual question answering challenge demonstrate significant improvements over baseline models, with enhanced performance in ChatGPT scores, BLEU scores, and CIDEr metrics, indicating closeness of model answer to ground truth. Our method represents a promising step towards more capable and interpretable autonomous driving systems. Possible safety enhancement enabled by detection modality is also discussed.","The rapid advancements in autonomous driving systems have led to an increased focus on developing end-to-end models capable of handling complex driving scenarios. Despite significant progress, current approaches still face challenges in generalisation, especially when faced with rare or unseen situations. Moreover, the ability to interact with human users and provide explanations for the model’s decisions is crucial for building trust and acceptance of autonomous vehicles. To address these challenges, the recently introduced DriveLM challenge Sima et al., (2023) aims to leverage the power of vision-language models (VLMs) and large language models (LLMs) in the context of autonomous driving. By combining the visual understanding capabilities of VLMs with the reasoning and natural language processing abilities of LLMs, DriveLM seeks to improve generalisation and enable interactive communication between autonomous vehicles and human users. Inspired by the DriveLM framework Sima et al., (2023), this paper presents a novel approach that integrates additional modalities into the LLMs to enhance its perception and reasoning capabilities for autonomous driving tasks. Our method builds upon the Llama-Adapter Zhang et al., 2023a , a parameter-efficient fine-tuning approach that allows for the incorporation of task-specific knowledge into the pre-trained LLM. The common practice for image perception capability in Llama-Adapter is to incorporate a pre-trained image embedder, specifically the CLIP Radford et al., 2021a model with trainable vision transformers (ViT) Dosovitskiy et al., (2020) to generate adaptation queries. The queries are then projected and appended onto layer-wise token embeddings. However, such a perceptual network has critical limitation, which is based on the employment of CLIP. Although CLIP is effective at capturing global contextual information, it struggles to accurately detect and locate objects in the image, as it is primarily trained on perceptual prompts rather than position-level annotations. To address this limitation, we propose the integration of a detection network into the Llama-Adapter framework. The detection network leverages pretrained YOLOS Fang et al., 2021b and postfix vision transformers to process multi-view camera inputs and generate rich feature representations that accurately capture object-specific information, such as positions and bounding boxes. Moreover, we introduced trainable ID-separator token to address confusion of object-camera relationship to concatenated YOLOS output. By incorporating the detection network, our approach enables: 1) The capability to sense local, fine-grained details in the driving scene, complementing the global understanding provided by the perceptual network. 2) Understanding of different viewpoints of BEV images via trainable ID tokens and detection-result-oriented fine-tuning. 3) enhancement of scene understanding robustness and potential defence against vulnerabilities from visual modality. We believe that a standalone detector is crucial for autonomous driving, which helps to ensure safer and more reliable decision-making by improving the capability to manage complex scenarios such as the detection of pedestrians, vehicles, and traffic signs. To evaluate the effectiveness of our approach, we conducted extensive experiments on the DriveLM challenge, comparing our model’s performance against state-of-the-art baselines. We demonstrate significant improvements in the ChatGPT score, the BLEU score, and the CIDEr score (see Sec. 4.2). The main contributions of this paper can be summarised as follows: (1) We identify the limitations of relying solely on CLIP-based features for perception in the Llama-Adapter framework and propose the integration of a detection network to overcome these challenges. (2) We leverage pretrained YOLOS and vision transformers in the detection network to accurately capture object-specific information and enhance the perceptual capabilities of the Llama-Adapter. (3) We demonstrate significant improvements in multiple matrices on overall driving performance through extensive experiments on the DriveLM challenge, showcasing the benefits of integrating the detection network."
https://arxiv.org/html/2411.05880v1,Towards Equitable ASD Diagnostics: A Comparative Study of Machine and Deep Learning Models Using Behavioral and Facial Data,"Autism Spectrum Disorder (ASD) is often underdiagnosed in females due to gender-specific symptom differences overlooked by conventional diagnostics. This study evaluates machine learning models, particularly Random Forest and convolutional neural networks, for enhancing ASD diagnosis through structured data and facial image analysis. Random Forest achieved 100% validation accuracy across datasets, highlighting its ability to manage complex relationships and reduce false negatives, which is crucial for early intervention and addressing gender biases. In image-based analysis, MobileNet outperformed the baseline CNN, achieving 87% accuracy, though a 30% validation loss suggests possible overfitting, requiring further optimization for robustness in clinical settings. Future work will emphasize hyperparameter tuning, regularization, and transfer learning. Integrating behavioral data with facial analysis could improve diagnosis for underdiagnosed groups. These findings suggest Random Forest’s high accuracy and balanced precision-recall metrics could enhance clinical workflows. MobileNet’s lightweight structure also shows promise for resource-limited environments, enabling accessible ASD screening. Addressing model explainability and clinician trust will be vital.","Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by difficulties in social communication, interaction, and behavior, typically manifesting in early childhood [1]. ASD affects approximately 1 in 36 children in the U.S., with boys diagnosed nearly 3.8 times more often than girls, leading to a gender-based diagnostic gap that frequently leaves females undiagnosed or misdiagnosed until later in life. Delays in detection can negatively impact long-term outcomes in communication, education, and mental health, contributing to significant economic costs; the annual financial burden of ASD in the U.S. was estimated at $268 billion in 2015, projected to reach $461 billion by 2025. These societal and personal impacts underscore the importance of recognizing and understanding the core symptoms of ASD, which often vary in their presentation across individuals and can complicate timely diagnosis. Fig. 1 visually represents these key characteristics, including social interaction difficulties, repetitive behaviors, restricted interests, language delays, eye contact avoidance, communication difficulties, sensory sensitivities, nonverbal challenges, emotional regulation issues, and a need for routine. Figure 1: Visual representation of the key characteristics associated with ASD, highlighting the core aspects such as social interaction difficulties, repetitive behaviors, restricted interests, language delays, eye contact avoidance, communication difficulties, sensory sensitivities, nonverbal challenges, emotional regulation, and need for routine. The icons around the central figure illustrate the common behavioral and sensory traits often associated with ASD. The underdiagnosis of ASD in females can be attributed to several factors. Diagnostic criteria for ASD, which have been largely derived from studies focusing on male populations, may not adequately capture the more subtle or socially masked manifestations of the condition seen in females. Many females engage in ”camouflaging,” a strategy involving the suppression or masking of autistic traits in social settings, making clinical detection more challenging. Furthermore, existing diagnostic tools may not be sufficiently sensitive to gender-specific presentations of ASD, resulting in significant diagnostic delays. On average, females are diagnosed with ASD later in life than males, with some remaining undiagnosed well into adulthood. Several theoretical models have attempted to explain these gender differences in autism presentation. The Female Autism Phenotype (FAP) theory suggests that females exhibit different behavioral characteristics compared to males, which may influence their diagnosis [2]. Similarly, the Female Protective Effect (FPE) posits that females require a higher genetic or environmental burden to display comparable autistic traits as males [3]. Another influential theory, the Extreme Male Brain (EMB) hypothesis, posits that ASD may exaggerate cognitive patterns typically associated with males, driven by higher androgen levels [4]. However, recent research has questioned the link between androgen levels and autism, highlighting the complexity of these theoretical frameworks. Given these challenges, there is an urgent need for diagnostic tools that are more sensitive to the unique presentation of ASD in females. While traditional clinical methods are helpful, they may not fully capture the complexity of the condition, particularly in underdiagnosed populations. Advances in machine learning (ML) offer a promising solution. ML techniques can analyze large, complex datasets to detect patterns that may elude human evaluators, making them particularly suitable for identifying subtle differences in ASD presentations across genders. Several studies have demonstrated the utility of ML in diagnosing ASD and differentiating it from other neurodevelopmental conditions, such as attention deficit hyperactivity disorder (ADHD). However, the potential for ML to specifically improve diagnosis in females has been underexplored. This study aims to address this gap by leveraging ML models to enhance the accuracy of ASD diagnosis in females. By applying convolutional neural networks (CNNs) to facial image data and traditional classifiers to behavioral data, we aim to identify gender-specific patterns that could improve early detection in females. Face detection, a subfield of computer vision, holds particular promise for this task, as facial features may differ between males and females with ASD, offering new biological insights. Although previous studies in this area have focused primarily on male subjects, this study seeks to extend these techniques to female populations, providing a novel approach to reducing the diagnostic gender gap. In summary, current clinical guidelines for ASD diagnosis, mainly based on male presentations, are inconsistently applied, which disproportionately impacts females. ML, mainly when applied to facial and behavioral data, holds the potential to provide a scalable, objective, and sensitive method for diagnosing ASD in females. By evaluating the performance of various ML models across multiple datasets, this study seeks to improve early diagnosis and address the long-standing gender disparity in ASD detection. 1.1 Research Problem ASD affects individuals across all genders, yet early diagnosis is essential for ensuring timely access to intervention, support, and tailored treatments. While early diagnosis can significantly improve long-term outcomes in communication, education, and mental health, current diagnostic tools are predominantly designed based on how ASD manifests in boys. This creates a significant gap in diagnosing females, whose symptoms may differ in presentation, often resulting in delayed diagnosis or misdiagnosis [5]. These diagnostic discrepancies prevent women from receiving appropriate early interventions, which are crucial for improving quality of life and functional outcomes. Moreover, ASD diagnostic procedures are time-consuming and complex, often requiring multidisciplinary assessments and lengthy evaluations, which further delay access to care. Although ML has emerged as a potential tool to streamline diagnosis, relying solely on ML models is insufficient to capture the full range of ASD presentations across genders. A more integrated approach—combining ML with complementary techniques—is needed to ensure that gender-specific diagnostic nuances are identified and addressed effectively. 1.2 Purpose of Study The purpose of this study is twofold: (1) to evaluate the limitations of current ASD diagnostic tools with a particular focus on their inability to detect ASD in females, and (2) to explore how face detection techniques can be applied to improve the accuracy and timeliness of ASD diagnosis across genders. We aim to investigate which facial features are most relevant for distinguishing ASD in males and females, thereby providing new insights into the biological and genetic factors that may influence ASD presentation. Specifically, this study seeks to answer the following research question: Can combining facial feature analysis with ML models enhance the accuracy of ASD diagnosis in females compared to traditional diagnostic methods? 1.3 Motivation Accurate and timely diagnosis of ASD is crucial, as failing to diagnose ASD when it is present (a false-negative) can result in missed opportunities for early intervention, which is known to improve long-term developmental outcomes significantly. Additionally, a delayed diagnosis can leave individuals and families without necessary support and access to services. On the other hand, incorrectly diagnosing ASD (a false-positive) can lead to unnecessary treatments and diagnostic procedures, increasing emotional and financial strain on families and placing additional burdens on healthcare systems. The lack of a definitive medical test for ASD further complicates the diagnostic process, requiring clinicians to rely on observed behaviors and developmental history, which may not accurately reflect the experiences of all individuals—particularly females. Many autistic women do not receive timely or accurate diagnoses, which hinders their ability to advocate for their needs and limits access to critical resources. Research has shown that receiving an ASD diagnosis later in life can significantly improve self-identity, mental health, and access to necessary accommodations. Our goal is to develop a more efficient and accurate method for identifying women who may be on the autism spectrum, enabling earlier intervention and empowering women to self-identify with greater confidence, even without a formal assessment. 1.4 Contributions This study contributes to the growing field of ASD diagnosis by proposing an innovative approach that combines ML with face detection techniques to improve diagnostic accuracy, particularly for females. While face detection has been explored in ASD research, few studies have examined its potential to capture gender-specific diagnostic features. Our approach integrates facial analysis with ML models to provide a more nuanced understanding of ASD in males and females. By comparing a baseline CNN model with a more advanced CNN architecture, we explore the trade-offs between model complexity and diagnostic performance, offering insights into which models are most effective for this task. Our contributions include: • Developing a hybrid approach using facial and behavioral data for ASD diagnosis. • Providing a comparative analysis of different CNN models for ASD classification. • Highlighting the importance of biological and genetic factors in understanding ASD manifestation across genders. Form the audience perspective, this research is designed for ML specialists, biologists, geneticists, and clinical practitioners interested in the intersection of technology and ASD diagnosis. For ML experts, the study provides a case study of how advanced models like CNNs can be applied to diagnostic challenges. Biologists and geneticists may find the focus on facial feature analysis and its potential links to ASD informative for their research on the biological basis of neurodevelopmental disorders. Clinicians and medical professionals will benefit from insights into how ML can support gender-sensitive diagnostic practices, particularly for underdiagnosed populations such as women. 1.5 Paper Organization The remainder of this paper is organized as follows: Section 2 reviews the related literature on ASD diagnosis and the use of ML in healthcare. Section 3 presents our proposed method, combining ML and face detection techniques. Section 4 describes the experimental setup, including datasets and model configurations. Section 5 discusses the results of our experiments, and Section 6 concludes the paper with final thoughts and suggestions for future research."
https://arxiv.org/html/2411.05877v1,Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass,"Large language models (LMs) are typically adapted to improve performance on new contexts (e.g., text prompts that define new tasks or domains) through fine-tuning or prompting. However, there is an accuracy compute tradeoff—fine-tuning incurs significant training cost and prompting increases inference overhead. We introduce GenerativeAdapter, an effective and efficient adaptation method that directly maps new contexts to low-rank LM adapters, thereby significantly reducing inference overhead with no need for finetuning. The adapter generator is trained via self-supervised learning, and can be used to adapt a single frozen LM for any new task simply by mapping the associated task or domain context to a new adapter. We apply GenerativeAdapter to two pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the adapted models in three adaption scenarios: knowledge acquisition from documents, learning from demonstrations, and personalization for users. In StreamingQA, our approach is effective in injecting knowledge into the LM’s parameters, achieving a 63.5% improvement in F1 score over the model with supervised fine-tuning (from 19.5 to 31.5) for contexts as long as 32K tokens. In the MetaICL in-context learning evaluation, our method achieves an average accuracy of 44.9 across 26 tasks, outperforming the base model. On MSC, our method proves to be highly competitive in memorizing user information from conversations with a 4x reduction in computation and memory costs compared to prompting with full conversation history. Together, these results suggest that GenerativeAdapter should allow for general adaption to a wide range of different contexts.","Adaptation is essential for language models (LMs) to acquire new world knowledge (Jiang et al., 2024; Hu et al., 2023; Mecklenburg et al., 2024), learn new tasks (Min et al., 2022), and personalize to individual users (Salemi et al., 2024). Existing adaptation methods typically involve either prompting or fine-tuning (Brown et al., 2020). As the scale of LMs continues to increase, adapting them becomes increasingly difficult due to efficiency constraints during both training and inference (Hu et al., 2022). Prompting with task-specific demonstrations (i.e., in-context learning (Brown et al., 2020)) or background knowledge (i.e., retrieval-augmented generation (Lewis et al., 2020)) is one way to enable models to temporarily encode such relevant information, allowing flexible adaptation to various tasks. However, to maintain additional memory across sessions, some extra prompts must be added to the input, which incur an inference-time or storage overhead (Chevalier et al., 2023). Fine-tuning is another way to embed new information into the LM’s parameters, retaining long-term memory. Nevertheless, it requires a training phase that is more computationally expensive than a single forward pass, and acquiring knowledge through continual pretraining has shown to be data-inefficient (Yang et al., 2024; Allen-Zhu & Li, 2024). Thus, we are interested in exploring alternative approaches for effectively and efficiently adapting pretrained LMs. In this work, we present GenerativeAdapter, a novel method for training a neural network (adapter generator) to generate adapters that contextualize pretrained LMs on-the-fly with temporary knowledge from incoming contexts. Inspired by fast weights (Ba et al., 2016; Schmidhuber, 1992, inter alia), our approach incorporates a lightweight adapter generator on top of pretrained LM as the slow network to produce updated parameters for the fast network (the adapted LM). As far as we know, we are the first to explore this direction. Specifically, the pretrained base LM remains frozen while we train the LM-specific adapter generator to generate layer-by-layer additive updates, similar to recent parameter-efficient fine-tuning (PEFT) techniques (Houlsby et al., 2019; Hu et al., 2022). For each layer, a adapter generator network uses the outer product of past context hidden states from the corresponding base LM layer to generate delta weights. These generated delta weights are then added to the base LM weights to form an adapted LM for future predictions. Similar to previous work on fast weights, our method achieves test-time adaptation using only forward passes, allowing dynamic updates as new context arrives in sequential chunks. We train the generator end-to-end in a self-supervised manner by compressing the context into a generated adapter and then computing the next-token prediction loss on a target sequence using the adapted LM. Once trained, our method can produce adapted LMs that effectively capture knowledge from the context to solve multiple downstream tasks, thus improving the adaptability of off-the-shelf pretrained LMs. We evaluate our method on three scenarios where on-the-fly contextualizing pretrained LMs is crucial: acquiring new factual knowledge, learning from demonstrations, and personalizing for individual users. These scenarios involve diverse forms of context with varying lengths, including documents with background knowledge, task-specific input-output examples and user-specific conversations. In the knowledge acquisition scenario, GenerativeAdapter effectively memorizes factual knowledge from provided documents, with minimal information loss compared to full-context prompting at short context lengths. Notably, our method excels in memorizing long-context documents, managing to handle context lengths up to 32K on StreamingQA (Liska et al., 2022) and 8K on SQuAD (Rajpurkar et al., 2016) better than continous pretraining. In learning from demonstrations on MetaICL, GenerativeAdapter follows demonstrations effectively, achieving superior accuracy compared to the in-context learning of its base model. This exemplifies the model’s ability to adapt to new tasks efficiently. For personalization, GenerativeAdapter is highly effective in retaining user information from conversations, achieving a fourfold reduction in computation and memory costs compared to full conversation prompting. In practical scenarios with many queries from the same user on edge computing devices, the benefits of our method are even more evident. This positions GenerativeAdapter as a highly efficient tool for personalized LMs. Our contributions are summarized as follows: 1. We introduce GenerativeAdapter, a novel method for efficiently adapting pretrained LMs on-the-fly using test-time contexts. To our knowledge, we are the first to explore retaining the relevant temporary knowledge through generated parameter-efficient model updates for state-of-the-art pretrained LMs. 2. We develop an adapter generator network on top of frozen pretrained LMs to transform text contexts into updated model parameters (adapted LMs) for future queries. We also design an efficient end-to-end training process to enhance the LMs’ adaptability, i.e., the resulting generator augmented LM can be used for various downstream tasks using only forward passes. 3. We validate the proposed method on two representative pretrained LMs. Empirically, we show the effectiveness of GenerativeAdapter in various adaptation scenarios, including knowledge acquisition from documents, learning from demonstrations, and personalized user interactions. Our method proves to be generalizable across different types of contexts and applicable to multiple downstream tasks."
https://arxiv.org/html/2411.05875v1,"Towards Improved Preference Optimization Pipeline: 
from Data Generation to Budget-Controlled Regularization","Direct Preference Optimization (DPO) and its variants have become the de facto standards for aligning large language models (LLMs) with human preferences or specific goals. However, DPO requires high-quality preference data and suffers from unstable preference optimization. In this work, we aim to improve the preference optimization pipeline by taking a closer look at preference data generation and training regularization techniques. For preference data generation, we demonstrate that existing scoring-based reward models produce unsatisfactory preference data and perform poorly on out-of-distribution tasks. This significantly impacts the LLM alignment performance when using these data for preference tuning. To ensure high-quality preference data generation, we propose an iterative pairwise ranking mechanism that derives preference ranking of completions using pairwise comparison signals. For training regularization, we observe that preference optimization tends to achieve better convergence when the LLM predicted likelihood of preferred samples gets slightly reduced. However, the widely used supervised next-word prediction regularization strictly prevents any likelihood reduction of preferred samples. This observation motivates our design of a budget-controlled regularization formulation. Empirically we show that combining the two designs leads to aligned models that surpass existing SOTA across two popular benchmarks.","Recently, Direct Preference Optimization (DPO) (Rafailov et al., 2024) and its variants (Meng et al., 2024; Azar et al., 2024; Ethayarajh et al., 2024; Liu et al., 2024; Pal et al., 2024; Xu et al., 2024) have gained popularity over traditional reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019), which involves training a reward model followed by reinforcement learning. DPO-based methods bypass the need for a reward model in optimization by directly optimizing the target model using preference data, leading to simpler and more efficient training. The pipeline of DPO (and its variants) consists of two key stages: (1) collecting preference data by scoring various outputs generated by the target LLM model, and (2) performing direct optimization using the preference data. The first stage of constructing preference data involves two steps: (1) the target model generates multiple completions for each input prompt; (2) then a reward model selects preferred and dispreferred completions from these candidates for each prompt (Xiong et al., 2024; Meng et al., 2024). Existing open-sourced reward models are mostly based on a classification architecture by modifying the last layer of a LLM (Liu and Zeng, 2024; Wang et al., 2024b, a). This scoring-based approach for evaluating the quality of a prompt-completion pair introduces considerable noise (Cui et al., 2023; Ganguli et al., 2022; Guo et al., 2024), and the issue becomes even more when the downstream task is out-of-distribution compared to the training data used to construct the reward model. After constructing high-quality preference data, standard preference optimization algorithms compute the relative probability of selecting one completion over another by using pairs of preferred and dispreferred completions (Rafailov et al., 2024; Meng et al., 2024; Azar et al., 2024). Optimizing towards this relative objective can potentially lead to a reduction of target model’s predicted likelihood of the preferred completions, as long as the relative probability between the preferred and dispreferred completions increases with the preference optimization. This may cause training instability issue (Pal et al., 2024; Feng et al., 2024; Liu et al., 2024). To address the challenge, several regularization techniques have been proposed to utilize supervised next-word prediction of the preferred examples. While these techniques effectively improve training stability, our empirical findings show that models trained using these regularization methods perform worse compared to those trained without such regularization. Figure 1: Overview for DPO pipeline. Preference data generation: existing scoring-based methods select preferred and dispreferred completions based on a single score, our proposed iterative pairwise ranking uses pairwise comparison signals to construct preference data. Regularizing preference optimization: we propose a budget-controlled regularization that balances training stability and model alignment performance. In this paper, we aim to improve the preference optimization pipeline. Our work introduces both high-quality preference data generation and improved regularization techniques to address the above limitations. Shown in Fig. 1, we first propose an iterative pairwise ranking method to construct high-quality preference data. Then we use this dataset to train a model with standard preference optimization objective augmented with a novel budget-controlled regularization. The contributions of this work are as follows: • We introduce an iterative pairwise ranking mechanism that employs pairwise comparison signals to construct preference data. Specifically, given multiple completions for an input prompt, an LLM judge sequentially compares the previous winning completion with the next candidate until an optimal completion is found. Empirical results demonstrate that preference data generated by our method consistently surpasses existing for both in-domain and out-of-distribution tasks. • We study the effects of supervised next-word prediction regularization and reveal that while this technique prevents significant reductions in target model’s predicted likelihood of preferred examples, preference optimization tends to achieve better results when the likelihood of both preferred and dispreferred examples are slightly reduced. This observation leads to a novel budget-controlled regularization we propose, which controls the amount of reduction on target model’s predicted likelihood of preferred completions. • We demonstrate that integrating the above two designs yields preference aligned models that outperform the current SOTA across two widely-adopted benchmark evaluations."
https://arxiv.org/html/2411.05874v1,Interplay between Federated Learning and Explainable Artificial Intelligence: a Scoping Review,"The joint implementation of Federated learning (FL) and Explainable artificial intelligence (XAI) will allow training models from distributed data and explaining their inner workings while preserving important aspects of privacy. Towards establishing the benefits and tensions associated with their interplay, this scoping review maps those publications that jointly deal with FL and XAI, focusing on publications where an interplay between FL and model interpretability or post-hoc explanations was found.In total, 37 studies met our criteria, with more papers focusing on explanation methods (mainly feature relevance) than on interpretability (mainly algorithmic transparency). Most works used simulated horizontal FL setups involving 10 or fewer data centers. Only one study explicitly and quantitatively analyzed the influence of FL on model explanations, revealing a significant research gap. Aggregation of interpretability metrics across FL nodes created generalized global insights at the expense of node-specific patterns being diluted. 8 papers addressed the benefits of incorporating explanation methods as a component of the FL algorithm.Studies using established FL libraries or following reporting guidelines are a minority. More quantitative research and structured, transparent practices are needed to fully understand their mutual impact and under which conditions it happens.","Figure 1: Schematic overview of a main research question: do explanations differ between federated models and centralized models? The development of trustworthy AI systems depends on multiple different ethical principles like privacy preservation and explicability [1, 2]. Ensuring that AI systems address such principles is crucial for maintaining trust, compliance, and ethical standards. Such principles are critical in data-sensitive applications such as banking [3] and healthcare [4, 5]. Privacy concerns are the main reason why healthcare data for AI models, such as patient outcome prognosis, mainly originate from single institutions. Anonymizing extensive healthcare data effectively is challenging due to the high risk of re-identifying patients [6]. Allowing models to be trained across multiple institutions without sharing raw data, for example, with federated learning (FL), addresses this issue. FL has gained prominence as a solution to train models from distributed data while preserving these privacy aspects [7, 8]. Specifically, FL enables the training of machine learning (ML) models from distributed data that are available to different users or institutions, without data being explicitly transferred. Therefore, FL solves certain privacy-related governance issues as it restricts access to sensitive information from each institution while still gaining insights from all available data [9, 10]. Using data from different institutions can lead to more diverse training data, which reduces the bias of ML models and improves generalizability compared to models trained on homogeneous data [11]. The three main categories within FL are horizontal FL (HFL), vertical FL (VFL), and federated transfer learning (FTL). HFL involves data scattered across the sample space, where multiple parties have different subsets of data points but all data have the same feature space. VFL allows learning from data distributed across different different feature spaces for the same data sample. FTL focuses on transferring knowledge across different federated learning settings [12]. Another ethical principle often advocated by ethicists and policymakers in high-risk AI domains such as healthcare is explicability. This means that: a) processes must be transparent, b) the capabilities and purpose of AI systems must be openly communicated, and c) one must be able to explain the decisions and predictions made by AI systems to those directly or indirectly affected by them [1, 2]. When a model’s inference or decision-making process is transparent and understandable, it is more likely to be trusted and accepted by stakeholders, such as customers, regulators, and users [13]. As stated by the European Union’s regulations, aspects (a) and (c) of explicability, whose technical manifestation is known as explainability [14], need to be provided through additional information — next to the performance of the AI system — on how an AI system arrives at a certain output [15]. The concept of explainable artificial intelligence (XAI) aims to explain the inner workings of ML algorithms and increasing model comprehension. XAI is thereby concerned with the development of algorithms and methods that achieve explainability [16]. Explicability can be achieved by either training interpretable ML models, or by using post-hoc explanation methods (i.e., (a) and (c) respectively regarding the European Union guideline). Unfortunately, XAI literature has not agreed on one definition of explainability. The terms “explainability”, “explicability”, and “interpretability” are often used interchangeably [17]. In this work, we adopt the definitions provided by [15], which include interpretable modeling and explanation methods, as detailed in the following paragraphs. An ML model is said to be interpretable if its decision process can be inherently and intuitively understood by the intended user [15]. The word ‘inherently‘ is crucial in this definition, as interpretability of a given model is defined as a passive property that reflects the level at which such a model makes sense for a human observer [18] without the need to perform extra computation. Examples of interpretable ML models are linear and logistic regression models, where variable importance can be inferred by investigating model weights, or decision trees, which can be intuitively followed by humans [15]. In contrast, post-hoc explanation methods try to explain non-interpretable models actively by performing additional computations with the intent of clarifying or detailing their internal functions [18, 15]. Such computations include evaluating or calculating gradients of the original model for a set of differently perturbed inputs [19]. With that, AI systems can calculate the importance of features [20], the dependence on feature variations of the output [21], or the minimal input change that causes a label flip [22]. An explanation method, therefore, is a comprehensible interface between humans and a decision maker while simultaneously providing approximations of the decision-maker [23]. Extensive research has been conducted on FL and XAI separately. Despite the importance of their combined use for creating trustworthy AI systems, it remains unclear whether these two technologies complement each other, present conflicting requirements, or can be addressed separately. FL and XAI coexist in many works, but few works discuss the impact of one on the other. Furthermore, the existing literature inadequately addresses the methodologies, challenges, and outcomes related to the integration of FL and XAI, leaving a critical gap in understanding their mutual influence. On one hand, XAI can mitigate the risks associated with training ML systems in federated settings. Learning from heterogeneous data can inadvertently lead to spurious correlations learned by the ML model, as the decentralized data might not represent the overall distribution and contain biases specific to subsets of the data. Explainability can help practitioners determine whether the model’s predictions are based on valid patterns rather than misleading correlations, not propagating undesired biases and, therefore, adhering to ethical standards. Additionally, explainability can help identify malicious agents trying to poison the FL process. On the other hand, the potential risks associated with the interplay between these technologies must be documented. There is no unified effort to determine, for example, whether FL leads to reduced interpretability or less accurate explanations. Also, explanations could make the FL network more vulnerable to attacks. Moreover, preliminary research [24] indicates differences in model explanations between FL and traditional centralized models, suggesting unique challenges in applying XAI in FL contexts. This underscores the need for further research on how FL influences the contribution of input variables and the effectiveness of XAI methods. To understand the interplay between FL and XAI, it is crucial to consider the findings, setups, and conditions under which these studies have been conducted. This scoping review aims to map both experimental and methodological studies that examine the interaction between FL and XAI. Since FL can potentially influence both model interpretability and post-hoc explanations, this review paper investigates both concepts of XAI. It focuses on research articles that either propose FL methods incorporating explainability, discuss practical applications where FL and XAI are jointly used, or analyze the interplay between these technologies through empirical studies. Given the fragmented and emerging nature of research at this intersection, this review provides a comprehensive overview of current work, identifies key concepts and evidence types, and highlights gaps to guide future research efforts. The remainder of this paper is structured as follows. Sec. II introduces the related work. Sec. III describes the methodology we followed. A more detailed analysis of the interplay between FL and XAI found in the reviewed papers is given in Sec. IV. A quantitative analysis of the extracted information is shown in Sec. V. The results are discussed in Sec. VI, and Sec. VII concludes the paper. The Appendix contains summarized information about how all included papers jointly with FL and XAI."
https://arxiv.org/html/2411.05867v1,Modeling Nonlinear Oscillator Networks Using Physics-Informed Hybrid Reservoir Computing,"Surrogate modeling of non-linear oscillator networks remains challenging due to discrepancies between simplified analytical models and real-world complexity. To bridge this gap, we investigate hybrid reservoir computing, combining reservoir computing with “expert” analytical models. Simulating the absence of an exact model, we first test the surrogate models with parameter errors in their expert model. Second, we assess their performance when their expert model lacks key non-linear coupling terms present in an extended ground-truth model. We focus on short-term forecasting across diverse dynamical regimes, evaluating the use of these surrogates for control applications. We show that hybrid reservoir computers generally outperform standard reservoir computers and exhibit greater robustness to parameter tuning. Notably, unlike standard reservoir computers, the performance of the hybrid does not degrade when crossing an observed spectral radius threshold. Furthermore, there is good performance for dynamical regimes not accessible to the expert model, demonstrating the contribution of the reservoir.","Networks of oscillators appear widely across engineering, and in both the physical and biological sciences. When the networks are non-linear their dynamical behavior can be complex, displaying synchronization, chaos, and traveling waves [1, 2, 3]. Creating analytical or data-driven models that can predict their dynamics is important in applications, for example in producing surrogate models. Downstream applications for a surrogate model of non-linear oscillator networks (NLONs) include smart electrical grid optimization [4, 5, 6], biological computing [7], synthetic biology [8, 9] and the diagnosis and treatment of neurological disorders such as epilepsy [10] and Parkinson’s disease [11, 12, 13, 14]. These surrogate models have two broad applications: parameter inference and control. Parameter inference can help identify critical states and associated parameter values within a system, ideally also exposing underlying mechanistic processes. Control using surrogate models aims to exploit system knowledge to improve control performance. Methods such as model predictive control [15] and model-based reinforcement learning [16] are examples. In this paper, we focus on hybrid reservoir computing (RC), a specific form of physics-informed machine learning (PIML). In particular, with a view to probing its viability for control applications, we investigate how well hybrid RC performs surrogate modeling of NLONs. Real NLONs are often high dimensional, partially observable, noisy, and involve complex intra-network interactions. As such, it can be extremely difficult to create accurate surrogate models. This challenging task may be approached in several ways. The classical approach is direct physics-based modeling, where mechanisms are pre-ordained and parameters fit to data. Machine learning (ML), or data-driven modeling, is an alternative approach which uses fully parameterized models and with parameters updated using learning algorithms. These two contrasting methods confer distinct benefits and drawbacks. Physics-based models are physically accurate within the bounds of the assumptions made in their construction. Similarly, within the domain of their training data, data-driven models perform well, however physical accuracy is not enforced. Failure when predicting out-of-domain is therefore common to the two approaches. Out-of-domain failure is guaranteed for physics-based models as there is no scope for adapting their structure or parameters. ML models however, can be updated online using new data to adapt to new situations, although this is in itself a challenging problem [17]. Physics-based models are inherently interpretable: each term generally has some understood physical meaning, or represents some physical laws or constraints. On the other hand, ML discards this prior knowledge in favor of complete parameterisation from observed data. The correspondence between the parameters and the physical system can be difficult to discern. When data-driven models have many parameters, they require large amounts of data. Physics-based models tend to have few parameters that need fitting to data, and therefore generally require less. Both methods can be computationally expensive as physics-based models rely on complex numerical schemes, and data-driven models often need to use extensive training algorithms for parameter updates. PIML is a recently-formulated approach for surrogate modeling and prediction applications [18]. It combines both physics-based and data-driven methods, in an attempt to make use of the best features of each approach. Its goal is to obtain physically constrained, robust, and interpretable models that capture both expert knowledge of dynamical processes, and the information that can be extracted from data obtained from sensing and recording devices. PIML models promise to be more data efficient as they do not require all of the dynamics to be learned from scratch, and they may also facilitate adaptivity through the use of machine learning. PIML-based control for NLONs may thus result in more robust, efficient and accurate controllers that are adaptable and generalisable. For PIML-based modeling of dynamical systems, it is natural to consider ML components with a time component or sequential nature. For instance, recurrent neural networks (RNNs) and their variants (LSTMs [19], GRUs [20]) are a common choice [21, 22, 23]. However, RC [24, 25] is a particularly promising alternative, due to its small size and simple training procedure. The simplicity of an RC may also offer a unique benefit for PIML parameter inference; while a large RNN can learn, or over learn, a complete model of the data without any input from the physics-based component, an RC has a limited capacity which may force the model to use the physics-based component, making it more interpretable. The small number of parameters used by an RC may also further enhance the low-data requirement conferred by the use of system knowledge. RCs are a restricted form of recurrent neural network (RNN), where learning only takes place in an external readout layer. Sequential data is passed into the reservoir via a fixed, random input weight matrix. An update rule then acts as a discrete non-linear map upon the internal state stored within the activations of the reservoir nodes. The result is a high-dimensional non-linear filter of the incoming data with a fading memory of past states. Scaling the weights of the input matrix and internal connectivity controls the extent to which past-state information is maintained in the hidden state and how much influence is exerted by the input data. To compute an output from the reservoir’s internal state, an output weight matrix is trained, often using regularized linear regression. The readout can be trained to perform classification of the reservoir state, and thus the input sequence, or regression to predict numerical features. The readout layer may also be trained to perform n-step-ahead prediction. When configured to predict the next step in a sequence, the reservoir may be run autoregressively with its output fed back in as the next input instance and thus used for time-series forecasting. This is the format we are considering here: using RCs for the prediction of dynamical system trajectories. The main advantage of RCs over more complex RNNs is their ease of initialization and simplicity of training. Good time-series forecasting performance can be achieved using only linear regression, even when chaotic dynamics are present [26], and issues such as the vanishing gradient problem are avoided. Since they require only general non-linear high-dimensional filtering of inputs and a fading memory of past inputs, RCs can also be constructed in a wide range of physical substrates [27, 28]. Recently, a PIML variant of an echo state network RC was proposed[29]. In hybrid RC (Fig. 1), the prediction from a standard RC is augmented by a single-step integration of an expert ordinary differential equation (ODE) model of the system being predicted. The next step prediction of the ODE model is passed into the reservoir alongside the current state. It is also passed around the reservoir to be considered by the output weight matrix on its own merit, during training and inference. The output weight matrix, still the only trained component, thus aims to combine the augmented reservoir state with the ODE model prediction to most accurately predict the next state. This approach allows the reservoir to compensate for errors in the expert ODE model, and has been shown to result in superior performance when compared to models that use only one of the two components, that is, a standard RC or an expert ODE model. CurrentExpertReservoirLinearPrediction Figure 1: A cartoon diagram of the hybrid RC. The current observed state acts as input for both the expert model and the reservoir; the reservoir also receives input from the expert model. The outputs from both the reservoir and expert model form the input to the linear regression layer, whose output maps to a prediction. The only tunable parameters in the model are in the regression layer. In particular, the hybrid RC was used to predict the dynamics of the Lorenz and Kuramoto-Sivashinsky systems when incorporating a model of each system with parameter error. With the correct model structure, the hybrid RC was shown to perform well, better than either a standard RC or ODE model in isolation. The hybrid RC also maintained good performance when, under particular parameter settings, the standard RC and ODE model performed poorly [29]. To investigate the potential of hybrid RCs for the novel example of NLON prediction and control — where the ground-truth dynamics is more complex, or the expert model is further from the true system than in previous work[29] — we evaluated their performance on two tasks: parameter error and residual physics. • Parameter error, the first, simpler, task tests how well a hybrid RC predicts the trajectory of a network of standard Kuramoto oscillators, when the parameters in the hybrid RC model do not correctly match the parameters of the ground truth model. This follows the previous evaluation but with the Kuramoto oscillator network replacing the Lorenz system [29]. The test is run across a range of hyperparameters to assess performance robustness to tuning, and across three qualitatively different dynamical regimes. • Residual physics, our second task, is more challenging. We measure the short term prediction performance of the hybrid RC when the hybrid RC uses a simpler model than the ground truth. This is intended to mimic real-world examples where the complex interactions of an oscillating system are unknown. In these examples a simpler, approximate model, is often used, but even small non-linear terms can quickly make predictions inaccurate. Our interest is in whether the reservoir component of the hybrid RC can compensate for the over-simplification of the model. In our particular implementation, the residual physics is an additional higher harmonic in the coupling term for the ground truth Kuramoto-like system: we give the hybrid RC the standard Kuramoto model without this addition. This bi-harmonic Kuramoto model [30] produces behaviors not accessible to the original Kuramoto model. For example, when clustering of the oscillators around a phase occurs in the standard Kuramoto model, there is only one cluster; with an extra harmonic term this need not be true. The residual physics task aims to replicate realistic control scenarios with incomplete knowledge of the system structure. This is an interesting challenge for the hybrid RC since exact knowledge of the ground truth non-linearities has previously been identified as being crucial for Kuramoto oscillator network attractor reconstruction when using the Next Generation Reservoir Computer [31, 32]. In a similar fashion to the parameter error task, we run this test across a range of hyper parameters, with four qualitatively different dynamical regimes. We then use this to inform a demonstrative grid-search optimization process simulating the development of a surrogate model for control applications."
https://arxiv.org/html/2411.05859v1,Enhancing Financial Fraud Detection with Human-in-the-Loop Feedback and Feedback Propagation,"Human-in-the-loop (HITL) feedback mechanisms can significantly enhance machine learning models, particularly in financial fraud detection, where fraud patterns change rapidly, and fraudulent nodes are sparse. Even small amounts of feedback from Subject Matter Experts (SMEs) can notably boost model performance. This paper examines the impact of HITL feedback on both traditional and advanced techniques using proprietary and publicly available datasets. Our results show that HITL feedback improves model accuracy, with graph-based techniques benefiting the most. We also introduce a novel feedback propagation method that extends feedback across the dataset, further enhancing detection accuracy. By leveraging human expertise, this approach addresses challenges related to evolving fraud patterns, data sparsity, and model interpretability, ultimately improving model robustness and streamlining the annotation process.","Financial fraud detection is essential for maintaining the security and integrity of financial systems. As fraud techniques become more sophisticated, traditional detection methods struggle to effectively identify and prevent fraud. Machine learning (ML) techniques have emerged as powerful tools, leveraging large datasets to detect patterns and anomalies indicative of fraud. However, these systems face challenges such as the need for extensive labeled data, the dynamic nature of fraud, and the complexity of domain-specific knowledge. Human-in-the-loop (HITL) feedback mechanisms offer a promising solution to these challenges by incorporating human expertise into the ML process. HITL involves active human participation in the machine learning pipeline, providing critical insights, annotations, and feedback to enhance model performance. It addresses key issues such as limited labeled data, model interpretability, and adapting to evolving fraud patterns. In financial fraud detection, HITL systems leverage domain knowledge to identify subtle patterns that automated models might overlook. This allows for more accurate model training and validation, reducing false positives and ensuring better fraud detection. Fraud detection presents several challenges ideal for HITL, including imbalanced datasets, adversarial fraudsters, and complex fraud patterns. Fraudsters continually adapt to evade detection, creating an environment where models must be updated frequently. Graph-based approaches, which model transactions as networks, have shown promise in capturing complex fraud patterns but require expert optimization. In this paper, we introduce a HITL framework for financial fraud detection, combining human expertise with advanced ML techniques. Our approach incorporates annotation from proprietary and public datasets, interactive model training, and a novel feedback propagation algorithm. We evaluate the impact of HITL feedback using standard metrics, demonstrating improvements in detection accuracy, robustness, and interpretability. By integrating HITL into fraud detection systems, we improve data annotation, model interpretability, and adaptability to dynamic fraud patterns. Our proposed framework combines advanced ML techniques with a novel feedback propagation method, significantly enhancing fraud detection performance across various algorithms. This research highlights the potential of HITL to improve both traditional and state-of-the-art methods in financial fraud detection while introducing a novel technique for propagating feedback signals throughout the dataset."
https://arxiv.org/html/2411.05856v1,Evaluating the Economic Implications of Using Machine Learning in Clinical Psychiatry,"With the growing interest in using AI and machine learning (ML) in medicine, there is an increasing number of literature covering the application and ethics of using AI and ML in areas of medicine such as clinical psychiatry. The problem is that there is little literature covering the economic aspects associated with using ML in clinical psychiatry. This study addresses this gap by specifically studying the economic implications of using ML in clinical psychiatry. In this paper, we evaluate the economic implications of using ML in clinical psychiatry through using three problem-oriented case studies, literature on economics, socioeconomic and medical AI, and two types of health economic evaluations. In addition, we provide details on fairness, legal, ethics and other considerations for ML in clinical psychiatry.","With the success of artificial intelligence (AI) and machine learning (ML) within areas such as transportation and finance, there is an increasing interest in using those within areas of medicine. One of the areas of interest is psychiatry. There is a growing interest in applying ML in clinical practice within psychiatry, and more recently, research is being conducted to understand its effectiveness in psychiatry. However, no research has investigated the economic implications associated with its use. Our study addresses this gap by evaluating the economic implications of using ML in clinical psychiatry. This paper will first review the current situation of clinical psychiatry and economics, and the economic, socioeconomic, and medical incentives for ML in clinical psychiatry. Then, using three cases studies, we will evaluate the economic implications of using ML in clinical psychiatry. We will conclude by discussing ethical, legal and other considerations for ML in clinical psychiatry."
https://arxiv.org/html/2411.05849v1,Input-Driven Dynamics for Robust Memory Retrieval in Hopfield Networks,"The Hopfield model provides a mathematically idealized yet insightful framework for understanding the mechanisms of memory storage and retrieval in the human brain. This model has inspired four decades of extensive research on learning and retrieval dynamics, capacity estimates, and sequential transitions among memories. Notably, the role and impact of external inputs has been largely underexplored, from their effects on neural dynamics to how they facilitate effective memory retrieval. To bridge this gap, we propose a novel dynamical system framework in which the external input directly influences the neural synapses and shapes the energy landscape of the Hopfield model. This plasticity-based mechanism provides a clear energetic interpretation of the memory retrieval process and proves effective at correctly classifying highly mixed inputs. Furthermore, we integrate this model within the framework of modern Hopfield architectures, using this connection to elucidate how current and past information are combined during the retrieval process. Finally, we embed both the classic and the new model in an environment disrupted by noise and compare their robustness during memory retrieval.","Since the beginning of the 80s, the words “Associative Memory Network"" have closely echoed with “Hopfield Network"" (Hopfield, 1982, 1984), and a plethora of subsequent works have endeavored to provide a detailed picture of the properties of such networks (Amit et al., 1987a; Crisanti et al., 1986; Treves and Amit, 1988). Drawing from the toolbox of statistical mechanics, Hopfield networks provided a convincing explanation for the multi-stability of memories as function of the neurons couplings, and therefore a plausible, dynamic retrieval mechanism over an energy landscape. Recently, in a machine-learning driven Renaissance for associative memory networks, the original framework has been generalized to higher order interactions (Krotov and Hopfield, 2016) and to multi-layered architectures (Krotov and Hopfield, 2020; Chaudhry et al., 2023), thus endowing the model with both a significantly improved capacity (Demircigil et al., 2017) and a direct bridge to state-of-the-art transformer models and their attention mechanism (Hubert et al., 2021). Moreover, the new framework has paved the way for new hypotheses on how neurons and astrocytes could interact (Kozachkov et al., 2023), at the functional level, to support cognitive processes. The effort to bridge formal approaches and neuroscience is of paramount importance for the advancement of both fields. As proposed in (Treves and Rolls, 1992), attractor dynamics may be a key component of hippocampal functioning, where the signal relayed by cortical areas is sparsified and orthogonalized in the CA3-CA1 regions (Yassa and Stark, 2011; Rolls, 2013). In addition, simple attractor models provide a viable tool to study global cortical dynamics in the brain (Russo and Treves, 2012; Naim et al., 2018), by partitioning the surface in interacting patches of cortex each idealized by Hopfield like networks. In classic treatments on computational neuroscience (Amit, 1989; Dayan and Abbott, 2005; Gerstner et al., 2014), memory retrieval in the Hopfield model is implicitly described as a two-step process. First, a noisy or incomplete input is presented as a cue and adopted as an initial condition. Then, driven by an energy landscape, the network state flows towards the closest energy minimum representing the prototypical memory. The literature however lacks an explanation for how an external input becomes an initial condition in the neural dynamics; it is worth noting that external inputs and initial conditions play distinct roles in the behavior of a dynamical system. Most importantly, while this classic two-step process is natural within an algorithmic paradigm, it fails to explain how neuronal circuits continuously react and adapt in real time to external inputs. In light of these limitations, we advocate for a paradigm shift from a two-step mechanism, akin to a standard algorithmic approach, to a input-driven dynamic mechanism, aligned with the principles of online algorithms and continual learning (Zenke et al., 2017; Hadsell et al., 2020; Lesort et al., 2020). To this extent, we propose a novel version of the Hopfield model that is driven by external inputs. A key feature of this model is that the input shapes the energy landscape and affects the resulting gradient descent flow (see Fig. 1). Furthermore, our model admits a simple representation as a modern Hopfield network (Krotov and Hopfield, 2020; Hubert et al., 2021); this representation provides a conceptual bridge with the recent literature on transformer models and machine learning. Finally, the addition of noise reveals the advantageous integration of past and present information by our model, thereby reducing misclassification errors induced by inconsistent or ‘glitchy’ inputs. Figure 1: Comparison between classic Hopfield and IDP Hopfield models. (a) A slowly morphing sequence of noisy images is presented as a input to the observer, who updates its belief state to retrieve the memory closest to the current image u. This adaptation process occurs continuously. (b) In the classic model, the network state is set to an initial condition x(0) equal to the current image u and then the Hopfield dynamics performs the memory retrieval task. (c) In the proposed input-driven plasticity model, the network initial condition is arbitrary, the image u modifies the synaptic weights W(u), and the Hopfield dynamics with modified synaptic weights performs the memory retrieval task. This dynamics is well posed and naturally tracks the morphing images also when the image is time-varying u=u(t). (d) In the classic model, the Hopfield dynamics is a gradient descent for the energy \mathrm{E}(x;W): the blue ball, representing the neural state, rolls from an initial condition towards a stable minimum point (cat memory). Therefore, the retrieval process is successful when the noisy image x(0)=u (dotted cat) lies in the region of attraction of the correct memory (cat memory). (e) In the proposed model, the noisy image u directly modifies the synaptic matrix W(u) and in turn the energy landscape \mathrm{E}(x;W(u)), thereby extending the region of attraction of the correct memory. The retrieval process is successful from generic initial conditions when the correct memory is the unique minimum of the landscape. Specifically, in the panel, the noisy image (dotted cat) renders the correct memory a minimum (cat memory) and the incorrect memory (dog memory) no longer an equilibrium of \mathrm{E}(x;W(u))."
https://arxiv.org/html/2411.05847v1,Federated Data-Driven Kalman Filtering for State Estimation,"This paper proposes a novel localization framework based on collaborative training or federated learning paradigm, for highly accurate localization of autonomous vehicles. More specifically, we build on the standard approach of KalmanNet, a recurrent neural network aiming to estimate the underlying system uncertainty of traditional Extended Kalman Filtering, and reformulate it by the adapt-then-combine concept to FedKalmanNet. The latter is trained in a distributed manner by a group of vehicles (or clients), with local training datasets consisting of vehicular location and velocity measurements, through a global server aggregation operation. The FedKalmanNet is then used by each vehicle to localize itself, by estimating the associated system uncertainty matrices (i.e, Kalman gain). Our aim is to actually demonstrate the benefits of collaborative training for state estimation in autonomous driving, over collaborative decision-making which requires rich V2X communication resources for measurement exchange and sensor fusion under real-time constraints. An extensive experimental and evaluation study conducted in CARLA autonomous driving simulator highlights the superior performance of FedKalmanNet over state-of-the-art collaborative decision-making approaches, in localizing vehicles without the need of real-time V2X communication.","Autonomous vehicles employ a variety of sensors such as cameras, LiDAR, GNSS (global navigation satellite systems), and IMUs (inertial measurement units) to perceive and interpret their environment. These vehicles are expected to be a fundamental component of future Intelligent Transportation Systems [1]. Moreover, vehicles enhance their perception capabilities beyond the individual sensor range through Vehicle-to-Vehicle (V2X) communication and 5G, allowing them to share crucial traffic information. Achieving precise 3D location awareness over time is essential for optimizing autonomous driving performance. A promising approach for enhancing location or situational awareness is to exploit collaboration among vehicles, either during training or the decision-making phase, relying on V2X information exchange [2, 3]. This approach becomes even more effective when the uncertainty of sensor measurements can be estimated using data-driven or deep learning techniques [4]. KalmanNet [5], a recurrent neural network (RNN) designed to estimate the uncertainty for a single agent through the principles of the Extended Kalman filter (EKF), will be used as a key module, exactly due to its interpretability and efficiency in capturing unknown system dynamics. This work will explore the potential of avoiding raw data exchange between vehicles, while still leveraging the information among connected agents. Specifically, we will investigate transitioning from collaboration during the decision-making phase to collaboration during the training phase, and the potential benefits of it. To be more concise, instead of exchanging raw information, the data collected by a group of vehicles can facilitate a continual learning paradigm in order to estimate sensor measurements uncertainty, thereby enhancing the accuracy of localization over time. Collaboration during training usually refers to a distributed scenario where clients jointly train models used for localization applications, using their own local models. In this federated learning (FL) scenario, a global server aggregates the local models and sends back to the clients the global model after some communication rounds [6, 7]. FedLoc [8, 9] is a very popular generic framework, focusing mainly on indoor localization scenarios of edge devices. However, despite its benefits, it requires extensive trainable parameters and large datasets, even for simple sequences, lacking the explainability of KalmanNet. Indoor localization based on WiFi measurements is also the focus of related FL works [10, 11]. Collaboration during the decision-making phase, usually refers to cooperative localization (CL) based on traditional optimization techniques. Understanding the statistics of measurement noise is crucial for enhancing location estimation accuracy [2]. Centralized methods, such as those using multidimensional scaling [12] or quadratically constrained quadratic problems [13], often either assume the noise covariance is known in advance or set it equal to the identity matrix. More practical distributed approaches, which utilize the concept of covariance intersection [14, 15], typically assume the true covariance matrices are known, without addressing how they can be estimated in practice. However, in all cases CL requires raw data exchange in order to localize vehicles, thus resulting in high communication costs and privacy issues. Thus, the challenge addressed in this paper is to design an explainable data-driven localization architecture that utilize the collaborative nature of FL in order to enhance autonomous vehicles localization. To address that challenge, this study combines FL with the inherent interpretable KalmanNet architecture. This novel integration promises high performance, due to diverse data shared by cooperating agents, as well as low computational complexity due to the explainable nature of KalmanNet. Moreover, motivated by distributed parameter estimation approaches [16], our method employs the adapt-then-combine (ATC) strategy. During the adaptation step, each vehicle utilizes its private dataset to train a local KalmanNet model. This model estimates the uncertainty of the specific vehicle’s measurements, which is then incorporated into the Kalman filter (KF) solution. In the combination step, we aim to develop a robust global KalmanNet model that integrates information across various vehicles operating in diverse environments, effectively learning the underlying system’s dynamics. Furthermore, our approach only requires sharing the weights of the local KalmanNet models. By exchanging and fusing the local models during training, we will achieve better performance than CL approaches. Therefore, the main contributions of this study can be summarized as follows: • Exploiting the ATC strategy, we reformulate standard KalmanNet to its FedKalmanNet counterpart, enabling the formulation of a highly efficient distributed learning framework for data-driven localization, limiting the need for data sharing and ensuring privacy protection. • The newly proposed collaborative training paradigm for autonomous vehicle localization is shown to outperform traditional optimization based CL approaches that require exchanging and fusing raw data in real-time V2X conditions. • Extensive numerical evaluations carried out in the renowned CARLA simulator [17] demonstrate the competitive advantages of the proposed federated data-driven localization approach, in terms of absolute pose error. Outline Section II introduces the preliminaries; Section III presents the proposed federated data-driven localization framework; Section IV is dedicated to experimental setup and results, while Section V concludes this work."
https://arxiv.org/html/2411.05832v1,"Diversify, Contextualize, and Adapt:
Efficient Entropy Modeling for Neural Image Codec","Designing a fast and effective entropy model is challenging but essential for practical application of neural codecs. Beyond spatial autoregressive entropy models, more efficient backward adaptation-based entropy models have been recently developed. They not only reduce decoding time by using smaller number of modeling steps but also maintain or even improve rate–distortion performance by leveraging more diverse contexts for backward adaptation. Despite their significant progress, we argue that their performance has been limited by the simple adoption of the design convention for forward adaptation: using only a single type of hyper latent representation, which does not provide sufficient contextual information, especially in the first modeling step. In this paper, we propose a simple yet effective entropy modeling framework that leverages sufficient contexts for forward adaptation without compromising on bit-rate. Specifically, we introduce a strategy of diversifying hyper latent representations for forward adaptation, i.e., using two additional types of contexts along with the existing single type of context. In addition, we present a method to effectively use the diverse contexts for contextualizing the current elements to be encoded/decoded. By addressing the limitation of the previous approach, our proposed framework leads to significant performance improvements. Experimental results on popular datasets show that our proposed framework consistently improves rate–distortion performance across various bit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline on the Kodak dataset.","Most neural image codecs [18, 17, 8, 9, 11, 15] first transform an image into a quantized latent representation. It is then encoded into a bitstream via an entropy coding algorithm, which relies on a learned probability model known as the entropy model. According to the Shannon’s source coding theorem, the minimum expected length of a bitstream is equal to the entropy of the source. Thus, accurately modeling entropy of the quantized latent representation is crucial. Entropy models estimate a joint probability distribution over the elements of the quantized latent representation. Generally, it is assumed that all elements follow conditionally independent probability distributions. To satisfy this, the probability distributions are modeled in context-adaptive manners, which is key to accurate entropy modeling [18]. Recent methods are based on the joint backward and forward adaptation where the probability distributions adapt by leveraging contexts in two different ways: directly using previously encoded/decoded elements (i.e., backward adaptation), and extracting and utilizing an additional hyper latent representation (i.e., forward adaptation). Here, the type of contexts leveraged can be diverse depending on the spatial range they cover. First, each element has dependencies with other elements in the same spatial location along the channel dimension. Since the channel-wise dependencies correspond to the local image area (e.g., a 16\times 16 patch), we denote them as the “local” context. Second, dependencies exist among spatially adjacent elements, and we refer to them as the “regional” context. Lastly, long-range spatial dependencies span the entire image area, referred to as the “global” context. For the backward adaptation, the modeling order, i.e., which elements are modeled first, is an important factor, and the key lies in how effectively we can utilize diverse contexts in the modeling process. Early studies employ spatial autoregressive (AR) models that access regional context including the most spatially adjacent elements. However, they suffer from significantly slow decoding times due to the inevitably large number of modeling steps, which is equal to the spatial dimensions [18]. To enhance efficiency in entropy modeling, several attempts reduce the number of modeling steps while leveraging diverse contexts: a 10-step channel-wise AR model [17], a 2-step spatial non-AR model with a checkerboard pattern [8], and a 4-step non-AR model that operates across spatial and channel dimensions using a quadtree partition [14]. Figure 1: DCA diversifies the hyper latent representations and contextualizes the current elements by leveraging the diverse hyper latent representations along with the previous elements. As a result, the probability distributions adapt effectively, leading to accurate entropy modeling. Entropy models based on the efficient backward adaptation methods have led to significant improvements. However, they are still limited in fully leveraging contexts for forward adaptation. Since they use multiple neural layers with downsampling and upsampling for modeling hyper latent representation, they can only access the regional context. This limits the performance improvement due to the insufficient contexts (Figure 1a). In particular, this limitation is exacerbated at the first step where only forward adaptation is utilized due to the absence of previous elements (Figure 6). Therefore, it is necessary to develop effective forward adaptation in synergy with the efficient backward adaptation. In this paper, we propose a simple yet effective entropy modeling framework, called DCA (Diversify, Contextualize, and Adapt), leveraging sufficient contexts for forward adaptation without compromising on bit-rate (Figure 1b). Building on the quadtree partition-based backward adaptation [14], we introduce a strategy of diversification, i.e., extracting local, regional, and global hyper latent representations unlike only a single regional one in the previous approach. Note that simply using more contexts for forward adaptation does not guarantee performance improvements because forward adaptation requires additional bit allocation unlike backward adaptation. Then, we propose how to effectively utilize the diverse contexts along with the previously modeled elements for contextualizing the current elements to be encoded/decoded. To consider step-wise different situations, e.g., increased number of previous elements over steps, our contextualization method is designed to utilize each hyper latent representation separately in a step-adaptive manner. Additionally, our contextualization method proceeds in the sequence of regional, global, and local hyper latent representations. Similarly to backward adaptation, we empirically observe that modeling order also matters in forward adaptation. Our main contributions are summarized as follows: • We propose a strategy of diversifying contexts for forward adaptation by extracting three different hyper latent representations, i.e., local, regional, and global ones. This strategy can provide sufficient contexts for forward adaptation without compromising on bit-rate. • We introduce how to effectively leverage the diverse contexts, i.e., previously modeled elements and the three hyper latent representations. We empirically show that the modeling order of three types of contexts affects the performance. • Through the diversification and contextualization methods, our DCA effectively adapts, resulting in significant performance improvements. For example, DCA achieves 3.73% BD-rate gain over the state-of-the-art method [14] on the Kodak dataset."
https://arxiv.org/html/2411.05829v1,Utilizing RNN for Real-time Cryptocurrency Price Prediction and Trading Strategy Optimization,"This research embarks on the exploration of leveraging Recurrent Neural Network (RNN) for real-time cryptocurrency price prediction and the optimization of trading strategies. Despite the notorious volatility and unpredictability of the cryptocurrency market, traditional forecasting methods and trading strategies often fail to deliver desired results. This study aims to bridge this gap by harnessing the power of RNN, renowned for their proficiency in capturing long-term dependencies and trends in time-series data. Over a concentrated period of ten weeks, the project unfolds through a series of meticulously planned phases, starting with a comprehensive review of the existing literature and the collection of extensive datasets encompassing historical price data, trading volumes and sentiment analysis derived from social networks and news sources. The subsequent weeks are dedicated to data preprocessing, feature engineering, and the iterative development and refinement of the RNN model to accurately predict cryptocurrency prices. This foundation paves the way for the formulation of dynamic trading strategies that are rigorously backtested to assess profitability and risk, culminating in an evaluation phase in which the efficacy of the model and the performance of the trading strategies are thoroughly analyzed. The anticipated outcome of this research is a robust RNN-based predictive model that not only surpasses traditional forecasting methods in accuracy but also empowers traders with optimized strategies tailored for the fast-paced cryptocurrency market. This study not only contributes to the academic discourse on financial market predictions using deep learning techniques, but also offers practical insights and tools for investors navigating the complexities of cryptocurrency trading. Through this endeavor, our goal is to set a precedent for future research to integrate advanced machine learning models with financial trading systems to navigate and profit from the digital currency ecosystem.","The foundation of modern financial systems is predominantly based on fiat currency, which has notable benefits such as divisibility, durability, and ease of transferability. However, fiat currency, being unbacked by a tangible asset, can lead to inflationary pressures and manipulation by centralized authorities, like governments. The centralized control of the monetary supply has historically caused issues like hyperinflation and rising income inequality [6]. Additionally, the financial system’s dependence on intermediaries like banks and credit card companies introduces higher costs, delays, and the risk of security breaches [7]. This reliance results in the loss of individuals’ control over personal data. Despite the limitations, trust in the current financial system is supported by government regulations and legal contracts. However, trust has been breached in the past due to events such as the dot-com bubble in the late 1990s and the 2008 real estate crisis, which led to significant financial losses [8]. These events highlight the need for a more secure, transparent financial system. In 2008, an anonymous entity known as Satoshi Nakamoto introduced blockchain technology, along with the first decentralized cryptocurrency, Bitcoin (BTC), which enabled peer-to-peer (P2P) transactions without the need for third-party intermediaries [9]. Blockchain technology has since gained traction across various industries and has become a subject of extensive research [10]. Cryptocurrencies, powered by blockchain technology, have emerged as a new form of digital currency that employs cryptography to secure and verify transactions [6]. Unlike fiat currencies, cryptocurrencies operate on decentralized networks without the oversight of a central authority. Bitcoin, the first cryptocurrency, is the most widely recognized, but other digital currencies, including Ethereum (ETH), Litecoin (LTC), and Ripple (XRP), have also gained prominence. Ethereum, in particular, introduced smart contracts and decentralized applications (dApps), broadening the use cases of blockchain technology [6]. A unique feature of cryptocurrencies is their extreme price volatility, which makes the market both lucrative and risky for investors. As the market matures, artificial intelligence (AI) and machine learning (ML) have been used to predict price movements. However, predicting cryptocurrency prices remains challenging due to the complexity of market drivers, such as government regulations, technological innovations, and public sentiment [7]. Despite these challenges, the cryptocurrency market is expected to continue growing, with forecasts estimating a compound annual growth rate (CAGR) of 11.1% [6]. To improve prediction accuracy, this paper applies deep learning (DL) techniques to identify hidden patterns in cryptocurrency price data. By employing advanced DL algorithms, we aim to enhance prediction models, enabling more informed investment decisions. Specifically, the contributions of this paper are: 1. Developing a price prediction model for Bitcoin (BTC), Ethereum (ETH), and Litecoin (LTC); 2. Utilizing DL algorithms such as Long Short-Term Memory (LSTM), Bi-directional LSTM (Bi-LSTM), and Gated Recurrent Units (GRU); 3. Evaluating the models using performance metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) and Mean Absolute Percentage Error (MAPE). The objective is to create reliable models that cryptocurrency investors and traders can use for price predictions based on historical data."
https://arxiv.org/html/2411.05826v1,From Pixels to Prose: Advancing Multi-Modal Language Models for Remote Sensing,"Remote sensing has evolved from simple image acquisition to complex systems capable of integrating and processing visual and textual data. This review examines the development and application of multi-modal language models (MLLMs) in remote sensing, focusing on their ability to interpret and describe satellite imagery using natural language. We cover the technical underpinnings of MLLMs, including dual-encoder architectures, Transformer models, self-supervised and contrastive learning, and cross-modal integration. The unique challenges of remote sensing data–varying spatial resolutions, spectral richness, and temporal changes–are analyzed for their impact on MLLM performance. Key applications such as scene description, object detection, change detection, text-to-image retrieval, image-to-text generation, and visual question answering are discussed to demonstrate their relevance in environmental monitoring, urban planning, and disaster response. We review significant datasets and resources supporting the training and evaluation of these models. Challenges related to computational demands, scalability, data quality, and domain adaptation are highlighted. We conclude by proposing future research directions and technological advancements to further enhance MLLM utility in remote sensing.","Remote sensing has evolved from basic image capture to advanced systems that integrate visual and textual data processing. This progression has led to the development of multi-modal language models (MLLMs) capable of interpreting and describing satellite imagery using natural language [1, 2]. Such integration enhances the analysis of Earth observation data, making it more accessible and informative. Applying MLLMs to remote sensing data has significantly improved automated Earth observation analysis. These models support vital applications in environmental monitoring, urban planning, and disaster response by enabling more efficient and accurate extraction of information from satellite imagery [3, 4]. This advancement enhances decision-making processes across various sectors. This review examines the integration of MLLMs in remote sensing, focusing on their technical foundations, current applications, and potential future developments. The objective is to provide a comprehensive overview of how these models enhance the interpretation of satellite imagery and to identify areas for further research and application. Fig 1 presents a taxonomy of MLLM in remote sensing, categorizing the key components and their interrelationships as discussed in this review. The taxonomy is structured into six primary branches: Technical Foundations, Current Applications and Implementations, Datasets and Resources, Challenges and Limitations, and Future Directions and Conclusions. It serves as a guide for readers through the complexities of integrating and applying MLLMs to remote sensing tasks. {forest} for tree= grow=east, scale=0.75, growth parent anchor=west, parent anchor=east, child anchor=west, l sep=8mm, s sep=4mm, edge path= [\forestoptionedge,-¿, ¿=latex] (!u.parent anchor) – +(5pt,0pt) —- (.child anchor) \forestoptionedge label; , align=left, [MLLM and Remote Sensing, root, [Future Directions and Conclusions, level1, [Potential Applications [3, 5, 1, 6, 7, 8], leaf] [Research Opportunities [3, 1], leaf] ] [Challenges and Limitations, level1, [Application-Specific Challenges, level2, [Domain Adaptation [9, 4, 10, 11], leaf] ] [Technical Challenges, level2, [Computational Requirements [12, 13, 14], leaf] [Model Scalability [15, 16, 17], leaf] [Data Quality Issues [18, 19, 20, 11], leaf] ] ] [Datasets and Resources, level1, [Evaluation Metrics, level2, [CIDEr [21], leaf] [METEOR [22], leaf] [BLEU [23], leaf] ] [Training Resources, level2, [Training Frameworks [24, 25], leaf] [Pre-trained Models [3], leaf] ] [Benchmark Datasets, level2, [ChatEarthNet [26], leaf] [RS5M [9], leaf] [RSICap [27], leaf] ] ] [Current Applications and Implementations, level1, [Cross-Modal Applications, level2, [Visual Question Answering [28, 29, 30, 31], leaf] [Image-to-Text Generation [32, 33, 34, 35], leaf] [Text-to-Image Retrieval [36, 4, 10, 11], leaf] ] [Image Understanding, level2, [Change Detection [37, 38, 39], leaf] [Object Detection [40, 41, 42, 43, 44, 45, 46], leaf] [Scene Description [47, 34, 48, 49], leaf] ] ] [Technical Foundations, level1, [Remote Sensing Data Characteristics, level2, [Temporal Aspects [37, 50, 51, 52], leaf] [Spectral Information [53, 54, 55, 56], leaf] [Spatial Resolution [3, 57, 58, 59], leaf] ] [Multi-Modal Language Models, level2, [Integration Approaches [60, 61, 62, 63], leaf] [Learning Mechanisms [3, 57, 64, 65], leaf] [Architecture Components [66, 67, 68], leaf] ] ] ] Figure 1: Taxonomy of Multi-Modal Language Models for Remote Sensing"
https://arxiv.org/html/2411.05825v1,SurfGNN: A robust surface-based prediction model with interpretability for coactivation maps of spatial and cortical features,"Current brain surface-based prediction models often overlook the variability of regional attributes at the cortical feature level. While graph neural networks (GNNs) excel at capturing regional differences, they encounter challenges when dealing with complex, high-density graph structures. In this work, we consider the cortical surface mesh as a sparse graph and propose an interpretable prediction model—Surface Graph Neural Network (SurfGNN). SurfGNN employs topology-sampling learning (TSL) and region-specific learning (RSL) structures to manage individual cortical features at both lower and higher scales of the surface mesh, effectively tackling the challenges posed by the overly abundant mesh nodes and addressing the issue of heterogeneity in cortical regions. Building on this, a novel score-weighted fusion (SWF) method is implemented to merge nodal representations associated with each cortical feature for prediction. We apply our model to a neonatal brain age prediction task using a dataset of harmonized MR images from 481 subjects (503 scans). SurfGNN outperforms all existing state-of-the-art methods, demonstrating an improvement of at least 9.0% and achieving a mean absolute error (MAE) of 0.827±0.056 in postmenstrual weeks. Furthermore, it generates feature-level activation maps, indicating its capability to identify robust regional variations in different morphometric contributions for prediction.","There is growing evidence that brain development/aging trajectories and developments of brain disorders both could be traced on the cerebral cortex [1]. A prevalent approach for characterizing cerebral cortex is to reconstruct the cortical surface and measure the morphological features, such as cortical thickness, surface area, sulcal depth, myelin content, etc. [2] Currently, an important application of cortical features is to predict phenotypes, such as age [3, 4], sex[5], and brain disease states[6] using machine learning methods, and it could help explore important biomarkers about the cortex evolutional process and diagnose brain disorders. For surface-based analysis, early approaches solely focused on vertex features without considering the topological structure of the surface mesh[8]. More recent techniques utilized GNN-based networks to collectively examine the node features and topological architecture of the cortical surface, due to the graph-like characteristics of the surface mesh[9]. However, managing cortical surface meshes that contain a vast number of vertices presents substantial computational difficulties in graph analysis. A common approach to address this is down-sampling the surface mesh, significantly reducing the vertex count before model learning [3, 4, 5]. Nevertheless, this could either diminish the prediction accuracy of the model or reduce its capacity to yield meaningful and interpretable results. Another commonly employed approach for cortical surface manipulation works within a spherical framework [10, 11, 12, 13]. It collects node features from the brain’s surface, sequentially down-samples them, adhering to the hierarchical spherical architecture of the cortical surface, and eventually combines them for prediction. These models are efficient in managing large scale graphs with a high density of nodes. However, they couldn’t flexibly identify the best sub-graph structures or important nodes that contribute best to the prediction task. This might be important because different regions exhibit diverse responses to various predictive demands[17]. In addition to exploring the spatial heterogeneity at the global level, researches to date have seldom focused specifically on the feature-level heterogeneity. That is, different features may exhibit different spatial patterns in prediction tasks. Every cortical feature corresponds to a distinct macro- or micro- structure of the cerebral cortex. Observations during rapid developmental stages[17], the aging process[20], or under pathological conditions[21] have demonstrated that the regional variation in different cortical features could serve as distinct biomarkers for varying brain conditions. Enabling the separate manipulation of each cortical feature within the model, before integrating them for prediction analysis, might significantly boost the model’s performance. We hypothesize that by allowing the autonomous expression of each feature, the model could capture more nuanced and impactful information for prediction tasks. It could help disaggregate the contribution of different cortical features to the prediction, enabling spatial-feature-level interpretations of the model for each subject. A further challenge faced by surface-based models is their interpretability. It involves exploring the diverse characteristics of the cortical surface and identifying biomarkers associated with specific phenotypes. Post-hoc saliency-based methods are widely used, which aim to pinpoint the most impactful input features that contribute to a prediction task by examining the gradients or activations within the network in relation to a specific input[22]. Nevertheless, these methods have limitations[23, 24] and may not always hold for neuroimaging and neuroscience research, where available data are typically small-sized and much more complex [22, 25]. Another strategy is to develop a deep learning model with inherent self-interpretability[26]. This entails creating an end-to-end framework that facilitates the identification of detailed explanatory factors, thereby improving the extraction of discriminative representations and leading to more accurate outcomes. Previous research in this area, such as SiT[18] and NeuroExplainer[25], has shown promising results. Motivated by these findings, creating a surface-based prediction model with built-in interpretability emerges as a significant and promising area of research. To fulfill the outlined requirements and tackle the previously mentioned challenges, inspired by the GNN-based networks and the spherical frameworks, we propose a Surface Graph Neural Network (SurfGNN) as a self-interpretable prediction model. The entire framework of SurfGNN consists of topology-sampling learning (TSL) and region-specific learning (RSL) structures for each cortical feature, and a score-weighted fusion (SWF) structure across all features for prediction. We assess our model within the context of a brain age prediction task. Predicting brain age from structural brain neuroimaging data poses challenges akin to those faced in various neuroimaging applications, serving as a foundation for developing and testing deep learning algorithms. Furthermore, this task has gained attention due to its potential clinical and biological significance [57]. To summarize our contributions as follows: 1. We formulate a graph analysis process comprising TSL and RSL structures, extending from low-level to high-level surface meshes characterized by higher and fewer numbers of vertices, respectively. The TSL efficiently performs sampling on sparse graphs, preserving the overall brain topological shape, while the RSL effectively conducts in-depth graph analysis, distinguishing the varied impacts of different brain regions on prediction. 2. We propose a novel score-weighted fusion mechanism to amalgamate node information derived from individual cortical features within the graph learning framework. This mechanism also facilitates the generation of node scores, providing interpretable results that are specific to each feature. 3. We apply the SurfGNN to a neonatal brain age prediction task on a dataset with morphological features including cortical thickness, sulcal depth, and gray matter/white matter (GM/WM) intensity ratio. Our model outperforms state-of-the-art approaches on it. For each cortical feature, we build the spatial maps based on the node score."
https://arxiv.org/html/2411.05823v1,FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models,"Recently, there is a growing interest in creating computer-aided design (CAD) models based on user intent, known as controllable CAD generation. Existing work offers limited controllability and needs separate models for different types of control, reducing efficiency and practicality. To achieve controllable generation across all CAD construction hierarchies, such as sketch-extrusion, extrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by fine-tuning large language models (LLMs). First, to enhance comprehension by LLMs, we represent a CAD model as a structured text by abstracting each hierarchy as a sequence of text tokens. Second, to address various controllable generation tasks in a unified model, we introduce a hierarchy-aware masking strategy. Specifically, during training, we mask a hierarchy-aware field in the CAD text with a mask token. This field, composed of a sequence of tokens, can be set flexibly to represent various hierarchies. Subsequently, we ask LLMs to predict this masked field. During inference, the user intent is converted into a CAD text with a mask token replacing the part the user wants to modify, which is then fed into FlexCAD to generate new CAD models. Comprehensive experiments on public dataset demonstrate the effectiveness of FlexCAD in both generation quality and controllability. Code will be available at https://github.com/microsoft/CADGeneration/FlexCAD.","A computer-aided design (CAD) model is a digital representation of a 2D or 3D object. It has been widely used across numerous industries, including architecture, product design and manufacturing, facilitating precise, efficient, and innovative development Ganin et al. (2021); Khan et al. (2024). In commonly used CAD tools like SolidWorks and AutoCAD, sketch-and-extrude modeling (SEM) is prevalent. This involves drawing 2D sketches and then extruding them into 3D shapes. Compared to other representations, such as Constructive Solid Geometry (CSG) Yu et al. (2024), B-rep Xu et al. (2024), or voxel Li et al. (2023) and point cloud Khan et al. (2024)-based formats, SEM, incorporating multiple CAD construction hierarchies including sketch-extrusion, extrusion, sketch, face, loop and curve (see Fig. 3(a)), directly illustrates the drawing process of a 3D object. This allows for easy editing and reuse of CAD models, which is essential in CAD development. Recently, there is an increasing interest in developing generative models to automatically produce SEM of a CAD model111In the following, we will use CAD model to refer to SEM of a CAD model for brevity.. Specifically, DeepCAD Wu et al. (2021) focuses on uncontrollable generation, where a CAD model is generated from a randomly sampled vector. However, providing controllability, i.e., generating CAD models according to user intent, is crucial for the practical application of generative models. To address this, SkexGen Xu et al. (2022) and Hnc-cad Xu et al. (2023) implement disentangled codebooks to offer some levels of control. As each codebook encodes a particular construction hierarchy, their controllability is quite restricted. For instance, SkexGen does not allow selecting a specific sketch for modifications when a CAD model comprises multiple sketches, nor can it handle finer-grained hierarchies such as faces and loops. Hnc-cad lacks control over the topology and geometry of curves. In summary, existing methods face challenges in providing adequate controllability across all CAD construction hierarchies. Additionally, they require separate models to deliver different types of control, which is inefficient and less practical. The emergence of large language models (LLMs) offers insights for addressing these challenges. First, LLMs have exhibited remarkable success in handling diverse user queries with a single and unified model Chung et al. (2024). This phenomenon not only occurs in natural language tasks but also extends to other areas with domain-specific fine-tuning, such as human motion generation Jiang et al. (2024) and crystal material synthesis Gruver et al. (2024). Second, LLMs might have acquired CAD-related knowledge during the pre-training by learning CAD-specific codes, such as JSCAD codes Makatura et al. (2023). Third, prior to the rise of LLMs, small transformer-based models were explored for tasks like uncontrollable generation and image-to-sketch translation in the 2D sketch domain Ganin et al. (2021), showcasing the possibility of LLMs from a different perspective. In this work, we introduce FlexCAD, a unified model designed for controllable CAD generation across all hierarchies by fine-tuning LLMs. As shown in Fig. 1, FlexCAD receives the original CAD model along with the part the user wants to modify (highlighted in blue). Here, users can specify the part in any hierarchy. FlexCAD then generates multiple new CAD models, altering only the selected part. To achieve these abilities, first, FlexCAD translates a CAD model into a concise and structured text (see Fig. 3). Specifically, in each sketch, the curve type (e.g., a line) is directly represented as textual tokens. The numerical data indicating geometry (e.g., point coordinates in a line) is converted into decimal integers and then into textual tokens. A special token is added to mark the end of each hierarchy. Tokens from the finer-level hierarchy are concatenated to form the representation for the coarser-level hierarchy. We use a similar way to convert each extrusion. Consequently, unlike the one-hot representation used in Xu et al. (2022), FlexCAD provides a concise text representation of a CAD model, facilitating easier processing and understanding by LLMs. Second, FlexCAD introduces a hierarchy-aware masking strategy to enable fine-tuning LLMs for various controllable CAD generation tasks (see Fig. 2). During training, we replace a hierarchy-aware field, which contains a sequence of tokens in the CAD text, with a mask token. This field can be set adaptably to reflect various hierarchies. Then, we ask LLMs to predict the masked field. To achieve this, we design prompt templates for all hierarchies, where the mask tokens are tailored to match the corresponding hierarchies. These templates are uniformly sampled at each epoch during the fine-tuning of LLMs. In this way, we ensure that the generation tasks for all hierarchies are learned in a single and unified model. Besides, unlike Xu et al. (2022; 2023) that requires multi-stage training, FlexCAD achieves end-to-end training. During inference, a CAD model is represented as a CAD text with a mask token replacing the part the user wants to change. The masked CAD text is fed into the fine-tuned LLMs to get predictions. After infilling the masked text with these predictions, FlexCAD produces CAD texts that can be rendered into new CAD models. Overall, our contributions are: • We propose FlexCAD, a unified and versatile model for controllable CAD generation across all hierarchies, including sketch-extrusion, extrusion, sketch, face, loop and curve. • To the best of our knowledge, FlexCAD is the first to leverage LLMs for controllable CAD generation. It converts a CAD model into a brief, structured text and employs hierarchy-aware masking to fine-tune LLMs for various controllable CAD generation tasks. • We conduct extensive experiments on public datasets. Despite its simplicity, FlexCAD greatly improves generation quality and controllability, showing its effectiveness on the tasks presented in this work and indicating potential for other CAD generation scenarios."
https://arxiv.org/html/2411.05820v1,Guiding Genetic Programming with Graph Neural Networks,"In evolutionary computation, it is commonly assumed that a search algorithm acquires knowledge about a problem instance by sampling solutions from the search space and evaluating them with a fitness function. This is necessarily inefficient because fitness reveals very little about solutions – yet they contain more information that can be potentially exploited. To address this observation in genetic programming, we propose EvoNUDGE, which uses a graph neural network to elicit additional knowledge from symbolic regression problems. The network is queried on the problem before an evolutionary run to produce a library of subprograms, which is subsequently used to seed the initial population and bias the actions of search operators. In an extensive experiment on a large number of problem instances, EvoNUDGE is shown to significantly outperform multiple baselines, including the conventional tree-based genetic programming and the purely neural variant of the method.","The blueprint of evolutionary algorithms assumes that the fitness function is the only means by which the search method is informed about the characteristics of a given problem instance. This design choice is inspired by natural evolution, where a species cannot improve its adaptations otherwise than by spawning randomly diversified offspring, some of which have the chance of being fitter than others. However, there is no reason to keep imposing this information bottleneck if other sources of informative guidance are available, which is relatively common in the practice of metaheuristic search algorithms. For instance, if a problem instance features constraints, one may seed the initial population with candidate solutions that comply with them; if the distributions of some variables happen to be known in advance, one may design search operators that take those distributions into account. In this study, we aim at eliciting problem-specific knowledge also from the candidate solutions themselves and from how they are being evaluated. As per the No Free Lunch Theorem (Wolpert and Macready, 1997), an optimization algorithm informed about the characteristics of a problem instance cannot perform worse on average than an uninformed algorithm. However, gathering useful knowledge about a problem and turning it into information that is ‘actionable’ for the search policy is difficult in domains where the fitness function depends on solutions in a complex way. One domain with this characteristic is genetic programming (GP), where solutions are programs or other symbolic expressions that reveal their characteristics only once executed. Turning the effects of program execution into search guidance is difficult, but can be realized as a learnable mapping. To this aim, we hybridize the GP heuristics with a bespoke graph neural network (GNN) designed to generate graphs of programs. Given an instance of a GP problem represented as a set of input-output examples, the GNN is queried on it to produce a sample of GP subprograms, which is then used to seed the GP population and bias the search operators. We apply this approach to symbolic regression (SR), but it can be easily generalized to other domains. The main contributions of this study are (i) EvoNUDGE, a neuro-evolutionary method for solving SR problems (Sec. 3) and its experimental assessment on a range of SR benchmarks (Sec. 5). The remaining sections comprise problem formulation (Sec. 2) and the review of related works (Sec. 4)."
https://arxiv.org/html/2411.05816v1,Learning Characteristics of Reverse Quaternion Neural Network,"The purpose of this paper is to propose a new multi-layer feedforward quaternion neural network model architecture, Reverse Quaternion Neural Network which utilizes the non-commutative nature of quaternion products, and to clarify its learning characteristics. While quaternion neural networks have been used in various fields, there has been no research report on the characteristics of multi-layer feedforward quaternion neural networks where weights are applied in the reverse direction. This paper investigates the learning characteristics of the Reverse Quaternion Neural Network from two perspectives: the learning speed and the generalization on rotation. As a result, it is found that the Reverse Quaternion Neural Network has a learning speed comparable to existing models and can obtain a different rotation representation from the existing models.","In recent years, machine learning technology has made tremendous progress with the rise of deep learning. In particular, it has achieved remarkable success in fields such as image recognition, natural language processing, and speech recognition, and is still being actively researched. A high-dimensional neural network [4, 3, 1, 5, 2] is a type of neural network that uses numbers of two or more dimensions, such as complex numbers and quaternions, to represent the parameters of the neural network . High-dimensional neural networks can deal with hypercomplex-valued signals naturally. It is well-known that complex-valued neural networks and quaternion neural networks require fewer parameters (weights, biases) and have several times faster learning speeds than usual real-valued neural networks [8, 9, 6, 7]. Recent studies have actively explored the application of high-dimensional neural networks in areas such as speech recognition and image processing. Zhu et al.[10] have extended the convolutional neural networks (CNN) [11] to quaternions. This approach represents the relationship between RGB colors in an image through the rotation of quaternions in the imaginary parts (i, j, and k axes), and has shown higher accuracy in color image processing compared to real-valued convolutional neural networks. The quaternion convolutional neural network, proposed by Parcollet et al. [12], introduces a model that divides the feature map into individual components of quaternions and convolves them. This approach has demonstrated superior recognition capabilities in speech recognition tasks compared to real-valued CNNs. A quaternion is a mathematical concept introduced by Hamilton [13] in 1843. A quaternion consists of a real part and three imaginary parts (i, j, k), each representing an independent dimension. This characteristic makes quaternions particularly suitable for representing rotations in a three-dimensional space. Furthermore, quaternions possess the property that the commutative law of multiplication does not apply, meaning that the order of multiplication significantly affects the outcome. As described below, neural networks utilizing quaternions exhibit different characteristics compared to real-valued and complex-valued neural networks in this meaning. A quaternion neural network is a neural network model where all parameters are represented as quaternions, enabling the network using quaternions to represent data in higher dimensions, and it particularly excels in representing rotations and orientations in three-dimensional space. By utilizing the rich expressiveness of quaternions, quaternion neural networks can capture spatial data features that traditional neural networks may miss, especially in fields where spatial information is crucial, such as 3D graphics and robotics. In the context of 3D spatial transformations, Matsui et al. [14] conducted an experiment comparing three tasks - scaling, parallel translation, and rotation - using geometric object data with the Quaternion Multi-layer Perceptron (QMLP) and the real-valued MLP (Multi-layer Perceptron). The results showed that while the real-valued MLP failed to learn the transformations, the QMLP successfully learned them and performed perfectly in all three tasks. The existence of two types of quaternion neurons, based on non-commutativity, was pointed out in [7]. Yoshida et al. [15] investigated the existence conditions of the energy functions for the Hopfield-type recurrent quaternion neural network and the one with weights applied in reverse order . As a result, they clarified that there is no difference between the usual Hopfield-type recurrent quaternion neural network model and the quaternion neural network with weights applied in reverse order. In this paper, we propose a new feedforward quaternion neural network architecture, Reverse Quaternion Neural Networks (called RQNN for short here), which utilizes the non-commutative property of quaternions. In the model, by altering the order of quaternion multiplication, it is possible to construct neural networks with different characteristics. This approach is expected to capture information and offer a different expressive power that traditional quaternion neural networks cannot achieve. As a matter of facts , we conducted experiments on the learning speed and the generalization ability on rotation of the RQNN and the usual multi-layer feedforward quaternion neural network, and showed the differences between the two quaternion neural networks."
https://arxiv.org/html/2411.05811v1,Neurophysiological Analysis in Motor and Sensory Cortices for Improving Motor Imagination,"Brain–computer interface (BCI) enables direct communication between the brain and external devices by decoding neural signals, offering potential solutions for individuals with motor impairments. This study explores the neural signatures of motor execution (ME) and motor imagery (MI) tasks using EEG signals, focusing on four conditions categorized as sense–related (hot and cold) and motor–related (pull and push) conditions. We conducted scalp topography analysis to examine activation patterns in the sensorimotor cortex, revealing distinct regional differences: sense–related conditions primarily activated the posterior region of the sensorimotor cortex, while motor–related conditions activated the anterior region of the sensorimotor cortex. These spatial distinctions align with neurophysiological principles, suggesting condition–specific functional subdivisions within the sensorimotor cortex. We further evaluated the performances of three neural network models–EEGNet, ShallowConvNet, and DeepConvNet–demonstrating that ME tasks achieved higher classification accuracies compared to MI tasks. Specifically, in sense–related conditions, the highest accuracy was observed in the cold condition. In motor–related conditions, the pull condition showed the highest performance, with DeepConvNet yielding the highest results. These findings provide insights into optimizing BCI applications by leveraging specific condition–induced neural activations.","I INTRODUCTION Brain–computer interface (BCI) is communicating technology between humans and devices by recognizing their intentions and status [1, 2, 3]. BCI technology can be broadly categorized into two types [4]. First, invasive BCI involves the insertion of microelectrodes into the brain to record the action potentials of neurons located at the electrode sites, enabling the extraction of necessary information [5]. Owing to this characteristic, invasive BCI offers a high signal–to–noise ratio (SNR), but it has the drawback of requiring surgical operation [6]. In contrast, non–invasive BCI refers to techniques that measure the brain’s electrical activity without the need for surgical operation [7, 8]. While non–invasive BCI has a lower SNR compared to invasive BCI, it offers the significant practical advantages owing to no need for surgical operation [9]. As a result, non–invasive BCI has been utilized for controlling various external devices, including a drone [10], a robotic arm [11], and a wheelchair [12]. Motor execution (ME) and motor imagery (MI) are types of BCI paradigms. ME paradigm is used to measure brain signals that occur when performing specific movements [13]. In contrast, MI paradigm is utilized to measure brain signals that occur when imagining the movement of muscles used to perform specific movements [14]. MI–based BCI has been instrumental in developing assistive technologies for individuals with motor impairments. Owing to this aspect, ME– and MI–based BCI systems have utilized electroencephalogram (EEG), which is the most practical method in non–invasive methods [15]. Since both ME and MI tasks are motor–related paradigms, EEG signals while conducting ME and MI tasks have the significantly similar features, including spatial features in the sensorimotor cortex [16]. Ogawa et al. [17] demonstrate the asymmetric representation of the directed functional connectivity while performing ME and MI tasks. They discovered new brain regions related to motor tasks while finding out similarities and differences between ME and MI tasks. Jia et al. [18] proposed the end–to–end deep learning model using the multi–branch spectral–temporal convolutional neural network with the channel attention and the LightGBM for decoding MI-based EEG signals. Their proposed method achieved 0.86 for classifying 2–class MI tasks and 0.74 for classifying 4–class MI tasks. The sensorimotor cortex, the main target for motor tasks, is a brain region that processes both sense– and motor–related information. Recent studies have highlighted the potential for sense–related information to enhance the decoding of motor–related information [19]. Incorporating sensory feedback has been shown to improve the performances of decoding motor–related information, especially in BCI applications. Sense–related information, such as tactile information, can provide valuable real–time information about limb position and movement, which complements motor–related information during motor planning and execution. Moreover, this integration has been found to improve the robustness of BCI systems in dynamic environments. Therefore, combining sense–related information with motor–related information not only enhances decoding performances but also facilitates more intuitive and naturalistic control of external devices. In this study, we compared and analyzed ME and MI signals while conducting four conditions (i.e., hot, cold, pull, and push) that could engage the sensorimotor cortex. Through this analysis, we explore the differences between sense–related conditions involving the information of the temperature and motor–related conditions involving arm movements. Additionally, by examining the distinctions between ME and MI tasks during these conditions, we demonstrate the potential for a more detailed analysis of the sensorimotor cortex. Figure 1: Experimental paradigms for acquiring ME– and MI–based EEG signals. (a) The paradigm for acquiring ME–based EEG signals. (b) The paradigm for acquiring MI–based EEG signals."
https://arxiv.org/html/2411.05809v1,"Is it me, or is A larger than B: Uncovering the determinants of relational cognitive dissonance resolution","This study explores the computational mechanisms underlying the resolution of cognitive dissonances. We focus on scenarios in which an observation violates the expected relationship between objects. For instance, an agent expects object A to be smaller than B in some feature space but observes the opposite. One solution is to adjust the expected relationship according to the new observation and change the expectation to A being larger than B. An alternative solution would be to adapt the representation of A and B in the feature space such that in the new representation, the relationship that A is smaller than B is maintained. While both pathways resolve the dissonance, they generalize differently to different tasks. Using Artificial Neural Networks (ANNs) capable of relational learning, we demonstrate the existence of these two pathways and show that the chosen pathway depends on the dissonance magnitude. Large dissonances alter the representation of the objects, while small dissonances lead to adjustments in the expected relationships. We show that this effect arises from the inherently different learning dynamics of relationships and representations and study the implications.","Imagine strolling through an art museum, expecting awe-inspiring masterpieces. Suddenly, disrupting your expectations, you encounter, displayed in the vitrine, … a banana. Dissonance arises. On one hand, the artistic value of museum displays is expected to be high. On the other hand, a banana seems to be endowed with limited artistic value. There are two ways of resolving this dissonance: recalibrating your expectations from museum displays (museums exhibit mundane objects) or finding a deeper appreciation for the artistic value of bananas. Resolving cognitive dissonances is the hallmark of scientific inquiry. Consider a scientist who has consistently observed that particles of type B are larger than particles of type A. New experimental data, however, suggests the opposite: particles A are actually larger than particles B. This unexpected finding forces the scientist to choose between two alternative adaptation pathways: They can question the validity of the experimental results and maintain the view that B are larger than A. Alternatively, they can update their view and conclude that A are, indeed, larger than B. Considering such resolutions more formally, we posit that the cognitive system is endowed with two modules [1, 2, 3]. The first is an input representational module, which encodes a relevant feature of the input. In the banana example, this module extracts a measure of artistic value from museum displays, be it a Rodin sculpture or a banana. In the particles’ example, it extracts the sizes of the particles from the experimental data. The second is a relational module, which characterizes the relationship between different input representations or between such representations and an anchor. The relational module expects that the artistic value of objects displayed in art museums would be higher than some threshold, which is higher than the artistic value of mundane objects. The banana, having an artistic value similar to mundane objects, violates this expectation. In the particles’ example, this module predicts that B would be larger than A. The dissonance can be resolved by adapting either the relational module or the representational module (or in some cases, both). While previous research on cognitive dissonance has not explicitly framed the resolution process in terms of representational and relational adaptations, classic experiments in the field can be reinterpreted through this lens. Consider induced-compliance studies, which are a common way of studying cognitive dissonance [4, 5]. In these experiments, a participant is coerced to perform an action, which they would rather avoid. This causes inconsistency between the participants’ belief that they are freely-acting agents that make their choices in accordance with their preferences, and the observation that, in fact, they act against their preferences. To resolve this dissonance, a participant can change their attitude toward their actions, interpreting them as less undesirable than they previously assessed. This corresponds to a representational adaptation of the action. Alternatively, they can resolve the dissonance by admitting they are less independent than previously believed, adapting their relational module. The determinants of the adaptation pathway used for resolving a cognitive dissonance have been a topic of research for several decades, although not in the framework of representation vs. relation (see review [6]). These determinants include the effort the adaptation pathway requires [7, 8], its likelihood to succeed [9], the dissonance magnitude [10], the associated affective state [5, 11], and contextual factors [12]. While these findings have significantly advanced our understanding, much of the research has focused on qualitative approaches or specific experimental paradigms. By introducing a computational framework that emphasizes the dichotomy between representational and relational adaptations, we aim to provide a quantitative model that offers insights into the underlying mechanisms of adaptation. To achieve this goal, we utilized ANNs that were designed to emulate relational learning and have distinct representational and relational modules [13, 14, 15, 16]. We first examine how these ANNs resolve relational dissonances. To simulate such dissonances, we constructed an order discrimination task that requires identifying a specific relationship between image features. Then, that relationship is reversed, and we ask whether the ANN adapts its representational or its relational module. We start the paper by presenting the order discrimination task and the ANN model. We use them to demonstrate the two adaptation pathways and how the adaptation pathway depends on the dissonance magnitude. Then, we explain the results by analyzing a simplified task and network. Finally, we discuss our results and the implications for understanding humans’ cognitive dissonance."
https://arxiv.org/html/2411.05806v1,SkipSNN: Efficiently Classifying Spike Trains with Event-attention,"Spike train classification has recently become an important topic in the machine learning community, where each spike train is a binary event sequence with temporal-sparsity of signals of interest and temporal-noise properties. A promising model for it should follow the design principle of performing intensive computation only when signals of interest appear. So such tasks use mainly Spiking Neural Networks (SNNs) due to their consideration of temporal-sparsity of spike trains. However, the basic mechanism of SNNs ignore the temporal-noise issue, which makes them computationally expensive and thus high power consumption for analyzing spike trains on resource-constrained platforms. As an event-driven model, an SNN neuron makes a reaction given any input signals, making it difficult to quickly find signals of interest. In this paper, we introduce an event-attention mechanism that enables SNNs to dynamically highlight useful signals of the original spike trains. To this end, we propose SkipSNN, which extends existing SNN models by learning to mask out noise by skipping membrane potential updates and shortening the effective size of the computational graph. This process is analogous to how people choose to open and close their eyes to filter the information they see. We evaluate SkipSNN on various neuromorphic tasks and demonstrate that it achieves significantly better computational efficiency and classification accuracy than other state-of-the-art SNNs.","Figure 1: The problem definition of efficient classification of spike trains. The spike trains are generated by an event camera, which is an imaging sensor that responds to local changes in brightness. Each pixel inside an event camera operates independently and asynchronously, reporting changes in brightness as they occur, and staying silent otherwise. Therefore, each image can be considered as binary event image. Motivation. Spike trains are sequences of binary signals where 1s are spikes and 0s are not spikes. Such data are common to a variety of domains and are classically analogous to electrochemical signals in the human brain. Spike train datasets are generated from event cameras, which resemble the human eye. Event cameras, also called neuromorphic cameras, require little energy and are designed to capture objects at high speed. Thus, spike train datasets naturally arise during the development of dynamic vision devices [1, 2, 3, 4, 5]. Recently, spike train classification has attracted much attention in the machine learning community [6, 7, 8, 9, 10, 11, 12]. Compared with the traditional sequence classification tasks, spike train classification is unique in the following two aspects [13, 14, 15, 16]: 1) Temporal-sparsity of signals of interest. The label of a spike train is only related with certain objects that may only appear in a very small portion of the whole time window. 2) Temporal-noise problem. The signals at the majority of time steps are generated from background activities that are not related to objects of interest. These two properties of spike train classification impede the application of the widely used deep learning models, e.g., recurrent neural networks (RNNs), because of their high computational costs, unnecessarily spent on the whole time window. Tasks on spike train data are instead usually processed on energy-sensitive platforms such as wireless monitors and drones, so more computationally efficient models are required. To this end, we propose a new design principle to guide developement of machine learning models for spike train classification: perform intensive computation only when signals of interest appear. Knowledge Gap. Spiking Neural Networks (SNNs) are potential candidates for spike train classification [8, 6, 9], since they are attempt to meet the aforementioned design principle by considering the temporal-sparsity of spike trains. They take spike trains as inputs and outputs, using biologically inspired, event-driven computation and communication in their design. An SNN neuron’s core function is to react only when its cummulative membrane potential exceeds a fixed value. As a result, the neuron has a chance to be activated only when it currently has an event signal, which is passed in as a binary spike. Thus, compared to traditional deep learning models, SNNs can build large-scale neural networks with far less energy and memory for spike train classification. However, SNNs primarily consider the temporal-sparsity of spike trains, overlooking the critical aspect of temporal-noise. As illustrated in Figure 1, consider a scenario where a drone equipped with an event camera detects obstacles to adjust its route. For most of the operational timeline, the camera records signals irrelevant to obstacle detection. Nevertheless, SNNs respond to any detected signals, even if they are merely noise. Consequently, SNNs often fail to adhere to the principle that models should activate only in response to signals of interest. We find that this misalignment is a fundamental factor contributing to the poor generalization and decreased energy efficiency observed in SNNs in real-world applications. To achieve high classification accuracy with low computational cost for real-world spike trains, we need to follow the aforementioned design principle by considering both temporal-sparsity of useful signals and temporal-noise issue. An intuitive approach, which we pioneer in this paper, is for the model to stop processing data when the relevant object is out of its field of view. This behavior is analogous to how we open and close our eyes to filter out the information we see. Challenges. We propose a novel method for allowing SNNs to efficiently classify spike trains. Solving this problem is challenging for two main reasons: • Neuron Consistency: The promise of SNNs comes from their likeness to real neural circuits in the human brain. Maintaining this similarity is essential to successful SNNs. However, in the standard SNN when a neuron enters a hibernation state, it is hard to wake it up again if there is no new signal input to the network. This means that if the model ignores the input at a timestep, the neurons in the network will lack new input signals. This keeps the neurons silent, making the model likely to ignore potentially useful signals in the future. Thus, when extending SNNs to our problem setting, it is challenging to train successful spiking neurons to skip updates. • Non-differentiability: SNNs are notoriously difficult to train due to non-differentiable nature of spike activity. Most related works use the rectangular function or sigmoid to approximate the corresponding derivative. However, in practice we find that when our optimization objective considers both accuracy and efficiency, this approximation leads to decayed performance. Additionally, optimization largely depends on the initial values of the parameters of the model. Even though some parameter initialization methods such as Glorot [17] work for traditional artificial neural networks, they lack theoretical basis in SNNs. Designing an efficient optimization algorithm is the second challenge we face. Figure 2: Differences among Recurrent Neural Network (RNN) , SkipRNN [18], Spiking Neural Network (SNN) [8, 9] and SkipSNN (ours). SkipSNN outperforms others in computational efficiency and classification accuracy with an event-attention mechanism for noise filtering. Proposed Method. To solve our problem, we introduce an event attention mechanism that enables SNNs to dynamically highlight useful signals in the input spike train. We extend existing SNNs to have two different states: awake and hibernating, inspired by how people’s eyes open and close, turning on and off data intake. If our SNN enters its awake state at time step t, it will consider the input at t. Otherwise, if it hibernates at time step t, it will ignore the input at t. To this end, we design a controller that switches the model between these two states. Since this is not differentiable, we also introduce a new loss function with a penalty that trades off accuracy and computational cost. In this way, our extended SNN learns to mask out noise by skipping updates and shorten the effective size of the computational graph without requiring any additional supervision signal. We refer to our model as SkipSNN and illustrate the difference between it and traditional SNN in Figure 2. Contributions. Our key contributions are as follows: • We define the problem and modeling principle of general spike train classification, which is important for smart dynamic sensor systems with limited energy. • We propose SkipSNN, which solves this problem and can be used on energy-limited dynamic sensor devices. • We develop an efficient optimization technique to train our SkipSNN model. • We demonstrate that our model outperforms recent state-of-the-art alternatives by achieving higher accuracy and lower computational cost when tested on both the neuromorphic MNIST and DVS-Gesture datasets. The rest of our paper is organized as follows. First, we review related work, then introduce details of the background methods. Next, in Section 4, we present our proposed method. We then describe our experimental setup and discuss our results in Section 5. Finally, we conclude the paper with key take-aways and give some directions for future work."
https://arxiv.org/html/2411.05802v1,Similarity-based Context Aware Continual Learning for Spiking Neural Networks,"Biological brains have the capability to adaptively coordinate relevant neuronal populations based on the task context to learn continuously changing tasks in real-world environments. However, existing spiking neural network-based continual learning algorithms treat each task equally, ignoring the guiding role of different task similarity associations for network learning, which limits knowledge utilization efficiency. Inspired by the context-dependent plasticity mechanism of the brain, we propose a Similarity-based Context Aware Spiking Neural Network (SCA-SNN) continual learning algorithm to efficiently accomplish task incremental learning and class incremental learning. Based on contextual similarity across tasks, the SCA-SNN model can adaptively reuse neurons from previous tasks that are beneficial for new tasks (the more similar, the more neurons are reused) and flexibly expand new neurons for the new task (the more similar, the fewer neurons are expanded). Selective reuse and discriminative expansion significantly improve the utilization of previous knowledge and reduce energy consumption. Extensive experimental results on CIFAR100, ImageNet generalized datasets, and FMNIST-MNIST, SVHN-CIFAR100 mixed datasets show that our SCA-SNN model achieves superior performance compared to both SNN-based and DNN-based continual learning algorithms. Additionally, our algorithm has the capability to adaptively select similar groups of neurons for related tasks, offering a promising approach to enhancing the biological interpretability of efficient continual learning.","Lifelong learning is the prominent capability of biological intelligence and the significant challenge of artificial intelligence. In the process of continuously encountering new environments, the brain effectively identifies the connections between new and old knowledge through task contexts. It reshapes the neural network by associating new tasks with similar prior knowledge to adapt to new information, while strengthening the old knowledge Bar (2007, 2004). However, there is still much room for existing continual learning research to improve context-based efficient and flexible learning. Spiking Neural Networks (SNNs) Maass (1997) have been extensively researched due to their high efficiency and bio-interpretability, incorporating SNNs with continual learning mechanisms of the brain provides natural advances. Nevertheless, to the best of our knowledge, there is little continual learning for SNNs. Only ASP Panda et al. (2017) and HMN Zhao et al. (2022) use STDP-based regularization and neuronal activity-based subnetwork selection to overcome catastrophic forgetting, but they are only suitable for shallow networks. DSD-SNN Han et al. (2023b) and SOR-SNN Han et al. (2023a) apply brain-inspired continual learning algorithms to deep SNNs, ignoring the effect of task-to-task associations. The brain adaptively modulates neuronal generation, allocation, extinction, and reuse for continual learning. Therefore, except for a few continual learning algorithms based on SNNs, we also focus on structure extension algorithms based on DNNs Van de Ven and Tolias (2019). Structure expansion methods assign separate sub-network structures for different tasks, ensuring that learning new tasks does not interfere with previous ones. They can be categorized into progressive neural networks and subnetwork selection algorithms. Among them, progressive networks require assigning a new network to each new task, with full connectivity between the old and new networks Rusu et al. (2016); Siddiqui and Park (2021). Subnetwork selection algorithms select task-specific sparse masks among a finite network Fernando et al. (2017); Gao et al. (2022) Chandra et al. (2023); Hu et al. (2024). However, these two structure expansion algorithms face the following challenges: 1) Catastrophic increase in energy consumption due to the growth of the network scale. The number of progressive neural network parameters increases linearly with the number of tasks Yan et al. (2021); Huang et al. (2023). Despite the subnetwork selection algorithms managing network size, the global network state transitions from sparse to fully connected configuration, resulting in augmented energy consumption Rajasegaran et al. (2019); Xu and Zhu (2018). 2) Cross-task knowledge transfer: the progressive network reuses all past knowledge indiscriminately Rusu et al. (2016), and the subnetwork algorithms do not take into account the association between tasks when learning task-specific masks Dekhovich et al. (2023); Sokar et al. (2021). 3) Structure expansion algorithms rely on prior knowledge of the task affiliation of the current sample to determine the relevant elements to utilize Yoon et al. (2018); Chandra et al. (2023). Thus, most algorithms are primarily suited for task-incremental learning (completing a given task in testing) and are insufficient for class-incremental learning (completing all learned tasks in testing). To address these issues, DNN-based continual learning has been drawn to some extent from the biological continual learning mechanisms Ma et al. (2023), such as contextual similarity recognition. They consider similarity usually focus on the synapses Ke et al. (2022) and data Wang et al. (2023) and involve designing additional evaluation networks Ke et al. (2020). Upon identifying similar tasks, all neurons of the similar tasks are directly reused without any distinction Wang et al. (2022). These factors can lead to different parts in similar tasks interfering, neither are they able to activate similar neural circuits in similar tasks as the brain does. Inspired by the brain contextual task knowledge recognition and flexible neural circuit allocate and reuse mechanisms, we propose the Similarity-based Context Aware continual learning for Spiking Neural Networks (SCA-SNN). Firstly, we designed a task similarity evaluation method that integrates the current data and network state to determine task similarity. Leveraging this assessment, we adaptively reuse neurons from previous tasks and generate a certain number. The underlying principle is that the more similar the new task is to the previous one, the less new network expansion and the more reuse of existing network. More importantly, to avoid redundancy caused by the extensive reuse of neurons, we draw inspiration from the developmental plasticity of human brain neurons, which adhere to the principle of ’use it or lose it’ Bruer (1999). This means that neurons habituated to certain tasks require stronger repetitive stimulation to be reactivated in a new task Grissom and Bhatnagar (2009), while those unrelated to the new tasks will be disconnected. We design a gradient-based method for selecting reused neurons, ensuring that only neurons contributing effectively to the new task are reused. We validate the proposed model on CIFAR100, mini-Imagenet general continual learning dataset, and FMNIST-MNIST, SVHN-CIFAR100 mixed dataset. Our model effectively identifies task similarity relationships to guide more flexible and efficient neuron allocation, thereby reducing energy consumption and achieving superior performance. Meanwhile, the proposed model can adaptively assign similar neuron populations to similar tasks like the human brain. Our SCA-SNN contribution points are as follows: \bullet We propose the Similarity-based Context Aware model in SNN, which leverages task similarity to selectively reuse similar neurons from previous tasks and flexibly expand new neurons for new tasks. The proposed model promotes cross-task knowledge transfer and belongs to advanced exploration for brain-inspired SNN continual learning. \bullet We design an effective neuron reuse strategy inspired by the habituation mechanism observed in the biological brain neurons. This approach carefully selects neurons that are truly beneficial for new tasks. By filtering out superfluous neurons, our approach enhances the efficiency of energy and utilization of known knowledge, preventing irrelevant neurons from interfering with the execution of new tasks. \bullet Extensive experiments across various class-incremental and task-incremental learning demonstrate that the proposed model achieves the state-of-the-art performance of the spiking neural networks. Meanwhile, due to the sparsity of the network and the discrete characterization of SNN, the proposed method significantly reduces the energy consumption."
https://arxiv.org/html/2411.05801v1,Do LLM Personas Dream of Bull Markets? Comparing Human and AI Investment Strategies Through the Lens of the Five-Factor Model,"Large Language Models (LLMs) have demonstrated the ability to adopt a personality and behave in a human-like manner. There is a large body of research that investigates the behavioural impacts of personality in less obvious areas such as investment attitudes or creative decision making. In this study, we investigated whether an LLM persona with a specific Big Five personality profile would perform an investment task similarly to a human with the same personality traits. We used a simulated investment task to determine if these results could be generalised into actual behaviours. In this simulated environment, our results show these personas produced meaningful behavioural differences in all assessed categories, with these behaviours generally being consistent with expectations derived from human research. We found that LLMs are able to generalise traits into expected behaviours in three areas: learning style, impulsivity and risk appetite while environmental attitudes could not be accurately represented. In addition, we showed that LLMs produce behaviour that is more reflective of human behaviour in a simulation environment compared to a survey environment.","Large Language Models (LLMs) have demonstrated the ability to adopt human personas to produce a believable simulation of human behaviour [20]. Past works have investigated LLM powered simulations of human personalities [8] and the effect of these personalities on model output [12]. There has been some research into the downstream behavioural impacts of these personalities in simulations [19]. However, these works are limited as they primarily focus on whether different behaviours can be produced through assigned personalities, rather than examining if these behaviours are truly representative of a human population. For LLM-powered simulations to be generally applicable to business problems, they need to accurately represent a broad range of human behaviours. If they can only reliably represent a small subset of personalities, any results will inherently be biased towards those groups. Therefore, any reliance on these systems for any business activities will exhibit the same biases, and potentially discriminate against other groups. This study aims to address this limitation by investigating if LLM-powered personas 111We will refer to an LLM-powered persona as simply ”persona” for the remainder of the paper can reliably interpret a human personality model (specifically the five-factor model) and map personality traits into specific behaviours that are consistent with past human research. We will do this by investigating the consistency and persistance of simulated behaviours in investment-related decision-making. By doing so, we aim to assess whether the simulated traits produce coherent behavioural patterns across diverse scenarios, similar to the relationship between personality and investment related behaviours observed in a human population. Extensive research has been conducted to determine the behavioural impacts of one’s personality. These studies have explored various behaviours including investment attitudes [10], creative decision making [21] and learning style [22]. Various models exist as means of simplifying human personality and representing it as a combination of traits. The five-factor model [7] proposes human personality as a combination of the following traits: • Openness: A trait that represents a need for variety, novelty and change. • Conscientiousness: A trait that represents achievement striving and aspirational behaviours. • Agreeableness: A trait that represents compliance and social deference. • Extraversion: A trait that represents companionship and social stimulation preferences. • Neuroticism: A trait that represents an individual’s emotional stability. This model has been shown to have consistency and replicability across different methodologies [3] and has proven validity across cultures [17]; hence becoming one of the most used metrics for personality assessment [6]. We lean on these findings and will use the five-factor model in our study. This study explores the intersection between LLM personality research and behavioural personality research, focusing on the following question: RQ: Can LLMs accurately translate assigned personality traits into behaviours, specifically in investment tasks, in a manner consistent with human personality? To answer this, we developed a set of LLM-powered personas that encompassed a full range of human personality traits. These personas completed a short behavioural survey derived from past research to determine if they can associate personality traits with specific behaviours. The personas were then given an investment task to determine if these results could be generalised and produce meaningful behavioural differences in a simulated environment."
https://arxiv.org/html/2411.05799v1,"NeoPhysIx: An Ultra Fast 3D Physical Simulator 
as Development Tool for AI Algorithms","Traditional AI algorithms, such as Genetic Programming and Reinforcement Learning, often require extensive computational resources to simulate real-world physical scenarios effectively. While advancements in multi-core processing have been made, the inherent limitations of parallelizing rigid body dynamics lead to significant communication overheads, hindering substantial performance gains for simple simulations.This paper introduces NeoPhysIx, a novel 3D physical simulator designed to overcome these challenges. By adopting innovative simulation paradigms and focusing on essential algorithmic elements, NeoPhysIx achieves unprecedented speedups exceeding 1000x compared to real-time. This acceleration is realized through strategic simplifications, including point cloud collision detection, joint angle determination, and friction force estimation.The efficacy of NeoPhysIx is demonstrated through its application in training a legged robot with 18 degrees of freedom and six sensors, controlled by an evolved genetic program. Remarkably, simulating half a year of robot lifetime within a mere 9 hours on a single core of a standard mid-range CPU highlights the significant efficiency gains offered by NeoPhysIx. This breakthrough paves the way for accelerated AI development and training in physically-grounded domains.","I INTRODUCTION To develop effective robot controllers, optimize robots’ planning behavior, or enable offline learning techniques such as Reinforcement Learning, very fast and stable physical simulators are essential. These simulators must handle 3D rigid bodies, as well as various types of sensors and actuators, to accurately model complex robotic systems [1, 2, 3, 4, 5, 6, 7]. Over the years, simulation algorithms have been significantly optimized to deliver faster results [8]. Many simulators now meet the real-time performance criterion. However, with the 4.0 GHz clock frequency barrier for CPUs in place [9], further improvements in simulation speed have stagnated. Real-time simulation is particularly important for the gaming industry, where the most advanced simulators are optimized to meet real-time requirements even in complex scenarios. Despite these improvements, extremely fast simulation speeds are still required for applications such as evolving learning neuro-controllers, executing real-time planning in complex environments, or optimizing robot construction parameters. Simulators like PhysX (NVIDIA), ODE (Open Dynamics Engine), Havok, and Bullet are fine-tuned to deliver data at rates exceeding real-time. However, achieving a frame rate that is a multiple of real-time often requires substantial software restructuring or implementing unconventional solutions, such as using separate threads for each computed scene. These optimizations reflect the primary focus on meeting game physics requirements rather than the unique needs of robotics. Projects involving Artificial Intelligence, where algorithms require the capability to test behaviors across diverse scenarios, demand specialized simulators. These simulators must not only meet real-time performance requirements but also deliver results at the highest possible speeds. The following sections introduce a new simulator, NeoPhysIx, capable of achieving up to one million frames per second on a single thread of a standard Intel i5 processor. Key methods that enable this speedup, such as point cloud collision detection, joint angle calculation, and friction force estimation, will be discussed in detail."
https://arxiv.org/html/2411.05798v1,A Genetic Algorithm for Multi-Capacity Fixed-Charge Flow Network Design,"The Multi-Capacity Fixed-Charge Network Flow (MC-FCNF) problem, a generalization of the Fixed-Charge Network Flow problem, aims to assign capacities to edges in a flow network such that a target amount of flow can be hosted at minimum cost. The cost model for both problems dictates that the fixed cost of an edge is incurred for any non-zero amount of flow hosted by that edge. This problem naturally arises in many areas including infrastructure design, transportation, telecommunications, and supply chain management. The MC-FCNF problem is NP-Hard, so solving large instances using exact techniques is impractical. This paper presents a genetic algorithm designed to quickly find high-quality flow solutions to the MC-FCNF problem. The genetic algorithm uses a novel solution representation scheme that eliminates the need to repair invalid flow solutions, which is an issue common to many other genetic algorithms for the MC-FCNF problem. The genetic algorithm’s efficiency is displayed with an evaluation using real-world \chCO2 capture and storage infrastructure design data. The evaluation results highlight the genetic algorithm’s potential for solving large-scale network design problems.","The Multi-Capacity Fixed-Charge Network Flow (MC-FCNF) problem is a well-studied optimization problem encountered in many domains including infrastructure design, transportation, telecommunications, and supply chain management [1, 2, 3]. In the MC-FCNF problem, each edge in the network has multiple capacities available to it, with each capacity having its own fixed construction and variable utilization costs. The objective of the MC-FCNF problem is to assign capacities to edges in the network such that a target flow amount can be hosted at minimal cost. The MC-FCNF problem is a generalization of the Fixed-Charge Network Flow (FCNF) problem, which has a single capacity (and fixed and variable costs) available per edge. The MC-FCNF problem is NP-Hard to approximate within the natural logarithm of the number of vertices in the graph [3]. As such, finding optimal solutions to large instances is often computationally infeasible. Significant work has already been done on solving the MC-FCNF and FCNF problems using many techniques including mathematical programming, branch and bound, and optimal approaches [4, 5, 6, 7, 8]. Multi-capacity edge networks are often referred to as buy-at-bulk network design problems, and are often framed as facility location problems, which is similar to the MC-FCNF problem but with added demand constraints on sinks [9, 10, 11]. Genetic algorithms have also been introduced for variants of the MCNF problem [12, 13, 14, 15, 16, 17, 18, 19]. In this paper, we introduce a novel genetic algorithm to solve the MC-FCNF problem. The novel contribution of our genetic algorithm is the representation of a flow solution by an array of parameters that scale the fixed-costs for each edge in the network. This representation ensures that each array corresponds to a valid flow, thereby eliminating the need for computationally expensive repair functions that are required by other genetic algorithms for the MC-FCNF problem [12, 13, 15, 20, 17, 18, 19]. By avoiding costly repair functions, the proposed algorithm is able to efficiently find high-quality solutions to very large MC-FCNF problem instances. The proposed genetic algorithm is inspired by slope scaling techniques previously employed for the FCNF problem [4, 21]. It is a matheuristic, as it employs mathematical programming to calculate a flow solution from a linear program parameterized with the fixed-cost scaling arrays [22]. Our genetic algorithm is similar to an algorithm proposed by [23], though ours takes a different two-stage approach to handle multi-capacity edges. Additionally, we provide more insight into the existence of the optimal solution in the search space. An evaluation is presented that designs \chCO2 capture and storage (CCS) infrastructure deployments using real-world data composed of thousands of vertices and tens of thousands of edges. In the evaluation, the genetic algorithm is compared to the the solution of an optimal integer linear program formulation of the MC-FCNF problem. Results from the evaluation demonstrate the utility of the genetic algorithm for very large networks, even if the solution is very small compared to the full network. The rest of this paper is organized as follows: Section 2 formally introduces the MC-FCNF program and formulates it as an integer linear program. Section 3 presents a linear programming modification to the integer linear program that serves as the core to the genetic algorithm. Section 4 introduces the genetic algorithm and discusses the existence of the optimal solution in the search space. Section 5 presents an evaluation of the genetic algorithm on real-world CCS data and the paper is concluded in Section 6."
https://arxiv.org/html/2411.05793v1,A Comprehensive Survey of Time Series Forecasting: Architectural Diversity and Open Challenges,"Time series forecasting is a critical task that provides key information for decision-making across various fields, such as economic planning, supply chain management, and medical diagnosis. After the use of traditional statistical methodologies and machine learning in the past, various fundamental deep learning architectures such as MLPs, CNNs, RNNs, and GNNs have been developed and applied to solve time series forecasting problems. However, the structural limitations caused by the inductive biases of each deep learning architecture constrained their performance. Transformer models, which excel at handling long-term dependencies, have become significant architectural components for time series forecasting. However, recent research has shown that alternatives such as simple linear layers can outperform Transformers. These findings have opened up new possibilities for using diverse architectures. In this context of exploration into various models, the architectural modeling of time series forecasting has now entered a renaissance. This survey not only provides a historical context for time series forecasting but also offers comprehensive and timely analysis of the movement toward architectural diversification. By comparing and re-examining various deep learning models, we uncover new perspectives and presents the latest trends in time series forecasting, including the emergence of hybrid models, diffusion models, Mamba models, and foundation models. By focusing on the inherent characteristics of time series data, we also address open challenges that have gained attention in time series forecasting, such as channel dependency, distribution shift, causality, and feature extraction. This survey explores vital elements that can enhance forecasting performance through diverse approaches. These contributions lead to lowering the entry barriers for newcomers to the field of time series forecasting, while also offering seasoned researchers broad perspectives, new opportunities, and deep insights.","Figure 1: Overview of This Survey (Section 1)(Section 2)(Section 3)(Section 4)(Section 5)(Section 6) Time series forecasting (TSF) is a task that predicts future values based on sequential historical data over time [34]. It is utilized as a key decision-making tool in various fields, such as economics and finance, supply chain management, transportation, energy, weather and healthcare [35, 1, 6, 135, 37, 164]. Such applications offer various opportunities, including cost reduction, increased efficiency, and enhanced competitiveness [35]. The inherent diversity and complexity of time series data unfortunately make forecasting challenging. In addition to the apparent information, various hidden patterns make it challenging to learn temporal dependencies, and irregular values at times further complicate the problem. In multivariate problems, additional factors such as channel correlation make the task even more difficult (Section 2.1). Furthermore, time series data exhibits different characteristics depending on the domain, and the various times and environments in which the series is collected result in significantly different patterns (Section 2.3). Because of this, TSF problems exhibit limited model generalizability, requiring diverse architectures and approaches. The increasingly complicated TSF problems are presenting researchers with growing challenges, which has recently led to the active development of new methodologies and algorithms to address these issues [104]. The explosive increase in the number of papers accepted at major AI and Machine Learning conferences (Fig. 2) demonstrates that TSF research is becoming increasingly important in the AI and Machine Learning fields. As various studies addressing time series forecasting problems are being actively conducted, survey papers are also being frequently published (Table 1). Over time, numerous survey papers have systematically organized the vast landscape of TSF, offering in-depth research that has provided valuable guidance and direction for researchers. However, existing survey papers still have room for improvement, particularly in addressing the inevitable increase in model diversity and the open challenges in the field. 20202021202220232024020406080Number of PapersNeurIPS: Conference on Neural Information Processing SystemsICLR: International Conference on Learning RepresentationsICML: International Conference on Machine LearningAAAI: Association for the Advancement of Artificial IntelligenceIJCAI: International Joint Conference on Artificial IntelligenceKDD: Knowledge Discovery and Data Mining Figure 2: Number of Top-tier AI and ML Conference Papers on Time Series Forecasting Models for TSF have undergone various stages of development over an extended period. In the past, statistical methods based on moving averages were predominantly used, which later evolved into traditional approaches such as Exponential Smoothing and ARIMA [11]. Machine learning techniques such as Tree Models [144] and Support Vector Machines (SVM) [31] have also been frequently used, but they had limitations in learning complex nonlinear patterns (Section 3.1). With the increase in available data and advancements in hardware computing power, various deep learning architectures such as MLPs [152], RNNs [74], CNNs [94], and GNNs [155] were developed, enabling the learning of more complex patterns. However, the performance of these early deep learning architectures was constrained by their intrinsic designs. To overcome these structural limitations, variants such as Long Short-Term Memory (LSTM) [71] and Temporal Convolutional Networks (TCN) [9] have been widely utilized (Section 3.2). Transformers [177], known for their ability to handle long-term dependencies, have demonstrated excellent performance in natural language processing and have been naturally extended to time series data as well. While Transformers have shown good performance in TSF and become widely popular, recent cases have shown that simple linear models can outperform Transformer models (Section 3.3). As a result, there has been a significant increase in reconsidering traditional deep learning methodologies, along with growing interest in various architectures such as foundation models, diffusion models and Mamba models (Section 4.2, 4.3, 4.4, 4.5). The Transformer model continues to improve in performance and still plays a significant role (Section 4.1). In this way, TSF has entered a renaissance of modeling, with various methodologies actively competing without being dominated by any single approach (Fig. 3). In this context, this survey offers two major strengths that set it apart from previous TSF survey papers. Figure 3: Evolution of Time Series Forecasting Models (Section 3.1)(Section 3.2)(Section 3.3)(Section 4.1)(Section 4.2)(Section 4.3)(Section 4.4)(Section 4.5) First, we focus on the inevitable diversification of architectures, providing a timely and comprehensive view to understand the current trend of architecture diversification. Existing TSF survey papers, such as [186, 216, 105, 129, 137], focus on providing detailed analyses of specific architectures but have limitations when it comes to broadly comparing diversified architectures, including newly emerging ones. This paper systematically compares the developmental progress of various architectures (MLPs, CNNs, RNNs, GNNs, Transformer, Diffusion, foundation models, Mamba) and analyzes the strengths, weaknesses, and contributions of each. In addition, it addresses the performance of hybrid models that combine the strengths of multiple architectures, clearly highlighting key trends in TSF. With these contributions, readers can effectively understand the continuously evolving trends and directions of advancement in the field. Through this, it lowers the entry barrier for newcomers to TSF and provides a comprehensive roadmap that opens up new research opportunities for established researchers. Second, we explore from the perspective of open challenges. Although numerous advanced architectures have resolved many issues, the core challenges in TSF continue to persist. In particular, issues such as channel correlation, distribution shifts, causality, and feature extraction (Section 5) remain significant challenges that need to be addressed (Fig. 4). This survey explores the latest methodologies aimed at addressing these challenges and provides readers with valuable insights for problem-solving. While previous surveys have provided useful perspectives on open challenges, they have not explored these issues in sufficient depth. This survey aims to bridge that gap by offering a more comprehensive analysis and proposing new solutions. Table 1: Summary of Survey Papers on Time Series Forecasting Articles Focus Broad Architecture Review Forecasting Task-Intensive Recent Work Reference Time-series forecasting with deep learning: a survey · Encoder-Decoder structures and Hybrid models · Interpretability and causal inference for decision support ✓ ✓ [104] Forecast Methods for Time Series Data: A Survey · Traditional forecasting methods: statistical, ML, and DL · Challenges in preprocessing, modeling, and parallel computation ✓ ✓ [115] Deep Learning for Time Series Forecasting: Tutorial and Literature Survey · Key components of DL TSF · Practical applicability ✓ ✓ [13] A Review on Deep Sequential Models for Forecasting Time Series Data · Common deep sequential models · Guidelines on implementation, application, and optimization ✓ ✓ [4] Transformers in Time Series: A Survey · Transformer structure variations and architectural improvements ✓ [186] Long Sequence Time-Series Forecasting with Deep Learning: A Survey · Definition of long sequence forecasting challenges from various perspectives · New classification system and performance evaluation methods ✓ [26] Machine Learning Advances for Time Series Forecasting · Economic/financial TSF · Categorized into linear and nonlinear ✓ [126] Diffusion Models for Time-Series Applications: A Survey · Diffusion models in time series analysis ✓ [105] The Rise of Diffusion Models in Time-Series Forecasting · Diffusion-based forecasting models with case studies ✓ ✓ [129] Foundation Models for Time Series Analysis: A Tutorial and Survey · Application of foundation models in time series analysis ✓ ✓ [103] Large Language Models for Time Series: A Survey · Categorization of methodologies for LLMs in time series · Explanation of bridging modality gaps ✓ ✓ [216] A Survey of Time Series Foundation Models · Approaches to Foundation Models and LLMs in time series analysis ✓ ✓ [204] Mamba-360: Survey of State Space Models · Emphasis on SSM advantages in long time series · Comparison of domain-specific applications and performance ✓ ✓ [137] This Survey · A comprehensive overview of the evolution of TSF models, introducing innovative architectures · Discussion of critical open challenges in forecasting tasks, along with advanced solutions to address them ✓ ✓ ✓ (Section 5.1)(Section 5.2)(Section 5.3)(Section 5.4) Figure 4: Overview of Latest Open Challenges in Time Series Forecasting This survey covers the fundamental concepts of time series data and the problem definition of forecasting in Section 2, followed by an examination of the evolution of past methodologies in Section 3. In Section 4, it analyzes the key features of the latest models, and finally, in Section 5, it explores the open challenges in TSF and their solutions. Through this, readers will gain a broad understanding of the past and present of TSF research and acquire new ideas for future studies."
