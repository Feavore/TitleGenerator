URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10422v1,Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash,"Large Language Models (LLMs) have shown impressive capabilities in complex tasks and interactive environments, yet their creativity remains underexplored. This paper introduces a simulation framework utilizing the game Balderdash to evaluate both the creativity and logical reasoning of LLMs. In Balderdash, players generate fictitious definitions for obscure terms to deceive others while identifying correct definitions. Our framework enables multiple LLM agents to participate in this game, assessing their ability to produce plausible definitions and strategize based on game rules and history. We implemented a centralized game engine featuring various LLMs as participants and a judge LLM to evaluate semantic equivalence. Through a series of experiments, we analyzed the performance of different LLMs, examining metrics such as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The results provide insights into the creative and deceptive capabilities of LLMs, highlighting their strengths and areas for improvement. Specifically, the study reveals that infrequent vocabulary in LLMs’ input leads to poor reasoning on game rules and historical context.111https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash","Large Language Models (LLMs) have recently been employed as agents in various complex tasks, showcasing their potential in dynamic, interactive environments (Dorbala et al., 2024; Singh et al., 2024). This has led to a growing interest in LLM-based multi-agent systems (LLM-MA), particularly within the realm of gaming (Mukobi et al., 2024; Xu et al., 2023). Games offer a structured yet flexible platform to analyze and understand LLM behavior under diverse scenarios (Light et al., 2023). Currently, LLMs are typically evaluated through static tasks (Lee et al., 2023; Zhao et al., 2024; Gómez-Rodríguez and Williams, 2023). Traditional games like Avalon (Wang et al., 2023) and Werewolf (Xu et al., 2024) have also been used to benchmark LLMs, focusing on logical reasoning and strategic interaction. These games require players to engage in deception, deduction, and negotiation, providing valuable insights into LLMs’ decision-making processes. However, these studies often overlook the assessment of creativity. To address this gap, we introduce a simulation framework for the game Balderdash. In this game, players generate plausible yet fictitious definitions for obscure terms, aiming to deceive other players while identifying the correct definitions. We argue that Balderdash can be used to evaluate both the creativity and logical reasoning of LLMs, challenging the models to balance these two crucial aspects and providing a comprehensive assessment of their capabilities. In this paper, we aim to assess the creativity of LLMs by evaluating their ability to generate plausible definitions for obscure words in Balderdash. We will further examine their logical reasoning skills by observing how effectively they deceive opponents and identify correct definitions in the context of the game. Finally, we will investigate the performance of these models in a multi-agent setting where both creativity and logical deduction are crucial for success."
https://arxiv.org/html/2411.09954v1,Reaching Resilient Leader-Follower Consensus in Time-Varying Networks via Multi-Hop Relays,"We study the problem of resilient leader-follower consensus of multi-agent systems (MASs) in the presence of adversarial agents, where agents’ communication is modeled by time-varying topologies. The objective is to develop distributed algorithms for the nonfaulty/normal follower agents to track an arbitrary reference value propagated by a set of leaders while they are in interaction with the unknown adversarial agents. Our approaches are based on the weighted mean subsequence reduced (W-MSR) algorithms with agents being capable to communicate with multi-hop neighbors. The proposed algorithms solve our resilient leader-follower consensus problem with agents possessing first-order and second-order dynamics. Moreover, we characterize tight necessary and sufficient graph conditions for the proposed algorithms to succeed in terms of the novel notion of jointly robust following graphs. Our graph condition is tighter than the sufficient graph conditions in the literature when agents use only one-hop communication (without relays). Using multi-hop relays, we are able to enhance robustness of leader-follower networks without increasing physical communication links and obtain further relaxed graph requirements for our algorithms to succeed. Numerical examples are given to verify the efficacy of the proposed algorithms.","\IEEEPARstart Over the past few decades, distributed consensus has emerged as a cornerstone of research in the fields of multi-agent systems (MASs) and distributed algorithms [3, 1, 2]. In such a problem, agents connected over a network try to reach consensus on a common value while interacting with only neighboring agents. Stemming from this concept, extensive applications and algorithms have been devised to overcome various industrial challenges [7, 5, 6, 4]. Concurrently, growing concerns over cyber security within MASs have amplified the significance of consensus protocols, especially in scenarios where adversarial agents induce failures or launch attacks, e.g., [9, 11, 8, 10]. Under this topic, the problems of resilient consensus have drawn much attention in areas of systems control, distributed computing, and cooperative robotics [14, 13, 12, 15, 16], where the nonfaulty, normal agents aim to reach consensus despite the possible misbehaviors of adversarial agents. A common goal in this setting is that normal agents arrive at the same value located within the convex hull of their initial states. However, for applications such as formation control and reliable broadcast, it is desirable that agents together track a specific value which is externally given and may be outside such a convex hull. Thus, it motivates us to extend resilient consensus algorithms for such objectives. A related problem in prior literature is the leader-follower consensus problem, where the goal is for normally behaving agents to come to an agreement on the reference value of a leader or a set of leaders [18, 17]. However, these works considered MASs without any adversarial agents, potentially rendering them vulnerable to random failures or deliberate attacks. Within the domain of distributed computing, considerable efforts have been dedicated to ensuring reliable broadcast [19] as well as the certified propagation algorithm (CPA) [20, 21]. In these works, the objective is for a secure leader to broadcast a reference value to all nodes in the network in the presence of adversarial agents. Additionally, there is a body of research addressing the problem known as resilient distributed estimation. For instance, the work [22] studied resilient parameter estimation, where certain reliable agents drive the errors of the remaining normal agents to the static reference value of zero. Moreover, the authors of [23] investigated a problem where the observation information of the system is resiliently transmitted from a group of source nodes to other nodes that cannot directly observe the system. In this paper, we develop distributed algorithms to tackle resilient leader-follower consensus in time-varying networks. In the literature, many efforts have been devoted to resilient consensus using the so-called mean subsequence reduced (MSR) algorithms [12, 13, 25, 24, 26]. In such algorithms, each normal agent disregards the most deviated states of neighbors to avoid being affected by possible faulty values from adversarial neighbors. Tight graph conditions on static (i.e., time-invariant) network structures guaranteeing the success of MSR algorithms have been derived for the class of malicious agents [13, 27, 14] as well as the class of Byzantine agents [12, 28]. Notably, [13] demonstrated that static networks utilizing MSR algorithms must adhere to a specific structural criterion, called graph robustness, to attain resilient consensus. However, the majority of these studies have been confined to static networks, i.e., communication topologies are fixed across iterations. However, in numerous applications of MASs with physical motions, e.g., formation control of drones and vehicle platoons, the underlying communication network may be time-varying due to limited communication ranges and temporal variations of communication channels [3, 29, 30]. Thus, there is a significant demand for investigating resilient leader-follower consensus in time-varying networks. For instance, the work [31] proved a sufficient condition for the sliding weighted-MSR (SW-MSR) algorithm from [32] to achieve resilient leader-follower consensus to arbitrary static reference values. It reduced the stringent connectivity requirements of MSR algorithms at each iteration. Later, [33] studied resilient leader-follower consensus in static networks with the leader in each network having a dynamic reference value. Meanwhile, several works relaxed the graph connectivity requirements for resilient consensus in static networks through multi-hop communication [15, 34, 28], which enables messages sent by an agent to reach beyond its direct neighbors through relays by middle agents [1, 35]. It can improve network resilience against adversaries without changing the original topology as shown in [15, 34, 36]. Motivated by these works, we are interested to investigate whether multi-hop relays could further help us to acquire a more relaxed condition for leader-follower consensus in time-varying networks. We summarize the contributions of this paper as follows. First, we characterize a necessary and sufficient graph condition for the Multi-hop Weighted-MSR (MW-MSR) algorithm to achieve resilient leader-follower consensus in time-varying networks. Consequently, the normal follower agents are able to track the reference value propagated by a set of leaders in the presence of Byzantine agents, which may also include adversarial leaders. Our graph condition is denoted by a novel notion of jointly robust following graphs with multi-hop communication. Compared to the SW-MSR algorithm [32, 31] storing neighbors’ values for the last certain time steps at each iteration, our approach utilizes neighbors’ values of only the current time step at each iteration. It is notable that even with one-hop communication, our graph condition is tighter than the ones in the resilient leader-follower consensus works with static reference values [31] as well as dynamic reference values [33]. Moreover, by increasing the number of relaying hops, our method can increase the graph robustness against adversaries without changing the network topology. Hence, our approach can tolerate more adversarial nodes compared to the one-hop MSR algorithms [13, 33, 11, 29] as well as the CPA-based methods [20, 21]. Moreover, numerical examples show that our method can achieve resilient leader-follower consensus in sparse time-varying networks where the algorithms in [31, 33] have difficulties. As a side result, we present that the tight graph condition for resilient leader-follower consensus under the malicious model is the same as the one for the Byzantine model, even though malicious agents are less adversarial. Second, we also deal with resilient leader-follower consensus in time-varying networks for agents with second-order dynamics and propose a multi-hop double-integrator position-based MSR (MDP-MSR) algorithm. This extension is vital since double-integrator dynamics are often used to characterize more accurate motions of agents in robotics; see, e.g., [37]. To the best of our knowledge, such a problem has not been investigated in the literature. Furthermore, we derive a necessary and sufficient graph condition for the MDP-MSR algorithm to handle this case. The condition is the same as the one for the MW-MSR algorithm. Moreover, we provide necessary properties for verifying whether network topologies meet our conditions or not. Both theoretical results and numerical examples verify that the proposed algorithm with multi-hop relays can improve the robustness against adversaries in static as well as time-varying networks for agents with second-order dynamics. Lastly, we apply the algorithm for achieving formation control in the leader-follower configuration in the presence of adversaries, which could serve as a basis for applications of, e.g., multi-robot manufacturing in complex industrial sectors. The rest of this paper is organized as follows. In Section II, we outline the problem settings. In Section III, we define the novel notion of joint robust following graphs with multi-hop communication. In Section IV, we derive a tight graph condition under which the MW-MSR algorithm guarantees resilient leader-follower consensus. In Section V, we introduce the MDP-MSR algorithm for MASs with second-order dynamics and provide tight graph conditions for the algorithm to achieve resilient leader-follower consensus in static and time-varying networks. In Section VI, we present numerical examples to verify the efficacy of our algorithms in sparse time-varying networks. Finally, we conclude the paper in Section VII. Compared to the preliminary version of this work [38], the current paper contains additional results for time-varying networks, the results for the secure leader, the results for second-order MASs, and extensive numerical examples."
https://arxiv.org/html/2411.10173v1,Semantics and Spatiality of Emergent Communication,"When artificial agents are jointly trained to perform collaborative tasks using a communication channel, they develop opaque goal-oriented communication protocols. Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly. In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances. We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction. We prove, under mild assumptions, that semantically inconsistent communication protocols can be optimal solutions to the discrimination task, but not to reconstruction. We further show that the reconstruction objective encourages a stricter property, spatial meaningfulness, which also accounts for the distance between messages. Experiments with emergent communication games validate our theoretical results. These findings demonstrate an inherent advantage of distance-based communication goals, and contextualize previous empirical discoveries.","Humans use language in multi-agent social interactions. Pressures of synchronization and collaboration play a central role in shaping the way we communicate. Motivated by this observation and the goal of creating artificial agents capable of meaningful communication, the field of emergent communication (EC) employs a multi-agent environment jointly trained to accomplish a task that requires active transmission of information. The agents utilize a messaging channel that usually takes the form of a discrete sequence of abstract symbols, resembling the structure of human language. Successful optimization results in synchronized agents operating a newly developed communication protocol tailored to the objective. We study a type of EC setup inspired by Lewis games [32] where a sender agent describes a given input and a receiver agent makes a prediction based on that description. The game objective is designed to make the receiver demonstrate knowledge of the original input, which in turn compels the sender to encode relevant information in the message. There are two common objectives used in this type of EC setup (illustrated in Figure 1): Reconstruction In the reconstruction task [34, 44, 45], the original input is re-generated based solely on the message. We are specifically interested in a reconstruction objective that quantifies distance between prediction and target, forming a discrete version of autoencoding [18]. Discrimination In the discrimination task [28, 17, 33], the original input is retrieved from a set of candidates, incentivized by negative log-likelihood. (a) (b) Figure 1: Illustration of the reconstruction and discrimination tasks. The discrimination receiver is given the candidates in addition to the message. A central goal in EC, which motivates our work, is understanding how different factors in the environment affect the emergent protocol, and specifically developing agents and objectives that create protocols with similar characteristics to natural language [41, 25, 45, 2, 40]. Tools and experiments have been developed to evaluate the proximity to natural language by testing for properties such as compositionality [6] or efficiency [7]. Unfortunately, many of these empirical methods show great dissimilarity to human communication. One particularly surprising experiment [3] revealed that protocols created via the discrimination task can generalize extremely well to random noise data, suggesting that they do not signal human-recognizable (meaningful) properties of the input. In this paper, we identify a fundamental property of any meaningful communication protocol, and thus a prerequisite for similarity to natural language. We observe that the discrete bottleneck forces EC protocols to be many-to-one mappings, i.e., that messages likely represent more than one input. With this in mind, we claim that inputs mapped to the same message should be semantically similar, as is the case with human language. We formalize this idea with a mathematical definition that we term semantic consistency. We further develop a stricter version of this definition, spatial meaningfulness, which also accounts for distances between messages, and is therefore better suited to indicate similarity to natural language. Armed with these definitions, we analyze theoretical solutions to the two common EC environments. In the reconstruction setting, under mild assumptions, we prove that every optimal solution is semantically consistent. With a different set of assumptions, we also show that reconstruction-induced communication protocols are spatially meaningful. In sharp contrast, we surprisingly find that the discrimination objective does not guarantee semantic consistency, i.e., a communication protocol can be optimal in a discrimination environment but still not semantically consistent nor spatially meaningful. In fact, we show that uniformly random messages can lead to a globally optimal discrimination objective value, meaning that the relationship between inputs mapped to the same message is potentially arbitrary despite optimally solving the task. Our results provide theoretical support to previous empirical findings, such as the discrimination generalization to noise. We further analyze several common variations of the discrimination game from the EC literature. To support our findings, we run experiments on Shapes [27] and MNIST [30]. The empirical results agree with most of our theoretical findings. However, we observe a gap between theory and practice regarding the level of channel utilization, which we leave for future work to further investigate."
https://arxiv.org/html/2411.10148v1,Multi-UAV Search and Rescue in Wilderness Using Smart Agent-Based Probability Models,"The application of Multiple Unmanned Aerial Vehicles (Multi-UAV) in Wilderness Search and Rescue (WiSAR) significantly enhances mission success due to their rapid coverage of search areas from high altitudes and their adaptability to complex terrains. This capability is particularly crucial because time is a critical factor in searching for a lost person in the wilderness; as time passes, survival rates decrease and the search area expands. The probability of success in such searches can be further improved if UAVs leverage terrain features to predict the lost person’s position. In this paper, we aim to enhance search missions by proposing a smart agent-based probability model that combines Monte Carlo simulations with an agent strategy list, mimicking the behavior of a lost person in the wildness areas. Furthermore, we develop a distributed Multi-UAV receding horizon search strategy with dynamic partitioning, utilizing the generated probability density model as prior information to prioritize locations where the lost person is most likely to be found. Simulated search experiments across different terrains have been conducted to validate the search efficiency of the proposed methods compared to other benchmark methods.","With the rapid advancement of electronic technologies, including more powerful and affordable onboard computers, improved battery life, advanced sensors, and better communication systems, Unmanned Aerial Vehicles (UAVs) have been widely deployed in various applications such as search and rescue operations [1, 2], agricultural monitoring [3, 4], and parcel delivery [5, 6]. In particular, Multi-UAV systems equipped with advanced sensors and communication equipment have gained significant attention. The cooperative operation with Multi-UAV can substantially improve mission success rates and work efficiency. Among various applications, Wilderness Search and Rescue (WiSAR) is particularly well-suited to the solutions provided by Multi-UAV systems. For instance, in 2019 in Queensland, Search and Rescue (SAR) authorities assisted 1,648 people, utilizing 8,733 police person-hours and over 34,000 volunteer hours in SAR [7]. Similarly, the National Missing and Unidentified Persons System in the USA reports that over 600,000 individuals go missing each year, while 70,000 missing persons are reported annually in the UK [8]. Implementing Multi-UAV systems could significantly reduce both the costs and time associated with these missions compared to using human or canine field teams [9]. In WiSAR operations, locating the target quickly is crucial due to the time-sensitive nature of search and rescue efforts. Research indicates that the survival rate of lost hikers and young children aged 4-6 years drops rapidly after the first 24 hours, owing to injuries, exposure, exhaustion, and dehydration [10]. Long search duration result in a larger area to cover, reducing the likelihood of finding the target. Therefore, minimizing search time not only limits the search area but also maximizes the chances of a successful rescue. Another significant challenge of WiSAR is operating in rugged terrains such as mountains, forests, and streams, which complicate tracking moving targets. Additionally, weather conditions, as well as the physiological and psychological state of the person, can influence their decisions in wilderness [11]. These factors collectively make it difficult to predict such target-specific probability distribution. Researchers have studied historical search and rescue cases to understand lost individuals’ behavior. For instance, [12] provides a comprehensive analysis of over 50,000 SAR incidents, detailing factors like recovery locations, survival rates, elevation changes, and dispersion angles. This data helps estimate where a lost person might be. Additionally, regional case studies, such as those in [7], [13], and [14], offer valuable insights for SAR teams before deployment. Historical datasets have led to various mathematical models for predicting a lost person’s location. The Euclidean distance ring model, introduced in 1998 [15], uses concentric rings at 25%, 50%, and 75% distances to estimate the probability of the target being within each range. However, it overlooks elevation changes and obstacles. In contrast, the Target Iso-probability Curves from [16] incorporate terrain and obstacles, providing a more realistic probabilistic representation. Similarly, a Bayesian approach in [17] updates prior beliefs about the person’s movement based on terrain features using a discretized hexagonal grid map. More recent work [18] models complex behaviors by assigning probabilities to actions like seeking higher ground or resting, aiming to better simulate the target’s movement patterns. Despite these advancements, existing methods still neglect how interactions with specific environmental features, such as streams or barriers, impact the probability distribution, which could offer SAR teams more precise location insights. Multi-UAV search strategies have been extensively researched over the past decades. When dealing with a probability distribution of a lost person over time, the challenge for a Multi-UAV search and rescue team differs from the typical coverage planning problem, which focuses on maximizing area coverage, as discussed in [19] and [20]. Instead, the emphasis is on searching guided by a probability map. While centralized control approaches, supported by a ground control station, have been employed in studies such as [21] and [22], there has been a growing preference for distributed systems due to their fault tolerance, scalability, and adaptability. Notable examples include the distributed real-time search path planning methods using distributed model predictive control introduced in [23] and [24], the multi-target search framework proposed in [25], and the moving target search method discussed in [26]. However, many of these methods are based on discretized cell-based systems, which can be limited in resolution and may miss finer details of probability distributions. In distributed systems, continuous communication is often prioritized to keep all UAVs connected, as seen in studies like [27], [23], and [28], focus on keeping all UAVs connected throughout operations, which can ensure stable communication but may limit movement and reduce search efficiency. An alternative is the recurrent connectivity method [29], where UAVs communicate only when valuable information is found. However, this approach is designed for exploration and data collection without using a probability map. In WiSAR, where search areas expand 82 km^{2} every one hour[30], commonly used LoRa devices in wilderness settings have a communication range typically limited to 10-20 km [31]. Thus, maintaining constant connectivity may restrict UAV mobility and efficiency, necessitating a balance between connectivity and search coverage. In multi-agent search operations, a common strategy is for each UAV to move towards high-probability locations while maintaining a safe distance from other UAVs, as presented in [23]. This approach works effectively when the probability areas are convex. However, in complex, non-convex probability landscapes, UAVs may converge on the same local high-probability areas, leading to insufficient exploration of unexplored regions. Some task area assignment methods have been well-investigated for WiSAR to ensure coverage of whole searching area, such as the layered search and rescue algorithm proposed in [32], which assigns UAVs to each probability layer. Since Iso-probability curves was proposed for predicting a lost person’s position by Macwan et al. [16], many researchers have explored specific search strategies utilizing the features of Iso curves. For example, the methods discussed in [11] and further in [33] extend a 3D Iso-probability curve. Similarly, an angular motion control method proposed in [34] and [35] achieves smooth transitions between curves, also incorporating the concept of a time-related confidence area into the search process. However, traditional task area assignment methods often fall short due to their static nature, where each UAV is assigned a specific zone without the flexibility to adapt as the mission evolves. This rigidity can lead to inefficiencies, particularly in complex environments like WiSAR, where search conditions are highly dynamic. Figure 1: Illustration of a distributed multi-UAV system in WiSAR. UAVs communicate with adjacent UAVs within their communication range. One of the predicted trajectories of the lost person are shown in green line. Figure 2: System architecture of the proposed framework. To address above limitations in existing researches, this paper proposes a novel multi-UAV cooperative search strategy designed for a continuous environment, incorporating a topology-based dynamic task area assignment. Additionally, the search is informed by prior information provided through a dynamic probability density map, which is generated using a smart agent-based model. An illustration of the proposed framework work in wildness can be seen in Fig. 1. The main contributions of this paper are summarized as follows: • We introduce a smart agent-based probability distribution model, developed using the Monte Carlo method, which integrates terrain features and the lost person’s behavior profile. This model simulates decision-making in wilderness settings, such as seeking high-altitude areas for better visibility and following known routes. Additionally, a guideline map is created to ensure that simulated trajectories align with natural paths a person might take, considering obstacles like rivers, streams, and impassable areas. • We establish an optimization-based search strategy that operates in continuous space using a dynamic probability distribution map. A receding horizon mechanism is incorporated into the search process to reduce the likelihood of a single UAV getting stuck in local minima. • A proximity control function is implemented as a soft constraint in the distributed multi-UAV search system. It ensures each UAV maintains an appropriate distance from others, facilitating reliable communication and avoiding collisions in expansive wilderness areas where constant connectivity is challenging. This balance maintains essential communication in the distributed system while minimizing the impact on search coverage. • We propose a dynamic partitioning method inspired by Voronoi diagrams for WiSAR missions. This approach assigns each UAV to a specific search area, adapting dynamically as the mission progresses. The method ensures that UAVs do not converge on the same high probability areas and encourages exploration of unsearched regions to maximize coverage. The organization of this paper is as follows: Section II introduces the smart agent-based probability distribution model, covering the guideline strategy, lost person behavior strategy, and the generation of the dynamic probability map. Section III then presents the formulation of the proposed multi-UAV strategy with dynamic task assignment. Finally, Section IV details the simulation results, comparing them with two other benchmark methods, followed by a discussion and conclusions in Section V."
https://arxiv.org/html/2411.09856v1,InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma,"InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments111Github repo: https://github.com/yuanjiayiy/InvestESG. Supported by both PyTorch and GPU-accelerated JAX framework, the benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL’s potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs.","Climate change poses a persistent threat to global stability, with droughts, storms, fires, and flooding becoming more intense and frequent (Field et al., 2012), leading to disruption of the natural ecosystem and significant impacts on the global economy. Addressing climate change requires coordinated efforts across multiple sectors, particularly from large corporations, which are reportedly responsible for over 70% of global industrial greenhouse gas emissions (Griffin and Heede, 2017). While adaptation—preparing for the inevitable consequences of climate change—tends to be party-specific and often driven by financial incentives, mitigation—reducing emissions—presents a de facto social dilemma (Leibo et al., 2017), where the benefits of reduced emissions are shared globally yet the costs are borne locally (Olson Jr, 1971; Dahlman, 1979; Buchanan and Stubblebine, 2006). As corporations are inherently self-interested, they are unlikely to reduce emissions voluntarily without external incentives or regulations. Numerous policies have been proposed to address this challenge. Among these, mandatory Environmental, Social, and Governance (ESG) disclosures have recently been hotly debated in the United States. The Securities and Exchange Commission’s (SEC) proposal, which would require publicly traded companies to disclose climate-related risks, mitigation strategies, and greenhouse gas emissions from their operations, has attracted over 15,000 comments, making it one of the most contentious proposals in the SEC’s history (SEC, 2024a; CNBC, 2024). This has resulted in an indefinite delay in enactment of the policy to allow for further discussion (SEC, 2024b). The US is not alone in facing such a pushback; similar delays are unfolding in the European Union and Korea (Bloomberg News, 2024; Korea Economic Daily, 2023). This highlights the need for thorough research to effectively inform the design and implementation of these policies. Traditional economics and policy research relies on either empirical analysis—which does not enable testing possible new policies (Doshi et al., 2013; Li and Wu, 2020; Krueger et al., 2021)—or theoretical economics models, which are often limited to scenarios with only two agents (e.g., Friedman et al. 2021), or single-period games (e.g., Pástor et al. 2021). In contrast, Multi-Agent Reinforcement Learning (MARL) enables simulating complex interactions between multiple agents over extended time periods, under diverse hypothesized policy settings. Leveraging MARL to address large-scale socio-economic questions is a growing field (Hertz et al., 2023). Prior work has demonstrated the potential of MARL to design effective taxation schemes that enhance both equality and productivity (Zheng et al., 2021), highlighting its relevance for tackling real-world social challenges. We propose using multi-agent reinforcement learning (MARL) to explore the impact of the ESG disclosure policy. We introduce InvestESG, an open-source MARL benchmark, to examine how profit-driven corporations balance short-term profits with long-term climate investments and whether ESG-informed investor choices influence corporate behavior. The simulation involves two agent types: companies and investors. Companies allocate funds to mitigation, greenwashing, and resilience, while investors decide whom to invest in based on their preferences for financial returns versus ESG benefits. This creates an intertemporal social dilemma (Hughes et al., 2018), where both agent classes must weigh immediate and local costs against long-term and global gains. Using Schelling diagrams, we demonstrate that in a fully profit-driven environment, a social dilemma arises, but with sufficiently ESG-conscious investors with enough capital, climate mitigation becomes optimal for some or all corporations. However, if companies are able to greenwash to cheaply increase ESG scores without genuine mitigation, the environment once again becomes a social dilemma. Our experiments with a state-of-the-art MARL implementation yield findings that align with real-world empirical evidence and provide novel insights. First, a sufficient number of highly ESG-conscious investors are needed to incentivize corporate mitigation efforts. In such cases, climate-focused companies emerge and attract most climate-conscious investments, while others prioritize profit maximization. When only some investors are climate-focused, the market bifurcates, with mitigating companies aligning with conscious investors. Additionally, sharing climate risk information helps companies to increase mitigation efforts, even when investors are not present. Figure 1: The InvestESG Environment. Corporations choose how much to invest in mitigating emissions, which affects their ESG Score. Climate-conscious investors can see ESG Scores when deciding how much to invest in each company. However, companies can engage in greenwashing to inexpensively and falsely improve ESG scores without actually mitigating climate change. InvestESG is a social dilemma, where selfish, profit-motivated corporations will not invest in mitigation without further incentives, leading to increased climate risks and decreased global wealth. More broadly, we demonstrate how a MARL framework can inform policy debates in the field of climate change. Assessing the effectiveness of a policy is inherently challenging, due to the fact that policy experiments are often prohibitively expensive and impractical to conduct, and even when they are feasible, it can be extremely time-consuming. However, the findings revealed by our environment match existing empirical results, suggesting the simulation has high ecological validity. Given the urgency of addressing climate change, our work provides a new vector for studying this problem, creating a simulated environment where a broad range of regulations can be explored and tested efficiently to provide novel insights into the problem. We present InvestESG as a challenging benchmark for the machine learning community; for researchers that are interested in developing MARL algorithms that can solve social dilemmas, InvestESG represents a social dilemma environment that could have real-world impact. We will provide the code for the benchmark and agent baselines in open-source. We aim to show that MARL algorithmic design can inspire real-world actions that can be leveraged to tackle climate change."
https://arxiv.org/html/2411.09168v1,Theory of Mind Enhances Collective Intelligence,"Collective Intelligence plays a central role in a large variety of fields, from economics and evolutionary theory to neural networks and eusocial insects, and it is also core to much of the work on emergence and self-organisation in complex systems theory. However, in human collective intelligence there is still much more to be understood in the relationship between specific psychological processes at the individual level and the emergence of self-organised structures at the social level. Previously psychological factors have played a relatively minor role in the study of collective intelligence as the principles are often quite general and applicable to humans just as readily as insects or other agents without sophisticated psychologies. In this article we emphasise, with examples from other complex adaptive systems, the broad applicability of collective intelligence principles while the mechanisms and time-scales differ significantly between examples. We contend that flexible collective intelligence in human social settings is improved by our use of a specific cognitive tool: our Theory of Mind. We identify several key characteristics of psychologically mediated collective intelligence and show that the development of a Theory of Mind is a crucial factor distinguishing social collective intelligence from general collective intelligence. We then place these capabilities in the context of the next steps in artificial intelligence embedded in a future that includes an effective human-AI hybrid social ecology.","All intelligence is collective intelligence. [70] Collectives are capable of achieving things that individuals alone cannot. Notwithstanding the simplicity or complexity of the individuals, their aggregate behaviour can often be understood as a complex processing of information that individuals store, modify, and transfer between each other producing ‘useful’ collective behaviour at the scale of the whole collective. In most instances of Collective Intelligence (CI), where the agents might be ants in an ant colony, bees in a beehive, or neurons in a neural network, the individual is not aware of the drivers of their behaviour or the behaviour of other agents. For example, a single neuron is neither aware of its own internal processes nor that of a neuron it is connected to, nor is it aware of the end goal to which its activity contributes. Despite both this lack of awareness and the lack of a centralised controller, evolutionary and learning processes have produced an intricate, precise, and highly adaptive system that is capable of functional behaviour that would be impossible for any single neuron to achieve. In other instances of CI, such as teams of humans, or businesses interacting in economic markets, the agents themselves may be highly complex and exhibit varying degrees of purposefulness and awareness. Within this context, we draw attention to the role of psychological factors in improving the CI of human social collectives and quantifying the intelligence of social collectives, both natural and artificial. In order to understand how collectives process information, we first consider the variety of ways in which agents interact. The topology of the network describing agent-to-agent interactions is well known to be important for the proper functioning of social groups [83, 79]. In particular it has been shown that mammalian social groups exhibit patterns of fractal-like topologies [40, 51] that are a result of a cognitive ability to form discrete social connections between conspicifics [49]. These links are often both spatially and temporally transient; people meet for a while, go their separate ways, and come back together later. Despite this transience, individual connections are often the basis of long term social relationships between specific individuals as in pair-bonding and friendships. Consequently an important distinction can be made regarding connections between agents in complex adaptive systems: they can be more fluid-like or more solid-like [101]. For example the links between neurons in the brain are relatively fixed in nature when compared to the brief communicative interactions between ants, either instantaneous interactions between individual ants or via transient pheromone trails that coordinate the behaviour of large numbers of ants. Solé and colleagues [101, 88] identify a distinction between solid brains, in which interactions between agents fixed in place are highly persistent in time (e.g. neural networks, spin glasses) and liquid brains, in which interactions between highly mobile agents are much more short-lived (e.g. ants, immune cells). As Solé et al. note regarding liquid brains [101]: “Here there are no neural-like elements and yet in many ways these systems solve complex problems, exhibit learning and memory, and make decisions in response to environmental conditions.” All biological agents are composed of sub-units such as organs, cells, and molecular networks [67, 69, 71]. Cells in particular are the simplest living organisms with individual intelligence, or competencies [67, 33], within their native contexts. Here, we briefly focus on the archetypal single-cell intelligence, the neural cells. It is well understood that the central nervous system is a highly developed, adaptive, complex system that exhibits emergent computational characteristics [52], both in biological and artificial neural networks. Naturally the artificial models are simplifications but the extent to which they are simplifications is not so well understood. In a 2021 study, Beniaguev et al. [9] concluded that between five and eight layers of an artificial deep neural network are required to approximate the input–output mapping of a (single) cortical neuron and that the dendritic branches can be understood as spatiotemporal pattern detectors. This demonstrates that a single neural cell can be modelled as an artificial agent with highly complex computational capabilities situated within an adaptive, complex network of other highly complex agents, all signalling to one another. These results can be compared with earlier studies in which neurons were modelled as a Bayesian agent that is trying to infer the state of a hidden variable [25]. In each of these interpretations, a single cell can be seen as an agent with computational competencies situated within the context of a network that is slowly and adaptively changing around it. We can also compare the competencies of neural cells in networks to the individual competencies of ants in an ant colony. In a recent study [56] it was shown that social structures of some ant colonies are conserved between species that are separated by more than 100 million years of evolution. In the five species studied by Kay et al. [56], they found two social clusters and similarities in the division of labour that are preserved between the species. In a different study, Richardson et al. [91] showed that individuals within an ant colony play an important leadership role and that the behaviour of these individuals significantly improved the collective performance of the ants. Ants are also capable of changing their social structure in the event of pathogenic infestation of their colony. In a 2021 article, Stockmaier and colleagues [102] review the research on social distancing and other social restructuring that occurs with conspecifics in order to reduce the impact of pathogens by changing their social cues, signals, and other behaviours for the collective benefit of the colony. These two very different systems, neural networks and ant colonies, are examples of complex collective intelligences where the individuals (neurons, ants) are complex in their own right, but they signal each other in order to restructure their relationships so as to adapt their collective competencies to external signals. The neural networks are prototypical solid brains and ant colonies are prototypical liquid brains. Human social interactions can also be viewed as a form of liquid intelligence. Migliano et al. [79] discuss the ‘fluidity’ of social relations in early human societies: “Quantification and mapping of hunter–gatherers’ social networks has revealed details of a fluid and multilevel sociality, where friendship links connect unrelated mobile households into camps of temporary composition”. They describe the key characteristics of early human society, such as egalitarianism, division of labour, cooperative living with unrelated individuals, multi-locality, fluid social structures, and high mobility between campsites, which might be thought of as a liquid brain composed of social interactions that both cluster and disperse in order to store, modify, and transfer information via social networks. The notion that human social interaction might be a form of computation is not new: Mirowski, Axtell and colleagues [82, 60, 81] have suggested that economic markets are a form of computation by which prices can be derived, and Harré recently hypothesised [45] that this could be measured using information theory as had been done earlier for financial markets [47, 42]. As Axtell et al. [60] wrote: “There is a close connection between agent computing in the positive social sciences and distributed computation in computer science, in which individual processors have heterogeneous information that they compute with and then communicate to other processors.” The emergence of computation in multi-agent systems is a well-studied area of complex adaptive systems [64, 84]. For example neuroscience has used information theory to describe the storage, transfer, and modification of bits of information in biological neural processes [117]. More broadly, Integrated Information Theory (IIT) [107, 77] has been put forward as a measure of the emergence of ‘consciousness’ in generic (non-biological, non-neural) systems. In this case, some forms of IIT explicitly use information theory [8, 78] to measure the amount of non-trivial computation a system is carrying out. More generally, there is a move towards understanding complex adaptive systems in computational terms [89, 74] by empirically measuring the inter-agent flow of information [12]. In this article we use information theory to quantify how much computation in a CI is ‘emergent’ and how much is simply independent information processing by single agents. In general, we wish to capture the notion of the whole (computational process) being greater than the sum of the (independent) parts. We translate this to the simple notion that to the extent to which this inequality holds: Whole - \sum(Parts)>0 is the extent to which we will say a system exhibits non-trivial CI, noting that there are multiple possible implementations of this approach [54]. The Parts is how much computation a single agent is carrying out from one time step to the next such that the sum is the total of all agents’ independent computations. The Whole is the totality of computation in the system, it includes all single agent computations, pairwise computations, and higher order interactions between agents. Our measure will not be unique in any of its specifics, but it serves to quantify the CI of a system for comparative analysis. This approach also has much in common with that of Moore et al. [84] in which information theory is used to measure the collective intelligence in biological systems. Not only is there diversity in the types of systems that can show positive measures of CI, but the ways in which agents manipulate a system’s computations is diverse as well. Take for example Watson and Levin’s discussion of a scientist manipulating the intercellular signalling in order to change their collective outcome [110]: This framework [of collective cellular intelligence] makes a strong prediction: if intercellular signalling (not genes) is the cognitive medium of a morphogenetic individual, it should be possible to exploit the tools of behavioural and neuro-science and learn to read, interpret and re-write its information content in a way that allows predictive control over its behaviour (in this case, growth and form) without genetic changes. A counter question is: How can single agents, such as human leaders, have predictive control over a social group? Just as a scientist external to a cell collective can manipulate inter-cellular signalling to control the outcomes of the cell collective, a leader internal to a human collective can manipulate inter-personal behaviours to control the outcomes of the human collective. In both cases, an agent with a goal-directed psychology is acting on inter-agent relationships, i.e. inter-cellular or inter-personal, to control outcomes at the next level higher, i.e. organism-scale or societal-scale. In this work we will ask an analogous question of human agents: What is there in human psychology that allows us to learn to read, interpret, and re-write our interpersonal information content in a way that allows predictive control over our collective behaviour? We will not be able to explore all of the possible interpretations of this question here, but we posit that our Theory of Mind (ToM) is a suite of cognitive skills that allows individuals to have goal directed control over collective outcomes. Originally ToM was used to describe our ability to infer the unobserved mental states of other people [34] such as desires and beliefs, an ability humans are particularly good at and other animals much less so [86, 59]. But recently it has been shown that ToM is predictive of group performance as well [121, 31], empirically demonstrating the role of ToM in going beyond representations of the internal states of others to using that knowledge in a social setting to improve the collective outcomes for the group. In order to model ToM in a tractable fashion, we will focus on the narrower game theory of mind [123], and the Beliefs, Preferences, and Constraints (BPC) interpretation of game-theoretic decisions put forward by Gintis [35]. In this approach, what agents understand of other agents’ hidden states are the BPC that structure their observable behaviours. We will consider this question in the framework of agent interactions that extend agent utilities in a simple but novel way. We quantify our results using information theory to show the impact that a correctly deployed ToM has to direct agents’ behaviours in order to increase our CI. The models are simple but they illustrate the central notion that understanding the “beliefs, preferences, and constraints” [36, 37] of others can be used to improve the CI of a complex social system. In Section 2, we describe the liquid–solid dichotomy of interacting agents, review extant models of ToM, and provide perspective on the interplay between social network structures and ToM. In Section 3, we provide illustrative examples supporting different aspects of our argument, introducing our measure of computation and applying it to a simple empirical example. In Section 4 we review the psychology of social fluidity and the variety of social outcomes that this fluidity makes possible. We also use a simple multi-agent system to describe how a ToM can be used to improve the computational processes, i.e. the CI, of interacting agents. Finally, in Section 5, we discuss the broader implications of this approach."
https://arxiv.org/html/2411.09636v1,Nash equilibrium seeking for a class of quadratic-bilinear Wasserstein distributionally robust games,"We consider a class of Wasserstein distributionally robust Nash equilibrium problems, where agents construct heterogeneous data-driven Wasserstein ambiguity sets using private samples and radii, in line with their individual risk-averse behaviour. By leveraging relevant properties of this class of games, we show that equilibria of the original seemingly infinite-dimensional problem can be obtained as a solution to a finite-dimensional Nash equilibrium problem. We then reformulate the problem as a finite-dimensional variational inequality and establish the connection between the corresponding solution sets. Our reformulation has scalable behaviour with respect to the data size and maintains a fixed number of constraints, independently of the number of samples. To compute a solution, we leverage two algorithms, based on the golden ratio algorithm. The efficiency of both algorithmic schemes is corroborated through extensive simulation studies on an illustrative example and a stochastic portfolio allocation game, where behavioural coupling among investors is modeled.","A wide range of applications, from smart grids Saad1 and communication networks Scutari to social networks Acemoglu2013 can be modelled as a collection of self-interested interacting decision makers optimizing different cost functions under operational constraints. Game theory Basar1 provides the fundamental theoretical framework for analyzing such systems. Although investigating deterministic games can be adequate in some case studies Scutari , Paccagnan2017 , most real-world applications involve decision making under uncertainty, which stresses the need for the inclusion of stochasticity in the existing models. Several studies have explored uncertainty within a game theoretic context, based on particular assumptions on the probability distribution Kouvaritakis , Singh and/or the properties of the uncertainty sample space Aghassi2006 ; FukuSOCCP . When the probability distribution is unknown and distribution models are not an accurate description of the stochastic aspect of the problem, sampling-based or data-driven methods have shown strong potential for proposing robust solutions against uncertainty. Works such as Feleconf2019 ; Fele2021 ; Dario_Scenario ; fele-a ; Pantazis2020 ; mammarela2023 ; Pantazis2023_apriori design distribution-free approaches for data-driven Nash equilibria based on statistical learning techniques. More specifically, fele-a ; Pantazis2020 ; mammarela2023 ; Pantazis2023_apriori account for possible strategic perturbations around the Nash equilibrium. Separately, works based on Sample Average Approximation (SAA) techniques, such as Franci_2021 ; Franci_2021_merely , develop algorithms for finding Nash equilibria in stochastic settings by using expected values of cost functions. The works mentioned above constitute data-driven methods for stochastic equilibrium seeking. These works, however, do not account for ambiguity in the probability distribution, where the distribution itself may be uncertain within some known bounds. The challenge of ambiguity in the distributions becomes pronounced in multi-agent settings, where heterogeneous uncertainties affect the agents’ costs, often necessitating the consideration of different ambiguity sets, each representing the individual risk-averse nature of each agent. To account for distributional uncertainty, distributionally robust optimization (DRO) uses a so-called ambiguity set of possible probability distributions to make decisions robust against probabilistic variations within this set. Unlike scenario-based methods, which require many samples for robustness, DRO can perform well with limited data by adjusting the ambiguity set. DRO includes special cases like sample average approximation (SAA) and robust optimization (RO). At the same time, DRO can be less conservative than RO and offer better out-of-sample performance than SAA, making it especially useful in data-driven applications with limited data. Recently, Wasserstein ambiguity sets villani_topics_2016 , which use empirical data and the Wasserstein metric to measure distributional deviations, have gained attention. These sets are favored for penalizing horizontal shifts and providing finite-sample guarantees. Research has focused on convergence of empirical estimates in the Wasserstein distance Dereich ; mohajerin_esfahani_data-driven_2018 ; Dedecker1 ; Weed ; Weed_2 ; Fournier_2023 , as well as obtaining tractable reformulations of Wasserstein distributionally robust optimization problems mohajerin_esfahani_data-driven_2018 ; netessine_wasserstein_2019 ; Lotfi1 ; Lotfi2 . Extensions of those works include distributionally robust chance-constrained programs Chen2018 ; Hota2018 ; Alamo2024risk . Despite the considerable body of literature on DRO with Wasserstein ambiguity sets, the exploration of data-driven Wasserstein distributionally robust Nash equilibrium problems with heterogeneous uncertainty in the cost functions represents a notably underexplored topic. Most works in the literature consider moment-based methods or other measures of distance between distributions. For instance, Peng2021 considers a non-cooperative game with distributionally robust chance-constrained strategy sets applied to duopoly Cournot competition. The work Liu2018 develops distributionally robust equilibrium models based on the Kullback-Leibler (KL) divergence for hierarchical competition in supply chains. Other works mainly consider ambiguity in the constraints, such as the recent work Xia_elliptical_2023 , which studies a game with deterministic cost for each agent and distributionally robust chance constraints with the centre of the Wasserstein ambiguity set being an elliptical distribution; fabiani2023distributionally reformulates an equilibrium problem with a deterministic cost and distributionally robust chance-constraints as a mixed-integer generalized Nash equilibrium problem leveraging the results in Chen2023 . The contributions of this work with respect to the related literature are the following: (i) We study a class of heterogeneous data-driven Wasserstein distributionally robust games, where each agent’s ambiguity set is centered around an empirical probability distribution based on their individual data, while the Wasserstein radius is also set by each individual agent. We reformulate the original game as a robust Nash equilibrium problem and establish the connection between the distributionally robust and robust Nash equilibria of the corresponding problems. For this class of games, we demonstrate that the inner maximization can be solved without the use of epigraphic variables kuhn2019wasserstein , pantazis2023_DRG , which in a game-theoretic setting can lead to unshared coupling constraints. As such, our approach decreases computational complexity significantly. To the best of our knowledge, this is the first distributionally robust game-theoretic reformulation that leads to data-scalable results by leveraging the structure of the problem at hand. (ii) The robust Nash equilibrium problem is then reformulated as a variational inequality (VI). Unlike results of similar classes of problems in optimization Boskos_2024 , where the reformulated variational inequality is monotone under certain assumptions, the mapping corresponding to the game can be nonmonotone in general due to the heterogeneity of the agents’ ambiguity sets and costs. However, we show that this problem can be efficiently solved empirically using two algorithms: the adaptive golden ratio algorithm (aGRAAL) malitsky_golden_2020 and a hybrid version of this algorithm (Hybrid-Alg in Reza_2024 ). Notably, our numerical results show that in several cases, the convergence speed is close to linear, and increasing the number of samples does not slow down the convergence. Our results are then applied to a portfolio allocation game that takes into account market uncertainty and behavioural coupling of market participants."
https://arxiv.org/html/2411.09493v1,Strategic Sacrifice: Self-Organized Robot Swarm Localization for Inspection Productivity,"Robot swarms offer significant potential for inspecting diverse infrastructure, ranging from bridges to space stations. However, effective inspection requires accurate robot localization, which demands substantial computational resources and limits productivity. Inspired by biological systems, we introduce a novel cooperative localization mechanism that minimizes collective computation expenditure through self-organized sacrifice. Here, a few agents bear the computational burden of localization; through local interactions, they improve the inspection productivity of the swarm. Our approach adaptively maximizes inspection productivity for unconstrained trajectories in dynamic interaction and environmental settings. We demonstrate the optimality and robustness using mean-field analytical models, multi-agent simulations, and hardware experiments with metal climbing robots inspecting a 3D cylinder.","Imagine a future where small robotic teams roam our infrastructure—bridges, pipelines, buildings, and satellites—detecting problems promptly, such as leaks and cracks (Figure 1(b)) [1, 2, 3, 4]. Teams of robots offer many advantages for inspection, including high parallelization, resilience to failure, and potentially low unit cost. However, for effective inspection, robots need to know where they are, a problem known as localization. In most indoor and remote locations, external localization mechanisms like GPS are unavailable or unreliable [5]. Robots must autonomously perform localization, overcoming difficulties such as sensor noise, limited landmarks, and slippage. While the literature primarily addresses the localization problem from a single-robot perspective, with approaches such as SLAM achieving good accuracy [6], these methods are computationally intensive. Computation is a finite and valuable resource; if most of an agent’s computation is used for localization, little remains for productive inspection tasks like finding cracks, measuring vibration, and monitoring rust accumulation. We explore if collaboration among robots can simplify this localization problem, allocating more computation for inspection. We draw inspiration from nature, where individuals sacrifice for the group’s benefit. In anti-predator vigilance, some members watch for predators, allowing others to eat safely [7, 8]. Army ants form bridges out of their bodies, facilitating cargo transport across gaps and cracks [9, 10]. The vigilant animals forego eating, and the bridge ants do not carry cargo; instead, by sacrificing individual productivity, they enhance group success. What is especially interesting about these biological cases is that the fraction of sacrificers is self-organized and adapts to environmental demands. We introduce a novel cooperative localization mechanism for robot swarms that leverages this idea of self-organized sacrifice for the group’s benefit. While previous studies have used cooperation to enhance localization accuracy and enable localization in unknown environments [11, 12, 13], these methods often increase computational requirements, constrain robot trajectories, and lack adaptability to changing environments. In our approach, individuals become dedicated localizers or inspectors, with this distribution self-organized based on local interactions. We demonstrate that this decentralized mechanism optimizes collective productivity in dynamic conditions, validated through theoretical models, numerical simulations, and hardware experiments. By deriving mean-field models (Sec 4), we prove that sacrificing agents as dedicated localizers improves swarm productivity. We also show that the swarm can reconfigure based on local interactions to always maximize productivity. Using agent-based numerical simulations (Sec 5), we demonstrate that group productivity increases further through smarter collaboration. Hardware experiments (Sec 6) conducted with a swarm of 10 metal climbing robots inspecting a 3D metal cylinder (Figure 1) demonstrate the effectiveness of this approach in navigating complex physical environments. Moreover, these experiments show that the emergent behavior optimizes productivity amidst dynamically changing interactions. Figure 1: Rovables [14] (a) on a 3D metal cylinder, (b) on a piping system, and (c) in the palm of a human hand. (d) Rovables with markers for Vicon Motion Capture."
https://arxiv.org/html/2411.09191v1,Informational Puts,"We fully characterize how dynamic information should be provided to uniquely implement the largest equilibrium in dynamic binary-action supermodular games. The designer offers an informational put: she stays silent in good times, but injects asymmetric and inconclusive public information if players lose faith. There is (i) no multiplicity gap: the largest (partially) implementable equilibrium can be implemented uniquely; and (ii) no intertemporal commitment gap: the policy is sequentially optimal. Our results have sharp implications for the design of policy in coordination environments.","1 Introduction Many economic environments feature (i) uncertainty about a payoff-relevant fundamental state, (ii) coordination motives, and (iii) stochastic opportunities to revise actions. These elements are present across all aspects of social and economic life e.g., macroeconomics, finance, industrial organization, and political economy.111In macroeconomics, firms are uncertain about economic conditions, face complementarities (Nakamura and Steinsson, 2010), and change their prices at ticks of a Poisson clock (Calvo, 1983). In finance, creditors are uncertain about the debtor’s profitability/solvency (Goldstein and Pauzner, 2005), have incentive to run if others’ run (Diamond and Dybvig, 1983), but might only be able to withdraw their debt at staggered intervals (He and Xiong, 2012). In industrial organization, consumers are uncertain about a product’s quality, have incentive to adopt the same product as others (Farrell and Saloner, 1985; Ellison and Fudenberg, 2000), and face stochastic adoption opportunities (Biglaiser, Crémer, and Veiga, 2022). Equilibria of such games are sensitive to dynamic information. Consider a player who, at any history of the game, finds herself with the opportunity to re-optimize her action. The fundamental state matters for her flow payoffs, so her decision must depend on her current beliefs. Moreover, since she plays the same action until she can next re-optimize, her decision also depends on her beliefs about what future agents will do. But those beliefs depend, in turn, on what she expects future players to learn, as well as their beliefs about the play of agents yet further out into the future. Thus, the stochastic evolution of future beliefs—even those arbitrarily distant—shape incentives in the present. We are interested in dynamic information policies which fully implement the largest time path of aggregate play i.e., as a unique subgame equilibrium of the induced stochastic game. Our main result (Theorem 1) fully characterizes the form, value, and sequential optimality of designer-optimal policies: 1. Form. The form of optimal dynamic information relies on the delivery of carefully chosen off-path information. If players take the designer’s preferred action, the designer stays silent. If, however, agents deviate from a target path of play specified by the policy, the designer injects an asymmetric and inconclusive public signal—this is the informational put.222This is analogous to the “Fed put” in which the Fed’s history of intervening to halt market downturns has arguably created the belief that they are insured against downside risk (Miller et al., 2002). This is as if the Fed has offered the market a put option as insurance against downturns. In our setting, the designer steps in to inject information when players start switching to action 0 which, as we will show, with high probability induces aggregate play to correct. This is as if the designer has offered players a put option as insurance against strategic uncertainty about the play of future players. The signal is asymmetric such that the probability that agents become a little more confident is far higher than the probability that agents become much more pessimistic. These small but high-probability movements in the direction of the dominance region—at which playing the designer’s preferred action is strictly dominant—are chained together such that the unique equilibrium of the subgame is for future players to play the designer-preferred action.333This is done via a ”contagion argument” which can be viewed as the dynamic analog of the interim deletion of strictly dominated strategies in static games of incomplete information. The signal is inconclusive such that, even if agents turn pessimistic, they do not become excessively so—this will be important for sequential optimality. 2. Value. The sequentially optimal policy uniquely implements the upper-bound on the time path of aggregate play. Thus, there is no multiplicity gap: whatever can be implemented partially (i.e., as an equilibrium) can also be implemented fully (i.e., as the unique equilibrium). This is in sharp contrast to recent work on static implementation via information design in supermodular games which finds there generically exists a gap even with private information and the ability to manipulate higher-order beliefs (Morris, Oyama, and Takahashi, 2024), or with both private information and transfers (Halac, Lipnowski, and Rappoport, 2021). 3. Sequential optimality. Our dynamic information policy is constructed such that at every history, the designer has no incentive to deviate.444With the caveat that for a small set of histories, deviation incentives can be made arbitrarily small. For histories where this is so, this is simply because optimal information policies continuing from those histories do not exist. Nonetheless, this can be approached via a sequence of policies so that the gap vanishes along this sequence. This openness property is also typical of static full implementation environments as highlighted by Morris, Oyama, and Takahashi (2024). Thus, there is no intertemporal commitment gap: whatever can be implemented with ex-ante commitment to the dynamic information structure can also be implemented when the sender can continually re-optimize her dynamic information.555We further emphasize that sequential optimality is not given—we offer examples of policies which are optimal but not sequentially optimal. Sequentially optimality arises through the delicate interaction between properties of our policy: asymmetry, chaining, and inconclusiveness. Asymmetric off-path information are chained together to obtain full implementation at all states in which the designer-preferred action is not strictly dominated. Then, inconclusive off-path information ensures that, even if agents turn pessimistic, full implementation is still guaranteed. Conceptually, our contribution highlights how off-path information should be optimally deployed to shape on-path incentives. Of course, it is well-known from implementation theory (Moore and Repullo, 1988; Abreu and Matsushima, 1992) that off-path threats are powerful, albeit not sequentially optimal—if the deviation actually occurs, there is no incentive to follow-through with the policy.666With the caveat that in implementation theory, the designer’s objective function is typically not specified: we have in mind an environment in which the designer is a player in the game, and punishing players is costly. See also work on mechanism design with limited commitment (Laffont and Tirole, 1988; Bester and Strausz, 2001; Skreta, 2015; Liu et al., 2019; Doval and Skreta, 2022) and macroeconomics where time-inconsistency plays a crucial role (Halac and Yared, 2014). Information is different in two substantive ways. It is less powerful: beliefs are martingales, which imposes severe constraints on what payoffs can be delivered off-path. But it is also more flexible: the designer has the freedom to design any distribution of off-path beliefs. What should we make of these differences? First, we will show that off-path information, through less powerful on its own, can be chained together to close the gap between full and partial implementation. Second, the flexibility of off-path information can be exploited to shape the continuation incentives of the designer. This ensures that the designer’s counterfactual selves at zero probability histories are willing to follow through with the promised information. Together, these insights offer a novel and unified treatment of dynamic information design in supermodular games. Economically, our results have sharp implications for a range of phenomena where coordination and multiple equilbiria feature prominently e.g., in finance (debt runs, currency crises), macroeconomics (price setting), trade and industrial policy (big pushes), industrial organization (network goods), and political economy (revolutions). We briefly discuss this after stating our main result. Related Literature Our results relate most closely to recent work on full implementation in supermodular games via information design (Morris, Oyama, and Takahashi, 2024; Inostroza and Pavan, 2023; Li, Song, and Zhao, 2023). In this literature, information design induces non-degenerate higher-order beliefs, and this is important to obtain uniqueness via a ”contagion argument” over the type space. By contrast, our dynamic information is public and higher-order beliefs are degenerate but we leverage a distinct kind of ”intertemporal contagion”. A key takeaway from this literature is that there is typically a gap between the designer’s value under adversarial equilibrium selection, and under designer-favorable selection (what we call a “multiplicity gap”); by contrast, we show that for dynamic binary-action supermodular games there is no such gap. Also related is the elegant and complementary work of Basak and Zhou (2020) and Basak and Zhou (2022). We highlight several substantive differences. First, we study different dynamic games: in Basak and Zhou (2020, 2022) players make a once-and-for-all decision on whether to play the risky action, and they focus on regime change games—both features play a key role in their analysis;777Basak and Zhou (2020) study a regime change game with private information where the designer can choose the frequency at which she discloses whether or not the regime has survived. Basak and Zhou (2022) study an optimal stopping game with a regime change payoff structure in which agents chooses when to undertake an irreversible risky action. in ours, agents can continually re-optimize at the ticks of their Poisson clocks and play a general binary-action supermodular game where the designer’s payoff is any increasing functional from the path of aggregate play. Importantly, our optimal dynamic information policies—and the reasons they work—are entirely distinct; we discuss this more thoroughly after stating our main result. Our paper also relates to work on the equilibria of dynamic coordination games. An important paper of Gale (1995) studies a complete information investment game where players can decide when, if ever, to make an irreversible investment and investing is payoff dominant.888See also Chamley (1999); Dasgupta (2007); Angeletos, Hellwig, and Pavan (2007); Mathevet and Steiner (2013); Koh, Li, and Uzui (2024a) all of which study the equilibria of different dynamic coordination games. The main result is that investment succeeds across all subgame perfect equilibria. Our environment and results differ in several substantive ways. For instance, our policy allows the designer to implement the largest equilibria—irrespective of whether it is payoff dominant.999Moreover, actions in our environment are reversible, so sans any information (and assuming beliefs are not in the dominance regions) there will exist subgame perfect equilbiria in which players “cycle” between actions; this is ruled out in the environment of Gale (1995) because of irreversibility. More subtly, our dynamic information works with—but does not rely on—atomless players i.e., we obtain full implementation even if each player believes that they will not change the aggregate state. By contrast, atomic players is an essential feature of Gale (1995). Our results are also connected to the literature on dynamic implementation. Moore and Repullo (1988) show that arbitrary social choice functions can be achieved with large off-path transfers.101010See also Aghion, Fudenberg, Holden, Kunimoto, and Tercieux (2012) for a discussion of the lack of robustness to small amounts of imperfect information, and Penta (2015) who takes a belief-free approach to dynamic implementation. Glazer and Perry (1996) show that virtual implementation of social choice functions can be achieved by appealing to extensive-form versions of Abreu and Matsushima (1992) mechanisms.111111See work by Chen and Sun (2015) who exploit the freedom to design the extensive-form. Sato (2023) designs both the extensive-form and information structure a la Doval and Ely (2020) and further utilizes the fact the designer can design information about players’ past moves; by contrast, we fix the dynamic game and past play is observed. Chen et al. (2023) weaken backward induction to initial rationalizability.121212That is, only imposing sequential rationality and common knowledge of sequential rationality at the beginning of the game, but ”anything goes” off-path; see Ben-Porath (1997). Different from these papers, our designer is substantially more constrained: (i) there is no freedom to design the extensive-form game which we take as given; (ii) the designer only offers dynamic information; and (iii) our policy is sequentially optimal. Our game is one where players have stochastic switching opportunities. Variants of these models have been studied in macroeconomics (Diamond, 1982; Calvo, 1983; Diamond and Fudenberg, 1989; Frankel and Pauzner, 2000), industrial policy (Murphy, Shleifer, and Vishny, 1989; Matsuyama, 1991), finance (He and Xiong, 2012), industrial organization (Biglaiser, Crémer, and Veiga, 2022), and game theory (Burdzy, Frankel, and Pauzner, 2001; Matsui and Matsuyama, 1995; Oyama, 2002; Kamada and Kandori, 2020).131313See also more recent work by Guimaraes and Machado (2018); Guimaraes, Machado, and Pereira (2020). Angeletos and Lian (2016) offer an excellent survey. A common insight from this literature is that switching frictions can generate uniqueness, and the risk-dominant profile is selected via a process of backward induction. Our contribution is to show how the largest equilibrium can be uniquely implemented by carefully choosing the dynamic information policy. Sequential optimality is an important property of our information policy and thus our work relates to recent work studying the role of (intertemporal) commitment in dynamic information design. Koh and Sanguanmoo (2022); Koh, Sanguanmoo, and Zhong (2024b) show by construction that sequential optimality is generally achievable in single-agent stopping problems. It will turn out that sequential optimal policies also exist in our environment, but for quite distinct reasons; we discuss this more thoroughly in Section 3."
https://arxiv.org/html/2411.09110v1,Information-Optimal Multi-Spacecraft Positioning for Interstellar Object Exploration,"Interstellar objects (ISOs), astronomical objects not gravitationally bound to the sun, could present valuable opportunities to advance our understanding of the universe’s formation and composition. In response to the unpredictable nature of their discoveries that inherently come with large and rapidly changing uncertainty in their state, this paper proposes a novel multi-spacecraft framework for locally maximizing information to be gained through ISO encounters with formal probabilistic guarantees. Given some approximated control and estimation policies for fully autonomous spacecraft operations, we first construct an ellipsoid around its terminal position, where the ISO would be located with a finite probability. The large state uncertainty of the ISO is formally handled here through the hierarchical property in stochastically contracting nonlinear systems. We then propose a method to find the terminal positions of the multiple spacecraft optimally distributed around the ellipsoid, which locally maximizes the information we can get from all the points of interest (POIs). This utilizes a probabilistic information cost function that accounts for spacecraft positions, camera specifications, and ISO position uncertainty, where the information is defined as visual data collected by cameras. Numerical simulations demonstrate the efficacy of this approach using synthetic ISO candidates generated from quasi-realistic empirical populations. Our method allows each spacecraft to optimally select its terminal state and determine the ideal number of POIs to investigate, potentially enhancing the ability to study these rare and fleeting interstellar visitors while minimizing resource utilization.","Interstellar objects (ISOs), astronomical objects traveling through space without any attachment to any system, present a unique opportunity to study various aspects of the universe, such as the formation and composition of other star systems, the origin of the universe, and possibly other forces at play in the expanses of space [1]. For example, one of the two ISOs observed to date [2, 3], named 1I/‘Oumuamua, visited us from the rough direction of the constellation Lyra exhibiting some unusual physical characteristics (a highly elongated shape, a lack of typical cometary volatiles, and a deviation from Keplerian trajectories [4, 5]), which might provide some clues into how our solar system and neighboring exoplanetary star systems are formed. Exploring these interstellar visitors in situ with dedicated spacecraft would offer an alternative means to acquire firsthand knowledge about interstellar space, which goes beyond what is obtained through remote observations via telescopes. Due to the hyperbolic nature of their orbit, ISOs only pass through the solar system once in their lifetime with inherently large and rapidly changing uncertainty in their state, often at high inclination and relative velocities [6]. This poses significant challenges in designing a guidance, navigation, and control (GNC) strategy for the ISO encounter, requiring fast response autonomous operations even with the limited computational capacity of spacecraft. These would involve some levels of approximations in the GNC policies for the sake of their onboard execution, which then introduces additional difficulties in formally quantifying the probability of a successful encounter. Furthermore, theoretical connections between this probability and our mission objective of maximizing the scientific return through the encounter (which we characterize by the amount of visual data collected by spacecraft cameras) still remain ambiguous. 1.1 Contributions This paper proposes a novel multi-spacecraft framework for locally maximizing the amount of information obtained through ISO encounters, with some probabilistic guarantees. Building on our previous work [7], we first derive a formal upper bound on the failure probability of the ISO encounter for hierarchical Itô stochastic nonlinear system, which models our spacecraft relative dynamics in the local-vertical local-horizontal (LVLH) frame [8, pp. 710-712] centered on the ISO with the large uncertainty in its state measurement. In order to achieve fast response autonomous operations even with the limited onboard computational capacity of the spacecraft, potential approximation errors in their control and estimation policies are explicitly considered when deriving the bound. The failure probability then enables the construction of an ellipsoid around the terminal position of the chief spacecraft, in which the ISO would be located at the terminal time with a finite probability. We further propose an approach to finding the terminal positions of the deputy spacecraft with respect to the chief spacecraft, optimally distributed around the ellipsoid to maximize the information we get from all the points of interest (POIs) in it. In particular, we utilize an information cost function characterized by the distance between the center of the ellipsoid and each spacecraft in the swarm and the visual coverage of the POIs, accounting for spacecraft positions and attitudes, camera specifications, and ISO position uncertainty, where the information here is defined as imaging data collected by spacecraft onboard cameras. Since this optimization only requires an upper bound for the ISO state uncertainty history over time, which we could obtain as in our previous work [6] with [9, 10], the optimal relative positions of the deputy spacecraft can be pre-computed offline, freeing onboard computational resources for some other highly autonomous tasks. Numerical simulations are performed to demonstrate the efficacy of our method using a quasi-realistic empirical population of ISOs [11, 9] with the empirical history of the navigation uncertainties [9, 10, 6, 7], where the optimization problems are solved using Nelder-Mead. Our first simulation examines the probability that a single spacecraft would be able to view the ISO, assuming an uncertainty ellipsoid with equal x, y, and z radii. As is expected from the aforementioned probability bound, the probability that the spacecraft views the ISO decreases as the ISO state uncertainty increases. Our second simulation empirically determines the optimal number of spacecraft to view as many POIs on the sphere as possible, where the size of the uncertainty sphere remains constant. It is demonstrated that a multi-spacecraft system both observes more POIs and results in a lower information cost than a singular spacecraft, however, an excess of spacecraft increases the overlap between the visual coverage of the system. 1.2 Related Work Previous studies have investigated the potential of ISO flyby missions for a single spacecraft, proposing viable methods for observing ISOs and other high-speed celestial objects [6, 7]. For example, it is rigorously shown that a control policy designed using the spectrally normalized deep neural network (SN-DNN) guarantees local, finite-time exponential boundedness of the expected spacecraft delivery error, assuming that the bounds for the estimation errors are known. These approaches would allow at least to encounter ISOs even under their large state uncertainty. However, the entirety of the ISO cannot be observed through a flyby as matching the spacecraft velocity with the ISOs’ is unrealistic with current propulsion technologies [9], making such missions inefficient for gathering valuable scientific data. Several numerical and theoretical methods have been proposed to circumvent these difficulties through the lens of information-based optimization. Previous works in this field with a swarm of spacecraft can be broken into two categories: gaining information from a body with a determined orbit, such as satellites or other spacecraft, and gaining information from an object with an undetermined orbit, such as celestial bodies. In [12], a passive observation technique is proposed for celestial bodies with known trajectories, whereas for the case of ISOs, an active approach should be taken to sufficiently capture information to address the large state uncertainty. Further, the algorithm utilized in the study only maximizes the amount of the celestial body covered by the field of view (FOV) of the spacecraft in the swarm. The overlap between spacecraft in the swarm must be minimized to make a more robust algorithm. This would prevent resources from being wasted by spacecraft redundantly examining the same area of the ISO. The collision avoidance techniques introduced in [13] utilize a minimum scalar distance that a spacecraft must be from each other. However, this constraint ignores any angular or directional constraints. With the highly uncertain trajectory of the ISO, it is crucial to minimize the amount of overlap in information gained by any two spacecraft. This consideration is crucial when optimizing information from bodies with controlled trajectories, such as during on-orbit inspection or servicing of satellites [14], [15]. These studies utilize an information cost to create a fuel and energy-efficient orbit around a target satellite. However, the information cost presented in these studies does not account for the spacecraft’s orientation or field of view. Instead, they determine a baseline distance for the spacecraft to start at and then algorithmically determine an orbit for the spacecraft to take. In the case of this study, orbiting the ISO would be infeasible again due to the ISO’s high relative velocity and the uncertainty of its dynamics. Some other studies focus on optimizing information from celestial bodies with uncontrolled trajectories; however, only comets and asteroids have been examined so far [16, 17]. In these studies, either the spacecraft that have to gain information are stationary, or the object they are observing is stationary and the spacecraft performs a flyby inspection of the celestial body. With an ISO, the spacecraft has to assume a formation in order to maximize the information gained, since the ISO’s state is uncertain and it never remains stationary. This paper proposes one solution to the issues listed above through the proactive deployment of multiple spacecraft, extending the ideas of information gain-based optimization. Hierarchical stochastic contraction also enables explicitly considering the state uncertainty of the ISO, having the spacecraft cooperate to gather visual data on all perspectives of the ISO locally optimally in one flyby mission."
https://arxiv.org/html/2411.08999v1,Learning-Based Control Barrier Function with Provably Safe Guarantees: Reducing Conservatism with Heading-Aware Safety Margin,"We propose a learning-based Control Barrier Function to reduce conservatism in collision avoidance of car-like robots. Traditional CBFs often use Euclidean distance between robots’ centers as safety margin, neglecting headings and simplifying geometries to circles. While this ensures smooth, differentiable safety functions required by CBFs, it can be overly conservative in tight environments. To address this limitation, we design a heading-aware safety margin that accounts for the robots’ orientations, enabling a less conservative and more accurate estimation of safe regions. Since the function computing this safety margin is non-differentiable, we approximate it with a neural network to ensure differentiability and facilitate integration with CBFs. We describe how we achieve bounded learning error and incorporate the upper bound into the CBF to provide formal safety guarantees through forward invariance. We show that our CBF is a high-order CBF with relative degree two for a system with two robots whose dynamics are modeled by the nonlinear kinematic bicycle model. Experimental results in overtaking and bypassing scenarios reveal a 33.5\text{\,}\mathrm{\char 37\relax}\text{/} reduction in conservatism compared to traditional methods, while maintaining safety.Code: github.com/bassamlab/sigmarl","Control Barrier Functions are critical tools for ensuring safety in control systems, particularly for autonomous robots in dynamic environments. They provide a formal way to enforce safety constraints by rendering a designated safe set forward invariant [1]. In the context of car-like robots, such as Connected and Automated Vehicles, safety is critical. These robots must navigate complex environments and avoid collisions with other agents, while achieving their intended goals efficiently. Motion planning for car-like robots involves generating trajectories that are not only feasible concerning the robot’s dynamics but also safe w.r.t. the environment and other agents. Traditional Control Barrier Functions often simplify robots’ geometries to facilitate the computation of Control Barrier Function conditions or simplify safety estimation. A common simplification is to model the robots as circles, which allows for straightforward distance computation but ignores the robots’ actual shapes and orientations. While this simplifies the safety analysis and ensures the smoothness and differentiability required by CBFs, it can lead to overly conservative behaviors, especially in confined or densely populated environments. To address these limitations, we propose a learning-based Control Barrier Function that incorporates a heading-aware safety margin, inspired by Separating Axis Theorem [2] and Minimum Translation Vector [3], which we term Minimum Translation Vector-based safety margin. By accounting for car-like robots’ orientations and actual geometries, our method provides a more accurate estimation of safe regions. This approach reduces conservatism in collision avoidance, allowing them to navigate more efficiently without compromising safety. I-A Related Work Collision avoidance for car-like robots is a well-studied problem. While optimization-based approaches such as model predictive control are widely used [4, 5, 6, 7], their can be computationally intensive. ently, CBFs have gained attention for their forward invariance and formal safety guarantees. Traditional CBFs often simplify robot geometries as circles and define safety margins based on Euclidean distances between centers, which we refer to as the Center-to-Center-based safety margin. Work [8] uses Control Barrier Functions to ensure safety distance to a so-called avoidable set, which defines safe boundaries around round-shaped moving obstacles. Work [9] introduced safety barrier certificates for collision-free multi-robot systems using Control Barrier Functions, where each robot is modeled as a circle to simplify collision avoidance constraints. Study [10] extended CBFs to systems with high relative degrees, maintaining the circular approximation for robots. Further advancements include integrating learning into CBF frameworks with Center-to-Center-based safety margin. Work [11] uses off-the-center disks to avoid the conservatism in the Euclidean distance-based safety margins, where the deviation direction of the disks depends on the direction of the obstacles w.r.t. the lane center of the ego robot. In the domain of Connected and Automated Vehicles, studies [12] and [13] employed Center-to-Center-based safety margins within multi-agent Reinforcement Learning frameworks to ensure safety of the learned policies. They introduce another term to the longitudinal distance between Connected and Automated Vehicles to consider lane-changing behavior. Other similar works using Center-to-Center-based safety margin are [14, 15, 16]. While the circle approximation simplifies computations and ensures differentiability, it does not accurately capture the actual shape and orientation of car-like robots. This discrepancy can lead to overly conservative behaviors, limiting the robots’ ability to navigate efficiently in complex environments. To improve upon the circle approximation, some researchers have modeled robots or obstacles as ellipses (or ellipses in case of 3D space), which better represent their elongated shapes, despite that their distances cannot be easily computed. Work [17] proposes a conservative distance estimate between ellipsoids, which is shown to be an eigenvalue problem. Study [18] derives a closed-form expression that represents a distance metric of two ellipsoids in 3D space. In [19], robots and obstacles are represented by sets of ellipsoids and a point-world transformation is proposed to transform these ellipsoids to points, simplifying collision avoidance through customized navigation functions in the point world. The proposed transformation has been successfully applied in many other works such as [20]. Furthermore, some works use a mixture of circles and ellipses for shape approximation. This can happen by either approximating the ego robot with a circle and its surrounding robots with ellipses [21, 22] or conversely [23]. Note that only [18, 22] combine Control Barrier Functions. These approaches reduce conservatism compared to the pure circle-based approximation but still cannot fully capture the actual shape of car-like robots. I-B Paper Contributions The main contributions of this work are threefold: 1. We propose a non-differentiable, heading-aware safety margin based on Minimum Translation Vector that considers the headings and geometries of car-like robots, offering a less conservative and more accurate estimation of safe regions for collision avoidance. We train a differentiable neural network to learn it with estimable upper bound on approximation errors. 2. We establish a theorem providing our learning- and Minimum Translation Vector-based safety margin as a high-order Control Barrier Function with relative degree two for a system with two robots modeled by the nonlinear kinematic bicycle model. 3. We validate the theoretical findings through numerical simulations in overtaking and bypassing scenarios involving two car-like robots, demonstrating reduced conservatism compared to traditional Center-to-Center-based approach. Our work appears to be the first work in using Minimum Translation Vector-based safety margin to compute safety distance in Control Barrier Function. I-C Notation A variable x is annotated with a superscript x^{i} if it belongs to robot i. A relative state includes two letters in its superscript to indicate direction, e.g., x^{ji} denotes the relative x-position of robot j w.r.t. robot i. If the relative state is expressed in robot i’s ego perspective rather than in the global coordinate system, an underline is used, e.g., x^{j\underline{i}}. Vectors, such as state vector \bm{x} and control input vector \bm{u}, are bolded, and the dot product of two vectors \bm{a} and \bm{b} is denoted by \bm{a}\cdot\bm{b}. Time arguments of time-variant variables are omitted throughout the paper for simplicity. I-D Paper Structure Section II introduces preliminaries required for this work. Section III proposes our Minimum Translation Vector-based safety margin and its integration with Control Barrier Functions. Section IV discusses experimental results and limitations of our work. Section V draws conclusions and outlines future research directions."
https://arxiv.org/html/2411.08634v1,On the Application of Model Predictive Control to a Weighted Coverage Path Planning Problem,"This paper considers the application of Model Predictive Control (MPC) to a weighted coverage path planning (WCPP) problem. The problem appears in a wide range of practical applications, such as search and rescue (SAR) missions. The basic setup is that one (or multiple) agents can move around a given search space and collect rewards from a given spatial distribution. Unlike an artificial potential field, each reward can only be collected once. In contrast to a Traveling Salesman Problem (TSP), the agent moves in a continuous space. Moreover, he is not obliged to cover all locations and/or may return to previously visited locations. The WCPP problem is tackled by a new Model Predictive Control (MPC) formulation with so-called Coverage Constraints (CCs). It is shown that the solution becomes more effective if the solver is initialized with a TSP-based heuristic. With and without this initialization, the proposed MPC approach clearly outperforms a naive MPC formulation, as demonstrated in a small simulation study.","Many path planning applications in robotics try to find an optimal path while maximizing some sort of reward. The reward is commonly related to the proximity to given reference value, guiding the system via an artificial potential field (AFP), or keeping it away from unsafe areas via barrier functions. Model Predictive Control (MPC) casts this problem into a numerical optimization program, with an objective function and constraints. Over the past years, it has become a standard approach for path planning, due to its natural handling of general reward functions, system dynamics, and input and state constraints. Coverage Path Planning (CPP) aims for the system to cover an entire area of the state space, or as large a part of it as possible. Problem instances appear in many robotic applications, such autonomous lawn mowers, vacuum cleaners, agricultural robots, and arial / underwater drones used for inspection or surveillance. From the perspective of common MPC-based path planning, this means a uniform objective function. However, for CPP, the objective function changes dynamically in the sense that the reward at an already visited position, and some radius around it, drops to zero. Efficient motion patterns or policies are commonly used for CPP, including a boustrophedon (snake-like) path or straight driving with random reflection angles when hitting boundaries [1]. The main principle is, clearly, to minimize any overlaps in the track of the robot. This paper considers the extended problem of Weighted Coverage Path Planning (WCPP). The main difference to the CPP is that the objective function is not uniform, but weighted by a coverage priority. Problem instances of this also appear frequently, e.g., in search and rescue (SAR) or surveillance missions, where the goal is to find or detect a target whose probability of presence on a map is not uniformly distributed. Correspondingly, the motivational example for this paper is an unmanned aerial vehicle (UAV) with the task of finding a missing person in a given area as fast as possible. The coverage priority is indicated by a probability map, which serves as the reward function and represents a belief distribution integrating all available information at the current time, e.g., from sensor measurements, the observation of eye witnesses, or human behavioral models in the given map [2]. For practical applications, a probabilistic model could be used to dynamically update the belief distribution based on all available information, and thus guide the planning of the mission [3]. I-A Existing Literature CPP is about finding a path that covers all specified points or an entire area or region of interest while avoiding obstacles [4]. A comprehensive overview of CPP algorithms for robotic applications is given in [5]. The article covers both classical and heuristic-based algorithms. These two categories include a whole variety of basic approaches, such as AFPs, greedy search and graph search algorithms, and bio-inspired approaches, such as genetic algorithms or ant colony optimization. A key point in the discussion in [5] is that the resulting paths should be as smooth as possible. Namely, the avoidance of sharp turns in the path prevent premature wear of the robot’s components and it increases the efficiency, especially in UAV applications, where it advantageous for the average speed of the UAV to fly straight ahead or in smooth curves. In this spirit, the approach in [6] proposes a path search algorithm using ant colony optimization based on the Lin-Kernighan heuristic, followed by a smoothing step using a customized approach based on Fourier series. The resulting dynamically smooth trajectory is then fed to the UAV, which is operated by an MPC-based controller. Numerical optimization has become increasingly popular for path planning over the recent years, due to the availability of more powerful hardware and increasingly efficient solvers. MPC, in particular, has been successfully employed for collision-free path planning for autonomous road vehicles [7] and for UAVs [8]. MPC has been used in combination with AFPs [9, 10], and also for CPP with obstacles using a mixed-integer linear programming (MILP) formulation [11]. The area is covered by a uniform grid, where each cell defines a single way point. The way points are subsequently represented as discrete decision variables within the optimization problem, where the objective is to cover the maximum number of (equally weighted) way points. Related problems to the CPP include the Traveling Salesman Problem (TSP) and its variants, most notably the Orienteering Problem (OP) [12]. In contrast to the TSP, where all vertices must be visited and the goal is to minimize the traveled distance, the OP is concerned with maximizing the total reward collected within a limited time frame, without necessarily visiting all the vertices [13]. Both of these problems are known to be NP-hard [14]. Yet it is desirable, for many applications, to extend them further by including the dynamics of an agent in the problem formulation [15]. Recently, a mathematical framework to tackle this problem using techniques from optimal control has been proposed [15, 16]. In addition to the dynamics of an agent, this formulation also accounts for the movement of the sensors, i.e., in this case, a camera. However, the approach relies on nonsmooth calculus and considers only discrete regions of interest. I-B Contributions This paper proposes a new MPC approach for the WCPP problem, using coverage constraints (CCs). The proposed MPC formulation works with any agent moving governed by a nonlinear dynamic model and moving in a continuous space with obstacles. The reward function is arbitrary, but in contrast to an AFP, each reward can only be collected once. This is enforced by the use of quadratic constraints. In contrast to the TSP, the agent is not obliged (and may in fact be far from able to) cover all locations within the given prediction horizon. Furthermore, the paper shows that the solution becomes more effective if the MPC solver is initialized with a TSP-based heuristic. The heuristic is based on a set of key points, which are derived from a Gaussian mixture model that is used to approximate the reward function."
https://arxiv.org/html/2411.07464v1,BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks,"Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4. Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting. With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls. Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning. With 94.2% reduction in the cost (from $0.931 per run cost averaged over all tasks for GPT-4 single agent system to $0.054), our system is able to yield better average success rate of 32.95% as compared to GPT-4 single-agent system yielding 22.72% success rate averaged over all the tasks of MLAgentBench.","Although recent advances have shown that Large Language Models (LLMs) are adept at handling a vast array of applications ranging from natural language (Fang et al., 2024; Huang and Chang, 2023; Zhu et al., 2023; Yi et al., 2024) to code-related tasks (Zheng et al., 2024; Zan et al., 2023; Zhang et al., 2024a), this capability does not often translate to more complicated and nuanced tasks (Yeadon et al., 2024). Most code-related efforts involving LLMs (Guo et al., 2024; Huang et al., 2024; Zhong et al., 2024) are based on tasks such as HumanEval (Chen et al., 2021) and MBXP (Athiwaratkun et al., 2023), that have a relatively easier level of complexity that is far from what is experienced by data scientists. However, real-world engineering challenges demand nuanced problem-solving and intricate planning, often involving multiple rounds of strategizing, experimentation, and recalibration. LLM agent systems excel in simulating this iterative process, since they comprise of an environment containing code files, description files and data files and a pre-defined action space allowing interaction with the environment. This demonstrates their capability to address intricate engineering challenges effectively. (Zhang et al., 2024b). Transitioning to codifying Machine Learning (ML) applications brings its own challenges since they often involve training models on datasets, tuning hyperparameters, devising ways to improve performance, etc. These applications are not straightforward and require a deep understanding of the underlying algorithms and techniques along with specific libraries used for implementation of plans. Although there exist AutoML-based approaches for automating such tasks (He et al., 2021; Salehin et al., 2024), these offer limited flexibility since they typically operate within predefined constraints and search spaces in the form of possible configurations of architectures and/ or hyper-parameters, which may limit their ability to explore solutions out-of-distribution of the search space. While works such as ChatDev (Qian et al., 2023) and MetaGPT (Hong et al., 2023) have explored the capabilities of LLM Agents in a software development environment, there is a notable scarcity of research on utilizing LLM Agents for solving ML tasks. Recent works like MLCopilot (Zhang et al., 2024c) introduce an assistant for solving ML tasks. However, these architectures are limited in the types of problems they can address and must strictly follow task description formats that do not align with real-world scenarios. Additionally, such assistants only suggest solutions, leaving the actual burden of implementation to the user. To the best of our knowledge, MLAgentBench (Huang et al., 2023) is the only significant benchmark addressing ML problem solving capabilities of LLM Agents directly dealing with code. Although they get good performance on some tasks in their benchmark, they focus on single-agent systems using expensive LLMs such as GPT-4, which costs approximately $0.52-$2.9 per run, depending on the task. For the experiments they conduct, they go for 8 runs per task for 15+ tasks, leading to very high experimental cost of approximately $200+. With such larger models becoming increasingly expensive to use, there is a natural incentive to develop no-cost or low-cost systems using smaller, open-source models and making them equally capable for niche tasks. However, existing agent creation frameworks like AutoGen (Wu et al., 2023) do not prioritize cost-reduction. Replacing single-agent systems using expensive LLMs with single-agent smaller, open-source LLMs may not serve the purpose. Our initial experiments with replacing all LLM calls in Huang et al. (2023) for auto-generating codes for ML tasks, with no or low-cost LLMs, namely, Gemini-Pro (Team et al., 2023)111gemini-pro 1.0 API from https://ai.google.dev/tutorials/python_quickstart. The rate limit for the free or no-cost version is sufficient for conducting our experiments, however, we also include costs for a no-cost version with pay-as-you-go pricing, CodeLlama (Rozière et al., 2024)222https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf and Mixtral(Jiang et al., 2024)333Mixtral-8x7B-v0.1 https://huggingface.co/mistralai/Mixtral-8x7B-v0.1, yield very poor results for all of the tasks in a single-agent setting. In real-world setting any complicated tasks are rarely tackled by a single individual alone, especially when all the individuals do not possess the required expertise to perform the task. Instead, teams of engineers collaborate, with each member having a unique role (persona) and contributing unique expertise and skills to achieve the target with collective efforts. Past works on LLM agents have simulated this real-world setting by designing multi-agent frameworks (Li et al., 2024; Shen et al., 2024), combining LLM experts (Wang et al., 2023; Ding et al., 2024) and defining cascades (Chen et al., 2023; Yue et al., 2024; Zhang et al., 2023) for tasks such as code generation, reasoning, question answering, etc. Cascades refer to the chaining of LLMs in a progressive fashion, where, a weaker LLM is invoked first and if the response is not satisfactory then stronger LLMs are invoked. However, to the best of our knowledge multi-agent frameworks with open-source LLMs as agents have not be explored for engineering of ML tasks. In this paper, we address the gap of utilizing LLMs for solving ML tasks by proposing a system that leverages - (i) Multi-LLM Agents as a combination of experts using profiling, (ii) LLM Cascades, (iii) Efficient retrieval of relevant past observations, and (iv) our novel occasional ask-the-expert calls to GPT-4 444gpt-4-0125-preview https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo for planning. Our approach aims to bridge the divide between capabilities of cheaper LLMs and the requirements of complex ML tasks, offering a more cost-efficient and scalable solution. Through empirical analysis, we validate the following claims: • Our best performing multi-agent system using no-cost or low-cost versions of Gemini-Pro as the base LLM, is able to perform tasks at a fraction of the cost (on an average average $0.054 for no-cost and $0.120 for low-cost version per run per ML task in MLAgentBench Dataset) as compared to benchmarked single-agent GPT4 system presented in Huang et al. (2023) (on an average $0.931 per run per task) • With 94.2% and 87.1% cost reduction for the no-cost and low-cost Gemini-Pro versions, our best performing multi-agent system is able to yield better success rate of 32.95% averaged for all the tasks in MLAgentBench as compared to the GPT4 based single-agent system yielding 22.72% average success rate for all tasks. • Our best performing multi-agent system is able to achieve equal or better performance for 45.45% of tasks when compared to the GPT4-based Single-Agent system in Huang et al. (2023), whereas it yields comparable performance for other tasks"
https://arxiv.org/html/2411.07362v1,Factorised Active Inference for Strategic Multi-Agent Interactions,"Understanding how individual agents make strategic decisions within collectives is important for advancing fields as diverse as economics, neuroscience, and multi-agent systems. Two complementary approaches can be integrated to this end. The Active Inference framework (AIF) describes how agents employ a generative model to adapt their beliefs about and behaviour within their environment. Game theory formalises strategic interactions between agents with potentially competing objectives. To bridge the gap between the two, we propose a factorisation of the generative model whereby each agent maintains explicit, individual-level beliefs about the internal states of other agents, and uses them for strategic planning in a joint context. We apply our model to iterated general-sum games with 2 and 3 players, and study the ensemble effects of game transitions, where the agents’ preferences (game payoffs) change over time. This non-stationarity, beyond that caused by reciprocal adaptation, reflects a more naturalistic environment in which agents need to adapt to changing social contexts. Finally, we present a dynamical analysis of key AIF quantities: the variational free energy (VFE) and the expected free energy (EFE) from numerical simulation data. The ensemble-level EFE allows us to characterise the basins of attraction of games with multiple Nash Equilibria under different conditions, and we find that it is not necessarily minimised at the aggregate level. By integrating AIF and game theory, we can gain deeper insights into how intelligent collectives emerge, learn, and optimise their actions in dynamic environments, both cooperative and non-cooperative.","Collective intelligence, the emergent ability of groups to solve problems more effectively than individuals, is a phenomenon observed across biological, social, and artificial systems. Understanding the mechanisms that drive this collective behaviour is essential for advancing fields as diverse as economics, neuroscience, and multi-agent systems. Game theory models incentivised social interactions with potentially competing objectives, where a utility function maps behaviour to the real numbers. A Nash equilibrium represents the point where agents, independently maximising their utility, have no incentive to change their strategy. Bridging the gap between idealised game-theoretical models and the often messy realities of agents interacting in complex environments presents a persistent challenge. Traditional game theory often falters when agents deviate from perfect rationality Camerer (2011). This challenge becomes particularly salient in the face of strategic uncertainty, where agents grapple with uncertainty about the actions and intentions of others, and equilibrium selection, where multiple potential equilibria exist without clear mechanisms for convergence. Shoham et al. Shoham et al. (2007) drew attention to a key issue with equilibrium selection: It seems to us that sometimes there is a rush to investigate the convergence properties, motivated by the wish to anchor the central notion of game theory in some process, at the expense of motivating that process rigorously. The \acAIF, a process theory rooted in neuroscience, can offer a compelling perspective on these challenges. AIF provides a principled way to describe how agents adapt their behaviour based on probabilistic beliefs about their environment. In recent years, AIF models for single-agent tasks have developed rapidly and reached maturity Friston et al. (2024a). However, the application of AIF to multi-agent settings remains in its infancy Friston et al. (2024b). We begin by reviewing recent work on the intersection of AIF and Bayesian agents, game theory, and multi-agent systems (§2). Integrating the two ends of the spectrum, we propose a factorisation of the generative model whereby an agent maintains explicit, individual-level beliefs about the internal states of other agents and uses them for strategic planning in a joint context (§3). We apply our model to iterated general-sum games with 2 and 3 players and study the ensemble effects of game transitions, where the agents’ preferences (game payoffs) and their associated equilibria change over time (§4). We present a dynamical analysis of two key active inference quantities: the \acVFE (§4.1) and the \acEFE (§4.2) from numerical simulation data. The ensemble-level EFE allows us to characterise the basins of attraction of games with multiple Nash Equilibria (such as the Stag Hunt) under different conditions, and we find that it is not necessarily minimised at the aggregate level."
https://arxiv.org/html/2411.07302v1,Merit-Based Sortition in Decentralized Systems,"In decentralized systems, it is often necessary to select an ‘active’ subset of participants from the total participant pool, with the goal of satisfying computational limitations or optimizing resource efficiency. This selection can sometimes be made at random, mirroring the sortition practice invented in classical antiquity aimed at achieving a high degree of statistical representativeness. However, the recent emergence of specialized decentralized networks that solve concrete coordination problems and are characterized by measurable success metrics often requires prioritizing performance optimization over representativeness. We introduce a simple algorithm for ‘merit-based sortition’, in which the quality of each participant influences its probability of being drafted into the active set, while simultaneously retaining representativeness by allowing inactive participants an infinite number of chances to be drafted into the active set with non-zero probability. Using a suite of numerical experiments, we demonstrate that our algorithm boosts the quality metric describing the performance of the active set by >2 times the intrinsic stochasticity. This implies that merit-based sortition ensures a statistically significant performance boost to the drafted, ‘active’ set, while retaining the property of classical, random sortition that it enables upward mobility from a much larger ‘inactive’ set. This way, merit-based sortition fulfils a key requirement for decentralized systems in need of performance optimization.","The term ‘sortition’ originally refers to the process of randomly selecting representatives in a democratic system, a practice dating back 2.5 millennia to ancient Athens, where the selection of public officials by lottery was seen as the best way of achieving fairness in society (e.g. Headlam-Morley, 1891). Since then, sortition has spread over the world as a way of obtaining representative selections of politicians, public officials, or advisors, going through ebbs and flows in terms of its popularity (e.g. Flanigan et al., 2021; Jacquet et al., 2022; Sintomer, 2023a). Advocates of sortition often highlight positive implications such as fairness, representativeness, and efficiency (e.g. Engelstad, 1989; Sintomer, 2023b). In permissionless, decentralized systems (e.g. Nakamoto, 2008; Buterin, 2014), a form of sortition is often needed too. Across a large set of anonymous contributors, validators, or other participants, there exists a high degree of redundancy that does not require all participants to be involved in the decision process (e.g. Wüst & Gervais, 2018). Sometimes, there may exist concrete computational limitations why it is infeasible to involve all participants. In any of such cases, (pseudo)-random subsets may be able to fulfil the same task without a serious loss of security or performance. In principle, this can be done using classical, random sortition techniques (of which numerous trustless examples exist, see e.g. Gilad et al. 2017; Saa & Stern 2019; Freitas et al. 2023) to achieve representativeness and efficiency. However, recent developments in decentralized networks have brought about a rapid growth of systems that aim to achieve concrete goals, often with measurable performance or success. Examples are decentralized machine intelligence and inference systems (e.g. Rao et al., 2021; Kruijssen et al., 2024), on-chain oracles (e.g. Ellis et al., 2017; Breidenbach et al., 2021), or internet-of-things networks (e.g. Banerjee et al., 2023). In such systems, which aim to achieve a quantifiable degree of success, the goal of sortition is to improve efficiency not only while maintaining representativeness, but also while optimizing performance. The latter goal is achieved not through random sortition, but by letting the performance of each participant influence their probability of being drafted. We refer to this concept as merit-based sortition. In this paper, we introduce a simple algorithm for merit-based sortition that can be used to increase computational efficiency by limiting active participation without sacrificing (and generally improving) performance. This is possible, because the algorithm: 1. optimizes the quality of the active set of participants by letting the probability of relegation out of the active set decrease with the participant’s quality; 2. retains fairness and representativeness by allowing inactive participants an infinite number of chances to be drafted into the active set, in such a way that the probability and frequency of promotion increase with the participant’s quality. In §2 we outline the proposed algorithm for merit-based sortition. We investigate its quantitative performance and behavior in §3 using a suite of numerical experiments. Finally, we summarize our results in §4."
https://arxiv.org/html/2411.07656v1,Mitigating Bias in Queer Representation within Large Language Models: A Collaborative Agent Approach,"Large Language Models (LLMs) often perpetuate biases in pronoun usage, leading to misrepresentation or exclusion of queer individuals. This paper addresses the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns (""he,"" ""she"") when inclusive language is needed to accurately represent all identities. We introduce a collaborative agent pipeline designed to mitigate these biases by analyzing and optimizing pronoun usage for inclusivity. Our multi-agent framework includes specialized agents for both bias detection and correction. Experimental evaluations using the Tango dataset—a benchmark focused on gender pronoun usage—demonstrate that our approach significantly improves inclusive pronoun classification, achieving a 32.6 percentage point increase over GPT-4o in correctly disagreeing with inappropriate traditionally gendered pronouns (\chi^{2}=38.57,p<0.0001). These results accentuate the potential of agent-driven frameworks in enhancing fairness and inclusivity in AI-generated content, demonstrating their efficacy in reducing biases and promoting socially responsible AI.","The advancement of Large Language Models (LLMs) has significantly advanced natural language processing (NLP), enabling machines to generate human-like text and perform complex language tasks with notable proficiency [3, 9]. However, LLMs often inherit and amplify societal biases present in their training data, leading to the marginalization of underrepresented groups [4, 2]. Among these groups, the queer community faces unique challenges in AI representation, particularly concerning pronoun usage and gender identity [16, 6]. Existing bias mitigation techniques, such as data augmentation [17], debiasing algorithms, and fairness-aware machine learning models [8, 7] primarily focus on broader demographic categories like binary gender and race. These methods often fail to address the variation of queer identities, which involve the fluidity and diversity of gender expressions and the evolving language used [10, 16, 1]. Pronouns such as ""they,"" ""xe,"" ""ey,"" and ""fae"" are used by non-binary and transgender individuals but are often underrepresented or misinterpreted by LLMs [5]. Misgendering and exclusionary language can lead to perpetuating discrimination against queer individuals [11, 15]. Therefore, addressing queer bias in LLMs requires specialized approaches that account for the complexities of gender identity and pronoun usage. In this paper, we address the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns when inclusive language is needed. We introduce a collaborative agent pipeline designed to reduce biases in pronoun usage, thereby improving the representation of queer individuals in AI-generated content. Our multi-agent framework includes specialized agents for bias detection and optimization, focusing on pronoun inclusivity."
https://arxiv.org/html/2411.07634v1,Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel Machine Scheduling,"Scheduling problems pose significant challenges in resource, industry, and operational management. This paper addresses the Unrelated Parallel Machine Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent Reinforcement Learning (MARL) approach. The study introduces the Reinforcement Learning environment and conducts empirical analyses, comparing MARL with Single-Agent algorithms. The experiments employ various deep neural network policies for single- and Multi-Agent approaches. Results demonstrate the efficacy of the Maskable extension of the Proximal Policy Optimization (PPO) algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced scenarios, Multi-Agent approaches reveal challenges in cooperative learning but a scalable capacity. This research contributes insights into applying MARL techniques to scheduling optimization, emphasizing the need for algorithmic sophistication balanced with scalability for intelligent scheduling solutions.","Scheduling problems constitute a subset of optimization challenges that find widespread applications across various sectors, encompassing resource management [1], industry [2, 3], and operational management [4]. In particular, production scheduling, an essential facet of manufacturing, revolves around the efficient and cost-effective allocation of limited resources to support production processes. In this context, implementing flexible and intelligent strategies for industrial scheduling emerges as an imperative. The primary objective of scheduling problem optimization is to identify an advantageous combination of decision variables within a defined search space. These decision variables dictate the order in which processes or tasks are assigned to a set of machines, typically entailing complex combinatorial problems. These problems often involve optimizing multiple objectives, depending on the system demands [5]. In industrial settings, the overall aim predominantly implies minimizing job completion times while accommodating other objectives, such as resource utilization or environmental considerations. It is important to note that real-world industrial scheduling problems imply exploring extensive search spaces, usually culminating in NP-hard problem instances [6]. This inherent intricacy poses complex challenges to the research and development community. This characteristic has attracted considerable attention from the research community, as exemplified in the comprehensive survey by Allahverdi et al. [7]. Notwithstanding the advancements in this field, current approaches have notable limitations [8], encompassing computational complexity [9] and the capacity to generalize and adapt to diverse problem instances [10, 11]. In response to these constraints, contemporary research has increasingly employed advanced decision-making techniques. Deep Learning techniques have obtained significant attention in intelligent decision-making systems within the research community [12]. Specifically, Reinforcement Learning (RL) approaches have emerged as valuable solutions to addressing scheduling in complex environments [13]. This machine learning paradigm relies on utilizing an intelligent agent that learns from the actions it takes and the rewards it receives in response to those actions. Reinforcement Learning models exhibit adaptability to non-deterministic environments, making it a flexible approach for optimization within dynamic and uncertain environments. This paper presents a study of a Multi-Agent Reinforcement Learning (MARL) approach to addressing an optimal job scheduling problem. The research is performed in the Unrelated Parallel Machine scheduling problem (UPMS) with setup times and resources proposed by Fanjul et al. [14], a single-stage job scheduling problem variant. This research presents the novelty of employing a MARL approach that extends beyond traditional methods. In this sense, the work reviews the RL environment deployed in the study and conducts empirical analyses, comparing the MARL approach with various Single-Agent algorithms. The contribution aims to provide a practical evaluation of the efficacy of addressing complex decision-making problems like UPMS. The remainder of the article is structured as follows: Section 2 provides an overview of reinforcement learning on scheduling problems. Section 3 formally describes the problem addressed in the paper. Section 4 describes the implementation details. Section 5 introduces the proposed approach and implementation insights. Section 6 presents the evaluation and validation process conducted in the experiments. Finally, section 7 contains the concluding remarks and outlines some areas for further research."
https://arxiv.org/html/2411.07161v1,RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration,"This study investigates the efficacy of Multi-Agent Systems in eliciting cross-agent communication and enhancing collective intelligence through group decision-making in a decentralized setting. Unlike centralized mechanisms, where a fixed hierarchy governs social choice, decentralized group decision-making allows agents to engage in joint deliberation. Our research focuses on the dynamics of communication and decision-making within various social choice methods. By applying different voting rules in various environments, we find that moderate decision flexibility yields better outcomes. Additionally, exploring the linguistic features of agent-to-agent conversations reveals indicators of effective collaboration, offering insights into communication patterns that facilitate or hinder collaboration. Finally, we propose various methods for determining the optimal stopping point in multi-agent collaborations based on linguistic cues. Our findings contribute to a deeper understanding of how decentralized decision-making and group conversation shape multi-agent collaboration, with implications for the design of more effective MAS environments.","Collaboration is a fundamental aspect of the nature and human society. Whether among humans or animals, working together allows groups to overcome individual limitations and achieve greater collective outcomes. In nature, collaboration often arises as a strategy to boost survival, enhance resource gathering, or increase efficiency in completing tasks (Schmidt & Mech, 1997). Similarly, in human societies, collaboration drives innovation, facilitates problem-solving, and fosters shared understanding, enabling individuals to address complex challenges that would be unmanageable otherwise (De Man & Duysters, 2005; Graesser et al., 2018; Bittner & Leimeister, 2013). This innate tendency to collaborate is evident across various domains, from social communities to technological systems, where multiple entities coordinate their efforts toward a common goal. As we advance in developing intelligent agents, understanding and replicating these collaborative dynamics in artificial systems has become increasingly important, predominantly to cope with the complexity and adaptability seen in real-world interactions. Agents powered by Large Language Models (LLMs) have demonstrated impressive problem-solving capabilities across a wide range of tasks. However, single-agent systems encounter significant difficulties when tasked with problems that are either too large or complex, often resulting in instability, misalignment with the intended request, and hallucination (Liu et al., 2024; Kuhn et al., 2023; Lyu et al., 2023). To address these limitations, research has increasingly turned toward Multi-Agent Systems (MAS). MAS have shown greater efficacy in harnessing collective intelligence by allowing individual agents to specialize in distinct skills and facilitating effective collaboration among them (Guo et al., 2024). When agents working together in a MAS, it is natural for them to have varying interpretations and perspectives. While some opinions may align, disagreements are also frequent. This creates an inevitable tension between cooperation and competition, stemming from differences in backgrounds, information access, and individual goals. Therefore, the process of aggregating models’ diverse predictions into a final group decision becomes a crucial aspect of dynamic multi-agent collaboration. Many existing LLM-driven MAS are designed based on centralized group decision-making, which typically involves layered or centralized architectures with a hierarchy. When there is a conflict between agents, it is resolved by a pre-assigned agent or a pre-defined process. These systems are often used for tasks where agent networks follow the waterfall method, leading to a stratified arrangement among agents (Hong et al., 2023; Qian et al., 2023; Dong et al., 2023). However, this hierarchical setup poses several critical challenges: (1) fairness: individual agent messages may not be accurately represented in the final group outcome, leading to potential misrepresentation (Jiang & Lu, 2019); (2) rigidity: the system’s fixed structure may lead to over-fitting to specific scenarios (Chen et al., 2023), reducing its adaptability to diverse and dynamic environments; and (3) bias: the agent with final decision-making power may introduce its own biases into the process, potentially distorting the outcomes (Owens et al., 2024). Additionally, centralized MAS cannot perform in environments requiring independent agent decisions, where private or incomplete information hinders a central authority from dictating optimal strategies (Xu et al., 2023). Figure 1: Overview of our multi-agent collaboration platform: RoundTable. It uses a round-based collaboration where agents simultaneously send messages, propose solutions, and vote. Based on a social choice mechanism, RoundTable selects the most preferred proposal for group decisions. Brief introduction of RoundTable is in Section 3.1, details are in Appendix A.1. Decentralized group decision-making can ease these issues by distributing power among agents, where each agent has the ability to participate in the process. This is a common setting in world simulation and embodied environment, where agents need to behave independently because there exists information asymmetry or data boundary between agents (Mandi et al., 2024; Zhang et al., 2023a; Xu et al., 2023; Park et al., 2023), and possibly variability in capabilities that different agents have. With decisions made by multiple agents, the flexible structure of decentralized MAS adapts to various environments, but the lack of a fixed hierarchy demands careful monitoring of collaboration patterns. Centralized and decentralized group decision-making attempt to mimic the ways in which human societies form collective policies, such as in monarchies and democracies. These decision-making mechanisms, known as social choice, are studied across various fields including economics, mathematics, philosophy, and social science, with the goal of aggregating and synthesizing individual preferences into a unified consensus. However, the impact of social choice methods on LLM-based MAS has yet to be explored. In this study, we evaluate various social choice methods across different environments to observe and analyze agents’ group behaviors and collaboration pattern. This paper aims to provide the following research contributions: • We investigate how collaborative behavior in decentralized MAS varies across social choice methods, providing insights into how they influence overall cooperation and outcomes. • We identify key language features in multi-agent conversations as indicators of collaboration, offering a novel approach to analyzing linguistic cues in effective or ineffective interactions. • We propose various methods for determining the optimal stopping point in multi-agent collaboration, utilizing the linguistic features we identified."
https://arxiv.org/html/2411.07039v1,"Learning Collective Dynamics of Multi-Agent Systems 
using Event-based Vision","This paper proposes a novel problem: vision-based perception to learn and predict the collective dynamics of multi-agent systems, specifically focusing on interaction strength and convergence time. Multi-agent systems are defined as collections of more than ten interacting agents that exhibit complex group behaviors. Unlike prior studies that assume knowledge of agent positions, we focus on deep learning models to directly predict collective dynamics from visual data, captured as frames or events. Due to the lack of relevant datasets, we create a simulated dataset using a state-of-the-art flocking simulator, coupled with a vision-to-event conversion framework. We empirically demonstrate the effectiveness of event-based representation over traditional frame-based methods in predicting these collective behaviors. Based on our analysis, we present event-based vision for Multi-Agent dynamic Prediction (evMAP), a deep learning architecture designed for real-time, accurate understanding of interaction strength and collective behavior emergence in multi-agent systems.Keywords Multi-Agent System \cdot Event Camera \cdot Swarm Behavior","The systems of large number (>10) of agents, hereafter referred to as a multi-agent system, are crucial in a wide range of autonomy applications, including swarm robotics [1] and fleets of autonomous vehicles [2]. Inspired by collective behaviors observed in nature such as fish schools and bird flocks, these systems aim to achieve collective goals through the interaction among individual agents using a set of decentralized rules. Analytical flocking models such as Reynolds model [3] or Vicsek model [4] replicate collective behaviors observed in nature, but these models require precise localization which is rarely possible in the real-world applications. Therefore, real-time prediction of collective behavior, like how and when agents will achieve a collective goal, is essential for adapting the local rules and controlling multi-agent systems in a real-world environment [5, 6] as illustrated in Figure 1. This prediction is valuable in competitive settings like swarm herding [7], where understanding the system dynamics of adversarial agents can enhance strategic control. The prediction is crucial for optimizing resources and minimizing risks in complex operations, such as coordinating astrobots in telescopes [8, 9], where precise maneuvering and dense formations are important. As swarm operations scale in complexity, the prediction of collective behavior becomes increasingly critical, underscoring the need for advancement in methods for learning and control of multi-agent systems. Figure 1: Application examples of collective dynamic prediction of multi-agent system. Multi-agent dynamic prediction is helpful for both systems that are under and beyond control. Figure 2: Several methods for understanding dynamics in a multi-agent system. (a) Many previous studies in multi-agent prediction require pre-processing for detecting agents. This paper focuses on scene-based perception: compared to (b-1) frame-based methods, (b-2) event-based methods demonstrate their effectiveness in understanding multi-agent dynamics. This paper introduces the novel problem of real-time prediction of collective behavior in multi-agent dynamics from visual observations. Many previous studies on multi-agent systems assume prior knowledge of agent states and are primarily designed to predict individual agent trajectories [10, 11, 12]. A potential approach involves integrating object detection with trajectory predictors (Figure 2(a)). However, challenges exist in both object detection and trajectory prediction for understanding multi-agent dynamics. In object detection, the small size of agents and their high density in the scenes (Figure 3) hinder deep learning models from accurately determining agent positions [13, 14]. For trajectory prediction, inferring agent-wise trajectories and collective behavior from multiple trajectories becomes computationally infeasible with M\times T\times N trajectories (M: number of agents, T: sequence length, N: number of sequences). Moreover, combining a multi-agent trajectory predictor with an object detector requires substantial computational resources. Even with exact positions provided using GPS augmented for each agent (e.g., drones) instead of object detection, high energy usage and latency from continuous GPS usage, as well as the computational burden from multi-agent trajectory predictions, present challenges in employing state (position)-based multi-agent prediction models for systems with large number of agents. In contrast, we are inspired by the success of deep learning (DL) methods in learning dynamics for prediction and control from visual inputs without the need for exact state knowledge [15, 16, 17, 18]. While existing vision-to-dynamics models have been demonstrated for systems with a few agents, predicting the dynamics of collective multi-agent systems (with more than 10 agents) from vision remains an unexplored area. Our objective is to directly learn and predict the collective dynamics (not the states of each agent) of a multi-agent system from scenes captured via frame and event-based cameras, as illustrated in Figure 2(b). Figure 3: Sample flocking scenes [19, 20] and simulators (NetLogo [21], AgentPy [22]). We propose leveraging advancements in event cameras [23], which capture per-pixel brightness changes with high temporal resolution and dynamic range, to predict multi-agent collective dynamics using vision. Event-based vision has recently achieved significant improvements in object recognition, detection, and segmentation, as well as tracking high-speed objects [24, 25, 26, 27, 28]. However, applying it to understand multi-agent dynamics remains largely unexplored (Figure 2(b-2)). To address the lack of datasets for vision-based analysis of large interacting agent groups, we have created a new dataset based on Reynolds’ rule [3] using NetLogo [21] and AgentPy [22]. Frame-based inputs were generated (Figure 3) and converted to event-based data using the v2e [29] framework. Our results demonstrate that event-based methods outperform frame-based methods in capturing real-time dynamics and predicting collective behavior from early observations. Additionally, our proposed model shows superior performance in capturing time-varying dynamics compared to other event-based approaches. This paper makes following unique contributions: • This paper introduces a novel problem, vision to prediction of collective multi-agent dynamics for real-time perception and control of multi-agent system. We study deep learning models for prediction of collective multi-agent dynamics from frame- and event- based visual inputs. To the best of our knowledge, this is the first study to discuss multi-agent dynamic prediction from visual observation. • This paper performs a comparative study between frame- and event-based methods, and empirically demonstrate the advantage of event representation in learning and predicting collective behavior of multi-agent systems. Prior works have studied processing event representation for various tasks, but to the best of our knowledge, this is the first work demonstrating event-based methods for predicting multi-agent dynamics. • We present a new transformer-based deep learning architecture, event-based vision for Multi-Agent dynamic Prediction (evMAP), to learn dynamics of multi-agent systems. In particular, the model is designed to efficiently recognize dynamic changes in the multi-agent systems."
https://arxiv.org/html/2411.06404v1,MA-DVF: A Multi-Agent Navigation Framework using Dynamic Velocity Vector Field,"In this paper we propose MA-DV2F: Multi-Agent Dynamic Velocity Vector Field. It is a framework for simultaneously controlling a group of vehicles in challenging environments. DV2F is generated for each vehicle independently and provides a map of reference orientation and speed that a vehicle must attain at any point on the navigation grid such that it safely reaches its target. The field is dynamically updated depending on the speed and proximity of the ego-vehicle to other agents. This dynamic adaptation of the velocity vector field allows prevention of imminent collisions. Experimental results show that MA-DV2F outperforms concurrent methods in terms of safety, computational efficiency and accuracy in reaching the target when scaling to a large number of vehicles. Project page for this work can be found here: https://yininghase.github.io/MA-DV2F/","I INTRODUCTION The task of multi-agent navigation has attracted widespread attention in recent years due to myriad applications in areas such as search and rescue missions [1], area exploration [2], pickup and delivery services [3], warehouses [4], self-driving cars [5] etc. The task of multi-agent path finding/navigation involves simultaneously directing a group of vehicles from their initial position to their desired destination while avoiding collisions with other agents. The task is known to be NP-hard even in the discrete setting [6]. An ideal algorithm must find the optimal solution in limited time. This leads to contradictory goals, since determining the optimal solution requires searching a larger solution space, which necessitates more time. In structured environments such as indoor spaces, prior knowledge and understating of the layout impose constraints, that can reduce the solution search space. In unstructured environments, there are no such constraints. This allows an algorithm the flexibility to find a solution. However, since the search space is much larger, there is no guarantee that the solution found is optimal. The problem is further exacerbated when the search space in continuous and agents are non-holomonic vehicles. The constraints arising from the vehicle kinematics add to the complexity. There have been various techniques and heuristics attempting to find (near-)optimal trajectories for multiple agents. The methods can be divided into two primary categories: 1) Learning based data driven methods [7, 8] and 2) Search/optimization based methods [9, 10]. Learning based algorithms involve training a neural network on data, with the understating that the network will generalize at inference time. The training data should encompass all the situations that the model is expected to encounter at test time. This necessitates a large amount of training samples. The greatest challenge with large training samples is determining the corresponding supervised labels for training; that might be too tedious to obtain. In contrast to supervised learning, an alternate would be to train using reinforcement learning (RL) where the model explores the environment and acquires rewards or penalties depending on the actions taken [11]. The model then exploits this experience to execute the correct control actions at test time. However, RL algorithms tend to be more sample inefficient than supervised methods. In contrast, optimization [12] or search based [13] methods involves simultaneously optimizing trajectories for multiple vehicles. As the number of vehicles are added, the complexity of the optimization/search becomes intractable making it infeasible for scaling to a large number of vehicles [14, 15]. Figure 1: Shows the pipeline for MA-DV2F. The state of all vehicles at time t is used to create the dynamic velocity vector field (DV2F) from which the reference control commands are determined. These commands are in turn used to determine the next state at time t+1 using the kinematic motion model. Note that the reference control commands can optionally be used to train a GNN in a self-supervised manner (indicated by dotted arrows). Meanwhile, the DV2F is shown for the black ego-vehicle with 2 other vehicles (red & blue) and 2 obstacles (green & brown) in the scene. The target state for the black ego-vehicle is shown by the dotted rectangular with a solid arrow at the top of the map. The dotted circle around the target state is the parking region. The black arrows indicate the ideal reference orientation as unit vectors which the ego-vehicle should attain at each different position on the map. The colored dotted circles around other vehicles and obstacles show the collision avoiding regions for the black ego-vehicle. Each black arrow in these regions is composed of an attractive target approaching component (gray arrow) and a repulsive collision avoidance component (colored arrow). Note that due to kinematic constraints, the ideal orientation might not be attainable. The shaded wedge in front of the ego-vehicle shows the reachable angle closest to the ideal. An example of the dynamic velocity vector field is shown in the project page: https://yininghase.github.io/MA-DV2F/#VD. Note that DV2F for the neighboring red & blue vehicles is likewise created separately. In this paper, we propose Multi-Agent Dynamic Velocity Vector Field (MA-DV2F), which generates vectors for the reference speed and orientation vectors for each position on the map for every vehicle. The vehicles then just need to follow in the direction of their respective velocity vector fields to successfully reach their destinations. The vector field for each vehicle is generated independently and can be adapted dynamically depending on the vehicle’s proximity to other agents (neighbouring vehicles or obstacles). Decoupling reduces the complexity of adding vehicles & allows for parallel generation of DV2F of each vehicle, thereby increasing the throughput. An added benefit of our approach is that the generated DV2F can be used to train a learning based graphical neural network (GNN) in a self-supervised manner. This self-supervised learning based approach neither requires tedious labeling of data nor necessitates sample inefficient environment exploration. We test our framework under challenging collision prone environments. A scenario is regarded to be challenging if trajectories of different vehicles considered independently from other agents intersect at multiple places at the same time. This would necessitate a collision avoidance maneuver for safe navigation towards the target. Figure 1 shows the pipeline for both the MA-DV2F (left branch, solid arrows) and optional training of the the self-supervised GNN counterpart (right branch, dotted arrows). The input is a continuous state representation of all the multiple vehicles and the outputs are the corresponding continuous control variables. The vehicles are non-holonomic with rotation radius determined by the kinematic model. We summarize the contribution of this letter as follows: • Our proposed MA-DV2F outperforms other concurrent learning and search based approaches for the task of multi agent navigation in challenging, collision prone environments. • Even the self-supervised learning based counterpart of MA-DV2F scales better than other learning and search based methods. • MA-DV2F can determine the solutions orders of magnitude faster than other SOTA search based approaches. • We shall release the complete code of MA-DV2F upon acceptance on the project page here: https://yininghase.github.io/MA-DV2F/. The project page also contains additional supplementary information such videos better depicting the operation of our method in comparison with other approaches, details of the challenging scenarios, dynamics of the velocity field at different regions on the navigation grid etc."
https://arxiv.org/html/2411.06398v1,Do you want to play a game? Learning to play Tic-Tac-Toe in Hypermedia Environments,We demonstrate the integration of Transfer Learning into a hypermedia Multi-Agent System using the Multi-Agent MicroServices (MAMS) architectural style. Agents use RDF knowledge stores to reason over information and apply Reinforcement Learning techniques to learn how to interact with a Tic-Tac-Toe API. Agents form advisor-advisee relationships in order to speed up individual learning and exploit and learn from data on the Web.,"Agents are fundamental to the earliest formulations of the Web as a network of machine-readable data [2]. The idea was that these agents could navigate open, discoverable and understandable areas of the Web, interacting with resource in order to achieve their goals. Where agents become aware of one another’s presence, this presents opportunities for collaboration and knowledge sharing between them. Where is that vision now? It is widely accepted that from a practical perspective, the Semantic Web agent vision has not been progressed [9]. However state-of-the-art research into a new breed of Web agents known as hypermedia agents looks to address this. Hypermedia Multi-Agent Systems (HMAS) is concerned with the creation of MAS that are designed to work with Web architecture [3]. A key goal of this research is to explore how hypermedia agents can learn and adapt on the Web. [15, 16, 17] explore this from a single agent perspective using human-led behaviour acquisition, third party LLM models and dynamic machine-readable action descriptions respectively. This paper illustrates how machine learning techniques can enhance agent learning in a hypermedia multi-agent system111Code available at https://gitlab.com/mams-ucd/examples/HyperTicTacToe. The particular approach we adopt is focused around Transfer Learning (TL), which is typically applied with agents equipped with Reinforcement Learning abilities. TL aims to alleviate some of the traditional shortcomings of Reinforcement Learning, the lack of training data [18], or sufficient time to explore and refine the model: in order to learn a policy, an individual agent must take many steps, exploring the environment and potentially performing multiple sub-optimal actions [12]. TL aims to speed up an agent’s learning so that it learns faster than it would if it learned individually. Agents with a pre- or partially trained model can transfer some or all knowledge to another learning agent [13]. The focus of this demonstration is to illustrate how these concepts and principles can be exploited in a hypermedia MAS using the currently available framework on top of ASTRA[5], an agent programming language that is an implementation of AgentSpeak(L) [11] and is based on to the Belief, Desire and Intention (BDI) paradigm. This forms part of wider research into how to integrate this type of learning into agent programming in ASTRA and the requirements for language-level changes. We present an Application Programming Interface (API) which allows agents to play Tic-Tac-Toe on the Web (TTT API). Multiple agents can interact with the API and each other. All are deployed on the Web and interact using HTTP protocols222https://www.w3.org/Protocols/. Agents use open actions: they navigate links and forms, without hard-coded knowledge of the endpoints. This reflects a move towards general models of interaction with the environment from bespoke actions to more abstract, open actions constrained by environmental signifiers such as hypermedia form actions and HTTP verbs. Tic-Tac-Toe is a well understood agent learning problem that has a small, finite state set 3333^{9}, compared to Backgammon with over 10^{20} [14] but it cannot be solved through classical techniques as they assume a certain opponent strategy [12]. It is used here to demonstrate how hypermedia agents can learn and engage in Transfer Learning in a Web environment."
https://arxiv.org/html/2411.05904v1,Autonomous Industrial Control using an Agentic Framework with Large Language Models,"As chemical plants evolve towards full autonomy, the need for effective fault handling and control in dynamic, unpredictable environments becomes increasingly critical. This paper proposes an innovative approach to industrial automation, introducing validation and reprompting architectures utilizing large language model (LLM)-based autonomous control agents. The proposed agentic system—comprising of operator, validator, and reprompter agents—enables autonomous management of control tasks, adapting to unforeseen disturbances without human intervention. By utilizing validation and reprompting architectures, the framework allows agents to recover from errors and continuously improve decision-making in real-time industrial scenarios. We hypothesize that this mechanism will enhance performance and reliability across a variety of LLMs, offering a path toward fully autonomous systems capable of handling unexpected challenges, paving the way for robust, adaptive control in complex industrial environments. To demonstrate the concept’s effectiveness, we created a simple case study involving a temperature control experiment embedded on a microcontroller device, validating the proposed approach.","Chemical plants are moving towards autonomous operations. Especially for routine operations that follow well-defined procedures, autonomous operation is considered technically feasible with currently available technologies (Borghesan et al. (2022)). However, a significant challenge in developing autonomous control systems is the need to account for long-tail events, which are rare, unpredictable occurrences that fall outside of the scope of typical operational scenarios. In industrial contexts, these long-tail events can range from unexpected equipment failures to highly unusual process disturbances. Traditional automation approaches struggle to handle such events, as they rely heavily on predefined rules and algorithms, rendering them overly rigid and poorly adapted to situations that deviate from expected patterns. Solutions leveraging machine learning models have made some progress in handling known unknowns such as known disturbances or possible plant-model mismatch but they tend to fail in handling anomalies. This is primarily because these models are trained on majority-class data, as anomaly data is scarce or available in too few samples. As a result, these solutions struggle to detect and react to anomalies in real-time, particularly in scenarios involving unknown unknowns—unforeseen disturbances that the system was not designed to handle. Currently, human operators play a key role in managing the type of unknown unknowns discussed previously. Leveraging their reasoning abilities and domain knowledge human operators can dynamically assess a situation and adjust their actions based on real-time feedback. The overarching goal of this work is to bridge these reasoning and knowledge use abilities to autonomous systems using generative machine learning models as intelligent control agents. We particularly focus on the use of Large Language Models (LLMs) for this purpose. LLMs, with their extensive knowledge bases and reasoning capabilities, represent a promising avenue for developing intelligent control agents capable of autonomously analyzing incoming data, diagnosing anomalies, and making informed control decisions in a zero-shot manner- making inferences and offering solutions to scenarios they have not explicitly encountered in training (Pantelides et al., 2024). The challenge is transitioning to a fully automated system that can evaluate responses and adjust actions independently. To address this, we propose a reprompting architecture that empowers LLMs to function as autonomous control agents. This architecture enables agents to validate their actions against a digital twin, implementing them in the physical system if they pass validation; if not, the agent is prompted to revise its approach. This iterative process significantly enhances decision-making capabilities and improves system performance in real-time."
https://arxiv.org/html/2411.07168v1,Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network,"Mining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations—on-device, on-gateway, or on-cloud—based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90% classification accuracy, while cloud-based inference reached 99%. On-sensor inference reduced power consumption by approximately 44%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.","THE mining sector is a vital component of the global resource economy, providing the raw materials essential for industrial production and infrastructure development. By 2026, the industry is projected to reach a market value of $3.36 trillion [1], driven largely by the growing demand for minerals such as lithium, cobalt, and copper, which are critical components in renewable energy technologies [2], electric vehicles [3], and consumer electronics [4]. Mining operations are inherently complex, encompassing multiple stages like exploration, extraction, processing, and transportation. To maintain efficiency and safety, the sector greatly depends on a wide range of machinery and mobile and semi-mobile equipment, including drilling rigs, shovels, excavators, haul trucks, front-loaders, and other auxiliary equipment [5]. These operations often occur in harsh, remote environments, exposing assets to extreme conditions such as high temperatures, humidity, dust, and heavy loads [6]. Prolonged exposure to these conditions leads to equipment degradation, reduced remaining useful life, increased maintenance costs, and elevated safety risks. PdM has become critical for optimizing mining operations, offering benefits such as improved system availability, cost savings, and enhanced failure prediction [7].By leveraging data from continuous condition monitoring sensors, PdM enables proactive decision-making and timely maintenance, reducing the risk of unplanned downtime [8]. The rise of Artificial Intelligence (AI) has further advanced PdM, as Machine Learning (ML) and Deep Learning (DL) algorithms can analyze vast datasets, identify patterns, and predict equipment failures more accurately than traditional methods [9]. Internet of Things (IoT) technologies, particularly Wireless Sensor Networks (WSNs), have become key components for collecting real-time data on machinery performance [10, 11, 12]. WSNs consist of spatially distributed sensor nodes and gateways that communicate wirelessly to collect and transmit data [13]. Typically, these sensor nodes are small-size, light-weight, energy-efficient, cost-effective and remarkably flexible to deploy, making them ideal for mining environments [14]. PdM frameworks are structured methodologies that encompass the entire PdM process: from data collection, preprocessing, and communication to ML and DL model development, training, and deployment. Traditional PdM frameworks often rely on a fixed inference location, either at the cloud in a dedicated serverless service (in a server on-premise far away from the operation instead) or closer to the edge on a gateway or sensor node [15, 16, 17]. Each approach has its advantages and limitations. Cloud-based inference offers superior accuracy and scalability at the cost of high latency and the need for stable network connectivity [10, 18]. Edge-based inference, both on gateways and nodes, minimizes latency and enables real-time decision-making but may increase power consumption and limited model complexity [11, 12, 19, 20]. Motivation: Each condition monitoring approach and PdM solution has its own strengths and weaknesses. Regardless of the inference approach, there is not a single technique that can detect, diagnose and predict all types of faults optimally. This call for more flexible and adaptive solutions for faster and accurate maintenance predictions under uncertainty in operational conditions. In this context, the inference location is critical to ensure high the system’s performance. The inference location in a hierarchical inference network with several levels directly determinates both the speed and accuracy of detected events. A fixed inference location may be inadequate due to dynamic changes in operational conditions over time. For instance, different expertise of machinery operators or mining fronts with different shape and leveling, expose equipment to different stress and wear levels. By leveraging on-cloud, on-gateway, and on-device inference capabilities, the system dynamically can adjusts inference locations based on trade-offs between real-time demands and conditions such as accuracy, latency, and battery range. By adapting the inference location dynamically, the PdM can be optimized the condition monitoring process, leveraging cloud resources when accuracy is critical and shifting to edge computing when real-time decision-making is mandatory. This paper presents a PdM framework that integrates edge inference approaches (such as on-gateway and on-sensor-based inference) and cloud computing services into an unified hierarchical inference system to enhance real-time condition monitoring of heavy machinery. The proposed framework leverages the strengths of each inference level to provide real-time and energy-efficient condition monitoring, adapting the inference location based on operational demands and conditions such as latency, accuracy, and energy consumption. The ESN-PdM system is evaluated through a case-study in a real-world industrial scenario, where vibration data from mining equipment is used to evaluate the operational state of the machinery and triggering alarms when anomalies arise. The main contributions of this work are as follows: • An open-source, end-to-end framework for condition monitoring and PdM of mobile mining machinery in non-stationary operations. • A novel adaptive inference mechanism that dynamically updates the inference location for a node based on operational conditions. • A guide on how to use state-of-the-art TinyML optimization approaches to achieve optimal accuracy and model compression for efficient deployment of DL models on limited hardware resources of IoT edge devices. • A comprehensive evaluation of the proposal in terms of operational status classification accuracy, inference latency, and node energy consumption through a real-world industrial case-study. This paper is organized as follows: Section II discusses about the applicability of time-varying classification of multivariate time series from mechanical and/or electrical systems, and addresses the further implementation challenges on TinyML and PdM frameworks. Section III presents the proposed ESN-PdM framework, describing its architecture, components, and adaptive inference mechanisms. Section IV introduces a case study based on DL strategies for PdM. In Section V, the ESN-PdM framework is evaluated in terms of classification accuracy for PdM, inference latency, and energy consumption. Finally, Section VI presents a summary of main findings and conclusions, and outlines future research directions."
https://arxiv.org/html/2411.07104v2,Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing,"Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0\% higher success rates and 24.5\% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.","Recent advances in quadrupedal robots have significantly improved their ability to traverse challenging terrains [1, 2, 3, 4, 5, 6]. While many studies have focused on enhancing their mobility and stability of locomotion, the manipulation capabilities of these robots remain relatively limited. Efforts have been made to improve the quadrupedal capabilities in prehensile manipulation through attaching grippers or robotic arms on the robot [7, 8, 9, 10, 11, 12], and non-prehensile manipulation by using legs [13, 14, 15, 16] or the head [17, 18] as the end-effectors. Although these advancements enable quadrupeds to handle some routine tasks, their limited ability to manipulate large and heavy objects still restricts their usefulness in demanding fields like search and rescue, construction, industrial automation, and room organization, where both dexterity and strength are essential. To address these challenges, researchers have explored adding support structures to the robots [19, 20], coordinating whole-body movements [21], and using multiple robots [22, 23] to strengthen contact forces and expand operational dimensions. However, achieving long-horizon manipulation of large objects in cluttered environments remains a largely unexplored and challenging task for quadrupeds. In this work, we focus on addressing the challenge of obstacle-aware, long-horizon pushing by coordinating the whole-body motions of multiple quadrupedal robots. We build our work upon recent works of quadrupedal pushing that demonstrate impressive results. As shown in Table I, while many approaches utilize multiple robots to enhance manipulation abilities, few focus on long-horizon pushing and obstacle avoidance, both of which are critical for real-world tasks. Additionally, the limited use of whole-body motions (e.g., relying solely on heads to push) [18, 22, 23] restricts the contact patterns between robots and objects, making it difficult for the robots to perform diverse movements and avoid collisions with obstacles. TABLE I: Comparisons between our proposed method and previous methods of quadrupedal pushing. Method Collaborative Long- Horizon Whole- Body Obstacle- Avoidance Sombolestan et al. [18] ✗ ✗ ✗ ✗ Jeon et al. [21] ✗ ✗ ✓ ✗ Sombolestan et al. [22] ✓ ✗ ✗ ✗ Nachum et al. [23] ✓ ✓ ✗ ✓ An et al. [24] ✓ ✗ ✓ ✗ Xiong et al. [25] ✓ ✗ ✓ ✗ Ours ✓ ✓ ✓ ✓ To achieve collaborative, obstacle-aware, long-horizon quadrupedal pushing through whole-body motions, we propose a hierarchical multi-agent reinforcement learning (MARL) framework with three levels of controllers. The high-level controller integrates an Rapidly-exploring Random Tree (RRT) planner [26] and a centralized adaptive policy, which processes the reference trajectory, environment, and agent information to generate subgoals for the object. The mid-level controller learns a shared decentralized goal-conditioned policy, enabling multiple robots to coordinate and push the object toward the sequential subgoals proposed by the high-level controller. The low-level controller is a pre-trained locomotion policy that executes commands from the mid-level controller. We validate our approach through a series of experiments in both simulation and real-world tests on Go1 robots, a few of which are visualized in Figure 1. Our results demonstrate that the proposed method achieves a 36.0\% higher success rate and a 24.5\% reduction in completion time compared to the best baseline approach in simulation. Furthermore, our method can be deployed on real robots to successfully complete obstacle-aware, long-horizon Push-Cuboid and Push-T tasks. The main contributions of this paper can be summarized as follows. • We propose a hierarchical MARL framework with three hierarchies that can handle long-horizon collaborative quadrupedal pushing in an environments with obstacles. • We benchmark our proposed method against baselines on various long-horizon pushing tasks involving obstacles in IsaacGym [27], demonstrating that our method significantly outperforms the baselines. • We deploy our trained hierarchical policy on real robots, successfully completing the collaborative long-horizon Push-Cuboid and Push-T tasks with coordinated whole-body motions. Figure 2: Overview of the proposed hierarchical MARL framework for collaborative long-horizon pushing tasks by quadrupedal robots. The framework comprises three layers: a high-level controller, a mid-level controller, and a low-level controller. The high-level controller utilizes an RRT planner to generate a trajectory and an adaptive policy to assign subgoals based on the dynamic states of the environment, object, and robots. The mid-level controller employs decentralized pushing policies to convert a common subgoal into agent-specific velocity commands, which are then executed by the low-level locomotion policy on each robot. Each layer is trained independently, leveraging frozen lower-level policies."
https://arxiv.org/html/2411.07099v1,Bounded Rationality Equilibrium Learning in Mean Field Games,"Mean field games (MFGs) tractably model behavior in large agent populations. The literature on learning MFG equilibria typically focuses on finding Nash equilibria (NE), which assume perfectly rational agents and are hence implausible in many realistic situations. To overcome these limitations, we incorporate bounded rationality into MFGs by leveraging the well-known concept of quantal response equilibria (QRE). Two novel types of MFG QRE enable the modeling of large agent populations where individuals only noisily estimate the true objective. We also introduce a second source of bounded rationality to MFGs by restricting agents’ planning horizon. The resulting novel receding horizon (RH) MFGs are combined with QRE and existing approaches to model different aspects of bounded rationality in MFGs. We formally define MFG QRE and RH MFGs and compare them to existing equilibrium concepts such as entropy-regularized NE. Subsequently, we design generalized fixed point iteration and fictitious play algorithms to learn QRE and RH equilibria. After a theoretical analysis, we give different examples to evaluate the capabilities of our learning algorithms and outline practical differences between the equilibrium concepts.","Learning equilibria in multi-agent games is of great practical interest but hard to scale to many agents (Daskalakis et al., 2009; Deng et al., 2023). Mean field games (MFGs) allow scaling to arbitrarily many exchangeable agents at fixed complexity. MFGs are of recent interest as a tractable method to learn approximate equilibria of rational, selfish agents (Guo et al., 2019; Cui & Koeppl, 2021; Xie et al., 2021; Laurière et al., 2022; Anahtarci et al., 2023). Thus, MFGs are applied in various settings ranging from finance to engineering (Djehiche et al., 2017; Achdou et al., 2020; Carmona, 2020). A common solution concept in the realm of multi-agent learning, and consequently in the context of MFGs, is the Nash equilibrium (NE). In a NE, each player’s strategy is considered optimal, given the strategies of the other agents, resulting in an equilibrium where no agent has an incentive to change their strategies. The optimality notion inherent in NE assumes full rationality of the individual agents. However, in many real-world situations individuals may not behave perfectly rational due to limited information processing capabilities, psychological factors, social considerations or other factors. Deviations from perfect rationality are described by the fundamental concept of bounded rationality (Simon, 1955, 1979; Kahneman & Tversky, 1982; Selten, 1990; Gigerenzer & Selten, 2002; Kahneman, 2013). Bounded rationality implies that for many real-world scenarios NE are insufficient due to their rigorous perfect rationality assumption. Instead of NE, we require a more realistic equilibrium concept accounting for partially irrational agents. A popular game-theoretic approach to modeling bounded rationality of agents are quantal response equilibria (QRE) (McKelvey & Palfrey, 1995, 1998) which are used, e.g. in economics (Breitmoser et al., 2010), robust RL (Reddi et al., 2024) and for efficient NE approximation (Gemp et al., 2024). Intuitively, in a QRE agents perceive rewards perturbed by noise and act optimally with respect to these perturbed rewards. In our work, we extend QRE to the domain of MFGs to model the behavior of a large number of agents who deviate from perfect rationality. Meanwhile on the control-theoretic side, a common approximately optimal control method is model predictive control (MPC) (Kouvaritakis & Cannon, 2016), also known as receding horizon control. To further enhance modeling of bounded rationality in MFGs, we incorporate a receding horizon method, where agents make decisions based on a limited future time horizon, reflecting more realistic decision-making processes. In contrast to MPC-based variants of MFGs such as (Inoue et al., 2021), we analyze the resulting novel receding horizon equilibria and instead focus on learning such equilibria, in a discrete-time setting. Beyond realism, introducing bounded rationality yields possible tractability advantages. NE computation for MFGs can be hard, motivating the search for alternative equilibrium notions. In this work, we show that under certain assumptions, QRE can be computed using a fixed point iteration (FPI). Moreover, QRE solutions can be seen as NE approximations with arbitrarily accurate design (Eibelshäuser & Poensgen, 2019). Recently, different equilibria have been introduced as NE approximations in MFGs (Cui & Koeppl, 2021). We compare QRE with these equilibria both theoretically and empirically and provide a new algorithm to compute QRE which extends to these equilibria. Meanwhile, for receding horizon equilibria we find novel general algorithms that work both in practice and in theory. Our main contributions are: • We formulate QRE for MFGs to incorporate bounded rationality for a more realistic MFG framework; • We integrate a receding horizon method tailored to the limited lookahead capacity of realistic agents; • We give theoretical and empirical results to put MFG QRE in context to existing equilibrium concepts; • We generalize the known fictitious play (FP) and FPI algorithms for NE to learn QRE and other equilibria; • We provide empirical examples to demonstrate the capabilities of our learning algorithms."
https://arxiv.org/html/2411.06833v1,Learning Interpretable Network Dynamics via Universal Neural Symbolic Regression,"Discovering governing equations of complex network dynamics is a fundamental challenge in contemporary science with rich data, which can uncover the mysterious patterns and mechanisms of the formation and evolution of complex phenomena in various fields and assist in decision-making. In this work, we develop a universal computational tool that can automatically, efficiently, and accurately learn the symbolic changing patterns of complex system states by combining the excellent fitting ability from deep learning and the equation inference ability from pre-trained symbolic regression. We conduct intensive experimental verifications on more than ten representative scenarios from physics, biochemistry, ecology, epidemiology, etc. Results demonstrate the outstanding effectiveness and efficiency of our tool by comparing with the state-of-the-art symbolic regression techniques for network dynamics. The application to real-world systems including global epidemic transmission and pedestrian movements has verified its practical applicability. We believe that our tool can serve as a universal solution to dispel the fog of hidden mechanisms of changes in complex phenomena, advance toward interpretability, and inspire more scientific discoveries.","From the Book of Changes in ancient China to the dialectical thinking in the West, there exists a common philosophical thought that the only constant is change. Undoubtedly, scientists have been striving to discover the laws of changes in complex phenomena, attempting to explain, forecast, and regulate all things [1], such as emergence [2], chaos [3], synchronization [4], and critical phenomena [5]. As a widely accepted modeling, the changing patterns of states from complex systems are generally governed by a set of nonlinear differential equations [6] as \dot{X}(t)=f(X(t),A,t), where X(t)\in\mathbb{R}^{N\times d} is the system states at time t, N and d are the number of system components (nodes) and the state dimension, respectively. A represents the extra information beyond the system states, such as topological interactions among system components. As shown in the above formula, the dynamic behaviors exhibited by complex systems are primarily contingent upon the intricate interdependence between their internal interactions A and dynamics governing equations f [6, 7]. This prompts people to seek reliable methodologies to formulate dynamics models of these complex systems [8, 9]. However, a remarkable challenge arises in this pursuit. In theoretically complete physical systems, laws of changes are delineated by well-discovered foundational principles [10, 11, 12], such as the electromagnetic laws dictating the microscale exchanges among propelled particles. For the majority of complex systems, f is agnostic, and equivalent foundational rules remain incompletely elucidated, such as global epidemic outbreak [13], extreme climate anomalies [14], and extinction of biological populations [15]. Consequently, this vague development has limited the exploration of these complex fields. Fortunately, in the current era of data acquisition gradually becoming easier, the emergence of data-driven technologies has assisted in increasing the frequency with which human experts discover system change patterns [16, 17, 18, 19, 20]. They can provide domain experts with richer and more meaningful inspiration for various fields, accelerating the process of scientific discovery, such as mathematics [21, 22] and physics [23, 24]. Although much excellent work has been developed to reconstruct the symbolic models for low-variate dynamics of complex systems [25], e.g., bivariate shear flow equation [26], trivariate metapopulation epidemic model [27], and up to 9-variate Newton’s law of gravitation [11], inferring governing equations for high-variate network dynamics remains important and challenging. This is mainly because the number of nodes N in network dynamics is usually large, such as the epidemic spreading with transmission areas or individual numbers ranging from tens to billions [13], and d is sometimes multi-dimensional, resulting in too many free variables (N\times d) in the equations and topological interactions with exponential growth, thereby increasing the complexity of inferring symbolic models [15]. At present, several cutting-edge work is attempting to deal with the discovery of governing equations from network dynamics [28, 20]. Two-phase sparse symbolic regression (TPSINDy) [8] simply parameterizes f as a learnable linear combination of pre-defined orthogonal or non-orthogonal elementary function terms. Although its equation inference efficiency is high, the rationality of pre-defined function terms directly affects the inference results, so sufficient and correct domain expert knowledge is usually required [29]. Another group of methods of using graph neural networks (GNN) to parameterize f overcomes excessive expert knowledge [30]. Still, due to the use of genetic programming (GP) to parse neural networks into symbolic equations, it brings the high-cost evolutionary search efficiency issue [10, 14]. Therefore, how to effectively balance expert knowledge and computational costs, while ensuring high computational efficiency, introducing only a small amount or no expert knowledge, lowering the threshold for use, and efficiently discovering governing equations remains a gap. To address the challenges above, we develop a universal neural symbolic regression tool that can automatically, efficiently, and accurately learn the changing patterns of complex system states by combining the excellent fitting ability from deep learning and the equation inference ability from pre-trained symbolic regression. Our analysis of various complex network dynamics scenarios from physics, biochemistry, ecology, epidemiology, etc., indicates that our tool has outstanding effectiveness and efficiency. It can accurately and efficiently discover the governing equations of network dynamics, even in the face of noisy and topologically missing data, and has achieved excellent results in chaotic systems and real-world systems including global epidemic transmission and pedestrian movements. We believe that our tool can serve as a new and general solution to eliminate the fog of hidden mechanisms of changes in complex phenomena from broad fields."
https://arxiv.org/html/2411.06601v1,OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control,"Efficient traffic signal control (TSC) is essential for modern urban mobility, but traditional systems often struggle to adapt to the complex and dynamic nature of city traffic. While Multi-Agent Reinforcement Learning (MARL) offers promising adaptive solutions, online MARL methods require a significant amount of interactions with the environment, which can be expensive and time consuming. Offline MARL addresses these concerns by leveraging historical traffic data for training, but it faces challenges due to the heterogeneity of behavior policies in real-world datasets—a mix of different controllers makes learning difficult. We introduce OffLight, a novel offline MARL framework specifically designed to handle heterogeneous behavior policies within TSC datasets. OffLight employs Gaussian Mixture Model Variational Graph Autoencoder (GMM-VGAEs) to model the complex distribution of behavior policies, enabling effective learning from diverse data sources. To enhance coordination between agents, we integrate Graph Attention Networks (GATs), allowing agents to make informed decisions based on aggregated information from neighboring intersections. Furthermore, OffLight incorporates Importance Sampling (IS) to correct for differences between the behavior and target policies and utilizes Return-Based Prioritized Sampling (RBPS) to focus on high-quality experiences, thereby improving sample efficiency. Extensive experiments across three real-world urban traffic scenarios—Jinan (12 intersections), Hangzhou (16 intersections), and Manhattan (196 intersections)—demonstrate that OffLight significantly outperforms existing offline RL methods. Notably, OffLight achieves up to a 7.8% reduction in average travel time and an 11.2% decrease in queue length compared to baseline algorithms, particularly in datasets with mixed-quality data. Our ablation studies confirm the effectiveness of OffLight’s components in handling data heterogeneity and enhancing learning performance. These results highlight OffLight’s ability to accurately model heterogeneous behavior policies, mitigate the impact of suboptimal data, and scale to large urban networks. By addressing the critical challenges of offline MARL in TSC, OffLight offers a practical and impactful solution for improving urban traffic management without the risks associated with online learning.","Efficient traffic signal control (TSC) is important for modern urban mobility, directly affecting congestion levels, travel times, and overall city livability. As urban populations grow and vehicular usage intensifies, traditional traffic management systems struggle to adapt to dynamic and complex traffic flows. Recent advancements in Multi-Agent Reinforcement Learning (MARL) offer promising solutions by enabling decentralized, adaptive, and intelligent control of traffic signals [1, 2]. Figure 1: General Offline MARL Framework for Traffic Signal Control While online MARL requires agents to interact with the environment in real-time—which can be impractical and unsafe in real-world traffic systems—offline MARL leverages historically collected traffic data to train agents without live experimentation [3]. This approach offers several advantages: • Extensive Data Utilization: Utilizes pre-collected traffic data, allowing agents to learn from diverse scenarios and rare events difficult to replicate online [4]. • Safety and Risk Mitigation: Eliminates risks associated with deploying untested policies in live traffic, ensuring only well-trained agents are implemented [5]. • Cost Efficiency and Scalability: Reduces the need for expensive simulations or field tests, facilitating deployment across various scales and environments [3]. • Accelerated Policy Development: Enables rapid iteration and refinement of control policies, expediting research and development [6]. Several offline reinforcement learning (RL) methods have been explored for TSC, often incorporating imitation learning to bootstrap training. For instance, DemoLight uses demonstrations from traditional controllers to initialize policies, enabling efficient exploration and faster convergence compared to pure RL approaches [7]. Cooperative Control for multi-intersection TSC combines imitation learning and deep RL to coordinate multiple intersections, highlighting the benefits of learning from expert demonstrations [8]. CrossLight begins with offline training using behavior cloning and transitions to online RL to adapt policies to new environments, showcasing the effectiveness of imitation learning for strong policy initialization [9]. Building upon these efforts, DataLight introduces an input space modeling technique that captures the spatial distribution of traffic using self-attention mechanisms, demonstrating improved policy performance without requiring live interactions [4]. Figure 1 represents a general framework of an offline RL algorithm. However, a key challenge in offline MARL for TSC remains unaddressed: real-world datasets often consist of a heterogeneous mix of behavior policies, making it difficult for agents to learn effective strategies. This heterogeneity can arise from a variety of factors, such as: • Local Heuristics: Many traffic signals operate on traditional rule-based controllers (e.g., fixed-time or reactive control). These heuristics are often suboptimal and vary between intersections [10]. • Varying Control Strategies: Over time, traffic systems may switch between different controllers (e.g., from manual control to AI-based systems), resulting in datasets that reflect a blend of expert and suboptimal policies [11]. • Temporal Variability: Traffic patterns fluctuate throughout the day, leading to variations in the policies being implemented during different time periods. For instance, a traffic signal might switch between morning peak-time strategies and off-peak strategies [10]. This heterogeneity hampers effective learning because: • Offline MARL methods are highly sensitive to the underlying distribution of policies in the dataset. Mixing high- and low-quality data complicates the learning process, leading to poor generalization [12]. • Mixed-policy data can introduce biases, making it harder for the agent to identify the most effective policies [13]. To address these challenges, we introduce OffLight, a novel offline MARL framework that combines Importance Sampling (IS) and Return-Based Prioritized Sampling (RBPS) to mitigate distributional shifts and focus on high-value experiences. This synergy between IS and RBPS enhances both sample efficiency and policy performance. GMM-VGAE models the diversity in behavior policies within the traffic network, capturing policy heterogeneity by disentangling them in the latent space. By leveraging Graph Neural Networks (GNNs), OffLight combines local observations across intersections into a structured global representation, which is crucial in traffic signal control where agent interactions are highly localized but collectively influence network-wide patterns. This enables OffLight to use IS effectively, correcting for distributional shifts and ensuring stable policy learning from varied data. Meanwhile, RBPS prioritizes high-return episodes, allowing OffLight to focus on the most informative experiences, thus accelerating convergence and optimizing performance. Key contributions of this paper include: • Developing OffLight, an offline MARL framework designed to handle heterogeneous behavior policies in traffic signal control (TSC) using a combination of Importance Sampling (IS) and Return-Based Prioritized Sampling (RBPS). • Introducing GMM-VGAE to accurately capture diverse behavior policies, supporting robust learning under policy heterogeneity and enabling effective IS. • Leveraging RBPS to prioritize high-return episodes, which enhances sample efficiency and accelerates convergence by focusing on the most informative experiences. • Demonstrating OffLight’s scalability and superior performance on real-world traffic datasets, where it consistently outperforms existing methods. The remainder of the paper is organized as follows. Section 2 reviews related work and provides necessary background on MARL and offline RL in the context of TSC. Section 3 details the OffLight framework, including its architecture and key components. Section 4 describes the experimental setup, and Section 5 presents the results and analysis. Finally, Section 6 concludes the paper and Section LABEL:sec:discussion discusses the key insights gained from this study and limitations of OffLight."
https://arxiv.org/html/2411.06121v1,SniffySquad: Patchiness-Aware Gas Source Localization with Multi-Robot Collaboration,"Gas source localization is pivotal for the rapid mitigation of gas leakage disasters, where mobile robots emerge as a promising solution. However, existing methods predominantly schedule robots’ movements based on reactive stimuli or simplified gas plume models. These approaches typically excel in idealized, simulated environments but fall short in real-world gas environments characterized by their patchy distribution. In this work, we introduce SniffySquad, a multi-robot olfaction-based system designed to address the inherent patchiness in gas source localization. SniffySquad incorporates a patchiness-aware active sensing approach that enhances the quality of data collection and estimation. Moreover, it features an innovative collaborative role adaptation strategy to boost the efficiency of source-seeking endeavors. Extensive evaluations demonstrate that our system achieves an increase in the success rate by 20\%+ and an improvement in path efficiency by 30\%+, outperforming state-of-the-art gas source localization solutions.","Rapid and accurate responses to gas leak incidents are essential for safeguarding human and environmental health, as leaked gases can rapidly create highly flammable or toxic conditions, posing significant risks of explosions and poisoning [1, 2, 3]. For instance, in the United States alone, 2,600 gas leakage incidents have been reported, with 328 resulting in explosions and 122 fatalities [4]. A key aspect of quick response requires localizing the gas source, which involves analyzing the concentration and distribution of the gas in the air to trace it back to its origin. With the knowledge of source locations, subsequent mitigation operations, such as shutting off valves or sealing the leaks, can be conducted more logically, efficiently, and safely [5]. Conventional gas source localization (GSL) solutions fall into two categories: (i) human expert-based solutions assign human operators to engage in affected areas. These laborious and perilous activities not only increase the risk of misinterpretation but also imperil the safety of the operators [6]; and (ii) wireless sensor network (WSN)-based methods utilize preinstalled static sensors to detect the gas source by monitoring gas concentration readings [7]. However, this approach is constrained by spatial resolution limitations, particularly in environments with extensive and intricate pipeline networks that present numerous potential leakage points. In this work, we aim to devise an effective strategy for collaboratively scheduling multiple olfactory robots to localize the gas source in real-world environments. Particularly, we utilize multiple robot’s activeness and collaborative capability to gather more information efficiently, enhancing the synergy between their sensing and scheduling abilities. Figure 1: Mobile olfactory robots autonomously search for and navigate towards the source of gas leakage. We take gas leakage in a factory with intricate pipelines as an example scenario, as shown in Fig. 1. Here, a fleet of robots equipped with gas concentration sensors and wind sensors are dispatched to search the surroundings and gather gas sensory data. By analyzing and interpreting the environmental conditions, the robots autonomously navigate to pinpoint the source of the gas leakage. However, translating this idea into a practical system is non-trivial and faces two challenges: \bullet The patchy nature of gas plumes confuses and traps the robots. The gas landscape is characterized by a patchy structure [8, 9], as evidenced by field measurements of gas concentration (see Fig. 2). As seen, gas plumes fragment into disjointed patches and form distinct and scattered areas. From the perspective of source-seeking robots, the concentration measured along the horizontal centerline exhibits pronounced fluctuations as the distance to the source increases, since gas patches are separated by regions where gas concentration is below detectable levels. This intermittent characteristic introduces uncertainty in determining the source direction when robots devise paths using noisy and local sensory data, ultimately leading to their entrapment within these patches and failure to complete GSL. Existing solutions, including bio-inspired [10, 11] and probabilistic model-based approaches [12, 13], either rely on local gas concentration gradient or oversimplified gas dispersion models, rendering them effective only in idealized scenarios with continuous gas concentration fields. \bullet Trade-off between source localization effectiveness and search efficiency for collaborative robots. Since gas distribution varies continuously over space and time, it is impossible to measure at every possible location all the time. Therefore, robots must strike a balance between two conflicting objectives within limited sampling: (i) identifying new potential gas source positions and (ii) excluding false positive source positions. Achieving these goals simultaneously complicates the localization of the true gas source efficiently and effectively. Specifically, the former goal necessitates extensive exploration and sampling of gas data, while the latter goal requires utilizing previously acquired information to reach and verify the estimated source position’s probability. Current multi-robot GSL solutions tend to exhibit either clustered or dispersed collaborative behaviors, thereby undermining the simultaneous achievement of both objectives [14, 11, 15]. Remark. As far as we are aware, previous works develop their operating principles ignoring the patchy nature of gas plumes, based on which they schedule agents without fully harnessing their versatility and collaborative capabilities at a system level. To tackle the above challenge, we design and implement SniffySquad, a collaborative olfactory robot scheduling system for gas source localization. Benefiting from SniffySquad, a team of olfactory robots can adapt to field patchiness and dynamically adjust their movements accordingly, enabling efficient and effective emission source localization. In general, SniffySquad excels in the following two aspects. \bullet At the individual level, we propose a Patchiness-aware active sensing method, which refines the probabilistic source estimation by incorporating the patchy characteristic of gas in each robot’s moving strategy. Inspired by the principles of Langevin MCMC [16], we adjust robots’ gradient-based moving direction with a regulation term, allowing them to escape false positive source positions and thereby collect more informative sensing data. \bullet At the team level, we design a Potential-instructed collaborative roles adaptation strategy that further enhances the source-seeking efficiency by adjusting robots’ roles based on their spatial distribution and measurements in the past. Each robot in the team adopts either a fine-grained search role to inspect spurious signals (i.e., exploiter) or a coarse-grained search role to discover potential new sources (i.e., explorer). These roles are adaptively adjusted based on the estimated probabilities of each robot’s proximity to the emission source, thereby parallelizing the exploration of the environment and exploitation of collected information in a flexible manner. Figure 2: Spatial characteristics of gas concentration. We conducted a proof-of-concept experiment to check gas characteristics in our indoor testbed, with a gas emission device at (x,y)=(1.0,0.0) and winds blowing along the x axis. The heatmap illustrates concentration values, representing the number of particles with a diameter >0.3um in 0.1L of air. The observed gas plume patches may mislead source-seeking robots into falsely identifying them as the actual gas emission source. We evaluate the performance of SniffySquad and compare it with state-of-the-art baseline methods through experiments on a real-time multi-robot testbed (12 hours) and extensive physical feature-based simulations (750 runs). Experiment results show that our system outperforms all baselines, achieving a 20\%+ success rate improvement and improving path efficiency by >30\%. We summarize the contributions of this paper as follows: • We propose SniffySquad, a collaborative multi-robot olfactory sensing system for accurate and efficient gas source localization, specifically designed to handle the patchy characteristics in real-world gas fields. • We introduce a patchiness-aware active sensing method that refines probabilistic source estimation by incorporating gas patchiness in each robot’s moving strategy. Building on this design, we further devise a potential-instructed collaborative roles adaptation strategy, which parallelizes the exploration of the environment and exploitation of collected information in an adaptive manner. • We develop a prototype system and evaluate SniffySquad through a real-world testbed and a physical-feature-based gas dispersion simulator. Extensive evaluation results show its effectiveness and superior performance. The remainder of the paper is organized as follows. We first introduce the background, motivation, and related works in Section II. In Section III, we present preliminaries, followed by the system overview in Section IV. In Sections V and VI, we introduce the algorithm design, involving detailed descriptions of the patchiness-aware active sensing and potential-instructed collaborative roles adaptation. Section VII showcases the implementation and evaluation. Finally, Section VIII concludes the paper."
https://arxiv.org/html/2411.05990v2,"Game-theoretic LLM:
Agent Workflow for Negotiation Games","This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models’ ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.Our research contributes to a deeper understanding of LLMs’ decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.","Large Language Models (LLMs), such as GPT-4 and Claude, have achieved remarkable progress in natural language understanding and generation zhang2024supervised ; ding2024hybrid ; fang2024large , driving advancements in fields ranging from conversational AI dam2024complete ; dong2023towards to content creation liang2024monitoring ; shao2024assisting and agentic task delegation guo2024embodied ; agashe2023evaluating ; xi2023rise . LLMs are increasingly integrated into applications that influence everyday activities, such as planning, acting, and decision-making. Therefore, the ability of LLMs to navigate complex situations has significant implications for their deployment in applications requiring strategic interaction, such as automated negotiations, economic modeling, and collaborative problem-solving bianchi2024well ; horton2023large ; li2024econagent ; chen2024comm ; li2023metaagents . Despite the wide exploration and utilization, LLM’s capacity for rational behavior, particularly in strategic settings represented by game theory, remains an open question leng2023llm ; stade2024large ; wu2024shall ; de2023emergent ; lan2023llm . In this context, rationality implies an agent’s ability to make decisions that maximize expected utility based on available information, an essential component of intelligent and adaptive decision-making. In the realm of game theory, rational agents are expected to act strategically, considering not only their own preferences but also the potential actions and preferences of others. This is especially critical in incomplete-information games, where uncertainty about other players’ information necessitates sophisticated reasoning and belief updating. This paper investigates the capacity of LLMs to behave rationally in game-theoretic scenarios and explores methodologies to enhance their rational decision-making capabilities. We begin by assessing the performance of several state-of-the-art LLMs, including Claude-3.5 Sonnet, Claude-3 Opus, GPT-4o and o1 zhong2024evaluation , in both complete-information and incomplete-information games such as the Prisoner’s Dilemma, Battle of the Sexes, the Escalation Game, and Deal-or-No-Deal lewis2017deal , presented in Figure 1. Our analysis reveals LLMs often deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees (Section 4). They also exhibit a lack of robustness to noise and uncertainty, leading to suboptimal outcomes (Section 6). To address these limitations, we introduce a novel approach by proposing game-theory-inspired workflows specifically designed to guide the reasoning and decision-making processes of LLMs. This is the first attempt to systematically integrate classic game-theoretic strategies into LLM-based agent workflow, aiming to enhance their rational behavior and decision-making capabilities in strategic settings. These workflows incorporate principles such as Dominant Strategy Search, which involves identifying strategies that yield the highest payoff regardless of the opponent’s actions; Backward Induction, a method of solving extensive-form games by analyzing them from the end states backward to the initial decision nodes to determine optimal strategies; and Bayesian belief updating, which allows agents to refine their beliefs about other players’ valuations based on observed actions and signals during the game. Cringed on these well-defined and well-studied game-theoretic methods, we design algorithms to guide the behavior and thinking process of LLM-based agents. Additionally, we integrate fairness considerations like envy freeness and pareto optimality, which promote equitable and efficient outcomes in negotiations by ensuring that no agent prefers another agent’s allocation to their own and that no improvements can be made without making at least one agent worse off. Contribution Summary • Comprehensive Evaluation of LLMs in Strategic Games and Identification of Rationality Limitations in LLMs (Section 4 and 6): Through empirical analysis, we uncover that LLMs often fail to behave rationally in strategic settings, exhibiting a lack of robustness to noise and randomness. • Design of Game-Theory-Inspired Workflows (Section 4.4 and 5.2): We develop novel workflows inspired by game-theoretic concepts to guide the reasoning and decision-making processes of LLMs, incorporating analysis and algorithms from classic game theory. • Emerging Research Direction (Section 5.5.3 and 5.6): Through the application of workflows, we identify a promising new research direction in meta-strategy, specifically focusing on the decision of whether to adopt a workflow and, potentially, which workflow to employ in varying scenarios. Figure 1: Game-theoretic Landscape Investigated in this Paper."
https://arxiv.org/html/2411.05945v1,": Toward Post Recognition Generative Correction 
Large Language Models with Task-Oriented Experts","Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an “expert” of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset’s tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.","Figure 1: Proposed NeKo, a new form multi-task model to boost post-recognition results over speech, text, and visual inputs. NeKo could work for (i) post automatic speech recognition (ASR) correction, (ii) post speech translation (ST) and machine translation (MT) correction, and (iii) post optical character recognition (OCR) correction. NeKo discover new state-of-the-art results in (iv) zero-shot ASR correction and performs competitively as a general-purpose (v) multi-task corrector. Human recognition (Biederman, 1987; Juang and Furui, 2000; Kanwisher et al., 1996) capabilities span multiple modalities, including speech recognition, visual patterns, and extensions to semantic and textual interpretations. These faculties, however, are not infallible and often incorporate mis-recognition errors. Despite these imperfections, humans efficiently communicate using speech, language, or facial expressions. For instance, two non-native speakers (Lev-Ari, 2015; Valaki et al., 2004) can often achieve mutual understanding through this imperfect recognition and subsequent interpretative processes, even when the conversation is marred by lexical inaccuracies and subdued accents. In other words, humans (as intelligent agents) exhibit a robust capacity for generative understanding Jiang et al. (2020); Cheng et al. (2021) that extends beyond initial recognition results. In neuroscience Zatorre and Gandour (2008), the inferior temporal gyrus and the temporal lobe are not confined to rudimentary perception but are also integral to the post-recognition processes that facilitate semantic understanding of language (Levinson and Evans, 2010), speech (Marshall et al., 2015), and visual patterns (Vink et al., 2020). This form of “post-recognition correction,” exemplified by the application of language modeling (LM) to initial recognition outputs, has been introduced to the field for both acoustic (automatic speech recognition, ASR) and visual (optical character recognition, OCR) modalities since the early explorations (Jelinek, 1976; Dixon and Silverman, 1975) of learning algorithms in 1970s. The most prevalent approaches to utilizing LMs for post-recognition boosting are predominantly ranking or retrieval-based. In these setups, the LM is tasked with ranking and scoring (Ljolje et al., 1999) the top n-best hypotheses generated by the first-pass recognition system. This process often incorporates “discriminative modeling” algorithms (Sukkar and Lee, 1996; Mangu et al., 2000) and representation embeddings, such as BERT (Salazar et al., 2020; Kenton and Toutanova, 2019), to minimize the word error rate (WER) during training (Prabhavalkar et al., 2018; Mangu et al., 2000). With the LMs scaling up to LLMs (Brown et al., 2020), recent efforts (Chan et al., 2023; Yang et al., 2023; CHEN et al., 2023; Hu et al., 2024a) have focused on exploring a “generative modeling” for post-recognition correction. This generative error correction (GER) approach uses LLMs to conduct final recognition from given first-pass text-based predictions from recognition models, including ASR, image captioning (IC), and machine translation (MT). This cascaded two-agents text-to-text GER model has outperformed larger single multi-modal and multi-task models in these tasks. Meanwhile, these GER solutions heavily depends on domain-specific fine-tuning processes (Chen et al., 2024a) that utilize parameter-efficient components, which often suffers a performance degradation from a lack of generalizability across different datasets, domains, and tasks. In other words, how to design and further openly provide a “general-purpose post (every) recognition correction model” is still one undiscovered and crucial topic within the research community. On the other hand, directly fine-tuning LLMs on a mixture of diverse error correction datasets can lead to suboptimal performance (CHEN et al., 2023; Lange et al., 2022) due to differences in input modalities, output formats, error types, and domain characteristics. For example, ASR errors stem from phonetic similarities or acoustic ambiguities, while OCR errors involve visual or character-level confusions. Additionally, error distributions can vary widely across datasets, even within the same task. To characterize “model generalization,” mixture-of-experts (MoE) (Jiang et al., 2024a) has emerged as a promising approach for multi-task learning, consisting of of a set of expert networks and a gating network that learns to route the input to the most appropriate expert(Sukhbaatar et al., 2024). This enables MoE models to learn more specialized and fine-grained representations compared to monolithic models. However, most MoE models are designed for general-purpose language modeling(Dai et al., 2024), with experts not explicitly assigned to specific tasks, but rather learn to specialize in different aspects of the input space through data-driven training, expect for a recent vision work (Ye and Xu, 2023). Effectively leverage MoE for multi-task error correction, where the experts need to capture task-specific features while allowing knowledge sharing, remains an open question. In this work, we propose NeKo, a “geNerative multi-tasK error correction” approach that leverages a pre-trained MoE model to drive diverse tasks and cross-domain knowledge, as shown in Figure 1. The key idea is to continuously pre-train MoE model on a mixture of error correction datasets, with each expert specializing in a specific domain. This task-oriented MoE fine-tuning approach enables the experts to capture task-specific features while allowing knowledge sharing through the router (Dai et al., 2024). NeKo captures the nuances of each task, benefiting from shared knowledge across experts. Evaluated on tasks such as ASR, ST, OCR, and unseen textual error correction (TEC), NeKo consistently outperforms baseline models, including Claude-Opus and GPT-3.5. It achieves state-of-the-art WER reduction on the Hyporadise benchmark and large-scale Open ASR Leaderboard (Srivastav et al., 2023). NeKo also significant improves in OCR error correction. Further analysis confirms its robust multi-task capabilities. In summary, the main contributions of this work include: 1. We introduce NeKo, a multi-task error correction LLM that leverages task-oriented mixture-of-experts for diverse post-recognition correction tasks. To the best of our knowledge, this is the first work that explores the use of MoE for multi-task error correction. 2. NeKo has been studied under a new form of cross-modalities post-recognition correction evaluation, serving as strong open-source ASR, ST, OCR, and TEC baselines. Our results show that NeKo discovers new state-of-the-art performance in ASR as a multi-task error correction model. 3. We discovered emergent abilities for cross-task correction from NeKo as a first-of-its-kind multi-task correction approach toward a general-purpose post-recognition LM designs. 4. The NeKo models, newly created source datasets, and training processes are scheduled to open source under the CC BY-SA 4.0 license to support reproducibility and to encourage future research."
https://arxiv.org/html/2411.05828v1,"AI Multi-Agent Interoperability
 Extension for Managing Multiparty Conversations","This paper presents a novel extension to the existing Multi-Agent Interoperability specifications of the Open Voice Interoperability Initiative (originally also known as OVON from the Open Voice Network). This extension enables AI agents developed with different technologies to communicate using a universal, natural language-based API or NLP-based standard APIs. Focusing on the management of multiparty AI conversations, this work introduces new concepts such as the Convener Agent, Floor-Shared Conversational Space, Floor Manager, Multi-Conversant Support, and mechanisms for handling Interruptions and Uninvited Agents. Additionally, it explores the Convener’s role as a message relay and controller of participant interactions, enhancing both scalability and security. These advancements are crucial for ensuring smooth, efficient, and secure interactions in scenarios where multiple AI agents need to collaborate, debate, or contribute to a discussion. The paper elaborates on these concepts and provides practical examples, illustrating their implementation within the conversation envelope structure.","1.1 Previous work There is increasing recognition that many applications of conversational systems can be addressed more successfully if the expertise required to perform the application is not expected to reside in one agent, but is allocated among independent agents with their own capabilities. In this approach, these independent agents contribute their individual knowledge to address specific aspects of the problem and communicate to provide their input toward achieving the overall goal. It is clear that this perspective broadly resembles the traditional and very successful object-oriented programming paradigm and shares similar advantages, such as encapsulation, maintainability, and scalability. The integration of multiple agents has been addressed in many approaches, which vary in the degree of autonomy of each agent, the degree of architectural similarity expected of the agents, and whether or not the agents have to be known ahead of time when the system is developed. We believe that the most useful systems will be those that maximally encapsulate the agents’ functionality, limit dependencies on specific semantic formats, and which can be configured dynamically at runtime. In addition, multi-agent systems that are based on proprietary frameworks are, by definition, constrained to coordinating agents that are based on those frameworks, automatically ruling out the possibility for the thousands of existing legacy conversational systems to participate. We focus here on previous work aimed at coordinating full agents as opposed to work aimed at coordinating specific modality components, such as the W3C Multimodal Architecture [12] and the Galaxy Communicator Software Infrastructure [8]. 1.1.1 Early systems Knowledge Query Manipulation Language (KQML) [13] was an early system that focused on knowledge sharing and communication among intelligent agents. While it supported cooperation among intelligent agents in multi-agent systems, it relied on shared ontological assumptions among agents, which created a barrier to deployment. In addition, KQML did not specifically address conversational interactions and was more focused on sharing knowledge among agents. VoiceXML [11] was another early approach to collaboration among agents. It allowed conversations to be passed to other Voice-XML based agents through the <transfer> element. However, VoiceXML required agents to be based on VoiceXML, conversations could only be transferred to a single receiving agent, and no previous conversational context could be passed to the receiving agent. Systems like the Open Agent Architecture [6] used Inter-Agent Communication Languages to facilitate collaboration among independent agents. While this reduced dependencies on specific internal architectures, it required agents to interpret highly structured semantic representations rather than natural language, which constrained the flexibility and scalability of the system. 1.1.2 Recent systems The emergence of very capable LLMs has led to a dramatic increase in both research and deployed conversational systems. Along with this increase, the value of multi-agent systems is becoming more apparent, and a number of new multi-agent frameworks have recently become available. Some examples include the following: • OpenAI Swarm [20]: A framework designed to orchestrate multiple AI agents collaboratively to accomplish complex tasks. Released with Open Source MIT License, it is currently considered to be an experimental system that is not ready for deployment. In particular, Swarm is currently an experimental sample framework intended to explore ergonomic interfaces for multi-agent systems. It is not intended to be used in production, and therefore has no official support. • Microsoft Autogen: An Open-Source Programming Framework for Agentic AI. AutoGen is powered by collaborative research studies from Microsoft, Penn State University, and University of Washington and licensed under the Creative Commons Attribution 4.0 International [3]. • CrewAI: A platform that orchestrates multiple AI models and services to perform cohesive workflows. It emphasizes flexibility in integrating diverse AI technologies and managing complex task sequences. [7]. • Multi-Agent Orchestrator framework: a tool by Amazon AWS Labs for implementing sophisticated AI systems comprising multiple specialized agents. Its primary purpose is to intelligently route user queries to the most appropriate agents while maintaining contextual awareness throughout interactions [5]. The project is Open Source and provides optimal integration and performance combined with the Amazon AWS Bedrock fully managed service [4]. • Mixture of Experts (MoE) Paradigm [22]: While not an architecture for multi-agent systems per se, the Mixture of Experts paradigm represents another example of an architecture where a larger system relies on encapsulated individual components that supply complementary expertise with respect to an overall problem. We will see in the following sections that, in fact, these multi-agent frameworks could be compatible with the framework presented in this paper. By providing any of them with an OVON wrapper, they could become interoperable with other OVON-compatible systems. For instance, an Amazon Bedrock service with multiple internal agents could externally present itself as a black box, communicating with other multi-agent frameworks (e.g., Microsoft Autogen, CrewAI, or other platforms not listed here) by leveraging the OVON Universal API specifications. 1.2 The Initial OVON Framework In contrast to other approaches, the OVON (Open Voice Network) framework introduced in our previous work[2] sought to overcome some interoperability limitations by establishing a highly scalable and flexible method for AI agent interoperability111For the remainder of this document, the term ”agent” will be used to refer to an entity with the capacity to act, while ”agency” or “agentic” will denote the exercise or manifestation of this capacity, in accordance with the definition provided by Markus Schlosser[21].. Our framework supports a wide range of independent assistants, regardless of their underlying technologies, enabling them to collaborate through minimal communication standards. This loose coupling dramatically reduces the complexity of integrating new assistants into the ecosystem, thereby enhancing scalability. 1.3 Enhancements to the OVON Framework However, this initial work covers only conversations between one user and one assistant at a time. That is, if the user wants to get information from more than one assistant, they have to access multiple assistants in sequence. This most likely will have two less-than-optimal consequences. In the first place, any information from the conversation with the first assistant that is required by the second assistant will have to be explicitly transferred to the second assistant when the second assistant is invited to the conversation. The second and more significant drawback is that any higher-level conclusions resulting from the various conversations will have to be determined by the user. That is, since the assistants don’t know about the other tasks, they won’t be able to make suggestions that combine information gathered from other assistants with their own information. 1.4 Use Cases Let’s look at an example use case for managing multi-party conversations via a multi-agent AI. Suppose a user is planning a trip that involves booking a flight, a rental car, and a hotel, and also involves looking for interesting things to do in the destination city. This planning could involve conversations with four or more assistants at different travel services. The travel dates, which all of these assistants need, have to be passed to each assistant in turn to avoid making the user repeat them. In addition, if the assistants are talking together, the tourist information assistant could point out that there is a music festival that the user would enjoy, but attending it would require extending the trip by one day. If the tourist assistant is involved in the flight booking conversation, it could tell the user about the festival before the user books their flight. This could save the user a lot of time. Figures 1 and 2 contrast these two situations. In Figure 1, the user is planning a trip by accessing three assistants with different expertise in sequence. Figure 1: One assistant at a time Here the user has to keep track of the tasks that she has to coordinate to plan her trip–flight, hotel, and rental car, and discusses her plans separately with each assistant, repeating some of the trip details in each conversation. This is the way that most planning with multiple assistants currently works. In contrast, Figure 2 shows multiple conversational agents gathered in a shared conversational space, referred to as the Floor, brought together by a Convener, with conversational turn-taking managed by the Convener itself. These concepts will be discussed more fully in the following sections of the paper. Figure 2: Multiple assistants in the same conversation A similar use case is described in [9], where several agents are jointly assigned the task of allocating beds to hospital patients.The agents (nurse proxy, bed allocation specialist and patient database, among others) all have widely differing knowledge that makes it impractical to combine the agents into a single expert. Each agent has its own knowledge which it brings to the discussion of how to allocate a bed to a specific patient, arguing why or why not a particular bed is suitable for that patient. It would be very cumbersome if the user had to consult each agent in sequence to perform this task. Many other AI healthcare-specific applications could benefit from having conversational AI multi-agents coordinate with each other to enhance awareness of patient situations, including, for example, this risk detection model[17] for assisting vulnerable people. 1.5 Requirements and proposed extensions For these reasons, we propose to extend the earlier two-party conversational specifications[2] to handle requirements for conversations involving multiple assistants. Multi-party dialog systems have been discussed in the literature, for example [9][18] among others. [18] describes a multi-agent system with user-initiative, where several agents can be present but the agents don’t collaborate – they simply respond individually to user questions. [9] describes a system for collaborative problem-solving among agents, but it is restricted to one domain in that all of the agents are experts in different aspects of a larger problem. Our goal is to be able to support mixed-initiative applications with multiple agents that collaborate across domains. These are the requirements that we propose for support of multi-party conversations: 1. It must be possible to hold a conversation among more than two conversants. 2. Conversants must be able to come and go during a conversation. 3. It should be possible for a subset of conversants to be able to hold private conversations among themselves. 4. There should be no fixed limit on the number of conversants. 5. There should be a way to control possible unruly conversants through techniques like muting or ejecting. Requirement 1 is the key requirement for support of multi-party conversations. The other requirements support it. This paper extends the initial specifications by introducing key concepts that address the specific requirements of managing multiparty conversations within the context of AI-driven multiparty conferences. The new concepts introduced in this work—such as the Floor, and related Multi-Conversant Support, Convener Agent, and mechanisms for handling Interruptions and Uninvited Agents—are designed to ensure that AI agents can collaborate effectively in dynamic, multi-agent environments. These extensions not only enhance the framework’s ability to handle complex, multi-party interactions but also ensure that the system can scale to accommodate a growing number of agents and tasks. For instance, in scenarios where a human interacts with multiple AI assistants for various tasks—such as coordinating events, managing appointments, or retrieving information—the framework ensures effective communication and task delegation among the agents. This is achieved independently of each agent’s underlying technologies or models, showcasing the system’s ability to scale across different applications and user needs. Previous work[16] laid the foundation for AI agent interoperability, establishing the basic framework for seamless communication between independent conversational agents. However, the extensions presented in this paper are essential for overcoming the challenges associated with scalability and effective management in multiparty conversational settings. These enhancements introduce a versatile and adaptable platform that ensures AI-driven multiparty conferences can be conducted smoothly, with agents collaborating efficiently and effectively, regardless of their technological diversity. This approach not only addresses the current needs of evolving AI ecosystems but also provides a robust and future-proof solution capable of integrating new agents and capabilities as they emerge."
