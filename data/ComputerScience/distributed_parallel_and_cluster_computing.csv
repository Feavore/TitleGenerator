URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10290v1,"The ParClusterers Benchmark Suite (PCBS): 
A Fine-Grained Analysis of Scalable Graph Clustering 
[Experiment, Analysis & Benchmark]","We introduce the ParClusterers Benchmark Suite (PCBS)—a collection of highly scalable parallel graph clustering algorithms and benchmarking tools that streamline comparing different graph clustering algorithms and implementations. The benchmark includes clustering algorithms that target a wide range of modern clustering use cases, including community detection, classification, and dense subgraph mining. The benchmark toolkit makes it easy to run and evaluate multiple instances of different clustering algorithms, which can be useful for fine-tuning the performance of clustering on a given task, and for comparing different clustering algorithms based on different metrics of interest, including clustering quality and running time.Using PCBS, we evaluate a broad collection of real-world graph clustering datasets. Somewhat surprisingly, we find that the best quality results are obtained by algorithms that not included in many popular graph clustering toolkits. The PCBS provides a standardized way to evaluate and judge the quality-performance tradeoffs of the active research area of scalable graph clustering algorithms. We believe it will help enable fair, accurate, and nuanced evaluation of graph clustering algorithms in the future.","Clustering is a critical tool in almost any scientific field that involves classifying and organizing data today. Examples of fields leveraging clustering range from computational biology and phylogenetics to complex network analysis, machine learning, and astrophysics (Manning et al., 2008; Shalita et al., 2016; Camerra et al., 2014; Patwary et al., 2015). Clustering has proven particularly useful in fields transformed by AI and machine learning because of its utility in understanding and leveraging high-dimensional vector representations (embeddings) of data (Monath et al., 2021; Bateni et al., 2017; Douze et al., 2024; Dhulipala et al., 2021, 2022). In this paper, we are interested in carefully characterizing the behavior (e.g., measuring quality, running time, and scalability) of parallel clustering algorithms for shared-memory multi-core machines that are scalable in the size of the dataset and the number of threads. Our specific focus is on graph clustering, which is a versatile and scalable clustering approach that can be used with different input types. On one hand, graph clustering is a natural approach whenever the input is a graph (e.g., friendships, interactions, etc.). On the other hand, graph clustering can also be applied in the other popular scenario, when the input is a collection of points in a metric space (e.g., embeddings). In this case, one can obtain a graph by computing a weighted similarity graph, where continuous or complete phenomena can be cast into sparse similarity graphs, e.g., by keeping only edges between nearby points or only the most significant entries of a similarity matrix. Despite substantial prior works that study the quality (e.g., precision and recall) and scalability of individual graph clustering methods (Dhulipala et al., 2023, 2021, 2022; Shi et al., 2021; Staudt et al., 2016; Tsourakakis et al., 2017; Tseng et al., 2021a), no prior works have systematically compared a large collection of different graph clustering methods (and their corresponding implementations) to understand how different methods compare against each other under different metrics. For example, celebrated and widely-utilized graph clustering algorithms, such as modularity clustering are well understood to be highly effective in community detection tasks on unweighted natural graphs, but little is known about their performance for clustering on vector embedding clustering tasks. This paper addresses this gap by performing a systematic comparison of a large and diverse set of graph clustering methods. Our evaluation includes methods tailored to both weighted and unweighted graphs and incorporates a diverse set of natural graphs and similarity graphs derived from point sets. We focus on undirected graphs, as converting directed graphs to undirected graphs is a common practice in many graph tasks, such as community detection (see, e.g., (Fortunato, 2010)). We stratify our evaluation based on four unsupervised clustering tasks that are commonly found in the literature and in practice—(1) community detection, (2) vector embedding clustering, (3) dense subgraph partitioning, and (4) high resolution clustering. Due to insisting on scalability, we focus our evaluation on the most scalable parallel graph clustering methods currently available in the literature. To make our evaluation easily reusable and extensible by future researchers, we designed a graph clustering benchmark called the ParClusterers Benchmark Suite (PCBS). PCBS enables users to accurately measure the scalability and accuracy of different shared-memory parallel graph clustering algorithms. In addition to providing a simple and easy to use benchmarking platform, we have also incorporated eleven parallel graph clustering methods into PCBS. The algorithms include algorithms from our recent prior work, as well as several new implementations. In addition to classic graph clustering methods such as modularity-based clustering (Shi et al., 2021), structural clustering (Tseng et al., 2021b), and label propagation methods (Raghavan et al., 2007), we include recently developed hierarchical agglomerative graph clustering methods (Dhulipala et al., 2022) and connectivity-based methods such as k-core and low-diameter decomposition (Dhulipala et al., 2018). Finally, unlike much of the existing work on graph clustering, which typically focuses on optimizing a specific graph clustering metric (e.g., modularity or conductance) that a clustering method is usually designed to optimize, PCBS supports evaluating any clustering algorithm using a very broad set of metrics, which helps us understand what different clustering algorithms are able to optimize for on real-world datasets, and help inform users of the best clustering algorithm for a given metric. Besides PCBS’s clustering implementations, PCBS also supports running many clustering implementations in other graph clustering frameworks and systems such as NetworKit (Staudt et al., 2016), Neo4j (neo, [n.d.]), and TigerGraph (tig, [n.d.]). PCBS can also be easily extended to include new datasets, algorithms, and parameter search methods. The datasets we study include both widely-used graph datasets from the SNAP repository, as well as several new graph clustering datasets that we have generated from spatial and embedding datasets using a simple nearest-neighbor-based graph building process, and which we will open-source as part of this work. We also contribute a new graph dataset for clustering, which represent similarities between 1.2M short texts. As far as we know, this is the first large-scale graph clustering dataset that provides a large number of ground-truth clusters. Our datasets cover a wide range of scales and clustering tasks, including community detection, vector embedding clustering, and dense subgraph partitioning. Key Contributions. The key contributions of our work include: • A comprehensive library that implements eleven state-of-the-art scalable graph clustering algorithms, providing a unified codebase for researchers and practitioners. • A benchmarking toolkit that facilitates the systematic evaluation of graph clustering algorithms across diverse datasets, parameter settings, and experimental configurations, enabling rigorous and comprehensive comparative analyses. • A new large graph clustering dataset containing many ground-truth clusters. • The first extensive evaluation of parallel graph clustering algorithms, encompassing their runtime performance, clustering quality, and the trade-off between these two critical dimensions. We also compare our library against other existing libraries and graph databases. Key Results. Some of our key takeaways and findings of our study of graph clustering algorithms include: • Our clustering implementations in PCBS are very fast compared to other clustering implementation in state-of-the-art graph libraries and databases. While graph databases, such as Neo4j (neo, [n.d.]) and TigerGraph (tig, [n.d.]), provide a richer functionality, on different graph clustering implementations, they are slower. For example, on the LiveJournal graph from SNAP (Leskovec and Sosič, 2016), PCBS is on average 32.5x faster than Neo4j and 303x faster than TigerGraph. Compared with state-of-the-art parallel graph library NetworKit (Staudt et al., 2016), PCBS is on average 4.54x faster. We compute the average using the geometric mean. • Correlation clustering (Shi et al., 2021) obtains the highest quality on three out of four tasks. ParHAC (Dhulipala et al., 2022) obtains the best quality on the fourth task. We consider this finding surprising, given that these two methods are not included in many popular graph clustering frameworks. The best performing method commonly found in existing graph clustering packages is modularity clustering. However, we observe that on a vast majority of datasets, correlation clustering obtains strictly better quality. • Parallel affinity clustering obtains high quality on the vector embedding clustering task and is consistently faster than correlation clustering and ParHAC on large graphs. Our code and the full version of our paper can be found at https://github.com/ParAlg/ParClusterers."
https://arxiv.org/html/2411.10289v1,Clock Synchronization Is Almost Impossible with Bounded Memory,"We study the problem of clock synchronization in a networked system with arbitrary starts for all nodes. We consider a synchronous network of n nodes, where each node has a local clock that is an integer counter. Eventually, clocks must be all equal and increase by one in each round modulo some period P. The purpose of this paper is to study whether clock synchronization can be achieved with bounded memory, that is every node maintains a number of states that does not depend on the network size. In particular, we are interested in clock synchronization algorithms which work in dynamic networks, i.e., tolerate that communication links continuously fail and come-up.We first focus on self-stabilizing solutions for clock synchronization, and prove that there is no such algorithm that is bounded memory, even in the case of static networks. More precisely, we show a lower bound of n+1 states at each node required to achieve clock synchronization in static strongly connected networks with at most n nodes, and derive a lower bound of n-2 rounds on synchronization time, in the worst case. We then prove that, when the self-stabilizing requirement is removed, the impossibility of clock synchronization with bounded memory still holds in the dynamic setting: every solution for the clock synchronization problem in dynamic networks with at most n nodes requires each node to have \Omega(\log n) states.","In this paper, we study the following clock synchronization problem: In a system of n agents receiving common regular pulses, where each agent is equipped with a discrete clock, all these local clocks must eventually increase by one at each pulse and be equal, modulo some fixed integer P, despite arbitrary activation times or arbitrary initializations. This problem, also referred to as synchronous counting [13], corresponds to synchronization in phase, as opposed to synchronization in frequency in non-synchronous systems (e.g., see [27, 17, 18, 24]). Clock synchronization is a fundamental problem, both in engineered and natural systems: For engineered systems, it is often required that processors have a common notion of time (e.g., see the universal self-stabilizing algorithm in [6] or the very many distributed algorithms structured into synchronized phases, like the 2PC and 3PC protocols, or the consensus protocols in [16]). Clock synchronization also corresponds to an ubiquitous phenomenon in the natural world and finds numerous applications in physics and biology, e.g., the Kuramoto model for the synchronization of coupled oscillators [28], synchronous flashing fireflies, collective synchronization of pancreatic beta cells [22]. In such systems with massive amounts of cheap, bulk-produced hardware, or swarms of small mobile agents that are tightly constrained by the systems they run on, the resources available at each agent may be severely limited. In this context, bounded memory algorithms, i.e., algorithms where agents have a finite set of states that is independent of the number n of agents, are highly desirable. For example, this is typically the case for the agents in the Population Protocol model [2]. In bounded memory algorithms, agents de facto send only O(1)-bit messages. Moreover, since the number of agents is arbitrary, agents may have no identifier. The main goal of this work is to study whether the bounded memory limitation is crippling or not for clock synchronization. For that, we consider a classical abstract model that captures the common requirements of the various settings cited above. A networked system is composed of n identical agents that have deterministic behaviors. They operate in synchronous rounds and communicate by simple broadcast. In order to take into account link failure and link creation, communication links may vary over time. The basic connectivity assumption is of a finite diameter, i.e., any pair of agents can communicate, possibly indirectly, over a period of time that is uniformly bounded throughout the execution. Agents may use only local informations; in particular they are unaware of the structure of the network and of its diameter. Contribution. We answer the above question of whether the bounded memory condition is crippling or not for the clock synchronization problem in the affirmative in two cases. We first consider self-stabilizing solutions, i.e., tolerating arbitrary initializations. We show that there exists no such solutions for the clock synchronization problem using bounded memory, even in the case of static networks with permanent communication links. More precisely, we show that n+1 states at each node are required to achieve clock synchronization in the class of static strongly connected networks with at most n nodes. We also derive a lower bound of n-2 rounds on synchronization time, in the worst case. These lower bounds demonstrate that the self-stabilizing clock synchronization algorithms proposed in [7, 19, 10] when an upper bound on the number of agents is available, are all asymptotically optimal with regards to both runtime and memory requirements. Our second result extends this impossibility result to non-self-stabilizing solutions: we show that clock synchronization with bounded memory is impossible in the dynamic setting. Indeed, we prove that clock synchronization in dynamic networks of size at most n requires each agent to have \Omega(\log n) states. Related works. Clock synchronization has been extensively studied in different communication models, under different assumptions, and with multiple variants in the problem specification. The pioneer papers on this question [3, 21, 7, 1], which all proposed self-stabilizing solutions for static networks, assume that a bound on the network size or on the diameter is available. Only the synchronization algorithms in [6] and [10] for static and dynamic networks, respectively, dispenses with this assumption. Both algorithms are self-stabilizing and use finite but unbounded memory, in the sense that the number of states used by each agent in any given execution is finite but is not uniformly bounded with respect to the number of agents. There are also numerous works on clock synchronization in the context of Byzantine failures (e.g., see [14, 15, 25, 26]). The goal is to tolerate one third of Byzantine agents, which obviously increases runtime and memory requirements. The clock synchronization algorithms deployed for tolerating Byzantine failures typically assume a fully connected network and port labellings, which requires each agent to use at least \log n bits. Synchronization has also been studied in the model of Population Protocols [2]. As in our model, there is a finite but unbounded number of anonymous agents, with bounded memory. The difference lies in the interaction mode between agents: this is an asynchronous model, where a unique random pair of agents interact in each step. The synchronization task is totally different as it consists in simulating synchronous rounds (synchronization in frequency). Moreover, only stabilization with probability one or with high probability is required, while our algorithms have to stabilize in all executions. The same probabilistic weakening of problem specification is considered for other probabilistic communication models. In particular, in the PULL model [23], Bastide et al. [5] proposed a self-stabilizing algorithm that synchronizes P-periodic clocks in O(\log n) rounds with high probability, in the case P is a power of 2. This demonstrates how randomness in communications can help to circumvent our impossibility results and lower bounds for clock synchronization. The clock synchronization problem has also been studied in the round-based Beeping model [12], which is a very restrictive communication model that relies on carrier sensing: in each round, an agent can either listen or transmit to all its neighbors one beep. In this model, for synchronizing P-periodic clocks, Feldmann et al. [19] proposed a (non self-stabilizing) solution with 4P state agents. This contrasts but does not contradict our second impossibility result since their algorithm assumes static symmetric networks (that is, static bidirectional communication links) and the restricted model of diffusive starts, where a passive node becomes active upon the receipt of a message from an active incoming neighbor. In this context, they also proved lower bounds on the number of states and the runtime of self-stabilizing solutions which are of the same order than ours, namely n (instead of n+1) states at each node are required and synchronization takes at least n (instead of n-2) rounds in the class of static symmetric networks with at most n nodes. Our lower bounds hence show that relaxing communication constraints – in particular, allowing to send more than one bit per message – does not help to reduce memory requirements and runtime for clock synchronization."
https://arxiv.org/html/2411.10143v1,Cascaded Prediction and Asynchronous Execution of Iterative Algorithms on Heterogeneous Platforms,"Owing to the diverse scales and varying distributions of sparse matrices arising from practical problems, a multitude of choices are present in the design and implementation of sparse matrix-vector multiplication (SpMV). Researchers have proposed many machine learning-based optimization methods for SpMV. However, these efforts only support one area of sparse matrix format selection, SpMV algorithm selection, or parameter configuration, and rarely consider a large amount of time overhead associated with feature extraction, model inference, and compression format conversion. This paper introduces a machine learning-based cascaded prediction method for SpMV computations that spans various computing stages and hierarchies. Besides, an asynchronous and concurrent computing model has been designed and implemented for runtime model prediction and iterative algorithm solving on heterogeneous computing platforms. It not only offers comprehensive support for the iterative algorithm-solving process leveraging machine learning technology, but also effectively mitigates the preprocessing overheads. Experimental results demonstrate that the cascaded prediction introduced in this paper accelerates SpMV by 1.33x on average, and the iterative algorithm, enhanced by cascaded prediction and asynchronous execution, optimizes by 2.55x on average.","Conjugate gradient (CG) and global minimum residual (GMRES), both grounded in the Krylov subspace, are the mainstream iterative algorithms for solving large-scale sparse linear equations. These algorithms include sparse matrix-vector multiplication (SpMV), vector inner product, and other computational kernels. Among them, SpMV comprises the largest proportion of the overall running time of the two iterative algorithms [1], thereby garnering extensive attention from researchers over the past two decades [2]. Typically, sparse matrices from real-world engineering problems are rich in zeros, necessitating storage in a compressed format. The irregular distribution of non-zeros in sparse matrices has attracted different compression format designs. The considerable research effort is focused on designing new sparse matrix compression formats and their associated SpMV algorithms [2]. However, according to existing studies, no compression format has been found that consistently delivers good performance across all sparse matrices [3], and there is significant variation in the performance of different SpMV algorithms designed for the same sparse format. The most notable example is the implementation of SpMV in the CSR format. Algorithms based on CSR employ diverse strategies such as sparse matrix partitioning, data cache optimization, and load balancing, resulting in significant disparities in SpMV’s computational efficiency across different sparse matrices. Moreover, even within a single algorithm, issues of parameter setting arise, with optimal parameter configurations differing for various sparse matrices. For instance, Gao et al. [4] examined the thread configuration issue of the CSR-Vector algorithm [5, 6] in CUSP [7]. The SpMV using the optimal thread configuration achieved an average speedup of 3x and a maximum speedup of 20x compared with the SpMV with default settings. As machine learning (ML) technology becomes more widespread, researchers address these issues by gathering performance data and training machine learning models. These efforts can be classified mainly into three categories: the optimal compression format prediction [8][9][10][3][11], the optimal SpMV algorithm prediction [12][13][14], and the optimal parameter setting prediction [12][15][16][17][4]. Although these methods have improved SpMV performance to some extent, they still face the following challenges: (1) Current ML-based optimization efforts only cover the prediction of a single area within the compressed formats, SpMV algorithms, or parameter settings. The search space is so small that there is still a gap from the best SpMV performance. (2) The ML-based SpMV and iterative algorithm optimization introduce significant preprocessing overhead, which offsets the performance benefits from ML-based optimization. Addressing the aforementioned challenges, this paper introduces an ML-based cascade prediction approach for typical iterative algorithms like CG and GMRES, targeting heterogeneous platforms. This optimization spans three key areas: selecting sparse matrix compression formats, SpMV algorithms, and parameter configurations. Building on this, an asynchronous execution model is proposed, integrating model inference with iterative computation. This model conceals the costs associated with model inference and matrix preprocessing, fully leverages the heterogeneous computing resources of CPU-GPU, and improves the operational efficiency of iterative algorithms on these platforms. This paper’s main contributions include: • We propose a lightweight cascading prediction framework for ML-based SpMV to mitigate the suboptimal performance caused by single area prediction. • We propose an asynchronous execution model for iterative algorithms on heterogeneous platforms to hide the preprocessing overheads. • We take the GMRES algorithm as an example to prove the efficiency of our method using 22 sparse matrices."
https://arxiv.org/html/2411.10026v1,SoK: DAG-based Consensus Protocols,"This paper is a Systematization of Knowledge (SoK) on Directed Acyclic Graph (DAG)-based consensus protocols, analyzing their performance and trade-offs within the framework of consistency, availability, and partition tolerance inspired by the CAP theorem.We classify DAG-based consensus protocols into availability-focused and consistency-focused categories, exploring their design principles, core functionalities, and associated trade-offs. Furthermore, we examine key properties, attack vectors, and recent developments, providing insights into security, scalability, and fairness challenges. Finally, we identify research gaps and outline directions for advancing DAG-based consensus mechanisms.","Distributed Ledger Technology (DLT) has become fundamental in supporting secure and transparent transaction systems across decentralized networks. Maintaining an immutable and append-only ledger, DLT enables a trustless environment where participants can transact directly without intermediaries. This technology, particularly its application in cryptocurrencies like Bitcoin and Ethereum, has attracted wide interest due to its potential for enhanced transparency, operational efficiency, and decentralization. However, standard blockchain architectures face key performance limitations, including low throughput and high confirmation latency. Studies have further highlighted trade-offs in speed, security [1], and performance [2], as well as the inherent challenge in achieving decentralization, consistency, and scalability simultaneously (the DCS-satisfiability theorem) [3, 4]. In response, DAG-based DLTs have emerged, using alternative consensus structures to improve scalability and support more efficient consensus mechanisms. I-A What is a DAG-based Consensus Protocol Traditional blockchain protocols, such as Bitcoin’s Nakamoto consensus [5], arrange blocks of transactions sequentially in a chain, where each block references its predecessor, extending back to the genesis block. Consensus in these systems often proceeds in rounds, with new blocks added to the longest chain, which participants recognize as the valid ledger. This linear structure, however, imposes limitations on scalability and throughput. In contrast, DAG-based consensus protocols allow blocks to reference multiple predecessors, creating a Directed Acyclic Graph (DAG) structure. This referencing mechanism enables a framework where multiple blocks can be added concurrently, supporting parallel processing of transactions. By diverging from the sequential constraints of traditional models, DAG-based protocols offer improved scalability and flexibility in transaction processing. time Figure 1: Blockchain and blockDAG I-B Why a DAG-based Consensus Protocol In the pursuit of transcending the inherent trade-off between security and performance and in response to the performance bottlenecks, DAG-based consensus protocols were proposed as a solution. These protocols promise high scalability and fast confirmation of transactions, hence effectively addressing the intricate balance between security and performance. Following, we describe the promises and challenges of DAG-based consensus protocols compared to linear chain protocols. Advantages and promises of DAG-based consensus protocols compared to blockchains 1. Scalability: DAG-based protocols can process transactions in parallel rather than sequentially, as in blockchains. This capability offers improved scalability and throughput compared to sequential blockchains. 2. Latency: Latency in DAG-based protocols, particularly regarding transaction confirmation time, varies with the underlying consensus mechanism. In PoW-based protocols, a DAG structure enables shorter block times and faster transaction confirmation. In some Proof of Stake (PoS)-based protocols, using additional reliable broadcast primitives increases the latency. Therefore, the extent to which DAG-based protocols reduce latency is conditional on the specific consensus approach and the architectural decisions around synchrony and security. 3. Flexibility: The DAG architecture allows for more flexible consensus mechanisms and can adapt to various network conditions, potentially making it more versatile than a blockchain. For example, in traditional blockchains, every block serves three roles: acting as a leader that validates transactions, providing content in the form of transaction data, and voting on the causal history. However, blocks in a DAG structure can have differentiated roles, e.g., see Section III-A2, enabling a more distributed approach to consensus and transaction validation. 4. Parallel writing: DAG-based protocols enable concurrent block production, diverging from the leader-based block generation model of traditional blockchains. This architecture permits multiple participants to simultaneously append transactions or blocks to the ledger, effectively broadening writing access. In its most extreme situation, every participant may independently produce blocks, eliminating reliance on a single or group of block producers. This property may remove the need for additional mempools of transactions, reduce the latency, and remove bottlenecks associated with sequential block generation. Challenges in DAG-based consensus protocols compared to blockchains 1. Security risks: The increased complexity of DAG structures introduces additional potential attack vectors, as more intricate systems often expose a broader range of vulnerabilities compared to simpler protocols. 2. Understanding and adoption: The more complex nature of DAG-based protocols can make them harder to understand and adopt, particularly for developers accustomed to traditional blockchain technology. Protocols that only provide partial ordering may encounter more difficulties adapting to existing infrastructures. 3. Consensus mechanism maturity: Consensus mechanisms used within DAG-based DLTs are often newer and less tested than those used with blockchains, creating uncertainty around their long-term stability and resilience against evolving network threats. 4. Reachability challenge: The concept of reachability within a DAG refers to the ability of a new block to reference earlier transactions or blocks, which is central to the structure’s integrity and efficient functioning. This ability has ramifications for functionalities, such as pruning (removing old data to save space) and the operation of light clients (nodes that do not store the full ledger data)."
https://arxiv.org/html/2411.10003v1,Pro-Prophet: Systematic Load Balancing Method for Efficient Parallel Training of Large-scale MoE Models,"The size of deep learning models has been increasing to enhance model quality. The linear increase in training computation budget with model size means that training an extremely large-scale model is exceedingly time-consuming. Recently, the Mixture of Expert (MoE) has drawn significant attention as it can scale models to extra-large sizes with a stable computation budget. However, inefficient distributed training of large-scale MoE models hinders their broader application. Specifically, a considerable dynamic load imbalance occurs among devices during training, significantly reducing throughput. Several load-balancing works have been proposed to address the challenge. System-level solutions draw more attention for their hardware affinity and non-disruption of model convergence compared to algorithm-level ones. However, they are troubled by high communication costs and poor communication-computation overlapping. To address these challenges, we propose a systematic load-balancing method, Pro-Prophet, which consists of a planner and a scheduler for efficient parallel training of large-scale MoE models. To adapt to the dynamic load imbalance, we profile training statistics and use them to design Pro-Prophet. For lower communication volume, Pro-Prophet planner determines a series of lightweight load-balancing strategies and efficiently searches for a communication-efficient one for training based on the statistics. For sufficient overlapping of communication and computation, Pro-Prophet scheduler schedules the data-dependent operations based on the statistics and operation features, further improving the training throughput. We conduct extensive experiments in four clusters and five MoE models. The results indicate that Pro-Prophet achieves up to 2.66x speedup compared to two popular MoE frameworks including Deepspeed-MoE and FasterMoE. Additionally, Pro-Prophet achieves a load-balancing enhancement of up to 11.01x when compared to a representative load-balancing work, FasterMoE.","Recent years, large-scale deep neural networks have achieved superior performance in various domains(e.g., NLP, CV). Previous works have shown that the model capacity is improved with the increased model size, further promoting the model scaling. However, the substantial computational demand of extra-large models makes the training process excessively time-consuming. As one of the most promising solutions, Mixture of Expert (MoE) enables a nearly constant computational budget as model scaling. Generally, we replace some layers of a foundation model with MoE ones to generate a MoE model. Each MoE layer contains a gate network and a range of sub-modules named experts. The gate network can route each input to top-k experts that excel in processing the input. As the k is a super-parameter, the MoE model can be scaled with consistent computational requirements by increasing the number of experts. As the model further scales, the effective collaboration of devices is necessary for extra-large MoE model training. Unfortunately, it is inefficient to train the model with traditional parallelism such as Data Parallelism (DP), Model Parallelism (MP), and Pipeline Parallelism (PP). To overcome the trouble, Gshard [1] introduced a specific parallel strategy named Expert Parallelism (EP). Nowadays, extra-large MoE models trained with EP have demonstrated the highest accuracy in multiple tasks[2, 3]. However, training MoE models using EP presents a dynamic load imbalance among devices. For each MoE layer, EP equally divides experts into devices before training and dynamically arranges its inputs according to the gate network during training. Most inputs are transferred to and processed by a few devices, resulting in prolonged communication and computation of inputs. Furthermore, the imbalance varies throughout the training process, making it difficult to resolve. Numerous attempts in load-balancing have been proposed to improve training throughput. Algorithmic works often restrict the upper bound of each expert’s load [4, 5] or add auxiliary losses to the loss function [6, 7] for a more balanced load. However, they impact the model convergence and even deteriorate the model quality. Considering the drawback above, systematic solutions of the MoE system draw more attention. Popular systematic works [8, 9] dynamically readjust the expert placement according to the load, achieving a balanced load without harming the model quality. However, these systematic solutions struggle to enhance training efficiency effectively due to two drawbacks. 1) Heavy communications of model states (i.e., parameters, gradients and optimizer states [10]) are introduced. The expert placement proposed by previous works introduces a global transfer of parameters and gradients or a whole model states communication. These strategies involve unnecessary communications across devices, hindering the improvement of training efficiency. 2) Devices experience significant communication and computation idle during training. Due to data dependencies among operators, the solutions have to perform some communications and computations sequentially. For example, only the experts have been selected and their model states have been transmitted, their computations of inputs can be launched. And then, the aggregation of gradients occurs only after the computation of gradients is finished. They neglect the potential of communication and computation overlapping, thus significantly influencing device utilization. In this paper, we present a systematic load-balancing solution, Pro-Prophet, which overcomes two drawbacks by a planner and scheduler respectively. To adapt dynamic features presented in the training of a MoE model, we profile the input distribution (i.e., the number of inputs processed by each expert) for each MoE layer. We observe that distributions of a MoE layer between adjacent iterations present high similarity. This locality is the key to effective load balancing. In contrast to these systematic works, Pro-Prophet planner introduces a series of lightweight expert placements. In a lightweight expert placement, each expert is independently allocated to a subset of devices. Communication of parameters and gradients for the expert occurs in these specific devices. Serve to select an optimal expert placement, the planner proposes a performance model that estimates the execution time of a MoE layer employing a lightweight expert placement. However, it is non-trivial to find it due to the combinatorial explosion of the number of expert placements. To tackle this, the planner designs a locality-based greedy algorithm. The algorithm employs a greedy strategy to search for a communication-efficient expert placement. Besides, its launching frequency is reduced based on the locality, further improving the training throughput. To exploit the potential of communication-computation overlapping, Pro-Prophet scheduler comprehensively schedules operations based on the locality and the feature of operations. The locality means that we can estimate the input distribution of the upcoming iteration according to the current one. Once the upcoming distribution is obtained, we can promptly determine a communication-efficient expert placement for the upcoming iteration and can transmit the parameters of experts in advance, which provides the opportunity to overlap communications and computations within adjacent iterations. Besides, the gradient aggregation can be scheduled backward for better overlapping. Based on these, the scheduler identifies a scheduling space and designs a block-wise scheduling strategy to comprehensively overlap communications and computations. We implement Pro-Prophet on top of PyTorch and conduct extensive experiments on four different clusters of up to 32 devices with five variant models. The results demonstrate that Pro-Prophet achieves speedups of up to 2.66x compared to two popular MoE frameworks. Additionally, Pro-Prophet has demonstrated load-balancing enhancements of up to 11.01x compared to a representative load-balancing work, FasterMoE. Our main contributions are summarized as follows: \bullet We profile input distributions among adjacent iterations and identify a locality that guides the design of Pro-Prophet. \bullet We design a Pro-Prophet planner that identifies several lightweight expert placements, abstracts a performance model and designs a locality-based greedy algorithm to reduce the heavy communication of model states. \bullet We propose a Pro-Prophet scheduler, which generates a scheduling space and establishes a block-wise scheduling strategy based on the locality and the feature of operations for comprehensive overlapping of computations and communications. \bullet We conduct comprehensive experiments for Pro-Prophet on different clusters and models. The results demonstrate that Pro-Prophet achieved up to 1.50x end-to-end speedup and 11.01x load-balancing enhancements with the representative load-balancing method."
https://arxiv.org/html/2411.09981v1,SoK: Consensus for Fair Message Ordering,"Distributed ledger systems, such as blockchains, rely on consensus protocols that constantly commit messages in an agreed order for processing. In practice, message ordering within these systems is often reward-driven. This raises concerns about fairness, particularly in decentralized finance applications, where nodes can exploit transaction orders to maximize rewards (Maximal Extractable Value, MEV). This paper provides a structured review of consensus protocols that order messages with different approaches, especially focusing on the ones that promote order fairness, using methods including First-In-First-Out (FIFO), random, and blind ordering. We review the challenges and trade-offs of deriving fair message ordering in a Byzantine fault-tolerant setting, and summarize the key steps for making a fair message ordering consensus protocol. We introduce a design guideline, with which we propose a performance optimization to the state-of-the-art FIFO ordering protocol Themis [1]. This work establishes a unified framework for accessing and enhancing fairness in distributed ledger systems.","Consensus protocols in distributed ledger systems establish agreements on the messages (or transactions, used interchangeably in this paper) to process in some specific orders. Decentralized applications, particularly those utilizing smart contracts, often result in different states if messages are processed in different orders. Since nodes in distributed ledger have significant freedom to censor the messages and decide on an order, they can manipulate the states of decentralized applications by altering the order of message processing. In practice, nodes frequently manipulate message ordering to maximize rewards. For example, in major blockchains like Bitcoin [2] and Ethereum [3], block proposers typically favor transactions with higher fees. Consequently, transactions with lower fees experience longer delays, leading to fairness concerns [4]. More importantly, in decentralized finance (DeFi) applications that rely on smart contracts, nodes have economic incentives to manipulate the order of message processing to generate additional profits known as Maximal Extractable Value (MEV), often resulting in substantial financial losses for users. For example, a node can create a transaction to buy an asset and prioritize it if the node observes a transaction in the message pool that can raise the asset price once processed. As a result of such profiting strategies through transaction ordering manipulation, around 200 million USD are extracted from DeFi users every year [5]. The core challenge in deriving a fair ordering of messages in distributed ledger systems is the inherent lack of synchronization across nodes. As shown in Figure 1, in decentralized networks, each node can independently receive messages from external users, resulting in differing local views of the message pool. Even if all nodes eventually receive the same set of messages by constantly exchanging their message pool, they are likely to receive them at varying times and in differing orders due to network delay and asynchrony [6], making it difficult for the nodes to reach an agreement of a fair message processing order. An easy way to prevent asynchronized message pools is to have a centralized entity receiving messages [7], but it violates the nature of decentralization. Moreover, most distributed ledger systems require Byzantine fault tolerance (BFT), which adds additional considerations to the robustness of the fair ordering rule in designing a fair ordering protocol. Figure 1: A fair ordering consensus protocol achieves fair order agreements among nodes Awareness of the message ordering problem has arisen in the past five years and has shown a growing interest as the number of related works arises. While there are proposals for fair ordering consensus under network asynchrony and Byzantine-fault nodes, a comprehensive understanding of the challenges, design choices of these systems is yet to be developed. Therefore, we present the first systematization of knowledge of fair ordering consensus protocols by examining existing fair ordering consensus protocols. The key contributions of our work are as follows: 1. We review, to the best of our knowledge, all proposals for message ordering and offer a unified perspective on the requirements and limitations of using these ordering rules. 2. We introduce the “Fair Consensus Factory”, a framework to integrate message order fairness into consensus protocols. 3. A case study of the state-of-the-art FIFO ordering protocol, Themis [1]. Through the fair consensus factory, we are able to optimize Themis to reduce its latency. 4. Key research gaps identified as future directions for achieving fair and efficient decentralized systems. The remainder of this paper is structured as follows: Section 2 provides background information on current message ordering approaches and MEV issues in distributed ledgers. Section 3 presents a unified perspective on existing message ordering rules. Section 4 presents the fair consensus factory that adds fairness to consensus protocols. Section 5 provides a case study of Themis regarding its consensus design and a latency optimization provided by the fair consensus factory framework. Section 6 discusses the open challenges in fair ordering consensus for future studies. Finally, Section 7 concludes the paper."
https://arxiv.org/html/2411.09957v1,Sublinear-time Collision Detection with a Polynomial Number of States in Population Protocols,"This paper addresses the collision detection problem in population protocols. The network consists of state machines called agents. At each time step, exactly one pair of agents is chosen uniformly at random to have an interaction, changing the states of the two agents. The collision detection problem involves each agent starting with an input integer between 1 and n, where n is the number of agents, and requires those agents to determine whether there are any duplicate input values among all agents. Specifically, the goal is for all agents to output false if all input values are distinct, and true otherwise.In this paper, we present an algorithm that requires a polynomial number of states per agent and solves the collision detection problem with probability one in sub-linear parallel time, both with high probability and in expectation. To the best of our knowledge, this algorithm is the first to solve the collision detection problem using a polynomial number of states within sublinear parallel time, affirmatively answering the question raised by Burman, Chen, Chen, Doty, Nowak, Severson, and Xu [PODC 2021] for the first time.","In this paper, we explore the population protocol model, introduced in 2004 and studied extensively since then [5, 6, 3, 2, 20, 21, 31, 27, 9, 23]. The model consists of a network, or population, of n state-machines, referred to as agents. At each time step, a pair of agents is selected uniformly at random to engage in an interaction (i.e., pairwise communication), during which they update their states. Agents are anonymous, i.e., they lack unique identifiers. In the population protocol model, time complexity is often measured in parallel time, defined as the number of time steps divided by n (the number of agents). This metric is practical because interactions typically occur simultaneously across the population. Throughout the remainder of this section (i.e.Section 1), we will discuss time complexity in terms of parallel time. Leader election has been extensively studied in the population protocol model. Leader election can be solved by a simple two-state protocol [5], where initially, all agents are leaders. The protocol employs only one transition rule: when two leaders meet, one of them becomes a follower (i.e., a non-leader). This simple protocol elects a unique leader in O(n) parallel time. Furthermore, this protocol is time-optimal: Doty and Soloveichik [18] demonstrated that any constant-space protocol requires linear time to elect a unique leader. Later, in 2015, Alistarh and Gelashvili [3] developed a leader election protocol that converges in O(\log^{3}n) parallel time and uses O(\log^{3}n) states per agent. Subsequently, numerous papers have focused on fast leader election, including [2, 20, 21, 27, 9]. Gąsieniec, Staehowiak, and Uznanski [21] developed an algorithm that converges in O(\log n\log\log n) time and uses a surprisingly small number of states: only O(\log\log n) per agent. This is considered space-optimal because it is established that every leader election protocol requiring O(n/\mathrm{polylog}(n)) time also requires \Omega(\log\log n) states [1]. Sudo et al. [27] presented a simple protocol that elects a unique leader within O(\log n) time and utilizes O(\log n) states per agent. This is time-optimal, as any leader election protocol requires \Omega(\log n) time, even if it uses an arbitrarily large number of states and the agents know the exact size of the population [24]. (This lower bound may appear obvious, yet it does not directly follow from a simple coupon collector argument because we can specify an initial configuration where all agents are followers.) Finally, in 2020, Berenbrink et al. [9] provided a time and space-optimal protocol, i.e., an O(\log n)-time and O(\log\log n)-states leader election protocol. Self-stabilizing leader election (SS-LE) has garnered significant attention within this model. This variant of leader election stipulates that (i) starting from any configuration, the population must reach a safe configuration where exactly one leader exists; and (ii) once a safe configuration is reached, the unique leader must be maintained indefinitely. These conditions ensure tolerance against finitely many transient faults, which is critical since many protocols (both self-stabilizing and non-self-stabilizing) assume the presence of a unique leader. Consequently, SS-LE is essential for enhancing the fault tolerance of the population protocol model itself. However, it is well known that no protocol can solve SS-LE unless each agent in the population knows the exact size n of the population [19, 12]111 Strictly speaking, the cited works proves a slightly weaker impossibility. Nevertheless, this impossibility can be proved using a similar technique: a simple partitioning argument. See [31] for details (page 618, footnote). . Numerous studies have focused on overcoming this impossibility by employing various strategies, including assuming oracles [7, 19, 13], assuming that agents precisely know the population size n [12, 11], restricting the topology [4, 15, 16, 33, 34], or slightly relaxing the requirements of SS-LE [22, 28, 25, 26, 29, 30, 31, 32, 23]. Among these, all algorithms that adopt the n-knowledge approach [12, 11] elect the unique leader by solving a more general problem, called self-stabilizing ranking. This problem stipulates that: (i) each agent v maintains an output variable v.\mathtt{rank}\in\{1,2,\dots,n\}, (ii) starting from any configuration, the population must reach a safe configuration where no two agents share the same \mathtt{rank} value; and (iii) once a safe configuration is reached, no agent updates its \mathtt{rank}. SS-LE can be straightforwardly reduced to self-stabilizing ranking because, once ranking is achieved, there is exactly one agent with \mathtt{rank}=1. This agent can thus be regarded as the unique leader. Cai, Izumi, and Wada [12] present an algorithm that solves the self-stabilizing ranking problem within O(n^{2}) parallel time, using n states per agent. In contrast, Burman, Chen, Chen, Doty, Nowak, Severson, and Xu [11] introduce significantly faster algorithms that, however, require more states. Specifically, they offer an O(n) parallel time algorithm with O(n) states per agent and an O(\log n) parallel time algorithm with a super-exponential number of states. This leads to a natural question: Can self-stabilizing ranking be solved in sublinear parallel time using only a polynomial number of states per agent? To solve self-stabilizing ranking, agents must detect whether any distinct agents share the same rank. Consequently, Burman et al. raise an open question: Is there a sublinear parallel time algorithm with \mathit{poly}(n) states that can solve the following problem, which we refer to as the collision detection problem in this paper: Each agent v is given an input \mathtt{rank}\in\{1,2,\dots,n\}. The goal for each agent is to decide whether at least one pair of agents have the same input value in \mathtt{rank}. (See Section 2.3 for the formal definition of this problem.) Our Contribution In this paper, we affirmatively answer the open question raised by Burman et al. [11]. Specifically, we introduce a collision detection algorithm, CollisionDetection, that stabilizes within O(\sqrt{n\log n}) parallel time in expectation and O(n^{1/2}\cdot\log^{3/2}n) parallel time with high probability. This algorithm uses \tilde{O}(\sqrt{n}) states per agent with high probability, excluding the input variable \mathtt{rank}, which requires O(n) states. The proposed algorithm is always correct; that is, it eventually reaches a stable configuration where all agents output the correct answer with probability 1. It remains an open question whether self-stabilizing ranking or weaker variants of this problem, such as loosely-stabilizing ranking, can be solved within sublinear parallel time using a polynomial number of states per agent. Organization of This Paper Section 2 introduces the preliminaries, including key terminologies and the definition of the model. Section 3 describes the basic submodules that are instrumental in designing the proposed algorithm CollisionDetection. Section 4 presents CollisionDetection, proves its correctness, and bounds its time and space complexities. In the remainder of this paper, we will not use parallel time; instead, we will discuss stabilization time in terms of the number of time steps (or interactions)."
https://arxiv.org/html/2411.10399v1,Game Theoretic Liquidity Provisioning in Concentrated Liquidity Market Makers,"Automated marker makers (AMMs) are a class of decentralized exchanges that enable the automated trading of digital assets. They accept deposits of digital tokens from liquidity providers (LPs); tokens can be used by traders to execute trades, which generate fees for the investing LPs. The distinguishing feature of AMMs is that trade prices are determined algorithmically, unlike classical limit order books. Concentrated liquidity market makers (CLMMs) are a major class of AMMs that offer liquidity providers flexibility to decide not only how much liquidity to provide, but in what ranges of prices they want the liquidity to be used. This flexibility can complicate strategic planning, since fee rewards are shared among LPs. We formulate and analyze a game theoretic model to study the incentives of LPs in CLMMs. Our main results show that while our original formulation admits multiple Nash equilibria and has complexity quadratic in the number of price ticks in the contract, it can be reduced to a game with a unique Nash equilibrium whose complexity is only linear. We further show that the Nash equilibrium of this simplified game follows a waterfilling strategy, in which low-budget LPs use up their full budget, but rich LPs do not. Finally, by fitting our game model to real-world CLMMs, we observe that in liquidity pools with risky assets, LPs adopt investment strategies far from the Nash equilibrium. Under price uncertainty, they generally invest in fewer and wider price ranges than our analysis suggests, with lower-frequency liquidity updates. We show that across several pools, by updating their strategy to more closely match the Nash equilibrium of our game, LPs can improve their median daily returns by $116, which corresponds to an increase of 0.009% in median daily return on investment.","Automated market makers (AMMs) are decentralized exchanges (DEXes) that allow users to exchange cryptocurrency via a smart contract that algorithmically manages liquidity and exchange rates (sok2023, ). As a dominant class of DEXes (dex-rank, ), AMMs play an outstanding role more generally as decentralized applications (dapp-rank, ). Today, AMMs drive billions of dollars in daily trading volume on several blockchains (ripple-amm, ; coingecko, ). The core functionality of AMMs is facilitated by liquidity pools, i.e., blockchain smart contracts that store and manage cryptocurrency tokens for trading. Most liquidity pools store two types of tokens, which we denote X and Y; we focus on the two-token class of AMMs in this work. A typical trade proceeds in three steps: (a) A trader proposes to pay \Delta x amount of the X token and asks for a quote. (b) The pool tells the trader they will obtain \Delta y amount of the Y token (assuming there is sufficient liquidity in the pool). (c) The trader decides whether to execute the token swap, in which case they also must pay a trading fee (in units of X) that is proportional to the amount of added tokens. The trading fee is used by the AMM to incentivize liquidity providers (LPs), who initially deposit liquidity tokens into the pool to support trades. An AMM can broadly be characterized by three intertwined policies, which are implemented algorithmically: (1) the exchange rate of \Delta x for \Delta y (and vice versa) based on the state of the pool, (2) the liquidity investment policy (i.e., what constitutes a valid deposit/withdrawal), and (3) the trading fee reward mechanism, i.e., how trading fees are allocated to LPs. In this paper, we consider two canonical types of LPs, which we call Legacy automated market makers (Legacy AMMs) and concentrated liquidity market makers (CLMMs). Legacy AMMs. Legacy AMMs determine the exchange rate via a constant product market maker (CPMM) (sok2023, ). CPMMs ensure that the product of pool reserves always remains constant. LPs that invest in legacy AMMs must deposit both X and Y tokens, and fee rewards are split among investing LPs proportionally to their initial investment (details in §2.2). Although legacy AMMs are extremely widely used (uv2, ; sushi, ; balancerv2, ; curve, ; cake, ; orca, ; raydium, ), they are known to suffer from the price slippage problem: the price of a large trade is significantly worse than that of a small trade; this is due to the hyperbolic shape of the price curve (adams2024dontletmevslip, ). Concentrated Liquidity Market Makers (CLMMs). To mitigate slippage, a number of AMMs (uv3, ; sushi, ; balancerv3, ; orca, ) adopted a scheme called concentrated liquidity, first introduced in Uniswap v3 in 2021 (uv3, ). In a CLMM, an LP decides not only the amount of liquidity they want to add, but also the price range in which the liquidity is active. As a result, the LP only earns fees from trades when the external token price lies in the LP’s range of investment. Investing in a narrower price range grants the LP a higher share of the fees from transactions within that range. In CLMMs, LPs suffer less price slippage in price ranges with high liquidity. However, CLMMs force LPs to be more strategic in their investment choices: they must choose which price range(s) to invest in, as well as the amount of investment. The consequences of these choices are complex — even if an LP could accurately predict future price ranges, they would still need to compete against other LPs over their share of the trading fee. To date, it remains unclear how strategic LPs should invest funds in CLMMs. Prior literature has studied incentives in AMMs, but no prior work has simultaneously modeled and analyzed the following three properties of CLMMs: (1) LPs can only invest up to a fixed budget, (2) LPs in CLMMs can invest different amounts in different price ranges, and (3) LPs compete against each other for fees, and thus must take into account other LPs’ strategies and their budgets (Frtisch23, ). Most existing works focus on the study of a single LP’s strategy (Fan2022, ) or on the case where LPs are identical (concave-pro-rata-game, ; Fan2022, ; Heimbach2023, ; Bayraktar24, ; he2024liquiditypooldesignautomated, ; fan2024strategicliquidityprovisionuniswap, ), and in both cases the budget is assumed to be unlimited (concave-pro-rata-game, ). For legacy AMMs, (concave-pro-rata-game, ) proposes a framework using symmetric games to show the uniqueness of the symmetric Nash equilibrium. However, the game in this work does not incorporate budget constraints, which is an important practical consideration. It also does not capture LP strategies involving complex combinations of liquidity positions in CLMMs when LPs have different capacity investments. Our goal in this work is to study the incentives of LPs in CLMMs under a game-theoretic model. In particular, we want to understand the following questions: Do there exist equilibrium investment strategies for CLMMs, and if so, what are their characteristics in relation to LPs’s investment capacity? How do the strategies of real-world LPs compare to those at Nash equilibrium? To answer these questions, we make the following contributions: (a) Game theoretic model: We model strategic liquidity provisioning as a game played by rational and selfish LPs. Each LP is constrained by their own budget, rewarded by trading fees, and penalized by impermanent loss (the opportunity cost of not choosing to hold the tokens in hand). Analysis of this game is complex due to its large strategy space, as LPs can invest in a set of price ranges that is quadratic in the number of feasible price range endpoints. However, we prove that this complex game is equivalent to a much simpler game in which each LP’s investment can be broken into a much smaller (linear) set of atomic price ranges (Thm. 3.5). (b) Nash equilibrium analysis: We prove the unique existence of a Nash equilibrium in our simplified game (Thm 3.3). We show that the Nash equilibrium exhibits a waterfilling pattern (Thm. 3.6), which reveals a division of LPs by their budget — poor LPs exhaust their budgets, while rich LPs spend equally. We characterize properties of the Nash equilibrium as a function of LP budgets. (c) Real-World Data Analysis: On the Ethereum blockchain, we compare our theoretical results to the actions of real LPs on two types of liquidity pools – stable pools and risky pools. We compare LPs’ real-world liquidity provisions against their (simulated) best-response actions and Nash equilibrium actions under our game formulation. In stable pools, we show that real LPs deploy strategies similar to the Nash equilibrium of our game, and return on investment at equilibrium is much lower than that in risky pools. In risky pools, we show that most real LPs prefer to invest in few (<5) price ranges, each with wide price coverage. While this behavior is far from our predicted Nash equilibrium strategies, we can partially predict LP behavior by solving a modified version of our game with stale information, suggesting that many LPs have not yet taken full advantage of the data available on the blockchain. We show that LPs’ median daily return on investment (ROI) can be improved by 0.009% by updating their strategy to more closely match the Nash equilibrium of our constructed game. In dollars, this corresponds to an increase in median daily utility of $116 and average daily utility of $222."
https://arxiv.org/html/2411.10385v1,"Low-Latency Task-Oriented Communications with Multi-Round, Multi-Task Deep Learning","In this paper, we address task-oriented (or goal-oriented) communications where an encoder at the transmitter learns compressed latent representations of data, which are then transmitted over a wireless channel. At the receiver, a decoder performs a machine learning task, specifically for classifying the received signals. The deep neural networks corresponding to the encoder-decoder pair are jointly trained, taking both channel and data characteristics into account. Our objective is to achieve high accuracy in completing the underlying task while minimizing the number of channel uses determined by the encoder’s output size. To this end, we propose a multi-round, multi-task learning (MRMTL) approach for the dynamic update of channel uses in multi-round transmissions. The transmitter incrementally sends an increasing number of encoded samples over the channel based on the feedback from the receiver, and the receiver utilizes the signals from a previous round to enhance the task performance, rather than only considering the latest transmission. This approach employs multi-task learning to jointly optimize accuracy across varying number of channel uses, treating each configuration as a distinct task. By evaluating the confidence of the receiver in task decisions, MRMTL decides on whether to allocate additional channel uses in multiple rounds. We characterize both the accuracy and the delay (total number of channel uses) of MRMTL, demonstrating that it achieves the accuracy close to that of conventional methods requiring large numbers of channel uses, but with reduced delay by incorporating signals from a prior round. We consider the CIFAR-10 dataset, convolutional neural network architectures, and AWGN and Rayleigh channel models for performance evaluation. We show that MRMTL significantly improves the efficiency of task-oriented communications, balancing accuracy and latency effectively.","In the rapidly advancing field of NextG communication systems, there is an increasing focus on task-oriented (or goal-oriented) communications. This approach is gaining prominence as it addresses the specific needs of various applications by ensuring that the transmission process is aligned with the ultimate objective of the task at hand [1, 2, 3, 4, 5, 6, 7, 8, 9]. Unlike traditional communication paradigms that focus on delivering raw data, task-oriented communications (TOC) aims to transmit only the information necessary to accomplish a specific task. Deep learning plays a crucial role in optimizing the encoding and decoding processes for TOC, allowing for efficient and effective transmission of information that directly contributes to the task’s success. By leveraging deep learning-driven TOC, NextG communication systems can achieve significant improvements in both performance and resource utilization [10, 11, 12], making them well-suited for the demands of modern applications such as the Internet of Things (IoT), augmented reality/virtual reality (AR/VR), and vehicle-to-everything (V2X) network systems. In IoT networks, sensors generate vast amounts of data that need to be processed and analyzed to make real-time decisions, such as in smart cities and industrial automation. TOC can significantly reduce the communication overhead by transmitting only the essential information required for decision-making, rather than the raw sensor data. Similarly, in AR/VR applications, low latency and high accuracy are critical to delivering immersive experiences. TOC can help achieve this by optimizing the transmission of visual and sensory data to meet the application’s specific needs. In V2X systems, vehicles need to communicate with each other and with infrastructure to ensure safe and efficient transportation. TOC can enhance these interactions by focusing on the transmission of critical information, such as collision warnings and traffic updates, thereby improving response times and reducing network congestion. One of the primary challenges in TOC is balancing task accuracy and latency objectives and requirements. To that end, the age of task information for TOC was studied in [13]. Achieving high accuracy often requires transmitting a large amount of data, which can lead to increased delay (measured by the number of channel uses) and higher bandwidth usage. Conversely, minimizing delay and bandwidth usage can compromise accuracy. This accuracy-delay tradeoff is a significant hurdle that needs to be addressed to realize the full potential of TOC. We propose a novel multi-round, multi-task learning (MRMTL) approach to address this challenge by dynamically updating the number of channel uses in iterative transmissions of TOC. MRMTL involves an encoder at the transmitter that learns compressed latent representations of input data (e.g., images), which are transmitted over a wireless channel. At the receiver, a decoder performs a machine learning task, specifically classifying the received signals. MRMTL is different from the autoencoder-based communications, where the typical setting has the source-coded data symbols as the input and the reconstruction of those symbols as the output [14]. On the other hand, MRMTL starts with (raw) input data and performs a machine learning task such as classification. The deep neural networks (DNNs) corresponding to the encoder-decoder pair are jointly trained, considering both channel and data characteristics to achieve high task accuracy with minimal channel uses. MRMTL introduces a dynamic update mechanism where the transmitter incrementally sends an increasing number of encoded samples over the channel. The receiver utilizes signals from a previous round to enhance task performance, rather than relying solely on the latest transmissions. The multi-round process utilizes multi-task learning that jointly optimizes accuracy across multiple rounds, with each configuration treated as a distinct task performed with a different number of channel uses. When the receiver’s confidence in the task decisions is low, it can then allocate additional channel uses to improve task accuracy. We demonstrate that MRMTL achieves the accuracy of conventional methods with single-round, single-task learning (SRSTL) for TOC that requires a large number of channel uses, but with significantly reduced delay by incorporating signals from a prior round. To evaluate MRMTL, we consider the CIFAR-10 dataset as the input, convolutional neural network (CNN) architectures as the DNNs, and AWGN and Rayleigh channel models. Our results show that MRMTL significantly improves the efficiency of TOC, effectively balancing accuracy and delay. This study represents a significant step forward in the development of efficient and effective TOC systems for NextG networks. The MRMTL approach can be extended to incorporate semantic communications [15] and integrated sensing and communications [16, 17] by adding tasks of reconstruction and sensing, respectively. The remainder of the paper is organized as follows. Section II describes SRSTL for TOC. Section III presents MRMTL for TOC. Section IV introduces the MRMTL’s process of dynamic initiation of multiple rounds in TOC. Figure 1: System model of SRSTL for TOC."
https://arxiv.org/html/2411.09473v1,Sample Paper Title,"Influence Maximization (IM) is vital in viral marketing and biological network analysis for identifying key influencers. Given its NP-hard nature, approximate solutions are employed. This paper addresses scalability challenges in scale-out shared memory system by focusing on the state-of-the-art Influence Maximization via Martingales (IMM) benchmark. To enhance the work efficiency of the current IMM implementation, we propose EfficientIMM with key strategies, including new parallelization scheme, NUMA-aware memory usage, dynamic load balancing and fine-grained adaptive data structures. Benchmarking on a 128-core CPU system with 8 NUMA nodes, EfficientIMM demonstrated significant performance improvements, achieving an average 5.9x speedup over Ripples across 8 diverse SNAP datasets, when compared to the best execution times of the original Ripples framework. Additionally, on the Youtube graph, EfficientIMM demonstrates a better memory access pattern with 357.4x reduction in L1+L2 cache misses as compared to Ripples.","The advent of large-scale data and advanced computational frameworks has significantly deepened our exploration of social networks, positioning Influence Maximization (IM) as a pivotal research area. This research domain aims to identify the most influential nodes within a network, optimizing the spread of information, behaviors, or products. These insights are invaluable not only for marketing and public health initiatives but also for enhancing our understanding of the complex dynamics of human interaction and information dissemination across vast networks [3, 6, 15]. The IM problem is formally delineated within the framework of graph theory. In brief, the goal of influence maximization is to be capable of identifying a subset of the most influential vertices in an input graph. The challenge inherent to the IM problem arises from its NP-hard nature, indicating the absence of a deterministic polynomial-time algorithm capable of deriving the optimal solution [5]. Therefore, it has prompted the development of numerous approximation algorithms. As a sketch-based approximation algorithm, Influence Maximization via Martingales (IMM) is renowned for its robust efficiency and broad adaptability, making it ideal for large-scale social networks [7]. As first introduced by Tang et al. [13], IMM employs advanced sampling techniques known as Reverse Influence Sampling (RIS) to generate sketches known as random reverse reachable sets or RRRsets for the input graph. To guarantee the quality of the sampled sketches, IMM utilizes martingale probability theory, which also ensures the linear time-to-solution cost corresponding to the increasing graph size [13]. Recognizing the growing significance and potential of IMM, it has been identified as a key workflow for further development and research. Significant efforts have been made to adapt IMM for handling different large-scale data inputs [11, 9, 8]. We focused on Ripples, a widely adopted open-source IMM framework, particularly suited to the complex and large-scale structure of modern social networks [9]. One of the primary challenges posed by Ripples is algorithm’s inability to scale effectively on the latest shared memory server-grade CPUs with multi-NUMA and multi-socket architecture. The system used for evaluation in the original Ripples paper, as described by Minutoli et al. [9], consists of a CPU with 10 cores, shared memory and cache spaces, and no NUMA control. Given the trend of shifting to a CPU system with many more NUMA nodes and also the increasing interest of designing scale-out shared memory system, many important benchmarks and frameworks have been adapted into this trend in system [16]. Therefore, it is crucial for us to strive for better performance on Ripples targeting the multi-NUMA systems. With our assessments of the Ripples algorithm across a variety of datasets on a multi-NUMA CPU system, we have pinpointed several critical limitations that hinder its performance: (i) The parallelization strategy employed for RIS sketches demonstrates suboptimal scalability across all CPU workers, (ii) excessive data access coupled with poor data locality, (iii) workload imbalance resulted from several aspects of graph processing. To address these challenges, this paper first analyzes social graphs and identifies features that impede the scalability of the algorithm. Then, this work introduces a new parallelization strategy that improves scalability far beyond what the current Ripples framework achieves. In addition to the parallelization strategy, we also redesigned the IMM algorithm for NUMA-awareness and employed optimizations based on fundamental characteristics of social graphs. This paper makes several contributions to the field of IMM, outlined as follows: • We conduct a comprehensive profiling of the current state-of-the-art solutions for the IMM problem with various contexts of the social graphs, pinpointing critical bottlenecks that necessitate optimization. • We introduce a new shared-memory partitioning and distribution strategy that greatly enhances the efficiency of constructing the influence counts for each vertices based on the sampled RIS sketches through concurrent updates. Using the new parallelization technique, we further adapted the algorithm with NUMA-aware data structures and greatly improved data reuse. • We have developed optimization techniques to address workload imbalances arising from the varying attributes of different graphs, including adaptive data structures and dynamic job balancing strategies. • We applied these optimizations to eight selected SNAP datasets, achieving an impressive performance speedup ranging from 1.6 to 12.1 times compare to Ripples’ best runtime. Also, our work efficient algorithm also brings 22.4x to 357.4x less L1+L2 cache misses as compared to Ripples."
https://arxiv.org/html/2411.09317v1,Pie: Pooling CPU Memory for LLM Inference,"The rapid growth of LLMs has revolutionized natural language processing and AI analysis, but their increasing size and memory demands present significant challenges. A common solution is to spill over to CPU memory; however, traditional GPU-CPU memory swapping often results in higher latency and lower throughput.This paper introduces Pie, an LLM inference framework that addresses these challenges with performance-transparent swapping and adaptive expansion. By leveraging predictable memory access patterns and the high bandwidth of modern hardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent data swapping without affecting foreground computation, expanding effective memory without added latency. Adaptive expansion dynamically adjusts CPU memory allocation based on real-time information, optimizing memory usage and performance under varying conditions.Pie maintains low computation latency, high throughput, and high elasticity. Our experimental evaluation demonstrates that Pie achieves optimal swapping policy during cache warmup and effectively balances increased memory capacity with negligible impact on computation. With its extended capacity, Pie outperforms vLLM by up to 1.9\times in throughput and 2\times in latency. Additionally, Pie can reduce GPU memory usage by up to 1.67\times while maintaining the same performance. Compared to FlexGen, an offline profiling-based swapping solution, Pie achieves magnitudes lower latency and 9.4\times higher throughput.","In recent years, the application of Large Language Models (LLMs) has seen widespread adoption, becoming a cornerstone of modern technology. These models are essential for a range of applications, from natural language processing to sophisticated AI-driven analysis. However, efficient memory management for LLM inference remains a challenge. LLMs are growing in size (yang2023harnessing, ; kaplan2020scaling, ), and so are the prompts they process. These models typically have billions of parameters and work with massive datasets and key-value caches when generating responses. The maximum allowed context length of commercially available LLMs has been increasing exponentially (anthropic2024claude, ; achiam2023gpt, ; google2024gemini, ). While efforts have been made to increase GPU memory capacity, the compute capacity continues to grow faster than the memory capacity. When memory demands exceed GPU memory, a natural solution is to spill over to CPU memory, solution also known as swapping. Unfortunately, swapping can lead to higher latency and lower throughput, as the GPU may need to wait for data spilled over to the CPU memory to be transferred back. While increasing the CPU memory capacity allows us to process larger data batches, the added wait time may results in both higher latency and lower throughput. With LLM inference, per-token latency consists of two parts: computation latency and queuing latency. Computation latency is the time it takes to compute the token, while queuing latency is the time the token has to wait because it cannot be scheduled as soon as it arrives, typically due to the system being fully utilized. A larger GPU memory capacity can achieve lower end-to-end latency because of lower queuing latency, and higher throughput because of increased processing capacity. Therefore, the optimal swapping solution would be to add more memory capacity without affecting per-token computation latency (i.e., without causing the GPU to wait for data transfers). Achieving this will result in lower end-to-end per-token latency and higher throughput. In this paper, we ask the following questions: ”What is the maximum CPU memory we can add without blocking GPU computation?” and ”What is the performance impact of extending capacity using CPU memory?” We explore these questions by designing Pie, an LLM inference framework that based on performance-transparent swapping and adaptive expansion. Performance-transparent swapping ensures that swapping has zero impact on the compute latency. Adaptive expansion dynamically allocates the optimal amount of CPU memory for swapping, maintaining performance transparency and preventing resource under-utilization. At its core performance-transparent swapping allows for the swapping process to occur concurrently with foreground computation. Performance-transparent swapping gives the application the illusion of a GPU with more memory without any negative impact on latency. Note this is different from virtual memory; while virtual memory also gives the illusion of more memory, it comes at the cost of latency due to paging when the data is not in memory. Performance-transparent swapping improves performance by using a larger memory without the associated latency penalties. Therefore, by adjusting the amount of swapping, the system can modify the effective memory size. Effective memory is defined as the maximum amount of memory that the application can use with no impact on the GPU performance. It includes the total available GPU memory plus effective extended memory, which is a certain amount of dynamically allocated CPU memory that can be used without blocking GPU computation. This approach effectively increases the memory size while introducing a degree of elasticity. To achieve true performance transparency in swapping, data required for upcoming accesses must be prefetched to the compute device before those accesses occur. The prefetch bandwidth imposes a strict constraint on the data size that can be transferred between devices within a given time interval. The prefetch efficiency measures the fraction of prefetched data that turns out to be useful and can be utilized without blocking the GPU. The effective extended memory size is bounded by the product of the prefetch bandwidth, the duration of the prefetch time interval, and the prefetch efficiency. Therefore, the swapping mechanism relies on high-bandwidth devices and must accurately predict upcoming memory accesses. There are two properties that facilitate performance transparent swapping. First, recent GPUs, such as NVIDIA’s GH200 Grace Hopper Superchip (nvidia2024gracehopper, ), provide high-bandwidth interconnect between the GPU and CPU memories. By leveraging NVLink, this bandwidth can be as high as 900GB/s. Second, the memory accesses of the LLM inference workloads are predictable, especially at the layer granularity. This predictability allows us to achieve 100% prefetching efficiency. Together, these two properties allow us to substantially increase the effective memory for LLM inference, potentially extending this memory by tens of gigabytes. Determining the appropriate amount of CPU memory for swapping is a non-trivial task. If the allocated size is too large, Pie will break the transparency of swapping, causing applications to experience delays as computations are blocked waiting for data. Conversely, if the size is too small, resources may be underutilized, leading to lower performance. Additionally, this decision is influenced by workload characteristics; some workloads do not benefit from increased memory because they are compute-bound and require only a small amount of memory. Adding more memory to such workloads would be a waste of resources. One technique to optimize the performance of LLM inference is using offline profiling: we profile and decide the memory allocations before running the application. Unfortunately, offline profiling ignores changes in the workloads and the system environment during the run time (e.g., another process might initiate data transfer during the inference workload). To address this challenge, Pie employs adaptive expansion, a lightweight online method, to determine the amount of CPU memory for swapping. It starts with zero CPU memory and then gradually increases this memory as long as the following conditions: (1) the GPU-CPU interconnect is not saturated; (2) the swapping latency remains lower than computation latency; (3) the workload throughput increases as we allocate more CPU memory. If any of these conditions are not met, Pie maintains the current allocation or reduces the amount of CPU memory allocated for swapping. This paper makes four key contributions: • We introduce performance-transparent swapping, enabling LLM inference systems to use CPU memory without blocking computation. • We present adaptive expansion, a technique that dynamically adjusts the swapping size to accommodate changes in the system environment and workload, achieving optimal performance under varying conditions. • We design and implement a system prototype, Pie, that achieves high throughput, low latency, and high elasticity. • We provide a thorough experimental evaluation showing that Pie outperforms vLLM by up to 1.9\times in throughput and 2\times in latency. Compared to FlexGen, Pie achieves up to 60\times lower latency and 9.4\times higher throughput. The rest of this paper is organized as follows: Section 2 gives background on LLM inferences and motivates Pie. In Section 3, we discuss the design, and in Section 4, we detail the implementation. Section 5 presents Pie’s performance. We discuss related work in Section 6 and conclude in Section 7."
https://arxiv.org/html/2411.09047v1,Anomaly Detection in Large-Scale Cloud Systems: An Industry Case and Dataset,"As Large-Scale Cloud Systems (LCS) become increasingly complex, effective anomaly detection is critical for ensuring system reliability and performance. However, there is a shortage of large-scale, real-world datasets available for benchmarking anomaly detection methods.To address this gap, we introduce a new high-dimensional dataset from IBM Cloud, collected over 4.5 months from the IBM Cloud Console. This dataset comprises 39,365 rows and 117,448 columns of telemetry data. Additionally, we demonstrate the application of machine learning models for anomaly detection and discuss the key challenges faced in this process.This study and the accompanying dataset provide a resource for researchers and practitioners in cloud system monitoring. It facilitates more efficient testing of anomaly detection methods in real-world data, helping to advance the development of robust solutions to maintain the health and performance of large-scale cloud infrastructures.","In recent decades, the adoption of Cloud Computing across government and business sectors has grown exponentially [1, 2]. At the core of this growth is the ability of cloud computing to offer high-capacity data centers as a reliable backbone for services. Cloud providers operate expansive data centers that support global workloads, necessitating sophisticated techniques to monitor, diagnose, and respond to failures in real-time. As cloud infrastructure expands in both scale and complexity, maintaining its reliability has become a critical concern. Even brief outages or performance issues can lead to significant losses for users hosting applications in the cloud [3]. To prevent such issues, cloud system administrators must continuously monitor hardware and software services to ensure compliance with service-level agreements (SLAs) [4]. System anomalies, which translate into unexpected behavior, reduced efficiency, or even downtime, pose a significant risk. Early detection of these anomalies is vital for taking preemptive measures to safeguard users, improve the overall user experience, and ensure SLAs. Various studies have introduced anomaly detection methods based on statistical and machine learning techniques, spanning supervised [5], semi-supervised [6, 7], and unsupervised approaches [8, 9, 10, 11]. These models have been tested using diverse datasets and systems of varying complexity and scale [12, 13]. One major challenge for anomaly detection methods is the high dimensionality of data generated in large-scale cloud computing environments [14]. Many existing methods struggle to maintain accuracy in the presence of this “curse of dimensionality” [15], which hampers both performance and precision. High-dimensional data requires more input for generalization and results in data sparsity, where data points become scattered and isolated. The abundance of irrelevant features often obscures true anomalies, reducing the effectiveness of traditional methods such as distance or clustering-based techniques [16]. Additionally, much of the existing work in anomaly detection has been conducted on relatively small datasets [17, 18, 19], which may not fully capture the challenges posed by larger-scale cloud systems. To help advance this area of research, we aim to share a large-scale dataset from a real-world IBM Cloud System [9] with the broader community. This will enable more comprehensive testing and evaluation of anomaly detection methods on large, complex datasets. We address the following research questions (RQs): RQ1: What are the key characteristics of telemetry datasets collected from Large-Scale Cloud Systems111Comprising numerous hardware and software components, which are often distributed across multiple data centers. (LCS)? RQ2: What are the main challenges in predicting anomalies within such large datasets? The main contributions of this paper are: • Introducing a new large-scale dataset for testing anomaly detectors in cloud systems. The dataset is available on Zenodo [20]. • Demonstrating predictive models for anomaly detection in cloud environments. • Discussing challenges related to handling high-dimensional telemetry data using domain knowledge and machine learning techniques. The remainder of the paper is organized as follows. Section II reviews the related literature. Section III introduces the dataset. Section IV details the construction of anomaly detectors. Section V discusses the challenges faced. Finally, Section VII concludes the paper."
https://arxiv.org/html/2411.08448v1,"In Serverless, OS Scheduler Choice Costs Money: A Hybrid Scheduling Approach for Cheaper FaaS","In Function-as-a-Service (FaaS) serverless, large applications are split into short-lived stateless functions. Deploying functions is mutually profitable: users need not be concerned with resource management, while providers can keep their servers at high utilization rates running thousands of functions concurrently on a single machine. It is exactly this high concurrency that comes at a cost. The standard Linux Completely Fair Scheduler (CFS) switches often between tasks, which leads to prolonged execution times. We present evidence that relying on the default Linux CFS scheduler increases serverless workloads cost by up to 10\times.In this article, we raise awareness and make a case for rethinking the OS-level scheduling in Linux for serverless workloads composed of many short-lived processes. To make serverless more affordable we introduce a hybrid two-level scheduling approach that relies on FaaS characteristics. Short-running functions are executed in FIFO fashion without preemption, while longer-running functions are passed to CFS after a certain time period. We show that tailor-made OS scheduling is able to significantly reduce user-facing costs without adding any provider-facing overhead.","For Function-as-a-Service (FaaS) serverless, applications are sliced into short-lived functions, allowing cloud providers to dynamically allocate resources and manage server provisioning in a fine-grained manner on behalf of the user [1, 2]. The fine-grained and elastic scaling characteristics allow cloud providers to have profit margins by ultimately taking advantage of packing many functions into the same server for high utilization. From the user perspective, serverless applications are scalable and cost-effective [3, 4]. Users do not need to configure, manage, and maintain servers, they just ship code or container images [5] to the platform and receive results back [6, 7]. Serverless functions exhibit runtime skew and invocation burstiness [8, 9, 10, 11, 12]. According to a Microsoft Azure study [13], 80% of the serverless functions execute less than 1 second (see the cumulative distribution function plot in Figure 2), more than 90% of functions allocate virtual memory less than 400MB, and 81% of functions are invoked once per minute or less. Since the execution of a wide majority of serverless functions is typically short, on the order of seconds or less [13, 10, 14, 15, 16], cloud providers charge users according to their function execution duration with a price for every 1 millisecond. Costs are also related to the amount of memory allocated to the user’s functions [17, 18, 19, 20]. When handling serverless workloads, cloud providers aim at low latency and execution time for the users, while aiming at high resource utilization on the provider side to achieve economies of scale [21]. For instance, AWS Lambda is thought to be able to deploy 4\sim8 thousands of instances on a single host machine [22, 23]. This shows that FaaS would not be cost-effective without massive CPU sharing between users (i.e., concurrent functions), which means noisy neighbors and scheduling influence overall runtime and cost. The cloud provider charges the user for wall clock time instead of CPU time. In AWS Lambda [24], for example, billing for wall clock time, users are also billed for time spent waiting for external resources such as databases. For example, if a function is actively running on CPU for 1 millisecond and waiting 1 minute for an external database to return a query, AWS Lambda will bill for the whole 1 minute, not just the 1 millisecond CPU time. Recent studies [25, 26, 27, 28, 9, 29, 30] indicate that scheduling for serverless functions impacts the efficiency of FaaS systems. Given the characteristic of serverless functions, where the majority of the invocations are very short, on the order of seconds or less, the de facto standard Linux scheduler, the Completely Fair Scheduler (CFS) may lead to performance degradation due to context switches. Such frequent context switches introduce unnecessary overhead on execution times, as a consequence of costly state saving and restoration [31]. CFS aims to ensure fairness by assigning a minimum CPU time slice to every running task [32]. However, CFS might not be optimal for serverless function scheduling [25, 30, 33]. This is because it does not address the special nature of serverless computing, where many thousands of short-running tasks are running concurrently. A previous study [30] modifies CFS to give a higher priority to long-tail functions and achieves a 5{\sim}30\% reduction in latency. However, this is insufficient for significant cost reduction because CFS-induced longer running times lead to larger user-facing cost. Figure 1 provides evidence for how much extra cost is incurred via CFS in comparison to FIFO for different function memory sizes. For these first 12,442 function invocations of the Azure trace [13], CFS introduces more than 10 times extra cost compared to FIFO. It is nevertheless true that FIFO is also suboptimal because it increases queueing time and hence user-facing latency significantly. We therefore raise awareness and make a case for rethinking OS-level scheduling for serverless functions. Figure 1: Cost for FIFO and CFS OS scheduling policies calculated using AWS Lambda pricing. The workload is using the first 12,442 functions in the Microsoft Azure trace. Although FIFO cost is significantly lower, it introduces unacceptably large latencies for functions that simply wait in queues. We explore this trade-off in the remainder of the article. To explore the latency-cost trade-off, we implement a hybrid approach for serverless function scheduling by separating the scheduler into two distinct groups of CPU cores using ghOSt [34], a scheduling policy delegation system in user space. A group of CPUs are specialized for rapidly processing short-lived serverless functions. Long-running functions will be migrated from these specialized CPUs after a certain time limit to leave room for other short tasks. If functions finish before running out of time, they have been running without any interruption by context switches, reducing their execution time and cost. Functions not completed in the time limit are preempted and scheduled on another group of CPUs, designated for handling long-running functions (e.g., longer than 1 second). We evaluate our hybrid scheduler in two different modes of running functions: regular Linux processes and AWS Lambda invocations [24, 23]. This covers both common serverless operating use-cases: using containers and using (micro-) virtual machines. In our experiments, we showcase how this approach decreases the number of context switches for the short-lived functions and leads to a shorter duration, hence less user-facing cost. This comes at no additional overhead for the provider. In this paper, we raise awareness for the fact that scheduling in serverless is by far not a solved problem. We thereby present an initial solution for reducing cost in serverless via OS-level scheduling. Furthermore, our hybrid scheduler introduces a mechanism for adapting the FIFO time limit based on recent past function executions. Additionally, to avoid the CPUs being under-utilized, we introduce a mechanism that dynamically adapts the number of CPU cores designated for the two scheduling policies. We monitor the CPU cores utilization and move cores from one group to another if needed. In this way, better load balance and CPU utilization is achieved for both groups of CPU cores, which adheres to the interests of cloud infrastructure providers. In summary, we make the following contributions: 1. We showcase evidence that CFS is sub-optimal for serverless functions and increases the cost for serverless customers (Section II). 2. We design and implement a hybrid scheduler in user space using Google ghOSt [34] that divides the CPU cores into two groups and both groups are designated a separate scheduling policy. The function will be preempted from one core group to the other if it can not finish in a specific time limit (Section IV). 3. We design and implement a time limit adaptation and a rightsizing mechanism for the number of CPU cores to keep CPU utilization high (Section IV). 4. We showcase empirically that our scheduler improves the execution time of serverless functions as well as reduces cost on real-world platforms, such as Firecracker [23] (Section VI)."
https://arxiv.org/html/2411.08446v1,: Communication-efficient MoE Training via Locality-Sensitive Hashing,"Larger transformer models always perform better on various tasks but require more costs to scale up the model size. To efficiently enlarge models, the mixture-of-experts (MoE) architecture is widely adopted, which consists of a gate network and a series of experts and keep the training cost constant by routing the input data to a fixed number of experts instead of all. In existing large-scale MoE training systems, experts would be distributed among different GPUs for parallelization, and thus input data requires additional all-to-all communications to access the target experts and conduct corresponding computations. However, upon evaluating the training process of three mainstream MoE models on commonly used GPU clusters, we found that the all-to-all communication ratio averaged around 45%, which significantly hinders the efficiency and scalability of training MoE models.In this paper, we propose LSH-MoE, a communication-efficient MoE training framework using locality-sensitive hashing (LSH). We first present the problems of scaling MoE training in existing systems and highlight the potential of exploiting token similarity to facilitate data compression. Then, we introduce an efficient LSH-based compression technique, which utilizes the cross-polytope hashing for rapid clustering and implements a residual-based error compensation scheme to alleviate the adverse impact of compression. To verify the effectiveness of our methods, we conduct experiments on both language models (e.g., RoBERTa, GPT, and T5) and vision models (e.g., Swin) for pre-training and fine-tuning tasks. The results demonstrate that our method substantially outperforms its counterparts across different tasks by 1.28\times - 2.2\times of speedup.","In recent years, large-scale pre-trained models have significantly advanced the performance of deep learning across various complex tasks, including computer vision [8, 20], natural language processing [7, 28, 3], and multi-modal learning [19]. Commonly referred to as foundation models, these pre-trained models are primarily built on Transformer architectures [34] and undergo extensive pre-training on large datasets, utilizing substantial GPU resources. OpenAI has validated the scaling law for large language models [15] and suggests that increasing the model’s parameter size, the volume of training data, and the duration of training can significantly enhance the model’s performance. However, this approach results in a considerable rise in training costs, making the development of foundation models extremely expensive. To reduce the high computational costs, the sparse mixture-of-experts (MoE) architecture is often adopted, which comprises a sparse gate network and a series of expert networks. This architecture routes input data to only a subset of experts, resulting in sparse activation of the experts and thereby reducing the model’s computational FLOPs (float point operations) as well as training costs. Prominent models such as Google’s Switch-Transformer [9], ST-MoE [41], Meta’s Hash Layer [31] and Mistral-AI’s mixtral models [14] have successfully implemented this design, demonstrating improvements in both performance and efficiency with MoE models. Meanwhile, effectively scaling the training of MoE models across hundreds or even thousands of GPUs remains a significant challenge. Researchers from Google have proposed the expert parallelism approach [17], which replicates the gating network on each GPUs and distributes different experts across multiple GPUs for parallel processing. Specifically, each input token is initially processed by the gating network to select the appropriate expert, after which it is routed to the designated experts via peer-to-peer (P2P) network communication. Once the designated experts complete their computation, the token is returned to the original GPU for further processing through an additional P2P communication. Since each GPU typically needs to exchange data with many other GPUs, these P2P transmissions results in an all-to-all communication pattern. Moreover, because the computation of the expert network relies on the outcomes of these communications, the communications cannot be effectively overlapped with ongoing computations. This dependency creates a significant performance bottleneck in model training across most commonly used GPU clusters. We conducted experiments on three widely-used MoE models, including RoBERTa-MoE, GPT-MoE and Swin-MoE, on four A100 servers, each with a cross-machine bandwidth of 200Gb/s. The results, as shown in Figure 3, reveal that the time cost of all-to-all communication constitutes an average of 45\% and can reach up to 67\% of the total model training time. Existing methods to improve distributed MoE training on bandwidth-limited clusters tackle communication challenges in various ways. TA-MoE [4] reduces cross-machine communication by adjusting the gating network to favor experts on the same server, while Pre-gated MoE [13] reduces dependency between communication and computation through a pre-gating mechanism that plans token routing in advance. However, both approaches require modifications to the gating mechanism and model structure, limiting their universal applicability. DeepSpeed-MoE [29] introduces PR-MoE, which selects one expert plus a shared expert, halving the all-to-all communication load. SCoMoE [40] organizes all-to-all communication by structuring data transfers along different dimensions and controlling data volumes across network levels, and also clusters tokens to improve routing. However, none of these works consider reducing the All-to-All communication volume in MoE training by compressing the forward activations. Therefore, they can be intergrated with our method for further improvement. In this paper, we present LSH-MoE, a communication-efficient MoE training framework that leverages locality-sensitive hashing to group similar tokens. Our key contributions are as follows: • We begin by identifying key challenges in scaling MoE training in existing systems, noting that all-to-all communication constitutes an average of 45\% of the total training time. Additionally, we investigate the potential of using token similarity to facilitate data compression to reduce communication costs. • We propose an efficient LSH-based compression technique that employs cross-polytope hashing for rapid clustering. This approach transmits only the clustering centroids, significantly reducing communication costs. To further enhance accuracy, we implement a residual-based error compensation scheme to mitigate the negative effects of compression. • Through extensive experiments with language models (RoBERTa-MoE, GPT-MoE, and T5-MoE) and vision models (Swin-MoE), across both pre-training and fine-tuning tasks, we demonstrate that our method maintains model quality while achieving a speedup of 1.28\times - 2.2\times in end-to-end training time."
https://arxiv.org/html/2411.08434v1,"Anonymous Distributed Localisation via
Spatial Population Protocols","In the distributed localization problem (DLP), n anonymous robots (agents) {A_{0}},\dots,{A_{n-1}} begin at arbitrary positions {p_{0}},\dots,{p_{n-1}}\in S, where S is a Euclidean space. Initially, each agent {A_{i}} operates within its own coordinate system in S, which may be inconsistent with those of other agents. The primary goal in DLP is for agents to reach a consensus on a unified coordinate system that accurately reflects the relative positions of all points, {p_{0}},\dots,{p_{n-1}}, in S. Extensive research on DLP has primarily focused on the feasibility and complexity of achieving consensus when agents have limited access to inter-agent distances, often due to missing or imprecise data. In this paper, however, we examine a minimalist, computationally efficient model of distributed computing in which agents have access to all pairwise distances, if needed. Specifically, we introduce a novel variant of population protocols, referred to as the spatial population protocols model. In this variant each agent can memorise one or a fixed number of coordinates, and when agents {A_{i}} and {A_{j}} interact, they can not only exchange their current knowledge but also either determine the distance d_{{i}{j}} between them in S (distance query model) or obtain the vector \overrightarrow{v_{{i}{j}}} spanning points {p_{i}} and {p_{j}} (vector query model).We examine three DLP scenarios, proposing and analysing several types of distributed localisation protocols, including:Self-stabilising localisation protocol with distance queries We propose and analyse self-stabilising localisation protocol based on pairwise distance adjustment. We also discuss several hard instances in this scenario, and suggest possible improvements for the considered protocol,Leader-based localisation protocol with distance queries We propose and analyse several leader-based protocols which stabilise in o(n) parallel time. These protocols rely on efficient solution to multi-contact epidemic, an extension of one-way epidemic in population protocols, andSelf-stabilising localisation protocol with vector queries We propose and analyse superfast self-stabilising DLP protocol which stabilises in O(\log n) parallel time.We conclude with a discussion on future research directions for distributed localization in spatial population protocols, including scenarios that account for higher dimensions, limited precision and susceptibility to errors.","Location services are crucial for modern computing paradigms, such as pervasive computing and sensor networks. While manual configuration and GPS can determine node locations, these methods are impractical in large-scale or obstructed environments. Recent approaches use network localisation, where beacon nodes with known positions enable other nodes to estimate their locations via distance measurements. Key challenges remain, including determining conditions for unique localisability, computational complexity, and deployment considerations. In the distributed localisation problem (DLP), n anonymous robots (agents) {A_{0}},\dots,{A_{n-1}} begin at arbitrary positions {p_{0}},\dots,{p_{n-1}}\in S, where S is a Euclidean space. Initially, each agent {A_{i}} operates within its own coordinate system in S, which may be inconsistent with those of other agents. The primary goal in DLP is for agents to reach a consensus on a unified coordinate system that accurately reflects the relative positions of all points, {p_{0}},\dots,{p_{n-1}}, in S. A network of agents’ unique localisability is determined by specific combinatorial properties of its graph and the number of anchors (agents aware of their real location). For example, graph rigidity theory [13, 17, 18] provides a necessary and sufficient condition for unique localisability [13]. Specifically, a network of agents located in the plane is uniquely localisable if and only if it has at least three anchors and the network graph is globally rigid. However, unless a network is dense and regular, global rigidity is unlikely. Even without global rigidity, large portions of a network may still be globally rigid, though positions of remaining nodes will remain indeterminate due to multiple feasible solutions. The decision version of this problem, often referred to as the graph embedding or graph realisation problem, requires determining whether a weighted graph can be embedded in the plane so that distances between adjacent vertices match the edge weights, a problem known to be strongly NP-hard [26]. Furthermore, this complexity holds even when the graph is globally rigid [13]. In sensor networks, where nodes measure distances only within a communication range r, the network is best represented as a unit disk graph. Here, two nodes are adjacent if and only if their distance is \leq r. The corresponding decision problem, known as unit disk graph reconstruction, requires determining whether a graph can be embedded in the plane such that distances between adjacent nodes match edge weights, while distances between non-adjacent nodes exceed r. This problem is also NP-hard [6], indicating that no efficient algorithm can solve localisation in the worst case unless P=NP. Furthermore, even for instances with unique reconstructions, no efficient randomised algorithm exists to solve this problem unless RP=NP [6]. Distributed localisation is also crucial in robotic systems, enabling robots to autonomously determine their spatial position within an environment – a fundamental requirement for applications such as navigation, mapping, and multi-robot coordination [30]. Accurate localisation allows robots to interact more effectively with their surroundings and with each other, facilitating tasks from autonomous driving to warehouse automation and search-and-rescue operations [24, 22]. Localisation approaches generally fall into two broad categories: centralised and distributed systems [31]. Centralised localisation systems, where a central server or leader node computes the locations of all robots, can offer high accuracy but may struggle with scalability and robustness, especially in dynamic or communication-limited environment [21]. In contrast, distributed localisation systems allow each robot to perform localisation computations independently or in collaboration with neighbouring robots, enhancing adaptability and resilience, although this may come at the cost of increased complexity [20, 19]. Within distributed systems, leader-based localisation mechanisms involve one or more designated robots that serve as reference points or coordinators for localisation [14], which can streamline computations but may create single points of failure. Leaderless localisation, where all robots contribute equally to position estimation without relying on specific leader nodes, is advantageous in decentralised applications where flexibility and fault tolerance are paramount [19, 27]. Both methods have been explored using probabilistic [29], geometric [23], and graph-based models [19], with leaderless approaches gaining traction due to their robustness in large-scale and dynamically changing settings. Various methods leverage tools such as Kalman filters [25], particle filters [21], and graph rigidity theory [28] to enhance localisation accuracy and efficiency in complex environments. 1.1 Spatial population protocols In this paper, we explore a minimalist, computationally efficient model of distributed computing, where agents have probabilistic access to pairwise distances. Our focus is on achieving anonymity while maintaining high time efficiency and minimal use of network resources, including limited local storage (agent state space) and communication. To meet these goals, we introduce a new variant of population protocols, referred to as the spatial population protocols model, specified later in this section. The population protocol model originates from the seminal work by Angluin et al. [4]. This model provides tools for the formal analysis of pairwise interactions between indistinguishable entities known as agents, which have limited storage, communication, and computational capabilities. When two agents interact, their states change according to a predefined transition function, a core component of the population protocol. It is typically assumed that the agents’ state space is fixed and that the population size n is unknown to the agents and is not hard-coded into the transition function. In self-stabilising protocols, the initial configuration of agents’ states is arbitrary. By contrast, non-self-stabilising protocols start with a predefined configuration encoding the input of the given problem. A protocol concludes when it stabilises in a final configuration of states representing the solution. In the probabilistic variant of population protocols, which is also used here, the random scheduler selects an ordered pair of agents at each step—designated as the initiator and the responder—uniformly at random from the entire population. The asymmetry in this pair introduces a valuable source of random bits, which is utilised by population protocols. In this probabilistic setting, besides efficient state utilisation, time complexity is also a primary concern. It is often measured by the number of interactions, {\cal I}, required for the protocol to stabilise in a final configuration. More recently, the focus has shifted to parallel stabilisation time (or simply time), defined as {\cal I}/n, where n is the population size. This measure captures the parallelism of independent, simultaneous interactions, which is leveraged in efficient population protocols that stabilise in time O(\text{poly}\log n). All protocols presented in sections 3 and 4 are stable (always correct) and guarantee stabilisation time with high probability (whp), defined as 1-n^{-\eta} for a constant \eta>0. Leader election is a fundamental problem in distributed computing, essential for symmetry breaking, synchronisation, and coordination mechanisms. In population protocols, the presence of a leader enables a more efficient computational setup [5], as further utilised in Section 3. However, achieving leader election in this model poses significant challenges. Foundational results [10, 11] demonstrate that it cannot be solved in sublinear time if agents are restricted to a fixed number of states [12]. Further, Alistarh and Gelashvili [3] introduced a protocol stabilising in O(\log^{3}n) time with O(\log^{3}n) states. Later, Alistarh et al. [1] identified trade-offs between state use and stabilisation time, distinguishing slowly (o(\log\log n) states) and rapidly stabilising (O(\log n) states) protocols. Subsequent work achieved O(\log^{2}n) time whp and in expectation with O(\log^{2}n) states [9], later reduced to O(\log n) states using synthetic coins [2, 8]. Recent research by Gąsieniec and Stachowiak reduced state usage to O(\log\log n) while retaining O(\log^{2}n) time whp [15]. The expected time of leader election was further optimised to O(\log n\log\log n) by Gąsieniec et al. in [16] and to the optimal time O(\log n) by Berenbrink et al. in [7]. Spatial embedding and geometric queries While population protocols provide an elegant and resilient framework for randomised distributed computation, they lack spatial embedding. To address this limitation, we introduce a new spatial variant of population protocols that extends the transition function to include basic geometric queries. In particular, in this model each agent can memorise one or a fixed number of coordinates, and during an interaction of two agents {A_{i}} and {A_{j}}, in addition to exchange of their current knowledge, the agents can determine: (1) the distance d_{{i}{j}} separating them in S, in distance query model, and (2) vector \overrightarrow{v_{{i}{j}}} spanning points {p_{i}} and {p_{j}}, in vector query model. Our contribution Using the example of the distributed localisation problem, we show that the adopted model provides a natural framework for developing efficient randomised solutions to distributed problems with geometric embeddings, including those in anonymous and self-stabilising settings. We examine three DLP scenarios, proposing and analysing different types of distributed localisation protocols, including: 1. Self-stabilising localisation protocol with distance queries We propose and analyse self-stabilising protocol based on pairwise distance adjustment. We also discuss some hard instances in this scenario, and suggest possible improvements for the considered protocol, see Section 2. 2. Leader-based localisation protocol with distance queries We propose and analyse several leader-based protocols which stabilise in sublinear parallel time. These protocols rely on a novel concept of multi-contact epidemic, an natural extension of one-way epidemic, see Section 3. 3. Self-stabilising localisation protocol with vector queries We propose and analyse a super fast self-stabilising protocol which stabilises in O(\log n) parallel time in this scenario, see Section 4. We conclude with a discussion on future research directions for distributed localisation in spatial population protocols, including scenarios that account for limited precision and susceptibility to errors."
https://arxiv.org/html/2411.08616v1,"Multiplexed bi-layered realization of fault-tolerant quantum computation 
over optically networked trapped-ion modules","We study an architecture for fault-tolerant measurement-based quantum computation (FT-MBQC) over optically-networked trapped-ion modules. The architecture is implemented with a finite number of modules and ions per module, and leverages photonic interactions for generating remote entanglement between modules and local Coulomb interactions for intra-modular entangling gates. We focus on generating the topologically protected Raussendorf-Harrington-Goyal (RHG) lattice cluster state, which is known to be robust against lattice bond failures and qubit noise, with the modules acting as lattice sites. To ensure that the remote entanglement generation rates surpass the bond-failure tolerance threshold of the RHG lattice, we employ spatial and temporal multiplexing. For realistic system timing parameters, we estimate the code cycle time of the RHG lattice and the ion resources required in a bi-layered implementation, where the number of modules matches the number of sites in two lattice layers, and qubits are reinitialized after measurement. For large distances between modules, we incorporate quantum repeaters between sites and analyze the benefits in terms of cumulative resource requirements. Finally, we derive and analyze a qubit noise-tolerance threshold inequality for the RHG lattice generation in the proposed architecture that accounts for noise from various sources. This includes the depolarizing noise arising from the photonically-mediated remote entanglement generation between modules due to finite optical detection efficiency, limited visibility, and presence of dark clicks, in addition to the noise from imperfect gates and measurements, and memory decoherence with time. Our work thus underscores the hardware and channel threshold requirements to realize distributed FT-MBQC in a leading qubit platform today—trapped ions.","Over the past decade, significant progress has been made in conceptualizing scalable, fault-tolerant quantum computing architectures [1, 2, 3]. However, realizing these architectures with current technology remains a formidable challenge. An effective architecture must not only employ quantum error-correcting codes that can withstand high loss and noise error thresholds to maintain a low error probability per logical gate, but must also enable the implementation of a universal set of logic gates. Additionally, it should have minimal spatial and temporal overheads, and process error information efficiently in parallel with the quantum computation [4, 5] A widely pursued approach to quantum computing is measurement-based quantum computation (MBQC) [6, 7, 8], where the computation proceeds through single-qubit measurements on a large entangled resource cluster state with feedforward. Fault-tolerant (FT)-MBQC robust to qubit loss and noise errors can be achieved using 3D cluster states, where logical qubits are encoded, e.g., in topologically degenerate states of non-Abelian anyons, and manipulated through braiding [9]. These 3D cluster states can be realized in bosonic systems such as optical modes [10, 11], and matter-based systems such as trapped ions [12] and negatively charged nitrogen vacancy (NV-) centers in diamond [1]. The main challenge lies in realizing large multi-qubit systems with accurate control [13] to host the cluster states. A viable option to tackling this problem is to optically link small few-qubit modules to scale up the system [14]. Cluster state generation in such a modular distributed setting where the modules occupy the cluster lattice sites typically involves generating remote entanglement between qubits across modules and connecting them via local entangling gates and measurements [15]. Along these lines, Monroe et al. [2] proposed a modular universal scalable ion trap quantum computer (MUSIQC) architecture, and showed that fault-tolerant quantum computing can be achieved in small-scale trapped ion modules in the presence of both fast and slow remote entanglement generation between modules. The architecture involves creating the 3D cluster state called the Raussendorf-Harrington-Goyal lattice [7] (up to local Hadamard gates on the edge qubits) by preparing Bell states between trapped ion modules (qubits therein) present at the lattice sites along the lattice edges and fusing them at the modules by performing CNOT gates followed by local Pauli measurements. The process of generating a unit cell of a 3D cluster state can be extended along all three spatial axes to construct as large a cluster state as required to perform large-scale quantum computations over multiple logical qubits and of arbitrary circuit depth. Ref. [12] demonstrated that the RHG lattice cluster state can in fact be realized with a 2D hexagonal array of trapped ion modules. This array unfolds into two layers of the RHG lattice, which, when reinitialized and measured repeatedly, form the RHG lattice cluster state over time, removing the need for spatial construction. Quantum computation then proceeds by teleporting information between these two layers. This approach has been analyzed for quantum computing over NV- centers in diamond in Ref. [1]. Remote entanglement generation is a crucial step in realizing the MUSIQC architecture. It is achieved via ion-photon entangled pair generation, followed by probabilistic linear optical Bell-state measurements over a pair of such photons for entanglement swapping, resulting in an ion-ion Bell pair. This process is inherently stochastic [16]. Failures lead to a defective RHG lattice with missing bonds, which, in the absence of measurement errors, can tolerate up to a 6.5\% heralded bond failure rate [17] (where heralded means that the locations of the missing bonds are known) in order for the system to remain fault tolerant. An adaptive scheme, in which qubits are measured in an alternative basis, improves this threshold to 14.5\%, mitigating the adverse impact of missing bonds [18]. However, remote Bell pair generation via passive linear optics and unentangled ancillae typically results in bond failure rates exceeding 25\% [17]. Practical implementations of photonically-mediated remote entanglement generation are typically also noisy, which add to the noise from other sources such as gate and measurement infidelities and memory decoherence with time, on the RHG lattice qubits. In the absence of bond failure errors, the overall qubit noise tolerance threshold of the RHG lattice was derived in Ref. [2]. In this work, we consider a bi-layered realization of the RHG lattice akin to the architecture of Ref. [12]. We incorporate spatial and time multiplexing to boost the success rate of remote Bell pair generation between modules. We determine the multiplexing and the associated ion resource requirements at the modules to operate within the bond failure tolerance threshold for FT-MBQC, including the case where the channel between modules is interspersed with quantum repeaters. We consider a realistic model for photonic interactions in remote entanglement generation based on single-photon dual-rail qubits [19], which produces noisy Bell pairs between ions across modules. We derive a noise-tolerance threshold inequality that factors in noise arising from the optical detection process in remote entanglement generation, along with noise from gate and measurement infidelities, and memory decoherence with time. We analyze the inequality over the space of the various relevant system parameters to identify valid operating regions in the parameter space for FT-MBQC. More precisely, the main contributions of our work include the following: • We outline the approach to generate the RHG lattice with just as many trapped ion modules as the number of sites in two layers of the lattice based on re-initializing the qubits therein post measurement in MBQC and space-and-time multiplexed remote entanglement generation between modules. • We derive the time required to complete one code cycle in FT-MBQC, accounting for various system timing parameters. Additionally, we calculate the ion resources required at the modules to support the multiplexed remote entanglement generation at a failure rate below the maximum bond failure tolerance rate threshold of the RHG lattice. • To address large distances between modules, we incorporate quantum repeaters and evaluate its impact on performance, as well as the associated ion resource requirements. • We consider noisy remote entanglement between modules, represented by two-qubit mixed entangled states, generated by considering realistic and imperfect single-photon dual-rail qubit-mediated interactions. The infidelity arises from factors such as limited channel transmissivity, detector noise, and dark clicks. Combining it with noise from imperfect gates and measurements, and memory decoherence over the course of generation of the RHG lattice, and comparing the overall noise to the maximum noise tolerance threshold of the RHG lattice, we derive a threshold inequality. Using the inequality, we identify valid regions in the parameter space where the threshold is met. In Sec. II, we briefly review ion trap quantum computing. In Sec. III, we discuss FT-MBQC, also referred to as topological cluster state quantum computation (TCQC) and its two layer realization. In particular, we discuss the RHG lattice cluster state as an illustrative example of a topologically protected cluster state, its fault tolerance, and bi-layered realization. In Sec. IV, we discuss the bi-layered realization of the RHG lattice cluster state over optically networked trapped ion modules, and the use of spatial and time multiplexed remote entanglement generation between modules to deal with bond failures. Here, we also discuss the role of quantum repeaters when the lattice sites (modules) are widely separated. In Sec. V, for the same setting of optically networked trapped ion modules, we consider a realistic model for the photon-mediated remote entanglement generation resulting in imperfect Bell pairs between modules. We discuss the implications therein due to the noise tolerance rate threshold of the RHG lattice. We conclude in Section VI with a summary."
https://arxiv.org/html/2411.08374v1,Federated Graph Learning with Graphless Clients,"Federated Graph Learning (FGL) is tasked with training machine learning models, such as Graph Neural Networks (GNNs), for multiple clients, each with its own graph data. Existing methods usually assume that each client has both node features and graph structure of its graph data. In real-world scenarios, however, there exist federated systems where only a part of the clients have such data while other clients (i.e. graphless clients) may only have node features. This naturally leads to a novel problem in FGL: how to jointly train a model over distributed graph data with graphless clients? In this paper, we propose a novel framework FedGLS to tackle the problem in FGL with graphless clients. In FedGLS, we devise a local graph learner on each graphless client which learns the local graph structure with the structure knowledge transferred from other clients. To enable structure knowledge transfer, we design a GNN model and a feature encoder on each client. During local training, the feature encoder retains the local graph structure knowledge together with the GNN model via knowledge distillation, and the structure knowledge is transferred among clients in global update. Our extensive experiments demonstrate the superiority of the proposed FedGLS over five baselines.","Recent years have witnessed a growing development of graph-based applications in a wide range of high-impact domains. As a powerful deep learning tool for graph-based applications, Graph Neural Networks (GNNs) exploit abundant information inherent in graphs (Wu et al., 2020) and show superior performance in different domains, such as node classification (Fu et al., 2023; He et al., 2022) and link prediction (Tan et al., 2023a; Zhang & Chen, 2018). Traditionally, GNNs are trained over graph data stored on a single machine. In the real world, however, multiple data owners (i.e., clients) may have their own graph data and hope to jointly train GNNs over their graph data. The challenge in this scenario is that the graph data is often not allowed to be collected from different places to one single server for training due to the emphasis on data security and user privacy (Voigt & Von dem Bussche, 2017; Wang et al., 2024b). Considering a group of hospitals, for instance, each of them has its local dataset of patients. These hospitals hope to collaboratively train GNNs for patient classification tasks (e.g., predicting whether a patient is at high risk of contracting a contagious disease) while keeping their private patient data locally due to strict privacy policies and commercial competition. To tackle the above challenge, Federated Learning (FL) (McMahan et al., 2017) enables collaborative training among clients over their private graph data orchestrated by a central server. Typically, FL can be categorized into horizontal and vertical FL based on how data is distributed among clients (Yang et al., 2019). This study focuses on horizontal FL where distributed datasets share the same feature space. During each training round, the selected clients receive the global model from the central server and perform local updates over their local graph data. The central server aggregates the updated local models from the clients and computes the new global model for the next training round. Numerous studies have been proposed to improve FL performance over graph data by reconstructing cross-client information (Zhang et al., 2021b; a), aligning overlapping instances (Peng et al., 2021; Zhou et al., 2022), and mitigating data heterogeneity (Xie et al., 2021; Tan et al., 2023b; Fu et al., 2024). Figure 1: An example of a healthcare system including four hospitals. In this example, Hospital A and Hospital D have their local datasets of patients (node features) and co-staying information (links) among them. In the meantime, Hospital B and Hospital C only have their local datasets of patients (node features). The four hospitals aim to jointly train a model for predicting whether a patient is at high risk of contracting a contagious disease, orchestrated by a third-party company over their local datasets while the company cannot directly access their private datasets. The above methods rely on a fundamental assumption that each client has graph structure information of its local graph data. In the real world, however, this assumption may not be feasible for all the clients. Instead, there may be a part of clients only having local node features, whereas other clients have both node features and edge information. To illustrate this scenario in practice, we provide a real-world example as follows. More practical examples can be found in Appendix A. Motivating example. Considering the aforementioned example of a healthcare system as shown in Figure 1, we may construct local graphs in each hospital by taking patient demographics as node features and co-staying in a ward as edges. In the real world, however, some hospitals may not record the co-staying information and cannot construct patient graphs. As a result, these hospitals are unable to directly train GNNs to predict whether a patient is at high risk of contracting a contagious disease in a federated manner. If we instead train a machine learning model only based on patient features, the classification performance will be unsatisfactory because we disregard the important co-staying information between patients, which can significantly determine the risk of contracting a contagious disease for a patient. The above scenario brings us a novel problem in the federated setting: how to jointly train a GNN model for the classification task from isolated graphs distributed in multiple clients while some clients only have node features? In this paper, we name such clients as graphless clients. Since directly training GNNs is obviously infeasible in this setting, collaboratively training non-GNN-based models such as multi-layer perceptrons (MLPs) and Support Vector Machines (SVMs) is a plausible solution to the above problem. However, a number of experiments in prior works have demonstrated that the non-GNN-based models are typically less accurate than GNNs for the classification task (Franceschi et al., 2019; Zhang et al., 2022). Another intuitive method is to let a graphless client construct graph structure based on the similarity of the features (e.g., using kNN (Gidaris & Komodakis, 2019)) and jointly train GNNs together with other clients. A disadvantage of this method is that the generated graphs are only dependent on node features and are not suitable for node classification (Franceschi et al., 2019; Liu et al., 2022). To overcome the disadvantage, it is natural to let the graphless clients produce graph structures with the structure knowledge of other clients. However, structure knowledge transfer and utilization in a federated manner are still challenging and unexplored. In this study, we propose a novel framework FedGLS to handle FGL with graphless clients. FedGLS aims to solve two key challenges of utilizing structure knowledge in this scenario: 1) how to transfer structure knowledge among clients; and 2) how to utilize the transferred knowledge on graphless clients? In FedGLS, we first design two modules - a GNN model and a feature encoder on each client. In particular, we deploy a third module - a local graph learner on each graphless client. The GNN model learns node embeddings over the local graph and the feature encoder approximates the output of the GNN model via knowledge distillation (Hinton et al., 2015). Therefore, the GNN model and the feature encoder together retain structure knowledge on each client. The central server collects the local parameters of the two modules from the clients and gets the global parameters. In this way, FedGLS transfers structure knowledge among clients. The local graph learner utilizes the structure knowledge by maximizing the consistency between the output of the global GNN model and feature encoder on each graphless client with a contrastive loss. We conduct extensive experiments over five datasets, and the results show that FedGLS outperforms other baselines. Our contributions can be summarized as follows. • Problem Formulation. We propose a novel research problem of FGL with graphless clients and provide the formal definition of the proposed problem. • Algorithm Design. We propose FedGLS to tackle the proposed problem. We devise a scheme for transferring structure knowledge among clients in FedGLS by letting a feature encoder imitate the node embeddings from a GNN model. The scheme enables a graph learner on each graphless client to learn its local graph structure with the structure knowledge transferred from other clients. • Experimental Evaluations. We conduct extensive experiments on real-world datasets, and the results validate the superiority of our proposed FedGLS against five baselines. Our implementation of FedGLS is available in the supplementary materials."
https://arxiv.org/html/2411.07752v1,ALANINE: A Novel Decentralized Personalized Federated Learning For Heterogeneous LEO Satellite Constellation,"Low Earth Orbit (LEO) satellite constellations have seen significant growth and functional enhancement in recent years, which integrates various capabilities like communication, navigation, and remote sensing. However, the heterogeneity of data collected by different satellites and the problems of efficient inter-satellite collaborative computation pose significant obstacles to realizing the potential of these constellations. Existing approaches struggle with data heterogeneity, varing image resolutions, and the need for efficient on-orbit model training. To address these challenges, we propose a novel decentralized PFL framework, namely, A Novel DecentraLized PersonAlized Federated Learning for HeterogeNeous LEO SatellIte CoNstEllation (ALANINE). ALANINE incorporates decentralized FL (DFL) for satellite image Super Resolution (SR), which enhances input data quality. Then it utilizes PFL to implement a personalized approach that accounts for unique characteristics of satellite data. In addition, the framework employs advanced model pruning to optimize model complexity and transmission efficiency. The framework enables efficient data acquisition and processing while improving the accuracy of PFL image processing models. Simulation results demonstrate that ALANINE exhibits superior performance in on-orbit training of SR and PFL image processing models compared to traditional centralized approaches. This novel method shows significant improvements in data acquisition efficiency, process accuracy, and model adaptability to local satellite conditions.","Over the past few decades, there has been a significant increase in the number of satellites launched into Low Earth Orbit (LEO), along with a considerable improvement in the complexity of their functions. Initially dedicated to specific tasks such as communication, navigation, and remote sensing, modern LEO satellites now integrate these functions to provide comprehensive services [1]. Additionally, the expansion in satellite numbers within constellations and improvements in computational capabilities have substantially boosted their on-orbit computational power, which enables more complex data processing tasks. Despite these advancements, managing the variability in data types and volumes collected by different satellites poses a considerable obstacle. The development of efficient inter-satellite collaborative computation methods is essential to fully utilize on-orbit computational resources for real-time data processing, which remains a critical bottleneck in realizing the potential of LEO satellite constellations [2]. This integration of diverse functionalities necessitates innovative solutions to synchronize and harmonize the operations among satellites, which ensures that data relay and processing can be achieved seamlessly across the constellation. In the context of heterogeneous LEO satellite constellations, characterized by differences in LEO satellite types and orbits, the obstacles previously discussed become even more pronounced [3]. The diversity in satellite specifications and orbital paths introduces new complexities in the coordination and scheduling of task transmissions. Typically, satellites engage in data exchange among themselves for the purpose of data processing. However, the increase in data volumes combined with the high velocity of satellite orbits results in compromised data transmission quality. Frequent data transfers exacerbate the situation, which leads to intense competition for limited satellite communication bandwidth [4]. This competition not only degrades the quality of data-processing results but also increases the latency in communication, further complicating real-time data analysis tasks. Efficient data processing emerges as a critical concern within the satellite constellation, and Federated Learning (FL) offers a promising solution as an innovative form of distributed processing technology [5]. By processing data locally on each satellite, FL necessitates only the transmission of model parameters, rather than raw data. This approach not only significantly reduces the volume of data transferred, but also enhances process efficiency and protects against potential data theft and tampering during transmission. Nevertheless, challenges arise within the framework of heterogeneous LEO satellite constellations. Traditional FL can lead to severe communication congestion and intense bandwidth competition between satellites and servers, which may adversely affect the overall performance of Machine Learning (ML) models. To address these issues, decentralized FL (DFL) supports dynamic and direct interactions between satellites, thus alleviating communication bottlenecks [6]. Furthermore, the variability in data quality collected by different satellites complicates the application of these data in local model training and global model aggregation, often leading to suboptimal model accuracy [7]. The deployment of a globally trained model to individual satellites for further local training may not always yield optimal results. This suboptimal performance can occur when the model fails to align adequately with the specific characteristics of local datasets, potentially leading to ineffective localized training effects [8]. To mitigate the issue of uneven data distribution, personalized FL (PFL) allows each satellite to employ a personalized model tailored to its local data distribution, rather than strictly adhering to a global model [9]. This adaptation enables more effective and efficient learning results tailored to specific local conditions. To address the challenges and build upon the motivations discussed in the previous discussions, we propose ALANINE, a novel decentralized PFL framework specifically designed for heterogeneous LEO satellite constellations. ALANINE employs image Super Resolution (SR) to enhance the quality of data acquired by satellites and utilizes DFL to facilitate on-orbit training of the SR model. Furthermore, it leverages inter-satellite links to transmit the trained SR model. Each satellite applies the trained SR model to improve the quality of its local data before engaging in PFL. Subsequently, decentralized PFL methods are used for training FL image processing models. Model pruning is employed to tailor the global model to the specific conditions of each satellite local dataset, thus enhancing the adaptability and usability of the global model in local training. ALANINE enables training of on-orbit DFL satellite image SR and PFL image processing models, which achieves efficient data acquisition and processing while improving the accuracy of PFL image processing models. The main contributions of this paper are as follows. • Considering the data heterogeneity of LEO satellites, we propose a novel decentralized PFL framework for heterogeneous LEO satellite constellation, specifically designed to manage the unique data heterogeneity of each satellite. This approach allows for the efficient aggregation and personalized training of local models, significantly improving the integration and effectiveness of global models. • To address the varing image resolutions that affect the training accuracy of satellite-based models, we implement a DFL method for satellite image SR. This innovative method significantly enhances the resolution of input data, thus directly improving the training accuracy, reliability, and robustness of models under varing image quality conditions, crucial for accurate remote sensing analysis. • To enhance the training efficiency of PFL image processing models in satellite constellations, our approach employs advanced model pruning during both the local training and global aggregation phases. This strategy effectively reduces model complexity, increases transmission efficiency, and allows quicker and more adaptive responses to the unique challenges of local satellite environments. These optimizations are crucial for maintaining high-performance PFL models in the resource-constrained and demanding contexts of satellite constellations. The rest of this paper is organized as follows: In Section 2, we introduce the related work. Section 3 presents the system model. Section 4 describes a DFL approach for satellite image SR, utilizes model pruning to enhance model efficiency, and develops a decentralized PFL architecture. The performance of our framework is evaluated in Section 5. Section 6 concludes the work. Figure 1: Illustration of the Walker Star satellite constellation."
https://arxiv.org/html/2411.07628v1,A Framework for Carbon-aware Real-Time Workload Management in Clouds using Renewables-driven Cores,"Cloud platforms commonly exploit workload temporal flexibility to reduce their carbon emissions. They suspend/resume workload execution for when and where the energy is greenest. However, increasingly prevalent delay-intolerant real-time workloads challenge this approach. To this end, we present a framework to harvest green renewable energy for real-time workloads in cloud systems. We use Renewables-driven cores in servers to dynamically switch CPU cores between real-time and low-power profiles, matching renewable energy availability. We then develop a VM Execution Model to guarantee running VMs are allocated with cores in the real-time power profile. If such cores are insufficient, we conduct criticality-aware VM evictions as needed. Furthermore, we develop a VM Packing Algorithm to utilize available cores across the data center. We introduce the Green Cores concept in our algorithm to convert renewable energy usage into a server inventory attribute. Based on this, we jointly optimize for renewable energy utilization and reduction of VM eviction incidents. We implement a prototype of our framework in OpenStack as openstack-gc. Using an experimental openstack-gc cloud and a large-scale simulation testbed, we expose our framework to VMs running RTEval, a real-time evaluation program, and a 14-day Azure VM arrival trace. Our results show: i) a 6.52\times reduction in coefficient of variation of real-time latency over an existing workload temporal flexibility-based solution, and ii) a joint 79.64\% reduction in eviction incidents with a 34.83\% increase in energy harvest over the state-of-the-art packing algorithms. We open source openstack-gc at https://github.com/tharindu-b-hewage/openstack-gc.","Data centers consumed approximately 1-1.3% of global electricity demand in 2022 [1]. Between 2015 and 2022, data center energy usage increased by 20-70% [1]. The recent unprecedented compute demand due to Artificial Intelligence (AI) and Machine Learning (ML) workloads indicates that this trend will continue to grow [2]. Data centers often connect to electricity grids with shares of energy generation based on fossil fuels. As a result, in 2020, data centers were responsible for 0.9% of energy-related greenhouse gas (GHG) emissions [1]. Climate crisis-driven road maps necessitate that data center emissions drop by half by 2030 to meet global Net Zero Emissions goals [1, 2]. In response to GHG emissions, electrical grids continue to integrate low-emission renewable energy sources. In 2022, the share of renewables in total electricity generation was 39% and is projected to be 91% by 2035 [3]. However, growing variable-availability (intermittent) renewable energy sources, such as solar and wind, challenge electrical grids [2]. Between 2022 and 2035, energy reports project the share of solar and wind renewables in total generation to rise from 12% to 58% [3]. Data centers develop various load matching strategies to match workload execution over intermittent renewable energy. Amongst them, load shifting is commonly practised [4, 5, 6, 7]. Load shifting uses workloads with temporal flexibility to suspend/resume their execution. For example, Google’s delay-tolerant workloads, such as machine learning, data compaction, and data processing, tolerate delays as long as their work gets completed within 24 hours [6]. Workloads execute in periods when renewable energy capacity is higher, resulting in reduced GHG emissions. However, load shifting falls short when applied to real-time workloads with strict response time boundaries [8]. Real-time workloads cannot tolerate the delays inherent in load shifting. Nevertheless, the growing prevalence of real-time cloud applications, such as autonomous vehicles, industrial automation [9], and railway control systems [10] expects to account for nearly 30% of the world data by 2025 [11]. As a result, cloud operators will eventually have to incorporate growing real-time workloads in intermittent renewable energy integration. In this context, one must find an alternative load matching strategy to load shifting for delay-intolerant real-time workloads. Existing solutions, such as applying CPU-wide low power profile to match renewable energy supply [12], often result in increased latency, making them unsuitable for real-time applications. Moreover, techniques like Harvest Virtual Machines (HVMs), which allow uninterrupted execution of workloads with reduced resources [13], can still degrade performance and fail to meet real-time constraints. Given these challenges, there is a need for an efficient strategy to integrate renewable energy into real-time cloud systems (Real-Time Clouds). To this end, we propose a framework to harvest renewable energy in Real-Time Clouds. We use Renewables-driven cores to integrate renewable energy at the server level. It dynamically switches the power profiles of each CPU core between a real-time power profile and a low power profile to match renewable energy intermittency. Then, our framework applies a twofold solution to utilize this dynamic core availability. First, we develop a VM Execution Model to guarantee that real-time virtual machines (VMs) occupy cores at the real-time power profile. Our model adopts renewable energy fluctuations by conducting criticality-aware VM evictions as needed. Secondly, we develop a VM Packing Algorithm to optimize the use of available cores across the data center. It reduces the likelihood of VM evictions while maximizing renewable energy utilization (renewable energy harvest). Our algorithm frames renewable energy management as a VM placement optimization problem by introducing the concept of Green Cores. Green Cores presents each server as an inventory of two virtual CPU core types: Green and Regular. Green cores quantify renewable energy usage, whereas Regular cores quantify core usage that does not increase risks of VM eviction incidents. Using Green Cores, we achieve a computationally inexpensive VM packing algorithm, which is required to handle VM throughput at the data center level [6]. We implement our framework in OpenStack [14] as openstack-gc. We combine OpenStack’s control plane with an on-node daemon service. The daemon service implements Renewables-driven cores in the server using per-core sleep states. openstack-gc’s control plane then communicates with the daemon service to orchestrate our VM Execution Model and VM Packing Algorithm. We evaluate our framework at the server level using VMs running RTEval, a program from the Real-Time Linux project to measure real-time performance [15]. We evaluate our framework at the data center level using a 14-day VM arrival trace from Azure [16]. We use two testbeds: an experimental openstack-gc cloud deployed on an HPE ProLiant server with a 12-core Intel Xeon CPU, and a large-scale simulation testbed. We make the following contributions in designing, implementing, and evaluating our framework. • We propose a server level VM Execution Model to utilize renewable energy without degrading real-time latency performance in VMs. We leverage criticality-aware VM evictions for that. • We propose a data center level VM Packing Algorithm to reduce VM eviction incidents over renewable energy utilization. • We implement a prototype of our framework in OpenStack, detailing its design and demonstrating its practicality. • We evaluate our approach against multiple baselines. Our results show: i) 6.52\times reduction in coefficient of variation of real-time latency in VMs over the existing workload temporal flexibility-based VM execution model, and ii) a joint optimization of 79.64\% reduction in VM eviction incidents and 34.83\% increase in utilized renewable energy over state-of-the-art packing algorithms [17]. The rest of the paper is organized as follows: Section II provides the background and motivation for our problem with a use case study. Section III details our system model and problem formulation. Section IV outlines the design of our proposed framework. Section V describes the implementation of openstack-gc. Section VI presents the performance evaluation of our framework. Section VII discusses related work, and Section VIII concludes the paper and outlines future work."
https://arxiv.org/html/2411.07444v1,Input-Based Ensemble-Learning Method for Dynamic Memory Configuration of Serverless Computing Functions,"In today’s Function-as-a-Service offerings, a programmer is usually responsible for configuring function memory for its successful execution, which allocates proportional function resources such as CPU and network. However, right-sizing the function memory force developers to speculate performance and make ad-hoc configuration decisions. Recent research has highlighted that a function’s input characteristics, such as input size, type and number of inputs, significantly impact its resource demand, run-time performance and costs with fluctuating workloads. This correlation further makes memory configuration a non-trivial task. On that account, an input-aware function memory allocator not only improves developer productivity by completely hiding resource-related decisions but also drives an opportunity to reduce resource wastage and offer a finer-grained cost-optimised pricing scheme. Therefore, we present MemFigLess, a serverless solution that estimates the memory requirement of a serverless function with input-awareness. The framework executes function profiling in an offline stage and trains a multi-output Random Forest Regression model on the collected metrics to invoke input-aware optimal configurations. We evaluate our work with the state-of-the-art approaches on AWS Lambda service to find that MemFigLess is able to capture the input-aware resource relationships and allocate upto 82% less resources and save up to 87% run-time costs.","The serverless computing paradigm is the latest cloud-native development model that enables application execution without the management of underlying resources. Serverless promotes the idea that a developer should be less concerned about the servers or infrastructure and focus more on productivity that adds value to the business. This shift of responsibility means offloading resource management tasks to the cloud service provider (CSP), such as resource allocation, application scaling and software updates. In the serverless landscape [1], Function-as-a-Service (FaaS) emerged as a microservices-inspired, event-driven execution model where function(s) are integrated with additional Backend-as-a-Service (BaaS) offerings like storage, networking and database services, to set-up an application. A serverless function is a stateless code fragment, executed on-demand within lightweight virtual machines (VM), microVMs or containers for short-term duration, and bills its resources as per usage. In 2014, Amazon Web Services (AWS) introduced AWS Lambda [2], [3] as its first FaaS offering, and since then, a range of FaaS services have emerged, including Google Cloud Functions [4], Azure Functions [5], and many open-source implementations such as OpenFaaS [6], Knative [7] and OpenWhisk [8]. In addition to serverless attributes such as on-demand scalability, zero idle-resource costs, and no resource management, FaaS uniquely features scale-to-zero capability where function resources are released after an extended period of inactivity, endorsing a multi-tenant resource-sharing and pay-per-use pricing model. FaaS has increasingly found its relevance in a variety of use cases like video streaming platform [9], multi-media processing [10], CI/CD pipeline [11], AI/ML inference task [12], and Large-Language-Model (LLM) query processing [13]. The operational model of FaaS hides the complex infrastructure management from end users and does not signify the absence of servers. A serverless function still requires resources, including computing, network and memory, for a successful execution. In the current FaaS implementations, a developer is responsible for requesting the right combination of resources to guarantee successful function execution. However, service providers only expose a small set of resource knobs, usually memory 111We refer to FaaS platforms like AWS Lambda that allow developers to provide only memory configuration and allocate CPU, network bandwidth, etc., in a proportional fashion. with proportionally allocated CPU, disk I/O, network bandwidth, etc. [14]. Prior studies [15][16][17] have identified that a higher memory configuration speeds up function execution and has a significant impact on its start-up performance and costs. However, the execution speedup is non-linear and has a diminishing marginal improvement with increasing memory allocations [18]. With limited observability into short-running functions and unaware of function performance, developers usually resort to speculative decisions for memory configuration or make experience-based ad-hoc decisions with an expectation to fulfil service level objectives (SLO) [19]. To validate such developer behaviour, an industry insight [20] reports the ease of controlling function execution duration via memory configuration, while 47% of production-level functions still run with the default memory configuration without exploring the entire configuration space. Additionally, selecting an optimal memory configuration from an exponentially large search space requires a careful understanding of the correlation between function performance and resource requirements. Hence, configuring the function with the right amount of memory that guarantees shorter execution times and lower execution costs is an intricate task. (a) Payload vs Duration matmul function metrics (b) Payload vs Duration linpack function metrics (c) Payload vs Memory Utilisation graph-mst function metrics (d) Payload vs Memory Utilisation matmul function metrics (e) Payload vs Memory Utilisation linpack function metrics (f) Payload vs Memory Utilisation graph-mst function metrics Figure 1: Function Metrics Insight - Payload vs Memory Utilisation vs Billed Duration TABLE I: List of collected function metrics Metric Name Description request_id unique function invocation ID payload function input parameter(s) memory_size amount of memory allocated to function memory_utilisation maximum memory measured as a percentage of the memory allocated to the function memory_used measured memory of the function sandbox billed_duration function execution time rounded to nearest millisecond billed_mb_ms total billed Gb-s, a pricing unit for function cold_start function cold start (true/false) init_duration amount of time spent in the init phase of the execution environment lifecycle function_error any function run-time error Recent research [17][21][22][23] that optimise the function resource allocation process has highlighted a drastic impact of input parameters on its performance. Additionally, a static memory configuration is used for concurrent function invocations while expecting similar performance for distinct function inputs. Therefore, setting a static memory configuration for all function invocations, regardless of their input, leads to a fluctuating performance with varying workload and input arguments. This performance unpredictability demands an input-argument-aware approach in determining the memory configuration for function invocations that balances execution cost and running time while reducing excess resource allocation. This input-based memory configuration has a two-fold effect of providing a more autonomous developer experience and a chance for CSPs to maximise resource utilisation and deliver a finer-grained, cost-effective pricing model for users. Additionally, existing efforts [17][21][22][23] to configure function resources either focus on an average-case function execution to recommend maximum used memory/resources or propose to re-run their solution for specific input parameters to optimise the memory allocation process. This may lead to higher run-time costs and resource wastage and on the other hand, running multiple models for previously unseen input values extends the data collection process as well as increases the model training and tuning complexity. Therefore, a solution is warranted that captures the relationship of input parameters with function resources to precisely model and predict the required memory configuration for successful execution and reducing excess resource allocation. To this end, we present MemFigLess, an end-to-end estimation and function memory allocation framework that makes input-aware memory configuration decisions for a serverless function. MemFigLess takes as an input the function details, such as the representative function input arguments, expected running time and cost SLOs and a range of memory allocations to explore. The framework executes an offline profiling loop to take advantage of a robust tree-based ensemble learning technique, multi-output Random Forest Regression (RFR), which analyses the relationship between input parameters and other function metrics such as execution time, billed cost, and function memory requirement. The RFR model is then exploited in an online fashion to make an optimal selection of memory configuration for individual function invocations. Additionally, the framework provides a feedback loop to re-train the model in a sliding-window manner with a new set of collected metrics to capture the performance variation."
https://arxiv.org/html/2411.07959v1,On the Convergence of Continual Federated Learning Using Incrementally Aggregated Gradients,"The holy grail of machine learning is to enable Continual Federated Learning (CFL) to enhance the efficiency, privacy, and scalability of AI systems while learning from streaming data. The primary challenge of a CFL system is to overcome global catastrophic forgetting, wherein the accuracy of the global model trained on new tasks declines on the old tasks. In this work, we propose Continual Federated Learning with Aggregated Gradients (C-FLAG), a novel replay-memory based federated strategy consisting of edge-based gradient updates on memory and aggregated gradients on the current data. We provide convergence analysis of the C-FLAG approach which addresses forgetting and bias while converging at a rate of \mathcal{O}(1/\sqrt{T}) over T communication rounds. We formulate an optimization sub-problem that minimizes catastrophic forgetting, translating CFL into an iterative algorithm with adaptive learning rates that ensure seamless learning across tasks. We empirically show that C-FLAG outperforms several state-of-the-art baselines on both task and class-incremental settings with respect to metrics such as accuracy and forgetting.","The concept of lifelong learning in AI is inspired by basic humans nature of learning and adapting to new experiences and knowledge continuously throughout ones life. Continual learning (CL) is an important aspect of lifelong learning, where the key is to gain knowledge of new tasks without forgetting the previously gained expertise. Centralized lifelong learners are well-known [30]. However, increasing privacy concerns, the volume and complexity of data generated by various sources such as sensors, IoT devices, online platforms, and communication bottlenecks have led to the advent of continual federated learning (CFL) mechanisms. A popular use-case of CFL is edge streaming analytics, where a stream of private data is analyzed continuously at the edge-user [11, 25], enabling organizations to extract insights for data-driven decisions without transmitting the data to a centralized server. Edge streaming analytics is well-suited for autonomous decision-making memory-constrained applications such as industrial IoT [27, 15], smart cities [29], autonomous systems [28] and remote monitoring [8]. Conventional ML techniques necessitate retraining in order to adapt to the non-stationary streaming data [33] while computational and memory constraints restrict the simultaneous processing of previous and current datasets, rendering retraining impossible. Further, edge-based analytics without federation results in models that can only learn from its direct experience [32]. A privacy-preserving strategy that allows continuous learning at the global level while circumventing all the above-mentioned issues is Continual Federated Learning (CFL) [31]. In CFL, clients train on private data streams and communicate their local parameters to a server, which subsequently shares the global model with the clients. Several issues in CFL, such as inter-client interference [32], dynamic arrival of new classes into FL training [7], and local and global catastrophic forgetting [2], have been studied. Memory-based replay techniques [9, 12, 19] offer a direct mechanism to revisit past experiences without requiring access to the historical data of other clients. In particular, episodic replay, wherein a small, fixed-size replay buffer of past data is stored along with new data at each client, has proven to be effective in reducing forgetting [9]. However, the replay buffer permits limited access to the dataset from past tasks, resulting in sampling bias [6]. Therefore, it is essential to jointly manage the bias and federation when developing replay-based CFL methods. Consider a federated real-time surveillance use-case as depicted in Fig. 1 (right), where the edge analytics task is the continuous monitoring and analysis of data streams to respond to events as they occur. Let \mathcal{P}^{i} comprise of data from all the previous tasks at the i-th client until a given observation point t=0. The i-th client samples data from \mathcal{P}^{i} and stores it in the buffer \mathcal{M}^{i}\subset\mathcal{P}^{i} as depicted in Fig. 1. Gradient updates on the data in \mathcal{M}^{i} lead to biased gradients since \mathcal{M}^{i} consists of a subset of data of all the previous tasks. At t=0, the server transitions from the previous to the current task, and the FL model starts to learn from the current non-stationary dataset \mathcal{C}_{i}. The goal of the CFL framework is to learn from \mathcal{C}_{i},\forall i, while mitigating the effect of catastrophic forgetting on \mathcal{P}_{i}. Figure 1: (Left) Illustration of C-FLAG: Initialised at the optimal point of the previous tasks \mathbf{x}^{*}_{\mathcal{P}}=\mathbf{x}_{0}, at the t-th iteration, i-th client takes E local steps towards its local optimal regions (pink regions). To balance learning and forgetting, C-FLAG takes a single step towards local memory and E steps on the local current data. The global aggregated model moves towards a common global minima \mathbf{x}^{*}_{\mathcal{P}\cup\mathcal{C}}. (Right) Real-time surveillance where a subset of previous tasks are stored in memory until t=0. Data arriving thereafter is the current task \mathcal{C}^{i}. Contributions: We propose the novel Continual Federated Learning with Aggregated Gradients (C-FLAG) technique, which is a memory replay-based CFL strategy consisting of local learning steps and global aggregation at the server. We consider the incremental aggregated gradient (IAG) approach, which significantly reduces computation costs and comes with the benefits of variance reduction techniques [24]. To jointly mitigate the issues of client drift, bias, and forgetting, an effective gradient, which is a combination of a single gradient on the memory buffer and multiple gradients on the current task, is proposed, as depicted in Fig. 1(left). Our contributions are as follows: - We formulate the CFL problem as a smooth, non-convex finite-sum optimization problem and theoretically demonstrate that the proposed C-FLAG approach converges to a stationary point at a rate of \mathcal{O}(\frac{1}{\sqrt{T}}) over T communication rounds. - We formulate an optimization sub-problem parameterized by the learning rate to minimize catastrophic forgetting, allowing the translation of C-FLAG into an iterative algorithm with adaptive learning rates for seamless learning across tasks. We evaluate C-FLAG on task-incremental FL setups, where it consistently outperforms baseline methods in terms of both average accuracy and forgetting. We also perform ablation studies on data heterogeneity, varying number of clients and the size/type of replay buffer. The results show that C-FLAG outperforms well-known and state-of-the-art baselines in mitigating forgetting and enhancing overall model performance. To the best of our knowledge, this work is the first of its kind to propose a replay-based CFL framework with convergence guarantees. The crux of the theoretical analysis deals with the handling of bias due to memory constraints and characterizing catastrophic forgetting. While prior FL works typically rely on convex or strongly convex assumptions, our analysis extends to the more general non-convex setting."
https://arxiv.org/html/2411.07826v1,Efficient Federated Finetuning of Tiny Transformers with Resource-Constrained Devices,"In recent years, \acpLLM through Transformer structures have dominated many machine learning tasks, especially text processing. However, these models require massive amounts of data for training and induce high resource requirements, particularly in terms of the large number of \acpFLOP and the high amounts of memory needed. To fine-tune such a model in a parameter-efficient way, techniques like Adapter or LoRA have been developed. However, we observe that the application of LoRA, when used in \acFL, while still being parameter-efficient, is memory and \acFLOP inefficient. Based on that observation, we develop a novel layer finetuning scheme that allows devices in cross-device \acFL to make use of pretrained \acpNN while adhering to given resource constraints. We show that our presented scheme outperforms the current state of the art when dealing with homogeneous or heterogeneous computation and memory constraints and is on par with LoRA regarding limited communication, thereby achieving significantly higher accuracies in \acFL training.","In recent years, \acpLLM have dominated various machine learning tasks, particularly next token prediction and text classification, while \acpVIT have closed the gap with \acpCNN in vision tasks. However, these models require massive amounts of data (e.g., huge quantities of text for \acpLLM) and are very power-hungry for training, as they have billions of parameters to adjust [li2020train]. As these \acpLLM, like GPT2 [radford2019language] or LLaMA [touvron2023llama], have billions of parameters and are trained on large quantities of text, they generalize well to many downstream tasks in the text domain. Similarly, in vision, multimodal Transformers can generalize well to detecting objects in an image [hu2021unit, wang2022git]. In many downstream applications, e.g., in next-word prediction or object classification on smartphones or \acIOT devices, deploying such large models imposes high resource requirements for inference, potentially causing high latency and high energy consumption. Although, through generalization, they can achieve the desired accuracy, for many tasks, they are not necessarily required, as tiny specialized models would suffice. In particular, we observe that tiny models that require \sim 60\times fewer resources (\acpFLOP) for inference compared to a lightweight GPT-2 124M model can perform similarly111Parts of the test set may be in GPT’s training set, hence increasing accuracy. in next-token prediction tasks (refer to fig. 1). However, these specialized models require a sufficient amount of problem-specific data to serve as a replacement. In many cases, this problem-specific data resides on resource-constrained edge devices (such as smartphones or \acIOT devices), is privacy-sensitive, and cannot be centrally stored and processed. While in centralized training, several hundred-watt server GPUs are available, edge devices are very limited in their resources and can only spend a few watts on training. Additionally, such devices can have heterogeneous capabilities [pfeiffer2023federated]. To make use of the devices’ data, training must be performed on the devices themselves. In recent years, \acFL has emerged as a privacy-sensitive alternative to centralized training and has shown success in many domains such as smartphone applications, healthcare, and robotics [chen2020fedhealth, yuan2020federated, ciftler2020federated, posner2021federated]. In this work, we study how tiny pretrained Transformer models can be adapted to downstream tasks using \acFL with heterogeneous resource-constrained devices. To adapt large models to downstream tasks, recently, popular techniques like Adapter [houlsby2019adapter] and LoRA [hu2021lora] have been introduced, mainly allowing the adaptation of such models in a parameter-efficient way. However, we observe that while being parameter-efficient, techniques like LoRA still require massive amounts of memory for training in the case of tiny models. The reason for this is that LoRA mainly reduces the memory overhead of gradients and optimizer states but does not lower the activation memory. For large models, this typically suffices as the weights and gradients account for most of the required training memory. We observe that the memory footprint of tiny language models and \acpVIT is mainly dominated by the activation memory (appendix A). Figure 1: Comparison of downstream performance and resource requirements for next-token prediction on Shakespeare [caldas1812leaf] using layer finetuning (with layers frozen from first to last, where each dot represents a specific number of layers being frozen) and LoRA with tiny Transformers pretrained on OpenWebText, having 3, 6, and 9 layers. We observe that while LoRA (with ranks 24, 12, 3) can achieve gains in communication efficiency, it requires significantly more peak memory and \acpFLOP to reach the same accuracy. Hyperparameters and details are provided in section 4.1. GPT2 is included to highlight inference costs (accuracy is based on GPT2’s tokenizer and test data may be part of GPT2’s training set). We compare LoRA against finetuning individual layers of a set of tiny \acpNN (Transformers with 3–12 layers, 3 heads, and an embedding size of 92) that were pretrained on OpenWebText [Gokaslan2019OpenWeb] and trained in a federated manner to perform next-token prediction on Shakespeare [caldas1812leaf] from the Leaf benchmark. From fig. 1, we can draw the following conclusion: For tiny language models, LoRA can achieve similar communication savings compared to layer finetuning. With LoRA, only the low-rank adapters need to be uploaded, while with layer finetuning, only the trained layers need to be uploaded. However, besides memory, we observe that compared to layer finetuning, LoRA requires significantly more computation (as backpropagation is needed from the last to the first layer). Motivated by that observation, we re-evaluate existing \acFL strategies that address resource constraints and heterogeneous devices in cross-device \acFL. Different from the state of the art, we assume that the \acFL models are already pretrained. Furthermore, we assume that we have a selection of differently sized \acpNN to choose from. We observe that when using pretrained \acpNN, layer finetuning outperforms LoRA-derived techniques [cho2023heterogeneous] as well as existing state-of-the-art methods [yao2021fedhm, alam2022fedrolex, diao2020heterofl, horvath2021fjord, kim2023depthfl]. Based on our observations, we propose a strategy that, through \acNN selection and layer finetuning, allows reaching the highest accuracy while adhering to a given device constraint. In summary, we make the following novel contributions: • We are the first to study resource-constrained cross-device \acFL with the availability of pretrained models. We rigorously evaluate existing \acFL strategies and observe that layer finetuning outperforms vanilla LoRA [hu2021lora] as well as recent \acFL adaptations for heterogeneity [cho2023heterogeneous]. Furthermore, with pretrained tiny \acNN models, layer finetuning also surpasses existing \acFL techniques [diao2020heterofl, alam2022fedrolex, horvath2021fjord, yao2021fedhm, kim2023depthfl]. Specifically, we study \acFL downstream tasks such as Shakespeare, CIFAR10, and CIFAR100. • Existing works assume a fixed \acNN that must be trained while adhering to device constraints. In this work, we assume a set of pretrained \acNN architectures is available for selection. Based on our observations, we propose \acfOURS, an architecture selection technique that achieves the highest accuracy given a device constraint. • We evaluate the performance of state-of-the-art techniques and \acOURS in heterogeneous settings. Additionally, our technique promotes greater fairness for weaker devices."
https://arxiv.org/html/2411.07622v1,A Performance Analysis of BFT Consensus for Blockchains,"Distributed ledgers are common in the industry. Some of them can use blockchains as their underlying infrastructure. A blockchain requires participants to agree on its contents. This can be achieved via a consensus protocol, and several BFT (Byzantine Fault Tolerant) protocols have been proposed for this purpose. How do these protocols differ in performance? And how is this difference affected by the communication network? Moreover, such a protocol would need a timer to ensure progress, but how should the timer be set?This paper presents an analytical model to address these and related issues in the case of crash faults. Specifically, it focuses on two consensus protocols (Istanbul BFT and HotStuff) and two network topologies (Folded-Clos and Dragonfly). The model provides closed-form expressions for analyzing how the timer value and number of participants, faults and switches affect the consensus time. The formulas and analyses are validated with simulations. The conclusion offers some tips for analytical modeling of such protocols.","There is considerable current interest in the use of blockchains to implement distributed ledgers for finance, health care, energy, logistics, etc. (Fan et al., 2020). This requires continual consensus among interested parties in the system. Research on consensus protocols is mostly split between proof-based consensus (like Bitcoin) and vote-based consensus (Xu et al., 2023). A vote-based protocol for enforcing consensus must guard against errant (possibly malicious) validator behavior. It is said to be Byzantine Fault Tolerant (BFT) if it can achieve consensus despite arbitrary misbehavior. There is a large variety of BFT protocols. They differ in terms of failure models, delay assumptions, cryptographic requirements, etc. Much of the impetus in developing new protocols lies in pushing their performance. The performance of these protocols are measured via complexity analysis (e.g. number of messages or rounds), in empirical or simulation experiments, or with analytical models. Despite decades of such analysis of BFT consensus, much remains unknown regarding their behavior. For example, to ensure the protocol progresses despite failures, a common technique lies in setting a timer; when it expires, the protocol assumes there is a fault, and restarts the consensus process. This timer has some initial value {\tau_{0}}, but the value is usually increased upon timer expiry. How should {\tau_{0}} be set? For guidance, we simulated HotStuff (Yin et al., 2019), a well-known BFT protocol for blockchains, and measured the time T to gather consensus for a block. Fig. 1(a) shows how T varies with {\tau_{0}} and the number of faults. How can one analytically describe this interesting, non-monotonic behavior? (a) non-monotonic behavior (b) crossover for n (c) crossover for switch rate Figure 1. Simulated consensus time for (a) HotStuff with varying number of faults (n=16, clique), (b) HotStuff vs IBFT on Dragonfly ({\nu_{d}}=5,{r^{\mathcal{P}}_{s}}=9) and (c) HotStuff vs IBFT on Dragonfly ({\nu_{d}}=3, n=31). The unit of time on the vertical axis follows that for message processing time at a validator (see Sec. 3.4). (a) IBFT (b) HotStuff Figure 2. IBFT and HotStuff have very different patterns for message exchange. In the simulations for Fig. 1(a), we factor out the impact of the network connecting the nodes by using 0 message delay between any two nodes. What if the network delays are not negligible? How might two topologies differ in their impact on T? A network can affect T through congestion delays. We therefore expect that the same topology can have different impact on two protocols, if their message patterns differ. For example, consider the pattern in Fig. 2(a) for IBFT (Instanbul BFT) protocol (Moniz, 2020), which is very different from the one in Fig. 2(b) for HotStuff. One may expect that, for the same topology, congestion delays can have greater impact on IBFT than HotStuff. However, simulation results in Fig. 1(b) show that, for the Dragonfly (Sensi et al., 2019), IBFT can be faster or slower than HotStuff, depending on the number of participants. Fig. 1(c) shows a similar slower/faster crossover depending on how fast a switch or router can process a message. How do the protocol and topology parameters determine the crossover point in their consensus time? This paper presents three contributions: (1) An analysis of how BFT consensus time depends on the number of validators and faults, timer value and network topology. (2) A comparison of two protocols’ performance, and two topologies’ impact. (3) An approach to analytic modeling of consensus protocols. Below, we begin with a review of related work in Sec. 2. Sec. 3 introduces necessary definitions and notation, and describes two protocols (IBFT and HotStuff) and two topologies (Dragonfly and Folded-Clos) that we use for analyzing consensus time; it also presents the approach to the performance analysis and its validation. A Byzantine validator can cause arbitrary performance degradation (Clement et al., 2009), so we only consider crash faults. The performance model is presented in Sec. 4 for cliques and Sec. 5 for non-clique topologies. Sec. 6 highlights some conclusions about the consensus time for HotStuff and IBFT, and the modeling approach."
https://arxiv.org/html/2411.07485v1,Decentralized Network Topology Design for Task Offloading in Mobile Edge Computing,"The rise of delay-sensitive yet computing-intensive Internet of Things (IoT) applications poses challenges due to the limited processing power of IoT devices. Mobile Edge Computing (MEC) offers a promising solution to address these challenges by placing computing servers close to end users. Despite extensive research on MEC, optimizing network topology to improve computational efficiency remains underexplored. Recognizing the critical role of network topology, we introduce a novel decentralized network topology design strategy for task offloading (DNTD-TO) that jointly considers topology design and task allocation. Inspired by communication and sensor networks, DNTD-TO efficiently constructs three-layered network structures for task offloading and generates optimal task allocations for these structures. Comparisons with existing topology design methods demonstrate the promising performance of our approach.","With the advancement in Internet of Things (IoT) technology, many delay-sensitive yet computing-intensive applications have emerged, such as automotive driving, face recognition and virtual reality[1]. However, IoT devices typically have limited computing power, making it difficult to meet the demands of these tasks. Centralized cloud computing is traditionally used to process these tasks, but offloading them to the remote cloud can cause significant transmission delays that degrade user Quality of service (QoS). To address this issue, Mobile Edge Computing (MEC) solutions were introduced[2], where MEC places servers closer to end users at the network edge, such as at base stations, to reduce transmission delays. To better serve end users, extensive research has been conducted in the field of MEC, addressing various design aspects such as system deployment, task offloading, resource allocation, mobility management, and privacy and security [3]. Nevertheless, little attention has been given to optimizing network topology to enhance computational efficiency. Most existing studies primarily focus on offloading tasks from users to one or more nearby MEC servers within communication range [4, 5]. A few studies [6, 7] have explored offloading tasks to servers multiple hops away, but these designs did not consider the impact of network topology. In our prior work [8], we proposed a multi-layered task offloading framework and demonstrated that computational efficiency can be improved by leveraging layered network structures. We also showed that computational efficiency is influenced not only by the computing and communication characteristics of the servers but also by their network topology. In this paper, we aim to further investigate the joint design of layered network structure and task allocation. Layered structures have been widely used in communication and practical sensor networks due to energy efficiency and network management simplicity [9]. In these networks, a base station is typically present alongside several clusters of sensors or communication devices, with each cluster comprising a Cluster Head (CH) and multiple Cluster Members (CMs). The CH collects data from its CMs and transmits it to the base station. Methods for selecting CHs and CMs can be broadly categorized into two types: cluster-based [10, 11, 12, 13] and grid-based [14, 15, 16, 17]. In cluster-based methods, CHs are selected directly based on certain criteria. In contrast, grid-based methods first divide the network into grids, and then select CHs within each grid. Despite the widespread use of layered structures in communication and sensor networks, their design for task offloading remains underdeveloped. In this paper, we introduce a decentralized network topology design strategy for task offloading (DNTD-TO). We explore three-layered network structures, similar to those commonly used in communication and sensor networks, to facilitate computing. In this setup, tasks are offloaded from the root node (referred to as master) to servers in the second layer (CHs), which then distribute the tasks to their child nodes in the third layer (CMs). To select CHs and CMs, our strategy iterates through two nested phases: a local cluster formation phase, where each server within master’s communication range selects CMs in a decentralized manner, and a cluster selection phase where the master selects CHs and their associated CMs. The selection of CHs and CMs is based on servers’ task processing capacities and their potential to enhance computational efficiency. Optimal task allocation is integrated into every step of the selection process. The rest of the paper is organized as follows. Sec. II covers system modeling and problem formulation. Our approach and simulation studies are presented in Sec. III and Sec. IV, respectively. Sec. V concludes the paper."
https://arxiv.org/html/2411.07391v1,Federated Learning Client Pruning for Noisy Labels,"Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, existing FL methods often assume clean annotated datasets, impractical for resource-constrained edge devices. In reality, noisy labels are prevalent, posing significant challenges to FL performance. Prior approaches attempt label correction and robust training techniques but exhibit limited efficacy, particularly under high noise levels. This paper introduces ClipFL (Federated Learning Client Pruning), a novel framework addressing noisy labels from a fresh perspective. ClipFL identifies and excludes noisy clients based on their performance on a clean validation dataset, tracked using a Noise Candidacy Score (NCS). The framework comprises three phases: pre-client pruning to identify potential noisy clients and calculate their NCS, client pruning to exclude a percentage of clients with the highest NCS, and post-client pruning for fine-tuning the global model with standard FL on clean clients. Empirical evaluation demonstrates ClipFL’s efficacy across diverse datasets and noise levels, achieving accurate noisy client identification, superior performance, faster convergence, and reduced communication costs compared to state-of-the-art FL methods. Our code is available at https://github.com/MMorafah/ClipFL.","Edge devices such as IoTs and mobile devices are increasingly ubiquitous, constituting a new computational platform for machine learning. Despite holding vast real-world data, these billions of devices often withhold their data due to privacy concerns. Federated Learning (FL) emerges as a decentralized machine learning paradigm, enabling model training with the collaboration of multiple clients while preserving privacy [30, 16]. FL shows promise in enhancing model performance without necessitating data sharing and finds applications across diverse domains [16, 45, 34, 6, 13]. However, FL encounters significant performance degradation in the presence of data heterogeneity [11, 25, 61]. Recent advancements introduce FL optimizers tailored to address data heterogeneity, achieving faster convergence [24, 18, 50, 38]. Despite these advancements, the majority of prior works operate under the assumption of accurately labeled and clean client data. In practice, however, acquiring precisely annotated clean datasets is arduous and resource-intensive, especially for edge devices lacking ample resources. Consequently, the labels in their datasets often contain noise. Unfortunately, FL experiences substantial performance degradation when confronted with noisy clients [56]. Thus, developing FL frameworks resilient to noisy labels is imperative. Several prior works propose methods to identify and rectify noisy samples using the global model’s predictions [53, 56]. For instance, FedCorr [53] presents a multi-stage FL framework that identifies noisy clients by measuring the local intrinsic dimensionality (LID) and corrects noisy labels using global model predictions. In order to control the negative impact of noisy client in achieving a well-trained reliable global model prior to label correction stage, they incorporate several techniques including client fraction scheduling scheme and local proximal regularization with mix-up. However, this approach heavily relies on a well-performing global model, which is challenging to obtain in the presence of data heterogeneity and noisy clients, leading to inaccurate label correction and suboptimal performance. Other approaches aim to mitigate the impact of noisy clients through client weighting strategies and robust local training methods [9, 57, 15]. For example, RHFL [9] utilizes symmetric cross-entropy loss during local training and introduces a client confidence re-weighting scheme to counteract the adverse effects of noisy labels during collaborative learning. However, these methods demonstrate limited efficacy and poor convergence, especially under high noise levels. In this paper, we address the challenge of noisy labels in FL by adopting an alternative approach compared to prior works. Rather than mitigating or correcting noisy clients, we propose a novel “Federated Learning Client Pruning” framework called ClipFL. ClipFL identifies noisy clients and excludes them from the FL training process. Our framework comprises of three phases. In the initial phase, we identify noisy clients based on their performance on a clean validation dataset, tracking the frequency of noisy identifications as Noise Candidacy Score (NCS) of each client. To mitigate the negative impact of noisy clients during server-side model aggregation, we only aggregate the top-m clients with the highest validation accuracy. Subsequently, in the second phase, we prune p\% of the clients with the highest noise candidacy score (NCS) in a one-shot manner. Finally, in the third stage, we conduct standard FL on the remaining clean clients to further refine the global model. (a) Prior works typically involve incorporating noisy clients into FL and addressing their negative impact through methods such as correcting noisy labels, re-weighting noisy clients, or designing robust local training methods. (b) In contrast to prior works, ClipFL introduces the Noise Candidacy Score (NCS) for each client, enabling robust identification of noisy clients from the FL process. Figure 1. Comparison between our approach and prior works: Prior works often face challenges related to poor convergence and reliance on a well-performing global model as a pseudo-labeler that corrects noisy labels, which can be difficult to obtain, especially in the presence of high noise levels and data heterogeneity (see Section 5.3). In contrast, ClipFL offers consistent performance improvements and reduces communication costs by robustly identifying noisy clients and excluding them from the FL process. We validate ClipFL on diverse datasets with varying noise levels for both IID and Non-IID data partitions, yielding several key observations: (1) ClipFL accurately identifies noisy clients with at least 80% accuracy on most of the cases. (2) ClipFL outperforms state-of-the-art (SOTA) FL optimizers without excluding noisy clients. (3) ClipFL surpasses existing FL methods addressing noisy labels, underscoring the effectiveness of our excluding approach. (4) ClipFL demonstrates faster convergence and reduced communication costs compared to both SOTA vanilla FL optimizers and FL methods designed to counteract noisy labels. We make our code publicly available at https://github.com/MMorafah/ClipFL. Contribution. Our contributions are threefold: • We introduce ClipFL, a novel federated learning client excluding method to address the challenge of noisy labels in FL. • ClipFL distinguishes clean clients from noisy ones through evaluation on a clean validation dataset and aggregates only the top-performing clients to mitigate the impact of noisy clients. • We empirically evaluate ClipFL across different datasets with varying noise levels for both IID and Non-IID data partitions, demonstrating significant performance improvements and reduced communication costs over SOTA FL methods. Organization. The rest of the paper is organized as follows: In Section 2, we discuss related works. Section 3 presents the background and problem formulation. Our proposed method is introduced in Section 4. Section 5 details our experimental results. We discuss our implementation and hyperparameters in Section 6. Finally, we conclude our work in Section 7."
https://arxiv.org/html/2411.07302v1,Merit-Based Sortition in Decentralized Systems,"In decentralized systems, it is often necessary to select an ‘active’ subset of participants from the total participant pool, with the goal of satisfying computational limitations or optimizing resource efficiency. This selection can sometimes be made at random, mirroring the sortition practice invented in classical antiquity aimed at achieving a high degree of statistical representativeness. However, the recent emergence of specialized decentralized networks that solve concrete coordination problems and are characterized by measurable success metrics often requires prioritizing performance optimization over representativeness. We introduce a simple algorithm for ‘merit-based sortition’, in which the quality of each participant influences its probability of being drafted into the active set, while simultaneously retaining representativeness by allowing inactive participants an infinite number of chances to be drafted into the active set with non-zero probability. Using a suite of numerical experiments, we demonstrate that our algorithm boosts the quality metric describing the performance of the active set by >2 times the intrinsic stochasticity. This implies that merit-based sortition ensures a statistically significant performance boost to the drafted, ‘active’ set, while retaining the property of classical, random sortition that it enables upward mobility from a much larger ‘inactive’ set. This way, merit-based sortition fulfils a key requirement for decentralized systems in need of performance optimization.","The term ‘sortition’ originally refers to the process of randomly selecting representatives in a democratic system, a practice dating back 2.5 millennia to ancient Athens, where the selection of public officials by lottery was seen as the best way of achieving fairness in society (e.g. Headlam-Morley, 1891). Since then, sortition has spread over the world as a way of obtaining representative selections of politicians, public officials, or advisors, going through ebbs and flows in terms of its popularity (e.g. Flanigan et al., 2021; Jacquet et al., 2022; Sintomer, 2023a). Advocates of sortition often highlight positive implications such as fairness, representativeness, and efficiency (e.g. Engelstad, 1989; Sintomer, 2023b). In permissionless, decentralized systems (e.g. Nakamoto, 2008; Buterin, 2014), a form of sortition is often needed too. Across a large set of anonymous contributors, validators, or other participants, there exists a high degree of redundancy that does not require all participants to be involved in the decision process (e.g. Wüst & Gervais, 2018). Sometimes, there may exist concrete computational limitations why it is infeasible to involve all participants. In any of such cases, (pseudo)-random subsets may be able to fulfil the same task without a serious loss of security or performance. In principle, this can be done using classical, random sortition techniques (of which numerous trustless examples exist, see e.g. Gilad et al. 2017; Saa & Stern 2019; Freitas et al. 2023) to achieve representativeness and efficiency. However, recent developments in decentralized networks have brought about a rapid growth of systems that aim to achieve concrete goals, often with measurable performance or success. Examples are decentralized machine intelligence and inference systems (e.g. Rao et al., 2021; Kruijssen et al., 2024), on-chain oracles (e.g. Ellis et al., 2017; Breidenbach et al., 2021), or internet-of-things networks (e.g. Banerjee et al., 2023). In such systems, which aim to achieve a quantifiable degree of success, the goal of sortition is to improve efficiency not only while maintaining representativeness, but also while optimizing performance. The latter goal is achieved not through random sortition, but by letting the performance of each participant influence their probability of being drafted. We refer to this concept as merit-based sortition. In this paper, we introduce a simple algorithm for merit-based sortition that can be used to increase computational efficiency by limiting active participation without sacrificing (and generally improving) performance. This is possible, because the algorithm: 1. optimizes the quality of the active set of participants by letting the probability of relegation out of the active set decrease with the participant’s quality; 2. retains fairness and representativeness by allowing inactive participants an infinite number of chances to be drafted into the active set, in such a way that the probability and frequency of promotion increase with the participant’s quality. In §2 we outline the proposed algorithm for merit-based sortition. We investigate its quantitative performance and behavior in §3 using a suite of numerical experiments. Finally, we summarize our results in §4."
https://arxiv.org/html/2411.07106v2,A Topological Characterization of Stabilizing Consensus,"We provide a complete characterization of the solvability/impossibility of deterministic stabilizing consensus in any computing model with benign process and communication faults using point-set topology. Relying on the topologies for infinite executions introduced by Nowak, Schmid and Winkler (JACM, 2024) for terminating consensus, we prove that semi-open decision sets and semi-continuous decision functions as introduced by Levin (AMM, 1963) are the appropriate means for this characterization: Unlike the decision functions for terminating consensus, which are continuous, semi-continuous functions do not require the inverse image of an open set to be open and hence allow to map a connected space to a disconnected one. We also show that multi-valued stabilizing consensus with weak and strong validity are equivalent, as is the case for terminating consensus. By applying our results to (variants of) all the known possibilities/impossibilities for stabilizing consensus, we easily provide a topological explanation of these results.","A substantial share of distributed computing research has been devoted to terminating tasks like consensus, where every process is given some input value, and must locally compute some output value and then terminate. Still, there are also distributed computing problems that cannot be described by such terminating tasks. Apart from self-stabilizing algorithms [1], which can recover from arbitrarily corrupted states, there are also tasks where the processes are allowed to continuously update their output values. Canonical examples are asymptotic consensus [2, 3], stabilizing consensus [4, 5], and approximate agreement [6, 7]. A number of services in practical distributed systems, like clock synchronization [8], [9] and sensor fusion [10], can be built atop of such non-terminating tasks. Unlike asymptotic consensus [2, 11, 12, 13, 14, 3] and approximate agreement [6, 15, 2, 16, 17, 18, 19, 7, 20, 21, 3], which have been studied in various computing models and are hence fairly well-understood, not much is known about stabilizing consensus [4, 5, 22, 23]. In stabilizing consensus, processes need to agree on a common decision value only eventually and do not have to decide irrevocably, i.e., may change their decisions arbitrarily before eventually stabilizing to the common value. As a straightforward relaxation of terminating consensus, stabilizing consensus is of particular interest also for the theory of distributed computing, namely, for studying the solution power of distributed computing models. For example, whereas terminating deterministic consensus is impossible to solve in the synchronous lossy-link model [24, 25, 26] or in asynchronous systems in the presence of just a single crash fault [27], for example, deterministic stabilizing consensus can be solved in those models. More specifically, stabilizing consensus algorithms for asynchronous systems with fair-lossy links were given in [4], both for crash faults and byzantine process faults [4]. Solution algorithms for synchronous dynamic networks controlled by message adversaries [28] were proposed in [5], where Charron-Bost and Moran introduced the strikingly simple anonymous MinMax algorithms. In particular, as argued in [23], a simple MinMax algorithm can be used to solve stabilizing consensus in the lossy-link model (LL) [24, 25, 26], where two processes are connected by a a pair of directed links that may lose at most one message in every round. In [22], Schmid and Schwarz developed a stabilizing consensus algorithm for processes with unique ids for the vertex-stable root message adversary studied in [29], by stripping-off the termination code from a terminating consensus algorithm. Even less is known regarding impossibility results for stabilizing consensus. In [4], it was shown that stabilizing consensus is impossibe to solve in any computing model with byzantine faults and anonymous processes. In [5], a partitioning argument was used to prove that the problem cannot be solved deterministically in synchronous systems controlled by a message adversary if the latter does not guarantee a non-empty kernel, i.e., at least one process that can reach all other processes (possibly via multiple hops) infinitely often. The, to the best of our knowledge, first non-obvious impossibility result for deterministic stabilizing consensus has been established recently by Felber and Rincon Galeana in [23]. The authors showed that the problem cannot be solved in the delayed lossy-link model (DLL), where the links between the two processes may also lose both messages in a round, provided this happens at most finitely often. Note that this result negatively answered the question raised in [5], namely, whether a non-empty kernel alone is also sufficient for solving stabilizing consensus. In this paper, we provide a complete characterization of the solvability/impossibility of deterministic stabilizing consensus, in any model of computation with benign process and communication faults, using point-set topology as introduced by Alpern and Schneider in [30]. In more detail: (1) Relying on the topologies for infinite executions introduced by Nowak, Schmid and Winkler [31, 32], we prove that semi-open sets and semi-continuous functions as proposed by Levine in [33] (already111Since we are not aware of any earlier usage of Levine’s results in the distributed computing context, our findings constitute another instance of the utility of purely theoretical (here: mathematical) research for eventually advancing science in more applied areas. in 1963) are the appropriate means for this characterization. Unlike the decision functions for terminating consensus, which are continuous, they do not require the inverse image of an open set to be open and hence allow to map a connected space to a disconnected one. Since “offending” limit points do not need to be excluded from the set of admissible executions here, this explains why stabilizing consensus is solvable in models where terminating consensus is impossible. (2) We show that, as in the case of terminating consensus, stabilizing consensus with weak and strong validity are equivalent. (3) We demonstrate the power of our novel topological characterization by applying it to (variants of) all the possibility and impossibility results for stabilizing consensus known so far, also including a new one. Our paper is organized as follows: In Section 2, we provide the cornerstones of the generic system model introduced in [31] and the variants of stabilizing consensus considered in this paper. Section 3 briefly recalls some point-set topology basic terms and restates the most relevant results from [33], Section 4 provides the cornerstones of the topological characterization of terminating consensus [31] needed for our results. In Section 5, we provide our topological characterization for stabilizing consensus, Section 6 is devoted to the equivalence of multi-valued stabilizing consensus with weak and strong validity. Finally, in Section 7, we apply our topological characterization to various possibility/impossibility results. Some conclusions in Section 8 round-off our paper."
https://arxiv.org/html/2411.07011v1,The Singular Optimality of Distributed Computation in LOCAL,"It has been shown that one can design distributed algorithms that are (nearly) singularly optimal, meaning they simultaneously achieve optimal time and message complexity (within polylogarithmic factors), for several fundamental global problems such as broadcast, leader election, and spanning tree construction, under the \text{KT}_{0} assumption. With this assumption, nodes have initial knowledge only of themselves, not their neighbors. In this case the time and message lower bounds are \Omega(D) and \Omega(m), respectively, where D is the diameter of the network and m is the number of edges, and there exist (even) deterministic algorithms that simultaneously match these bounds.On the other hand, under the \text{KT}_{1} assumption, whereby each node has initial knowledge of itself and the identifiers of its neighbors, the situation is not clear. For the \text{KT}_{1} CONGEST model (where messages are of small size), King, Kutten, and Thorup (KKT) showed that one can solve several fundamental global problems (with the notable exception of BFS tree construction) such as broadcast, leader election, and spanning tree construction with \tilde{O}(n) message complexity (n is the network size), which can be significantly smaller than m. Randomization is crucial in obtaining this result. While the message complexity of the KKT result is near-optimal, its time complexity is \tilde{O}(n) rounds, which is far from the standard lower bound of \Omega(D). An important open question is whether one can achieve singular optimality for the above problems in the \text{KT}_{1} CONGEST model, i.e., whether there exists an algorithm running in \tilde{O}(D) rounds and \tilde{O}(n) messages. Another important and related question is whether the fundamental BFS tree construction can be solved with \tilde{O}(n) messages (regardless of the number of rounds as long as it is polynomial in n) in \text{KT}_{1}.In this paper, we show that in the \text{KT}_{1} LOCAL model (where message sizes are not restricted), singular optimality is achievable. Our main result is that all global problems, including BFS tree construction, can be solved in \tilde{O}(D) rounds and \tilde{O}(n) messages, where both bounds are optimal up to polylogarithmic factors. Moreover, we show that this can be achieved deterministically.","The efficiency of distributed algorithms is traditionally measured by their time and message complexities. The time complexity is the number of rounds in the algorithm, whereas the message complexity is the total amount of messages sent during its execution. Distributed algorithms that separately optimize for either the number of rounds or the total amount of messages have been studied extensively; more recently, researchers have designed several algorithms that are simultaneously (near) optimal with respect to both measures: such algorithms are called singularly optimal [39], and are the focus of this paper. Singular Optimality in \text{KT}_{0}. In the \text{KT}_{0} model (i.e., Knowledge Till radius 0), also called the clean network model [41], where nodes have initial local knowledge of only themselves (and not of their neighbors), it has been established that one can obtain (near) singularly optimal distributed algorithms, i.e., algorithms that have simultaneously optimal time and message complexity up to polylogarithmic factors, for many fundamental global problems111These problems require traversing the entire network to compute their solution and hence take \Omega(D) rounds, where D is the network diameter. such as leader election, broadcast, Spanning Tree (ST), Minimum Spanning Tree (MST), minimum cut, and approximate shortest paths (under some conditions) [34, 40, 17, 26]. For problems such as leader election, broadcast, and ST, it has been shown that one can design a singularly optimal algorithm in the \text{KT}_{0} model that takes \tilde{O}(m) messages (m is the number of edges of the network, and \tilde{O}(\cdot) suppresses logarithmic factors) and O(D) rounds (D is the network diameter); both are tight (up to a \text{polylog}(n) factor where n is the number of nodes) due to matching lower bounds that hold even for Monte Carlo randomized algorithms [34]. In addition, MST also admits a singularly optimal algorithm [40, 17] with message complexity \tilde{O}(m) and round complexity \tilde{O}(D+\sqrt{n}) (both bounds are tight up to polylogarithmic factors). The singular optimality of MST in the \text{KT}_{0} model also implies the singular optimality of many other problems, such as approximate minimum cut and graph verification problems [12]. \text{KT}_{1} Model and the KKT Result. On the other hand, in the \text{KT}_{1} model (i.e., Knowledge Till radius 1), in which each node has initial knowledge of itself and the identifiers222We stress that only knowledge of the identifiers of the neighbors is assumed, not other information such as the degree of the neighbors. of its neighbors, singular optimality of all the above fundamental problems is wide open. The \text{KT}_{1} model arises naturally in many settings, e.g., in networks where nodes know the identifiers of their neighbors (as well as other nodes), e.g., on the Internet, where a node knows the IP addresses of other nodes. Similarly, in models such as the Congested Clique [27] and the k-machine model [32, 28], it is natural to assume that each processor knows the identifiers of all other processors. For the \text{KT}_{1} CONGEST model (where messages are of small size, typically O(\log{n})), King, Kutten, and Thorup (henceforth, KKT) [31] showed that one can solve several fundamental global problems (with the notable exception of BFS tree construction) such as broadcast, leader election, and spanning tree construction in message complexity of \tilde{O}(n) (n is the network size) which can be significantly smaller than m. (This is in contrast to the \text{KT}_{0} model where \Omega(m) is a lower bound of the message complexity.) Randomization is crucial in obtaining this result, and the algorithm is not comparison-based.333Comparison-based algorithms can operate on identifiers only by comparing them, i.e., given two IDs, one can only determine whether one is less, greater, equal to the other. We note that Awerbuch, Goldreich, Peleg, and Vainish [2] show that \Omega(m) is a message lower bound for MST even in the \text{KT}_{1} CONGEST model, for all deterministic algorithms (even non-comparison based) and all randomized (even Monte Carlo) comparison-based algorithms. The KKT result breaks the \Omega(m) message barrier in \text{KT}_{1} CONGEST by using a randomized non-comparison-based technique that uses node IDs as inputs to hash functions. While the message complexity of the KKT result is near-optimal (as \Omega(n) is a lower bound on the message complexity even in \text{KT}_{1} — see, e.g., [38]), their round complexity is \tilde{O}(n), which is far from the standard lower bound of \Omega(D) for the leader election, broadcast, and ST problems. An important open question is whether one can achieve singular optimality for the above problems in the \text{KT}_{1} CONGEST model, i.e., whether there exists an algorithm running in \tilde{O}(D) rounds and \tilde{O}(n) messages. Another important and related question is whether the fundamental BFS tree construction can be solved in \tilde{O}(n) messages (regardless of the number of rounds as long as it is a polynomial in n) in \text{KT}_{1}. Similarly, for the MST problem, the KKT algorithm also showed that one can achieve \tilde{O}(n) message complexity, but it is not time-optimal — it can take significantly more than \tilde{\Theta}(D+\sqrt{n}) rounds, which is a time lower bound for MST that applies even for Monte Carlo randomized algorithms [12]. In subsequent work, Mashreghi and King [35] presented a trade-off between messages and time for MST: a Monte Carlo algorithm that uses \tilde{O}(\frac{n^{1+\epsilon}}{\epsilon}\log\log n) messages and runs in O(n/\epsilon) time for any 1>\epsilon\geq\log\log n/\log n. This algorithm also takes \Omega(n) time. Hence another natural and fundamental open question is whether we can design an MST algorithm in the \text{KT}_{1} CONGEST model that is singularly optimal, i.e., runs in \tilde{\Theta}(D+\sqrt{n}) rounds, while using \tilde{\Theta}(n) messages. We note that a singularly optimal algorithm for such a problem is far from certain. Robinson [43] showed a message complexity lower bound for constructing a (2k-1)-spanner in the \text{KT}_{1} CONGEST model, which implies that obtaining singularly optimal algorithms for graph spanners of O(n^{1+1/k+\epsilon}) edges, for any constant \epsilon>0, is impossible in polynomial time. However, for many fundamental graph problems, including broadcast, ST, leader election, and MST, the quest for singularly optimal algorithms is still unresolved. Time-Message Tradeoffs in \text{KT}_{1}. Gmyr and Pandurangan [24] presented several results that show that tradeoffs between time and messages in the \text{KT}_{1} CONGEST model are possible for various fundamental problems. (See also the related results of [19].) The time-message tradeoff results are based on a uniform and general approach which involves constructing a sparsified spanning subgraph of the original graph — called a danner (i.e., “diameter-preserving spanner”) — that trades off the number of edges with the diameter of the sparsifier. A key ingredient of this approach is a distributed randomized algorithm that, given a graph G and any \delta\in[0,1], with a high probability constructs a danner that has diameter \tilde{O}(D+n^{1-\delta}) and \tilde{O}(\min\{m,n^{1+\delta}\}) edges in \tilde{O}(n^{1-\delta}) rounds while using \tilde{O}(\min\{m,n^{1+\delta}\}) messages, where n, m, and D are the number of nodes, edges, and the diameter of the input graph G, respectively. Using a danner, they show that the leader election, broadcast, and ST problems can be solved in \tilde{O}(D+n^{1-\delta}) rounds using \tilde{O}(\min\{m,n^{1+\delta}\}) messages for any \delta\in[0,1]. Another important consequence of the danner construction is that the MST and connectivity problems can be solved in \tilde{O}(D+n^{1-\delta}) rounds using \tilde{O}(\min\{m,n^{1+\delta}\}) messages for any \delta\in[0,0.5]. In addition to getting any desired tradeoff (by plugging in an appropriate \delta), one can get a time-optimal algorithm by choosing \delta=0.5, which results in a distributed MST algorithm that runs in \tilde{O}(D+\sqrt{n}) rounds and uses \tilde{O}(\min\{m,n^{3/2}\}) messages. (Note that this result is time-optimal but not message-optimal.) While these results improve over prior bounds in the \text{KT}_{1} CONGEST model [31, 35, 2], they do not answer the question of whether singularly optimal algorithms exist for fundamental global problems in the \text{KT}_{1} CONGEST model, nor do they show that the tradeoff bounds are tight. Singularly Optimality of Local Problems. The work of Bitton et al. [7] is the most similar in spirit to this work and raises and answers the same questions but for local problems. By local problems, we mean problems with t-round algorithms for small t (say, polylogarithmic in n) in the LOCAL model. In particular, they ask the following question: Given a LOCAL algorithm that runs in t rounds, is it possible to design an algorithm that takes O(t) rounds while sending only O(n^{1+\epsilon}) messages for an arbitrarily small constant \epsilon>0? They point out that this question can be resolved if one can construct an \alpha-spanner of G with O(1) stretch and O(n^{1+\epsilon}) edges in O(1) rounds sending O(n^{1+\epsilon}) messages. They also point out that, despite the extensive literature on distributed spanner construction algorithms, it is not known whether such a LOCAL spanner construction algorithm exists. They then present message-reduction schemes for LOCAL algorithms under the \text{KT}_{1} assumption444Actually, their results also hold in a somewhat weaker model where edges have unique IDs which are known to both endpoints. that preserve their asymptotic time complexity. They point out that the gossip-based algorithms of [8, 25] imply that a t-round algorithm in the LOCAL model can be transformed into an algorithm in the LOCAL model that runs in O(t\log n+\log^{2}n) rounds and O(nt\log n+n\log^{2}n) messages. Then they show how to transform a t-round LOCAL algorithm into an O(t)-round algorithm that uses O(tn^{1+\epsilon}) messages for an arbitrarily small constant \epsilon>0. We refer to [7] for more details. An implication of the work of [8, 25] is that local problems admit (nearly) singularly optimal algorithms. For example, fundamental local problems such as MIS (Maximal Independent Set), coloring, and maximal matching admit O(\log n)-round algorithms, and hence one can design algorithms for these problems that run in O(\log^{2}n) rounds and O(n\log^{2}n) messages in \text{KT}_{1} LOCAL. It is important to point out that the above results do not help in designing singularly optimal algorithms for global problems, which require \Omega(D) rounds. This is the focus of this paper. Open Questions. The motivating question underlying this work is understanding the status of various fundamental global problems in the \text{KT}_{1} model — whether they are singularly optimal or exhibit trade-offs (and, if so, to quantify the trade-offs). In particular, an important open question is whether one can design a randomized (non-comparison-based) algorithm for broadcast or for leader election that takes \tilde{O}(D) time and \tilde{O}(n) messages in the \text{KT}_{1} CONGEST model. Also, King et al. [31] ask whether it is possible to construct an ST in o(n) rounds with o(m) messages. As mentioned earlier, algorithms that are message-optimal, i.e., taking \tilde{O}(n) messages, take O(n) rounds. The situation is also wide open from a lower bound point of view: no lower bound is known on one measure conditional on the other measure being optimal. In particular, even o(n)-rounds message-optimal algorithms are not known. Another important and related question is whether the fundamental BFS tree construction can be solved in \tilde{O}(n) messages (in polynomial number of rounds) in \text{KT}_{1}. We note that the polynomial rounds restriction is necessary since the KKT result has an important implication for the \text{KT}_{1} CONGEST model in general. Using this result, one can show that any problem can be solved in the \text{KT}_{1} CONGEST model using \tilde{O}(n) messages, provided exponentially many rounds are allowed [43]. This algorithm uses the KKT result to first construct a spanning tree and uses the “time encoding” trick to upcast the entire graph topology up the tree. In the “time encoding” trick, clock ticks are used to encode information, and a node can convey a lot of information by being silent for many clock ticks and then sending a bit at an appropriately chosen clock tick. The main question that we address in this paper is whether singularly optimal algorithms for the above-mentioned fundamental global problems are possible in the \text{KT}_{1} LOCAL model (where the size of a message is not restricted). Surprisingly, the answer is yes! Note that this is not obvious. In particular, the KKT algorithm to construct a spanning tree (even) in the LOCAL model takes \tilde{O}(n) rounds in the worst case. On the other hand, one can construct a BFS tree and aggregate the entire topology in O(D) rounds which is time optimal, but it is not known how to construct a BFS tree in \tilde{O}(n) messages in \text{KT}_{1} LOCAL. 1.1 Distributed Computing Model As is standard, the communication network is modeled as an undirected graph G=(V,E), n=|V|, m=|E|. Nodes in the graph can thus be viewed as processors or machines, and each node has a unique ID drawn from a space whose size is polynomial in n. The undirected edges model communication links. We assume the synchronous communication model, where both computation and communication proceed in lockstep, i.e., in discrete time steps called rounds. In the CONGEST model we allow only small message sizes (typically logarithmic in n, the number of nodes) to be sent per edge per round, whereas in the LOCAL model the message size is unrestricted. In each round of the synchronous model, each node (i) receives all messages sent to it in the previous round, (ii) performs arbitrary local computation based on the information it has, and (iii) sends a message to each of its neighbors in the graph. For an algorithm \mathcal{A} in the synchronous model (whether LOCAL or CONGEST), its round complexity is the number of rounds it takes to finish and produce output, and its message complexity is the total number of messages sent by all nodes throughout the algorithm. The CONGEST and LOCAL models come in two standard versions, depending on the type of initial knowledge available to nodes. In the \text{KT}_{0} (Knowledge Till radius 0) model, also called the clean network model [41], nodes have initial knowledge of only themselves and do not know anything about their neighbors (e.g., IDs of neighbors). In the \text{KT}_{1} model, each node has initial knowledge of itself and the IDs of its neighbors. So in the \text{KT}_{1} model, a little knowledge about neighbors comes for free. Both \text{KT}_{0} and \text{KT}_{1} have been used extensively in the distributed computing literature for several decades. From a round complexity point of view, it is unnecessary to distinguish between the \text{KT}_{0} and \text{KT}_{1} versions because it takes just one round for nodes to gather IDs of all neighbors. However, this distinction turns out to be quite critical for message complexity as discussed earlier. 1.2 Our Results and Techniques Our Main Result. Our main result is that singular optimality is achievable in the \text{KT}_{1} LOCAL model for global problems. This partially answers the central motivating question raised above and shows that unrestricted message sizes allow one to solve various global problems in a singularly optimal fashion in \text{KT}_{1}. Specifically, we show that all global problems, including the key BFS tree construction problem, can be solved in \tilde{O}(D) rounds and \tilde{O}(n) messages, where both bounds are optimal up to polylogarithmic factors. Furthermore, we show this can be achieved deterministically. We also note that, as mentioned earlier, due to a result of [2], any algorithm (even randomized) that breaks the \Omega(m) message lower bound has to be non-comparison based in \text{KT}_{1} CONGEST. In contrast, our algorithms are comparison-based thus showing that the \Omega(m) message lower bound of [2] does not apply to \text{KT}_{1} LOCAL. Although our results (detailed below) do not answer the question of whether singular optimality is possible in \text{KT}_{1} CONGEST, they give several insights into the complexity of solving problems under the \text{KT}_{1} assumption. Singularly Optimal Algorithms. We present two singularly optimal algorithms, one randomized and one deterministic, for solving the BFS tree construction problem, our main technical contribution. Using the BFS tree construction, we show how to solve several other problems, including leader election. The two algorithms have similar complexities but follow two different approaches, illustrating the power of \text{KT}_{1} LOCAL. A Local Approach. In the first approach (cf. Section 3), we build a BFS tree in the usual “level-by-level” fashion starting from the root node. This clearly takes O(D) rounds. However, if done naively, this BFS exploration from level i to level i+1 (root is level 0) takes a number of messages proportional to the number of edges between the two levels as well as the edges between nodes in level i. This leads to O(m) message complexity overall, where m is the total number of edges. To obtain \tilde{O}(n) message complexity instead, a crucial idea is to use a sparse neighborhood cover to ensure that the number of messages needed to grow the BFS tree from level i to level i+1 is essentially (up to the O(\log n) factor) proportional to the number of BFS tree edges between the levels. Importantly, we show that sparse neighborhood cover construction (which is done as a preprocessing step) itself can be done singularly optimally in O(\log^{3}n) rounds and O(n\log^{3}n) messages in \text{KT}_{1} LOCAL (cf. Section 2.3). We refer to Section 3 for details regarding how the cover is useful in avoiding sending unnecessary messages through non-BFS tree edges. This approach is randomized since we use the sparse neighborhood cover construction algorithm due to Elkin [16], which is randomized. We modify this algorithm to use \tilde{O}(n) messages (we note that the message complexity of the original implementation is \tilde{O}(m) [16]). As noted at the end of Section 3, we could have also used a deterministic cover construction scheme. However, this has a significantly higher (in terms of polylog factors) message and time complexities compared to the randomized algorithm and a more efficient deterministic algorithm that uses a global approach (described below). The main feature of the local approach algorithm is that BFS tree construction can be done in a fashion where each node needs to know information only from its local neighborhood. Specifically, the cover construction algorithm and the subsequent BFS exploration can be implemented in such a way that each node needs to know the topology information of nodes (including its IDs and state information) of only its O(\log n) neighborhood (cf. Section 3). A Global Approach. Our second algorithm (cf. Section 4) for BFS tree (which is also deterministic) uses a different approach. It first uses a deterministic gossip-based algorithm due to Haeupler [25] to construct a sparse spanning subgraph H of the original graph G. In gossip, each node, in each round, can send at most one message (of arbitrary size) to one neighbor. Gossip-based communication is very lightweight (only O(n) messages are sent by all nodes in a round) and has been studied extensively [30]. Here we use a result due to Haeupler which showed that each node can (locally) broadcast a message to all nodes within a distance k in O(k\log n+\log^{2}n) rounds in the \text{KT}_{1} LOCAL model [25]. Let the edges used in the Haeupler algorithm to send messages constitute the subgraph H. We show that H has only O(n\log n) edges and is a O(\log n)-spanner, i.e., all distances in H between each of pair of nodes are at most a O(\log n) factor of the (true) distance in G [41]. In particular, the property we need is that the diameter of H is O(D\log n) where D is the diameter of G. Once H is built, one can simply do a usual flooding-based BFS tree construction (from the given source node) in H as in the \text{KT}_{0} CONGEST model (e.g., see [41]). Since H has O(n\log n) edges and O(D\log n) diameter, these determine the same message and time complexities respectively for the BFS tree construction in H. Once a BFS tree T_{H} has been constructed in H, we use it to collect the entire topology of G at the root of T_{H} by convergecasting. In the LOCAL model, this takes O(D\log n) rounds and O(n\log n) messages. Since the root has the entire topology, it can construct a BFS tree on G locally, and then broadcast the solution to all nodes in O(D\log n) and O(n\log n) messages using the BFS tree of H. This approach to BFS construction is “global” since the root needs the entire graph’s topology information. Leader Election. Both approaches enable one to solve leader election efficiently in \text{KT}_{1} LOCAL as well. If we allow randomization (cf. Section 3), we can start with \Theta(\log n) candidates (this can be achieved by each node becoming a candidate with probability \Theta(\log n/n)). Then each of the candidates can run the BFS tree construction algorithm in parallel to elect a leader among the \Theta(\log n) candidates (e.g., the one with the highest ID). In the deterministic setting (cf. Section 4), we simply run a deterministic leader election algorithm of Kutten et al. [34] on the O(\log n)-spanner H of G constructed earlier. Global Problems. In both approaches, one can solve any global problem (i.e., a problem that requires \Omega(D) rounds) in \text{KT}_{1} LOCAL in a singularly optimal fashion, i.e., in \tilde{O}(D) rounds and \tilde{O}(n) messages, as follows. Once a leader is elected and a BFS tree is built, one can collect the entire topology by convergecasting it to the root in O(D) rounds and using \tilde{O}(n) messages. The root solves the problem locally and broadcasts the solution with the same time and message bounds. Summary of Results. To summarize, we present the following results. • A distributed randomized Monte Carlo algorithm that constructs a BFS tree (given a designated root node) in time O(D\log n+\log^{3}n) and using O(n\log^{3}n) messages in the \text{KT}_{1} LOCAL model. • A distributed deterministic algorithm that constructs a BFS tree (given a designated root node) in time O(D\log n+\log^{2}n) and using O(n\log^{2}n) messages in the \text{KT}_{1} LOCAL model. While we note that the bounds for the deterministic algorithm for BFS are better than the corresponding randomized algorithm, the deterministic algorithm follows the global approach where the root node learns information about the entire topology, unlike the randomized algorithm that follows a local approach where nodes learn information only within their O(\log n)-radius neighborhood. • A distributed randomized Monte Carlo algorithm that elects a leader in time O(D\log n+\log^{3}n) and using O(n\log^{4}n) messages in the \text{KT}_{1} LOCAL model. • A distributed deterministic algorithm that elects a leader in time O(D\log^{2}n+\log^{2}n) and using O(n\log^{2}n) messages in the \text{KT}_{1} LOCAL model. • All global problems, including broadcast, MST, shortest paths, minimum cut, etc., can be solved in the above (leader election) randomized and deterministic time and message bounds, respectively. 1.3 Other Related Work In the \text{KT}_{1} model, the early work of Awerbuch et al. [2] studied time-message trade-offs for broadcast. King et al. [31] surprisingly showed that the basic \Omega(m) message lower bound that holds in the \text{KT}_{0} model for various problems such as leader election, broadcast, MST, and more [34] does not hold in the \text{KT}_{1} model by showing a randomized Monte Carlo algorithm to construct an MST (which also holds for leader election and broadcast) in \tilde{O}(n) messages and in \tilde{O}(n) time. Their algorithm uses the powerful randomized technique of graph sketching to identify edges going out of a cut efficiently, without probing all the edges in the cut; this crucially helps in reducing the message complexity. In contrast, our work shows that graph sketching is not necessary to obtain \tilde{O}(n) message complexity in \text{KT}_{1} LOCAL. There has also been work in understanding the message complexity of local problems such as MIS, coloring, and maximal matching in \text{KT}_{0} and \text{KT}_{1}. In the \text{KT}_{0} CONGEST model, Pai et al. [37] showed that any MIS algorithm requires \Omega(n^{2}) messages. The same quadratic lower bound was shown to hold for (\Delta+1)-coloring [38] and also for maximal matching [15]. These lower bounds are all tight, to within a logarithmic factor, since Luby’s algorithm solves these problems using \tilde{O}(m) messages. In contrast, the message complexity of these problems is largely unknown in the \text{KT}_{1} CONGEST model. Pai et al. [38] showed an \Omega(n^{2}) lower bound for comparison-based algorithms for MIS, (\Delta+1)-coloring, and maximal matching in this model. However, whether one can obtain an o(m)-message algorithm is open for arbitrary algorithms. It is worth noting that a quadratic lower bound for comparison-based algorithms does not constitute strong evidence that the problem may have a quadratic lower bound in general. For example, it was shown that (\Delta+1)-coloring can also be solved using \tilde{O}(n^{1.5}) messages in \tilde{O}(D+\sqrt{n}) rounds in the \text{KT}_{1} CONGEST model [38]. The quest for singularly optimal distributed algorithms has also been investigated in the asynchronous setting [36, 33, 14] and with more relaxed assumptions on the nodes’ initial knowledge [29]. Recently there has been a lot of interest in designing awake or energy-efficient algorithms for various problems in the LOCAL, CONGEST, and radio network models—see, e.g., [10, 6, 13, 20, 18, 9, 11, 22, 1] and references therein. In these works, the goal is to minimize the number of rounds a node is active (when it could be sending/receiving messages). It might be interesting to study singular optimality in the context of awake or energy-efficient algorithms, where the goal is to simultaneously minimize the message, round, and awake/energy complexities."
https://arxiv.org/html/2411.06895v1,: Secure and Adaptive Blockchain Sharding Protocol with Hybrid Consensus and Dynamic Shard Management,"Blockchain sharding has emerged as a promising solution to the scalability challenges in traditional blockchain systems by partitioning the network into smaller, manageable subsets called shards. Despite its potential, existing sharding solutions face significant limitations in handling dynamic workloads, ensuring secure cross-shard transactions, and maintaining system integrity. To address these gaps, we propose DynaShard, a dynamic and secure cross-shard transaction processing mechanism designed to enhance blockchain sharding efficiency and security. DynaShard combines adaptive shard management, a hybrid consensus approach, plus an efficient state synchronization and dispute resolution protocol. Our performance evaluation, conducted using a robust experimental setup with real-world network conditions and transaction workloads, demonstrates DynaShard’s superior throughput, reduced latency, and improved shard utilization compared to the \acftsbs method. Specifically, DynaShard achieves up to a 42.6% reduction in latency and a 78.77% improvement in shard utilization under high transaction volumes and varying cross-shard transaction ratios. These results highlight DynaShard’s ability to outperform state-of-the-art sharding methods, ensuring scalable and resilient blockchain systems. We believe that DynaShard’s innovative approach will significantly impact future developments in blockchain technology, paving the way for more efficient and secure distributed systems.","Blockchain technology has emerged as a groundbreaking innovation that has transformed the landscape of digital transactions, offering a decentralized, transparent, and secure framework for various applications [42, 18, 19, 17], including \aciot-based systems [30, 20, 8, 23]. The core principles of blockchain, such as immutability, transparency, and distributed consensus, have the potential to revolutionize \aciot networks by enhancing security, data integrity, and trust among devices and participants [50]. However, the scalability of blockchain systems remains a significant challenge, particularly for \aciot applications that require high transaction throughput and low latency [28, 22, 7, 21]. The increasing number of \aciot devices and transactions leads to longer confirmation time, higher transaction fees, and reduced overall system performance [3]. To address this scalability challenge, researchers have proposed various solutions, including off-chain scaling techniques like payment channels [10, 12, 27] and sidechains [15, 35, 9], as well as on-chain scaling approaches like sharding [36]. By improving scalability, these solutions can enable the seamless integration of blockchain technology with \aciot systems, ensuring efficient and secure management of \aciot data and transactions. Existing efforts Blockchain sharding has gained significant attention as a promising on-chain scaling solution that aims to improve the throughput and latency of blockchain systems by partitioning the network into smaller shards, each responsible for processing a subset of transactions in parallel [36]. By distributing the transaction processing workload across multiple shards, blockchain sharding enables the system to scale horizontally, allowing for a higher transaction throughput and lower confirmation time [25]. Several blockchain sharding frameworks have been proposed in recent years, each addressing specific aspects of the sharding process. For instance, Elastico [24] introduces a secure sharding protocol that utilizes a distributed randomness generation process for shard formation and a Byzantine fault-tolerant consensus mechanism within each shard. OmniLedger [13] builds upon Elastico by incorporating a more efficient cross-shard transaction processing mechanism based on atomic commits and a ledger pruning technique to reduce storage overhead. RapidChain [51] further improves the scalability of sharded blockchains by introducing a fast and efficient cross-shard transaction verification scheme that leverages intra-shard consensus and inter-shard gossiping. These frameworks have laid the foundation for the development of scalable and efficient blockchain sharding solutions. Research gap Despite the progress made in blockchain sharding, several limitations and research gaps still exist, presenting opportunities for further improvement. One major challenge is the lack of dynamic and adaptive mechanisms for managing shards based on the system’s workload [6, 37]. Most existing sharding frameworks rely on static shard configurations, which can lead to suboptimal resource utilization and performance, especially in the presence of fluctuating transaction volumes and network conditions [11]. Another significant challenge lies in ensuring the security and atomicity of cross-shard transactions [4]. Malicious actors may attempt to exploit the distributed nature of sharded systems by launching attacks such as double-spending, replay attacks, or shard-level collusion [52]. Existing cross-shard transaction processing techniques, such as two-phase commit protocols [2] and asynchronous consensus [37], provide some level of protection against these attacks, but they often come at the cost of increased complexity and communication overhead [6]. Furthermore, there is a lack of comprehensive frameworks that integrate shard reconfiguration, state synchronization, and dispute resolution mechanisms to ensure the overall security and efficiency of sharded blockchain systems. Designing a holistic solution that addresses these challenges while maintaining the core principles of decentralization, transparency, and security is a non-trivial task that requires careful consideration of various trade-offs and design choices. DynaShard To bridge the research gap, we propose DynaShard, a dynamic and secure cross-shard transaction processing mechanism. DynaShard combines adaptive shard management, secure cross-shard transaction processing, and efficient state synchronization and dispute resolution to enhance scalability and resilience in blockchain systems. By dynamically adjusting shard configurations based on workload, it optimizes resource utilization and performance, adapting to varying transaction volumes and network conditions. It employs a hybrid consensus approach that integrates intra-shard and inter-shard mechanisms to minimize coordination overhead while ensuring transaction integrity and consistency. Designing a dynamic and secure cross-shard transaction processing mechanism involves several challenges. C1: Effective shard management requires monitoring workload and resource usage to make informed decisions on splitting or merging shards, necessitating an understanding of system dynamics and future transaction predictions. C2: Secure and efficient cross-shard transaction processing demands a protocol ensuring atomicity and consistency while minimizing overhead, incorporating novel consensus mechanisms, cryptographic techniques, and scalable data structures. C3: Robust state synchronization and dispute resolution require decentralized mechanisms to detect and resolve inconsistencies and conflicts, integrating insights from distributed systems, cryptography, game theory, and economics. DynaShard addresses these challenges by introducing a comprehensive framework that dynamically adjusts shard configurations based on transaction volume and resource usage, employs a hybrid consensus approach combining lightweight global consensus with parallel intra-shard processes, and utilizes Merkle trees alongside a decentralized dispute resolution protocol to maintain consistency and resolve conflicts in a trustless manner. Novelty The key novelty of DynaShard lies in its holistic and adaptive approach to blockchain sharding, distinguishing it from existing solutions. Unlike previous works that focus on specific aspects such as shard formation [24], cross-shard transaction processing [2], or consensus mechanisms [51], DynaShard integrates all these components into a cohesive and dynamic system. The novelty of DynaShard includes: (i) continuously monitoring and adjusting shard configurations based on system workload, offering an efficient and flexible sharding scheme adaptable to the evolving demands of real-world blockchain applications; (ii) employing a hybrid consensus approach to address the challenge of secure and atomic cross-shard transaction processing, balancing global coordination with local processing efficiency; and (iii) integrating adaptive shard management, secure cross-shard transaction processing, and efficient state synchronization and dispute resolution techniques. In summary, this paper makes the following contributions: • We propose DynaShard, an adaptive shard management mechanism that dynamically adjusts shard configurations based on transaction volume and resource usage, ensuring optimal performance and resource utilization. • We propose a secure and atomic cross-shard transaction protocol using a hybrid consensus approach. This integrates lightweight global consensus with parallel intra-shard processes, reducing overhead while maintaining transaction integrity and consistency. • We develop an efficient shard state synchronization mechanism based on Merkle trees. This mechanism maintains consistency across shards and incorporates a decentralized dispute resolution protocol to resolve potential conflicts in a trustless and resilient manner. • We perform comprehensive evaluation of DynaShard through theoretical analysis and experimental simulations. Results demonstrate improvements in throughput, latency, and shard utilization compared to existing solutions, validating its effectiveness and robustness."
https://arxiv.org/html/2411.06788v1,Designing Local Distributed Mechanisms,"In this work we introduce a new notion: local mechanisms. These are truthful mechanisms that have an implementation as fast distributed algorithms and non-trivial approximation guarantees. We show how monotone distributed optimisation algorithms can be turned into truthful mechanisms using Myerson’s Lemma. We demonstrate mechanisms for four fundamental graph problems: maximum-weight independent set, minimum-weight vertex cover, minimum-weight dominating set, and a variant of weighted colouring.We show how these mechanisms can be implemented in the distributed setting. The key observation is that computing the so-called critical prices of a monotone algorithm can be done with the same time complexity as the original algorithm in the LOCAL model of distributed computing. Our work establishes a new connection between algorithmic mechanism design and distributed graph algorithms. We pose several open questions, such as can critical prices be computed with small messages. It also points to the importance of designing monotone distributed optimisation algorithms.Our work extends previous work in Distributed Algorithmic Mechanism Design (DAMD) in a new direction. Instead of studying global problems like routing or leader election, we study local resource allocation problems. Our algorithms are simple and thus potentially practical. Local algorithms are particularly interesting for highly dynamic large-scale systems, and there are many potential future application domains, e.g. demand-side load management in electric grids or resource allocation in IoT computing.","In mechanism design the goal is to implement a social choice function as a game. Participating agents hold private information that is critical to choosing a good assignment. The task of the mechanism designer is to come up with an algorithm and incentives such that the agents should always reveal their private information to the mechanism. A classic example is selling a single item: how should we assign the item and what should it cost? One of the most common auctions is a sealed-bid first-price auction: each participant sends a bid in secret and the largest bidder gets the item, paying its bid. The problem with this mechanism is that it is game-theoretically unstable: the optimal behaviour for the winner is to bid just above the (unknown) second-highest bid. Trying to reason about the valuations of the other agents makes participating in this auction difficult. The second-price auction fixes this issue: the winner pays the second bid, removing the need to guess what the second bid was [Vic61]. This auction is incentive-compatible: bidding the true valuation is a dominant strategy. Mechanism design has many important practical applications in addition to auctions. The deferred acceptance algorithm can be used to assign students to schools [GS62, otRSAoS12] and the top trading cycles algorithm to create stable chains of organ donations [RSÜ04]. Google uses large-scale automated auctions to sell the advertisements on its searches [EOS07]. Mechanism design typically assumes a single entity that runs the underlying algorithm and with which all participating agents must interact. In this work we study how this bottleneck can be removed: we want to design mechanisms that can be implemented as fast distributed algorithms. Mechanism design for distributed systems has been studied previously (distributed algorithmic mechanism design (DAMD)) [FSS07]. To our knowledge, all previous work in this area has been on fundamentally global problems, such as routing and leader election. In contrast, we want to study fundamentally local graph problems, where the goal is to solve locally constrained graph problems, such as scheduling conflicting transmissions or ensuring spatial coverage. We believe understanding such mechanisms will be an important building block in future automated systems: as examples, mechanisms have been proposed for resource allocation in IoT networks [SMH17] and simple mechanisms are already used to change consumer behaviour in electric grids, trying to match electricity demand and generation [Jor19]. We introduce the notion of local distributed mechanisms. These are mechanisms for resource allocation in networks that can be implemented as fast distributed algorithms. As an example, consider the problem of computing an independent set of large weight in the mechanism design setting: each agent has a private parameter, its weight, that tells how much utility it gains from being part of the independent set. The goal is to, essentially, arrange a distributed auction to decide which agents should join the independent set. We study distributed algorithms in the standard LOCAL and CONGEST models [Pel00]. A distributed algorithm is local, if its output for each node in the network only depends on the inputs around that node. In contrast, a problem is global, if it requires \Omega(n) time (allowing information to be gathered from the whole network). A distributed auction can be designed based on the result known as Myerson’s lemma [Mye81]. This result states that in single-parameter settings such as the independent set problem, an incentive-compatible mechanism exists exactly when the allocation rule is monotone: increasing an agent’s bid will never cause it to no longer be selected. Crucially the result does not depend in any way on the algorithm that implements the allocation rule: we can use a monotone distributed optimisation algorithm. Myerson’s lemma also gives the formula for the payments. In the context of the independent set mechanism, these are the so-called critical prices: each selected agent has to pay the smallest bid that would have gotten it selected. This is a generalisation of the second-price auction [Vic61]: the algorithm essentially bids optimally for a first-price auction. Local distributed mechanisms are well-suited to dynamic settings, as the input changing in one part of the network only requires re-computation in the local neighbourhood of the change. The mechanism design setting typically requires a centralised authority to run the mechanism – A local mechanism can be run in distributed manner without a single point of failure. Local mechanisms may also be of interest from the centralised perspective, as such mechanisms are by definition decomposable. This allows for a natural parallelisation to multiple entities responsible for running the mechanism. We emphasise that in this work, the assumption is that the communication network itself is not controlled by the strategic agents. Instead, we assume a trusted distributed network that will faithfully execute the distributed implementation of the mechanism based on the values reported by the agents. 1.1 Our contribution We present incentive-compatible approximate mechanisms for several classic graph problems. These mechanisms are implemented as distributed algorithms in the standard LOCAL and CONGEST models of message passing algorithms [Pel00]: These are the first examples of local mechanisms, and our work opens a new direction in the study of distributed graph algorithms. In Section 3 we discuss the generic template for a distributed mechanism given by Myerson’s Lemma (Theorem 1). The key insight is that there is a large class of optimisation problems such that any monotone optimisation algorithm can be automatically transformed into an incentive-compatible mechanism (Theorem 2). An optimisation algorithm is monotone if winning agents cannot lose by bidding more (in maximisation problems). To demonstrate specific mechanisms, we present four mechanisms for classic graph problems. Here n denotes the number of nodes, \Delta the maximum degree, and W the number of possible private values (see third challenge, below). 1. Maximum-weight independent set (MWIS): we present a mechanism that computes a \Delta-approximation and runs in O(\Delta W+\log^{*}n) rounds in the CONGEST model. It is based on the simple greedy algorithm for MWIS [STY03] (Theorem 3). This is a mechanism for which we can compute the critical prices in the CONGEST model. 2. Minimum-weight vertex cover (MWVC): we present a mechanism that computes a 2-approximation and runs in O(\Delta+\log^{*}n) rounds in LOCAL model (Theorem 4). It is based on the local ratio algorithm [BYE81] which has no dependency on W in the running time. 3. Minimum-weight dominating set (MWDS): we present a mechanism that computes a (1+\ln(\Delta+1))-approximation and runs in O(\Delta^{3}W+\log^{*}n) rounds in the LOCAL model (Theorem 5). It is based on the greedy set cover algorithm [Joh74, Chv79]. 4. Slot assignment (a type of weighted graph colouring): we present a mechanism that computes an O(\Delta)-approximation and runs in O(\Delta W+\log^{*}n) rounds (Theorem 6). It uses a simple greedy algorithm. This mechanism showcases the application of Myerson’s Lemma to problems more general than selecting a subset of nodes. All mechanisms have optimal dependency on n, as breaking symmetry requires \Omega(\log^{*}n) rounds [Lin92]. We make the following standard assumption: Mechanisms compute solutions in the sense that bidding the true valuation is a weakly dominant strategy and therefore we assume that the agents bid truthfully. There are four main challenges to designing distributed local mechanisms: First, we must use monotone optimisation algorithms, which does restrict the design space. We mostly use greedy algorithms, but in Section 5 we consider the local ratio algorithm [BYE81] for weighted vertex cover. Second, in order to have a mechanism we must also be able to compute payment function. As we show in Section 3, this can be done with the same complexity as the original algorithm if we don’t put any limits on the size of the messages. However, this can be highly non-trivial when messages have bounded size, such as in the CONGEST model [Pel00]. Computing the critical prices given by Myerson’s Lemma with small messages is the key primitive for implementing mechanisms in the CONGEST model. Third, since we are using monotone algorithms, these are often greedy algorithms. Greedy algorithms typically rely on some sequential ordering in which the agents are considered. If we assume no restrictions on the weights, such weights can form arbitrarily long chains of dependency. To get around this issue, we show in Section 8 how the values can be discretised without losing the incentive-compatibility of the mechanism. Finally, fourth, a mechanism requires tie-breaking. In the centralised setting this is essentially trivial, but it becomes an issue in a distributed setting. We use a natural approach of colouring the input to break symmetry. This ensures that the tie-breaking does not cause new long chains of dependency. We discuss the necessity of colouring as tie-breaking in Section 2.3.2. 1.2 Related work There exist two lines of research that are related to the two aspects of our work. Distributed Algorithmic Mechanism Design (DAMD) was introduced for studying mechanism design for distributed systems [FPS01, FPSS02]. The goal is to have distributed implementations of mechanisms run by the strategic agents themselves. The problems studied in this line of research have been fundamentally global in nature (i.e. computation requires gathering information from the whole network to one point). These include problems such as routing [FPSS02, Rou07], multicast cost sharing [AFK+04, FPS01], scheduling [CG11] or leader election [AGLFS14]. For a survey on DAMD, we refer to a book chapter by Feigenbaum, Schapira, and Schenker [FSS07]. Second, in algorithmic mechanism design there has been some work on mechanisms for local graph problems. Studied problems include weighted vertex cover [Cal15, EGG07] and set cover [DMV03, LSW+10]. These works have been, however, from the centralised perspective. The Vickrey-Clarke-Groves (VCG) mechanism is a generic framework for solving any optimisation problem truthfully [Vic61, Cla71, Gro73]. This mechanism does not have a fast distributed implementation, as it requires the computation of an optimal solution. This requires linear time in the LOCAL model for any reasonable optimisation problem. Distributed graph algorithms for the tasks we study here have received a lot of attention, but most works do not consider any game-theoretic elements. Collet, Fraigniaud, and Penna [CFP18] study the equilibria of simple randomised algorithms for basic symmetry breaking tasks. Hirvonen, Schmid, Schmid, and Chatterjee [HSCS24] present games where best-response dynamics simulate distributed algorithms. The incentive-compatible deferred acceptance algorithm [GS62] for computing a stable matching has a natural implementation as a distributed proposal algorithm. While the operations are local, the algorithm can take \Omega(n^{2}) rounds to converge [OR15] and requires \Omega(n) rounds in the LOCAL model [FKPS10]. To circumvent this, Floréen, Kaski, Polishchuk, and Suomela [FKPS10] and Ostrovsky and Rosenbaum [OR15] study distributed algorithms for computing so-called almost stable matchings. These algorithms are no longer incentive-compatible. Hirvonen and Ranjbaran showed that the variant where one side has common preferences admits a local distributed algorithm [HR24]. 1.3 Open questions Our work demonstrates a connection between algorithmic mechanism design and distributed graph algorithms. This opens many questions regarding the implementation of mechanisms as distributed algorithms. (Q1) Mechanisms with small messages. Our generic distributed algorithm for implementing a mechanism based on a monotone optimisation algorithm requires that each node simulates the algorithm with different input to compute the critical prices. This is easy in the LOCAL model, but requires large messages. In Section 4 we show that a simple combinatorial property of the algorithm allows computing the critical prices in CONGEST. We ask if there exists a general method for computing the critical prices efficiently with small messages? (Q2) Monotonicity in distributed optimisation. Myerson’s lemma automatically gives a distributed mechanism based on any monotone distributed local optimisation algorithm. This means that it is important to understand which distributed algorithms are monotone and which are not. We propose that this is an important property that is of interest in any new proposed distributed optimisation algorithms. Similarly it is interesting to understand which algorithmic design techniques contradict this property. For example, algorithms working on some ”good” subset of the input most likely are not monotone, but, as we have shown in Section 8, grouping nodes by their weight can be effective and retain truthfulness. (Q3) Mechanisms with large degrees. We have studied distributed mechanisms with the assumption that the underlying graph has small maximum degree \Delta. This is because we use colouring for tie-breaking, forcing a linear in \Delta term into the running time of each of our algorithms. This appears to be difficult to avoid, as we discuss in Section 2.3.2. We ask whether there exist distributed mechanisms with non-trivial optimisation guarantees and sublinear-in-\Delta running times? (Q4) Strategy-proof implementations. In the distributed setting it would be preferable if the mechanism itself could be run in a strategy-proof way by the strategic agents themselves. One major challenge for this is that information cannot be propagated without further assumptions. For example, the model may assume that the communication network is complete, removing the issue of an agent withholding information by refusing to pass it along (correctly) [AGLFS14]. It is important to understand what would be the minimal assumptions that would allow strategy-proof execution of distributed graph algorithms. (Q5) Impossibility results for mechanism design. Since distributed mechanisms operate in a narrower design space, it is natural to ask if we can prove improved lower bounds for monotone distributed optimisation. In particular, it would be interesting to understand if the running time dependencies on \Delta and W are necessary. 1.4 Outline We continue in Section 2 by giving the necessary background information in mechanism design and distributed computing. In Section 3 we present the template for designing distributed local mechanisms for optimisation problems. In Sections 4–7 we present four mechanisms for different optimisation problems. In Section 8 we show how real-valued weights can be turned into discrete weights without losing the incentive-compatibility of a mechanism."
https://arxiv.org/html/2411.07182v1,"Revisiting Ensembling 
in One-Shot Federated Learning","FL is an appealing approach to training machine learning models without sharing raw data. However, standard FL algorithms are iterative and thus induce a significant communication cost. \AcOFL trades the iterative exchange of models between clients and the server with a single round of communication, thereby saving substantially on communication costs. Not surprisingly, one-shot federated learning (OFL) exhibits a performance gap in terms of accuracy with respect to federated learning (FL), especially under high data heterogeneity. We introduce Fens, a novel federated ensembling scheme that approaches the accuracy of FL with the communication efficiency of OFL. Learning in Fens proceeds in two phases: first, clients train models locally and send them to the server, similar to OFL; second, clients collaboratively train a lightweight prediction aggregator model using FL. We showcase the effectiveness of Fens through exhaustive experiments spanning several datasets and heterogeneity levels. In the particular case of heterogeneously distributed CIFAR-10 dataset, Fens achieves up to a 26.9\% higher accuracy over state-of-the-art (SOTA) OFL, being only 3.1\% lower than FL. At the same time, Fens incurs at most 4.3\times more communication than OFL, whereas FL is at least 10.9\times more communication-intensive than Fens.","\Ac FL is a widely adopted distributed machine learning (ML) approach, enabling clients to collaboratively train a common model over their collective data without sharing raw data with a central server [27]. Clients in FL engage in iterative parameter exchanges with the server over several communication rounds to train a model. While providing high accuracy, this process incurs substantial communication cost [19]. \AcfOFL [11] has been introduced to address the communication challenges in FL by reducing the exchange of models to a single round. Not surprisingly, this came with a loss of accuracy with respect to FL. Typical OFL methods execute local training at the clients up to completion and form an ensemble of locally trained models at the server [7, 10, 46, 11]. The ensemble is distilled into a single model, through means of either auxiliary public dataset [10, 11] or synthetic data generated at the server [7, 14, 46]. While these OFL methods address communication challenges by reducing model exchanges to a single round, they often achieve lower accuracy compared to iterative FL. This is especially true when data distribution across clients is highly heterogeneous as OFL methods typically rely on simple prediction aggregation schemes such as averaging [11, 46], weighted averaging [7, 10] or voting [8]. We introduce Fens, a hybrid of OFL and standard FL. Fens aims to approach both the accuracy of iterative FL as well as the communication cost of OFL. Learning in Fens proceeds in two phases. In the first phase, similar to OFL, clients upload their locally-trained models to the server. Instead of using the traditional OFL aggregation, Fens employs a second phase of FL: the server constructs an ensemble with a prediction aggregator model stacked on top of the locally trained models. This advanced aggregation function is then trained by the clients in a lightweight FL training phase. The overall learning procedure is illustrated in Figure 1, alongside iterative and one-shot FL. Figure 1: Fens in comparison to iterative and one-shot federated learning. 1.1 Our Contributions Figure 2: Test accuracy and communication cost of OFL, Fens and FL on CIFAR-10 dataset under high data heterogeneity. We show for the first time, to the best of our knowledge, that a shallow neural network for the aggregator model suffices to satisfactorily bridge the gap between OFL and FL. Leveraging a shallow aggregator model enables two major benefits: first, it induces significantly lower communication cost in the iterative phase, and second, the iterative refinement of this aggregator model significantly improves accuracy over existing OFL methods. By utilizing elements from both OFL and FL in this novel ensembling scheme, Fens achieves the best of both worlds: accuracy of FL and communication efficiency of OFL. Through extensive evaluations on several benchmark datasets (CIFAR-100, CIFAR-10, SVHN, and AG-News) across different heterogeneity levels, we demonstrate the efficacy of Fens in achieving FL-like accuracy at OFL-like communication cost. We extend our empirical evaluations to the FLamby benchmark [32], a realistic cross-silo FL dataset for healthcare applications. Our results show that in heterogeneous settings where even iterative FL algorithms struggle, Fens remains a strong competitor. We then conduct an extensive study of different aggregator models and highlight the accuracy vs. communication trade-off. Lastly, we show that Fens maintains high accuracy even with a comparable memory footprint. To showcase Fens’s performance, we compare its accuracy and communication costs against Co-Boosting [7], a state-of-the-art OFL method, and FedAdam [33], a popular iterative FL algorithm, as shown in Figure 2. These evaluations are performed on the CIFAR-10 dataset with 20 clients across three heterogeneity levels: \alpha=0.01 (very high), \alpha=0.05 (high), and \alpha=0.1 (moderate). Co-Boosting exhibits an accuracy gap of 13.7-26.9\% compared to FedAdam. Fens closes this accuracy gap, being only 0-3.1\% lower than FedAdam. To achieve this, Fens incurs only 3.8-4.3\times more communication than Co-Boosting whereas FedAdam is 10.9-22.1\times more expensive than Fens. 1.2 Related Work One-shot Federated Learning. Guha \etal[11] introduced one-shot FL, which limits communication to a single round. They proposed two main methods: (i) heuristic selection for final ensemble clients, and (ii) knowledge distillation(KD) for ensemble aggregation into a single model at the server using an auxiliary dataset. Subsequent methods based on KD [10, 22] require large, publicly available datasets similar to local client data for good performance, which are often difficult to obtain [50]. To address this, synthetic data generation using generative adversarial networks (GAN)s has been proposed [7, 46]. The SOTA Co-Boosting algorithm [7] iteratively generates and refines synthetic data and the ensemble model. In FedCVAE-Ens [14], clients train variational autoencoders locally and upload decoders to the server, which generates synthetic samples for classifier training. FedOV [8] trains an open-set classifier at each client to predict “unknown” classes, with the server ensembling these models and using open-set voting for label prediction. Other OFL approaches either do not fully consider data heterogeneity [22, 37], or face difficulties under high data heterogeneity [48]. Another line of research in OFL focuses on aggregating fully trained client model parameters [42, 45]. PFNM [45] matches neurons across client models for fully-connected networks, while FedMA [42] extends this to convolutional neural networks and LSTMs. However, the performance of these methods drops with more complex models. Few theoretical works exist, such as [18], which analyze global model loss for overparameterized ReLU networks. Despite the advances, OFL still exhibits accuracy gap with iterative FL. We show that Fens narrows this accuracy gap while preserving communication efficiency. Ensembles in Federated Learning. Ensembles have been previously studied in FL for a variety of different goals. FedDF [24] performs robust model fusion of client ensembles to support model heterogeneity. The FedBE algorithm [5] uses Bayesian Model Ensemble to aggregate parameters in each global round, improving over traditional parameter averaging. Hamer \etalpropose FedBoost [12] that constructs the ensemble using simple weighted averaging and analyze its optimality for density estimation tasks. However, these works are designed for standard FL and rely on substantial iterative communication. In the decentralized edge setting, [38] show that collaborative inference via neighbor averaging can achieve higher accuracy over local inference alone. However, they assume a setting where clients can exchange query data during inference and consider only IID data replicated on all edge devices. The idea of learning an aggregator model closely resembles late fusion techniques in multimodal deep learning [25]. The key difference is that Fens focuses on fusing single modality models trained on heterogeneous data under the communication constraints of federated settings."
https://arxiv.org/html/2411.07168v1,Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network,"Mining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations—on-device, on-gateway, or on-cloud—based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90% classification accuracy, while cloud-based inference reached 99%. On-sensor inference reduced power consumption by approximately 44%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.","THE mining sector is a vital component of the global resource economy, providing the raw materials essential for industrial production and infrastructure development. By 2026, the industry is projected to reach a market value of $3.36 trillion [1], driven largely by the growing demand for minerals such as lithium, cobalt, and copper, which are critical components in renewable energy technologies [2], electric vehicles [3], and consumer electronics [4]. Mining operations are inherently complex, encompassing multiple stages like exploration, extraction, processing, and transportation. To maintain efficiency and safety, the sector greatly depends on a wide range of machinery and mobile and semi-mobile equipment, including drilling rigs, shovels, excavators, haul trucks, front-loaders, and other auxiliary equipment [5]. These operations often occur in harsh, remote environments, exposing assets to extreme conditions such as high temperatures, humidity, dust, and heavy loads [6]. Prolonged exposure to these conditions leads to equipment degradation, reduced remaining useful life, increased maintenance costs, and elevated safety risks. PdM has become critical for optimizing mining operations, offering benefits such as improved system availability, cost savings, and enhanced failure prediction [7].By leveraging data from continuous condition monitoring sensors, PdM enables proactive decision-making and timely maintenance, reducing the risk of unplanned downtime [8]. The rise of Artificial Intelligence (AI) has further advanced PdM, as Machine Learning (ML) and Deep Learning (DL) algorithms can analyze vast datasets, identify patterns, and predict equipment failures more accurately than traditional methods [9]. Internet of Things (IoT) technologies, particularly Wireless Sensor Networks (WSNs), have become key components for collecting real-time data on machinery performance [10, 11, 12]. WSNs consist of spatially distributed sensor nodes and gateways that communicate wirelessly to collect and transmit data [13]. Typically, these sensor nodes are small-size, light-weight, energy-efficient, cost-effective and remarkably flexible to deploy, making them ideal for mining environments [14]. PdM frameworks are structured methodologies that encompass the entire PdM process: from data collection, preprocessing, and communication to ML and DL model development, training, and deployment. Traditional PdM frameworks often rely on a fixed inference location, either at the cloud in a dedicated serverless service (in a server on-premise far away from the operation instead) or closer to the edge on a gateway or sensor node [15, 16, 17]. Each approach has its advantages and limitations. Cloud-based inference offers superior accuracy and scalability at the cost of high latency and the need for stable network connectivity [10, 18]. Edge-based inference, both on gateways and nodes, minimizes latency and enables real-time decision-making but may increase power consumption and limited model complexity [11, 12, 19, 20]. Motivation: Each condition monitoring approach and PdM solution has its own strengths and weaknesses. Regardless of the inference approach, there is not a single technique that can detect, diagnose and predict all types of faults optimally. This call for more flexible and adaptive solutions for faster and accurate maintenance predictions under uncertainty in operational conditions. In this context, the inference location is critical to ensure high the system’s performance. The inference location in a hierarchical inference network with several levels directly determinates both the speed and accuracy of detected events. A fixed inference location may be inadequate due to dynamic changes in operational conditions over time. For instance, different expertise of machinery operators or mining fronts with different shape and leveling, expose equipment to different stress and wear levels. By leveraging on-cloud, on-gateway, and on-device inference capabilities, the system dynamically can adjusts inference locations based on trade-offs between real-time demands and conditions such as accuracy, latency, and battery range. By adapting the inference location dynamically, the PdM can be optimized the condition monitoring process, leveraging cloud resources when accuracy is critical and shifting to edge computing when real-time decision-making is mandatory. This paper presents a PdM framework that integrates edge inference approaches (such as on-gateway and on-sensor-based inference) and cloud computing services into an unified hierarchical inference system to enhance real-time condition monitoring of heavy machinery. The proposed framework leverages the strengths of each inference level to provide real-time and energy-efficient condition monitoring, adapting the inference location based on operational demands and conditions such as latency, accuracy, and energy consumption. The ESN-PdM system is evaluated through a case-study in a real-world industrial scenario, where vibration data from mining equipment is used to evaluate the operational state of the machinery and triggering alarms when anomalies arise. The main contributions of this work are as follows: • An open-source, end-to-end framework for condition monitoring and PdM of mobile mining machinery in non-stationary operations. • A novel adaptive inference mechanism that dynamically updates the inference location for a node based on operational conditions. • A guide on how to use state-of-the-art TinyML optimization approaches to achieve optimal accuracy and model compression for efficient deployment of DL models on limited hardware resources of IoT edge devices. • A comprehensive evaluation of the proposal in terms of operational status classification accuracy, inference latency, and node energy consumption through a real-world industrial case-study. This paper is organized as follows: Section II discusses about the applicability of time-varying classification of multivariate time series from mechanical and/or electrical systems, and addresses the further implementation challenges on TinyML and PdM frameworks. Section III presents the proposed ESN-PdM framework, describing its architecture, components, and adaptive inference mechanisms. Section IV introduces a case study based on DL strategies for PdM. In Section V, the ESN-PdM framework is evaluated in terms of classification accuracy for PdM, inference latency, and energy consumption. Finally, Section VI presents a summary of main findings and conclusions, and outlines future research directions."
https://arxiv.org/html/2411.06980v1,: Unleashing Storage Hardware-Software Co-design,"NVMe SSD hardware has witnessed widespread deployment as commodity and enterprise hardware due to its high performance and rich feature set. Despite the open specifications of various NVMe protocols by the NVMe Express group and NVMe being touted as the new language of storage, there is a complex labyrinth of software abstractions to program the underlying hardware. The myriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and userspace I/O libraries have different syntax and semantics that complicate software development and stand in the way of mass adoption and evolution of the NVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that exposes a single message-passing API to support both asynchronous and synchronous communication with NVMe devices. xNVMe provides various command sets to support diverse storage I/O paths in different OS (e.g., Linux, FreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal overhead. xNVMe is an Open Source project and has gained traction amongst various industry stakeholders. In this paper, we elaborate on the lessons that we have learned in the project during its evolution. We also provide some ongoing and future work planned for the project. We hope the database and storage systems community can join in the effort to both extend xNVMe and leverage it as a building block for innovative co-design of storage systems on modern NVMe hardware.","The past decade has witnessed the widespread evolution and deployment of NAND flash memory as commodity and enterprise storage hardware due to its high bandwidth and low latency. The growth of NAND flash SSDs has necessitated the birth of NVMe (Non-volatile memory express) access technology to sidestep the performance limitations of the SATA interface. The current global NVMe technology market share stands at 54.1 billion US$ and is speculated to grow to 412 billion US$ in 2031 (nvm, 2024b). To coordinate the evolution and interoperability of NVMe technologies, the NVM Express working group was formed to create open specifications that could be implemented by hardware and software vendors (nvm, 2024a). Despite the open specifications of NVMe hardware, the challenge of software abstractions to program NVMe hardware remains and is growing with its popularity. Classically, the POSIX storage abstractions (pread, pwrite) had been the holy grail of programmability, stability, and portability to hide the underlying hardware complexity. However, the rise of diverse NVMe hardware has created a fissure in this perfect world. The need for a low-overhead, asynchronous programming model to leverage the high performance of modern NVMe hardware has created multiple complex I/O storage stacks differing from the original POSIX API. For the Linux kernel, there is the POSIX aio_* APIs, libaio, and io_uring storage interfaces(Joshi et al., 2024) that applications can program against. These interfaces differ widely in their API, semantics, and performance. The landscape of storage interfaces gets even more complicated if you factor in different OS. The complexity of the storage I/O stack grows if you factor in support for newer SSD technologies such as ZNS SSDs, FDP SSDs, KV SSDs, and computational storage, to name a few. Either the rich-feature set is hidden behind the block layer interface of the OS or exposed via a userspace I/O stack such as SPDK, custom vendor libraries that are a thin shim over an NVMe device driver. This complexity and fragmentation of various storage I/O paths is unfortunate and creates unnecessary barriers in the adoption of the storage I/O paths by application stacks. The unnecessary complexity also stifles cooperation between software application designers and hardware vendors. As the usage of GPUs and hardware accelerators grows to support AI workloads, a similar API fragmentation is occurring for storage devices that require direct and efficient access by accelerators (Markussen et al., 2020; Qureshi et al., 2023). xNVMe was envisaged to fill the programmability gap for NVMe storage technologies by creating a single unified API that applications can program against to flexibly multiplex the desired storage I/O path with minimal overhead (Lund et al., 2022). Instead of creating yet another abstraction for storage, xNVMe provides a single message passing API for interacting with NVMe devices along with support for various storage I/O paths using this API. Since storage systems are not locked into the API and semantics of a specific I/O path, they can flexibly experiment with different storage I/O paths based on need. xNVMe currently supports various I/O paths in the Linux kernel e.g., libaio, io_uring and the classic POSIX abstractions of pread and pwrite. It also supports other OS e.g., FreeBSD and Windows and userspace I/O stack such as SPDK. xNVMe is an Open Source project (xnv, 2024a) that became publicly available in 2019. It began as an experimental platform for emerging NVMe interfaces (e.g., Open-Channel SSDs (Bjørling et al., 2017)) and matured over time, gaining traction among academic researchers and industry practitioners. In this experience cum vision paper, we present our experiences and lessons learned during the project that has shaped its evolution. We outline the reasons behind its existence in Section 3 and some of the ongoing and future work in the project in Section 5. Our aim with xNVMe is to lower the entry barrier of building innovative data-intensive systems that leverage features of modern NVMe hardware. By providing I/O storage independence and an Open Source collaboration environment, we hope the project can foster co-design of data-intensive software systems and NVMe hardware than what is currently possible."
https://arxiv.org/html/2411.06878v1,GraphRPM: Risk Pattern Mining on Industrial Large Attributed Graphs,"Graph-based patterns are extensively employed and favored by practitioners within industrial companies due to their capacity to represent the behavioral attributes and topological relationships among users, thereby offering enhanced interpretability in comparison to black-box models commonly utilized for classification and recognition tasks. For instance, within the scenario of transaction risk management, a graph pattern that is characteristic of a particular risk category can be readily employed to discern transactions fraught with risk, delineate networks of criminal activity, or investigate the methodologies employed by fraudsters. Nonetheless, graph data in industrial settings is often characterized by its massive scale, encompassing data sets with millions or even billions of nodes, making the manual extraction of graph patterns not only labor-intensive but also necessitating specialized knowledge in particular domains of risk. Moreover, existing methodologies for mining graph patterns encounter significant obstacles when tasked with analyzing large-scale attributed graphs. In this work, we introduce GraphRPM, an industry-purpose parallel and distributed risk pattern mining framework on large attributed graphs. The framework incorporates a novel edge-involved graph isomorphism network (EGIN) alongside optimized operations for parallel graph computation, which collectively contribute to a considerable reduction in computational complexity and resource expenditure. Moreover, the intelligent filtration of efficacious risky graph patterns is facilitated by the proposed evaluation metrics. Comprehensive experimental evaluations conducted on real-world datasets of varying sizes substantiate the capability of GraphRPM to adeptly address the challenges inherent in mining patterns from large-scale industrial attributed graphs, thereby underscoring its substantial value for industrial deployment.","Figure 1: Example risk patterns. Risk pattern A describes the behavior of fraudsters who defraud funds from multiple victim users and quickly transfer them to different downstream bank cards. Risk pattern B describes that the fraudster collects the victim’s funds multiple times in the name of investment through a shop, giving rewards in the early stage but no longer paying in the later stage. Precision and recall metrics are the evaluation criteria for measuring risk patterns in the industry. Graph pattern mining constitutes a pivotal task within the ambit of mining and machine learning, with profound applications extending to various industrial and business domains such as social network analysis [13], financial fraud detection [2, 10], and computational bioinformatics [21]. Taking the financial transaction scenario as an example, fraudsters would try to cheat normal users and make illegal money transfers. The distinctive behavioral patterns of these fraudsters, termed ’risk patterns’, are critical for the detection of fraudulent activity and the prevention of financial fraud, as exemplified in Fig. 1. Compared to black-box neural network models used for identifying fraudsters [12], industry experts express a preference for summarizing these risk patterns, as they provide more granular insight into the conduct of fraudulent entities, thereby facilitating a more explainable approach to fraud detection. Nonetheless, the manual delineation or construction of these patterns by experts is a labor-intensive process that demands considerable domain-specific knowledge. Consequently, the automation of risk graph pattern mining is an avenue warranting exploration. GRAMI [3] presents a method for frequent subgraph mining by leveraging a novel canonical labeling technique to efficiently discover patterns within a single large graph. Bliss [8] introduces an optimized tool for canonical labeling, specifically designed to handle the challenges posed by large and sparse graph structures, enhancing the performance of graph mining tasks. T-FSM [20] outlines a task-based framework that enables massively parallel processing for frequent subgraph pattern mining, addressing the scalability issues associated with big graph data. Despite this interest, extant automated graph pattern mining algorithms [3, 8, 14, 20] are impeded by two principal limitations: 1. Challenges in processing attributed graphs. In numerous real-world applications, simplistic representations of graph topology fall short of accurately depicting risk scenarios. There is a necessity to leverage high-dimensional attributes associated with nodes or edges for a nuanced characterization of entities, which is beyond the capabilities of methods that are restricted to or can only process one-dimensional attributes. 2. Deficiencies in scalability. Graph data within industrial environments is characteristically voluminous, spanning millions or even billions of nodes. Existing methodologies lack the integration of computational optimization strategies that are critical for the effective and efficient management of data at such an industrial scale. This shortfall in capability significantly undermines the suitability of these methods for application in industrial tasks, which necessitate robust data manipulation and analytical capacity to handle the sheer volume and complexity of the data involved. In this paper, we address the problem of Risk Patterns Mining on large transaction attributed graphs (GraphRPM). Although our research is primarily focused on financial fraud detection, the versatility of the proposed framework allows for its extension to a multitude of industrial applications, including but not limited to analysis within social network contexts. The challenge of managing and processing large-scale attributed graphs in industrial settings is a nontrivial hurdle, particularly in the realm of data mining. The primary objective of this study is to establish a robust and efficacious methodological framework capable of discerning distinct graph patterns as discriminative entities, enabling the differentiation of various graphical structures and the identification of fraud risk patterns. GraphRPM introduces a pioneering Edge-Involved Graph Isomorphism Network (EGIN) that addresses the challenge of fuzzy matching in attributed graph patterns, striking a balance between computational complexity and accuracy. Furthermore, this study implements a two-stage mining strategy coupled with a parallel distributed processing framework to diminish computational redundancy and enhance efficiency. Additionally, we present a Pattern Risk Score as an evaluative measure for identifying salient risk patterns. Comprehensive evaluations across diverse real-world datasets, varying in size, corroborate GraphRPM’s proficiency in resolving pattern mining issues within expansive industrial attributed graphs. Our research represents a significant advancement in the application of data mining and machine learning to industrial and business analytics. We contribute to the field in two pivotal ways. 1. We meticulously conceptualize and address the hitherto underexplored issue of discerning risk patterns on large-scale attributed graphs. 2. We introduce an all-encompassing analytical framework that not only incorporates the cutting-edge EGIN algorithm but also integrates a scalable distributed computation system, thereby enhancing computational efficacy. To our knowledge, this is the first proposition of an approximation algorithm based on graph neural networks for risk pattern mining on large transaction-attributed graphs."
https://arxiv.org/html/2411.06773v1,"Model Partition and
Resource Allocation for Split Learning in Vehicular Edge Networks","The integration of autonomous driving technologies with vehicular networks presents significant challenges in privacy preservation, communication efficiency, and resource allocation. This paper proposes a novel U-shaped split federated learning (U-SFL) framework to address these challenges on the way of realizing in vehicular edge networks. U-SFL is able to enhance privacy protection by keeping both raw data and labels on the vehicular user (VU) side while enabling parallel processing across multiple vehicles. To optimize communication efficiency, we introduce a semantic-aware auto-encoder (SAE) that significantly reduces the dimensionality of transmitted data while preserving essential semantic information. Furthermore, we develop a deep reinforcement learning (DRL) based algorithm to solve the NP-hard problem of dynamic resource allocation and split point selection. Our comprehensive evaluation demonstrates that U-SFL achieves comparable classification performance to traditional split learning (SL) while substantially reducing data transmission volume and communication latency. The proposed DRL-based optimization algorithm shows good convergence in balancing latency, energy consumption, and learning performance.","Autonomous driving technology stands at the forefront of automotive innovation, promising to revolutionize transportation systems and reshape urban mobility [1]. As vehicles evolve towards higher levels of autonomy, the potential for enhanced road safety, improved traffic efficiency, and reduced environmental impact becomes increasingly apparent [2]. Central to the realization of comprehensive autonomous driving is the concept of vehicular networks (VNs), which facilitate crucial vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications [3]. The integration of autonomous driving technologies with vehicular networks, while promising, presents significant challenges that must be addressed to ensure robust and efficient operation. These challenges primarily stem from the unique characteristics of the vehicular networks and the stringent requirements of autonomous systems [4]. Key issues include the need for real-time processing to ensure safe operation, privacy concerns arising from the continuous exchange of sensitive information [5], and communication efficiency challenges due to the large volume of data generated by autonomous vehicles. The rapid advancement of autonomous driving technologies have created an urgent need for sophisticated, distributed learning methods tailored to vehicular environment. This necessity is driven by several key factors: privacy concerns, communication overhead, latency requirements and resource constraints. Traditional centralized machine learning approaches require the aggregation of vast amounts of data from vehicles, including sensitive information such as location history, driving patterns, and potentially personal data captured by in-vehicle sensors. The high mobility and large scale of vehicular networks result in substantial communication overhead when transmitting raw data to central servers [6]. This overhead can lead to increased latency, network congestion, and higher operational costs. While modern vehicles are increasingly equipped with computational resources, they still face limitations in processing power, energy consumption, and storage capacity compared to centralized data centers [7]. Efficient utilization of these limited resources is crucial for implementing sophisticated ML models in vehicular settings. To address the challenges in autonomous driving and vehicular networks, several distributed learning approaches have been proposed. However, these methods have limitations when applied to the unique environment of vehicular networks. Federated learning (FL) has emerged as a promising distributed learning paradigm that allows model training on decentralized data [8]. While FL addresses some privacy concerns, it faces several limitations in vehicular networks. FL requires multiple rounds of model updates, which can be challenging in the high-mobility environment of vehicular networks [9]. FL assumes clients have sufficient computational resources to train local models, which may not always be the case for all vehicles [10]. Traditional split learning (SL) is a distributed learning approach that divides the neural network model between clients and the server, offering a balance between privacy preservation and computational efficiency [16]. While SL provides several advantages, it faces significant limitations in the context of vehicular networks. Traditional SL operates sequentially, which can lead to inefficiencies in multi-client scenarios typical in vehicular networks [11]. Although SL keeps raw data on clients, the transmitted activations may still leak sensitive information [12]. In traditional SL, labels are typically shared with the server, which can lead to privacy leakage through label inference attacks. This is particularly concerning in vehicular networks where labels might correspond to sensitive driving behaviors or locations [13]. The fixed split point in traditional SL may not be optimal for all vehicles due to varying computational capabilities and network conditions. While SL reduces data transmission compared to centralized approaches, it still requires significant communication between clients and the server, which can be challenging in dynamic vehicular environment [14]. These limitations underscore the need for a more adaptive and efficient distributed learning approach tailored to the unique challenges of autonomous driving in vehicular networks. Such an approach should address the privacy concerns, communication efficiency issues, and resource constraints while leveraging the distributed nature of vehicular networks to enhance learning performance. In the context of vehicular networks, the size of intermediate features in deep neural networks (DNNs) used for autonomous driving tasks often exceeds that of the original input data, the problem is further exacerbated in multi-vehicle environments. This challenge necessitates the development of effective feature compression techniques to reduce transmission overhead. In this context, semantic communication emerges as a promising solution. By focusing on transmitting the semantic meaning of data rather than raw bits, semantic communication can significantly reduce the amount of data transmitted while preserving essential information. This approach is particularly relevant in vehicular networks, where the semantic content of data (e.g., road conditions, traffic patterns, or obstacle detection) is often more crucial than the exact pixel values of images or precise numerical readings from sensors. Integrating semantic communication techniques into the SL framework could potentially address both the challenges of feature compression and the preservation of critical information for autonomous driving tasks. Meanwhile, a single edge server (ES) typically serves multiple vehicles, with these vehicles communicating through a shared channel. Even with intermediate features compression, the training between vehicles can still lead to significant additional latency. To address this issue, it becomes crucial to optimize resource allocation, including bandwidth and computational resources. However, this optimization problem is typically NP-hard [15], making it challenging to find optimal solutions in real-time, especially in the dynamic environment of vehicular networks. In this paper, we propose a novel U-shaped split federated learning (U-SFL) framework, specifically tailored for autonomous driving applications in vehicular networks. This framework builds upon the foundations of traditional SL while introducing key innovations to address its limitations. By integrating advanced techniques such as semantic-aware auto-encoder (SAE) and deep reinforcement learning (DRL) for resource allocation, U-SFL aims to provide a comprehensive solution that balances privacy preservation, communication efficiency, and learning performance in the challenging context of vehicular networks and autonomous driving. Our main contributions are as follows: • We propose a novel U-SFL framework that enhances privacy protection and enables parallel processing while maintaining comparable classification performance to traditional SL. The U-shaped architecture allows for efficient distribution of the learning process across vehicles and ES while keeping both raw data and labels on the vehicle side, significantly improving privacy compared to traditional SL approaches. • We introduce a SAE component into the U-SFL framework to improve communication efficiency. By leveraging the principles of semantic communication, the SAE reduces the dimensionality of transmitted data while preserving important semantic information, thereby minimizing communication overhead in bandwidth-constrained vehicular networks. This integration of semantic communication principles with SL represents a significant advancement in addressing the unique challenges of vehicular edge computing. • We develop a sophisticated DRL-based algorithm to solve the NP-hard problem of dynamic resource allocation and split point selection in the context of vehicular networks. Our approach addresses the complex, high-dimensional state space that includes vehicle locations, network conditions, and computational loads, while managing a hybrid action space that combines discrete decisions (split point selection) with continuous actions (resource allocation). This DRL-based solution optimizes the utilization of shared resources across multiple vehicles in a realistic, time-varying vehicular network setting. The rest of this paper is organized as follows: Section II reviews related work. Section III presents the system model, the proposed U-SFL framework and the semantic-aware communication techniques. Section IV details the computation and communication modeling. Section V introduces the DRL-based multi-objective optimization algorithm. Section VI presents and discusses the simulation results. Finally, Section VII concludes the paper and outlines future research directions."
https://arxiv.org/html/2411.06681v1,WDMoE: Wireless Distributed Mixture of Experts for Large Language Models,"Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but the role of wireless networks in supporting LLMs has not been thoroughly explored. In this paper, we propose a wireless distributed Mixture of Experts (WDMoE) architecture to enable collaborative deployment of LLMs across edge servers at the base station (BS) and mobile devices in wireless networks. Specifically, we decompose the MoE layer in LLMs by placing the gating network and the preceding neural network layer at BS, while distributing the expert networks among the devices. This deployment leverages the parallel inference capabilities of expert networks on mobile devices, effectively utilizing the limited computing and caching resources of these devices. Accordingly, we develop a performance metric for WDMoE-based LLMs, which accounts for both model capability and latency. To minimize the latency while maintaining accuracy, we jointly optimize expert selection and bandwidth allocation based on the performance metric. Moreover, we build a hardware testbed using NVIDIA Jetson kits to validate the effectiveness of WDMoE. Both theoretical simulations and practical hardware experiments demonstrate that the proposed method can significantly reduce the latency without compromising LLM performance.","The exciting advancements in large language models (LLMs) have sparked a new wave of AI innovation. LLMs, exemplified by ChatGPT[2], have demonstrated emergent abilities[3], including better generalization, nuanced meaning comprehension, and remarkable reasoning and generation capabilities. These advancements have led to widespread applications across various fields, illuminating the vision of artificial general intelligence (AGI)[4]. In the field of 6G wireless networks, LLMs have been used for wireless network resource allocation[5, 6, 7], and applied in internet of vehicles[8] and immersive communications[9]. The emergent abilities of LLMs stem from extensive computation, a large number of model parameters, and massive training datasets[3, 10, 11]. The vast number of model parameters poses significant challenges for training, inference, and deployment. The training phase of LLMs involves significant costs in time and computational power for most individuals and organizations. Regarding LLMs inference and deployment, they also require fast responses and ample memory. In this paper, we mainly focus on LLMs inference and deployment. Currently, LLMs can be classified into cloud-based LLMs and on-device LLMs based on their deployment characteristics. Cloud servers with numerous graphics processing units (GPUs) and sufficient power supply are responsible for the majority of model inference and deployment. Due to concerns over latency and data privacy, the potential of on-device LLMs is gaining increasing attention[12]. Researchers compress LLMs through pruning[13], quantization[14], and distillation[15] to meet the memory, computation, and energy requirements of mobile devices. Limited by generation speed and model capabilities, even a company as strong as Apple has not been able to deploy a fully satisfactory LLM on mobile devices. On the latest iPhone 16 Pro series, only simple tasks are completed locally by a model with around 3 billion parameters, whereas complex tasks are still handled by cloud-based models like ChatGPT[16, 17]. Although in practical LLMs application transformer’s KV cache[18] can speed up the inference, it will cause considerable memory overhead, which presents an obstacle for on-device LLMs. In light of the rapid advancements of LLMs and the widespread adoption of 5G/6G wireless networks, a natural question arises: can wireless networks support LLMs? If so, how? The answer lies in fully leveraging the multidimensional resources of wireless networks, incorporating computing, communications, and caching (3C) to support LLMs and enhance user experience [19]. As a key technology of 5G, mobile edge computing (MEC) has been thoroughly studied to improve the quality of service for various network applications. Recently, edge-cloud collaborative training and fine-tuning are also researched[20, 21]. However, there is a lack of specialized optimization research tailored to the characteristics of LLMs in the scenario of distributed deployment in wireless networks. To address this issue, this paper aims to bridge the gap by proposing a distributed deployment of LLMs powered by mixture of experts (MoE), modeling and optimizing the latency during the inference phase, which can efficiently utilize the 3C resources at both the MEC server and mobile devices. The core idea of MoE architecture is to sparsify the neural networks (NNs) based on the observation of sparse activation of NNs and allow each expert to specialize in their specific tasks, reducing the floating point operations (FLOPs) to achieve acceleration and increasing model capacity[22]. Transformer with MoE111In this paper, MoE refers to Transformer with MoE unless otherwise specified.[23] replaces the original single dense feedforward neural network (FFN) with a gating network and multiple smaller expert networks, which often have the same structure and different hidden layer dimensions. The expert networks within the same MoE layer operate in parallel and independently, providing high fault tolerance and making it well-suited for wireless distributed deployment. Besides, we find during the inference phase of MoE-based LLMs, decreasing the number of participating experts will not degrade model performance, showcasing the robustness of MoE. Capitalizing on these attributes, we can deploy the gating network at MEC server and each expert network in an MoE layer on diverse mobile devices. Furthermore, due to the diversity of computing capabilities among the mobile devices and communication qualities between the base station (BS) and the mobile devices, how to achieve a good balance between the inference accuracy and latency also requires careful consideration. I-A Related Work Current mainstream LLMs utilize Transformer architecture[24]. LLMs based on the Transformer architecture can be categorized into three types: encoder-decoder, decoder-only, and encoder-only. For encoder-decoder structure, in the vanilla Transformer[24], the multi-head attention mechanism fully replaces traditional recurrent neural networks, making the architecture more suitable for parallel computing. For decoder-only structure, in [25], a decoder-only Transformer is utilized with the optimization objective of standard language modeling. This work achieves state of the art performance, and the model is named GPT. For the encoder-only structure, [26] designs Bidirectional Encoder Representations from Transformers (BERT) and applies the training paradigm of pre-training and fine-tuning to language model. BERT includes a base model with 110 million parameters, similar to GPT and a large model with 340 million parameters. The work [25] and [26] lay the foundation for the model architecture, training paradigm, and development direction of large language models. GPT-2 is a scaled-up version of GPT with 1.5 billion parameters, focusing on zero-shot performance in [27]. The work [10] proposes the scaling law of transformer-based language models and suggests that larger models trained on more data tend to perform better, supporting the later development of large language models. GPT-3 is released in [28] with 175 billion parameters and was the largest language model at that time. It is applied without fine-tuning and achieves state-of-the-art performance on various benchmarks. Following the scaling law, an increasing number of large language models have appeared with more parameters, and the largest open source model is Llama 3.1, with 405 billion parameters, based on a dense Transformer[29]. Large model size makes the training and inference computationally expensive. Various methods of training and inference have been proposed to improve model performance without excessive computational costs[22], among which the MoE architecture has been well-researched. The basic MoE layer consists of a gating network and a number of experts, which are simple feed-forward neural networks[30]. The work [22] introduces a Sparsely-Gated MoE and expands the LSTM-based language model to a configuration with 137 billion parameters. In the Transformer era, [23] replaces all Feed-forward layers in the model with MoE layers and achieves enhanced model capacity while substantially reducing the training time. Mistral AI successively releases two open-source MoE large language models, Mixtral-8x7B and Mixtral-8x22B, both achieving the state-of-the-art performance among open-source large models at the time of their release[31]. The work [32] introduces an adaptive MoE framework, AdaMV-MoE, designed for multi-task vision recognition. Unlike conventional MoE approaches with a fixed number of experts, AdaMV-MoE dynamically adjusts the number of active experts per task based on training dynamics, eliminating the need for manual tuning of model capacity. Based on the idea that harder tasks need more experts, the work [33] develops a method for dynamically adjusting the number and selection of experts to reduce computational costs for simple tasks while enhancing model performance for complex ones. Advance in MoE models have unlocked the potential of model capability and computational efficiency. On-device LLMs have been extensively researched as a promising solution for personal assistants. By running LLMs directly on devices such as smartphones or edge servers, they offer enhanced privacy, lower latency, and reduced dependence on cloud servers. The work [34] devises a KV cache compression and swapping method with accuracy tolerance awareness, significantly reducing the context switching latency and extending the capacity of active contexts. In [35], a highly efficient computation and memory framework named Edge-LLM is proposed, reducing computation and memory overhead through layer-wise unified compression and adaptive backpropagation depth reduction. MobileLLM is a model family with sub-billion parameters that surpasses previous sub-billion models on mainstream benchmarks by adopting deep and thin network architectures, embedding sharing, and a grouped-query attention mechanism[36]. With privacy considerations, [37] employs derivative-free optimization to update the local LLM’s parameters during on-device fine-tuning phase under the resource-constrained circumstances. (a) (b) Figure 1: (a) MoE-based LLMs architecture[23]; (b) The proposed WDMoE-based LLMs system model. I-B Contributions Motivated by the above, we explore a wireless distributed deployment architecture for LLMs with MoE, define and model key performance metrics of LLMs within the framework, and optimize resource allocation and expert selection to improve the LLMs service capabilities within the wireless network. The main contributions of this paper are summarized as follows: • We propose a novel wireless distributed MoE architecture, WDMoE, for LLMs. The fundamental principle of the WDMoE architecture is to leverage the independence and parallelism of expert networks by deploying them on multiple mobile devices, thereby utilizing the resource advantages of wireless networks to facilitate distributed deployment of large models. This architecture places the computationally intensive and memory-demanding multi-head attention mechanism on the MEC servers at BS. Through collaborative token processing between BS and mobile devices, the wireless network can support a large number of LLMs service requests, addressing the challenges of limited memory on a single device and data privacy concerns in cloud-based LLMs deployment. • We establish latency-aware performance metrics of the WDMoE-based LLMs. By analyzing the attention mechanism, we find that, during the wireless distributed deployment of LLMs with MoE, the latency for expert networks on different mobile devices to process tokens and return them to the BS varies. This discrepancy can cause earlier-arriving tokens to wait at the attention module. We model this latency, referred to as attention waiting latency, and design a bilevel optimization problem where the upper-level objective is to minimize this latency while ensuring model capabilities at the lower level, thereby reducing latency without compromising model performance. In addition, we propose an innovative metric, the weight latency ratio (WLR), to comprehensively consider the output weights of the MoE gating network and the attention waiting latency of each device. • We develop an expert selection policy to improve the performance. Based on the defined WLR, the expert selection policy can process tokens by considering the processing time and weight of each mobile device for allocated tokens. It dynamically adjusts the number of experts per token, thereby reducing network transmission and computational load. • We build a hardware testbed using NVIDIA Jetson kits to validate the effectiveness of the proposed WDMoE. We use three NVIDIA Jetson kits and one personal computer (PC) with an NVIDIA RTX 4070 Ti, serving as four mobile devices, each running one expert for each layer, and communicating with the server via WiFi. Both simulation results and hardware experiment results show that the proposed WDMoE architecture and expert selection policy effectively reduce the latency experienced by users. For example, compared to vanilla expert selection, the proposed WDMoE reduces latency by 45.75% on average on the PIQA dataset without model capability deterioration. The rest of this paper is organized as follows. The WDMoE-based LLM architecture is introduced in Section II. The system model and problem formulation are presented in Section III. The WDMoE expert selection policy and bandwidth allocation algorithm are introduced in Sections IV. Section V shows extensive simulation results. Finally, the hardware testbed experiments are presented in Section VI, and conclusions are drawn in Section VII."
https://arxiv.org/html/2411.06618v1,Using Diffusion Models as Generative Replay in Continual Federated Learning – What will Happen?,"Federated learning (FL) has become a cornerstone in decentralized learning, where, in many scenarios, the incoming data distribution will change dynamically over time, introducing continuous learning (CL) problems. This continual federated learning (CFL) task presents unique challenges, particularly regarding catastrophic forgetting and non-IID input data. Existing solutions include using a replay buffer to store historical data or leveraging generative adversarial networks. Nevertheless, motivated by recent advancements in the diffusion model for generative tasks, this paper introduces DCFL, a novel framework tailored to address the challenges of CFL in dynamic distributed learning environments. Our approach harnesses the power of the conditional diffusion model to generate synthetic historical data at each local device during communication, effectively mitigating latent shifts in dynamic data distribution inputs. We provide the convergence bound for the proposed CFL framework and demonstrate its promising performance across multiple datasets, showcasing its effectiveness in tackling the complexities of CFL tasks.","Federated learning (FL) has emerged as a prevalent decentralized learning paradigm, allowing training of a global model through interactions with distributed clients while maintaining the privacy of their local data [1, 2]. Most FL frameworks operate under the assumption that the client datasets at each client remain static throughout extensive learning cycles and iterations. However, this assumption does not align with the dynamic nature of real-world scenarios [3, 4]. The global model trained on such fixed datasets often fails to adapt effectively to the constantly evolving real world [5]. Furthermore, in real-world scenarios, clients often encounter new environments, objectives, and tasks – an aspect of adaptability that conventional FL frameworks have not yet fully addressed. Continual learning (CL) methods were proposed to handle the phenomenon of catastrophic forgetting, where historical data may become inaccessible due to privacy regulations or storage constraints [6]. These methods were proposed that focus on developing systems capable of continuously learning from new tasks without erasing previously acquired knowledge. Classical CL scenarios can be broadly categorized into three types, ranging from task incremental learning (TIL) and domain incremental learning (DIL) to class incremental learning (CIL) [7]. However, these CL scenarios may face broader and more diverse challenges in the context of FL, as it is essential to consider cross-client scenarios and the non-IID data distribution among clients. We aim to address the challenges of continual federated learning (CFL) tasks in practical settings. Specifically, in CFL, learning is decentralized across multiple heterogeneous devices and is coordinated by a central server, where devices encounter new data and tasks over time. This poses challenges in handling both catastrophic forgetting issues induced by timely-shifted data distribution and non-IID problems in FL [8]. Recent approaches have been proposed to mitigate these challenges, such as leveraging replay memory to store historical data experienced by the model in the past [9] and utilizing generative adversarial networks (GANs) to generate historical data on each device to help remember past experiences [10]. Some methods utilize the global model on the server to train a generative model through knowledge distillation, but this approach leads to low-quality synthetic data and introduces additional noise [11, 12]. While storing real historical data [13] is useful for memory replay, it may not be feasible in cases where the data is available only for limited-time usage. Alternatively, FOT [14] performs global subspace extraction to identify features of previous tasks, aiming to prevent forgetting. However, FOT incurs higher communication costs between clients and servers due to the transfer of subspace information and orthogonal projections. GANs, as traditional generative models, on the other hand, involve learning two models (generator and discriminator) to reach a stable equilibrium, which can be difficult to train and sometimes susceptible to mode collapse [15] problems. Therefore, the powerful data generation capability demonstrated by the diffusion model [16] in various domains [17, 18, 19] makes it a strong candidate to replay data within the CFL context. In this paper, we propose a novel framework DCFL that integrates CFL with conditional diffusion. At each local device, the embedded diffusion model serves to alleviate the impact of catastrophic forgetting by generating synthetic historical data. Since the diffusion model is not shared with anyone, our framework adheres to general privacy restrictions. Subsequently, the target models (i.e., models performing FL) are aggregated on the global server to obtain a generalized global model. We also provide a convergence analysis of DCFL by separately examining the convergence of the FL backbone, the data distribution shift, and the data generation convergence with the diffusion model. By combining these results, we demonstrate that the overall convergence of the system ultimately hinges on the performance of the introduced diffusion model, of which the bounded characteristic contributes to the system’s convergence. DCFL has been tested on three CFL scenarios and four mainstream benchmark datasets, where DCFL significantly outperformed classical FL, classical CL, traditional generative model, and state-of-the-art (SOTA) baselines. The main contributions of this paper are provided as follows: • We introduce a novel CFL framework, termed DCFL, which eliminates the need for replay memory, enabling model learning for both local clients and the global server with dynamic data inputs. DCFL leverages the modern diffusion model to generate synthetic historical data based on previously observed data distributions (Section 3.1). • We provide the convergence analysis for our DCFL framework. Our convergence result captures the bound of the FL model, the bound affected by the diffusion model, and the effect of data distribution shift between time steps (Section 3.2). • We conduct extensive experiments using MNIST, FashionMNIST, CIFAR-10, and PACS datasets under three practical CFL environments. The results demonstrate that our DCFL framework improves upon the best baseline by 32.61\% in the Class Incremental IID scenario, 15.16\% in the Class Incremental Non-IID scenario, and 7.45\% in the Domain Incremental scenario (Section 4). To the best of our knowledge, our DCFL is the first work that successfully integrates diffusion models into continual federated learning, addressing its unique challenges with theoretical analysis."
https://arxiv.org/html/2411.06581v1,Federated LLMs Fine-tuned with Adaptive Importance-Aware LoRA,"Federated fine-tuning of pre-trained Large Language Models (LLMs) enables task-specific adaptation across diverse datasets while preserving data privacy. However, the large model size and heterogeneity in client resources pose significant computational and communication challenges. To address these issues, in this paper, we propose a novel Heterogeneous Adaptive Federated Low-Rank Adaptation (LoRA) fine-tuned LLM framework (HAFL). To accommodate client resource heterogeneity, we first introduce an importance-based parameter truncation scheme, which allows clients to have different LoRA ranks, and smoothed sensitivity scores are used as importance indicators. Despite its flexibility, the truncation process may cause performance degradation. To tackle this problem, we develop an importance-based parameter freezing scheme. In this approach, both the cloud server and clients maintain the same LoRA rank, while clients selectively update only the most important decomposed LoRA rank-1 matrices, keeping the rest frozen. To mitigate the information dilution caused by the zero-padding aggregation method, we propose an adaptive aggregation approach that operates at the decomposed rank-1 matrix level. Experiments on the 20 News Group classification task show that our method converges quickly with low communication size, and avoids performance degradation when distributing models to clients compared to truncation-based heterogeneous LoRA rank scheme. Additionally, our adaptive aggregation method achieves faster convergence compared to the zero-padding approach.","Large Language Models (LLMs) have exhibited exceptional performance in understanding and generating natural language across a wide range of tasks, including applications in chatbots and search engines[1]. To achieve optimal performance on specific tasks, these pre-trained models often require further fine-tuning. However, conventional fine-tuning methods are typically centralized, involving the collection of raw data on a single client, which raises significant privacy concerns. To address this issue, Federated Learning (FL) [2] has emerged as a promising solution by enabling collaborative model training across decentralized LLM agents, where only model weights are shared instead of raw data, thereby preserving data privacy. Integrating FL with LLMs allows for leveraging diverse data sources, resulting in more robust and generalized models. As FL increasingly relies on wireless networks to support collaboration across clients, communication has emerged as a critical bottleneck in fine-tuning LLMs. Unlike centralized settings, FL requires frequent transmission of model updates over limited-bandwidth connections, posing significant challenges for models with billions of parameters, such as LLaMA3-8B[3]. Even in half-precision, transmitting these parameters demands approximately 16 GB per update, resulting in delays that significantly impact training efficiency. Moreover, the large parameter size imposes high demands on the client’s computational capabilities. Parameter-Efficient Fine-Tuning (PEFT) has been proposed to alleviate the aforementioned challenges[4, 5]. One of the widely used PEFT methods is Low-Rank Adaptation (LoRA), which freezes the original pre-trained parameters of LLM and trains a smaller number of additional parameters instead[6]. This approach significantly reduces computational and storage requirements while maintaining model performance comparable to full fine-tuning. Instruction tuning, a technique to improve LLMs’ ability to follow diverse natural language instructions across various tasks, can benefit from such parameter-efficient methods. Authors in [7] proposed an approach that integrates FL with LoRA to enhance the instruction tuning of LLMs. This method enables clients to only train and optimize the LoRA layers using locally diverse instruction data, reducing privacy exposure risks by sharing only the LoRA weights, which significantly lowers the demands on communication and client computational capabilities. However, the study assumed a uniform LoRA rank across all clients and does not consider the potential impacts of client resource heterogeneity. To accommodate heterogeneous computational capacities, the authors in [8] proposed HETLoRA that assigned different LoRA ranks to different clients. Additionally, this work introduced a zero-padding aggregation method, and a global model truncation distribution method to address the challenges posed by distinct LoRA ranks. However, the zero-padding aggregation method can dilute the information learned by high-rank clients, and the truncation approach may lead to performance loss in the models distributed to clients. To address the challenges posed by different LoRA ranks, the authors in [9] synthesized a full-size LoRA through the direct multiplication of the A and B matrices, ensuring consistent dimensions of the full-size LoRA across clients. This full-size LoRA is then aggregated to form an updated global model, which is then decomposed using Singular Value Decomposition (SVD). The SVD components are further processed using a low-rank approximation to fit different client ranks before being distributed to the clients. However, the reconstruction method in [9] may lose information on the cross-relation across clients during aggregation, and the low-rank approximation distribution method still inevitably leads to performance loss. In this paper, to address the limitations of previous work [7, 8, 9], we propose a novel Heterogeneous Adaptive Federated LoRA fine-tuned LLMs framework (HAFL) to handle the challenges posed by heterogeneous client resources. This framework ensures that each client’s LoRA rank is the same as the rank of the global LoRA at the cloud server for model compatibility. Each client selectively fine-tunes a portion of the LoRA layers based on its computational capabilities, keeping the remaining layers frozen rather than being truncated. Compared to HETLoRA using different ranks for different clients[8], our approach does not result in performance loss due to the truncation when adapting the highest-rank global LoRA to clients. Additionally, the freezing operation ensures that our scheme does not increase the consumption of training resources such as computational power and memory usage. Our contributions can be summarized as follows: • We propose a federated LoRA fine-tuned LLM framework. To accommodate client resource heterogeneity, we first introduce an importance-based parameter truncation scheme, which allows clients to have different ranks to fine-tune the more critical LoRA layers. To avoid the performance degradation resulting from the truncation process, we further develop an importance-based parameter freezing scheme. In this approach, both the cloud server and clients maintain the same high LoRA rank, while clients selectively update only the most important LoRA rank-1 matrices while keeping others frozen. • We propose an adaptive global model aggregation method for the cloud server. Rather than aggregating the entire LoRA matrices, we perform aggregation at the decomposed rank-1 matrix level. For each rank-1 matrix, only the clients that update and upload it will participate in the aggregation process. Unlike the zero-padding method, which allows clients with non-updated rank-1 matrices to join the aggregation process using zero values, our method effectively prevents information dilution. • We conduct experiments on the text classification task. The results show that our proposed HAFL method converges quickly with low communication overhead. Moreover, our importance-based partial freezing scheme effectively maintains client model accuracy during global model parameter distribution, outperforming the truncation approach. Additionally, our proposed adaptive aggregation method achieves faster convergence compared to the zero-padding approach in [8]. The rest of the paper is organized as follows: Section II introduces the system model; Section III presents our novel importance-based parameters truncation and freezing schemes and our adaptive global aggregation; Section IV presents the numerical results; and finally, Section V concludes the paper. Figure 1: HAFL Framework"
https://arxiv.org/html/2411.06225v1,RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks,"Parallel-in-time (PinT) techniques have been proposed to solve systems of time-dependent differential equations by parallelizing the temporal domain. Among them, Parareal computes the solution sequentially using an inaccurate (fast) solver, and then “corrects” it using an accurate (slow) integrator that runs in parallel across temporal subintervals. This work introduces RandNet-Parareal, a novel method to learn the discrepancy between the coarse and fine solutions using random neural networks (RandNets). RandNet-Parareal achieves speed gains up to x125 and x22 compared to the fine solver run serially and Parareal, respectively. Beyond theoretical guarantees of RandNets as universal approximators, these models are quick to train, allowing the PinT solution of partial differential equations on a spatial mesh of up to 10^{5} points with minimal overhead, dramatically increasing the scalability of existing PinT approaches. RandNet-Parareal’s numerical performance is illustrated on systems of real-world significance, such as the viscous Burgers’ equation, the Diffusion-Reaction equation, the two- and three-dimensional Brusselator, and the shallow water equation.","Parallel-in-time (PinT) methods have been used to overcome the saturation of well-established spatial parallelism approaches for solving (prohibitively expensive) initial value problems (IVPs) for ordinary and partial differential equations (ODEs and PDEs), described by systems of d\in\mathbb{N} ODEs (and similarly for PDEs) \frac{d\boldsymbol{u}}{dt}=h(\boldsymbol{u}(t),t)\enspace\text{ on }t\in\left[% t_{0},t_{N}\right],\enspace\text{with }\boldsymbol{u}\left(t_{0}\right)=% \boldsymbol{u}^{0},\enspace N\in\mathbb{N}, (1) where h:\mathbb{R}^{d}\times\left[t_{0},t_{N}\right]\rightarrow\mathbb{R}^{d} is a smooth multivariate function, \boldsymbol{u}:\left[t_{0},t_{N}\right]\rightarrow\mathbb{R}^{d} is the time dependent column vector solution, and \boldsymbol{u}^{0}\in\mathbb{R}^{d} is the initial value at t_{0}. PinT schemes are particularly important when the sequential application of an accurate numerical integrator \mathscr{F} over \left[t_{0},t_{N}\right] is infeasible in a reasonable wallclock time. There are three general approaches for PinT computation: parallel across-the-problem, parallel-across-the-step, and parallel-across-the-method. In [17, 55], another classification is provided: multiple shooting, methods based on waveform relaxation and domain decomposition, multigrid approaches, and direct time-parallel methods. Parallel-across-the-step methods, in which solutions at multiple time-grid points are computed simultaneously, include Parareal (approximation of the derivative in the shooting method) [45], Parallel Full Approximation Scheme in Space and Time (PFASST) (multigrid method) [13, 50], and Multigrid Reduction in Time (MGRIT) [14, 16] methods (see [19] for details). Among them, Parareal [45] has garnered popularity, with extensive theoretical analyses, improved versions, and empirical applications [17, 55]. This is due to its non-intrusive nature which allows seamless integration with arbitrary temporal and spatial discretizations, and to its successful performance across diverse fields, such as plasma physics [64, 66, 67], finance [4, 56], and weather modeling [59, 60]. Limited theoretical results are available for MGRIT and PFASST, with a few extensions and empirical applications. Interestingly, combined analyses have shown equivalences between Parareal and MGRIT, and connections between MGRIT and PFASST. In Parareal, a coarse and fast solver \mathscr{G} is run sequentially to obtain a first approximation of the solution, which is then corrected by running a fine (accurate) but slow integrator \mathscr{F} in parallel across N temporal subintervals. This procedure is then iterated until a convergence criterion is met after k\leq N iterations, leading to a speed-up compared to running \mathscr{F} sequentially over the entire time interval. A recent advancement, GParareal [57], improves Parareal convergence rates (measured as k/N) by learning the discrepancy \mathscr{F}-\mathscr{G} using Gaussian Processes (GPs). This method outperforms Parareal for low-dimensional ODEs and a moderate number of computer cores N. However, the cubic cost (in the number of data points, roughly kN at iteration k) of inverting the GP covariance matrix hinders its broader application. Subsequent research introduced nearest neighbors (nns) GParareal (nnGParareal) [21], enhancing GParareal’s scalability properties in both N and d through data reduction. Significant computational gains were achieved by training the GP on a small subset of nns, resulting in an algorithm loglinear in the sample size. This allowed scaling its effectiveness up to systems with a few thousand ODEs, beyond which it loses its potential. Indeed, being based on the original GP framework, it uses a costly hyperparameter optimization procedure that requires fitting one GP per ODE dimension. This study introduces RandNet-Parareal, a new approach using random neural networks (RandNets) to learn the discrepancy \mathscr{F}-\mathscr{G}. RandNets are a family of single-hidden-layer feed-forward neural networks (NNs), where hidden layer weights are randomly sampled and fixed, and only the output (or readout) layer is subject to training. Compared to standard artificial NNs, RandNets are hence much simpler to train: the input data are fed through the network, the predictions observed, and the weights of the linear output (or readout) layer are obtained as minimizers of a penalized squared loss between the NN outputs and the training targets. Since this optimization problem admits a closed-form solution, no backpropagation is required, and the issues of vanishing and exploding gradients persisting for standard fully trainable NNs are therefore avoided. The literature on the topic is rich and somewhat fragmented, and different names are used for essentially the same model. RandNets are related to Random Feature Networks [6, 49, 62, 63, 65] and Reservoir Computing [24, 26, 25, 27, 28], Random Fourier Features (RFFs) and kernel methods [41, 61, 70, 74]. Some authors use the name Extreme Learning Machines (ELMs) [34, 35, 36, 37, 44] to refer to RandNets, while others use the term randomized or random NNs [5, 32, 39, 46, 78, 82] for the same paradigm. RandNets show excellent empirical performance, and have been used in the context of mathematical finance [22, 33, 38], mathematical physics [52], electronic circuits [69], photonic [47] and quantum systems [23, 48], random deep splitting schemes [53], scientific computing [10, 11, 79, 81], and have shown excellent empirical performance in numerous further applications. Moreover, recent work [22, 25] proves that RandNets are universal approximators within spaces of sufficiently regular functions, and provides explicit approximation error bounds, with these results generalized to a large class of Bochner spaces in [52]. These contributions show that RandNets are a reliable machine learning paradigm with provable theoretical guarantees. In this paper, we show that endowing Parareal with RandNets-based learning of \mathscr{F}-\mathscr{G}, the new proposed RandNet-Parareal algorithm, leads to significantly improved scalability, convergence speed, and parallel performance with respect to nnGParareal, GParareal, and Parareal. This allows us to solve PDE systems on a fine mesh of up to 10^{5} discretization points with negligible overhead, outperforming nnGParareal by two orders of magnitude and reducing its model cost by several orders. Here, we compare the performance of Parareal, nnGParareal, and RandNet-Parareal on five increasingly complex systems, some of which are drawn from an extensive benchmark study of time-dependent PDEs [75]. These include the one-dimensional viscous Burgers’ equation, the two-dimensional Diffusion-Reaction equation, a challenging benchmark used to model biological pattern formation [76], the two- and three-dimensional Brusselator, known for its complex behavior, including oscillations, spatial patterns, and chaos, and the shallow water equations (SWEs). Derived from the compressible Navier-Stokes equations, the SWEs are a system of hyperbolic PDEs exhibiting several types of real-world significance behaviors known to challenge numerical integrators, such as sharp shock formation dynamics, sensitive dependence on initial conditions, diverse boundary conditions, and spatial heterogeneity. Example applications include of tsunamis or flooding simulations. We intentionally chose two hyperbolic equations (Burgers’ and SWE) to challenge RandNet-Parareal on systems for which Parareal is known to struggle, with slow or non-convergent behavior [2, 3, 9, 18, 72]. Previous works have developed ad-hoc coarse solvers to address Parareal’s slow convergence for Burgers’ [7, 40, 68, 71], and for SWE [1, 31, 54, 73]. Here, we adopt a different strategy: by leveraging the generalization capabilities of RandNets within the Parareal algorithm, we enhance the performance of standard, off-the-shelf integration methods such as Runge-Kutta, obtaining speed gains up to x125 and x22 compared to the accurate integrator \mathscr{F} and Parareal, respectively. All experiments have been executed on Dell PowerEdge C6420 compute nodes each with 2 x Intel Xeon Platinum 826 (Cascade Lake) 2.9 GHz 24-core processors, 48 cores and 192 GB DDR4-2933 RAM per node. To illustrate our proposed algorithm and facilitate code adoption, we provide a step-by-step Jupyter notebook outlining RandNet-Parareal. Moreover, all simulation outcomes, including tables and figures, are fully reproducible and accompanied by the necessary Python code at https://github.com/Parallel-in-Time-Differential-Equations/RandNet-Parareal. It is well acknowledged that comparing PinT methods based on different working principles is extremely hard, with [55] representing a recent survey article with some comparisons. Quoting [55],“caution should be taken when directly comparing speedup numbers across methods and implementations. In particular, some of the speedup and efficiency numbers are only theoretical in nature, and many of the parallel time methods do not address the storage or communication overhead of the parallel time integrator”. [19] is one of very few recent attempts to systematically compare different PinT classes. However, it is limited exclusively to the Dahlquist problem. Thus, it has become conventional to compare new techniques to the existing state-of-the-art methods within the same group of solvers. This is why, in this work, we compare RandNet-Parareal with the original Parareal and its recently improved versions, GParareal [57], and nnGParareal [21]. The rest of the paper is organized as follows. In Section 2, we describe the Parareal algorithm. Section 3 briefly explains GParareal and nnGParareal, focusing on the latter. RandNet-Parareal is introduced in Section 4, while Sections 5 and 6 present our numerical results, and a final discussion. A computational complexity analysis of RandNet-Parareal, a robustness evaluation of the proposed algorithm, complementary simulation studies, and other additional results are available in the Supplementary Material. Notation. We denote by \boldsymbol{v}\in\mathbb{R}^{n} a column vector with entries {v}_{i}, i\in\{1,\ldots,n\}, and by \|\boldsymbol{v}\| and \|\boldsymbol{v}\|_{\infty} its Euclidean and infinity norms, respectively. We use A\in\mathbb{R}^{n\times m} to denote a real-valued n\times m matrix, n,m\in\mathbb{N}, with elements A_{ij}, jth column A_{(\cdot,j)}, j\in\{1,\dots m\}, and ith row A_{(i,\cdot)}, i\in\{1,\dots,n\}. We write A^{\top}, A^{\dagger}, and \|A\|_{\rm F} for the A matrix transpose, Moore-Penrose pseudoinverse, and Frobenius norm, respectively. \mathbb{I}_{n} denotes the identity matrix of dimension n."
https://arxiv.org/html/2411.06137v1,A Sharded Blockchain-Based Secure Federated Learning Framework for LEO Satellite Networks,"Low Earth Orbit (LEO) satellite networks are increasingly essential for space-based artificial intelligence (AI) applications. However, as commercial use expands, LEO satellite networks face heightened cyberattack risks, especially through satellite-to-satellite communication links, which are more vulnerable than ground-based connections. As the number of operational satellites continues to grow, addressing these security challenges becomes increasingly critical. Traditional approaches, which focus on sending models to ground stations for validation, often overlook the limited communication windows available to LEO satellites, leaving critical security risks unaddressed. To tackle these challenges, we propose a sharded blockchain-based federated learning framework for LEO networks, called SBFL-LEO. This framework improves the reliability of inter-satellite communications using blockchain technology and assigns specific roles to each satellite. Miner satellites leverage cosine similarity (CS) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to identify malicious models and monitor each other to detect inaccurate aggregated models. Security analysis and experimental results demonstrate that our approach outperforms baseline methods in both model accuracy and energy efficiency, significantly enhancing system robustness against attacks.","The advancement of satellite technology has driven the development of large Low Earth Orbit (LEO) satellite networks, with hundreds to thousands of satellites being launched. This trend has accelerated the commercialization of satellite-based Internet of Things (IoT) services and led to continuous upgrades in satellite technology [1]. As a result, modern satellites are now equipped with advanced cameras, processors, and antennas, enabling them to collect and process vast amounts of Earth imagery and sensor data through artificial intelligence (AI)-based solutions [2]. The traditional approach in LEO satellite networks relies on transmitting data to a central server. However, as data volumes grow, the centralized model training approach is becoming impractical due to high bandwidth costs, transmission delays, and the heightened vulnerability of satellite links compared to ground links [3]. Implementing blockchain [4] and federated learning (FL) [5] offers an effective solution to this problem. In FL, each satellite aggregates locally calculated parameters and transmits model updates instead of raw data to jointly train a global model. Blockchain, as a decentralized, immutable, and traceable technology, eliminates the necessity of a central server in FL. Through its decentralized ledger, blockchain enables FL to transparently track updates and client operations across the entire network. For large-scale satellite networks, blockchain sharding technology is applied to enhance entire system performance [6, 7]. Applying FL to LEO satellite networks still faces several challenges. One challenge is that FL assumes all distributed nodes are trustworthy, which is difficult to guarantee [8]. Another challenge arises from the short, intermittent communication windows between satellites and ground data centers, making data transmission to the ground both time-consuming and often unnecessary[9]. Several studies have been conducted to address the aforementioned challenges. For example, Wang et al. proposed a method utilizing cosine similarity (CS) to filter malicious models by measuring the differences in model features[10]. However, as the FL model converges, the CS between the local model in the current round and the global model from the previous round increases significantly. This makes it essential to establish a CS threshold for accurate model classification. Chen et al. divided the model into two categories by extracting model features and selected the model with the highest accuracy as the global model[8]. This method can mistakenly classify some benign models as malicious in the absence of attacks, causing a reduction in accuracy. Zhu et al. transmitted the models learned from satellites to the ground for model verification and aggregation [11]. Although their methods effectively resist poisoning attacks, they did not consider the issue of short communication windows between satellites and the ground. With the increasing number of satellites, there is an urgent need for efficient and decentralized solutions that can operate directly within satellite networks [12]. To address the aforementioned issues, we propose SBFL-LEO, a fully decentralized FL framework that integrates blockchain technology with sharding, enabling secure training on satellite networks. SBFL-LEO integrates CS and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to mitigate poisoning attacks. Specifically, model features are extracted by calculating the CS between each local model and the previous global model. DBSCAN then groups the models based on their CS values, automatically determining the number of clusters based on density thresholds and a minimum data point requirement. By enforcing a minimum cluster size, each cluster is limited to a maximum of two groups, enhancing resilience against poisoning attacks. The aggregated model with the highest accuracy is selected as the cluster model, effectively resisting poisoning attacks. In addition, satellites are categorized into three distinct roles, allowing the model trained by a learning satellite to be validated by a miner satellite rather than being transmitted to the ground. The main contributions of this paper are summarized as follows: • We propose SBFL-LEO, a blockchain-based federated learning framework to address the challenges of limited communication windows between satellites and ground stations while ensuring secure satellite communications. By assigning specific roles to satellites, SBFL-LEO avoids the need to transmit models to the ground for verification. • Within this framework, we introduce CS to extract model characteristics rather than explicitly rejecting models. DBSCAN is then applied to dynamically cluster models, preserving all benign models unaffected by compromise in the current round and reducing the accuracy loss caused by model rejection. • We evaluate SBFL-LEO using a real dataset and perform a thorough security analysis. Experimental results demonstrate that SBFL-LEO outperforms baseline methods in both learning accuracy and energy efficiency."
https://arxiv.org/html/2411.06135v1,Online Parallel Multi-Task Relationship Learning via Alternating Direction Method of Multipliers,"Online multi-task learning (OMTL) enhances streaming data processing by leveraging the inherent relations among multiple tasks. It can be described as an optimization problem in which a single loss function is defined for multiple tasks. Existing gradient-descent-based methods for this problem might suffer from gradient vanishing and poor conditioning issues. Furthermore, the centralized setting hinders their application to online parallel optimization, which is vital to big data analytics. Therefore, this study proposes a novel OMTL framework based on the alternating direction multiplier method (ADMM), a recent breakthrough in optimization suitable for the distributed computing environment because of its decomposable and easy-to-implement nature. The relations among multiple tasks are modeled dynamically to fit the constant changes in an online scenario. In a classical distributed computing architecture with a central server, the proposed OMTL algorithm with the ADMM optimizer outperforms SGD-based approaches in terms of accuracy and efficiency. Because the central server might become a bottleneck when the data scale grows, we further tailor the algorithm to a decentralized setting, so that each node can work by only exchanging information with local neighbors. Experimental results on a synthetic and several real-world datasets demonstrate the efficiency of our methods.","Online multi-task learning (OMTL) processes related to learning tasks sequentially aim to leverage the correlation among multiple tasks to improve overall performance. In each online round, the learner receives multiple instances per task, predicts their labels, and then updates the model based on the true labels. A principal assumption for OMTL is the existence of potential similarities among multiple tasks—the samples of a single task obey a probability distribution similar to the probability distributions of other tasks. This assumption enables the OMTL to learn several models collaboratively using the shared information among different tasks. Compared with learning each task separately or treating all tasks as a whole, such a collaborative learning approach can enhance the performance of all tasks together. OMTL is a real-time, scalable, and continuously adaptive learning method [1]. It has been applied in sequential decision making fields that require prompt response, such as online personalized recommendations [2], targeted display advertising [3], and sales forecasts for online promotions [4]. During the past decades, several OMTL algorithms have been proposed, most of which are based on online gradient descent (OGD), such as mirror descent, dual averaging, and their proximal versions [5, 6]. In particular, OGD is typically used for solving OMTL problems when it is easy to compute the gradient (or sub-gradient) of the online objective, and there are no constraints on the model. Proximal OGD is usually applied when the regularization term of the model is non-smooth (e.g., L1 norm) [7]. Its proximal objective frequently enjoys a closed-form solution. However, for some regularization terms, such as the graph-guided L1 norm \|\mathbf{F}\mathbf{w}\|_{1} [8], adapting (proximal) OGD methods for distributed online learning settings is non-trivial because sub-gradient methods cannot make \mathbf{F}\mathbf{w} sparse and its proximal objective has no closed-form solution. Furthermore, the scalability of OGD-based multi-task algorithms deteriorates when the gradient’s dimensionality and the number of tasks increases, making them inadequate for large-scale learning problems. Unlike OGD methods, the alternating direction multiplier method (ADMM) [9] is more applicable to general learning tasks because it does not require the objective to be differentiable. Specifically, it decomposes the global problem into smaller, easier-to-solve sub-problems suitable for independent workers. Each worker solves its own sub-problem, which depends only on its own variables. Subsequently, a server optimizes the global problem by aggregating dual variables from all sub-problems. Owing to these advantages, ADMM is more suitable for general distributed tasks and is regarded as a viable alternative to OGD for large-scale learning problems [10, 11]. Since ADMM has shown superior ability at optimizing multi-task in the batch learning setting [12, 13], it is attractive to study it in the online scenario, particularly with a distributed computing architecture, so that the learning efficiency can be considerably enhanced by processing multiple tasks in parallel. Therefore, we propose to perform distributed OMTL using ADMM in this study. The task-similarity assumption is imposed by decomposing the model-to-learn into two parts: several unique patterns per task and a global pattern shared by all tasks. The unique patterns are further used to learn the potential relations among tasks on the fly to meet the constant changes in online learning. We explore the architecture’s two distributed forms, namely, the centralized version with a central server and the relatively decentralized version where all workers involved in solving the optimization problem communicate asynchronously. The goal of this study is to parallel execute online multi-task learning in distributed computing frameworks, where a task covariance matrix of multiple tasks is exploited to mine the potential relationships among them. This is expected to enhance the effectiveness of the proposed method and reduce communication consumption during the optimization process. We conducted numerical experiments on a synthetic and five real-world datasets 111Our code is released: https://github.com/Alberta-Lee/NC-24.git. The experimental results demonstrate the effectiveness of this optimization framework. The rest of this paper is organized as follows: We outline related works in Section 2 before introducing the OMTL problem setting in Section 3. We then deduce the ADMM optimization framework for online parallel multi-classification tasks in Section 4 and analyze its performance experimentally on several datasets in Section 5. Finally, we conclude our study in Section 6."
https://arxiv.org/html/2411.05873v1,Poor Man’s Training on MCUs: A Memory-Efficient Quantized Back-Propagation-Free Approach,"Back propagation (BP) is the default solution for gradient computation in neural network training. However, implementing BP-based training on various edge devices such as FPGA, microcontrollers (MCUs), and analog computing platforms face multiple major challenges, such as the lack of hardware resources, long time-to-market, and dramatic errors in a low-precision setting. This paper presents a simple BP-free training scheme on an MCU, which makes edge training hardware design as easy as inference hardware design. We adopt a quantized zeroth-order method to estimate the gradients of quantized model parameters, which can overcome the error of a straight-through estimator in a low-precision BP scheme. We further employ a few dimension reduction methods (e.g., node perturbation, sparse training) to improve the convergence of zeroth-order training. Experiment results show that our BP-free training achieves comparable performance as BP-based training on adapting a pre-trained image classifier to various corrupted data on resource-constrained edge devices (e.g., an MCU with 1024-KB SRAM for dense full-model training, or an MCU with 256-KB SRAM for sparse training). This method is most suitable for application scenarios where memory cost and time-to-market are the major concerns, but longer latency can be tolerated.","On-device training, that is training deep neural networks (DNN) on edge devices, enables a DNN model pre-trained on cloud to improve itself on newly observed data and adapt to cross-domain or out-of-domain distribution shifts after edge deployment. It also allows the model to adapt to user personalization locally, which protects user privacy over sensitive data (e.g., healthcare and financial data). As physic-informed machine learning has been increasingly used for safety-critical decision-making in autonomous systems, there has been also growing interest in on-device fine-tuning or end-to-end training. In federated learning, a machine learning model also needs to be trained periodically on each local edge node, then updated on a global centralized server. Backward propagation (BP) [1] is used in almost all neural network training frameworks for gradient computation. BP is actually a reverse-mode automatic differentiation (AD) [2, 3] approach implemented based on the information of a computational graph. While a forward-mode AD is suitable for computing the gradient of a single-input multiple-out function, BP is more suitable for a multiple-input (i.e., many network parameters) and single-output (i.e., training loss) function. With sophisticated AD packages, operating systems, and compilers, BP can be called with just one command (e.g., loss.backward() in PyTorch) on a CPU- or GPU-based desktop or cloud computing platform. This has greatly simplified the development and deployment of modern neural network models. However, training a neural network on resource-constrained edge hardware [e.g., a microcontroller unit (MCU), FPGA or photonic platform] is completely different from the training task on a desktop or cloud platform, due to the limited hardware resources and software support. Specifically, implementing a standard BP-based training framework on edge devices are often prevented by three major challenges: • Memory Challenge. Edge devices like MCU have a very limited run-time memory (e.g., STM32F746 with only 256-KB user SRAM, or STM32H7B3 with 1024-KB user SRAM). This budget is often below the memory requirement of storing all network parameters, making full-model BP-based training impossible for most realistic cases. By choosing tailored network models (e.g., MCUNet [4]), using real-quantized graphs and a co-designed lightweight back-end (e.g., the TinyEngine [4, 5]), one may perform edge inference with a low memory cost (e.g., 96 KB for the MCUNet-in1 model [4]). However, the memory cost of a full-model BP-based training (e.g., 7.4 MB for MCUNet-in1) is far beyond the memory capacity. Existing training methods on MCU update only a small subset of model parameters (e.g., only the last layer [6, 7], bias vectors [8] to reduce the memory cost, yet leads to significant (e.g., ¿10%) accuracy drop. Sparse update [4, 9]) could narrow this gap, yet requires computation-intensive searches and compilation-level optimization on cloud. • Precision Challenge. Low-precision quantized computation is often utilized on digital edge hardware (e.g., MCU and FPGA) to reduce latency, memory cost, and energy consumption. However, low-precision operations pose great challenges for BP-based training. BP was originally designed for the gradient computation of smooth functions. Thus, error-prone approximation techniques such as straight-through estimators [10] are required to handle non-differentiable functions in quantized neural network training. The errors introduced by these approximation techniques increase as hardware precision reduces. They also propagate and accumulate through different layers, leading to dramatic accuracy drop, unstable training behaviors, or even divergence [11, 12]. • Time-to-Market Challenge. While BP can be done on CPU or GPU with just one line of code (e.g., loss.backward() in PyTorch), implementing it on edge devices can be very challenging. Due to the lack of automatic differentiation packages [2] and sophisticated operating systems on edge platforms, designers often have to implement the math and hardware of gradient computation manually. On some platforms (e.g., integrated photonics), novel devices must be invented and fabricated to perform BP [13]. This error-prone process needs numerous debugs and design trade-offs. As a result, designing edge training hardware is more time-consuming than designing inference hardware. For instance, our own experience shows that an experienced FPGA designer can design a high-quality inference accelerator within one week, yet it takes over one year to implement an error-free training accelerator on FPGA. This long time to market is often unacceptable in the industry due to the fast evolution of AI models. Paper Contributions. The above challenges motivate us to ask the following question: {mdframed} [userdefinedwidth=5.8inch, align=center] Can we make the edge training hardware design as easy and memory-efficient as inference hardware design? In this paper, we show that the answer is affirmative, with the assumption that memory budget and time to market are given higher priority over runtime latency. Our key idea is to completely bypass the complicated BP implementation by proposing a quantized zeroth-order (ZO) method to train a real-quantized neural network model on MCU. This training method only uses quantized forward evaluations to estimate gradients. As a result, we can use a similar memory cost of inference to achieve full-model training under the tiny memory budget of an MCU. This quantized ZO training framework can be used as a plug-and-play tool added to quantized inference hardware, therefore the design complexity and time to market can be dramatically reduced. Our specific contributions are briefly summarized below: 1. ZO Quantized Training for Edge Devices. We propose a BP-free training framework via quantized zeroth-order optimization to enable full-model and real-quantized training on MCUs under extremely low memory budget (e.g., 256-KB SRAM for sparse training or 1024-KB SRAM for dense training). This framework enjoys low memory cost and easy implementation. Furthermore, it shows better accuracy than quantized BP-based training in low-precision (e.g., INT8) settings since no error-prone straight-through estimator is needed. 2. Convergence Improvement. ZO training suffers from slow convergence rates as the number of training variables increases. Previous assumption of low intrinsic dimensionality [14] or coordinate-wise gradient estimation [15] does not work in on-device training. To improve the training convergence, we propose a learning-rate scaling method to stabilize each training step. We also employ a few dimension-reduction methods to improve the training convergence: (i) a generic layer-wise gradient estimation strategy that combines weight perturbation and node perturbation for ZO gradient estimation, (ii) a sparse training method with task-adaptive block selection to reduce the number of the trainable parameters. 3. MCU Implementation. We implement the proposed BP-free training framework on an MCU (full-model training on an STM32H7B3 with 1024-KB user SRAM, and sparse training on an STM32F746 with 256-KB user SRAM). A quantized inference engine is easily converted to a training engine, with only an additional control unit, a temporary gradient buffer, and a pseudo-random number generator. To our best knowledge, this is the first framework to enable full-model training under such a tiny memory budget (STM32H7B3 with 1024-KB user SRAM). 4. Experimental Validation. We conduct extensive experiments on adapting a pre-trained image classification model to unseen image corruptions and fine-grained vision classification datasets. On adapting image corruptions, our BP-free training outperforms current quantized BP-based training with an average 6.4% test accuracy improvement. Our method can also match the performance of back-propagation training on fine-grained vision classification datasets. Figure 1: (a): Overview of BP-free training framework. A quantized inference engine is easily converted to a training engine by adding control unit and repeatedly calling the inference accelerator. (b): Training memory comparison of different training methods. The numbers are measured with MCUNet-in1 [4], batch size 1, and resolution 128\times 128. Our Key idea is summarized in Fig. 1 (a). As demonstrated in Fig. 1 (b), our method is the only solution to enable full-model training on commodity-level MCU (e.g., STM32H7B3 with 1024-KB user SRAM) without auxiliary memory. This memory cost is the minimum to enable full-model training (478 KB model parameters plus 190 KB peak inference memory), 5.46\times more memory-efficient than the memory cost of quantized BP, and >400\times more memory efficient than BP-based training in PyTorch which includes back-end memory overhead. BP-free sparse training further reduces the memory cost to fit a smaller budget (e.g., 256-KB SRAM)."
