URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10442v1,"Enhancing the Reasoning Ability of Multimodal Large Language Models 
via Mixed Preference Optimization","Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce a preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset; and (2) on the model side, we explore integrating PO with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10\times larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released.","Figure 1: Open-source model performance on MathVista. The X- and Y-axes represent the accuracy evaluated with direct-answer responses and CoT responses, respectively. The bubble size is positively correlated with the number of model parameters. The values in parentheses indicate the performance gap between CoT and direct-answer responses. Notably, most open-source models perform worse when answering with CoT. With the remarkable success of large language models (LLMs) [92, 93, 26, 5, 89, 11, 10, 1] in the field of natural language processing, the training paradigm comprising pre-training and supervised fine-tuning (SFT) have also swept the multimodal field, becoming the primary choice for the research and development of multimodal large language models (MLLMs). Benefiting from the large-scale pre-training corpora [48, 99, 80, 90, 114, 43] and high-quality SFT data [98, 55, 53, 20, 24], a series of open-source MLLMs [98, 20, 52, 46, 105, 44, 6, 96] exhibit strong performance across various domain and tasks, some even achieving results comparable to commercial models such as GPT-4o [70] and Gemini [88, 78]. However, open-source MLLMs still exhibit limited reasoning capabilities. As shown in Figure 1, InternVL2-8B [20] achieves a score of 58.3 on MathVista [61], a benchmark for multimodal reasoning, when using direct answers but drops to 56.8 with Chain-of-Thought (CoT) reasoning, indicating that CoT reasoning actually reduces its performance. This decline is commonly observed across open-source MLLMs [44, 105, 20, 96]. We attribute this phenomenon primarily to a distribution shift introduced by the SFT loss. Specifically, SFT relies on teacher forcing, where the model is trained to predict the next token based on previous ground-truth tokens. However, during inference, models must predict each token based on their own prior outputs, leading to a distribution shift between training and inference. Since the direct-answer approach requires only brief responses, while CoT reasoning involves generating a long rationale, the distribution shift problem becomes more severe during CoT. This results in models performing worse with CoT reasoning compared to direct-answer responses. To address the limitations of CoT reasoning in MLLMs, we draw inspiration from recent NLP approaches [74, 42, 103] that use Preference Optimization (PO) techniques to align model outputs with desired reasoning patterns. Specifically, methods like Direct Preference Optimization (DPO) [76] allow models to learn from preference signals to generate responses that better align with user requirements, offering the foundation for Reinforcement Learning from Human Feedback (RLHF). While RLHF has been explored for MLLMs primarily to reduce hallucinations [85, 106, 18], its application for enhancing multimodal reasoning remains under-explored. Building on these insights, we conduct a systematic study on using PO to strengthen the multimodal reasoning capabilities of MLLMs. Enhancing the multimodal reasoning abilities of MLLMs through PO presents several challenges: (1) Limited multimodal reasoning preference data and high annotation cost. Existing multimodal preference datasets [107, 106, 85, 47, 111] primarily address hallucination issues and focus on natural images and perception data, lacking scientific images and reasoning data. Annotating these types of data requires human annotators to carefully compare the given reasoning processes, making it both time-consuming and costly. (2) Lack of open-source methods for improving multimodal reasoning via PO. Although previous works have explored fine-tuning MLLMs using feedback from various sources, these models typically exhibit performance gains on hallucination benchmarks, with little enhancement in general reasoning abilities. Thus, leveraging PO to improve multimodal reasoning capabilities remains largely under-explored. This work addresses these challenges from both the data and model sides. (1) On the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset. (2) On the model side, we explore various PO methods with MLLMs, introducing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance without the requirement for a reward model. Specifically, we propose a continuation-based pipeline called Dropout Next Token Prediction (DropoutNTP) for samples lacking clear ground truth and a correctness-based pipeline for samples with clear ground truth. In DropoutNTP, the responses generated by InternVL2-8B are considered as positive samples. For a given chosen response, we truncate it by half and then prompt InternVL2-8B to complete the remaining portion of the truncated answer without access to the image input. This generated completion serves as the rejected answer for the paired sample. Experimental results in Section 5.2 demonstrate that this straightforward method achieves comparable performance in reducing hallucinations compared to the divide-and-conquer method proposed in RLAIF-V [107]. In the correctness-based pipeline, multiple solutions to each question are sampled from InternVL2-8B. Solutions matching the ground truth answer are used as chosen responses, while those that do not are used as rejected responses. Additionally, we propose the MPO method. The key insight behind this algorithm is that an effective PO process should enable the model to learn the relative preference between pairs of responses, the absolute quality of individual responses, and the process for generating preferred responses. Compared to previous multimodal PO methods [107, 106, 85, 47, 75, 111], our approach excels in the following aspects: (1) Efficient automated data construction pipeline: Our pipeline enables high-quality preference pair generation at a controlled cost. (2) Effectiveness across diverse domains: Models fine-tuned with our data and approach show superior performance across reasoning, question-answering, and hallucination benchmarks. (3) Improvements over SoTA settings: Our results are based on InternVL2-8B, one of the leading open-source MLLMs, further highlighting the potential of our method. In summary, our main contributions are as follows: (1) We propose an efficient preference data construction pipeline. Based on this pipeline, we create MMPR, a high-quality, large-scale multimodal reasoning preference dataset containing approximately 3 million samples. (2) We introduce MPO, an effective PO algorithm designed to improve the reasoning abilities of MLLMs. The resulting model, InternVL2-8B-MPO, exhibits enhanced multimodal reasoning ability and fewer hallucinations compared to its baseline model (i.e., InternVL2-8B). (3) We conduct extensive experiments to explore practical approaches for improving multimodal reasoning via PO. Results show that PO significantly improves reasoning abilities over SFT. Notably, the proposed InternVL2-8B-MPO achieves an accuracy of 67.0 on MathVista [61], outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10\times larger InternVL2-76B."
https://arxiv.org/html/2411.10436v1,Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization,"Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.","Large Language Models (LLMs) have been verified in various field OpenAI (2024); Dubey et al. (2024); Sun et al. (2024), while they encounter challenges such as hallucination. Multimodal Large Language Models (MLLMs) are also known to hallucinate Bai et al. (2024). Specifically, they often produce unfaithful content that does not align with the visual input, which undermines their reliability and practicality, particularly in critical applications such as autonomous driving Cui et al. (2024) or medical tasks Liu et al. (2023a). Hence, addressing MLLM hallucination (M-hallu) is essential. Recent efforts have aimed at mitigating M-hallu through various approaches, including inference-stage strategies like contrastive decoding Leng et al. (2024), and post-hoc corrections that employ external visual models to refine responses Yin et al. (2023). While these methods are simple and training-free, they do not fundamentally enhance the model’s intrinsic capabilities. Meanwhile, some pioneer preference optimization methods like Direct Preference Optimization (DPO) Rafailov et al. (2024) have been introduced, which encourage the model to learn from the comparisons between positive and negative samples, alleviating hallucinations Zhao et al. (2023); Pi et al. (2025). However, most current methods cannot deliver consistent improvements across all types of MLLM hallucination tasks (e.g., VQA and captioning tasks, as shown in our experiments of Table 1). Additionally, it appears that the model’s improvement on specific tasks is closely related to the format of the training data. For instance, the DPO data of SeVa Zhu et al. (2024) primarily consists of VQA, which explains its strong performance on VQA-related hallucination evaluation. However, its results on captioning tasks are relatively unsatisfactory. Moreover, these methods do not explicitly consider diverse sources of M-hallu. Hence, we argue that if we focus on mitigating multimodal hallucinations, we should be able to address diverse types of hallucination causes and tasks, and design hallucination-targeted preference pairs for DPO accordingly. Our goal is to comprehensively alleviate all multimodal hallucination problems, including both discriminative tasks (e.g., VQA) and generative tasks (e.g., image captioning). Different from the hallucinations in LLMs, M-hallu primarily arises from the following three aspects: (1) Insufficient visual capability: This occurs when the MLLM’s visual encoder lacks the necessary strength, being distracted by relatively unimportant visual information, leading to hallucinations; (2) Incapable long-context generation: We observe that hallucinations become more pronounced as the generated content grows longer, similar to long-range forgetting, which needs to be addressed in practical applications; (3) Multimodal conflicts: Multimodal conflicts frequently arise in real-world scenarios due to the inevitable noises in texts and images. MLLMs are more prone to hallucinations with conflicting information existing between text and image Liu et al. (2024b). To address the aforementioned challenges, we propose Hallucination-targeted Direct Preference Optimization (HDPO) to mitigate M-hallu. Our approach constructs hallucination-targeted preference pairs, specifically designed to address various forms and causes of hallucinations. Specifically, we design three types of DPO data reflecting the corresponding hallucination causes as follows: (1) For insufficient visual capability, during the model’s autoregressive decoding, we preserve only some visual tokens with the lowest attention scores to produce targeted negative responses that reflect incorrect visual information distraction, urging MLLMs to pay attention to more effective visual information. (2) For incapable long context generation, we specifically select positive examples from high-quality long-form captions, while creating negative examples where the latter part of the response deviates from the image content, simulating long-form hallucinations. (3) For multimodal conflicts, we add conflicting information with images into prompts to generate negative examples. We provide positive and negative pairs with questions featuring conflicting prefixes to train the model to correctly respond to the question even containing conflicting information. We conduct extensive experiments to evaluate our approach across various types of M-hallu tasks. The results demonstrate that our HDPO framework achieves the overall best performance in effectively mitigating MLLM hallucinations on various tasks. Our contributions are summarized as follows: • We analyze three key causes behind MLLM hallucinations from visual capability, long-context generation, and multimodal conflicts aspects, offering valuable insights to guide future advancements. • Based on these analyses, we propose a novel HDPO, aiming to jointly address all types of M-hallu tasks. To the best of our knowledge, we are the first to adopt hallucination-targeted DPO from diverse aspects with our novel DPO data construction strategies. • Through extensive experiments on different datasets, HDPO demonstrates consistent improvements in all types of M-hallu tasks."
https://arxiv.org/html/2411.10416v1,Towards Automatic Evaluation of Task-Oriented Dialogue Flows,"Task-oriented dialogue systems rely on predefined conversation schemes (dialogue flows) often represented as directed acyclic graphs. These flows can be manually designed or automatically generated from previously recorded conversations. Due to variations in domain expertise or reliance on different sets of prior conversations, these dialogue flows can manifest in significantly different graph structures. Despite their importance, there is no standard method for evaluating the quality of dialogue flows. We introduce FuDGE (Fuzzy Dialogue-Graph Edit Distance), a novel metric that evaluates dialogue flows by assessing their structural complexity and representational coverage of the conversation data. FuDGE measures how well individual conversations align with a flow and, consequently, how well a set of conversations is represented by the flow overall. Through extensive experiments on manually configured flows and flows generated by automated techniques, we demonstrate the effectiveness of FuDGE and its evaluation framework. By standardizing and optimizing dialogue flows, FuDGE enables conversational designers and automated techniques to achieve higher levels of efficiency and automation.","One of the most promising applications of Conversational AI lies in Customer Service Automation, where task-oriented dialogue systems aim to address customer concerns effectively without human intervention. These systems often rely on dialogue flows—structured conversation schemes—to retrieve appropriate information from knowledge bases or back-end systems. Over the years, frameworks like Dialogflow CX222https://cloud.google.com/dialogflow/cx/docs/basics, Rasa333https://rasa.com, and Amazon Lex444https://aws.amazon.com/lex/ have facilitated the creation of task-oriented dialogue systems by leveraging flows comprising user intents, agent actions, and other metadata. As shown in Figure 1, dialogue flows define possible paths a customer and agent can take during a conversation, enabling structured automation. Figure 1: An illustration of a dialogue flow (left) that is used to configure a task-oriented dialogue agent for a fictitious company named ”Everyclothing.” The chat widget (right) illustrates a customer asking about canceling their order. Dialogue flows are typically handcrafted by domain experts through an iterative process involving historical data analysis. This iterative process depends on the designer’s expertise, the quality of the data, and the time invested, resulting in dialogue flows or directed acyclic graph (DAG) that vary significantly in flow attributes such as the number of user intents, intent sequences, and graph paths. Alternatively dialogue flows can be can be generated using automated flow discovery algorithms, which leverage historical dialogue data. These algorithms rely on hyperparameters that significantly influence the resulting flow structure. However, evaluation of flows with respect to historical dialogues has been a relatively unexplored area of NLP. Previous work has predominantly focused on improving the quality of task-oriented dialog agents through better intent discovery (Zhang et al. 2022; Shi et al. 2018; Perkins and Yang 2019) which is often categorized as a sub-problem in the dialogue flow discovery process. The domain of flow discovery from historical human-human dialogues has had relatively sparse research despite being crucial for building task-oriented dialogue agents. There is a need to automatically evaluate the quality of the dialogue flows with respect to the historical dialogue transcripts used to generate them to deliver a consistent baseline, better versioning and tracking progress of flows, and corresponding automation with time. To that end, we introduce a novel evaluation framework to assess the flow discovery algorithm or to guide the human conversation designer while building the flows. Our evaluation framework enables comparison between the flows generated from the same corpus by assigning a score to a dialogue flow. A good flow should cover the important and representative conversations from the corpus while simultaneously being as compressed as possible. Note that there is a trade-off between the degree of compression and the amount of information being lost. A graph containing all conversations potentially covers the entire corpus, but it is not beneficial for designing a dialogue system because it will strictly imitate historical conversations and will not generalize well to unseen dialogues. On the other hand, a simple graph start -> Hello -> Goodbye is still a dialogue flow, but it loses most of the information about the corpus. Our evaluation framework takes both of these aspects into consideration. As the core of our evaluation framework, we propose FuDGE, Fuzzy Dialogue-Graph Edit distance, an algorithmic way of computing the distance between a specific conversation and a given dialogue flow paths. FuDGE allows us to measure how well the dialogues are represented and covered by a dialogue flow. We combine the FuDGE score with the complexity of the DAG representing the flow and produce Flow-F1 (FF1) score that captures the trade-off between the compression and complexity. Overall, our contributions are as follows: 1. We introduce a novel dialogue flow evaluation framework that provides a metric to quantify the goodness of a dialogue flow generated from a dialogue corpora, taking into consideration the inherent trade-off between flow complexity and information contained in the flows 2. We propose FuDGE distance, an efficient edit-distance metric between a dialogue flow and a conversation that can independently be used in any dialogue evaluation task, including zero-shot dialogue generation from predefined schemes. We show that FuDGE can effectively separate within-task versus out-of-task conversations for a given set of dialogues. 3. We demonstrate that FF1 score can help hyperparameter tuning, ranking, and pruning the dialogue flows to identify the most optimal flows for a given dialogue corpus."
https://arxiv.org/html/2411.10371v1,"A Survey of Event Causality Identification: Principles, Taxonomy, Challenges, and Assessment","Event Causality Identification (ECI) has become a crucial task in Natural Language Processing (NLP), aimed at automatically extracting causalities from textual data. In this survey, we systematically address the foundational principles, technical frameworks, and challenges of ECI, offering a comprehensive taxonomy to categorize and clarify current research methodologies, as well as a quantitative assessment of existing models. We first establish a conceptual framework for ECI, outlining key definitions, problem formulations, and evaluation standards. Our taxonomy classifies ECI methods according to the two primary tasks of sentence-level (SECI) and document-level (DECI) event causality identification. For SECI, we examine feature pattern-based matching, deep semantic encoding, causal knowledge pre-training and prompt-based fine-tuning, and external knowledge enhancement methods. For DECI, we highlight approaches focused on event graph reasoning and prompt-based techniques to address the complexity of cross-sentence causal inference. Additionally, we analyze the strengths, limitations, and open challenges of each approach. We further conduct an extensive quantitative evaluation of various ECI methods on two benchmark datasets. Finally, we explore future research directions, highlighting promising pathways to overcome current limitations and broaden ECI applications.","As big data continues to proliferate, the channels and methods for acquiring unstructured text are constantly expanding. One of the primary challenges now lies in how to automatically extract valuable information and knowledge from these texts. This has become a significant area of research in Natural Language Processing (NLP) and Knowledge Reasoning. Events form the core content of texts, and various research directions have emerged around events, such as Event Extraction (EE) [1], Event Relation Extraction (ERE), and Event Coreference Resolution (ECR) [2]. In recent years, Event Causality Identification (ECI) has gained increasing attention as an important and challenging task [3]. As a critical subtask of ERE, ECI aims to predict whether there is a causal relationship between given events in a text. ECI has been widely applied in tasks such as question answering systems [4], information retrieval [5], event prediction [6], knowledge graph construction [7], and reading comprehension [8]. In the ECI task, events are represented by their triggers, known as ""event mentions."" The task then becomes determining which event mentions in a given text have a causal relationship. Figure 1 provides an example of this. ECI is vital for text understanding and decision-making, as it uncovers the causes and effects of events, helping to analyze risks and opportunities for more informed, data-driven decisions. This capability is particularly essential in domains that require complex reasoning, such as finance [9], law [10], healthcare [11], and the military [12]. Figure 1: An example of ECI. The red boxes indicate event mentions. The blue solid arrows represent intra-sentence event causalities, while the green dashed arrows represent inter-sentence event causalities. ECI focuses solely on extracting specific types of relations—cause and effect. However, compared to the general ERE task, identifying causalities, this task is more challenging due to several factors: 1. Implicitness: Causal links are often implied rather than directly stated, requiring deeper contextual and semantic understanding. 2. Long-distance dependencies: Causalities may span multiple sentences or paragraphs, requiring models to capture distant interactions. 3. Complexity: Causal chains can involve multi-step links (e.g., ""earthquake→collapsed→trapped""), adding complexity to identification. 4. Sample imbalance: In supervised settings, negative samples (non-causal pairs) dominate, challenging models to learn effectively from fewer positive cases. 5. Limited labeled training data: Most datasets are small, requiring models to parse semantics well and generalize causal patterns from limited annotations. Early ECI relied on feature pattern recognition, using cues like lexical signals [13, 14, 15], temporal features [16, 17], and co-occurrences [18, 19]. With the rise of Deep Learning (DL), more advanced methods emerged, enabling models to better capture contextual information by deeply encoding text semantics [20]. Transformer-based [21] Pre-trained Language Models (PLMs) [22] have revolutionized ECI, as they are trained on large corpora, enhancing semantic understanding and producing high-quality event and context representations, thereby improving causal identification [23, 24, 25]. Since 2023, Large Language Models (LLMs) have become widely popular. Using vast datasets and large-scale self-supervised learning, LLMs have gained stronger knowledge and contextual understanding, enabling them to handle few-shot and zero-shot tasks [26, 27]. This has advanced research in event extraction, relation extraction, and question answering. However, research by Gao et al. [26] highlights that while LLMs can perform zero-shot ECI with simple prompts, they are prone to ""causal hallucination,"" leading to many false positives. Hence, even though LLMs have significantly advanced tasks related to textual event analysis, ECI remains highly challenging. Research on ECI is still in its early stages, but the field is expanding rapidly. Recent years have seen a notable increase in ECI-related publications, particularly between 2022 and 2024, indicating growing interest and engagement. Though still a relatively small area, ECI research is diversifying in methodological approaches and gaining visibility in high-impact venues. This upward trend suggests that ECI will continue to attract significant attention in the near future. Several surveys have examined event causality and ERE in natural language. Asghar [28] reviewed early rule-based and statistical methods for causal extraction but did not cover deep learning (DL) approaches. Zhao et al. [29] and Xie et al. [30] provided overviews of advances in ERE, with Zhao et al. summarizing general trends in entity causality and Xie et al. comparing DL-based supervision methods. Liu et al. [31] focused on causal and temporal relationships but with limited coverage of recent methods, while Yang et al. [32] and Ali et al. [33] reviewed explicit and implicit causality extraction, limited to work prior to 2021. Liu et al. [34] covered methods, challenges, and datasets for event relation identification but lacked detailed method classification. In this paper, we provide a comprehensive review and analysis of the current state of research on ECI. We systematically outline the core concepts, key technologies, and methodological frameworks in this field, and conduct a quantitative evaluation and comparison of existing approaches. Furthermore, we present a thorough outlook on the future development trends of this promising research area. Our main contributions are as follows: • We detail several concepts of ECI, including problem formalization, datasets, evaluation protocols, and key technologies. • We propose the first comprehensive classification framework for ECI (Figure 2), summarizing the technical features of various methods, evaluating their strengths and limitations. • We perform a quantitative comparison of different ECI methods using experimental data reproduced on two common datasets. • We discuss future directions in ECI, highlighting key challenges and potential solutions for advancing the field. Figure 2: Taxonomy of ECI models, distinguishing SECI and DECI subtasks. For Feature Pattern-Based Matching SECI methods, only reference indices are provided due to the age of these methods and absence of model names. Other methods are labeled as ""Model Name/Author Name + Reference Index"" for clarity. The remainder of this paper is organized as follows: Section 2 presents the relevant concepts of ECI, reviews commonly used datasets, and evaluation metrics. Section 3 introduces key technologies for ECI. Section 4 provides a comprehensive overview of the classification framework for ECI, summarizes core techniques of various models, and analyzes their strengths and weaknesses. Section 5 presents a quantitative evaluation of the performance of existing classical methods on two datasets. Section 6 outlines future research directions for ECI."
https://arxiv.org/html/2411.10298v1,"Unveiling Topological Structures in Text: 
A Comprehensive Survey of Topological Data Analysis Applications in NLP","The surge of data available on the internet has led to the adoption of various computational methods to analyze and extract valuable insights from this wealth of information. Among these, the field of Machine Learning (ML) has thrived by leveraging data to extract meaningful insights. However, ML techniques face notable challenges when dealing with real-world data, often due to issues of imbalance, noise, insufficient labeling, and high dimensionality. To address these limitations, some researchers advocate for the adoption of Topological Data Analysis (TDA), a statistical approach that discerningly captures the intrinsic shape of data despite noise. Despite its potential, TDA has not gained as much traction within the Natural Language Processing (NLP) domain compared to structurally distinct areas like computer vision. Nevertheless, a dedicated community of researchers has been exploring the application of TDA in NLP, yielding 85 papers we comprehensively survey in this paper. Our findings categorize these efforts into theoretical and non-theoretical approaches. Theoretical approaches aim to explain linguistic phenomena from a topological viewpoint, while non-theoretical approaches merge TDA with ML features, utilizing diverse numerical representation techniques. We conclude by exploring the challenges and unresolved questions that persist in this niche field. Resources and a list of papers on this topic can be found at: https://github.com/AdaUchendu/AwesomeTDA4NLP. 111DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This material is based upon work supported by the Department of the Air Force under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Department of the Air Force. © 2024 Massachusetts Institute of Technology. Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.","Proliferation of the Internet has given rise to the generation of massive amounts of data. These massive amounts of data when processed can solve many crucial issues plaguing our current society. Due to this well-established notion among stake-holding institutions, the Machine Learning (ML) field has been thriving as a tool that extracts trends and solutions to non-trivial problems. However, real-world data tends to be noisy, heterogeneous, imbalanced, have missing labels, contain high-dimensionality, etc., often making the adoption of ML techniques to such datasets non-trivial. Therefore, to extract meaningful findings from data, specifically real-world data, clever techniques that extract additional features, while preserving the structure of the data need to be employed. To that end, a small niche community for Topological Data Analysis (TDA) applications in NLP has emerged. Being promised as a technique that can extract and analyze the shape/topology of data, TDA has great potential in mitigating such issues witnessed in real-world data. TDA is a “collection of powerful tools that can quantify shape and structure in data”222https://www.indicative.com/resource/topological-data-analysis/ and is inspired by the algebra and geometry mathematical fields. The benefits of TDA are vast, including the ability to extract additional features, typically not captured by other feature extraction techniques Uchendu et al. (2024); Michel et al. (2017). We call these features that TDA extracts - topological features. Unsurprisingly, since TDA is used to capture topological features, it has been applied to many tasks where data has distinct graphical structures Papamarkou et al. (2024); Hensel et al. (2021). These include tasks that have obvious graph-like structures such as protein classification Dey and Mandal (2018); Lamine et al. (2023); Valeriani et al. (2024) and drug discovery Alagappan et al. (2016); to those that are not so obvious such as diabetes classification Wamil et al. (2023); Skaf and Laubenbacher (2022), image classification Horn et al. (2022); Trofimov et al. (2023), time series analysis Petri and Leitao (2020); Tymochko et al. (2021); Gholizadeh and Zadrozny (2018), and speech processing Tulchinskii et al. (2023). However, since the shape of a text is not apparent, it has not gained as much attention in Natural Language Processing (NLP) as it has done in the Computer Vision Horn et al. (2022); Trofimov et al. (2023) and Medical Singh et al. (2023); Nielson et al. (2015) domains. Still, several researchers have found ways to extract additional features using TDA that the other typical numerical representation techniques in text such as TF-IDF, Word2Vec embeddings, BERT embeddings, etc have not captured. Figure 1 shows that there has been a gradual acceleration in the number of published works in TDA for NLP applications, and we project this trend will continue in the future. TDA aims to answer the main question - what is the true shape of a data? We survey 85 papers that have attempted to find an answer through various approaches. We categorize all observed approaches that incorporate TDA in NLP into two categories, namely theoretical Karlgren et al. (2014); Port et al. (2018) and non-theoretical Zhu (2013); Doshi and Zadrozny (2018). Theoretical approaches involve using TDA to explain linguistic phenomena by probing the topological space, shape, and evolution of topics. On the other hand, Non-theoretical approaches mainly discuss how to effectively apply existing numerical representation techniques in NLP to extract novel topological features with TDA. We will first discuss the principles behind TDA and the two main techniques employed for TDA feature extraction: Persistent Homology and Mapper."
https://arxiv.org/html/2411.10227v1,Entropy and type-token ratio in gigaword corpora,"Lexical diversity measures the vocabulary variation in texts. While its utility is evident for analyses in language change and applied linguistics, it is not yet clear how to operationalize this concept in a unique way. We here investigate entropy and text-token ratio, two widely employed metrics for lexical diversities, in six massive linguistic datasets in English, Spanish, and Turkish, consisting of books, news articles, and tweets. These gigaword corpora correspond to languages with distinct morphological features and differ in registers and genres, thus constituting a diverse testbed for a quantitative approach to lexical diversity. Strikingly, we find a functional relation between entropy and text-token ratio that holds across the corpora under consideration. Further, in the limit of large vocabularies we find an analytical expression that sheds light on the origin of this relation and its connection with both Zipf and Heaps laws. Our results then contribute to the theoretical understanding of text structure and offer practical implications for fields like natural language processing.","Entropy and type-token ratio (TTR) are useful metrics for evaluating the richness of lexical variation in writing. The TTR simply measures the proportion of unique words (types) to the total number of words (tokens) in a given text [1, 2], serving as a rough indicator of lexical diversity [3, 4, 5, 6]. The TTR has been originally leveraged in the study of children vocabulary [2, 7] or illnesses which affect linguistic capabilities [8]. However, the notion of TTR is problematic given its dependence on text length. As a consequence, its usefulness reduces when comparing two samples with differing token numbers. Efforts have been made to propose alternatives that show text length independence and are valid for both written and spoken corpora of different registers [9]. In contrast, entropy takes into account the distribution of these words in terms of their occurrence frequencies, capturing both the diversity of lexical items and also how uniformly they are used. As such, understanding the word entropy of texts [10] offers valuable insights into the complexity and unpredictability of language [11]. Originally proposed within information theory [12, 13], in the context of language higher entropy suggests more unpredictability in word choices, which can reveal underlying structures of language use and cognitive processing. Texts with high entropy exhibit greater diversity in word usage, reflecting more complex linguistic structures. Specifically, this measure and related operationalizations have been applied in a broad range of linguistic studies, including differentiation between translated and original documents [14], the study of ambiguity in different legal systems [15], quantification of language change [16], identification of stopwords [17], or observation of change in literary preference [18], among others. Hence, entropy and TTR would address lexical diversity from two different, seemingly independent perspectives. Nonetheless, the production of natural language texts are constrained by statistical laws [19]. These empirical rules highlight patterns that occur in word frequency and vocabulary growth across languages. On the one hand, Zipf law [20] states that a few words are used very frequently, while most are rare. On the other hand, Heaps law [21, 22] dictates that the number of types in a text increases more slowly (sublinearly) than its total number of tokens. These laws, among others [23], help linguists and computational scientists model language, offering insights into communication efficiency and the structure of human language systems. Below, we provide evidence that there exists a functional relation between entropy and type-token ratio. It follows that these diversity measures are not fully independent from each other, and we understand their dependence making use of Zipf and Heaps laws. Quite generally, Zipf and Heaps laws are closely related to the concept of diversity in text analysis. Zipf law, by highlighting the unequal distribution of word frequencies, influences the predictability of words in a text. As mentioned above, this distribution plays a key role in calculating the entropy of natural language texts, where frequent words reduce unpredictability and rarer words increase it. Similarly, Heaps law, which describes vocabulary growth, directly affects the lexical diversity of a text as calculated from the TTR definition. Our work discusses two main findings. First, we uncover an empirical relation between the entropy and the type-token ratio based on our analysis of six gigaword corpora. This contribution is significantly noteworthy since the TTR has typically been computed in texts containing around 10^{5} tokens [4, 6], much smaller than our datasets. While understaning the interplay between the two diversity measures remains a complicated task, the aforementioned relation is consistently found across the studied massive corpora. Second, to gain insight about this correlated behaviour of the entropy and the type-token ratio, we fit an analytical expression for the word entropy to the corpus data. This expression is based on the validity of Zipf law and holds for large vocabularies. Using Heaps law, we express the word entropy in terms of the type-token ratio and fit it to the six massive corpora, finding an excellent agreement with the numerical results. The agreement is observed for languages of different morphological types and sources of distinct genres and registers or styles. This is particularly relevant since it suggests that the entopy-TTR relation is robust against how language users vary their word choices, at least in large corpora. Our findings would then be valuable in optimizing performance and efficiency across various tasks and applications of natural language processing (NLP) and machine learning including quantifying information content, evaluating large language models and enhancing data compression. The remaining of the manuscript is structured as follows. In Section II, we describe the corpora used and analyze their corresponding Zipf and Heaps laws. In Section III we derive an analytical expression for the entropy of large vocabularies and fit this formula to the different datasets, obtaining good results. In Section IV we investigate the type-token ratio in the various corpora using the Heaps law. We combine both diversity metrics in Section V, where we discuss the functional relation that connects H and TTR and its agreement with the empirical results. Finally, Section VI contains our conclusions and suggestions for further work."
https://arxiv.org/html/2411.10172v1,Increasing the Accessibility of Causal Domain Knowledge via Causal Information Extraction Methods: A Case Study in the Semiconductor Manufacturing Industry,"The extraction of causal information from textual data is crucial in the industry for identifying and mitigating potential failures, enhancing process efficiency, prompting quality improvements, and addressing various operational challenges. This paper presents a study on the development of automated methods for causal information extraction from actual industrial documents in the semiconductor manufacturing industry. The study proposes two types of causal information extraction methods, single-stage sequence tagging (SST) and multi-stage sequence tagging (MST), and evaluates their performance using existing documents from a semiconductor manufacturing company, including presentation slides and FMEA (Failure Mode and Effects Analysis) documents. The study also investigates the effect of representation learning on downstream tasks. The presented case study showcases that the proposed MST methods for extracting causal information from industrial documents are suitable for practical applications, especially for semi structured documents such as FMEAs, with a 93% F1 score. Additionally, MST achieves a 73% F1 score on texts extracted from presentation slides. Finally, the study highlights the importance of choosing a language model that is more aligned with the domain and in-domain fine-tuning.","Causal domain knowledge plays a vital role in various downstream tasks, such as risk assessment [12], root cause analysis [32] and data mining [1]. The smallest unit of the causal domain knowledge is known as a causal relationship. The causal relationship is a connection between two or more events or variables where one event or variable is the cause of the other. Causal domain knowledge is commonly documented either in unstructured or semi structured forms. Tabular formatted documents, typically utilized in the Failure Mode Effect Analysis (FMEA), serve as a prime example of semi structured documents. Furthermore, much causal domain knowledge is also found in presentation slides, which are documents that do not follow a predefined structure, and are typically used in industry for sharing information between the different teams and with customers. In FMEA documents, the manually created textual content presents several challenges such as non-standardized descriptions of the failure modes, effects, and root causes, and in many cases merged cells. In the case of merged cells, the description of multiple events within a single FMEA table cell [27], which can involve enchained relations. In an enchained causal relation, a cause or an effect in a causal relation is the cause or the effect of another causal relation. This creates a complex network of interconnected events where each cause and effect is both influenced by and influences other events in the chain. Furthermore, the increasing use of digital presentation slides as a medium for presenting and sharing information has made them a valuable source of knowledge [11]. Presentation slides stemming from processes like failure analysis requests encapsulate documented causal relations. These relations shed light on failures detected in products, providing a comprehensive understanding of the factors contributing to such issues. However, the unstructured nature of presentation slides, which combine visual and textual elements and use spatial positioning to document information, can make automated information access difficult. While these features are useful for presenting information to a live audience, they can make it difficult for automated systems to extract and understand the information contained in the slides. Presentation slides and FMEA documents are a rich source of causal domain knowledge. However, the sheer volume of documents in the industry can make it difficult to manually process and extract the information contained within them. This can lead to increased product development cycle time, which can be detrimental to overall productivity. Therefore, automatically extracting causal domain knowledge from unstructured documents like presentation slides, as well as from semi structured documents like FMEA documents, can be highly beneficial in increasing the availability and accessibility of this knowledge. Scholars have been addressing the topic of causal information extraction from text devising natural language processing (NLP) methods. Two common NLP approaches are lexical pattern-based methods [8] and statistical machine learning-based methods [41]. More recently, pre-trained transformers based language models are devised for extracting meaningful representation of the text in sequence tagging based approach for extracting causal information [10, 7, 34]. Also, research indicated that fine-tuning on domain specific data can enhance the vector text representation by better capturing the nuances and complexities of the domain-specific language, which can improve the overall performance on downstream tasks [19]. Even though causal information extraction has become a well studied topic in recent years, streamlined causal information extraction methods tend to struggle with the detection of detailed causal relations including nested and enchained relations [34, 7]. Furthermore, testing of these methods on different types of industrial documents is limited. To address this gap, the following research questions are addressed in this paper: • How effective are existing causal information extraction methods on different types of industrial documents? • What is the effect of text representation learning on the overall performance of causal information extraction in industrial settings? In summary, the aim of this research is to develop a method for detailed causal information extraction on industrial data, with which the availability and consistency of causal domain knowledge can be improved in the industrial world, facilitating more reliable data analysis and more informed decision-making. Our work has the potential to be applicable to a variety of domains, including teaching, where much causal domain knowledge is documented in unstructured or semi structured documents. By addressing these research questions, the contribution of this paper can be summarized as follows: • Extending causal information extraction methods to industrial documents, increasing the availability of causal domain knowledge for downstream tasks in the industry. • Providing guidance for practitioners working in different industries with similar types of documents. • Addressing data consistency issues commonly found in semi structured documents, like the merged cells in FMEA (Failure Mode and Effects Analysis) documents. • Contributing to the body of research that highlights the effect of representation learning on downstream tasks."
https://arxiv.org/html/2411.10163v1,Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions,"Large language models (LLMs) demonstrate remarkable performance across various tasks, prompting researchers to develop diverse evaluation benchmarks. However, existing benchmarks typically measure the ability of LLMs to respond to individual questions, neglecting the complex interactions in real-world applications. In this paper, we introduce Compound Question Synthesis (CQ-Syn) to create the Compound-QA benchmark, focusing on compound questions with multiple sub-questions. This benchmark is derived from existing QA datasets, annotated with proprietary LLMs and verified by humans for accuracy. It encompasses five categories: Factual-Statement, Cause-and-Effect, Hypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion. It evaluates the LLM capability in terms of three dimensions including understanding, reasoning, and knowledge. Our assessment of eight open-source LLMs using Compound-QA reveals distinct patterns in their responses to compound questions, which are significantly poorer than those to non-compound questions. Additionally, we investigate various methods to enhance LLMs performance on compound questions. The results indicate that these approaches significantly improve the models’ comprehension and reasoning abilities on compound questions.","Figure 1: Examples of non-compound and compound questions. (Left) The user presents multiple questions in a multi-turn dialogue, with one question at a time. (Right) The user poses several questions simultaneously in a single conversational turn. Large language models (LLMs) have achieved remarkable success in natural language processing (NLP), demonstrating exceptional performance across a wide range of tasks due to their advanced language understanding and generation capabilities (Achiam et al., 2023; Dubey et al., 2024; Ouyang et al., 2022; Jiang et al., 2024; Team et al., 2024). Existing benchmarks evaluate these models’ abilities across various dimensions (Kwan et al., 2024; Zhou et al., 2023a; He et al., 2024a; Li et al., 2023), such as knowledge (Liu et al., 2023b; Jin et al., 2019), reasoning (Yang et al., 2018; Zhu et al., 2024), and understanding (Bartolo et al., 2020; Li et al., 2023). However, these benchmarks primarily focus on the model’s ability to respond to individual questions or instructions, which is a critical limitation when considering the complexity of real-world interactions. In real-world scenarios, users often ask a series of interrelated questions within a single query, expecting to obtain a comprehensive and precise response for each question, as illustrated in Figure 1. We refer to this as Compound Questions, which include multiple sub-questions within a single turn. These sub-questions may be correlated as discussed in Section 3.1. This question format is common in human-AI interactions and agent-based scenarios, where tasks are decomposed into sub-instructions that require individual responses. While humans can effectively address compound questions by answering each sub-question separately without omission or interference, LLMs face challenges such as identification of sub-questions and the elliptical phenomena in natural language (van Craenenbroeck and Temmerman, 2018). The presence of adjacent questions and answers can induce the LLMs to focus on the former context while ignoring the unanswered sub-questions and their context. It has been shown that LLMs are susceptible to irrelevant context, suggesting that they may also be influenced by other sub-questions and their answers (Wu et al., 2024). Recent studies explore the performance of LLMs in handling multiple-problem tasks (Wang et al., 2024) and sequential instruction following (Chen et al., 2024), finding that multi-problem hints significantly reduce inference costs. However, these studies primarily focus on classification tasks or fixed-answer tasks. In contrast, our study emphasizes real-world QA scenarios, with a particular focus on open-ended QA tasks. To this end, we introduce a data synthesis framework called Compound Question Synthesis (CQ-Syn). This framework leverages LLM to generate and refine compound questions according to carefully developed guidelines, followed by a thorough human review to ensure quality. Using CQ-Syn, we construct the Compound-QA benchmark, which is designed to evaluate LLMs’ ability to handle compound questions. This benchmark consists of 1,500 compound questions covering scenarios in language understanding, reasoning, and knowledge, divided into five types: Factual-Statement, Cause-and-Effect, Hypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion. Comprehensive experiments on eight open-source LLMs reveal that their effectiveness on compound instructions is significantly lower compared to single-question tasks. However, supervised fine-tuning with instruction data augmented by compound questions substantially improves this performance. We anticipate that this work will encourage further research and advancements to enhance the ability of LLMs to answer compound questions.111The dataset utilized in this study will be made publicly available to foster continued research."
https://arxiv.org/html/2411.10145v1,An Effective Framework to Help Large Language Models Handle Numeric-involved Long-context Tasks,"Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long texts and have almost perfect performance in traditional retrieval tasks. However, their performance significantly degrades when it comes to numerical calculations in the long-context. Numeric-involved long-context tasks typically cannot be addressed by current LLMs in normal settings due to their inherent limitations in simultaneously handling complex and massive information. Some CoT like prompting methods can improve accuracy but demands massive output tokens, which is costly and slow. To address this issue, we propose a workflow, which decompose a numeric-involved long-context task into 4 low-level subtasks: judging, extracting and processing with code and conclusion. The former 2 subtasks is relatively simple, which allows us to use smaller models for efficiently processing long context. When numerical calculations are required, we use code generated by LLMs to avoid the disadvantage of LLM not being good at calculations. The results in 2 numeric-involved long-context benchmarks demonstrate our workflow can not only improve accuracy, but also significantly reduce the cost of API calls.","In the past year, long-context language models (LCLMs) such as GPT-4o-128k (OpenAI, 2023) and Gemini-1.5-1000k (Team et al., 2023) have surged in popularity, raising questions about their efficacy in handling extended context tasks. While various LCLMs have demonstrated excellent long-context retrieval ability by passing the “Needle in a Haystack” test (gkamradt, 2023) in over 100k context length, benchmarks like Loogle (Li et al., 2023) and Loong (Wang et al., 2024) have highlighted their shortcomings in more complex tasks. Looking throughout those long-context evaluation works, it can be clearly observed that, though LLMs’ performance varies in different types of long-context tasks and there is gap between small and large models, all the models generally perform very badly on numeric-involved long-context tasks. Numeric-involved means the task requires some arithmetic operations, such as comparison, sorting or counting. An example of a numeric-involved (it involves comparing the ages) long-context task is as follows: The long context: Hallie Turner is a 21 years old student, graduated from …… The student named Sonali Jain is graduated from …… His age is 25 …… Jack likes basketball and …… Question: Which student is the oldest? According to the results of previous researches (Wang et al., 2024; Li et al., 2023; Yu et al., 2024), as long as involving some even very low-level numerical calculations, the long-context task become very challenging for most LLMs. Though this phenomenon has been discovered in previous evaluations (Wang et al., 2024; Li et al., 2023; Yu et al., 2024), however, there is currently no research targeting at solving this type of problems. Just like using Chain-of-Thoughts Wei et al. (2022) can greatly help LLMs better solve math problems, using some CoT like prompting methods, such as let LLMs first carefully analyze the data in the context and then answer (Yu et al., 2024), can indeed help improve accuracy. However, due to the potential for massive amounts of data to be analyzed in the long context, such methods will have to make LLMs generate numerous tokens for analyzing, which cause the inference speed extremely slow and generate huge API call costs. Figure 1: The structure of our method for numeric-involved long-context tasks. Intuitively, a numeric-involved long-context task can be seperated into at least 2 steps: extracting data and analyzing data. Merely extracting data which has been presented in the context is a low-level long-context task, which is not hard for most long-context language models, even small ones like Qwen2.5-1.5b-instruct Yang et al. (2024). But when analyzing data, due to the poor arithmetic ability of LLMs, they often fail, that is why previous researches proposed to prompt LLMs to write code and execute it to solve arithmetic problems. Therefore, considering the above two characteristics, we propose a more efficient and economic methods, which decomposes a numeric-involved task into multiple low-level steps, and uses small models to offload the pressure of extracting information from long texts, which can avoid the cost of using closed-source models. And when involving calculating the extracted data, we prompt LLMs to write code and execute it with a external code interpreter to process the data. Finally, the LLM use the processed data to conclude a final answer. Figure 1 shows the overall process of our method. We test our method on 2 numeric-involved long-context benchmarks, Loong Wang et al. (2024) and difficult-retrieval Yu et al. (2024). The results demonstrate our method can not only improve accuracy, but also greatly save money. It is an effective, efficient and economic framework, and can be easily applied to long-context QA tasks. Method Dense Sparse Accuracy(%) Cost($) Acc comp.(%) Acc clus.(%) Cost($) normal prompt 6.0 0.09 31.2 54.4 0.49 CoT prompt 28.0 0.15 41.2 84.4 0.52 Ours 99.0 0.01 36.6 64.2 0.01 Table 1: Performance of different methods on numerical-dense and numerical-sparse long-context tasks."
https://arxiv.org/html/2411.10137v1,Legal Evalutions and Challenges of Large Language Models,"In this paper, we review legal testing methods based on Large Language Models (LLMs), using the OPENAI o1 model as a case study to evaluate the performance of large models in applying legal provisions. We compare current state-of-the-art LLMs, including open-source, closed-source, and legal-specific models trained specifically for the legal domain. Systematic tests are conducted on English and Chinese legal cases, and the results are analyzed in depth. Through systematic testing of legal cases from common law systems and China, this paper explores the strengths and weaknesses of LLMs in understanding and applying legal texts, reasoning through legal issues, and predicting judgments. The experimental results highlight both the potential and limitations of LLMs in legal applications, particularly in terms of challenges related to the interpretation of legal language and the accuracy of legal reasoning. Finally, the paper provides a comprehensive analysis of the advantages and disadvantages of various types of models, offering valuable insights and references for the future application of AI in the legal field.","In recent years, the breakthrough of deep learning technology in natural language processing (NLP), particularly the rapid advancement of Transformer technology, has led to the flourishing of LLMs [1]. Models like OpenAI’s GPT series have demonstrated exceptional capabilities in NLP, excelling not only in traditional NLP tasks such as machine translation and Question Answering, but also in some multimodal tasks, such as image-to-text translation, speech recognition, and subtitle generation [2, 3, 4, 5]. These models are capable of accurately understanding relationships between various data forms and enabling cross-modal information transformation, significantly enhancing automation and efficiency across these fields. In the legal field, LLMs are seen as a transformative force with the potential to revolutionize traditional legal services, owing to their comprehensive legal knowledge base and exceptional capabilities in natural language understanding and generation [6]. Some studies have explored the application of LLMs in the analysis and generation of legal texts, evaluating their performance in tasks such as legal reasoning, case retrieval, and legal question answering, and investigating their potential to improve the efficiency and accuracy of legal work [7]. Meanwhile, other researchers have focused on developing LLMs specifically tailored for legal domains, enabling these models to better understand legal terminology, apply legal provisions accurately, and adapt to the nuances of different legal systems. This specialization aims to increase the practical value of LLMs in legal practice [8, 9, 10]. However, effectively evaluating the performance of LLMs across various legal systems and linguistic environments remains a significant challenge. Additionally, addressing the technical and ethical concerns associated with their application is an urgent issue that requires further attention and resolution. The application of LLMs in the legal field also faces numerous challenges and issues. First, legal language is highly specialized and precise, making it crucial to ensure the accuracy and legality of the content generated by these models [11, 12]. Second, LLMs may absorb biases and inaccuracies from their training data, which can have serious repercussions when applied in the legal context [13, 14, 15]. Additionally, the automation of legal decision-making processes could lead to ethical concerns and disputes over legal accountability [16, 17]. As shown in Fig 1 Based on this background, this work aims to provide a comprehensive overview of the performance of LLMs in the legal field, offering valuable insights for both the academic community and legal practitioners. The study is structured as follows: Section 1: This Section explains the background, purpose, and significance of the study, outlining the motivations and objectives behind the research. Section 2: This Section provides a detailed analysis of legislation related to large models on a global scale, exploring the similarities and differences in policies and regulations across various countries. Section 3: The focus is on models specifically tailored to the legal domain, examining their technical features and evaluating their potential applications in legal practice. Section 4: This Section presents a comprehensive assessment of the models discussed in Section 3 using thirteen Chinese and thirteen English legal cases. The cases were selected to include a complete set of four components: judgment, background, analysis and conclusion. The Section systematically evaluates the performance and applicability of each model through a comparative analysis of results and quantitative metrics. Section 5: This Section discusses key issues related to the use of LLMs in the legal field, including data privacy, legal liability, ethical considerations, and technical limitations. Section 6: The final Section summarizes the findings of the study and provides an outlook on future research directions. Through this study, we aim to provide in-depth insights into the application of LLMs in the legal area, fostering their rational and sustainable integration into legal practice. This will not only contribute to improving the efficiency and quality of legal services but also lay a solid foundation for future innovations in legal technology. Figure 1: Overview of the study"
https://arxiv.org/html/2411.10083v1,Xmodel-1.5: An 1B-scale Multilingual LLM,"We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and English. In addition, we contribute to the research community by releasing a Thai evaluation dataset, which includes hundreds of questions annotated by students from Chulalongkorn University’s School of Integrated Innovation. While the results are promising, we acknowledge that there is still room for improvement. We hope this work advances ongoing efforts in multilingual AI research and promotes better cross-linguistic understanding in various natural language processing tasks. Our models and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelLM-1.5.","The rapid globalization of communication has created an urgent need for advanced multilingual natural language processing (NLP) models that can bridge linguistic divides across regions. Traditional NLP models often struggle with less-represented languages, limiting their effectiveness in global applications. As cross-cultural interactions increase, there is a growing demand for AI systems that can understand and generate multiple languages with high accuracy and relevance. In response to this challenge, Xiaoduo Technology’s AI Lab has developed a 1-billion-parameter multilingual large model. Our model excels not only in widely spoken languages such as Chinese and English, but also in languages like Thai, Arabic, and French, demonstrating top-tier performance among models of similar scale. This work addresses the critical need for more inclusive AI systems capable of serving a wider array of linguistic and cultural contexts. Beyond the model itself, we also contribute to the research community by open-sourcing a Thai evaluation dataset. This dataset, consisting of hundreds of questions annotated by students from Chulalongkorn University’s School of Integrated Innovation, offers a valuable resource for future research in Thai language processing. These efforts highlight our commitment to advancing multilingual AI and improving the tools available for global communication and research."
https://arxiv.org/html/2411.10080v1,"Understanding The Effect Of Temperature On Alignment With Human
Opinions","With the increasing capabilities of LLMs, recent studies focus on understanding whose opinions are represented by them and how to effectively extract aligned opinion distributions. We conducted an empirical analysis of three straightforward methods for obtaining distributions and evaluated the results across a variety of metrics. Our findings suggest that sampling and log-probability approaches with simple parameter adjustments can return better aligned outputs in subjective tasks compared to direct prompting. Yet, assuming models reflect human opinions may be limiting, highlighting the need for further research on how human subjectivity affects model uncertainty.","Large Language Models (LLMs) have shown notable skills across a diverse range of tasks. Their increasing prevalence underscores the need for effectively modeling and representing a diverse range of human values and perspectives [1]. Several studies investigated the extent to which LLMs represent multiple human opinions [2, 3, 4, 5, 6, 7]. While studies [4] and [5] explored whose opinions are reflected by LLMs on a subjective opinion QA task, study [7] assessed their capability to capture human opinion distributions insubjective classification tasks. In [6] and [3] on the other hand, the authors analysed whether LLMs align with human opinion distributions in NLI (natural language inference) tasks. Recent studies observed misaligned LLM and human opinion distributions [5, 6, 7]. Others found methods that improve the alignment with human distributions [3, 2]. However, none of the studies examined the effect of temperature. We consider that allowing models to chose beyond the maximum likelihood token (0 temperature) may reveal more diverse opinions that could align with broader human views."
https://arxiv.org/html/2411.10069v1,Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity,"Evaluating the importance of different layers in large language models (LLMs) is crucial for optimizing model performance and interpretability. This paper first explores layer importance using the Activation Variance-Sparsity Score (AVSS), which combines normalized activation variance and sparsity to quantify each layer’s contribution to overall model performance. By ranking layers based on AVSS and pruning the least impactful 25%, our experiments on tasks such as question answering, language modeling, and sentiment classification show that over 90% of the original performance is retained, highlighting potential redundancies in LLM architectures. Building on AVSS, we propose an enhanced version tailored to assess hallucination propensity across layers (EAVSS). This improved approach introduces Hallucination-Specific Activation Variance (HSAV) and Hallucination-Specific Sparsity (HSS) metrics, allowing precise identification of hallucination-prone layers. By incorporating contrastive learning on these layers, we effectively mitigate hallucination generation, contributing to more robust and efficient LLMs(The maximum performance improvement is 12%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and WikiQA datasets demonstrate the efficacy of this method, offering a comprehensive framework for both layer importance evaluation and hallucination mitigation in LLMs.","Evaluating the importance of different layers in deep learning models is crucial for improving model efficiency, interpretability, and robustness. Identifying key layers allows for effective model compression and a more informed model design. Recently, large language models (LLMs) have shown remarkable capabilities across diverse applications, including question answering, language modeling, and sentiment analysis. However, there is limited research on the functional contributions of individual layers in LLMs, particularly from the perspective of activation variance and sparsity, which could reveal each layer’s unique role in model performance and interpretability Wang et al. (2024); Xiong et al. (2020). Moreover, studies specifically focusing on hallucination propensity based on layer activation patterns in LLMs remain largely unexplored, leaving a critical gap in understanding and mitigating layer-specific hallucination generation. Previous works on layer importance have introduced several sophisticated methodologies. Saarela et al. Saarela and Jauhiainen (2021) proposed Gradient-Based Importance Scores (GBIS), which assess layer importance by calculating the sensitivity of gradients relative to inputs, thereby reflecting model reliance on each layer’s activations. Zopf et al. Bach et al. (2015) introduced Layer-wise Relevance Propagation (LRP), analyzing information flow through the model and helping to understand the role of each layer in the model’s decision process. Additionally, Mencía et al. Zopf et al. (2016) developed Contextual Importance Measures (CIM), dynamically evaluating layer importance based on specific input conditions. While these methods offer valuable insights, they often fall short in capturing complex activation patterns and identifying redundancy in LLMs, particularly as model depth and size increase. Figure 1: Illustration of the Activation Variance-Sparsity Score (AVSS) method for assessing layer importance in large language models. (a) Layer Structure: Overview of model layers (1 to 32) analyzed for activation properties. (b)Activation Variance and Sparsity: Top: High-variance layers capture diverse information. Bottom: Darker cells indicate sparse activations, suggesting redundancy. (c) AVSS Calculation and Ranking: AVSS, normalized AVSS, and cumulative AVSS formulas are used to rank layers, identifying low-scoring layers as pruning candidates. In this work, we propose an enhanced approach, the Activation Variance-Sparsity Score (AVSS), to evaluate layer importance in LLMs. AVSS combines normalized activation variance and sparsity to quantify each layer’s role in model performance. By ranking layers based on AVSS and removing approximately the lowest 25% of layers, we retain over 90% of the original model performance on tasks such as question answering, language modeling, and sentiment analysis, indicating potential redundancy within LLM architectures.Achiam et al. (2023); Azadi et al. (2023); Azaria and Mitchell (2023); Bai et al. (2022); Bradley (1997) To address the unexplored area of hallucination generation across layers, we extend AVSS to introduce the Enhanced Activation Variance-Sparsity Score (EAVSS), a framework designed to quantify hallucination propensity within each layer of LLMs. By incorporating Hallucination-Specific Activation Variance (HSAV) and Hallucination-Specific Sparsity (HSS), EAVSS precisely identifies hallucination-prone layers based on their unique activation patterns during hallucination events. The EAVSS method fills a significant gap in LLM research, providing a comprehensive layer-wise analysis of hallucination potential. Moreover, we apply contrastive learning on layers with high hallucination scores, effectively mitigating hallucination generation and contributing to improved model robustness and reliability. Brier (1950); Burns et al. (2023); Chen et al. (2024a, b); Chiang et al. (2023); Chuang et al. (2024); Cohen et al. (2023); Daheim et al. (2024) The main contributions of our paper are as follows: • We propose the Activation Variance-Sparsity Score (AVSS) as a novel metric for evaluating layer importance in LLMs, combining variance and sparsity to improve interpretability and performance retention. • We introduce an enhanced AVSS framework for assessing hallucination propensity, using Hallucination-Specific Activation Variance (HSAV) and Hallucination-Specific Sparsity (HSS) to identify and target hallucination-prone layers. • We demonstrate that a contrastive learning approach on high-hallucination layers can effectively mitigate hallucination generation, contributing to improved model robustness and efficiency."
https://arxiv.org/html/2411.10018v1,"Once More, With Feeling: Measuring Emotion of Acting Performances in Contemporary American Film","Narrative film is a composition of writing, cinematography, editing, and performance. While much computational work has focused on the writing or visual style in film, we conduct in this paper a computational exploration of acting performance. Applying speech emotion recognition models and a variationist sociolinguistic analytical framework to a corpus of popular, contemporary American film, we find narrative structure, diachronic shifts, and genre- and dialogue-based constraints located in spoken performances.","Film is rich in its supply of semiotic resources, communicating meaning from the interaction of language (encoded in a script), visuals (choices of composition, blocking, cinematography), sound and more. Much computational work has arisen to examine slices of this semiotic field, including measuring how gender stereotypes or plot arcs are reflected in dialogue [1, 2, 3] or how visual features like color variance and shot length constitute genre [4, 5]. One critical area, however, that has been neglected in this study is the role of performance in creating meaning. As Naremore [6] notes, film is a medium in which meaning is acted out; an acting performance provides a semiotic frame through which we can understand the events that unfold. Given the fixed text of a script, the rendering of the final performance is an interpretive process in which the actor, director and editor jointly imbue the words with additional meaning. In this view, the same line of dialogue exhibits variation in meaning when performed in distinct diegetic contexts. As one example, consider the following line in Knives Out (2019): “I’m warning you.” Much of the film revolves around these three words, overheard in a conversation between the wealthy Harlan Thrombey and his grandson, Ransom. The line is uttered by multiple characters as the film unfolds: angrily shouted by Ransom, somberly recalled by the eavesdropper, and gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located within a single diegetic event, has great capacity for meaning-making in performance. When viewed in this light, we can apply the analytical framework from variationist sociolinguistics to better understand this space of performance. Given a fixed line of dialogue (equivalent to a linguistic variable), a performance entails a choice — a selection from the set of possible variants. It is this choice, and the meaning contained within, which we study. In this work, we design computational models to explore this form of variation by considering the emotional range of performances in contemporary American film, exploring in particular the tension between what characters say and how they say it. As distinct from prior work in the computational humanities that has measured emotion from text alone [3, 7], we measure acted emotion from speech, allowing us to disentangle the emotion present in the script from the choices made in creating the performance. Using a speech emotion recognition model, we construct a parallel dataset of spoken performances (utterances) aligned with the text of the words being spoken (dialogue phrases). This dataset allows us to isolate and examine how performances vary in meaning from their paralinguistic features in addition to the textual meaning of the screenplay. We use this dataset to carry out several case studies exploring variation in performance in American film: 1. First, we carry out a structural analysis of emotion as performed over narrative time. Doing so allows us to characterize film as performance text, relating emotional performance to larger narrative structure. 2. Second, we study diachronic variation by comparing emotionality of films across release years, testing the degree to which performances have intensified over time (following Bordwell’s theories of visual style [8]). 3. Finally, we examine the capacity for performance by constructing a novel measure of emotional range for an utterance—the space of possible emotions that can be performed. In doing so, we demonstrate how both contextual (genre) and textual (dialogue) aspects of film can carry constraints and affordances on acting performance. In this work, we use computational methods to survey how both textual and contextual variables inform and reflect the performances rendered on screen."
https://arxiv.org/html/2411.10006v1,"Orca: Enhancing Role-Playing
Abilities of Large Language Models by Integrating Personality Traits","Large language models has catalyzed the development of personalized dialogue systems, numerous role-playing conversational agents have emerged. While previous research predominantly focused on enhancing the model’s capability to follow instructions by designing character profiles, neglecting the psychological factors that drive human conversations. In this paper, we propose Orca, a framework for data processing and training LLMs of custom characters by integrating personality traits. Orca comprises four stages: (1) Personality traits inferring, leverage LLMs to infer user’s BigFive personality trait reports and scores. (2) Data Augment, simulate user’s profile, background story, and psychological activities. (3) Dataset construction, personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4) Modeling and Training, personality-conditioned instruction tuning (PTIT and PSIT), using the generated data to enhance existing open-source LLMs. We introduce OrcaBench, the first benchmark for evaluating the quality of content generated by LLMs on social platforms across multiple scales. Our experiments demonstrate that our proposed model achieves superior performance on this benchmark, demonstrating its excellence and effectiveness in perceiving personality traits that significantly improve role-playing abilities.","Building human-like conversation agents is a long-term challenge for AI researchers. The emergence of groundbreaking language models such as ChatGPT and GPT-4 (OpenAI, 2023), coupled with their intrinsic capacity for emergent in-context learning (ICL) (Brown et al., 2020) abilities and a three-stage reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) algorithm which have largely raised the capacity bar of existing AI systems. LLMs have acquired a wealth of knowledge during their pre-training stage. ICL utilizes LLMs in a few-shot or zero-shot way that can instruct LLMs to understand the tasks in the form of natural language text. Therefore, personality-based responses have gained significant attention. Despite GPT-4 exhibit advanced role-playing capabilities because human-generated conversations are combined with the Instruct tuning dataset in a dialogue format for training, it is widely recognized that LLMs, suffer from a lack of consistent personality traits often failing to be engaging. This is the result of the existing LLMs are predominantly trained on general domains and lack specific optimization for personalized LLMs. Recently, human social behavior is being changed by role-playing applications, such as Character.AI which has attracted a growing number of researchers to bridge the gap between the text and behavior of dialogue agents and humans (Team, 2023; Wang et al., 2024). Personality-based dialogue systems can be broadly categorized into two types. (1) Persona-based Dialogue, represented by the work in Zhang et al. (2018) where the manipulation of profile information is employed to enhance the appeal of chit chat. These ideas are also used in the latest character LLMs such as CharacterGLM (Zhou et al., 2023), Ditto (Lu et al., 2024), and ChatHaruhi (Li et al., 2023), aiming to improve the humanity of customized characters. However, these approaches primarily create profile settings as prompts for model training, overlooking the psychological factors of human language and behavior. (2) Personality-aware Dialogue. It is more novel to attempt to establish a connection between personality traits and character compatibility. Wang et al. (2024) proposed a Social Support Conversation (S2Conv) framework–CharacterChat. To achieve this goal, it created a group that of virtual characters with distinct profiles called MBTI-1024 Bank based on the MBTI (Myers-Briggs Type Indicator) to train LLMs. In order to link individuals with persona-compatible virtual supporters. It designed a series of support agents and the interpersonal matching mechanism. But the psychological theory-based personality traits with implicit expression and behavior are not well modeled. Also in the field of emotional support, Dan et al. (2024) proposed a mixture of experts (MoE)-based personalized LLMs, named P-tailor, to model the Big Five Personality Traits such as openness, conscientiousness, extraversion, agreeableness and neuroticism. In fact, each BigFive dimension has six sub-dimensions (Gosling et al., 2003), P-tailor only categorizes the BigFive high and low into 10 routes, ignoring the low-dimensional features and failing to model continuous personality trait scores, which is still a challenge to deeply fuse personality traits and language models. In addition, the data for the above work were entirely produced by LLMs, and the personality traits of the characters have low confidence, which limits the effectiveness of the model in fusing personality traits. Figure 1: The workflow for developing our personalized agent system, Orca, to provide personalized interaction on social media platforms. Orca comprises four stages: (1) Personality traits inferring; (2) Data Augment through designing numerous simulation prompts. (3) Dataset construction, serial instruction for the connection of labels and character and personality traits. (4) Modeling and Training, personality traits instruction tuning (PTIT) and personality scores tuning (PSIT), using the generated data to enhance existing open-source LLMs. In our opinion, there will be three stages in the development of personalized modeling: The first is the inclusion of character information profiles, such as changing the system prompt, appending character information at the end of the prompt, and stimulating LLM-related responses in the form of zero-shot/few-shot (Tu et al., 2023). The second stage is integrating psychological theories. How to integrate psychological theories and LLMs is a meaningful research topic, although there are some research works trying to train LLMs perceive and express emotions (Wen et al., 2023), the existing research works are still insufficient. The third stage is to fuse personality trait modalities. Higher dimensional vectors retain more information than discrete token ids that can be perceived by LLMs, analogous to the embedding fusion of LLMs and vision models (Zhu et al., 2024; GLM et al., 2024) that is the soul of personalized AI assistant. From a macro perspective, the emergence of LLM-based agents allows us to take a more microscopic view of simulated society, which leads to more discoveries from the new representation (Xi et al., 2023). Our work aims to improve the second stage and then try to move towards the third stage. In this paper, we propose a framework for enhancing the role-playing capabilities of LLMs by integrating personality traits to customize AI characters. Specifically, we collected the last 200 posts from 500 users on social media platforms. Inspired by Peters & Matz (2024b), we improved the prompts for inferring personality traits to obtain 35 dimensions continuous scores (as each main dimension has 6 sub-dimensions as mentioned above) and text format reports. Then, we simulate a profile for each user using the LLMs. In order to increase the generalization capacity of the model, we give each post the motivation and potential knowledge. In short, the inputs give the model all the reasonably necessary preconditions to generate the content. It is worth noting that our model has the capability of multi-modal perception and generation, since the media resources for each post are captioned accompanies the text perform the above inputs and outputs. For the coarse-grained model, we train the model by splicing personality trait reports into queries. For the fine-grained model, we design a score interpreter to transcribe numerical input into text description. To assess the effectiveness of the model and training method, we construct a multi-scale benchmark for evaluating the quality of content generated by the personalized AI characters. Experiments show that our model is trained to make connections to user profiles and personality traits and exhibits high personalized performance. Our work can be summarized as follows: • We propose Orca, a framework for data processing and training LLMs of custom characters by integrating personality traits, accompanying products include instruction prompt (PCIP) and dataset, dubbed OrcaData. • We propose PTIT and PSIT, two approaches for modeling coarse and fine-grained fusion of personality trait features, and has considerably improved the quality of generate content. • We propose OrcaBench, a benchmark for multi-scale assessment of the quality of content generated by social AI characters."
https://arxiv.org/html/2411.09978v1,HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of Historical Texts - A Case Application of Yantie Lun,"This paper proposes HistoLens, a multi-layered analysis framework for historical texts based on Large Language Models (LLMs). Using the important Western Han dynasty text ""Yantie Lun"" as a case study, we demonstrate the framework’s potential applications in historical research and education. HistoLens integrates NLP technology (especially LLMs), including named entity recognition, knowledge graph construction, and geographic information visualization. The paper showcases how HistoLens explores Western Han culture in ""Yantie Lun"" through multi-dimensional, visual, and quantitative methods, focusing particularly on the influence of Confucian and Legalist thoughts on political, economic, military, and ethnic. We also demonstrate how HistoLens constructs a machine teaching scenario using LLMs for explainable analysis, based on a dataset of Confucian and Legalist ideas extracted with LLM assistance. This approach offers novel and diverse perspectives for studying historical texts like ""Yantie Lun"" and provides new auxiliary tools for history education. The framework aims to equip historians and learners with LLM-assisted tools to facilitate in-depth, multi-layered analysis of historical texts and foster innovation in historical education.","Traditional historical research often relies on qualitative methods that depend on the subjective judgments and experiences of scholars Feng (2023) and requires substantial time for reading historical texts. To address these challenges, this study proposes an interdisciplinary analysis framework, HistoLens, which combines natural language processing technologies, including Large Language Models (LLMs), to integrate qualitative and quantitative methods in historical research Feng (2023). This approach not only allows for drawing objective and verifiable conclusions but also enables rapid processing of large volumes of historical text data, presenting knowledge through diverse charts and graphs to offer new insights and perspectives Duan et al. (2023). LLMs, which are trained on vast amounts of textual data, demonstrate powerful capabilities in understanding and generating text across various languages Litaina et al. (2024), including classical Chinese Yu et al. (2024); Chen et al. (2024); Zhao et al. (2024). The use of LLMs to assist in the analysis of historical texts has become a novel and effective approach for historians. With well-crafted prompts Ozdemir (2023); Qin and Eisner (2021), historians can adeptly utilize digital tools for rapid and multifaceted textual analysis. The LLM systems utilized in this study include Claude 3.5 Sonnet Anthropic (2024), GPT-4o mini OpenAI (2024), as well as moonshot-v1111https://kimi.moonshot.cn/ and ERNIE 3.5222https://yiyan.baidu.com/. The LLM-powered historical text analysis framework includes the following main steps: Step 1. thematic word frequency analysis: Tokenization, removal of stop words, and statistical analysis of key thematic word frequencies and their distribution in the original text. Step 2. LLM-based named entity and relationship recognition: Identifying named entities such as individuals and places and extracting relationships between them by using LLM. Step 3. knowledge graph construction: Building knowledge graphs based on entity and relationship information extracted by LLM. Step 4. spatiotemporal analysis: Combining Geographic Information System (GIS) technology to visualize geographical distributions and time series in historical texts. Step 5. ideological school analysis: Constructing text datasets containing historical thoughts, annotated with LLM assistance. Step 6. machine teaching scenario: Based on the above ideological text datasets, using LLMs’ text classification explanation capabilities to teach learners key features of thoughts in historical texts. In this paper, we use ""Yantie Lun"" Gale (2022) as an example to demonstrate the application of HistoLens in analyzing Confucian and Legalist ideological cultures, which to a certain extent determined the politics, economy, and military affairs of the Western Han period. ""Yantie Lun"" which records the Salt and Iron Conference held in 81 BCE Liu (2021), is a crucial historical source for understanding the politics, economy, and ideologies of the mid-Western Han period. Through our proposed HistoLens, we explore and verify a range of cultural and historical viewpoints and phenomena in a multi-layered, visual, and quantitative manner, and offer new perspectives and vitality to the study of Western Han history, and construct a novel Confucianism and Legalism dataset while making a novel machine teaching scenario via explainable analysis. Details of the Salt and Iron Conference, Legalism and Confucianism are shown in appendix. The code and data are available. (a) (b) Figure 1: (a) Confucius. (b) A photo of the Yantie Lun. HistoLens is not limited to analyzing ""Yantie Lun"" but provides a generalized, universal LLM-assisted analysis framework for the study and learning of other historical texts. HistoLens offers new tools and methods for historical research and learning, enabling scholars to process large volumes of historical documents more rapidly and comprehensively, discovering new historical patterns and insights through multi-layered analysis."
https://arxiv.org/html/2411.09972v1,Large Language Models as User-Agents for Evaluating Task-Oriented-Dialogue Systems,"Traditionally, offline datasets have been used to evaluate task-oriented dialogue (TOD) models. These datasets lack context awareness, making them suboptimal benchmarks for conversational systems. In contrast, user-agents, which are context-aware, can simulate the variability and unpredictability of human conversations, making them better alternatives as evaluators. Prior research has utilized large language models (LLMs) to develop user-agents. Our work builds upon this by using LLMs to create user-agents for the evaluation of TOD systems. This involves prompting an LLM, using in-context examples as guidance, and tracking the user-goal state. Our evaluation of diversity and task completion metrics for the user-agents shows improved performance with the use of better prompts. Additionally, we propose methodologies for the automatic evaluation of TOD models within this dynamic framework. We make our code publicly available 111https://github.com/TaahaKazi/user-agent","Task-oriented dialogue (TOD) systems are designed to assist users in completing specific tasks or goals through turns of natural language interactions [1, 2, 3]. These systems are typically built to guide users through a series of steps to accomplish a particular objective, such as booking a flight, ordering food, or scheduling an appointment. Along with the emergence of large language models (LLMs) [4, 5, 6, 7, 8], TOD systems have been fundamentally boosted by the complex reasoning/understanding ability and the domain adaptation power of LLMs. The functionality as well as the design concepts of TOD systems have also been tightly integrated into the training and grounding of LLMs. From the real-world deployment side, at least tens of LLMs have been developed and integrated into hundreds of downstream tasks, for example, the travel planning LLM agents [9]. However, with such a large number of downstream application scenarios, the capacities of traditional benchmark-based TOD system evaluations are falling far behind. This essentially indicates that a gap exists between the TOD system evaluations and the TOD system deployments. There are dataset-based evaluation methods where the inputs such as the dialogue schema or the dialogue history are fixed and the evaluation based on the expected output is rigid, which means the evaluation systems cannot evaluate multiple turns of utterances from TOD systems coherently [10]. This, however, generally leads to a policy mismatch between the evaluation systems and the TOD systems. There are also traditional interactive evaluation methods that generally result in evaluation simulators in pursuit of maximal information exchange [11]. Existing LLM-based evaluation methods, however, lack of emphasis on modeling more complex user behaviors [11]. To tackle the above issues, in this work, we set up our goal as building an automated evaluation system (a user simulator and an evaluator agent) for TOD systems. Specifically, given any TOD system, our user simulator agent will simulate conversations with the system given a set of initial user goals and the evaluator agent will output several metric scores gauging the comprehensive performance of TOD systems being evaluated. Figure 1 shows our proposed framework that involves the interactions of the user simulator with the TOD system in evaluation and the evaluation module. The user simulator can state the user goal and associated information, discuss options provided as solutions by the TOD system, and negotiate to find options that match user needs. We follow several principles for our simulator design: • The user simulator should be plug and play, which means it does not need scenario-specific annotations to fine-tune the simulator and it should be adapteble to different TOD systems or different evaluation domains. • The user simulator should be rich in linguistic diversity. • The user simulator should be able to model complex user behaviors. In this work, LLMs are used by simulators to evaluate TOD systems. Given an initial user goal, the simulator prompts the LLM to generate responses that interact with TOD systems to achieve the given goal. We propose three prompting strategies: the Vanilla Prompt with straightforward instructions and in-context examples, the Thought Prompt incorporating reasoning steps via Chain-of-Thought, and the User State Tracking Prompt, which updates a user-state tracking dictionary to avoid premature conversation ending or looping. We use multiple metrics to assess the TOD system’s performance, including task completion, naturalness, coherence, and dialogue-level diversity; with task completion evaluated using GPT-4. This approach addresses the limitations of traditional evaluation methods in a dynamic setting. In summary, our work introduces an automated framework using LLMs as user simulators to evaluate TOD systems. This framework enhances the evaluation process, making it more reflective of real-world interactions."
https://arxiv.org/html/2411.09947v1,LoRA-LiteE: A Computationally Efficient Framework for Chatbot Preference-Tuning,"Effective preference tuning is pivotal in aligning chatbot responses with human expectations, enhancing user satisfaction and engagement. Traditional approaches, notably Reinforcement Learning from Human Feedback (RLHF) as employed in advanced models like GPT-4, have demonstrated considerable success in this domain. However, RLHF methods are often computationally intensive and resource-demanding, limiting their scalability and accessibility for broader applications. To address these challenges, this study introduces LoRA-Lite Ensemble (LoRA-LiteE), an innovative framework that combines Supervised Fine-tuning (SFT) with Low-Rank Adaptation (LoRA) and Ensemble Learning techniques to effectively aggregate predictions of lightweight models, which aim to achieve a balance between the performance and computational cost. Utilizing the Chatbot Arena benchmark dataset, we conduct a comprehensive comparative analysis among our LoRA-LiteE model, corresponding base models at different scales, and GPT-4 trained with RLHF. Our empirical results demonstrate that the proposed LoRA-LiteE model achieves comparable performance to un-finetuned GPT-4 and outperforms the single larger-scale models under limited resource constraints. These findings highlight that our LoRA-LiteE provides a feasible and efficient methodology for human preference prediction in chatbot systems, enhancing scalability and accessibility, and thereby broadening the applicability of preference-tuned chatbots in resource-constrained environments.","In recent years, chatbots have become integral to various industries, including customer service, healthcare, education, and entertainment [1, 2, 3]. These conversational agents serve as the first point of contact between organizations and users, providing information, resolving queries, and facilitating seamless interactions. The effectiveness of a chatbot is largely determined by its ability to generate responses that are not only contextually relevant but also aligned with human preferences and expectations [4]. This alignment, known as preference tuning, is pivotal in enhancing user satisfaction and engagement, thereby determining the overall success and adoption of chatbot technologies [5, 6]. Traditional preference tuning methods have employed various machine-learning techniques to align chatbot responses with human expectations. Early approaches, such as supervised fine-tuning and imitation learning, involved training models on large conversational datasets to replicate desired response patterns [7]. While these methods have been effective, they often lack adaptability and a nuanced understanding of complex scenarios [8]. In contrast, Reinforcement Learning from Human Feedback (RLHF) has emerged as the most advanced methodology for preference tuning. RLHF leverages feedback from human evaluators, enabling chatbots to iteratively refine their responses for better alignment with user preferences [9, 10]. This approach has been successfully implemented in state-of-the-art language models like OpenAI’s GPT-4, which demonstrates exceptional capabilities in generating coherent, contextually appropriate, and preference-aligned responses [11]. The success of RLHF in models such as GPT-4 underscores its potential to create highly effective and user-centric chatbot systems. Despite its successes, RLHF requires substantial computational and human resources, limiting the feasibility of deploying RLHF-based models in smaller organizations or for applications with limited budgets. To address these challenges, this study introduces LoRA-Lite Ensemble (LoRA-LiteE), an innovative framework that integrates Low-Rank Adaptation (LoRA) with ensemble learning to fine-tune lightweight chatbot models. Our approach offers several innovations: First, through parameter-efficient fine-tuning techniques such as LoRA, we significantly reduce the number of trainable parameters, enabling model training with limited computational resources [12]. Second, we adopt a multi-model ensemble strategy, leveraging the complementary advantages of different models to enhance overall system performance. Finally, we design targeted training strategies that optimize training efficiency while maintaining model performance. This integrated approach can not only alleviate the resource demands associated with RLHF but also improve the scalability and accessibility of preference-tuned chatbots, making advanced conversational agents feasible for a broader range of applications and organizations. To evaluate the effectiveness of LoRA-LiteE, we utilize the Chatbot Arena benchmark dataset, a comprehensive collection of conversational interactions designed to assess chatbot performance across various tasks [13]. Our experiments include LoRA-LiteE employing Llama-3-8b and Gemma-2-9b, single larger-scale LoRA-finetuned Llama-3-70b and Gemma-2-27b, and OpenAI’s GPT-4 [14, 15, 11]. Our results reveal that our LoRA-LiteE model outperforms the single fine-tuned llama-3-8b and gemma-2-9b models and achieves comparable accuracy to the un-finetuned GPT-4 in specific tasks, demonstrating LoRA-LiteE’s capability to effectively align chatbot responses with human preferences. Notably, while the larger-scale models (Llama-3-70b and Gemma-2-27b) exhibit superior performance in environments with ample computational resources, their advantages diminish under limited resource constraints. As training time and available resources decrease, LoRA-LiteE begins to outperform these larger models, offering substantial computational efficiency and resource savings. These findings highlight the potential of our LoRA-LiteE architecture as an efficient and scalable alternative to RLHF methodologies, enhancing the accessibility and practicality of preference-tuned chatbots under resource constraints."
https://arxiv.org/html/2411.09944v1,"SlimLM: An Efficient Small Language Model for 
On-Device Document Assistance","While small language models (SLMs) show promises for mobile deployment, their real-world performance and applications on smartphones remains underexplored. We present SlimLM, a series of SLMs optimized for document assistance tasks on mobile devices. Through extensive experiments on a Samsung Galaxy S24, we identify the optimal trade-offs between model size (ranging from 125M to 7B parameters), context length, and inference time for efficient on-device processing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on DocAssist, our constructed dataset for summarization, question answering and suggestion tasks. Our smallest model demonstrates efficient performance on S24, while larger variants offer enhanced capabilities within mobile constraints. We evaluate SlimLM against existing SLMs, showing comparable or superior performance and offering a benchmark for future research in on-device language models. We also provide an Android application, offering practical insights into SLM deployment. Our findings provide valuable insights and illuminate the capabilities of running advanced language models on high-end smartphones, potentially reducing server costs and enhancing privacy through on-device processing.","The evolution of language models is diverging along two paths: large language models (LLMs) pushing the boundaries of artificial general intelligence in data centers Chowdhery et al. (2022); OpenAI (2023a); Team et al. (2023); Touvron et al. (2023a, b); Alibaba (2023.11, 2024.09), and small language models (SLMs) designed for resource-efficient deployment on edge devices like smartphones Meituan (2023.12); MBZUAI (2024.02); Zhang et al. (2024); Liu et al. (2024). While LLMs have attracted significant attention, the practical implementation and performance of SLMs on real mobile devices remain understudied, despite their growing importance in consumer technology. Recent developments, such as Qwen-2 Alibaba (2024.06), SmolLM HuggingFace (2024.07), Gemini Nano Reid et al. (2024), Apple Intelligence Apple (2024.09) or LLaMA-3.2 Meta (2024.09) underscore the increasing relevance of SLMs in mobile applications. However, a comprehensive understanding of how these models perform on high-end smartphones is lacking. Unlike previous works that primarily focus on developing smaller models without extensive real-device testing Meituan (2023.12); MBZUAI (2024.02); Zhang et al. (2024); Liu et al. (2024), our approach aims to bridge that gap by presenting an in-depth study of SLM development and deployment on a Samsung Galaxy S24 (also known as S24), focusing on three document assistance tasks: summarization (SUMM), question suggestion (QS), and question answering (QA). By enabling efficient on-device document processing, our approach has the potential to significantly reduce server costs associated with API calls to cloud-based services, while enhancing user privacy. We address critical questions about optimal model size, maximum context length, inference latency, memory constraints, and performance trade-offs on mobile devices. To answer these questions, we introduce SlimLM, a series of small language models specifically designed and optimized for mobile deployment. SlimLM is pretrained on the SlimPajama-627B Soboleva et al. (2023) and finetuned on DocAssist, our specialized dataset constructed based on \sim83K documents for document assistance. Our models range from 125M to 1B parameters, allowing us to explore the full spectrum of what is possible on current mobile hardware. Our results show that SlimLM models perform comparably or even better than existing SLMs of similar sizes across standard metrics such as BLEU Papineni et al. (2002), ROUGE Lin (2004), Semantic Textual Similarity (STS), Self-BLEU Zhu et al. (2018) for text diversity and GEval Liu et al. (2023). The smallest model SlimLM-125M demonstrates efficient performance on S24, making it suitable for widespread deployment. Larger variants, up to 1B parameters, offer enhanced capabilities while still operating within mobile constraints. To demonstrate real-world applicability, we develop an Android application showcasing SlimLM’s document assistance capabilities111Code and data will be released at anonymous.slimlm (Sec. 4). Our key contributions are: 1. We identify the sweet spot between model size, inference time, as well as the longest context length that can be efficiently processed on the latest Samsung device S24 (Sec. 2.1). 2. We construct DocAssist, a specialized dataset for finetuning models on three critical document assistance tasks (Sec. 2.2). 3. We propose a set of small language models pretrained on SlimPajama with 627B tokens and finetuned on the DocAssist dataset (Sec. 2.3). 4. SlimLM outperforms or performs comparably to existing SLMs of similar sizes while handling a maximum of 800 context tokens (Sec. 3)."
https://arxiv.org/html/2411.09884v1,Research on Domain-Specific Chinese Spelling Correction Method Based on Plugin Extension Modules,"This paper proposes a Chinese spelling correction method based on plugin extension modules, aimed at addressing the limitations of existing models in handling domain-specific texts. Traditional Chinese spelling correction models are typically trained on general-domain datasets, resulting in poor performance when encountering specialized terminology in domain-specific texts. To address this issue, we design an extension module that learns the features of domain-specific terminology, thereby enhancing the model’s correction capabilities within specific domains. This extension module can provide domain knowledge to the model without compromising its general spelling correction performance, thus improving its accuracy in specialized fields. Experimental results demonstrate that after integrating extension modules for medical, legal, and official document domains, the model’s correction performance is significantly improved compared to the baseline model without any extension modules.","Existing Chinese spelling correction datasets are mostly focused on open-domain content, such as Wang271K [Wang et al.(2018)Wang, Song, Li, Han, and Zhang], SIGHAN [Tseng et al.(2015)Tseng, Lee, Chang, and Chen], and CSCD-IME [Hu et al.(2024)Hu, Meng, and Zhou]. Models trained on these datasets typically demonstrate strong correction capabilities on general-domain Chinese texts. However, in real-world applications, texts that require correction often originate from specific domains and contain a substantial amount of domain-specific terminology. Due to the lack of these specialized terms in training datasets, current models generally struggle to perform accurate corrections in such cases. To address this issue, some research efforts have designed specific correction solutions tailored to different domains. These existing methods can be broadly categorized into two approaches: The first approach involves building correction systems by targeting unique features of the desired domain. For instance, [Jiang et al.(2022b)Jiang, Ye, Ou, Zhao, Zheng, Liu, Li, Liu, Yang, and Zheng] introduced the MCSCSet dataset for the medical question-answering domain and developed a domain-specific spelling correction model, MedSpell. Although this approach shows good performance within its target domain, it has limited transferability to other domains. The second approach focuses on fine-tuning existing open-domain models using domain-specific spelling correction datasets to enhance their performance in the target domain. For example, Hongqiu Wu et al. proposed the LEMON dataset [Wu et al.(2023)Wu, Zhang, Zhang, and Zhao], covering six distinct domains, and explored the performance of existing correction models on this dataset. [Lv et al.(2023b)Lv, Cao, Geng, Ai, Yan, and Fu] introduced the ECSpell model, applying it to datasets from three different domains. While this approach extends model capabilities across multiple domains, it requires collecting dedicated correction datasets and retraining the model for each target domain. Additionally, this method often faces the issue of ""catastrophic forgetting,"" where the model loses its original correction capabilities for open-domain texts after fine-tuning on a specific domain. To overcome the limitations of existing models in correcting domain-specific Chinese texts, this paper proposes a plugin-based extension module that enhances the model’s vocabulary, enabling it to correct domain-specific terminology while preserving its original correction capabilities. The proposed method is compatible with various neural network-based correction models. Expanding to new domains only requires providing a list of domain-specific terms and performing a small amount of fine-tuning on the extension module. Furthermore, to address the common issue of over-correction in existing models, we propose a dual-prediction change analysis algorithm. This algorithm performs two rounds of predictions and analyzes the changes in prediction results for each character to determine whether it has been over-corrected, thereby reducing the likelihood of over-corrections."
https://arxiv.org/html/2411.09853v1,KULCQ: An Unsupervised Keyword-based Utterance Level Clustering Quality Metric,"Intent discovery is crucial for both building new conversational agents and improving existing ones. While several approaches have been proposed for intent discovery, most rely on clustering to group similar utterances together. Traditional evaluation of these utterance clusters requires intent labels for each utterance, limiting scalability. Although some clustering quality metrics exist that do not require labeled data, they focus solely on cluster geometry while ignoring the linguistic nuances present in conversational transcripts. In this paper, we introduce Keyword-based Utterance Level Clustering Quality (KULCQ), an unsupervised metric that leverages keyword analysis to evaluate clustering quality. We demonstrate KULCQ’s effectiveness by comparing it with existing unsupervised clustering metrics and validate its performance through comprehensive ablation studies. Our results show that KULCQ better captures semantic relationships in conversational data while maintaining consistency with geometric clustering principles.","Over the past few years, there has been a proliferation of frameworks to develop task-oriented dialog systems. Most such systems utilize intent classification to discover the intent of a user. Since intents are specific to the domain, they vary significantly depending on the tasks that they are configured for. Thus, it is crucial to perform open intent discovery Zhang et al. (2021) in order to build task-oriented dialog systems. While there are metrics like Normalized Mutual Information (NMI) McDaid et al. (2011) and Adjusted Rand Index(ARI) to evaluate the performance of open intent discovery, they require annotated intent labels. This is hard to scale and slows down the process of building task-oriented dialog systems. There is a need to measure the goodness of the intent clusters obtained from open intent discovery in an unsupervised way. Clustering textual data is used in various fields such as Computational Social Science Hox (2017), Conversational AI Cuayáhuitl et al. (2019), and Recommendation Systems Das et al. (2014). Intent detection is one of the most crucial tasks in conversational AI, implemented via classification or clustering. Classification being supervised, requires labels, making clustering a natural choice for discovery and scalablility. Various unsupervised clustering evaluation metrics such as the Silhouette coefficient Rousseeuw (1987), Calinski-Harabasz index Caliński and JA (1974), Davies-Bouldin index Davies and Bouldin (1979), and Dunn index Dunn (1973) take in data belonging to any domain and apply the same algorithm each time, as long as there is a similarity measure defined between pairs of data points. These metrics only focus on the geometry of the clusters, and are agnostic to the domain-specific nuances of the data. Therefore, it is important to have metrics that evaluate clusters by leveraging aspects that are important to the domain the data belongs to. In this paper we introduce a Keyword-based Utterance Level Clustering Quality (KULCQ) measure which is specific to analysing clusters of conversation data (Section 2.2). The motivation behind leveraging keywords for conversational data is to capture similar intents across utterances despite differences in sentence formation and helping words used - which may lead to a big difference in the sentence embeddings. For example: ""How can I get a taxi from A to B?"", and ""I want a taxi from A to B, can you help?"", may differ in representation even though taxi, A, and B are the most important words in both cases. In our experiments we focus on evaluating the clustering of intents from user query utterances in dialogues between customer support agents and customers. We show in Section 3.1 that our metric can be used as a universal clustering metric, because it follows the trend of a standard metric in an adverse scenario, and correctly captures the general quality of the clusters. In the Sections 3.2 and 3.3 of the paper, we highlight the shortcomings of a standard domain-agnostic metric such as Silhouette coefficient in cases specific to conversational data. Our proposed metric handles these cases and provides an advantage in accurately measuring the clustering quality of conversational data."
https://arxiv.org/html/2411.09834v1,A Benchmark for Long-Form Medical Question Answering,"There is a lack of benchmarks for evaluating large language models (LLMs) in long-form medical question answering (QA). Most existing medical QA evaluation benchmarks focus on automatic metrics and multiple-choice questions. While valuable, these benchmarks fail to fully capture or assess the complexities of real-world clinical applications where LLMs are being deployed. Furthermore, existing studies on evaluating long-form answer generation in medical QA are primarily closed-source, lacking access to human medical expert annotations, which makes it difficult to reproduce results and enhance existing baselines. In this work, we introduce a new publicly available benchmark featuring real-world consumer medical questions with long-form answer evaluations annotated by medical doctors. We performed pairwise comparisons of responses from various open and closed-source medical and general-purpose LLMs based on criteria such as correctness, helpfulness, harmfulness, and bias. Additionally, we performed a comprehensive LLM-as-a-judge analysis to study the alignment between human judgments and LLMs. Our preliminary results highlight the strong potential of open LLMs in medical QA compared to leading closed models.","The majority of existing LLM evaluation benchmarks in medical question answering (QA) have focused on automatic metrics and multiple-choice questions [14, 9, 19, 22]. Although valuable, such metrics and question formats fall short of reflecting the realistic settings of real-world clinical scenarios [24, 19] and do not fully assess or capture the nuances and factual accuracy of LLMs in the medical domain [9, 23, 25]. Additionally, there are concerns about the potential leakage of well-known benchmarks into the training data of LLMs that are subsequently evaluated on those same benchmarks [3]. Furthermore, benchmarks that have not leaked may contain label errors or be outdated [18], leading to flawed and unrealistic evaluations. Moreover, the limited research that has explored human evaluation of long-form medical QA has not made the associated labels publicly available, hindering reproducibility and the ability to study human annotations for insights to inform future work. Figure 1: Overview of our benchmark creation process To address these challenges, we introduce a new publicly available benchmark of real-world consumer medical questions with long-form answer evaluation, annotated by medical doctors. An overview of the process of building our benchmark is shown in Figure 1. Our contributions are as follows: • We develop a publicly available benchmark for long-form medical QA, based on real-world consumer health and medical questions. • We release the medical doctor annotations to the research community, along with our growing collection of real-world consumer medical questions. • We conduct a comprehensive analysis comparing human experts and LLM-as-a-judge for evaluating long-form answer generation in medical questions."
https://arxiv.org/html/2411.10414v1,Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations,"We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for human-AI conversations that involves image understanding: it can be used to safeguard content for both mutimodal LLM inputs (prompt classification) and outputs (response classification). Unlike the previous text-only Llama Guard versions (Inan et al., 2023; Llama Team, 2024b, a), it is specifically designed to support image reasoning use cases and is optimized to detect harmful multimodal (text and image) prompts and text responses to these prompts. Llama Guard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong performance on the internal benchmarks using the MLCommons taxonomy. We also test its robustness against adversarial attacks. We believe that Llama Guard 3 Vision serves as a good starting point to build more capable and robust content moderation tools for human-AI conversation with multimodal capabilities.","The past few years have witnessed an unprecedented improvement in the capabilities of Large Language Models (LLMs), driven by the success in scaling up autoregressive language modeling in terms of data, model size, and the amount of compute used for training (Kaplan et al., 2020). LLMs have demonstrated exceptional linguistic abilities (Brown, 2020; Achiam et al., 2023), general tool use (Schick et al., 2024; Cai et al., 2023), and commonsense reasoning (Wei et al., 2022; OpenAI, 2024), among other impressive capabilities. The success of LLMs as general-purpose assistants motivates research and development to extend instruction-tuning to the vision-language multimodal space (Liu et al., 2023; Gemini Team, 2023). These vision-language multimodal models, which can process and generate both text and images, also achieve human-expert performance on a wide range of tasks, such as (document) visual question answering (Antol et al., 2015; Mathew et al., 2021), image captioning (Lin et al., 2014), and image-text retrieval (Plummer et al., 2015). While these vision-language multimodal models hold tremendous promise for many applications, they should be used along with proper system guardrails to ensure safe and responsible deployment, because they can generate or propagate harmful content when interacting with online users. However, most existing guardrails (Inan et al., 2023; Llama Team, 2024b, a; Yuan et al., 2024; Ghosh et al., 2024) for the interaction (e.g., conversation) between humans and AI agents are text-only: conversation data involving other modalities, such as images, cannot be used as inputs for such guardrails. This calls for a safeguard tool for classifying safety risks in prompts and responses for conversations with multimodal contents involved. In this work, we introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for human-AI conversations that involves image understanding: it can be used to safeguard content for both mutimodal LLM inputs (prompt classification) and mutimodal LLM responses (response classification). Unlike text-only Llama Guard versions (Inan et al., 2023; Llama Team, 2024b, a), it is specifically designed to support image reasoning use cases and is optimized to detect harmful multimodal (text and image) prompts and text responses to these prompts. Figure 1 gives an example of how Llama Guard 3 Vision classifies harmful content in the response classification task. Llama Guard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong performance on our internal benchmark using the 13 hazards of the MLCommons taxonomy. To better understand the adversarial robustness of Llama Guard 3 Vision, we evaluate it against two existing powerful white-box attacks (Zou et al., 2023; Madry, 2017) using both textual and visual inputs. We find that Llama Guard 3 Vision is more robust in the response classification task compared to the prompt classification task under practical threat model scenarios, since Llama Guard 3 Vision primarily relies on model response for classification while effectively ignoring prompt-based attacks. We hope our white paper can help practitioners build better customizable safeguard models for multimodal use cases, and facilitate further research and development of multimodal LLM safety. Paper Organization In Section 2, we present the hazard taxonomy and policy used to train Llama Guard 3 Vision. In Section 3, we describe the process of building Llama Guard 3 Vision. Our experiment section (Section 4) is divided into two parts: In Section 4.1, we evaluate Llama Guard 3 Vision on our internal benchmark and compare it with baselines such as GPT-4o (Achiam et al., 2023), and in Section 4.2, we present the details and results of our adversarial robustness experiments. Figure 1: Llama Guard 3 Vision classifies harmful content in the response classification task."
https://arxiv.org/html/2411.10397v1,"Features that Make a Difference: 
Leveraging Gradients for Improved Dictionary Learning","Sparse Autoencoders (SAEs) are a promising approach for extracting neural network representations by learning a sparse and overcomplete decomposition of the network’s internal activations. However, SAEs are traditionally trained considering only activation values and not the effect those activations have on downstream computations. This limits the information available to learn features, and biases the autoencoder towards neglecting features which are represented with small activation values but strongly influence model outputs. To address this, we introduce Gradient SAEs (g-SAEs), which modify the k-sparse autoencoder architecture by augmenting the TopK activation function to rely on the gradients of the input activation when selecting the k elements. For a given sparsity level, g-SAEs produce reconstructions that are more faithful to original network performance when propagated through the network. Additionally, we find evidence that g-SAEs learn latents that are on average more effective at steering models in arbitrary contexts. By considering the downstream effects of activations, our approach leverages the dual nature of neural network features as both representations, retrospectively, and actions, prospectively. While previous methods have approached the problem of feature discovery primarily focused on the former aspect, g-SAEs represent a step towards accounting for the latter as well.","Sparse Autoencoders (SAEs) have emerged as a promising method for interpreting neural networks, aiming to recover a model’s features via dictionary learning (Bricken et al., 2023; Cunningham et al., 2023; Templeton et al., 2024). While there is no universally accepted definition of features, they are generally understood to be the atomic units of language models’ computation, possessing the quality of monosemanticity as both representations and causal intermediates. The success of SAEs in identifying directions in language model activation space that are causally relevant and interpretable provides evidence that a significant fraction of the internal activations of language models are sparse, linear combinations of vectors which are each oriented in a direction corresponding to a feature of the model. (Park et al., 2024b, a). The elements of the SAE-learned dictionary corresponding to these directions we call latents. However, SAEs are trained to encode input activations such that the reconstruction loss over many tokens from a training corpus is minimized. This has raised some concerns that SAEs may not learn latents corresponding to the features of a model, but rather the features of a dataset (Braun et al., 2024; Dooms and Wilhelm, 2024). That is, SAEs might give the illusion of interpretability by learning latents based on frequent and distinct concepts in the training corpus, rather than learning latents because they correspond to features that play an important and distinct role in the model’s decision-making process. We would like a better guarantee that our interpretability tools discover the features of a model, rather than primarily reflecting the training data. Additionally, if one is interested in interpretability as a means to exert more fine-grained control over the behavior of models, it would be desirable to have interpretability tools that are biased towards uncovering the features that are most responsible for a model’s output. Templeton et al. (2024) speculate that current Large Language Models (LLMs) may represent orders of magnitude more features than the size of the dictionaries of the largest SAEs trained to date. The limited capacity of SAEs incentivizes them to neglect features that strongly affect model outputs if overlooking them usually results in small reconstruction errors (Braun et al., 2024). Figure 1: The setup of g-SAE training. Here \mathcal{L}(\mathbf{x}) is the function mapping the residual stream activation at the layer the SAE was trained on, to the predictive cross entropy loss it yields. The red dotted line denotes backpropagation. These desiderata motivate our approach, which aims to address the challenges above by connecting dictionary learning more closely with model outputs. In this paper we show: 1. Distance in LLM activation space correlates poorly with degree of downstream effect, but gradients can yield accurate first order predictions. We show that there is only a weak correlation between the norm of a perturbation vector and it’s effect on LLM downstream outputs, as measured by change in cross-entropy loss. In contrast to the norm alone, the first order approximation given by gradients of activations with respect to loss strongly predict downstream effects for perturbations of moderate size. This suggests that reconstruction loss minimization doesn’t directly incentivize the SAE to learn reconstructions that minimize difference in effect. 2. In SAE training, incorporating gradients produces improvements on various Pareto frontiers defined by popular SAE architectures, and results in more complete use of the SAE’s capacity. For a given expansion size, using a gradient-aware TopK activation function leads to improvements in the minimization of difference in model loss when reconstructed outputs are propagated downstream. It also leads to fewer permanently inactive units of the SAE hidden layer. 3. g-SAE latents provide superior steering with no cost to interpretability. g-SAEs recover latents that appear to be as interpretable as existing architectures, and on average are more influential to the specific logits they are associated with, when used as steering vectors in arbitrary contexts."
https://arxiv.org/html/2411.10329v1,"Safe Text-to-Image Generation: 
Simply Sanitize the Prompt Embedding","In recent years, text-to-image (T2I) generation models have made significant progress in generating high-quality images that align with text descriptions. However, these models also face the risk of unsafe generation, potentially producing harmful content that violates usage policies, such as explicit material. Existing safe generation methods typically focus on suppressing inappropriate content by erasing undesired concepts from visual representations, while neglecting to sanitize the textual representation. Although these methods help mitigate the risk of misuse to some extent, their robustness remains insufficient when dealing with adversarial attacks.Given that semantic consistency between input text and output image is a fundamental requirement for T2I models, we identify that textual representations (i.e., prompt embeddings) are likely the primary source of unsafe generation. To this end, we propose a vision-agnostic safe generation framework, Embedding Sanitizer (ES), which focuses on erasing inappropriate concepts from prompt embeddings and uses the sanitized embeddings to guide the model for safe generation. ES is applied to the output of the text encoder as a plug-and-play module, enabling seamless integration with different T2I models as well as other safeguards. In addition, ES’s unique scoring mechanism assigns a score to each token in the prompt to indicate its potential harmfulness, and dynamically adjusts the sanitization intensity to balance defensive performance and generation quality. Through extensive evaluation on five prompt benchmarks, our approach achieves state-of-the-art robustness by sanitizing the source (prompt embedding) of unsafe generation compared to nine baseline methods. It significantly outperforms existing safeguards in terms of interpretability and controllability while maintaining generation quality.","Diffusion models [11, 39], as the current state-of-the-art (SOTA) generative paradigm, drive the development of text-to-image (T2I) generation systems. These T2I models are guided by textual prompts and aim to generate realistic images that align with the provided text descriptions, finding widespread application in fields such as art creation [44] and graphic design [51]. Between 2022 and 2023 alone, T2I models generated over 15 billion images [6]. While T2I models demonstrate significant potential in terms of generation performance, they also face risks of unsafe generation. For instance, the BBC reported that AI-generated child sexual abuse materials were being widely sold over the internet, severely violating ethical and legal norms [5]. Therefore, ensuring that the content generated by T2I models adheres to usage policies [8, 25] is an urgent necessity. To address this issue, many commercial T2I online services have implemented various safeguards. For instance, DALL-E [30] uses a Moderation API to detect input prompts that may violate usage policies. Stable Diffusion is equipped with a SafetyChecker [31] to filter out generated images containing explicit content. However, as passive defense mechanisms, input and output moderation typically offer coarse-grained protection, which may cause T2I models to overly intercept and refuse to generate any images. In contrast, safe generation methods [17, 7, 15] focus on erasing inappropriate concepts from the model’s internal representations. These methods aim to suppress the emergence of these concepts in the generated content, rather than indiscriminately blocking inputs or outputs. For instance, SafeGen [17] fine-tunes the model to remove visual representations linked to explicit concepts, ensuring that the generated content does not reflect these concepts. Such methods offer more fine-grained protection for T2I models, enabling them to produce high-quality, safe images even when given prompts with inappropriate concepts. However, despite the fact that existing safe generation methods are capable of suppressing specific target concepts to certain extent, their robustness remains limited [43, 46]. Considering that semantic consistency between the input text and the generated image is a fundamental requirement for T2I models, which means that inappropriate prompts can be the primary source of unsafe generations by T2I models. Existing methods usually focus on fine-tuning the denoiser of a T2I model or modifying the denoising process to erase inappropriate concepts from the visual representation, neglecting the sanitization of inappropriate concepts in the prompt embeddings. This exposes their limitations, as these methods typically only suppress explicitly deleted concepts and lack sufficient generalization ability when handling synonyms or related concepts. Adversarial attacks on T2I models, such as SneakyPrompt [49], exploit this weakness by using synonym substitutions to bypass safeguards, leading to unsafe generation. Therefore, a natural question arises: Could we propose a robust safety generation framework by erasing inappropriate concepts at the source (i.e., prompt embeddings)? Figure 1: ES operates by sanitizing prompt embeddings to generate safe images and providing interpretability to indicate harmful tokens. Our work. In this paper, we provide an affirmative answer by introducing a novel, vision-agnostic safe generation framework, Embedding Sanitizer (ES). As shown in Figure 1, ES operates as a plug-and-play module applied after the text encoder, without necessitating modifications to any T2I model components. This design ensures compatibility, allowing ES to function independently or integrate seamlessly with other protective measures. ES focuses on erasing representations of inappropriate concepts from prompt embeddings, producing sanitized embeddings that guide the T2I model in generating appropriate content. A key feature of ES is its internal scoring network, which assigns a score to each token in an input prompt to gauge its potential harmfulness. For example, when processing the prompt ”A person in a birthday suit,” the token ”suit” receives a high score, indicating it as a significant contributor to inappropriate content generation. During the sanitization phase, ES dynamically adjusts the intensity of sanitization based on these scores, minimizing the impact on low-score (harmless) tokens while preserving strong sanitization effects on high-score (harmful) tokens. The scoring mechanism and adaptive sanitization function not only enhance the identification and management of inappropriate content but also provide interpretability and controllability in the generation process. ES is a plug-and-play deep learning model that accesses only the text encoder during training, without involving other T2I model components. This design supports the use of a customized objective function for efficient end-to-end training. As a vision-agnostic safe generation framework, ES does not rely on image data for training. Instead, training begins with the selection of target concepts for erasure (e.g., nude) and their corresponding anchor concepts (e.g., dressed), providing explicit learning targets for sanitization. To augment training data, we use randomly synthesized prompts to pair toxic prompts P_{t} (e.g., A nude person) with clean prompts P_{c} (e.g., A dressed person). This approach produces abundant, diverse contextual samples, enabling ES to effectively learn to identify and sanitize inappropriate content across various scenarios. During training, a mean squared error loss is employed to encourage ES to align the embedding of the toxic prompt P_{t} with that of the clean prompt P_{c}. Post-training, ES sanitizes input prompts in real-time to produce clean embeddings, ensuring that generated content adheres to usage policies. Contributions. Our main contributions are as follows: • We propose and investigate a vision-agnostic safe generation method, Embedding Sanitizer (ES), designed as a plug-and-play module that sanitizes prompt embeddings in a controllable manner and provides interpretability for inappropriate elements within prompts. This modular design ensures orthogonal compatibility with existing safeguards, allowing ES to function independently or integrate seamlessly with other protective measures. • We introduce a synthetic data-driven training strategy that supplies ES with a nearly limitless variety of contextual training data. By specifying target and anchor concepts for removal, we reformulate the sanitization task as a transformation from target to anchor concepts, enabling efficient end-to-end training of ES. • We conduct a comprehensive evaluation of ES on five prompt datasets and compare it with nine latest safeguard baselines. Results demonstrate that ES achieves SOTA robustness while maintaining content quality, validating that prompt embedding sanitization can significantly enhance resilience to adversarial attacks. To facilitate future studies, we open-source our code in the following repository: https://anonymous.4open.science/r/Embedding-Sanitizer-166E/. Ethical Considerations. For inappropriate content displayed in the manuscript, we apply masking measures to ensure that readers are not directly exposed to sensitive materials, thereby adhering to ethical standards for handling such content."
https://arxiv.org/html/2411.10323v1,"The Dawn of GUI Agent: A Preliminary 
Case Study with Claude 3.5 Computer Use","The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use’s unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project: https://github.com/showlab/computer_use_ootb.","Automating desktop tasks has become an increasingly popular area of research, driven by the need to enhance users’ productivity and accessibility across various application environments. From web navigation to professional software and even video games, users frequently encounter repetitive tasks that could benefit from automation. While large language models like GPT-4 and Qwen-2-VL have demonstrated their potential in automating tasks through general GUI interaction, the capacity of these models is still far from enough for applicable desktop task automation. Recent studies in GUI automation agents have leveraged general-purpose LLMs to interact with graphical user interfaces (GUIs) by understanding the GUI state and generating actions. However, the release of Claude 3.5 Computer Use by Anthropic marks a significant advancement in this domain, introducing the first frontier AI model to offer computer use in public beta. Unlike previous models, Claude 3.5 Computer Use offers an end-to-end solution through API calls, actions will be generated from user instruction and observed purely visual GUI state, without requiring further external knowledge such as reference plan and GUI parsing. Despite this advancement, the community needs a comprehensive analysis that evaluates the performance of API-based GUI automation models in depth. To take the first steps to explore the capacities and limitations of such models, we propose a comprehensive case study based on real-world desktop environments, encompassing a diverse range of software domains, including web navigation, professional tools, and games. The selected cases are designed to reflect the needs of various user groups, ensuring that the evaluation covers a broad spectrum of desktop automation tasks. To isolate specific aspects of the model’s capability, we evaluate the performance of API-based GUI automation models rigorously across three dimensions: • Planning: Assessing the model’s ability to generate an executable plan from the user’s query. The plan should have a correct flow, allowing the overall successful operations of the software, with each step being clear and executable. • Action: Evaluating whether the model can accurately ground the interactable GUI elements and execute the action step-by-step from the derived plan. • Critic: Measuring the model’s awareness of the changing environment, including its ability to adapt to the outcomes of its actions, such as retrying tasks if unsuccessful or terminating execution when the task is completed. To our best knowledge, this is the first comprehensive case study on Claude 3.5 Computer Use and API-based GUI automation models. We hope that our research provides the community with valuable insights into the capacities and limitations of these models. Our case study aim to lay the foundation for the continued exploration and benchmarking of API-based GUI automation. Additionally, to facilitate the community to discover and benchmark the newly released model, we also release an out-of-the-box universal framework, namely Computer Use OOTB, providing a seamless solution for users and researchers to deploy these models in local environments without the need for complex setup or configuration, aiming to improve the accessibility of GUI automation research field. Our contributions in this report are summarized as follows. • We present a comprehensive case study for Claude 3.5 Computer Use on desktop task automation, covering domains such as web search, professional software, and games, designed to reflect the needs of various user groups. • We introduce an out-of-the-box, cross-platform agent framework for deploying API-based GUI automation models, offering a universal solution for easy implementation and benchmarking. • We conduct extensive human evaluations and provide in-depth analyses, demonstrating both the advancements and limitations of the newly released API-based GUI automation model."
https://arxiv.org/html/2411.10272v1,Scaling Law for Post-training after Model Pruning,"Large language models (LLMs) based on the Transformer architecture are widely employed across various domains and tasks. However, their increasing size imposes significant hardware demands, limiting practical deployment. To mitigate this, model pruning techniques have been developed to create more efficient models while maintaining high performance. Despite this, post-training after pruning is crucial for performance recovery and can be resource-intensive. This paper investigates the post-training requirements of pruned LLMs and introduces a scaling law to determine the optimal amount of post-training data. Post-training experiments with the Llama-3 and Qwen-2.5 series models, pruned using depth pruning, width pruning, and 2:4 semi-structured pruning, show that higher pruning ratios necessitate more post-training data for performance recovery, whereas larger LLMs require less. The proposed scaling law predicts a model’s loss based on its parameter counts before and after pruning, as well as the post-training token counts. Furthermore, we find that the scaling law established from smaller LLMs can be reliably extrapolated to larger LLMs. This work provides valuable insights into the post-training of pruned LLMs and offers a practical scaling law for optimizing post-training data usage.","Large language models (LLMs) based on the Transformer architecture Vaswani et al. (2017) have attracted widespread attention and are applied across diverse domains and tasks. However, as LLMs grow in size, their hardware demands increase substantially, limiting their practical deployment in real-world scenarios. To address this challenge, researchers have focused on developing compact models through model pruning techniques Han et al. (2016) that maintain high performance while reducing hardware requirements. Model pruning can be broadly categorized into unstructured pruning Frantar and Alistarh (2023); Sun et al. (2024); Zhang et al. (2024) and structured pruning Chen et al. (2024); Hu et al. (2024); Liu et al. (2024); Muralidharan et al. (2024); Ma et al. (2023); Ashkboos et al. (2024); Men et al. (2024). Unstructured pruning removes individual elements from weight matrices, resulting in sparse matrices while maintaining model performance, but it is not hardware-friendly and does not effectively accelerate computation. Semi-structured pruning, a variant of unstructured pruning, leverages hardware support Mishra et al. (2021) to achieve acceleration, though it may cause greater performance degradation compared to unstructured pruning. Structured pruning, on the other hand, removes entire components such as attention heads or layers, effectively reducing the parameter count but often with a higher performance loss compared to other pruning techniques. Figure 1: The post-training loss curve of Llama-3.1-8B after depth pruning with different pruning ratios. Figure 2: The post-training loss curve of Llama-3.1-8B, Llama-3.2-3B and Llama-3.2-1B after 2:4 semi-structured pruning. To effectively utilize compact models obtained from semi-structured or structured pruning, post-training after pruning Ashkboos et al. (2024); Chen et al. (2024); Yang et al. (2024); Ma et al. (2023); Kim et al. (2024) is essential to mitigate performance decline. Some model pruning approaches employ continual pre-training or Parameter-Efficient Fine-Tuning (PEFT) as the post-training stage after pruning. For example, Shortened Llama Kim et al. (2024) uses 627B tokens of pre-training data for continual pre-training of the pruned LLMs, whereas LLM-Pruner Ma et al. (2023) utilizes 50,000 instruction data points for LoRA fine-tuning Hu et al. (2021). However, LoRA fine-tuning with a limited amount of instruction data is insufficient to fully restore the model’s performance, whereas continual pre-training with a large dataset can achieve full recovery but requires substantial hardware resources. Given the significant hardware demands, this raises the question: is it truly necessary to use such a vast amount of data for model recovery? LLM-Streamline Chen et al. (2024) finds that using large amounts of data for post-training only slightly improves performance compared to using a more appropriate amount. Therefore, identifying the optimal amount of post-training data is crucial for the performance recovery of pruned LLMs and is more resource-efficient. This naturally leads to the question of whether a similar scaling law, like those observed during pre-training of LLMs, could be established to predict the the optimal amount of post-training data after model pruning. In this paper, we conduct post-training experiments on the Llama-3 series Dubey et al. (2024) and Qwen-2.5 series models Team (2024), employing depth pruning, width pruning, and 2:4 semi-structured pruning. From these experiments, we establish a scaling law for post-training after model pruning. Specifically, we have two key observations. First, we find that as the pruning ratio increases, the amount of data required for adequate post-training also increases. For example, as shown in Figure 2, with depth pruning, the loss curve of Llama-3.1-8B approaches convergence after post-training on approximately 1B tokens at a 16% pruning ratio. However, at pruning ratios of 24% and 33%, the loss curve continues to decline, indicating a need for more post-training data. Second, we find that larger LLMs require less data to recover their performance after pruning, which contradicts the intuitive assumption that larger LLMs would need more data. As illustrated in Figure 2, with 2:4 semi-structured pruning, increasing model parameter counts generally results in a flatter post-training loss curve. For instance, the loss curve of Llama-3.1-8B approaches convergence with approximately 0.5B tokens, while the smaller Llama-3.2-1B’s loss curve continues to decline, suggesting that larger LLMs require less post-training data. Considering these two key observations and the established scaling laws for the pre-training of LLMs Hoffmann et al. (2022), we determine the form of the scaling law for post-training after model pruning and fit these laws to the three pruning methods across the Llama-3 series and Qwen-2.5 series models. Given the model parameter counts before and after pruning, as well as the post-training token counts, we can predict the model’s loss based on the fitted scaling law, enabling us to determine the optimal amount of post-training data. Finally, we find that the scaling law established from smaller LLMs can be reliably extrapolated to larger LLMs. suggesting that the scaling law we established has the potential to predict the rate of decline in the loss curve for even larger pruned LLMs. Overall, this paper makes the following contributions: • We identify two key observations in the post-training of LLMs pruned using depth pruning, width pruning, and 2:4 semi-structured pruning: pruned LLMs with higher pruning ratios require more post-training data to restore their performance, while larger LLMs require less post-training data to recover performance. • Based on the observations we have discovered, we determine the form of the scaling law for post-training after model pruning and fit these laws to the three pruning methods across the Llama-3 seires and Qwen-2.5 series models. This scaling law allows us to predict the model’s loss based on its parameters counts before and after pruning, as well as the post-training token counts. Additionally, we find that the scaling law established from smaller LLMs can be extrapolated to larger LLMs, suggesting it has the potential to predict the rate of decline in the loss curve for even larger LLMs."
https://arxiv.org/html/2411.10129v1,Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation,"Generating accurate code review comments remains a significant challenge due to the inherently diverse and non-unique nature of the task output. Large language models pretrained on both programming and natural language data tend to perform well in code-oriented tasks. However, large-scale pretraining is not always feasible due to its environmental impact and project-specific generalizability issues. In this work, first we fine-tune open-source Large language models (LLM) in parameter-efficient, quantized low-rank (QLoRA) fashion on consumer grade hardware to improve review comment generation. Recent studies demonstrate the efficacy of augmenting semantic metadata information into prompts to boost performance in other code-related tasks. To explore this in code review activities, we also prompt proprietary, closed-source LLMs augmenting the input code patch with function call graphs and code summaries. Both of our strategies improve the review comment generation performance, with function call graph augmented few-shot prompting on the GPT-3.5 model surpassing the pretrained baseline by around 90\% BLEU-4 score on the CodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA fine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging from 25\% to 83\% performance improvement) on this task. An additional human evaluation study further validates our experimental findings, reflecting real-world developers’ perceptions of LLM-generated code review comments based on relevant qualitative metrics.","Code review, the manual process of inspecting authored source code by fellow teammates, is a crucial part of the software development lifecycle that helps detect errors and encourages further code improvement [1]. First formalized by Fagan [2], it is a systematic and collaborative software quality assurance activity where developers check each other’s code for improvement. Code reviews not only help in identifying bugs and potential issues early in the development cycle but also enhance code readability, maintainability, and overall software quality. However, despite the numerous benefits, the traditional review process demands significant manual effort, forcing developers to spend an excessive amount of time (more than 6 hours per week) reviewing their peers’ code, as shown in [3, 4]. It is also responsible for frequent context switch of the developers from the actual tasks they are expected to focus on[5]. Hence, automating code review activities is in significant demand. One distinct task stands out in the Modern Code Review (MCR) practices: Review Comment Generation (RCG), which can help reduce the burden from a code reviewer, automatically suggesting a potential change in the code submitted for review. We focus on improving the automation performance of review comment generation task in this study. With the rapid advances in deep learning and natural language processing techniques, researchers proposed many Pretrained Language Models (PLM) on source code focusing on review tasks, notably including encoder-only BERT models [6, 7] and encoder-decoder based T5 models [8, 9, 10]. Novel pretraining and fine-tuning attempts on large-scale datasets showed promising results. However, training on such domain-specific huge datasets requires a substantial amount of costly computing resources, imposing a negative impact on the carbon footprint globally [11]. While these models can usually generalize well, they might lack deep knowledge of project specific codebases, organizational coding standards, or niche libraries. This can lead to generic or less relevant code review suggestions, missing context-specific nuances. However, decoder-only unified language models (e.g. based on GPT architecture) have shown superior performance when scaled to large parameter sizes. Also generally known as LLMs, these models can reduce the need for repetitive training while offering amazing few-shot learning capabilities [12]. This refers to prompt engineering of the model with a few similar query-response pairs, also known as Few Shot Learning. Designing efficient LLM prompts for the mentioned task yet remains less explored, motivating us toward this research direction. Apart from proprietary LLMs, there has been a lot of work going on in the open-source landscape. General purpose open-source LLMs (e.g. Llama, Mistral) when fine-tuned, show performance improvement over PLMs. LLMs further trained on code-specific data, also known as Code LLMs (e.g. Codex, Code Llama) are currently the superior options for various code-related subtasks (including code generation, code summarization) [13]. The best-performing versions of these LLMs nearly have 30-70B parameters, which is quite impossible to fit into a consumer grade GPU having around 16GB VRAM. Hence, fine-tuning the smaller versions of these LLMs (7-8B) is a promising cost-effective strategy to ensure project-specific, context-aware use cases. Parameter Efficient Fine-Tuning (PEFT) approaches (Low-Rank Adaptation, 4-bit Quantization) are found to assist in such endeavors [14]. Augmenting statically analyzed, semantic structural facts to prompt the code model proved to be beneficial in code summarization tasks [15]. Inspired by this, we propose a new methodology to design cost-effective few-shot prompts for proprietary LLMs, augmented with a programming language component- function call graph and a natural language component- code summary. We also explore further ablation studies to understand their standalone contributions for code review comment generation task. Additionally, we fine-tune open-source general-purpose LLMs and code LLMs in automating review comment generation task in a low resource-constrained setting. Specifically, we leverage the Quantized Low-Rank Adaptation (QLoRA) approach to fine-tune our models in a supervised way. To summarize, we particularly investigate the following research questions in this study: RQ1: How effective is code review comment generation using fine-tuned open-source Large Language Models? RQ2: How well do the closed-source Large Language Models perform in code review comment generation task when prompt engineered in a few-shot setting? RQ3: When incorporated into prompts, what are the impacts of function call graph and code summary in improving review comment generation performance? RQ4: How effective Large Language Models are in generating review comments from a real-world developer’s perspective? Our contributions can be summarized as follow: • Evaluating code review comment generation performance with open-source LLMs (Llama 2, Code Llama, Llama 3 series) in quantized, low-rank adapted parameter-efficient fine-tuning setup • Exploring the performance of different closed-source, proprietary LLMs (GPT 3.5-Turbo, GPT-4o, Gemini-1.0 Pro) in few-shot prompt setting without any further data augmentation • Investigating the impact of incorporating manually extracted function call graph and CodeT5 model generated code summary into the few-shot prompts to observe their impact on review comment generation performance • Manual analysis and a developer study focusing on evaluating the LLM-generated review comments based on relevant qualitative metrics • A replication package with all the scripts for data, code and result processing for our study, which can be found here. Figure 1: Overview of the methodology"
https://arxiv.org/html/2411.10115v1,Memorization in Attention-only Transformers,"Recent research has explored the memorization capacity of multi-head attention, but these findings are constrained by unrealistic limitations on the context size. We present a novel proof for language-based Transformers that extends the current hypothesis to any context size. Our approach improves upon the state-of-the-art by achieving more effective exact memorization with an attention layer, while also introducing the concept of approximate memorization of distributions. Through experimental validation, we demonstrate that our proposed bounds more accurately reflect the true memorization capacity of language models, and provide a precise comparison with prior work.","1 INTRODUCTION Modern large language models, especially Transformers, showcase great memorization capacity ([karpukhin-etal-2020-dense, roberts_how_2020]). Among recent works, researchers have shown that facts are memorized in the MLPs of a Transformer, and have even identified which MLPs ([meng2022locating, meng2023massediting, nanda_fact_2023]). However, they were not able to understand how these MLPs store information. Both exact and approximate theoretical memorization in an MLP are well documented in the literature: a ReLU MLP can memorize exactly as many real-valued label as it has neurons n ([Bubeck2020NetworkSA]), and can memorize exactly n discrete labels with only \tilde{O}(\sqrt{n}) neurons ([vardi2022on]). Contrary to the MLPs, the memorization power of multi-head attention layers has not been empirically studied. The main role of the attention layer is not viewed as remembering information but rather as moving between residual streams the information retrieved by the MLPs ([nanda_fact_2023, wang2023interpretability, variengien2024look]). For the theoretical aspect of the memorization in attention layers, there exists results on the expressivity of the attention patterns ([pmlr-v119-bhojanapalli20a]), the memorization capacity of attention layers ([mahdavi2024memorization]), and the memorization capacity of Transformers ([kim2023provable]). We will discuss related works in depth in section 6. In this article, we are interested in moving the state-of-the-art in terms of memorization capacity for the attention layer. We will thus consider the memorization capacity of an Attention-only Transformer (AoT). We need to specify what memorization capacity means. We will distinguish two types of memorization tasks, namely the association task and the distribution task. The association task, already studied in [bietti2023birth, cabannes2024scaling, kim2023provable, mahdavi2024memorization], consists of predicting a token given a sequence of tokens as input. We only require the AoT to predict the right next-token at the last position. This memorization is exact and hence, we want to know the maximal set of sequence-token associations (t_{in},t_{out}) that can be exactly memorized by an AoT. The distribution task consists of predicting the correct distribution, measured using the KL-divergence, for an input sequence of tokens. We use the KL-divergence since it is the default loss function used to train most Transformers. To our knowledge, we are the first to introduce and study this task. Memorizing distribution happens in natural language modeling: take the sentence ""Arnold Schwarzenegger was a"", it can be completed with ""actor"", ""writer"" or ""bodybuilder"". Thus, language models need to memorize not one but several correct next-tokens, each with a possibly different probability depending on the importance of the answer. Our contributions are: 1. We improve the state-of-the-art on the association task by proving that a one layer AoT with H heads each of dimension d_{h}, and an embedding dimension d can memorize Hd_{h}+d associations. In the context of language model, this improves on the previous result on the attention layer expressivity by [mahdavi2024memorization] which requires a limited context-windows and has memorization capacity of H(d_{h}-1)+1. We compare our result with other constructions using deep Transformers ([kim2023provable]) as well as MLPs memorization ([huben2023attentiononlytransformersimplementingmlps, Bubeck2020NetworkSA]). 2. We introduce the distribution task for Transformers as a way to quantify memorization when there is not a unique correct next-token. We provide upper and lower bounds for that distribution task on the error made by the best AoT. The divergence of that AoT will approximate that of a sequence encoder, which is a mapping from token sequences to logits, and has a rank constraint. 3. Finally, we prove upper bound for the divergence of sequence encoders when the target distribution is almost a look-up table. Proofs for the statements can be found in appendix with experimental details. The code-base is available at this repo."
https://arxiv.org/html/2411.10060v1,CMATH: Cross-Modality Augmented Transformer with Hierarchical Variational Distillation for Multimodal Emotion Recognition in Conversation,"Multimodal emotion recognition in conversation (MER) aims to accurately identify emotions in conversational utterances by integrating multimodal information. Previous methods usually treat multimodal information as equal quality and employ symmetric architectures to conduct multimodal fusion. However, in reality, the quality of different modalities usually varies considerably, and utilizing a symmetric architecture is difficult to accurately recognize conversational emotions when dealing with uneven modal information. Furthermore, fusing multi-modality information in a single granularity may fail to adequately integrate modal information, exacerbating the inaccuracy in emotion recognition. In this paper, we propose a novel Cross-Modality Augmented Transformer with Hierarchical Variational Distillation, called CMATH, which consists of two major components, i.e., Multimodal Interaction Fusion and Hierarchical Variational Distillation. The former is comprised of two submodules, including Modality Reconstruction and Cross-Modality Augmented Transformer (CMA-Transformer), where Modality Reconstruction focuses on obtaining high-quality compressed representation of each modality, and CMA-Transformer adopts an asymmetric fusion strategy which treats one modality as the central modality and takes others as auxiliary modalities. The latter first designs a variational fusion network to fuse the fine-grained representations learned by CMA-Transformer into a coarse-grained representations. Then, it introduces a hierarchical distillation framework to maintain the consistency between modality representations with different granularities. Experiments on the IEMOCAP and MELD datasets demonstrate that our proposed model outperforms previous state-of-the-art baselines. Implementation codes can be available at https://github.com/cjw-MER/CMATH.","Emotion Recognition in Conversation (ERC) aims to identify the corresponding emotion of each utterance in a conversation. It has become a critical problem in recent years due to its potential applications in social media analysis (Kumar et al., 2015), recommendation systems (Xu et al., 2018; Zheng et al., 2022), healthcare services (Pujol et al., 2019), and affective computing systems (Zhou et al., 2020). Early research works usually assume that the emotion of the current utterance depends on its surrounding utterances as well as the speaker’s emotional state. DialogueRNN (Majumder et al., 2019) presents a RNN-based neural network to keep track of individual party states. DialogueGCN (Ghosal et al., 2019) models inter- and self-party dependency with a graph convolutional network to capture emotional inertia of individual speakers. Although these methods demonstrates impressive performance in emotion recognition, they only focus on unique modality (i.e., textual modality). In recent years, multimodal emotion recognition in conversation has attracted considerable attention which attempts to model emotions of utterances by utilizing multiple modality information, such as textual, audio, and visual cues. Recent works mainly employ transformer mechanisms (Lian et al., 2021; Zou et al., 2023) or interaction graphs (Hu et al., 2021, 2022; Nguyen et al., 2024; Tu et al., 2024) to model interactive relationships across different modalities (Ma et al., 2024). Lian et al. (2021) apply a cross-modal transformer to capture intra- and inter-modal interactions. Zou et al. (2023) introduce a prompt transformer for cross-modal information interaction. Hu et al. (2021) propose a multimodal fused graph convolutional network to explore both multimodal and long-distance contextual information. Hu et al. (Hu et al., 2022) design a graph-based dynamic fusion paradigm to reduce redundancy and boost complementarity between different modalities. Nguyen et al. (2024) employ a directed acyclic graph to integrate multimodal information. Tu et al. (2024) develop an adaptive interactive graph network to enhance intra- and cross-modal interactions. Despite previous works have demonstrated promising results, they still suffer from several limitations: Figure 1: An example of a multimodal conversation scenario, in which the conversational contents are from textual, audio and visual modalities. Firstly, they usually treat different modalities as equivalent quality and employ symmetric architectures to fuse information in these modalities. However, in reality, modality qualities would be various, and the symmetric fusion strategy inevitable leads to inferior performance. Secondly, existing fusion methods mainly focus on fusing multimodal information in a unique information granularity by employing a unique-stage fusion strategy. They ignore the rich multi-granularity information embedded in different modalities, including the fine-grained feature representation and the coarse-grained semantic representation. To address the aforementioned issues, this paper proposes a novel Cross-Modality Augmented Transformer with Hierarchical Variational Distillation (CMATH). It consists of two major components: Multimodal Interaction Fusion module and Hierarchical Variational Distillation module. In the Multimodal Interaction Fusion module, we introduce a cross-modality augmented transformer (CMA-Transformer) for fine-grained feature-fusion. Different from previous symmetric fusion strategy, CMA-Transformer adopts an asymmetric fusion strategy, which treats one modality as the central modality and the others as auxiliary ones. To be specific, it attempts to enhance the quality of the central modality by exploring interaction information from these auxiliary modalities. We employ CMA-Transformer for each modality by taking it as the central modality, and obtain its augmented representation. In the Hierarchical Variational Distillation Module, we first introduce a variational fusion network for coarse-grained semantic information fusion, which takes the above augmented fine-grained feature representation for each modality as input and learns a multimodal Gaussian distribution. Then, we introduce a Hierarchical Distillation strategy to maintain the consistency between modality representations with different granularities to further enhance the effectiveness of the semantic distribution integration. To validate the effectiveness of the proposed model CMATH, we conducted extensive experiments on two widely used datasest, i.e., IEMOCAP and MELD. Experimental results demonstrate that CMATH significantly outperforms all baseline methods and achieves the state-of-the-art performance. Specifically, the proposed method obtains 73.90% and 73.96% in terms of two metrices ( i.e., accuracy and weighted F1-score) on the dataset IEMOCAP, with the relative performance improvement of 4.84% and 4.55% over the best performance baseline AdalGN, respectively. We further carry out experiments to analyze the effectiveness of each component of CMATH in depth in order to explore their contribution to the performance of multimodal emotion recognition in conversation. In summary, the main contributions of this work are as follows: • We propose to treat different modalities as nonequivalent quality during the fusing process and propose a novel CMA-Transformer which fuses multi-modality in an asymmetric strategy. We focus on enhancing the quality of each modality by taking it as the central modality and exploiting the remaining modalities as auxiliary information. • We introduce a hierarchical variational distillation framework, which first leverages a light variational fusion module to extract coarse-grained semantic information by fusing the Gaussion distribution of each modality, and then applies a hierarchical distillation strategy to maintain the consistency between modality representations with different information granularities. • We validate the effectiveness of our proposed method on two widely used benchmark datasets, i.e., IEMOCAP and MELD. Experimental results demonstrate its superiority over existing state-of-the-art methods."
https://arxiv.org/html/2411.10055v1,Towards unearthing neglected climate innovations from scientific literature using Large Language Models,"Climate change poses an urgent global threat, needing the rapid identification and deployment of innovative solutions. We hypothesise that many of these solutions already exist within scientific literature but remain underutilised. To address this gap, this study employs a curated dataset sourced from OpenAlex, a comprehensive repository of scientific papers. Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers on seven dimensions, covering climate change mitigation potential, stage of technological development, and readiness for deployment. The outputs of the language models are then compared with human evaluations to assess their effectiveness in identifying promising yet overlooked climate innovations. Our findings suggest that these LLM-based models can effectively augment human expertise, uncovering climate solutions that are potentially impactful but with far greater speed, throughput and consistency. Here, we focused on UK-based solutions, but the workflow is region-agnostic. This work contributes to the discovery of neglected innovations in scientific literature and demonstrates the potential of AI in enhancing climate action strategies.","The International Energy Agency (IEA) notes that about half the projected CO2 reductions that will be required to achieve Net Zero by 2050 will depend on technologies that are currently not commercially viable– highlighting the critical need for breakthrough innovations to mitigate the impacts of climate change [1]. Unlike the clear relationship between life sciences and biotech innovation, for example, there is no one academic field that dominates climate innovation. Potential solutions can emerge from disparate fields. Therefore, one likely reason for the neglect of certain climate solutions is the sheer volume and diversity of scientific literature. Traditional methods of knowledge discovery and synthesis may fail to capture innovative approaches buried in vast datasets, leading to missed opportunities for policy and technological advancement [2]. This is especially relevant for countries like the UK, which has a world-leading academic culture and made substantial investments to foster climate innovation but may still have untapped potential in its existing scientific outputs [12]. To address this challenge, we propose the use of machine learning (ML) and Large Language Models (LLMs) to systematically identify climate innovations in scientific literature. We leverage OpenAlex [8], a comprehensive open dataset of scholarly papers and comprehensive meta-data, to provide test data for analysis by state-of-the-art language models, such as GPT4-o from OpenAI. These models are prompted to evaluate paper abstracts across seven dimensions: climate emissions reduction/removal potential, technology level, deployability, market need, potential to enable subsequent innovation, mission focus of research, and neglectedness. Our hypothesis is that the research evidence base for many high-impact climate solutions is already documented in scientific papers from the UK but these have not yet been fully identified or systematically prioritised. Benchmarking the outputs of the LLMs against parallel human evaluations, we aim to assess the effectiveness of these models in finding overlooked innovations and identify any potential advantages over human reasoning. This research contributes to both the discovery of neglected climate solutions and the application of ML in enhancing domain-specific knowledge extraction, potentially accelerating climate action by uncovering actionable insights hidden within the existing literature."
https://arxiv.org/html/2411.09933v1,JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging,"With the rapid advancement of large language models (LLMs), foundational models (FMs) have seen significant advancements. Healthcare is one of the most crucial application areas for these FMs, given the significant time and effort required for physicians to analyze large volumes of patient data. Recent efforts have focused on adapting multimodal FMs to the medical domain through techniques like instruction-tuning, leading to the development of medical foundation models (MFMs). However, these approaches typically require large amounts of training data to effectively adapt models to the medical field. Moreover, most existing models are trained on English datasets, limiting their practicality in non-English-speaking regions where healthcare professionals and patients are not always fluent in English. The need for translation introduces additional costs and inefficiencies. To address these challenges, we propose a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging (JRadiEvo). This is the first attempt to extend a non-medical vision-language foundation model to the medical domain through evolutionary optimization of model merging. We successfully created a model that generates accurate Japanese reports from X-ray images using only 50 translated samples from publicly available data. This model, developed with highly efficient use of limited data, outperformed leading models from recent research trained on much larger datasets. Additionally, with only 8 billion parameters, this relatively compact foundation model can be deployed locally within hospitals, making it a practical solution for environments where APIs and other external services cannot be used due to strict privacy and security requirements.","In recent years, foundational models (FMs) have seen remarkable advancements, transforming various fields by offering more sophisticated and powerful solutions [1]. A key driver of this progress has been the rise of large language models (LLMs), which have greatly expanded the capabilities of FMs, particularly in processing and generating text with high accuracy and contextual understanding. This has sparked exponential growth in research [2], leading to the development of vision-language models that integrate visual and textual data [3, 4, 5], as well as fine-tuning approaches that enhance model performance for specific tasks [6, 7]. Healthcare is one of the most critical application areas for foundational models. The need to develop models tailored to healthcare is essential, particularly because physicians often face the challenge of reviewing large volumes of medical data, such as X-rays, which can be time-consuming and demanding. Advanced FMs can help alleviate this burden by enabling quicker and more efficient diagnoses, improving the overall effectiveness of healthcare delivery and patient outcomes. In response to this need, various FMs have been fine-tuned specifically for the healthcare domain, further enhancing their accuracy and effectiveness in clinical settings [7, 8, 9]. However, despite these advancements, several challenges remain. One significant issue is that most of the models developed so far, such as LLaVA-Med [7] and MedPaLM 2 [8], are predominantly in English, whereas many healthcare professionals and patients are not always proficient in English. For these models to be truly practical, there is a pressing need to expand their capabilities to non-English languages. Relying on a two-step process, where the model first generates output in English and then translates it, can introduce additional costs and complexity, making it less efficient and accessible. Additionally, publicly available datasets that can be used to train these models, such as MIMIC-CXR [10] and IU X-Ray [11], are overwhelmingly in English, with very few datasets available in other languages. Translating the large amounts of data needed for training into other languages with high quality is a costly and resource-intensive process. This scarcity of non-English datasets makes it difficult to develop models that can handle non-English languages. Furthermore, due to privacy concerns, it is challenging to collect and use patient data for model training, further complicating the creation of such datasets. Also, the use of large models through APIs, such as GPT-4 [12], is often impractical in healthcare settings because of the stringent privacy regulations that protect patient data, which limits the deployment of these models in real-world clinical environments. To address these challenges, this paper presents a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging [13] (JRadiEvo), a first attempt to extend a multimodal vision-language model for non-English medical text generation by utilizing evolutionary optimization of model merging [13]. JRadiEvo was developed by merging a non-medical vision-language model, medical text-to-text models, and a Japanese-language text-to-text model using an evolutionary algorithm. This innovative approach enabled the efficient creation of a Japanese radiology report generation model using only a minimal amount of Japanese-language data, addressing the critical need for non-English medical text generation in a resource-constrained environment. Below we outline our key contributions, which aim to advance the field of multimodal foundational models in healthcare: 1. Efficient use of limited non-English medical data: In the context of the difficulty in collecting non-English datasets, JRadiEvo demonstrates the ability to create a non-English medical report generation model by translating and utilizing only 50 cases from publicly available English datasets. This approach highlights the efficiency of the development process, demonstrating how a non-English medical report generation model can be created using extremely limited data and annotations. Additionally, it is noteworthy that not only was the dataset used after translation limited to 50 cases, but the entire dataset used to create JRadiEvo consisted of just 50 cases. This underscores the fact that JRadiEvo efficiently utilizes a very limited amount of data, demonstrating an effective approach to handling medical data under strict privacy and security constraints. 2. Novel application of model merging in the medical vision-language model: Traditionally, adapting models to the medical domain has relied on fine-tuning or training from scrach. To the best of our knowledge, there are no existing study of applying model merging alone to adapt a vision-language model to the medical domain. While recent research [13] has proposed using evolutionary optimization of model merging for vision-language models, this approach has been limited to natural images. To our knowledge, no prior studies have extended this technique to medical images or other domain-specific imagery beyond natural images. 3. Lightweight model for local deployment: JRadiEvo is an 8B parameter model, making it lightweight enough to be deployed on local hospital computing systems without the need for external APIs. This local deployment capability addresses critical privacy and security concerns, allowing hospitals to maintain control over patient data. Additionally, given the challenges of equipping facilities with expensive GPUs proportional to patient numbers, JRadiEvo’s compact size and low GPU memory requirements make it practical for widespread use. 4. Cost-efficient training process: JRadiEvo eliminates the need for computationally expensive backpropagation during training, enabling a far more efficient learning process compared to training a new model or fine-tuning. Additionally, by leveraging model merging instead of fine-tuning, JRadiEvo avoids the common issue of catastrophic forgetting [14, 15, 16] that often occurs during fine-tuning, allowing for a more stable and efficient development process."
https://arxiv.org/html/2411.09694v1,A Bayesian Optimization Approach to Machine Translation Reranking,"Reranking a list of candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate remains a simple and effective method for improving the overall output quality. Translation scoring models continue to grow in size, with the best models being comparable to generation models. Thus, reranking can add substantial computational cost to the translation pipeline. In this work, we pose reranking as a Bayesian optimization (BayesOpt) problem. By strategically selecting candidates to score based on a balance of exploration and exploitation, we show that it is possible to find top-scoring candidates when scoring only a fraction of the candidate list. For instance, our method achieves the same CometKiwi score using only 70 scoring evaluations compared a baseline system using 180. We present a multi-fidelity setting for BayesOpt, where the candidates are first scored with a cheaper but noisier proxy scoring model, which further improves the cost-performance tradeoff when using smaller but well-trained distilled proxy scorers.","Figure 1: A machine translation system generates candidates Aa, Bb, Cc, Dd, and Ee. The goal of BayesOpt is to find the highest scoring candidate with fewer scoring calls. An acquisition function selects the next candidate to score repeatedly until budget is reached, and the candidate with the highest score so far is returned. The quality of machine translation (MT) generation models has surged in recent years due to innovations in neural network architecture (Stahlberg, 2020), training data augmentation (Sennrich et al., 2016; Edunov et al., 2018), and scale of training data (Ott et al., 2018). At the same time, translation quality evaluation models have made similar gains for similar reasons (Rei et al., 2020; Juraska et al., 2023; Sellam et al., 2020) in addition to the new abundance of quality estimation training data (Freitag et al., 2023b). Reranking a list of translation candidates with an external evaluation model, a technique with a long history in natural langauge processing (Collins and Koo, 2005; Charniak and Johnson, 2005), thus remains a relevant method for gaining additional performance from an MT system (Fernandes et al., 2022). The term “reranking” is used to refer to any method which manipulates or replaces the scores assigned by a generation model. It typically refers to ranking an n-sized set of completed generations with an arbitrary scorer, although it can also be integrated into the generation process (Singhal et al., 2023). A wide array of scoring methods exist, including discriminative classifiers (Lee et al., 2021; Bhattacharyya et al., 2021), noisy channel decoding (Yee et al., 2019), and sampling-based Minimum Bayes Risk decoding (MBR; Eikema and Aziz, 2020). In each of these previous works, a list of candidate sequences is generated, rescored, and the highest-scoring candidate is returned. In this work, we present a general-purpose reranking algorithm for MT which reduces the computational burden of scoring all n candidates. Crucially, our aim is to support arbitrary scoring functions by using a combination of methods that have seen widespread adoption across many domains: Bayesian optimization (BayesOpt) and Gaussian processes (GP). BayesOpt (Shahriari et al., 2016) is a sequential black-box optimization method that uses the posterior mean and variance of unobserved data points to decide which points to evaluate next. GPs (Rasmussen and Williams, 2005) are flexible priors over functions with a tractable posterior that can effectively handle nonlinearity. We apply BayesOpt and GPs (BayesOpt+GP) to MT list reranking in a straightforward manner and show that it obtains close to the maximum achievable score with only a fraction of score evaluations. For example, when the maximal obtainable score across 200 randomly sampled candidates is 0.8216 CometKiwi, our method achieves 0.8210 with 70 score evaluations on average, while scoring 70 random candidates attains 0.8149, a difference of 0.0061 which is likely to be human-detectable according to (Kocmi et al., 2024b). Then, building upon previous works that use a faster but noisier proxy scoring function to prune the candidate list (Fernandes et al., 2022; Eikema and Aziz, 2022), we propose a multi-fidelity extension to BayesOpt which incorporates proxy scores to improve estimation. This is related to coarse-to-fine methods (Petrov, 2011) and model cascades (Kaynak and Alpaydin, 2000). Using this method, we find that smaller proxy scoring models distilled from the main model can further improve the performance of BayesOpt."
https://arxiv.org/html/2411.09661v1,Adaptive Decoding via Latent Preference Optimization,"During language model decoding, it is known that using higher temperature sampling gives more creative responses, while lower temperatures are more factually accurate. However, such models are commonly applied to general instruction following, which involves both creative and fact-seeking tasks, using a single fixed temperature across all examples and tokens. In this work, we introduce Adaptive Decoding, a layer added to the model to select the sampling temperature dynamically at inference time, at either the token or example level, in order to optimize performance. To learn its parameters we introduce Latent Preference Optimization (LPO) a general approach to train discrete latent variables such as choices of temperature. Our method outperforms all fixed decoding temperatures across a range of tasks that require different temperatures, including UltraFeedback, Creative Story Writing, and GSM8K.","Figure 1: The AdaptiveDecoder. This learned module is added to the standard transformer in order to select decoding hyperparameters. It consists of a new decoder head attached to the last hidden state which assigns probabilities to different hyperparameter choices per token (right) or sequence (left), and the highest probability choice is selected in each case. This allows the LLM to select low temperatures for tokens requiring factual consistency, and higher temperatures for tasks requiring creativity and diversity. For the token level adaptive decoder, a different temperature can be selected for different parts of the response given a single instruction. Large language models (LLMs) are powerful generalist models that can be used on a wide variety of tasks, ranging from fine-grained reasoning to open-ended creative writing (OpenAI, 2023; Dubey et al., 2024). Yet, early works showed that after training, the decoding method still has a large effect on performance across these tasks, leading to the proposal of various temperature sampling techniques (Holtzman et al., 2019; Welleck et al., 2019; Fan et al., 2018). In current LLMs, temperature (Ackley et al., 1985) is a key post-training parameter for generation. Temperature is used to scale the next token probabilities to be either more uniform or more sharp. Lower temperature leads to less creative, more factual generations, and higher temperature leads to more creative and original generations. Certain tasks, such as math problems or factual question answering, require the model to optimize accuracy of a single correct solution, and benefit from a low temperature, or greedy decoding (Shi et al., 2024). Others, like story generation, benefit from diverse and creative outputs, and a high decoding temperature. Intuitively, a complex task involving a number of these requirements might thus benefit from an adaptive temperature for different parts of its solution. Existing LLM evaluation pipelines often rely on a fixed choice of temperature which is therefore suboptimal on some tasks, or else manual tuning is used to control the level of diversity in LLMs, which can be time-consuming, task-specific, and limited in its ability to adapt to changing requirements and prompts. To overcome this limitation, we introduce Adaptive Decoding, which consists of a new learnable layer, as well as a novel method to train it. The new learnable neural layer, which we call the AdaptiveDecoder, is added to the final layers of the transformer architecture, enabling the LLM to dynamically adjust its output diversity based on context (i.e, the task at hand). Specifically, the AdaptiveDecoder allows the model to select an ideal temperature for decoding the next token by adding a new decoder head attached to the last hidden state. We can either apply this at the example (sequence) level where a single temperature is predicted for all generated tokens, or the token level where a new temperature is predicted for each generated token. Training the AdaptiveDecoder layer requires discrete optimization over latent variables (i.e., the choice of temperature). In order to make this feasible, we introduce a general method for such training, called Latent Preference Optimization (LPO). LPO involves sampling multiple responses from the model, where the AdaptiveDecoder layer will select temperatures (latent variables) that will affect the final token choices. Those responses are then evaluated by a reward model in order to build chosen and rejected preference pair examples. Given these pairs, we use the LPO loss to learn the optimal parameters of the AdaptiveDecoder layer for selecting temperatures during decoding. Our approach thus learns the hyperparameters of generating text across diverse tasks, allowing the model to balance exploration and exploitation in a task-aware manner. To validate our method, we experiment on a diverse set of tasks, ranging from math reasoning to creative writing and general instruction following. We show that the decoder learns to select low temperatures for reasoning tasks like math, higher temperatures for creative writing, and somewhere in between for general prompts. We find that when the training data includes all types of tasks, the model adaptively adjusts the temperature to the ideal value for each task by conditioning output token temperature choices on the input context. This enables the AdaptiveDecoder to be incorporated as part of the standard post-training pipeline to produce a model that can adjust its diversity adaptively depending on the task at hand for general instruction following, and even use different decoding parameters within a single response for the best outcome. Additionally, our proposed approach is general, and the AdaptiveDecoder could be potentially used to convert hyperparameters other than temperature (e.g. top-p, top-k) effectively into model parameters. Furthermore, we show that LPO is also a general tool to train discrete latent variables that can be used for other architecture choices that contain discrete decisions. Figure 2: Latent Preference Optimization (LPO) Training Mechanism. We demonstrate how preference pairs are constructed for training the LPO loss (we show a Sequence-Level AdaptiveDecoder, but the procedure remains the same for Token-Level). Here we have N=2 generated response samples for a single prompt, and the Reward Model (RM) scores Response1 better than Response2. Therefore, we use \tau=0.6 as the chosen temperature, and \tau=0.2 as the rejected temperature, and then apply the loss to prefer the chosen temperature over the rejected one for the given context (prompt)."
https://arxiv.org/html/2411.09613v1,"PTR: recision-Driven ool ecommendation 
for Large Language Models","By augmenting Large Language Models (LLMs) with external tools, their capacity to solve complex problems has been significantly enhanced. However, despite ongoing advancements in the parsing capabilities of LLMs, incorporating all available tools simultaneously in the prompt remains impractical due to the vast number of external tools. Consequently, it is essential to provide LLMs with a precise set of tools tailored to the specific task, considering both quantity and quality. Current tool retrieval methods primarily focus on refining the ranking list of tools and directly packaging a fixed number of top-ranked tools as the tool set. However, these approaches often fail to equip LLMs with the optimal set of tools prior to execution, since the optimal number of tools for different tasks could be different, resulting in inefficiencies such as redundant or unsuitable tools, which impede immediate access to the most relevant tools. This paper addresses the challenge of recommending precise toolsets for LLMs. We introduce the problem of tool recommendation, define its scope, and propose a novel Precision-driven Tool Recommendation (PTR) approach. PTR captures an initial, concise set of tools by leveraging historical tool bundle usage and dynamically adjusts the tool set by performing tool matching, culminating in a multi-view-based tool addition. Additionally, we present a new dataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness of tool recommendation for LLMs. We further validate our design choices through comprehensive experiments, demonstrating promising accuracy across two open benchmarks and our RecTools dataset.","Large Language Models (LLMs) have established themselves as powerful intermediaries, demonstrating remarkable impacts across a variety of downstream tasks, including text generation, code debugging, and personalized recommendations (Brown et al., 2020; Touvron et al., 2023; Nam et al., 2024; Chen et al., 2024; Zhao et al., 2024). However, as these models continue to evolve, they still struggle to solve highly complex problems due to limitations arising from their pre-training data (Mialon et al., 2023; Mallen et al., 2022; Yuan et al., 2023). To expand the potential of LLMs in managing more complex tasks efficiently, recommendations at various levels have been increasingly applied to LLMs. Typically, memory recommendations (Borgeaud et al., 2022) and knowledge-based recommendations (Gao et al., 2023; Hu et al., 2023) enhance consistency and context awareness in ongoing tasks for LLMs, while data augmentation recommendations (Xu et al., 2020) facilitate the inclusion of additional data to augment training. Furthermore, architecture recommendations (Elsken et al., 2019; Fedus et al., 2022) and prompt recommendations (Shin et al., 2020; Pryzant et al., 2023; Liu et al., 2023) optimize efficiency and generate more relevant outputs. Simultaneously, to reduce the cognitive load on LLMs and enhance their complex problem-solving capabilities by enabling actions beyond natural language processing, it is crucial to augment LLMs with recommendations of optimal external tool sets, an aspect currently lacking in existing recommendation frameworks for LLMs. Furthermore, this approach will be helpful to address the challenge of input length limitations encountered when incorporating a large number of external tools into the prompt. Providing LLMs with a precise and dynamically adaptable recommended toolset can help to enhance the effectiveness of LLM’s task-solving ability. Figure 1: Tool retrieval often provides a broad and variable number of tools with inconsistent quality, whereas tool recommendation delivers a precise, high-quality set of tools directly. Considering that the capability of LLMs to master and control external tools is instrumental in overcoming some of their fundamental weaknesses, the field of tool retrieval—which aims to identify the top-K most suitable tools for a given query from a vast set of tools—has been increasingly explored. The advent of tool retrieval (Zhuang et al., 2023; Li et al., 2023; Tang et al., 2023; Yang et al., 2024) signifies a nuanced evolution, most directly employing term-based methods (Sparck Jones, 1972; Robertson et al., 2009) or semantic-based techniques (Kong et al., 2023; Yuan et al., 2024; Gao et al., 2024). Generally, the primary objective of these methods is to refine the ranked list of tools and subsequently select a fixed number of tools from the top (top-K) (Qu et al., 2024a; Zheng et al., 2024; Qu et al., 2024b). Although such approaches have demonstrated good performance when retrieving a single tool (Patil et al., 2023; Xu et al., 2023) or a small number of tools (generally fewer than three) (Qin et al., 2023; Huang et al., 2023), they remain susceptible to under-selection or over-selection, as illustrated in Figure.1. This limitation may prevent LLMs from addressing the current query or cause them to over-interpret the query, thereby reducing the effectiveness of LLMs in solving complex problems with external tools. Additionally, the validation of these methods often relies on datasets that use a fixed number of tools for each query, meaning that during testing, the number of tools to be used is known in advance—an unrealistic scenario in practical applications where the number of tools needed can vary dynamically. Therefore, recommending a precise and dynamically adjustable set of external tools to LLMs in a single step prior to query execution is increasingly important. This approach not only enhances the thoroughness of problem-solving but also improves efficiency by reducing the need to execute additional tools. To address these limitations, we first provide a comprehensive explanation of tool recommendation and clearly define the problem, considering the lack of definition and the incompleteness of goals pursued by existing tool retrieval methods. Toward this objective, we propose PTR, a novel model-agnostic Precision-Driven Tool Recommendation approach aimed at recommending a precise tool set for LLMs prior to query execution. By leveraging historical tool bundle usage data to uncover patterns of idiomatic use and dependencies between tools, this method is structured into three main stages: Tool Bundle Acquisition, Functional Coverage Mapping, and Multi-view-based Re-ranking. Initially, using traditional pre-trained language models, we acquire semantic matching information between queries and previously used tool bundles, thereby addressing potential performance issues of these models in zero-shot scenarios for tool recommendation tasks. Subsequently, to evaluate the effectiveness of the selected tool bundle in solving the query, LLMs are prompted to match tools with the specific subproblems they can address and to identify unresolved issues. Based on this, a multi-view-based re-ranking method is employed to select tools that can help resolve the identified issues and complement the existing tool sets. More specifically, to address the unresolved issues, we construct the final ranked list by aggregating three tool lists and ranking each tool based on their frequency of occurrence. The ranked tool list, constructed from multiple views, reduces the randomness associated with selecting tools from the entire available set. Additionally, we construct a dataset, RecTools, tailored to specific queries with recommended tool sets. In contrast to previous tool datasets that standardize the number of tools used for each query (Huang et al., 2023) or employ a small number of tools (Qu et al., 2024a), our tool recommendation set incorporates varying numbers of tools for different queries, with up to ten tools used for a single query. This is achieved through an automated process in which LLMs are prompted to generate specific queries to be addressed by given tool bundles. These queries and tool bundles are subsequently evaluated by prompting LLMs to determine whether the selected tools adequately address the corresponding queries, ensuring that neither excess nor insufficient tools are utilized. Dedicated validation and deduplication steps are implemented to ensure the precision of tool usage, thereby enhancing the quality of the tool recommendation set. Furthermore, traditional retrieval metrics such as Recall (Zhu, 2004) and Normalized Discounted Cumulative Gain (NDCG) (Järvelin & Kekäläinen, 2002), fail to capture the level of precision required for effective tool recommendation. The absence of necessary tools can lead to the failure of LLMs in performing tasks, while the redundancy of tools may cause LLMs to generate unnecessary responses. This indicates that metrics focusing solely on completeness are inadequate for evaluating tool recommendation tasks. To bridge this gap, we introduce TRACC, a novel metric designed to assess tool recommendation performance, considering both the accuracy of the quantity and the quality of the recommended tools. TRACC serves as a reliable indicator of the effectiveness of tool recommendation processes. To summarize, the main contributions of this work are as follows: • We introduce tool recommendation as a novel problem, necessitating the provision of precise tool sets to LLMs for a given query. We propose PTR, an effective tool recommendation approach that leverages historical tool bundle information between queries and tools, resulting in a more accurate and comprehensive final recommended tool list. • We present a new dataset, RecTools, and an effective evaluation metric, TRACC, specifically designed to assess tool recommendation for LLMs. This not only addresses gaps in existing tool sets but also advances future research related to tool recommendation. • Extensive experiments validate the effectiveness of RecTools and demonstrate the efficacy of PTR in recommending tools for LLMs. The recommended tool sets are both comprehensive and accurate, enhancing the overall performance of LLMs in processing tasks."
https://arxiv.org/html/2411.09587v1,"BabyLM Challenge: Exploring the Effect of Variation Sets 
on Language Model Training Efficiency","While current large language models have achieved a remarkable success, their data efficiency remains a challenge to overcome. Recently it has been suggested that child-directed speech (CDS) can improve training data efficiency of modern language models based on Transformer neural networks. However, it is not yet understood which specific properties of CDS are effective for training these models. In the context of the BabyLM Challenge, we focus on Variation Sets (VSs), sets of consecutive utterances expressing a similar intent with slightly different words and structures, which are ubiquitous in CDS. To assess the impact of VSs on training data efficiency, we augment CDS data with different proportions of artificial VSs and use these datasets to train an auto-regressive model, GPT-2. We find that the best proportion of VSs depends on the evaluation benchmark: BLiMP and GLUE scores benefit from the presence of VSs, but EWOK scores do not. Additionally, the results vary depending on multiple factors such as the number of epochs and the order of utterance presentation. Taken together, these findings suggest that VSs can have a beneficial influence on language models, while leaving room for further investigation.","While current language models (LMs) demonstrate outstanding performance in a range of linguistic and reasoning tasks, there is ample scope to enhance their data efficiency. A state-of-the-art LM like Chinchilla uses as much as 1.4 trillion words for pretraining, whereas humans master their native language by hearing less than 100M words by the age of 13 (Warstadt and Bowman, 2022). Child language acquisition could provide insights into it, given that children acquire basic grammar by the age of six (Paul, 1981; Kemp et al., 2005), without as varied and abundant linguistic inputs as those given to modern LMs. Various studies argue that this highly efficient learning is aided by children’s limited cognitive abilities and specific types of inputs towards children (Newport, 1990; Fernald, 1985; Jusczyk, 1997; Rowe, 2012; Kempe et al., 2024). Inspired by this, the BabyLM Challenge aims at improving data efficiency in language models as well as providing insights into child language acquisition. It is also suggested that CDS is a preferable domain for facilitating the acquisition of linguistic knowledge compared to other domains of data. The findings of these studies include efficient pretraining without sacrificing the performance (Huebner et al., 2021), enhanced semantic extraction (You et al., 2021), and superior induction of hierarchical structure (Mueller and Linzen, 2023). While these studies suggest that CDS helps LMs learn from limited datasets, further research is needed to determine which specific properties of CDS provide an advantage to LMs. As one of such properties, some studies highlight Variation Sets (VSs), which are sets of (mostly consecutive) utterances expressing a similar intent with slight variations in the use of words and structures (Küntay and Slobin, 1996). This specific pattern is ubiquitous in CDS, but not in other speech genres. In first and second language acquisition, several studies indicate that VSs in CDS support learning of syntactic structure (Hoff-Ginsberg, 1986; Brodsky and Waterfall, 2007; Onnis et al., 2008) by maintaining children’s attention on the circumscribed topic and promoting comprehension by introducing new information (Lester et al., 2022). These findings suggest that VSs are beneficial for language learning in general and thus could enhance the learning process in LMs. In this work, we explore this hypothesis by examining the effect of VSs on language models’ data efficiency. To fully control the impact of VSs, we construct artificial VSs based on the description by Küntay and Slobin (1996), mixing it with actual CDS at various rates (0%, 20%, 40%, 60%, 80%, 100%). Then we compare the models’ accuracy on these constructed datasets and shuffled datasets on BLiMP (Warstadt et al., 2020), EWOK (Ivanova et al., 2024), and GLUE (Wang et al., 2018)."
https://arxiv.org/html/2411.09547v1,Piecing It All Together: Verifying Multi-Hop Multimodal Claims,"Existing claim verification datasets often do not require systems to perform complex reasoning or effectively interpret multimodal evidence. To address this, we introduce a new task: multi-hop multimodal claim verification. This task challenges models to reason over multiple pieces of evidence from diverse sources, including text, images, and tables, and determine whether the combined multimodal evidence supports or refutes a given claim. To study this task, we construct MMCV, a large-scale dataset comprising 16k multi-hop claims paired with multimodal evidence, generated and refined using large language models, with additional input from human feedback. We show that MMCV is challenging even for the latest state-of-the-art multimodal large language models, especially as the number of reasoning hops increases. Additionally, we establish a human performance benchmark on a subset of MMCV. We hope this dataset and its evaluation task will encourage future research in multimodal multi-hop claim verification. Data and code are available: https://mmcv-dataset.github.io/","Due to the rapid growth in AI-generated content, it is difficult for automated fact-checking systems to keep up with verifying the accuracy of claims with multimodal evidence. This challenge is further exacerbated by the recent development of diffusion models such as DALL-E Ramesh et al. (2021) and Stable Diffusion Rombach et al. (2022), which can generate realistic images from textual prompts Liu et al. (2024b). These powerful tools could enable attackers to produce misleading information Wang and Shu (2024); Pan et al. (2023c) at a low cost. Additionally, these claims often require multi-hop reasoning, where a set of connected evidence pieces leads to the final verdict of a claim Yang et al. (2018). As a result, there is a need for automated tools to assist human fact-checkers in evaluating the veracity of multimodal multi-hop claims. Figure 1: An illustration of a 2-hop claim from MMCV. To correctly verify this claim, the system must reason over both the image evidence and the table evidence. Claim verification, which involves assessing the veracity of an input claim against a collection of evidence, is a vital tool in combating the spread of misinformation Thorne and Vlachos (2018); Guo et al. (2022); Jin et al. (2022, 2023); Yang et al. (2022). However, verifying multi-hop multimodal claims introduces new challenges in both dataset construction and effective modeling. Unlike single-hop claims, which require only straightforward one-step reasoning, multi-hop claims require multiple reasoning steps to reach a final verdict. Furthermore, the inclusion of multimodal evidence requires models to understand and integrate information across various modalities, such as text, images, and tables, making it more complex to comprehend and extract relevant information. For instance, to verify the claim shown in Figure 1, a system must understand the semantic content of the image, integrate all relevant information from the table evidence, and apply multi-step reasoning to arrive at the final conclusion. In this paper, we introduce the task of multi-hop multimodal claim verification to evaluate the veracity of multi-hop claims against multimodal evidence. To study this task, we construct Multi-hop Multimodal Claim-Verification (MMCV), a dataset of 16K multi-hop claims paired with multimodal evidence that either SUPPORT or REFUTE each claim. To create the dataset, we develop a novel pipeline that uses large language models (LMMs) for data annotation, supported by human feedback. This method significantly reduces the workload on human annotators and cuts costs, while ensuring high quality and factual accuracy of the dataset. Our pipeline first uses LLMs to re-formulate multi-hop multimodal question-answer pairs into atomic multi-hop claims and generate a set of candidate claims. These candidate claims are then modified to include additional hops and refined for fluency and clarity according to a set of annotation guidelines. To ensure the accuracy of the claims, we use a Retrieval-Augmented Generation (RAG)-based validation method to verify their validity. Finally, we ask a group of human annotators to score the claims based on their fluency, correctness, and clearness, and manually rewrite the claims that are below a certain threshold. We establish performance baselines on MMCV using three state-of-the-art multimodal large language models (MLLMs) and highlight their limitations in verifying complex multimodal claims. We further demonstrate the challenges posed by the dataset, especially as the number of reasoning hops increases, by illustrating the constrained performance of various prompt techniques designed to enhance MLLMs’ reasoning capabilities, including chain-of-thought, self-ask, and symbolic-guided reasoning. Additionally, we establish a human performance benchmark on a subset of MMCV. Overall, we introduce a challenging multi-hop multimodal claim verification dataset that includes claims with up to 4 reasoning hops. These complex claims often consist of multiple sentences linked by coreference and demand evidence from various modalities, such as text, images, and tables. Table Piecing It All Together: Verifying Multi-Hop Multimodal Claims provides a comparison between MMCV and existing popular claim verification datasets. While current datasets typically focus on either multimodal claims or multi-hop textual claims, none of them incorporate multi-hop multimodal claims that necessitate cross-modal reasoning. We hope that the introduction of MMCV and its corresponding evaluation task will inspire further research in complex multi-hop multimodal reasoning for claim verification. In summary, our contributions include: • We introduce and formalize the multi-hop multimodal claim verification task. • We develop a novel pipeline that leverages LLMs for data annotation, enhanced by human feedback, to construct a benchmark dataset for multi-hop multimodal claim verification. This method significantly lowers the cost and labor required to produce a large-scale dataset. • We establish baseline performance on this task using MLLMs and human evaluation. Our analysis shows that this is a non-trivial task, with several challenges that remain to be addressed in future work. 2 Background Multimodal Claim Verification. Previous research on claim verification has primarily focused on textual data. However, with the growing recognition that misinformation often appears across multiple modalities and that multimodal misinformation is perceived as more credible and spreads faster than text-only misinformation, recent efforts have shifted toward verifying multimodal claims Akhtar et al. (2023). As a result, several multimodal claim verification datasets have been proposed including FakeNewsNet Shu et al. (2020), COSMOS Aneja et al. (2021), InfoSurgeon Fung et al. (2021), Factify Mishra et al. (2022), Fauxtography Zlatkova et al. (2019), and Mocheg Yao et al. (2023). However, to the best of our knowledge, there are no existing datasets for multi-hop multimodal claim verification, which challenges the system’s reasoning capability by requiring it to integrate and interpret multiple pieces of evidence from different modalities. Multi-hop Reasoning. Verifying complex claims often requires multi-step (multi-hop) reasoning Mavi et al. (2022), which requires combining information from multiple pieces of evidence to predict the veracity of a claim. Many recently proposed datasets are created to challenge a model’s ability to reason across multiple sentences or documents. These include MultiRC Khashabi et al. (2018), QAngaroo Welbl et al. (2018), ComplexWebQuestion Talmor and Berant (2018), HotpotQA Yang et al. (2018), and HoVer Jiang et al. (2020). In contrast to these datasets, MMCV incorporates context from various modalities, such as images and tables, further challenging the system’s ability to understand and integrate evidence from different sources. Construct Synthetic Dataset with LLMs. The emergence of advanced large language models has sparked growing interest in automating the data annotation process using LLMs Tan et al. (2024), driven by their advanced capabilities, including in-context learning Dong et al. (2022) and learning from human feedback Ouyang et al. (2022). Wang et al. (2023) propose an explain-then-generate pipeline using LLMs for iterative data synthesis, while Pace et al. (2024) combine the Best-of-N and Worst-of-N sampling strategies to introduce the West-of-N approach. With this same objective, the multi-hop claims in MMCV are created and refined by LLMs using human feedback, following guidelines and rules specifically designed to enforce a multi-hop structure within each claim. Figure 2: Overview of data collection flow chart for MMCV. In the first stage, we re-formulate question-answer pairs from MultimodalQA to generate candidate claims. In the second stage, we modify and refine the candidate claims, and apply a Retrieval-Augmented Generation (RAG)-based method to verify their correctness. In the final stage, we ask human annotators to rank the candidate claims to select the best one and label the final claims accordingly. 3 The MMCV dataset The main goal of our work is to compile a diverse and extensive collection of multi-hop claims that require joint reasoning across evidence from different modalities, such as text, tables, and images, for verification. One approach to achieving this is to transform multimodal question-answering pairs into atomic claims and refine them to incorporate additional reasoning steps, making them more natural. However, there are two major challenges in creating such a dataset: first, building a large-scale dataset is labor-intensive and costly; second, in our pilot studies, we found that simply providing instructions to crowd workers and asking them to rewrite multi-hop claims is counterproductive, as it is difficult to control quality and challenging for workers to create meaningful multi-hop claims. Instead, we develop a pipeline that leverages the emerging capabilities of large language models to generate text and learn from feedback, with human input to ensure the quality of the final output. In this approach, LLMs handle the mundane task of rewriting claims consistently according to the instructions, while human effort is significantly reduced to quality control of the final claims based on a set of guidelines. Figure 2 shows the overall workflow of our data construction pipeline, which contains three stages: LLM-Based Claim Generation (§3.1), LLM-Generated Claim Refinement (§3.2) and Claim Annotation by Human (§3.3). 3.1 Claim Generation In this stage, we leverage the in-context learning capabilities of large language models to transform question-answer pairs from the MultimodalQA dataset Talmor et al. (2021) into verifiable claims. To minimize the impact of in-context examples on the quality of the generated claims, we carefully craft a pool of 20 in-context examples and randomly select 3 for use during execution. The claims are formulated to ensure that no information is omitted from the original QA pairs and no new information is introduced. Since the claims are derived directly from the question and the correct answer, they are automatically labeled as SUPPORT. The prompt template for claim generation is listed in Appendix A.2. 3.2 Claim Refinement After generating the initial claims from the question-answer pairs, we modify and refine them to ensure they are more naturally phrased and more accurately supported by the facts. Next, we review the claims for any factual errors that may have been introduced during the modification process and make corrections as needed. Claim Modification and Refinement. To introduce additional reasoning steps to the claim candidate, we employ a modify-then-refine approach that iteratively enhances the quality of the modified claim candidate based on feedback from LLMs Pan et al. (2023a). Specifically, we begin by identifying the Wikipedia entities mentioned in the answers from the question-answer pairs. If there is only one Wikipedia entity in the answer, we leave the claim candidate unchanged. However, if there are multiple Wikipedia entities, we use the summaries of their respective Wikipedia articles as context and instruct the LLMs to modify the claim in such a way that it incorporates this contextual information to replace the entity, ensuring that the entity’s name does not appear directly in the claim. To help LLMs understand the modification task, we provide them with 3-5 randomly selected in-context examples from a pool of hand-crafted examples. After modifying the claim, we obtain feedback from LLMs regarding the fluency, correctness, and clarity of the modified claim. The criteria used for this assessment are listed in the Appendix A.2. If the feedback suggests further improvement, the claim is sent back to the modification step, incorporating the LLMs’ feedback until a certain iteration threshold is reached. If the modified claim still does not pass the quality check, it is marked for manual review and revision by human annotators. RAG-based Truthfulness Validation. Since we introduce additional contextual information from Wikipedia when modifying the claims, there is a risk that LLMs might hallucinate and produce outputs that are not faithful to the input context. To eliminate potential factual errors, we use a retrieval-augmented generation (RAG) Lewis et al. (2020)-based pipeline to retrieve the full Wikipedia articles of the relevant entities and validate the factual accuracy of the modified claims. To mitigate the impact of prompt sensitivity on the model’s output Lu et al. (2022); Sclar et al. (2023), we diversify the prompts by randomly changing their format for each verification step. For instance, instead of consistently using Is it true that {claim}?, the prompt is randomly chosen from a set of equivalent alternatives, such as Verify the following statement: {claim} or What evidence supports the claim that {claim}? 3.3 Claim Annotation At this stage, we have obtained claims that have been modified and refined by LLMs and factually validated by RAG-based pipelines. Next, we use LLMs to generate negated claims by applying a set of specific negation rules. We employ three distinct methods for generating these negated claims. For instance, given the claim, “Since its construction in 1889, the Eiffel Tower in Paris attracts millions of visitors annually.”, the results after applying the negation rules are as follows: Negation \triangleright Word substitution: The Eiffel Tower in Paris houses millions of residents annually. \triangleright Entity substitution: The Colosseum in Paris attracts millions of visitors annually. \triangleright Temporal mutation: Ever since its construction in 2050, the Eiffel Tower has been Paris’s top tourist site. Next, a group of human annotators is tasked with evaluating the claims based on three dimensions: fluency, correctness, and clarity, scoring each dimension on a scale of 1 to 5. Fluency assesses how naturally the claim reads, as outputs generated by language models can sometimes sound artificial. Correctness evaluates whether the claim is factually accurate based on the evidence. Clarity determines if the claim is easily understood, as entity substitution might make it difficult to comprehend. Once the claims are scored, the average of the fluency, correctness, and clarity scores is calculated to determine the final score for each claim. If a claim’s final score falls below a predetermined threshold, it is flagged and sent back to the annotators for manual revision. Detailed annotation guidelines are listed in Appendix A.3. 4 Dataset Analysis Dataset Statistics. MMCV contains 16,439 multi-hop multimodal claims, with their statistics detailed in Table 2. The number of hops is determined by the count of multimodal evidence associated with each claim. The dataset includes a balanced distribution of SUPPORT and REFUTE claims. Specifically, there are 6,178 1-hop claims with an average of 21.7 tokens per claim; 8,969 2-hop claims averaging 25.32 tokens per claim; 870 3-hop claims with an average of 25.44 tokens per claim; and 422 4-hop claims averaging 26.17 tokens per claim. An example from the dataset is provided in Appendix A.1. Multi-hop Reasoning Types. We provide examples of each reasoning type in Table 6. Most 1-hop and 2-hop claims require at least one supporting fact from either image or table evidence for verification. In contrast, the majority of 3-hop and 4-hop claims require evidence from all three modalities. The process of removing a bridge entity and replacing it with a relative clause or phrase significantly increases the informational load of a single hypothesis. As a result, some 3-hop and 4-hop claims are relatively longer and exhibit complex syntactic and reasoning structures. Our experimental results also indicate that the difficulty for models to verify claims escalates as the hop count increases. Data 1-hop 2-hop 3-hop 4-hop # Claims 6,178 8,969 870 422 Ave. # Tokens in Claim 21.7 25.32 25.44 26.17 Max. # Tokens in Claim 48 58 51 63 # Text Evidence 2,755 7,770 1,241 801 # Image Evidence 2,059 3,149 682 554 # Table Evidence 1,364 7,019 687 333 # SUPPORT Labels 3,118 4,514 415 184 # REFUTE Labels 3,060 4,455 455 238 # Doc/Img/Tab in Collection 5,496 11,743 1,778 787 Table 2: Dataset Statistics of MMCV. 1-hop 2-hop 3-hop 4-hop Retrieval Model P R F1 P R F1 P R F1 P R F1 Closed-book GPT-4o 76.86 72.94 71.79 67.96 63.30 60.66 62.88 58.89 56.17 67.93 62.39 61.20 Gemini 75.67 71.44 70.15 69.10 64.19 61.73 66.74 61.10 58.44 63.78 59.90 58.69 LLaVA 64.18 63.78 63.57 64.06 63.93 63.87 66.78 66.81 66.76 64.64 64.84 64.64 Open-book GPT-4o 76.95 72.95 71.78 68.03 63.24 60.53 62.67 58.78 56.08 67.75 62.46 61.35 Gemini 79.58 79.25 79.20 72.38 71.85 71.66 66.37 65.90 65.86 67.21 66.86 66.97 LLaVA 62.86 59.68 57.21 64.17 62.48 61.50 65.47 64.64 63.76 66.50 66.76 66.42 Table 3: We report the Precision, Recall, and F1 scores of various MLLMs on MMCV for zero-shot multimodal claim verification. In the closed-book setting, the model verifies the claim without access to any external knowledge sources. In the open-book setting, the model is provided with a set of gold evidence. The best-performing model for each hop is highlighted in Green for both settings. 5 Experiments and Results In this section, we discuss our experiment settings (§5.1), the experiment results (§5.2), and the error analysis (§5.3). We begin by formally defining the MMCV task below. Task Definition. The formulation of multi-hop multimodal claim verification is defined as follows: Given a claim C, and a list of multimodal evidence \mathcal{E}(C), which includes text, images, and tables, the system must reason over all the evidence and predict the label of the claim as either SUPPORT or REFUTE. 5.1 Experiment Settings As there are no existing models specifically designed for multi-hop multimodal supervised claim verification, we conduct our experiments using MLLMs. Moreover, previous studies in textual claim verification and multimodal claim verification indicate that LLMs and MLLMs can significantly enhance task performance compared to traditional supervised approaches Pan et al. (2023b); Wang and Shu (2023); Li et al. (2024); Geng et al. (2024). Furthermore, supervised methods often require extensive annotated corpora, which are difficult to acquire and limit domain transferability, as training data typically covers only a single domain. Zero-shot Claim Verification. We establish performance baselines for zero-shot multimodal claim verification using various MLLMs under two settings. In the closed-book setting, the model does not retrieve information from external knowledge sources and must rely on its parametric (internal) knowledge to verify the claim. In the open-book setting, the model is provided with a set of gold evidence. Specifically, we use the prompt from Geng et al. (2024), which extracts the models’ predictions, explanations, and confidence levels. The prompt is listed in Appendix A.2. We use macro precision, recall, and F-1 score to evaluate the model performance. MLLM. We utilize two state-of-the-art MLLMs: GPT-4o Achiam et al. (2023) and Gemini 1.5 Flash Team et al. (2023). Additionally, we evaluate the performance of an open-source MLLM, LLaVA-V1.5-7B Liu et al. (2024a), on MMCV. The temperature is set to 0.0, and the maximum number of tokens is set to 5000. Prompts for Enhanced Reasoning In addition to the prompt mentioned above, we conduct experiments using specialized prompting techniques aimed at eliciting reasoning from LLMs, such as Chain-of-Thought Wei et al. (2022) and Self-Ask Press et al. (2023). We also test symbolic-guided reasoning prompts like ProgramFC Pan et al. (2023b) and Visual Programming Gupta and Kembhavi (2023). To minimize the overall cost of the experiments, we randomly select 100 examples from each hop of the MMCV dataset for testing. The experiments are conducted using open-book setting. Human Performance To benchmark human performance on our dataset, we used the same randomly selected examples employed in the enhanced reasoning prompt experiments. We recruited four experts in automated fact-checking research to classify multihop claims from MMCV based on the provided evidence. The SMART Chew et al. (2019) framework 111https://github.com/RTIInternational/SMART was used to deploy the annotation task, and human performance was evaluated using the macro F-1 score. Figure 3: The left figure shows the confidence score distribution of GPT4-o, Gemini, and LLaVA on MMCV under both open-book and closed-book settings, categorized by the number of hops. The right figure shows their calibration curves. 5.2 Experiment Results Main Results. We report the comprehensive results of the three MLLMs on MMCV in Table 3, highlighting the best-performing models for each hop under both open-book and closed-book settings. Overall, Gemini 1.5 outperforms others in the open-book setting with an average F-1 of 70.92, while LLaVA achieves the highest performance in the closed-book setting with an average F-1 of 66.77. This is surprising, given that LLaVA is a much smaller model compared to GPT4-o and Gemini, and therefore possesses less parametric knowledge. Upon manually analyzing a subset of 100 randomly selected outputs from LLaVA, we found that the model frequently hallucinates, even when it predicts the correct label, particularly as the hop count increases. This is consistent with its open-book performance, where its accuracy declines when provided with gold evidence. Additionally, we observe that GPT4-o performs slightly better in closed-book settings than in open-book settings, suggesting a tendency to hallucinate. In contrast, Gemini’s performance drops significantly in closed-book settings compared to open-book, demonstrating its robustness in effectively utilizing provided gold evidence. Confidence Level Analysis The left panel of Figure 3 presents the confidence distributions for all three MLLMs, categorized by the number of hops and divided into 10 intervals. The results show that the majority of the MLLMs are concentrated in the 90-100 confidence range, with only a small number exhibiting low confidence (0-10 range), which occurs solely in open-book settings. This indicates that the MLLMs consider the provided gold evidence. The right panel of Figure 3 displays the calibration curves, illustrating the relationship between the models’ confidence levels and their actual classification accuracy. These curves reveal a positive correlation between confidence and accuracy for 1-hop and 2-hop claims, as exemplified by the red line (GPT-4-o on 2-hop), the teal line (LLaVA on 1-hop), and the purple line (Gemini on 1-hop). In contrast, the downward curves, mostly observed in 3-hop and 4-hop claims, suggest that the models tend to be overconfident when classifying more complex claims. Additionally, the results indicate that open-book settings generally have better-calibrated confidence scores than closed-book settings, further suggesting that the models exhibit overconfidence when not provided with gold evidence. Model Method 1-hop 2-hop 3-hop 4-hop Gemini 1.5 CoT 78.52 69.66 67.45 70.24 Self-Ask 75.47 66.58 60.94 70.67 Symbolic 74.89 63.82 54.61 72.36 GPT4-o CoT 80.43 83.33 71.20 72.99 Self-Ask 77.42 80.12 70.52 75.23 Symbolic 80.56 78.78 68.72 75.67 Table 4: Results of Gemini and GPT4-o on 100 randomly sampled claims for each hop using three types of reasoning prompts. Model performance is evaluated using F-1 score. Reasoning Prompt Results. Table 4 reports the performance of Gemini and GPT4-o on the randomly sampled subset of MMCV under open-book settings using various prompts that elicit LLMs’ reasoning abilities. For symbolic approach, we ask LLMs to first generate a Python-like program that decomposes the mutli-hop claim into a set of function calls that describe the reasoning steps required to verify the claim, and use the symbolic information provided by the generated program to elicit better step-by-step reasoning from the model. We observe that GPT-4-o gains more from the enhanced reasoning prompt compared to Gemini, achieving a higher average F1 score of 75.93 in symbolic guided reasoning, whereas Gemini attains an average F1 score of 66.42 for the same task. Additionaly, we found that Symbolic approach are more effective on 4-hop claims, having a higher F1 score than CoT and self-ask. However, this observation is different on simpler 2-hop and 3-hop claims, where CoT appears to be more effective. Annotator # Hops 1-hop 2-hop 3-hop 4-hop Annotator 1 83.33 86.20 78.42 79.82 Annotator 2 82.46 88.29 79.45 82.16 Annotator 3 80.60 90.53 80.62 85.24 Annotator 4 79.64 86.50 82.32 83.87 Table 5: Results of human performance on 200 random samples. Performance are measured by F-1 score. Human Performance Results To establish human performance on our dataset, we randomly sampled 200 examples, with 50 examples from each hop from MMCV. We recruited four annotators to perform claim verification given the gold evidence. We trained our annotators on the task by providing them with guidelines and sample annotations to ensure consistency and accuracy in their evaluations. After training, the annotators independently verified each claim using the provided gold evidence, allowing us to assess the human baseline performance on the dataset. Table 5 reports the results from the human annotators. We observe that the human annotators achieve very high performance in verifying the claims across all 4 hops. The human performance is 23.3% and 27.3% higher than the best-performing MLLMs on 3-hop and 4-hop claims respectively. This suggests that although MLLMs perform relatively well, there is still room for improvement to match human performance. 5.3 Error Analysis Figure 5, 6, and 7 shows the error analysis of the false positive examples from GPT4-o, Gemini, and LLaVA respectively. We observe that visual misinterpretation is a major issue, with the system often misidentifying or miscontextualizing image elements. This problem is especially pronounced in examples involving sports logos and movie posters, highlighting the need for improvements in the visual processing component. Another notable issue is the system’s handling of temporal and factual information. Errors related to player career timelines and historical events reveal shortcomings in temporal reasoning and the integration of world knowledge. The system’s confidence levels, often between 80% and 100% for incorrect predictions, suggest a miscalibration in certainty estimation. This overconfidence in erroneous conclusions highlights the need for a more refined approach to confidence scoring. Last but not least, examples from higher hop categories reveal significant weaknesses in handling complex reasoning tasks. The system often struggles with multi-step logical inferences, frequently failing to coherently link disparate pieces of information. This limitation is especially problematic for claims that require advanced analysis or the cross-referencing of multiple facts. 6 Conclusion In this paper, we introduce MMCV, a multi-hop multimodal claim verification dataset that requires models to aggregate information from up to four multimodal evidence to verify a claim. To create this large-scale dataset, we developed a novel data collection pipeline that leverages the capabilities of LLMs combined with human feedback. Specifically, our approach includes a module that iteratively refines modified claims using feedback from a judge LLM based on a set of predefined criteria, as well as an actuality validation module that employs RAG to ensure the factual accuracy of the claims. Our results show that state-of-the-art MLLMs struggle to verify more complex claims as the number of reasoning hops increases, often displaying overconfidence in their predictions. We also present findings from experiments utilizing prompts tailored to enhance the reasoning abilities of MLLMs, alongside human performance benchmarks for comparison. Additionally, we categorize and provide a detailed error analysis of false positive results from each model. We hope that MMCV will inspire the development of models capable of conducting complex, multi-hop reasoning in the challenging task of multimodal claim verification. 7 Limitations We identify two main limitations of MMCV. First, the construction of MMCV depends on in-context learning coupled with self-refinement to convert a natural language question-answer pair into a multi-hop claim. While this method has proven to be effective, it may face difficulties when dealing with questions with intricate grammar structures and logical structures. This arises from the difficulty in conveying complex grammatical rules to the language model through a limited number of demonstrations within a constrained context size. Second, our aggregation method purely relies on LLMs themselves, which could introduce potential hallucination problems. On the other hand, by using a more robust logic solver could help with the hallucination issues, but there would be a tradeoff between the applicability and the robustness of the model. 8 Ethical Statement Biases. We acknowledge the possibility of biases existing within the data used for training the language models, as well as in certain factuality assessments. Unfortunately, these factors are beyond our control. Intended Use and Misuse Potential. Our models have the potential to verify complex multimodal claims. However, it is essential to recognize that they may also be susceptible to misuse by malicious individuals. Therefore, we strongly urge researchers to approach their utilization with caution and prudence. Environmental Impact. We want to highlight the environmental impact of using large language models, which demand substantial computational costs and rely on GPUs/TPUs for training, which contributes to global warming. However, it is worth noting that our approach does not train such models from scratch. Instead, we use few-shot in-context learning. Nevertheless, the large language models we used in this paper are likely running on GPU(s). References Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Akhtar et al. (2023) Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, and Andreas Vlachos. 2023. Multimodal automated fact-checking: A survey. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5430–5448, Singapore. Association for Computational Linguistics. Aneja et al. (2021) Shivangi Aneja, Chris Bregler, and Matthias Nießner. 2021. Cosmos: Catching out-of-context misinformation with self-supervised learning. arXiv preprint arXiv:2101.06278. Chew et al. (2019) Rob Chew, Michael Wenger, Caroline Kery, Jason Nance, Keith Richards, Emily Hadley, and Peter Baumgartner. 2019. Smart: an open source data labeling platform for supervised learning. Journal of Machine Learning Research, 20(82):1–5. Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey on in-context learning. arXiv preprint arXiv:2301.00234. Fung et al. (2021) Yi Fung, Christopher Thomas, Revanth Gangi Reddy, Sandeep Polisetty, Heng Ji, Shih-Fu Chang, Kathleen McKeown, Mohit Bansal, and Avi Sil. 2021. InfoSurgeon: Cross-media fine-grained information consistency checking for fake news detection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1683–1698, Online. Association for Computational Linguistics. Geng et al. (2024) Jiahui Geng, Yova Kementchedjhieva, Preslav Nakov, and Iryna Gurevych. 2024. Multimodal large language models to support real-world fact-checking. arXiv preprint arXiv:2403.03627. Guo et al. (2022) Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics, 10:178–206. Gupta and Kembhavi (2023) Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953–14962. Jiang et al. (2020) Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3441–3460, Online. Association for Computational Linguistics. Jin et al. (2023) Yiqiao Jin, Yeon-Chang Lee, Kartik Sharma, Meng Ye, Karan Sikka, Ajay Divakaran, and Srijan Kumar. 2023. Predicting information pathways across online communities. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1044–1056. Jin et al. (2022) Yiqiao Jin, Xiting Wang, Ruichao Yang, Yizhou Sun, Wei Wang, Hao Liao, and Xing Xie. 2022. Towards fine-grained reasoning for fake news detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 5746–5754. Khashabi et al. (2018) Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252–262, New Orleans, Louisiana. Association for Computational Linguistics. Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474. Li et al. (2024) Miaoran Li, Baolin Peng, Michel Galley, Jianfeng Gao, and Zhu Zhang. 2024. Self-checker: Plug-and-play modules for fact-checking with large language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 163–181, Mexico City, Mexico. Association for Computational Linguistics. Liu et al. (2024a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024a. Visual instruction tuning. Advances in neural information processing systems, 36. Liu et al. (2024b) Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. 2024b. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177. Lu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086–8098, Dublin, Ireland. Association for Computational Linguistics. Luo et al. (2021) Grace Luo, Trevor Darrell, and Anna Rohrbach. 2021. NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6801–6817, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Mavi et al. (2022) Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022. A survey on multi-hop question answering and generation. arXiv preprint arXiv:2204.09140. Mishra et al. (2022) Shreyash Mishra, S Suryavardan, Amrit Bhaskar, Parul Chopra, Aishwarya N Reganti, Parth Patwa, Amitava Das, Tanmoy Chakraborty, Amit P Sheth, Asif Ekbal, et al. 2022. Factify: A multi-modal fact verification dataset. In DE-FACTIFY@ AAAI. Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744. Pace et al. (2024) Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2024. West-of-n: Synthetic preference generation for improved reward modeling. arXiv preprint arXiv:2401.12086. Pan et al. (2023a) Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023a. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188. Pan et al. (2023b) Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023b. Fact-checking complex claims with program-guided reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6981–7004, Toronto, Canada. Association for Computational Linguistics. Pan et al. (2023c) Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Wang. 2023c. On the risk of misinformation pollution with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1389–1403, Singapore. Association for Computational Linguistics. Press et al. (2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687–5711, Singapore. Association for Computational Linguistics. Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821–8831. Pmlr. Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695. Sclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023. Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324. Shu et al. (2020) Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. 2020. Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media. Big data, 8(3):171–188. Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. arXiv preprint arXiv:1803.06643. Talmor et al. (2021) Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. 2021. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint arXiv:2104.06039. Tan et al. (2024) Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large language models for data annotation: A survey. arXiv preprint arXiv:2402.13446. Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Thorne and Vlachos (2018) James Thorne and Andreas Vlachos. 2018. Automated fact checking: Task formulations, methods and future directions. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3346–3359, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Thorne et al. (2018) James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana. Association for Computational Linguistics. Wang and Shu (2023) Haoran Wang and Kai Shu. 2023. Explainable claim verification via knowledge-grounded reasoning with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6288–6304, Singapore. Association for Computational Linguistics. Wang and Shu (2024) Haoran Wang and Kai Shu. 2024. Trojan activation attack: Red-teaming large language models using steering vectors for safety-alignment. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM ’24, page 2347–2357, New York, NY, USA. Association for Computing Machinery. Wang et al. (2023) Ruida Wang, Wangchunshu Zhou, and Mrinmaya Sachan. 2023. Let’s synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11817–11831, Singapore. Association for Computational Linguistics. Wang (2017) William Yang Wang. 2017. “liar, liar pants on fire”: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422–426, Vancouver, Canada. Association for Computational Linguistics. Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837. Welbl et al. (2018) Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287–302. Yang et al. (2022) Ruichao Yang, Xiting Wang, Yiqiao Jin, Chaozhuo Li, Jianxun Lian, and Xing Xie. 2022. Reinforcement subgraph reasoning for fake news detection. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2253–2262. Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, Brussels, Belgium. Association for Computational Linguistics. Yao et al. (2023) Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2023. End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2733–2743. Zlatkova et al. (2019) Dimitrina Zlatkova, Preslav Nakov, and Ivan Koychev. 2019. Fact-checking meets fauxtography: Verifying claims about images. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2099–2108, Hong Kong, China. Association for Computational Linguistics. Appendix A Appendix A.1 Dataset Example Here is an example of dataset schema from MMCV: Example claim: Stoke City, a club that was part of the top-tier league before 1992, was promoted to the highest level of English football in 2018. wiki_context: The Premier League is the highest level of the English football league system. Contested by 20 clubs, it operates on a system of promotion and relegation with the English Football League (EFL). Seasons usually run from August to May, with each team playing 38 matches: two against each other, one home and one away. Most games are played on weekend afternoons, with occasional weekday evening fixtures. text_evidence: [ ""f369cee1ca92368c8b1ea564c5e41fc1"" ] image_evidence: [] table_evidence: [ ""c120efadd518b5f32c11d40b456c8570"" ] label: SUPPORT Additional examples of 1-hop, 2-hop, 3-hop, and 4-hop claims are listed in Table 6 A.2 Experiment Prompt Claim Verification Prompt. To test MLLMs’ claim verification performance under zero-shot settings, we follow Geng et al. (2024) and use the following prompt. Prompt Given a claim and evidence (which can be text, table, or an image), determine whether the claim is SUPPORT or REFUTE by the evidence. Use the following format to provide your answer: Prediction: [True or False] Explanation: [put your evidence and step-by-step reasoning here] Confidence Level: [please show the percentage] Note: The confidence level indicates the degree of certainty you have about your answer and is represented as a percentage. For instance, if your confidence level is 80%, it means you are 80% certain that your answer is correct and there is a 20% chance that it may be incorrect. Claim Generation Prompt. We use the following prompt to convert multimodal QA pairs into claim candidates: Prompt You are an expert in converting question-answers into claims. For example: Question: Telos was an album by a band who formed in what city? Answer: Indianapolis. Claim: Telos was an album by a band formed in Indianapolis. Convert the question-answer into claim. Return only the claim and nothing else. Claim Modification Prompt. We use the following prompt to modify the claim candidates: Prompt Generate a multi-hop specific claim based on the given general claim and Wikipedia context. The specific claim should: Incorporate information from Wikipedia context. Provided context should always be factually correct. Obscure key information by: a) Replacing one or two central entities with related fact using the Wikipedia context. b) Alluding to critical details without explicitly stating them. Claim should be short and concise. For example: - General Claim: The Mona Lisa is a famous painting by Leonardo da Vinci. - Wikipedia Context: The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as ẗhe best known, the most visited, the most written about, the most sung about, the most parodied work of art in the world.̈ The painting’s novel qualities include the subject’s enigmatic expression, the monumentality of the composition, the subtle modelling of forms, and the atmospheric illusionism. It is housed in the Louvre Museum in Paris, where it was first put on display in 1797. - Specific Claim: The Mona Lisa is a half-length portrait painting created by Italian artist who is considered as archetypal masterpiece of the Italian Renaissance. Claim Refinement Prompt. We use the following prompt to refine the claim candidates: Prompt You are tasked with improving a claim focusing on three key areas: Fluency, Correctness, and Clearness. Your goal is to enhance the text while maintaining its original meaning and intent. Improvement Criteria: Fluency: 1. Review the text for grammar, syntax, and punctuation errors. 2. Rephrase any awkward or unnatural sentences to make the text flow more smoothly. 3. Ensure that the text reads naturally and is easy to follow. Correctness: 1. Verify the factual accuracy of the content and correct any errors. 2. Ensure that the text adheres to the prompt’s instructions. 3. Clarify any ambiguities and correct any inconsistencies in the information presented. Clearness: 1. Simplify complex sentences or ideas to make the text easier to understand. 2. Improve the organization of ideas to enhance readability. 3. Ensure that the message is conveyed clearly and effectively, eliminating any confusion or ambiguity. Final Output: Once you have made the necessary improvements, provide the revised text. Ensure that the improved version is more fluent, accurate, and clear than the original while preserving the original meaning and intent. Example Improvement: Original Claim: ""The results of the survey was very positive, with many respondents saying that they would recommend the service to others, however, some were also mentioned issues with the customer support."" Improved Claim: ""The survey results were overwhelmingly positive, with many respondents stating they would recommend the service to others. However, some also noted issues with customer support. A.3 Annotation Guidelines We ask our annotators to score the quality of the claim from three aspects: fluency, correctness, and clearness. Here is the detailed guidelines provided to the human annotators. Guidelines \triangleright Scoring Criteria: Fluency: Rate on a scale of 1-4. Correctness: Rate on a scale of 1-3. Clearness Rate on a scale of 1-3. \triangleright Fluency (1-4): 4: Excellent - Reads naturally, no awkward phrasing. 3: Good - Mostly smooth, minor phrasing issues. 2: Fair - Several awkward phrases or constructions. 1: Poor - Difficult to read, very unnatural phrasing. \triangleright Correctness (1-3): 3: Fully correct - All information is accurate. 2: Partially correct - Some information is accurate, some errors. 1: Incorrect - Significant factual errors or misrepresentations. \triangleright Clearness (1-3): 3: Very clear - Easy to understand, no ambiguity. 2: Somewhat clear - Some parts may be confusing or ambiguous. 1: Unclear - Difficult to understand the intended meaning. A.4 Crowd Worker Interface We use SMART Chew et al. (2019), an open-source project designed to help data scientists and research teams efficiently build labeled training datasets for supervised machine learning tasks. Figure 4 shows an example of the worker interface during scoring procedure. #H Claim Evidence 1 Claim: Marisa Coughlan played the role of Chante Lefort on television in 1996. 2 Claim: The driver seen signing autographs outside had a significant points total during a specific race in 2001 while competing for a well-known team in stock car racing. 3 Claim: The Green Bay Packers were one of the two teams that played in the first Super Bowl and also faced the New York Giants at MetLife Stadium during the 2013 regular season. Doc A: The first AFL-NFL World Championship Game in professional American football, known retroactively as Super Bowl I and … … Doc B: The National Football League (NFL) champion Green Bay Packers defeated the American Football League (AFL) champion Kansas City Chiefs … … Table: Not Included Here 4 Claim: The team that was promoted to the Premier League in the 2018-19 season received a higher accolade in the Third Division PFA Team of the Year during the 1980s than a club renowned for its West London rivalries. Doc A: Manchester City are the defending champions. Wolverhampton Wanderers, Cardiff City and Fulham join as the promoted clubs from the 2017–18 EFL Championship. … … Doc B: … They will replace West Bromwich Albion, Swansea City and Stoke City who were relegated to the 2018–19 EFL Championship. … Table: Not Included Here Image: Not Included Here Table 6: Examples of 1-hop, 2-hop, 3-hop and 4-hop claims from MMCV. Figure 4: UI for human annotators. Figure 5: Error Analysis: Gemini Figure 6: Error Analysis: GPT4-o Figure 7: Error Analysis: LLaVA"
https://arxiv.org/html/2411.09539v1,A Practical Guide to Fine-tuning Language Models with Limited Data,"Employing pre-trained Large Language Models (LLMs) has become the de facto standard in Natural Language Processing (NLP) despite their extensive data requirements. Motivated by the recent surge in research focused on training LLMs with limited data, particularly in low-resource domains and languages, this paper surveys recent transfer learning approaches to optimize model performance in downstream tasks where data is scarce. We first address initial and continued pre-training strategies to better leverage prior knowledge in unseen domains and languages. We then examine how to maximize the utility of limited data during fine-tuning and few-shot learning. The final section takes a task-specific perspective, reviewing models and methods suited for different levels of data scarcity. Our goal is to provide practitioners with practical guidelines for overcoming the challenges posed by constrained data while also highlighting promising directions for future research.","Pre-trained Language Models (PLMs) are transforming the field of NLP, showing outstanding capabilities to learn and model the underlying distributions of natural language data from complex and diverse domains (Han et al., 2021). Yet, their training demands extensive data and computational resources, which can be prohibitive in many real-world scenarios (Bai et al., 2024), particularly for languages other than English and for specialized domains such as, i.a., medicine (Crema et al., 2023; Van Veen et al., 2023), chemistry (Jablonka et al., 2024), law (Noguti et al., 2023), finance (Zhao et al., 2021), engineering (Beltagy et al., 2019). The predominant approach to deal with this common issue relies on the transfer learning paradigm, which involves a self-supervised pre-training phase on vast amounts of general- or mixed-domain data, followed by (possibly multiple) domain adaptation and fine-tuning or few-shot learning steps on domain- and task-specific data. Notably, the second stage of this process is also data-hungry. Data scarcity can lead to overfitting, poor generalization, and suboptimal performance. Fine-tuning PLMs with limited data requires careful selection of pre-training strategies, domain adaptation, and efficient parameter optimization to achieve optimal performance by leveraging the model’s pre-existing knowledge effectively to avoid catastrophic forgetting (Kirkpatrick et al., 2017; Ramasesh et al., 2021). This paper addresses the challenge of training LLMs with limited data, particularly for low-resource languages and specialized domains, by exploring recent advances in transfer learning (Table 1). We conduct a systematic review, starting with more than 2500 papers collected from Scopus, Web of Science, Google Scholar and ACL Anthology. It is intended for both researchers and practitioners in NLP, providing an overview of the current state-of-the-art methods and practical guidelines for optimizing model performance in data-scarce scenarios. We examine the process of adapting LLMs to specific tasks and domains with limited data, focusing on (1) selecting appropriate (continued) pre-training methods to leverage prior knowledge effectively in the low-resource scenario at hand (§ 3); (2) maximizing the utility of limited data during fine-tuning (§ 4) and few-shot learning (§ 5); (3) discussing the assumptions, benefits and limitations of various transfer learning strategies highlighting open challenges for researchers; and (4) task-specific perspectives on overcoming different levels of data scarcity as guidance for practitioners (§ 6)."
https://arxiv.org/html/2411.09497v1,The Use of Readability Metrics in Legal Text: A Systematic Literature Review,"Understanding the text in legal documents can be challenging due to their complex structure and the inclusion of domain-specific jargon. Laws and regulations are often crafted in such a manner that engagement with them requires formal training, potentially leading to vastly different interpretations of the same texts. Linguistic complexity is an important contributor to the difficulties experienced by readers. Simplifying texts could enhance comprehension across a broader audience, not just among trained professionals. Various metrics have been developed to measure document readability. Therefore, we adopted a systematic review approach to examine the linguistic and readability metrics currently employed for legal and regulatory texts. A total of 3566 initial papers were screened, with 34 relevant studies found and further assessed. Our primary objective was to identify which current metrics were applied for evaluating readability within the legal field. Sixteen different metrics were identified, with the Flesch-Kincaid Grade Level being the most frequently used method. The majority of studies (73.5\%) were found in the domain of ”informed consent forms”. From the analysis, it is clear that not all legal domains are well represented in terms of readability metrics and that there is a further need to develop more consensus on which metrics should be applied for legal documents.","1.1 Complexity of legislation Law and legislation texts can be notoriously difficult to read. The increasing number and complexity of legal documents only adds a further barrier for engagement with law by the general public [1]. In addition, overly complex regulations can lead to unnecessary economic costs to society [2]. At this point, we should probably define a bit better what is meant by complexity. According to the Oxford web dictionary, ”complexity” refers to something being difficult to understand, complicated, or intricate [3]. It also refers to something consisting of parts or elements that are not simply coordinated. The complexity of regulation may stem from various factors, including the design of regulations, the methods used to develop them, and the entities they regulate. This makes it difficult for people from all walks of life to engage with the information that is contained within these text and it can result in different interpretations between readers [4]. The issue of complexity in legal or law texts is crucial because it can lead to confusion, loss of direction, and even malicious interpretation of the text by readers [5]. Regulations or laws should provide guidance to all stakeholders and create a platform for dialogue and development while implementing the principles of law and bringing legislation into effect. Complexity issues in regulation have been researched in various fields, including general law [2], clinical settings [6, 7, 8, 9], business areas [10], financial regulation [11], and tax [12]. According to researcher, the inability of regulation to incentivize proper behavior may have contributed to the 2008 financial crisis [13], or at least the regulation did not manage to mitigate its consequences. Furthermore, the interpretation of legislation frequently poses challenges due to the specialized language and unique format that distinguishes it from other document types. This complexity creates an ongoing navigational and comprehension barrier for non-specialists [14]. 1.2 Measuring readability of legislation and regulations Legal law field language, which includes complex sentences archaic or apply large amount of words or expressions rarely used in other industry, or inclusion of foreign words, is often been criticized for its abstruseness [15]. People further realized the important in legal area as language as a communication tool to be understandable by most people, whether to professionals or non-professionals [16]. In the book of laws of simplicity, the researcher state that the simplest way to achieve simplicity is through thoughtful reduction and design[17]. In legal and law field, what are been deemed as simple and useful regulation is always debatable. Readability metrics can’t fix all aspects of evaluation work, for example Andrew points out that the complexity of regulation is increase not only because page number is increased, more come from major metrics calculation method has changed [18]. Researcher has proved readability has many limitations for example, most readability metrics are developed for children education not technical documentation[19]. Lawyers have “the inclination to use prepositional and other phrases in place of simple adverbs or prepositions” and they may not realize that their ”stylistic choice is merely a matter of habit, because this is how lawyers traditionally speak or write” [20]. As a result, the Plain English Movement emerged in the 1980s. They were campaigning to e.g. replace old-fashioned words, avoid redundancy, reduce sentence length [15]. From this initiative, and others such as the Good Law Initiative, it became apparent that there was a need for plainer language in legal text. Simpler language would allow for better engagement with the law, whether it was for legal professionals or non-professionals. It is important to include the non-professionals, since most people in society will come in contact with it at some stage of their life [16]. The Plain English movement reinforced the importance of plain words to make legal language clearer. It highlighted the importance of using simple language to convey ideas and replacing complex expressions in legal texts. However, there is also criticism around this movement. Some argue that the plain language heritage is opposing traditional decent legal writing, signifying a new attitude and a fundamental change from previously well-established practices. Others argue that it is essentially choosing between either accuracy or clarity [21]. Legal text remain complex [15, 22], despite the interest to make legal documents more accessible to the reader. People have come to realize the importance of using language that is easy to understand in order to communicate effectively with both legal professionals and non-professionals [16]. In his book ”The Laws of Simplicity,” it is suggested that the easiest way to achieve simplicity is through thoughtful reduction and design [17]. However, what is considered simple and useful in the legal field is debatable. An ability to measure linguistic simplicity could help to reduce complexity. Readability metrics already aim to quantify how easy it is to understand a written text. However, it should be noted that the most common methods for assessing text complexity originated from techniques that were developed to determine the expected level of education required to read the text [9]. To what extend these metrics are applied within the legal field remains unclear. Interpretation of (law text difference) 1.3 Importance of readability metrics The development of natural language processing (NLP) techniques could enable the analysis and extraction of relevant information from legal and regulatory texts. Exploration in this field also started to include machine learning methods to assess the readability of legislative sentences [23]. AI could find relevant or potentially problematic clauses in millions of documents, improving efficiency and accuracy in legal analysis [24]. Researches emphasize the need for a bridge between legal interpretation and computational challenges, highlighting the potential of NLP methods and semantic technologies in this context [25]. Researcher further explores the potential of data and text mining, particularly using NLP, in the semantic analysis of legal documents [26]. These studies collectively underscore the transformative role of NLP in the legal domain, particularly in the analysis and extraction of relevant information from legal and regulatory texts. However, they don’t show if there is common way of measuring readability nor do they reflect on which approaches have been leveraged thus far. While improving the readability of legislation is critical, establishing a standard method to assess and enhance readability through algorithms or readability scores remains an unresolved issue within the scientific community. In this manuscript, we acknowledge these advancements, but also recognize that there is an absence of a systematic review of the current literature. Consensus on how the measure readability of legal sentences before employing NLP tools can help to better compare outcomes between different studies. It can also help optimize NLP models to better handle complex legal language. A standard approach can support a much needed quality control mechanism, enabling quantitative evaluation of how changes in a text can improve readability. The application of different readability formulas has long been a staple in evaluating the comprehensibility of texts across various domains. However, how widely these traditional formulas are applied to specialized fields remains under explored. Currently, no studies have systematically examined the readability metrics applied specifically to legal texts. This research aims to identify the most widely used readability metrics within the legal field and to determine the areas in which they are predominantly employed. By conducting this systematic review, we seek to offer valuable insights into the current landscape of readability assessment in legal documents and highlight areas that may require further investigation."
https://arxiv.org/html/2411.09492v1,"MM-Eval: A Hierarchical Benchmark for
Modern Mongolian Evaluation in LLMs","Large language models (LLMs) excel in high-resource languages but face notable challenges in low-resource languages like Mongolian. This paper addresses these challenges by categorizing capabilities into language abilities (syntax and semantics) and cognitive abilities (knowledge and reasoning). To systematically evaluate these areas, we developed MM-Eval, a specialized dataset based on Modern Mongolian Language Textbook I and enriched with WebQSP and MGSM datasets.Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat, Llama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models performed better on syntactic tasks than semantic tasks, highlighting a gap in deeper language understanding; and 2) knowledge tasks showed a moderate decline, suggesting that models can transfer general knowledge from high-resource to low-resource contexts.The release of MM-Eval—comprising 569 syntax, 677 semantics, 344 knowledge, and 250 reasoning tasks—offers valuable insights for advancing NLP and LLMs in low-resource languages like Mongolian. The dataset is available at https://github.com/joenahm/MM-Eval.","Figure 1: Workflow for Constructing the MM-Eval Dataset In recent years, large language models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable capabilities in understanding and generating human language, excelling in tasks such as context comprehensionJin et al. (2024), language generationMalik et al. (2024), summarizationSong et al. (2024), question answeringSchimanski et al. (2024), and translationXu et al. (2024). Models like ChatGPTOpenAI (2023) and LlamaTouvron et al. (2023) have set new benchmarks across a wide range of languages, primarily high-resource ones such as Chinese and English. However, the support for low-resource languages like Mongolian remains largely unexplored. Mongolian, spoken by millions across Mongolia and Inner Mongolia of China, presents unique linguistic challenges due to its complex grammar, script, and historical evolution. In Mongolia, modern Mongolian is written using the Cyrillic script, based on the Russian alphabet, while in Inner Mongolia, China, the traditional Mongolian script, derived from the Sogdian-Uyghur script, is used. This paper focuses on modern Mongolian written in the Cyrillic script. Despite some efforts to include Mongolian in NLP research, there is still a significant gap in understanding how well LLMs can handle Mongolian across various linguistic dimensions. This research aims to fill the gap in Mongolian language support by systematically evaluating modern LLMs’ capabilities in processing Mongolian. Unlike existing task-oriented datasets, this study focuses on models proven effective in high-resource languages. For Mongolian, we adopt a linguistic perspective, constructing a dataset based on language proficiency levels and previous LLM performance. Our dataset is organized into four hierarchical levels: syntax, semantics, knowledge, and reasoning. This structure allows for a detailed evaluation of model performance at different proficiency levels, providing deeper insights into their strengths and limitations. By uncovering both the strengths and weaknesses of current models, we aim to provide a benchmark for future Mongolian NLP research, contribute to the broader understanding of LLM support for low-resource languages, and help enhance their Mongolian language capabilities. We summarize the main lessons learned and our main contributions as follows: • This paper introduces MM-Eval, a specialized dataset for evaluating the capabilities of large language models (LLMs) in modern Mongolian, which is a low-resource language. • This paper proposes a Dual Capability Framework that evaluates LLMs by dividing their capabilities into language abilities (syntax and semantics) and cognitive abilities (knowledge and reasoning). This framework allows for a detailed understanding of model performance at different language proficiency levels. • This paper provides a comprehensive evaluation of LLMs in Mongolian, covering syntax, semantics, knowledge, and reasoning. This evaluation reveals the strengths and weaknesses of current models in processing Mongolian."
https://arxiv.org/html/2411.09431v1,Everyone deserves their voice to be heard: Analyzing Predictive Gender Bias in ASR Models Applied to Dutch Speech Data,"Recent research has shown that state-of-the-art (SotA) Automatic Speech Recognition (ASR) systems, such as Whisper, often exhibit predictive biases that disproportionately affect various demographic groups. This study focuses on identifying the performance disparities of Whisper models on Dutch speech data from the Common Voice dataset and the Dutch National Public Broadcasting organisation. We analyzed the word error rate, character error rate and a BERT-based semantic similarity across gender groups. We used the moral framework of Weerts et al. (2022) to assess quality of service harms and fairness, and to provide a nuanced discussion on the implications of these biases, particularly for automatic subtitling. Our findings reveal substantial disparities in word error rate (WER) among gender groups across all model sizes, with bias identified through statistical testing.","Automatic Speech Recognition (ASR) technology has increasingly become integral to a wide range of applications, enhancing user interaction through virtual assistants and making multimedia content more accessible via automatic subtitling. Recent studies have substantially advanced the accuracy of ASR systems in transcribing human speech. However, research consistently reveals that these systems often manifest performance disparities that correlate with demographic variables. Biases in ASR systems can have profound implications, potentially exacerbating social inequalities and raising serious ethical concerns in high-stakes applications like employment, where ASR technology is used in tools for automated video interviews [20, 33, 29, 2]. This research aims to address these substantial issues by employing a rigorous, morally justified framework to evaluate biases and fairness within Whisper, a state-of-the-art ASR model [31], specifically analyzing its performance with Dutch language data. Previous research has shown performance differences in the recognition rate of ASR models due to speaker variations in demographic attributes. Some works show better recognition for male speech [36, 16, 17, 27], while most indicate better performance for female speech [14, 32, 28, 1, 25, 13, 15] and other report mixed findings or no (significant) differences [37, 26, 8]. Age-related disparities also surface, with studies generally showing superior ASR performance for teenagers over children [14, 13, 15]. Accent and native language further influence ASR performance, with native speakers or those who acquired the language early typically experiencing better recognition rates compared to non-native speakers or those with heavy accents [8, 40, 41, 13, 15, 30, 37]. Despite the widespread acknowledgment of biases, the methodological rigor in evaluating such biases often lacks. A substantial portion of existing studies use statistical tests such as ANOVA, Kruskal-Wallis, and linear mixed models to analyze performance disparities [35, 8, 16, 15, 27, 13]. However, these studies rarely discuss the validity of the underlying assumptions of these tests, which could potentially lead to misleading conclusions. This is one of the indications that a more rigorous methodology still needs to be established in the ASR (predictive) bias research. Existing work also typically does not provide moral justification regarding the usage of metrics to measure and argue about fairness. As noted by Blodgett et al., the techniques used to measure fairness and biases must be justified and reasoned, matching with the motivations [6]. Furthermore, the authors propose that future research should “Provide explicit statements of why the system behaviors that are described as ‘bias’ are harmful, in what ways, and to whom. Be forthright about the normative reasoning (Green, 2019) underlying these statements."" [6]. Therefore, simply coming up with a metric a model must abide by is not always morally justified. Highlighting the mathematical properties does not vindicate using a fairness metric without considering the moral implications, assumptions, or considerations [19]. This study makes first steps to address the lack of moral justification by integrating moral considerations into the evaluation of ASR systems, moving beyond the mere identification of biases to encompass a broader discussion on ethical implications and fairness [39]. Most existing research on ASR bias focuses on English data, leaving performance across other languages and demographic settings less explored. We use data from both Mozilla’s Common Voice dataset and a dataset comprising Dutch TV and radio shows from the NPO, the Dutch National Public Broadcasting organisation (Nederlandse Publieke Omroep). Our study aims to provide a comprehensive analysis of the Whisper ASR model’s performance across a range of speech types and gender groups, highlighting both its capabilities and limitations. This thorough evaluation helps identify potential areas for improvement in ASR technology, ensuring it delivers equitable and effective service across all user groups. The rest of the paper details our methodology, presents findings on gender bias in Whisper models, interprets these findings within our moral framework, and concludes with insights and recommendations for future research."
https://arxiv.org/html/2411.09318v2,DriveThru: a Document Extraction Platform and Benchmark Datasets for Indonesian Local Language Archives,"Indonesia is one of the most diverse countries linguistically. However, despite this linguistic diversity, Indonesian languages remain underrepresented in Natural Language Processing (NLP) research and technologies. In the past two years, several efforts have been conducted to construct NLP resources for Indonesian languages. However, most of these efforts have been focused on creating manual resources thus difficult to scale to more languages. Although many Indonesian languages do not have a web presence, locally there are resources that document these languages well in printed forms such as books, magazines, and newspapers. Digitizing these existing resources will enable scaling of Indonesian language resource construction to many more languages. In this paper, we propose an alternative method of creating datasets by digitizing documents, which have not previously been used to build digital language resources in Indonesia. DriveThru is a platform for extracting document content utilizing Optical Character Recognition (OCR) techniques in its system to provide language resource building with less manual effort and cost. This paper also studies the utility of current state-of-the-art LLM for post-OCR correction to show the capability of increasing the character accuracy rate (CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR. The platform is available online at https://ocrdt.ragambahasa.id/ and the benchmark dataset, evaluation script, and the models are available at our GitHub repository111https://github.com/ragambahasa. We also provide a short (\sim1-minute) screencast of our system on YouTube: https://youtu.be/q5uJOHKcBsg","Indonesia is known as one of the world’s most populated countries with a population exceeding 270 million. Spreading over 17 thousand islands, Indonesia is also one of the most diverse countries in the world with over 1,300 ethnic groups speaking over 700 local languages Eberhard et al. (2024). In terms of the number of speakers, the top 20 of Indonesian languages are spoken by over 1 million people each. Despite this linguistic diversity, Indonesian languages remain underrepresented in Natural Language Processing (NLP) research and technologies Aji et al. (2022). Thus, very little of NLP’s significant progress in the past few years have found its way to applications for these languages. To spur the development of research and technologies for Indonesian languages, in the past two years several efforts have been carried out to construct NLP resources for Indonesian languages Cahyawijaya et al. (2023a); Winata et al. (2023); Cahyawijaya et al. (2023b). However, these efforts have been focused on a small number of Indonesian languages (top 10 languages in terms of existing Web presence). The resources are created either manually or translated from English resources using existing machine translation (MT) systems. As the process of hiring annotators and managing annotation is costly and time-consuming, and because existing MT systems are limited in coverage, these efforts are difficult to scale to more languages. Although many Indonesian languages do not have a web presence, locally there are resources that document these languages well in printed forms such as books: textbooks, grammar books, dictionaries, story books, etc., magazines, and newspapers. Digitizing these existing resources will enable scaling of Indonesian language resource construction to many more languages. In addition, digitizing existing resources such as books can alleviate some of the drawbacks of prior works. Firstly, a book must have passed through quality assurance stages before being published, thus, the requirement of recruiting native speakers as a dataset curator can be altered since the works will be mostly focused on collecting books and identifying languages of these books instead of creating resources from scratch. Secondly, the time and cost for constructing resources can be reduced since it takes less time and costs to digitize books than creating resources from scratch. Several books are available online and published openly by the Indonesian National Library or the Indonesian government itself. In this study, we propose DriveThru platform, an alternative system that digitizes documents to assist Indonesian NLP researchers in their language resource collection. In this work, aside from using an off-the-shelf OCR system, TesseractOCR, we also benchmark LLMs for post-OCR error correction in local Indonesian languages, which occurs when off-the-shelf OCR systems are unable to recognize certain characters. We explore OCR and post-OCR error correction four low-resource Indonesian local languages: Javanese (jav), Sundanese (sun), Minangkabau (min), and Balinese (ban) written in latin scripts. This work demonstrates our approach for collecting underrepresented language resources through document extraction i.e., digitization of printed documents written in these languages."
https://arxiv.org/html/2411.09297v1,DTELS: Towards Dynamic Granularity of Timeline Summarization,"The rapid proliferation of online news has posed significant challenges in tracking the continuous development of news topics. Traditional timeline summarization constructs a chronological summary of the events but often lacks the flexibility to meet the diverse granularity needs. To overcome this limitation, we introduce a new paradigm, Dynamic-granularity TimELine Summarization, (DTELS), which aims to construct adaptive timelines based on user instructions or requirements. This paper establishes a comprehensive benchmark for DTLES that includes: (1) an evaluation framework grounded in journalistic standards to assess the timeline quality across four dimensions: Informativeness, Granular Consistency, Factuality, and Coherence; (2) a large-scale, multi-source dataset with multiple granularity timeline annotations based on a consensus process to facilitate authority; (3) extensive experiments and analysis with two proposed solutions based on Large Language Models (LLMs) and existing state-of-the-art TLS methods. The experimental results demonstrate the effectiveness of LLM-based solutions. However, even the most advanced LLMs struggle to consistently generate timelines that are both informative and granularly consistent, highlighting the challenges of the DTELS task.111Codes are available at https://github.com/chenlong-clock/DTELS-Bench.","With the surge in news production, the volume of news articles published on the internet is expanding rapidly, making it increasingly challenging to track the developments of news topics. TimeLine Summarization (TLS) (Wang et al., 2016; Li et al., 2021; Chen et al., 2023b; Zhang et al., 2024) aims to construct a sequence of chronologically ordered summaries. These timelines provide a traceable skeleton, supporting various applications, including policymaking, crisis management, and stock market analysis. Figure 1: (a) In traditional TLS, a timeline with a predefined number of node summaries is constructed. (b) DTELS provides timelines at different granular levels: network engineers require the technical causes and solutions to data breaches, therefore, a fine-grained granularity is preferred to track the technical details. For investors, a coarse-grained timeline showing the full picture of the breach’s influence on investment may suffice. Traditional TLS typically constructs static timelines at a fixed granularity: in Figure 1a, for a specific news topic, the granularity is heuristically predefined by the number of “salient events”. However, in practice, the granularity of the timeline should change dynamically, depending on user needs and the nature of news topics: For readers: Different readers have very different requirements on granularity for the same topic (see example in Figure 1b). For news topics: A reader’s need for granularity varies across topics. One may require fine-grained timelines for trending news such as local disasters to follow the progression and immediate impacts. In contrast, for long-standing topics like the Russian-Ukrainian war, people may warrant coarse-grained timelines with wider intervals to capture broader developments. Unfortunately, existing TLS ignores the importance of providing timelines at dynamic granularities. Existing evaluations also lack appropriate reference annotations and metrics to comprehensively evaluate timelines at dynamic granularities. In this paper, we propose a new paradigm: Dynamic granularity TimELine Summarization (DTELS). We define the granularity of a timeline by the degree of omission between the node summaries. Given a collection of news articles on the specific news topic and granularity requirements, our task aims to construct dynamic-granularity timelines tailored to various requirements. Meanwhile, to take the study a step further, grounded in the criteria from journalism (Kunelius, 2006), an ideal timeline should: (1) convey information effectively, avoiding redundant events, and ensuring that no important events are missed. (2) maintain consistency with the granular requirements. (3) ensure the mentioned events in each summary are factually correct. (4) be self-contained, allowing the reader to clearly understand the context. By adhering to these criteria, we set the standard that not only meets the dynamic granularity needs but also upholds high quality. We construct a benchmark including: Evaluation Framework. To comprehensively measure a timeline, We propose metrics that address the aforementioned criteria: Informativeness: This metric evaluates the effective volume of information in the node summaries. We propose a “mount-then-measure” paradigm to align predicted node summaries to those in the reference timeline based on the entailment score of the “event atoms”, which represents the smallest unit of event information within a sentence. Granular Consistency: The granularity is reflected by the amount of event information omitted between adjacent nodes. The more events omitted, the coarser the granularity is. We regard adjacent nodes as edges and calculate the ratio of mounts on the correct reference granularity edge. Factuality: Considering the hallucinated contents and misinformation in the era of Large Language Models (LLMs) (Ji et al., 2023; Li et al., 2023; Zhang et al., 2023), it is crucial to ensure the information accuracy. We introduce a factuality metric that incorporates atoms-level entailment verification from reference news articles to measure the non-fabricated information in each summary. Coherence: Coherence is pivotal in summarization tasks (Goyal et al., 2022; Steen and Markert, 2022). We adopt this metric for our task, ensuring that summaries are generated in a structurally, linguistically, and stylistically coherent manner. To facilitate this, we design a review form to guide the most advanced LLMs for coherence evaluation. We verify the effectiveness of the metrics, showcasing high alignment with humans. Dataset Construction. To ensure evaluation across varying granularities, we meticulously construct a dataset called DTELS-Bench. We initially collect diverse news topics and journalists’ annotations on timelines from news events websites222https://events.baidu.com . We then gather corresponding large-scale news articles from diverse sources, resulting in a large-scale, multi-source Chinese dataset. Subsequently, the reference timelines are annotated at three predefined granularities through a consensus-based automated annotation. Finally, the timelines are refined by specialists to ensure the authority. Comprehensive Evaluation. In the experiments, we present two LLM-based solutions for long-context and context-limited LLMs. We systematically evaluate our proposed solutions with multiple LLMs. In addition, we compare existing state-of-the-art extractive TLS approaches. Experiments show that our LLM-based solutions dominate in all dimensions, however, they fall short of providing high-quality information and aligning the required granularity. We then analyze the performance of these methods across various settings of DTELS. The results indicate that there is still substantial room for improvement in DTELS. To sum up, our contributions are as follows: • We propose a new task: Dynamic granularity TimELine Summarization (DTELS). It aims to summarize timelines tailored to the unique needs of dynamic granularities. • We build an event-centric evaluation framework. Extending from journalism, we propose metrics to evaluate timelines in four dimensions: informativeness, granular consistency, factuality, and coherence. Experiments with human annotators demonstrate the effectiveness of our metrics. • We collect a large-scale, multi-source Chinese dataset, DTELS-Bench333All Chinese information in the paper is translated into English for ease of understanding., which contains 543 news topics with 55,432 articles from 2,858 sources. It covers three predefined granularities annotated via a consensus-based mechanism. The expert’s refinement enhances the annotation authority. • We evaluate existing state-of-the-art TLS methods as well as LLMs with two proposed DTELS methods. Through extensive experiments, we find the proposed solutions outperform existing TLS methods, however, they are far from being an ideal solution to DTELS. Figure 2: Examples of metrics. Green nodes indicate positive examples and red nodes indicate negative examples."
https://arxiv.org/html/2411.09289v1,StreamAdapter: Efficient Test Time Adaptation from Contextual Streams,"In-context learning (ICL) allows large language models (LLMs) to adapt to new tasks directly from the given demonstrations without requiring gradient updates. While recent advances have expanded context windows to accommodate more demonstrations, this approach increases inference costs without necessarily improving performance. To mitigate these issues, We propose StreamAdapter, a novel approach that directly updates model parameters from context at test time, eliminating the need for explicit in-context demonstrations. StreamAdapter employs context mapping and weight absorption mechanisms to dynamically transform ICL demonstrations into parameter updates with minimal additional parameters. By reducing reliance on numerous in-context examples, StreamAdapter significantly reduce inference costs and allows for efficient inference with constant time complexity, regardless of demonstration count. Extensive experiments across diverse tasks and model architectures demonstrate that StreamAdapter achieves comparable or superior adaptation capability to ICL while requiring significantly fewer demonstrations. The superior task adaptation and context encoding capabilities of StreamAdapter on both language understanding and generation tasks provides a new perspective for adapting LLMs at test time using context, allowing for more efficient adaptation across scenarios and more cost-effective inference.","Large language models (LLMs) have emerged as a powerful tool in natural language processing, demonstrating exceptional performance across a diverse range of tasks, including text generation (Yuan et al., 2022), question answering (Kumar et al., 2023), open-ended conversations (Zhang et al., 2023a), and mathematical problem-solving (Shao et al., 2024). A key factor behind the success of LLMs is their ability to perform in-context learning (ICL) (Brown et al., 2020), where the model adapts to new tasks by conditioning on a small number of input-output demonstrations provided in the context. Without any gradient updates, ICL enables LLMs to acquire new knowledge and capabilities at test time, while also enabling LLMs to solve complex tasks through step-by-step guidance (Wei et al., 2023). Despite its remarkable capabilities, ICL faces several limitations that hinder its full potential. Firstly, the effectiveness of ICL heavily depends on the quality and relevance of the provided demonstrations, making the selection of appropriate examples a challenging task that often requires domain expertise (Agarwal et al., 2024; Sahoo et al., 2024). Moreover, the number of demonstrations that can be included is constrained by the model’s context window size. While recent advancements have expanded these windows (Ding et al., 2024; Team et al., 2024), accommodating more examples introduces significant computational overhead (Fu, 2024). Although recent studies have attempted to use heuristic rules to select the most important subset of context to improve the robustness and efficiency of ICL (Li et al., 2024c; Zhang et al., 2023c), these methods inevitably cannot ensure that the discarded tokens that are currently unimportant will not become important in future decoding steps. Other investigations have focused on constructing meta-ICL approaches to enhance ICL’s robustness and reduce reliance on perfect prompts (Coda-Forno et al., 2023). Yet, these methods remain constrained by limited context length and often require hand-crafted prompt strategies, potentially leading to suboptimal performance. On the other hand, recent studies suggest that ICL is actually performing a meta-gradient update for adapting to new tasks given the context information (Dai et al., 2023; von Oswald et al., 2022). These findings lead us to a crucial question: Instead of implicitly ""updating"" model parameters to adapt to a new domain or task via context, is it possible to directly convert the context into parameter updates, thus updating the network at test time without any backpropagation and without requiring demonstrations in the context window? To answer this question, we propose StreamAdapter, a novel approach that leverages the inherent capabilities of LLMs to encode context information into their parameters. Instead of storing demonstrations explicitly in the input context, StreamAdapter dynamically maps these demonstrations into temporary parameter updates. This approach allows the model to benefit from context to adapt to new tasks similar to ICL at test-time, without consuming the context window or requiring backpropagation, thereby reducing the resource requirements of traditional ICL methods. StreamAdapter employs two key mechanisms to achieve this goal: a) Context Mapping: This mechanism utilizes intra-chunk cross-attention and inter-chunk recurrence to adaptively condense the variable cached context into a constant context state for each parameter in the linear layer of LLMs. b) Weight Absorption: The condensed context state interacts with two lightweight low-rank matrices to be absorbed into the original model parameters. This process updates the LLM’s knowledge with minimal additional learnable parameters and incurs no additional inference latency. By combining these mechanisms, StreamAdapter effectively distills the context into parameter updates, allowing for more efficient test-time adaptation (TTA). Comprehensive experiments across diverse language understanding and long-context generation tasks, with various model architectures and scales, demonstrate that StreamAdapter achieves comparable or superior adaptation capability to full context evaluation while outperforming other context compression variants and TTA methods. Moreover, StreamAdapter not only demonstrates constant inference generation time and lower memory consumption compared to full context generation, but also shows better scalability when provided with more adaptation context and improved robustness across various scenarios. The contributions of our work can be summarized as follows: • We propose a new TTA strategy, StreamAdapter, that directly maps the given context into parameter updates, rather than conditioning on the context. This method enables models to quickly adapt to new tasks or acquire new temporary knowledge at test time like ICL, but with fewer or no demonstrations in context, thereby reducing memory consumption and inference time. • We design StreamAdapter with innovative context mapping and low-rank adaptation mechanisms. These allow StreamAdapter to map the context into parameter updates with minimal additional learning parameters and without inducing any additional inference latency. • We validate StreamAdapter on both language understanding and language generation tasks across various model scales and architectures. The results demonstrate the effectiveness of StreamAdapter over ICL and other TTA methods in various adaptation scenarios. Analyses of efficiency and robustness further highlight StreamAdapter’s advantages in terms of computational resources and generalization capabilities."
https://arxiv.org/html/2411.09273v1,Cross-Modal Consistency in Multimodal Large Language Models,"Recent developments in multimodal methodologies have marked the beginning of an exciting era for models adept at processing diverse data types, encompassing text, audio, and visual content. Models like GPT-4V, which merge computer vision with advanced language processing, exhibit extraordinary proficiency in handling intricate tasks that require a simultaneous understanding of both textual and visual information. Prior research efforts have meticulously evaluated the efficacy of these Vision Large Language Models (VLLMs) in various domains, including object detection, image captioning, and other related fields. However, existing analyses have often suffered from limitations, primarily centering on the isolated evaluation of each modality’s performance while neglecting to explore their intricate cross-modal interactions. Specifically, the question of whether these models achieve the same level of accuracy when confronted with identical task instances across different modalities remains unanswered. In this study, we take the initiative to delve into the interaction and comparison among these modalities of interest by introducing a novel concept termed cross-modal consistency. Furthermore, we propose a quantitative evaluation framework founded on this concept. Our experimental findings, drawn from a curated collection of parallel vision-language datasets developed by us, unveil a pronounced inconsistency between the vision and language modalities within GPT-4V, despite its portrayal as a unified multimodal model. Our research yields insights into the appropriate utilization of such models and hints at potential avenues for enhancing their design.","Figure 1: Visualization of the performance gap between the modality of text and image in seven different tasks. Recent large multimodal models have showcased remarkable capabilities in tasks that require the integration of multiple modalities and sources of information Huang et al. (2023). Among these, the performance of Vision Large Language Models (VLLMs) Zhang et al. (2023a); Yang et al. (2023) stands out, thanks to the vast amounts of image and text data available for training and the rapid progress in both computer vision and language modelling. However, due to the distinct training methodologies employed by these models, such as contrastive learning Radford et al. (2021); Jin et al. (2024) and embodied image-language modeling Driess et al. (2023), and the varying quality of training data for each modality Yin et al. (2023), these networks often exhibit performance disparities across different modalities. Previous research has extensively evaluated the performance of individual modalities in multimodal systems. For instance, Yang et al. (2023) conducted a thorough assessment of GPT-4V’s vision understanding capabilities, and Chen et al. (2023) analyzed model’s decision-making abilities. However, assessing a model’s performance on each individual modality in isolation does not fully evaluate its true multimodal abilities. It is possible, for example, for a model to excel in numerous vision tasks but still lag significantly behind in language understanding. Moreover, simply testing performance on individual tasks provides no insight into whether and how each modality of the model influences the others. Unfortunately, the cross-modality relationship is frequently overlooked in the aforementioned research. In this study, we go beyond the traditional approach of simply evaluating multimodal systems through separate downstream tasks and reporting their scores. Our focus is primarily on measuring the inherent differences in capabilities between various modalities, with special attention to vision and language, given their prominence among other modalities. To enable a comprehensive analysis, we introduce the concept of cross-modal consistency, complete with a formal definition and an evaluation framework. We consider cross-modal consistency to be an essential element in the design of complex multimodal systems with neural components, as it guarantees coherence and reliability in the system’s performance. This is crucial for both interpretability and for fostering user trust. We subsequently construct a comprehensive vision-language parallel dataset encompassing seven tasks, each designed to highlight different facets of vision and language capabilities. This dataset serves as a tool for evaluating the vision-language consistency of VLLMs. Our experiments with the GPT-4V model on the dataset reveal significant inconsistencies between its vision and language capabilities. The results indicate that its performance varies considerably depending on whether the same task instance is prompted in one modality versus the other. Our contributions are: (1) We introduce the novel concept of cross-modal consistency, along with a comprehensive evaluation framework. This approach transcends traditional assessment methods for multimodal models, which typically evaluate each modality in isolation. (2) We develop and release seven diverse datasets, carefully designed for vision-language consistency evaluation, opening up opportunities to exploit these datasets in future research. (3) Our experiments on GPT-4V reveal a significant disparity between vision and language abilities within such a system, prompting the introduction of the Vision-Depicting-Prompting (VDP) method as a potential remedy. Our findings offer valuable guidance for more effective future use of such multimodal models."
https://arxiv.org/html/2411.09255v1,DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form Text through a Benchmark Dataset in Biomedicine,"We introduce DAHL, a benchmark dataset and automated evaluation system designed to assess hallucination in long-form text generation, specifically within the biomedical domain. Our benchmark dataset, meticulously curated from biomedical research papers, consists of 8,573 questions across 29 categories. DAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs) by deconstructing responses into atomic units, each representing a single piece of information. The accuracy of these responses is averaged to produce the DAHL Score, offering a more in-depth evaluation of hallucinations compared to previous methods that rely on multiple-choice tasks. We conduct experiments with 8 different models, finding that larger models tend to hallucinate less; however, beyond a model size of 7 to 8 billion parameters, further scaling does not significantly improve factual accuracy. The DAHL Score holds potential as an efficient alternative to human-annotated preference labels, being able to be expanded to other specialized domains. We release the dataset and code in public111https://github.com/seemdog/DAHL.","The rapid advancements in Large Language Models (LLMs) have significantly propelled various fields, by enabling sophisticated natural language understanding and generation (Zhao et al., 2023). Nonetheless, these advancements also present challenges, with hallucination emerging as a prominent and inevitable concern in their evaluation and mitigation (Xu et al., 2024). Hallucinated responses from LLMs may contain inaccurate or biased information, leading to ethical issues (Rawte et al., 2023a). In specialized domains such as biomedicine, law, and finance, where factual precision is paramount, hallucination poses an even greater risk as it can result in the dissemination of false information with potentially severe consequences (Lakkaraju et al., 2022). Therefore, the evaluation and mitigation of hallucination are particularly crucial in these high-stakes domains (Rawte et al., 2023b). This concern is further emphasized in biomedical, clinical applications such as medical diagnosis, clinical report generation, medical language translation, and medical support (Zhou et al., 2024; Lee et al., 2023). As these applications rely on the purported encoding of biomedical knowledge by LLMs (Singhal et al., 2022), the development of a robust evaluation system is imperative for effectively addressing hallucination in biomedicine. There have been various endeavors to evaluate hallucination or the truthfulness of LLM outputs such as AutoHall (Cao et al., 2023), which automatically constructs model-specific hallucination datasets and HaloCheck (Elaraby et al., 2023), where a small blackbox model is used to detect the severity of hallucination. However, most existing trials predominantly focus on the general domain, as evidenced by extensive research efforts (Ji et al., 2023; Zhang et al., 2023). Although some domain-specific hallucination evaluation benchmarks and systems exist in the biomedical domain, they are often limited to multiple-choice tasks or rely heavily on human annotation (Pal et al., 2023; Liao et al., 2023). However, evaluating long-form text generation is crucial, particularly in biomedical applications where making binary quality judgments is not possible due to the presence of both accurate and inaccurate information in one response. Moreover, relying on human annotation is highly costly and time-consuming, necessitating the development of an automated hallucination evaluation system. To address these challenges, we propose DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form Text Generation, a benchmark dataset and automated system designed specifically for evaluating LLM hallucination within the biomedical domain. The dataset, comprising 8,573 questions generated based on research papers from PubMed Central(PMC), covers diverse biomedical literature encompassing 29 categories. Inspired by Factscore (Min et al., 2023), we break down the responses of LLMs when prompted with the questions into atomic units, each representing a single piece of information. We then compute the average factual accuracy of these atomic units, which we term as the DAHL Score. Our approach aims to automatically assess the factuality of long-form text generation at the atomic unit level. Since our hallucination evaluation dataset generation framework is automated, it can be readily expanded to other domains, and frequently updating the dataset with new knowledge source is possible. The contributions of this research are as follows: • Benchmark Dataset for Hallucination Evaluation in the Biomedical Domain: We introduce a domain-specific benchmark dataset tailored for evaluating hallucination in long-form text generation within the biomedical domain. Covering 29 categories, this dataset addresses the gap in existing evaluation resources, providing a nuanced insight into LLM hallucination in biomedical contexts. • Automated Evaluation System for Long-form Text Generation : We introduce a holistic system for evaluating LLM hallucination tendencies of long-form text generation in the biomedical domain through a fully automated assessment system, thereby minimizing the need for human annotation costs. • Framework for Scalability: We introduce a framework designed to automatically generate domain-specific questions dataset for hallucination evaluation sourced from reliable knowledge repositories. This framework is scalable to other domains and allows for dataset update to incorporate new knowledge. • Public Release of the Benchmark Dataset and Evaluation Pipeline: We publicly release our benchmark dataset in the field of biomedicine. Additionally, we open-source the code for our automated evaluation system, allowing the community to freely use it even with their own datasets. Using DAHL, we evaluate gpt-4o from OpenAI alongside 7 open-source models, comparing their performance with human evaluations to validate the reliability of our approach. In the following sections, we outline the construction of our benchmark dataset, the methodology behind our automated evaluation system, and the detailed results of our assessments across various LLMs. Our analysis includes the DAHL Score for models of different sizes within the same model family. Additionally, we conduct an ablation study to examine the effect of temperature during text generation. Through this research, we aim to contribute to the ongoing discourse on assessing the reliability of LLMs, particularly in domains where accurate and precise information is paramount. Additionally, by evaluating the model responses to each question, the DAHL Score can be used as a preference score to create a preference dataset for alignment tuning."
https://arxiv.org/html/2411.09214v1,HateGPT: Unleashing GPT-3.5 Turbo to Combat Hate Speech on X,"The widespread use of social media platforms like Twitter and Facebook has enabled people of all ages to share their thoughts and experiences, leading to an immense accumulation of user-generated content. However, alongside the benefits, these platforms also face the challenge of managing hate speech and offensive content, which can undermine rational discourse and threaten democratic values. As a result, there is a growing need for automated methods to detect and mitigate such content, especially given the complexity of conversations that may require contextual analysis across multiple languages, including code-mixed languages like Hinglish, German-English, and Bangla. We participated in the English task where we have to classify English tweets into two categories namely Hate and Offensive and Non Hate-Offensive. In this work, we experiment with state-of-the-art large language models like GPT-3.5 Turbo via prompting to classify tweets into Hate and Offensive or Non Hate-Offensive. In this study, we evaluate the performance of a classification model using Macro-F1 scores across three distinct runs. The Macro-F1 score, which balances precision and recall across all classes, is used as the primary metric for model evaluation. The scores obtained are 0.756 for run 1, 0.751 for run 2, and 0.754 for run 3, indicating a high level of performance with minimal variance among the runs. The results suggest that the model consistently performs well in terms of precision and recall, with run 1 showing the highest performance. These findings highlight the robustness and reliability of the model across different runs.","The advent of social media platforms such as Twitter (currently known as X) and Facebook has revolutionized the way individuals communicate, enabling people from diverse backgrounds to share their thoughts, experiences, and opinions freely [1]. This democratization of content creation has led to an exponential increase in user-generated data. While these platforms have facilitated global connectivity and discourse, they have also become hotbeds for hate speech [2, 3, 4, 5, 6] and offensive content. Such content not only disrupts meaningful communication but also poses significant threats to social cohesion and democratic values. Addressing the proliferation of hate speech [7, 8, 9, 10] on social media is a complex challenge. The nature of online communication, where context and nuance often play a crucial role, makes it difficult to detect offensive language accurately [11, 12, 13]. This challenge is further compounded by the multilingual nature of online communities, where users frequently employ code-mixed languages such as Hinglish (a mix of Hindi and English), German-English, and Bangla, among others [14, 15]. As these languages blend cultural and linguistic elements, the task of identifying hate speech becomes even more intricate. In response to this growing concern, technology companies and social media platforms have begun to invest in automated methods to detect and manage offensive content [2, 16, 17, 18]. The goal is to strike a balance between preserving open and free dialogue while preventing the spread of harmful speech. In this work, we focus on the classification of English tweets into two categories: Hate and Offensive and Non Hate-Offensive. By leveraging state-of-the-art large language models such as GPT-3.5 Turbo, we experiment with prompting techniques to classify tweets accurately. Run 1 achieved the highest Macro-F1 score at 0.756, indicating it balanced precision and recall across different classes better than the other runs. Run 2 had a slightly lower score of 0.751, suggesting a small decline in performance, either in precision, recall, or both. Run 3 scored 0.754, which was slightly lower than Run 1 but higher than Run 2, indicating its performance was similar to Run 2’s with a minor improvement."
https://arxiv.org/html/2411.09213v1,Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering,"Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs’ ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models’ limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs’ reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.","Large language models (LLMs) have demonstrated remarkable capabilities in solving complex medical problems, achieving state-of-the-art performance across various benchmarks. However, ensuring the reliability and truthworthiness of an artificial intelligent (AI) medical system remains a critical challenge, especially in healthcare applications. Retrieval-augmented generation (RAG) has emerged as a promising approach to reduce LLMs’ hallucination problem by integrating external knowledge sources. Figure 1: Blue texts are useful information that should be extract to help determine the answer. Red texts are factual errors that potentially mislead the LLMs. While RAG has potential to improve the factual accuracy of LLMs’ response, incorporating an information retriever also presents new complexities that warrant careful evaluation. Consider the example in Fig. 1. The retrieved documents can contain not only useful knowledge that helps determine the true answer, but also noise information, or more serious, factual errors that can misleads the LLMs. To consciously apply RAG for medical QA, we must consider these practical scenarios and evaluate LLMs ability to interact with retrieved documents reliably. Recent efforts have been made to evaluate AI systems with LLMs in the medical domain (Nori et al. 2023; He et al. 2023; Xiong et al. 2024). For example, MedEval (He et al. 2023) presents a large-scale, expert-annotated benchmark that cover various medical tasks and domains. (Xiong et al. 2024) evaluates RAG extensively based on their MIRAGE benchmark which cover 5 medical QA datasets. However, they only focus on the effect of RAG modules on target accuracy, missing other important aspects of a AI medical system. Several recent works have explore RAG evaluation more comprehensively in general domain (Es et al. 2023; Chen et al. 2024b), RAGAS (Es et al. 2023) assess 3 qualities of RAG’s outputs for QA tasks including: Faithfulness - degree to which responses align with the provided context, Answer Relevance - the extent to which generated responses address the actual question posed, and Context Precision-Recall - the quality of retrieved context. We follow the work from (Chen et al. 2024b) which establishes Retrieval-Augmented Generation Benchmark (RGB) to measure 4 abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. In particular, using questions from 4 medical QA datasets from MIRAGE as basis, we create Medical Retrieval-Augmented Generation Benchmark (MedRGB) to evaluate RAG system in the following 4 test scenarios: • Standard-RAG: evaluates LLMs performance when presented with multiple retrieved signal documents to create a context to answer to question. • Sufficiency: evaluates LLMs reliability when there are noise documents within the retrieved context. By adding ”Insufficient Information” as an additional response option, LLMs should only answer when they are confident to have enough information to determine the correct answer. This requires LLMs to not only be aware of its own internal knowledge, but also be able to filter out noisy information from external documents. • Integration: evaluates LLMs ability to answer multiple supporting questions and integrate the extracted information to help address the main question. • Robustness: evaluates LLMs resiliency to factual errors in the retrieved context. A trustworthy AI medical system should be able detect factually incorrect documents and provide the corrected information. In total, MedRGB consists of 3480 instances for 4 test scenarios, which is over 5 times that of RGB. Using MedRGB, we evaluation 7 LLMs, including both state-of-the art commercial LLMs and open-source models. In summary, our contributions are three-fold: • We establish MedRGB with four test scenarios to evaluate LLMs for medical QA tasks in RAG settings. To best of our knowledge, it is the ﬁrst benchmark comprehensively assess medical RAG systems in these practical setting. • Using MedRGB, we extensively evaluate 7 LLMs, including both state-of-the art commercial LLMs and open-source models, across multiple RAG conditions. Experiment results demonstrate their limitation in addressing the more complex scenarios. • We analyzed the errors of the LLMs and their reasoning process to provide insights and suggest future directions for developing more reliable and trustworthy medical RAG systems. Figure 2: The overall construction process of MedRGB. The green OpenAI symbol implies that the block involves data generation using the GPT-4o model."
https://arxiv.org/html/2411.09166v1,Unstructured Text Enhanced Open-domain Dialogue System: A Systematic Survey,"Incorporating external knowledge into dialogue generation has been proven to benefit the performance of an open-domain Dialogue System (DS), such as generating informative or stylized responses, controlling conversation topics. In this article, we study the open-domain DS that uses unstructured text as external knowledge sources (Unstructured Text Enhanced Dialogue System, UTEDS). The existence of unstructured text entails distinctions between UTEDS and traditional data-driven DS and we aim to analyze these differences. We first give the definition of the UTEDS related concepts, then summarize the recently released datasets and models. We categorize UTEDS into Retrieval and Generative models and introduce them from the perspective of model components. The retrieval models consist of Fusion, Matching, and Ranking modules, while the generative models comprise Dialogue and Knowledge Encoding, Knowledge Selection, and Response Generation modules. We further summarize the evaluation methods utilized in UTEDS and analyze the current models’ performance. At last, we discuss the future development trends of UTEDS, hoping to inspire new research in this field.","Dialogue systems (DS) attract great attention because of its wide application prospects. Early DS such as Eliza (Weizenbaum, 1966), Parry (Colby et al., 1971), and Alice (Fischer and Grodzinsky, 1993) attempted to imitate human behaviors in conversations and challenge different forms of the Turing Test (Turing, 2009). They worked well but only in constrained environments, an open-domain DS remained an elusive task until recently (Huang et al., 2020). One of the main challenges in open-domain DS is that the generated responses lack sufficient information (Li et al., 2016a). Previous researchers proposed different methods to alleviate this issue. Diversity enhancement strategies (Zhao et al., 2017; Holtzman et al., 2020; Gao et al., 2019a; Welleck et al., 2020) and large scale parameters (Radford et al., 2019; Zhang et al., 2020d; Brown et al., 2020) have been proven to be effective. In addition to the above methods, introducing external knowledge, such as structured knowledge graph (KG) (Parthasarathi and Pineau, 2018; Agarwal et al., 2018; Wu et al., 2020, 2019) or unstructured text (Ghazvininejad et al., 2018; Parthasarathi and Pineau, 2018; Zhang et al., 2018a; Dinan et al., 2019; Zhou et al., 2018a; Yang et al., 2018b), into the dialogue generation attracts great attention. External knowledge acts as an extra resource besides the context of dialogues and provides more diversified and grounded information for dialogue generation. Structured knowledge usually means Knowledge Graph111Following Ji et al. (2021), we use the terms knowledge graph and knowledge base interchangeably in this paper., which is a semantic map obtained after defining and extracting the named entities and the relations (Zhou et al., 2018b; Yang et al., 2019b) in unstructured text. It was first proposed to better understand the natural language used in search engine222https://www.blog.google/products/search/introducing-knowledge-graph-things-not-strings. Although it has been widely applicated in natural language processing (NLP) (Dong et al., 2014; Nickel et al., 2016a; Wang et al., 2017; Hogan et al., 2021) and achieved great success, some researchers pointed out that complex reasoning (Qu and Tang, 2019; Zhang et al., 2020a), unified representation framework (Liu et al., 2017; Hayashi and Shimbo, 2017; Han et al., 2018), interpretability (Xie et al., 2017; Zhang et al., 2019a), scalability (Nickel et al., 2016b; Yang et al., 2017), knowledge aggregation (Petroni et al., 2019), and knowledge automatic extraction (Ji et al., 2021) were still facing challenges. Besides the traditional KG, there are other forms of knowledge, to some extent, that can be seen as structured and used as external knowledge. For example, Event Graph (Xu et al., 2020a) or Event Logic Graph (Ding et al., 2019) uses event descriptions (a sentence or a predicate phrase) as vertices and logical relations (causal, sequential, etc.) between events as edges; Enhanced KG (Zhou et al., 2020; Lin et al., 2020a) takes a KG as its backbone and aligns unstructured text to related entities; Chen et al. (2020b) used Table Text (Lebret et al., 2016) as external knowledge for a data-to-text generation task. Table 1. Classification according to the properties of text. Category Examples Factual description Wikipedia articles (Dinan et al., 2019; Qin et al., 2019), News reports (Gopalakrishnan et al., 2019; Hedayatnia et al., 2020), Domain specific knowledge (Szpektor et al., 2020) Fictional information Stories (Xu et al., 2020a), Novel (Gao et al., 2019b), Persona information (Zhang et al., 2018a), Virtual world description (Urbanek et al., 2019). Movie script (Tigunova et al., 2019), Movie plots (Zhou et al., 2018a), TV dialogues (Su et al., 2019), Emotions (Rashkin et al., 2019). Subjective comments Reviews of goods (Ghazvininejad et al., 2018) or movies (Moghe et al., 2018), Reddit comments (Yoshino et al., 2019), Interview (Majumder et al., 2020b) Unstructured text can be Factual description, Fictional information, or Subjective comments, as shown in Table 1. Factual descriptions are statements of objective facts such as news reports (Gopalakrishnan et al., 2019), historical events (Dinan et al., 2019) and domain specific knowledge (Szpektor et al., 2020). Fictional information is usually artificially constructed text such as stories (Xu et al., 2020a), movie script (Tigunova et al., 2019) or TV dialogues (Su et al., 2019). Subjective comments are usually reviews of things in the world that are recognized by more people on social networks (Ghazvininejad et al., 2018; Moghe et al., 2018). Generally, comments with a high degree of recognition have high authenticity. Apart from the categories above, unstructured text can also be divided into independent sentences and documents. Independent sentences usually do not have logical relationships between themselves (Vougiouklis et al., 2016; Ghazvininejad et al., 2018; Zhang et al., 2018a). In contrast, a document contains multiple logically related sentences, which together constitute a description of the topic of the document (Zhou et al., 2018a; Moghe et al., 2018; Qin et al., 2019). But in the current stage, most works treat them in a similar way. Hence in this paper, we use a unified term ”unstructured text” to integrate them. But we do believe that there are more sophisticated semantic structures that can be extracted from documents in future work. Table 2 presents examples of independent sentences and a document in UTED tasks. The dialogue using document as external knowledge is also called document-grounded conversation (DGC) (Zhou et al., 2018a) or background-based conversation (BBC) (Meng et al., 2020b). Notably, the Conversational QA is also based on background documents. The differences between DGC/BBC and Conversational QA (such as CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018) and MANTIS (Penha et al., 2019)) are the dialogue of DGC/BBC is more diversified (including chit-chat or recommendation) and not limited to QA. Table 2. The examples of independent sentences and a document as external knowledge in UTED tasks. The information used in the dialogue is marked with the same color and font in the external text. Sentences as external knowledge (Persona-Chat (Zhang et al., 2018a)) ”I am an artist”; ”I have four children”; ”I recently got a cat”;”I enjoy walking …”; ”I love watching Game of Thrones” Speaker 1: My children and I were just about to watch Game of Thrones. Speaker 2: Nice! How old are your children? Speaker 1: I have four that range in age from 10 to 21. You? Speaker 2: I do not have children at the moment. Document as external knowledge (CMUDoG (Zhou et al., 2018a)) The Shape of Water is a 2017 American fantasy drama film directed by Guillermo del Toro … It stars Sally Hawkins … Set in Baltimore in 1962, the story follows a mute custodian at a high-security government laboratory who falls in love with a captured humanoid amphibian creature. Rating Rotten Tomatoes: 92 % and average: 8.4/10 … Critical Res- ponse: one of del Toro’s most stunningly successful works, a powerful vision of a creative master feeling totally Speaker 1: I thought The Shape of Water was one of Del Toro’s best works. What about you? Speaker 2: Yes, his style really extended the story. Speaker 1: I agree. He has a way with fantasy elements that really helped this story be truly beautiful. Speaker 2: It has a very high rating on rotten tomatoes, too. I don’t always expect that with movies in this genre. Compared with structured knowledge such as the KG, unstructured text has the following advantages. Firstly, the unstructured text contains diversified information, such as commonsense knowledge (Majumder et al., 2020a), stylized information (Gao et al., 2019b; Su et al., 2020), syntactic structures (Du and Black, 2019b), and event logic (Xu et al., 2020a). Secondly, unstructured text is ubiquitous, continuously updated and easy to obtain due to the rapidly increased web-page content (Ren et al., 2020b). These advantages indicate that unstructured text owns a greater potential to serve as external knowledge in DS than structured knowledge. Hence, incorporating unstructured text into dialogue attracted the attention of academia (e.g. the Alexa Prize challenge (Ram et al., 2018), Dialogue System Technology Challenges (DSTC) (Yoshino et al., 2019; Kim et al., 2021)) and industry (Roller et al., 2021; Szpektor et al., 2020; Kim et al., 2020b; Gao et al., 2020). Thanks to the progressing of the semantic representation learning technology such as pre-trained models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018, 2019; Liu et al., 2019b; Yang et al., 2019a; Dong et al., 2019; Lewis et al., 2020), leveraging knowledge in unstructured text to build an informative and engaging dialogue agent have witnessed great improvement (Ghazvininejad et al., 2018; Dinan et al., 2019; Guu et al., 2020; Szpektor et al., 2020). In this paper, we give a detailed investigation of the Unstructured Text Enhanced Dialogue System (UTEDS). Although UTEDS is considered as a promising research direction, the earlier survey papers in DS research (Chen et al., 2017b; Serban et al., 2018; Shum et al., 2018; Yan, 2018; Huang et al., 2020; Deriu et al., 2021) did not cover this topic. Recent literature reviews in natural language generation (NLG) research noticed the emerging of UTEDS. Guo et al. (2021) focused on the conditional-NLG technology and introduced knowledge-enhanced text generation. Santhanam and Shaikh (2019) focused on the NLG techniques in DS and introduced several models incorporating world knowledge into dialogue generation. These work only partially reviewed some related UTED models. Most recently, Yu et al. (2020) considered a variety of different knowledge-enhanced text generation tasks and answered two questions: how to acquire knowledge and how to incorporate different forms of knowledge to facilitate text generation. However, their investigation about free-form text grounded DS was also incomplete. In contrast, we focus on DS with unstructured text as external knowledge and give a detailed and systematic survey in this domain. We give definitions of the UTEDS related concepts, divide UTEDS into retrieval and generative models and summarize current models with a unified paradigm. We introduce the related datasets, specific architectures, methods, training objects, and evaluation metrics used in UTEDS. Then we summarize and analyze the experimental results of current models. In addition, we leverage the modules defined in this paper to point out the future research trends of UTEDS. As far as we know, we are the first to make a systematic review of the UTEDS. We believe that incorporating unstructured text information into dialogue generation is the future trend of the open-domain DS because a large amount of human knowledge is contained in these raw texts. The research of the UTEDS can assist machines in utilizing human knowledge stored on the internet and understanding natural language. The structure of the rest of the paper is as follows: • In Section 2, we review the UTED datasets. • In Section 3, we present the system components of Retrieval approaches. • In Section 4, we outline the system components of the Generative approaches. • In Section 5, we summarize the current evaluation metrics. • In Section 6, we analyze the current models’ performance in UTED. • In Section 7, we put forward promising research directions."
https://arxiv.org/html/2411.09125v1,"DROJ: A Prompt-Driven Attack against 
Large Language Models","Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks. Due to their training on internet-sourced datasets, LLMs can sometimes generate objectionable content, necessitating extensive alignment with human feedback to avoid such outputs. Despite massive alignment efforts, LLMs remain susceptible to adversarial jailbreak attacks, which usually are manipulated prompts designed to circumvent safety mechanisms and elicit harmful responses. Here, we introduce a novel approach, Directed Rrepresentation Optimization Jailbreak (DROJ), which optimizes jailbreak prompts at the embedding level to shift the hidden representations of harmful queries towards directions that are more likely to elicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat model show that DROJ achieves a 100% keyword-based Attack Success Rate (ASR), effectively preventing direct refusals. However, the model occasionally produces repetitive and non-informative responses. To mitigate this, we introduce a helpfulness system prompt that enhances the utility of the model’s responses. Our code is available at https://github.com/Leon-Leyang/LLM-Safeguard.","Large Language Models (LLMs) are powerful conversational systems that have demonstrated significant potential across numerous domains, including content generation, data analysis, and the healthcare industry. Their remarkable performance is largely due to training on extensive text datasets, which enables them to generate high-quality responses on a wide range of topics. However, because these datasets are often sourced from internet materials that may contain inappropriate content, there is a risk of such content appearing in the model’s outputs (Ousidhoum et al., 2021). To mitigate this risk, LLMs typically undergo extensive safety alignment through human feedback during development to prevent harmful or inappropriate responses (Anwar et al., 2024). Despite these safety measures, LLMs—such as the widely used ChatGPT—remain vulnerable to adversarial attacks. Upon its release, vulnerabilities were exploited using carefully crafted prompts that elicited responses capable of spreading misinformation, hate speech, and other harmful content, posing significant societal risks (Zvi, 2022). Recent research on enhancing the adversarial robustness of LLMs has largely focused on two approaches: (1) developing defense mechanisms to improve models’ ability to detect and decline malicious queries (Xie et al., 2023; Alon & Kamfonas, 2023), and (2) designing new jailbreak attacks, which often involve specially optimized prefixes or suffixes added to malicious queries (Lapid et al., 2023; Wang et al., 2024; Wei et al., 2023). One common safeguarding technique is the use of a safety prompt at the system level, which typically includes explicit instructions to ensure safe outputs and prevent harmful responses, as illustrated in Figure 1. This approach has been successfully implemented in various LLMs, including Mistral (Jiang et al., 2023) and GPT-4 (Achiam et al., 2023). Further research by Zheng et al. (2024) examined the effect of safety prompts on LLMs’ representational space, revealing that both harmful and benign queries are shifted toward a refusal direction in the model’s representation space, where the LLM is less likely to provide responses, as depicted in Figure 2. Building on these findings, we introduce Directed Representation Optimization Jailbreak (DROJ), a novel jailbreak method that optimizes prompts to shift representations of both harmful and benign queries toward directions more likely to elicit compliance from the model. Figure 1: Prepending a safety prompt (right) to the input query can help the model refuse to respond to malicious prompts it might otherwise comply with. Figure 2: Visualization of the hidden state of Mistral-7B-Instruct-v0.1 after applying 2-dimensional Principal Component Analysis (PCA). Left: Visualization of eight groups of queries (harmful/harmless + three kinds of safety prompts). The boundary (black dotted line) is fitted by logistic regression, showing that harmful and harmless queries can be distinguished without safety prompts. The red and blue arrows indicate the similar movement direction of both harmful and harmless queries, respectively, when safety prompts are added. Right: Color represents the empirical refusal rate, with the fitted boundary (gray line) indicating separation of accepted and refused queries. The gray arrow (normal vector of the fitted regression) indicates the direction of higher refusal probability. Figure reproduced from (Zheng et al., 2024)."
https://arxiv.org/html/2411.09116v1,": A Parallel Multilingual Multitask Benchmark
for Consistent Evaluation of LLMs","Recent advancements in large language models (LLMs) showcase varied multilingual capabilities across tasks like translation, code generation, and reasoning. Previous assessments often limited their scope to fundamental natural language processing (NLP) or isolated capability-specific tasks. To alleviate this drawback, we aim to present a comprehensive multilingual multitask benchmark. First, we present a pipeline for selecting available and reasonable benchmarks from massive ones, addressing the oversight in previous work regarding the utility of these benchmarks, i.e., their ability to differentiate between models being evaluated. Leveraging this pipeline, we introduce P-MMEval, a large-scale benchmark covering effective fundamental and capability-specialized datasets. Furthermore, P-MMEval delivers consistent language coverage across various datasets and provides parallel samples. Finally, we conduct extensive experiments on representative multilingual model series to compare performances across models, analyze dataset effectiveness, examine prompt impacts on model performances, and explore the relationship between multilingual performances and factors such as tasks, model sizes, and languages. These insights offer valuable guidance for future research. The dataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.","In recent years, large language models (LLMs, Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023; Bai et al., 2022, 2023) have raised significant interest in the artificial intelligence (AI) community. As most LLMs are English-centric, when we focus on the performances of a specific LLM, it generally refers to the evaluation results on English benchmarks. For example, early research focuses on reporting evaluation results on fundamental natural language processing (NLP) benchmarks. i.e, how accurately the LLM understands and generates text, including TriviaQA Joshi et al. (2017a), WinoGrande Sakaguchi et al. (2020), and HellaSwag Zellers et al. (2019). Nowadays, researchers are more interested in capability-specialized benchmarks, i.e., how well LLM performs on a group of specific task-solving problems, including GSM8K Cobbe et al. (2021) for mathematical reasoning, MMLU Hendrycks et al. (2021a) for knowledge acquisition, and HumanEval Chen et al. (2021) for code generation. However, there is currently little work on systematically evaluating the multilingual capabilities of LLMs. When developing and iterating LLMs, giving accurate and parallel evaluation results is crucial for identifying their multilingual capabilities and quantifying their performances. Building a benchmark with both inclusive task coverage and strong linguistic parallelism is difficult. Measuring the multilingual abilities of a specific LLM, or comparing the quality of generated multilingual responses from one LLM to another, remains a big challenge in developing multilingual LLMs. Early work focuses on an isolated evaluation pipeline for a specific task, or to be more concrete, a specific perspective of LLM abilities: MHellaSwag Dac Lai et al. (2023) aims at collecting the multilingual understanding abilities, XLSum Hasan et al. (2021) mainly focus on evaluating the quality of generated multilingual text, HumanEval-XL Peng et al. (2024) is used for quantify how well-executed the generated code segments are, and MGSM Shi et al. (2023) is made for testifying the performance on arithmetic reasoning. In modern research, for delivering simpler aggregation and comprehensive evaluation when judging model abilities, researchers collect several popular isolated benchmark tasks and propose a united, large-scale multilingual benchmark system like XTREME Hu et al. (2020), XTREME-R Ruder et al. (2021), XGLUE Liang et al. (2020), MEGA Ahuja et al. (2023), and BUFFET Asai et al. (2024) for multi-task assessments. However, these large-scale benchmarks 1) are tailored predominantly to fundamental NLP tasks and 2) inconsistently cover multiple languages across their selected datasets. In this paper, our goal is to present a pipeline to develop a comprehensive multilingual multitask benchmark. To this end, we first select representative and challenging datasets from fundamental NLP tasks to reduce redundant testing and enhance the efficiency of evaluation. The second phase of our endeavor involves a meticulous curation of the most intensely studied capability-specialized tasks in contemporary research including code generation, knowledge comprehension, mathematical reasoning, logical reasoning, and instruction following. Finally, we construct a collection of datasets P-MMEval, consisting of three fundamental NLP datasets and five advanced capability-specialized datasets. To maintain language coverage among all selected datasets, we unify 10 languages considering the cost and computational limitations via expert translation review to construct the missing multilingual portions. To summarize, our contributions are as follows: • We present a pipeline for selecting available and reasonable benchmarks to assess the multilingual abilities of LLMs. Innovatively, we employ a statistical analysis method to identify effective datasets from a collection of datasets. Our method can enhance the objectivity and scientific rigor of the selection process. • We develop a multilingual multi-task benchmark P-MMEval that includes both fundamental and capability-specialized tasks, which ensures consistent language coverage across various datasets and provides parallel samples across different languages. This benchmark facilitates a thorough assessment of multilingual capabilities and enables unprecedented fairness and consistency in evaluating cross-lingual transfer capabilities. • Our experiments offer a comprehensive analysis of the multilingual capabilities of various LLMs, showcasing performance across different prompts, models, languages, and tasks. Importantly, we analyze the utility of each dataset within P-MMEval in distinguishing model performance, thus identifying specific benchmarks that differentiate model performance across model series and sizes. Source Task Benchmarks # Examples Test sets Metric Existing Generation Flores-200 Costa-jussà et al. (2022) 1012 \times 10 Annotation BLEU Extension Understanding XNLI Conneau et al. (2018) 120 \times 10 (3) Translation Acc MHellaSwag Dac Lai et al. (2023) 120 \times 10 (3) Translation Acc Code generation HumanEval-XL Peng et al. (2024) 80 \times 10 (3) \times 12 Translation Pass@1 Mathematical reasoning MGSM Shi et al. (2023) 250 \times 10 (3) Translation Acc Logic reasoning MLogiQA Liu et al. (2020) 80 \times 10 (8) Translation Acc Knowledge MMMLU Hendrycks et al. (2021a) 400 \times 10 (2) Translation Acc Instruction following MIFEval Zhou et al. (2023) 96 \times 10 (9) Translation Acc Table 1: An overview of the P-MMEval benchmark. In total, P-MMEval takes seven multilingual tasks into consideration, which is built on eight benchmarks. “# Examples” denotes “the number of examples per language” \times “the number of involved languages” \times “the number of programming languages” (special for HumanEval-XL), and the numbers of extended languages are in parentheses. “Test sets” section describes the nature of the test sets (whether they are translations of English data or independently annotated)."
https://arxiv.org/html/2411.09109v1,Personalized Help for Optimizing Low-Skilled Users’ Strategy,"AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment Cicero, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances even surpass them. The mere presence of advice can be advantageous, even if players do not follow it.","1 Leveraging Human-AI Collaboration AI and humans are frequent collaborators: in writing Lee et al. (2022), making decisions Bansal et al. (2019), or creating artwork Kim et al. (2022a). The most fruitful collaborations are those where humans and computers have complementary skills. We posit that the board game Diplomacy is an apt testbed for studying this type of collaboration. Wongkamjan et al. (2024) study Cicero Bakhtin et al. (2022), the best Diplomacy-playing AI capable of communicating in natural language, and show that while the state-of-the-art ais have find near-optimal moves, human players remain better at communication. Figure 1: Our advisor pholus generates move and message advice based on the game state and the player’s message history. This example shows a player’s opening moves and advice at the start of the turn. The player takes the message advice, proposing an alliance with Italy. Instead of fully accepting move advice committing two units to fight Turkey, the player uses one to defend against potential Russian threat. Changed opening moves are highlighted in the final decision. We introduce Personalized Help for Optimizing Low-Skilled Users’ Strategy(pholus),111We use the name pholus because he was a centaur, a mythological combination of a human and a horse. After his defeat by Deep Blue, Gary Kasparov Wilkenfeld (2019) advocated for “centaur chess”—where humans and computers play together—as a way of maintaining competitive games. a natural language agent based on Cicero Bakhtin et al. (2022) that provides both move and message advice to Diplomacy players in real-time. We run a user study to collect a dataset with 12 games, 1070 player turns, and 117 playing hours. pholus enables novices—who barely know the rules of Diplomacy—to compete with experts (Figure 2). But this does not simply mean the novices blindly follow the advice. First, they use pholus’s strategic insights to inform their communication strategies with other players. Second, pholus helps experienced players, although they are less inclined to take the advice than the novices. Overall, both advice types from pholus improve players’ game outcomes (Section 3.1)."
https://arxiv.org/html/2411.09073v1,Code-mixed LLM: Improve Large Language Models’ Capability to Handle Code-Mixing through Reinforcement Learning from AI Feedback,"Code-mixing(CM) or code-switching(CSW) refers to the juxtaposition of linguistic units from two or more languages during the conversation or sometimes even a single utterance. Code-mixing introduces unique challenges in daily life, such as syntactic mismatches and semantic blending, that are rarely encountered in monolingual settings. Large language models (LLMs) have revolutionized the field of natural language processing (NLP) by offering unprecedented capabilities in understanding human languages. However, the effectiveness of current state-of-the-art multilingual LLMs has not yet been fully explored in the CM scenario. To fill this gap, we first benchmark the performance of multilingual LLMs on various code-mixing NLP tasks. Then we propose to improve the multilingual LLMs’ ability to understand code-mixing through reinforcement learning from human feedback (RLHF) and code-mixed machine translation tasks. Given the high-cost and time-consuming preference labeling procedure, we improve this by utilizing LLMs as annotators to perform the reinforcement learning from AI feedback (RLAIF). The experiments show the effectiveness of the proposed method.","Code-mixing or code-switching is a linguistic phenomenon where two or more languages are mixed together within a single utterance, conversation, or speech. It allows individuals to convey culturally specific concepts, forge connections or differentiate themselves from others, and reinforce their identities. Code-mixing is common in multilingual societies, with recent studies indicating that up to 20% of online content in regions like South Asia, parts of Europe, and Singapore is code-mixed. In regions with significant bilingual or multilingual populations, code-mixing is more than a cultural expression; it is a core component of everyday communication. This widespread use highlights the necessity for NLP systems to process and interpret code-mixed language accurately. In recent years, large language models (LLMs) have shown promising performance in comprehending, producing, and interacting with human language. Trained on extensive text corpora, these models can capture a broad range of linguistic patterns and subtleties. Relying on the corpus containing independent monolingual texts in different languages, these LLMs have also achieved notable success in multilingual contexts. As a result, these multilingual LLMs have enabled cross-lingual transfer, which has proven to be a valuable method to leverage resources from high-resource languages to improve downstream task performance for other languages which have similar linguistic structures. To some extent, this property of multi-lingual large language models makes them a natural choice to handle code-mixing Yet, existing multilingual LLMs are not specifically trained with objectives for managing CM scenarios or large-scale corpus only including CM texts. The effectiveness of current state-of-the-art multilingual LLMs has not yet been explored and analyzed on code-mixed text fully. Hence, assessing the capabilities of the current multilingual LLMs to deal with code-mixing is essential. The main challenges of developing multilingual LLMs optimized for CM are not limited to the imbalanced language ratio in the pre-training corpora, the data scarcity for CM in different languages, and the trade-off between model capacity and language coveragePhilippy et al. (2023). Previous work focuses on data augmentation for CM data, fine-tunes LLMs in specified code-mixed language pairs Zhang et al. (2023b). These methods do not utilize the text-generation capabilities of LLMs that are fundamentally designed for. The adoption of these prompt-based multilingual LLMs for code-mixing remains a challenge. This observation motivates us to explore - Can existing multilingual LLMs effectively understand CM? Are there generalized ways to improve the LLMs’ capability of dealing with CM? In this paper, we assess the capabilities of the current LLMs in processing code-mixed texts through various NLP tasks. Experiment results indicate that models with the additional fine-tuning step outperform these prompted-based LLMs. We also prove that these prompt-based multilingual LLMs are not good at dealing with code-mixed inputs. This benchmarking could serve as the theoretical basis for choosing small models or prompt-based LLMs when dealing with code-mixed input. Additionally, we develop a class of code-mixed LLMs through the model alignment. To the best of our knowledge, we are the first to attempt model alignment for the code-mixing scenario. To be more specific, we apply reinforcement learning from AI feedback (RLAIF) to the CM scenario. From experiment results, this method not only has the potential to greatly improve LLMs’ ability to handle code-mixed languages, but it also creates new opportunities for creating more adaptable and human-centric AI systems."
https://arxiv.org/html/2411.08979v1,"CoCoP: Enhancing Text Classification with LLM 
through Code Completion Prompt","Text classification is a fundamental task in natural language processing (NLP), and large language models (LLMs) have demonstrated their capability to perform this task across various domains. However, the performance of LLMs heavily depends on the quality of their input prompts. Recent studies have also shown that LLMs exhibit remarkable results in code-related tasks. To leverage the capabilities of LLMs in text classification, we propose the Code Completion Prompt (CoCoP) method, which transforms the text classification problem into a code completion task. CoCoP significantly improves text classification performance across diverse datasets by utilizing LLMs’ code-completion capability. For instance, CoCoP enhances the accuracy of the SST2 dataset by more than 20%. Moreover, when CoCoP integrated with LLMs specifically designed for code-related tasks (code models), such as CodeLLaMA, this method demonstrates better or comparable performance to few-shot learning techniques while using only one-tenth of the model size. The source code of our proposed method will be available to the public upon the acceptance of the paper.","Large Language Models (LLMs) show commendable performance in both classification and generation tasks. However, to enhance their performance, fine-tuning task-specific data is imperative. The fine-tuning process requires collecting substantial, high-quality data pertinent to the domain task. Also, fine-tuning is expensive and may not be suitable for all situations (Kaddour et al., 2023). On the other hand, in-context learning techniques, such as few-shot learning, offer more flexibility (Brown et al., 2020). Unlike fine-tuning, these techniques do not require extensive domain-specific data and work only with a few examples (Brown et al., 2020). It makes them accessible for anyone to use during inference. It is important to emphasize that the effectiveness of these methods relies on the context and examples given as input. LLMs are trained on massive datasets that include diverse input styles. LIMA (Zhou et al., 2024) claims that if a model can learn the style of interaction with the user, it can reach superior performance in alignment and effectively communicate its knowledge to the user. This concept works in the reverse direction as well. When users interact with LLMs using prompt structures familiar to LLMs, the model demonstrates better performance. The efficacy of a prompt format for the model is contingent on its training data. Code-related datasets exist on the internet and are among the most critical resources for the pre-training phase of LLMs. For instance, LLaMA (Touvron et al., 2023a) is pre-trained on a substantial 328 GB dataset sourced from GitHub, constituting 4.5% of the overall model training data. Additionally, Google has mentioned that code-related data are one of the most important parts of their pre-training data for the Gemini model (Team et al., 2023). Choosing code-related datasets highlights the importance of teaching LLMs about coding style and code creation. Moreover, LLMs show good performance in code-related benchmarks. For instance, both LLaMA and LLaMA2 (Touvron et al., 2023b) have reported commendable results in code-related benchmarks. The proficiency of LLMs in code-related tasks can be leveraged for tasks beyond coding. To enhance LLMs performance in classification tasks, we introduce the Code Completion Prompt (CoCoP) method in this paper. Our suggested method uses LLMs’ capabilities in code-related tasks, particularly in code completion. Additionally, we employ the in-context learning in this method. CoCoP creates incomplete-code which consists of a few demonstrations in code format. Next, the LLM completes the code to determine the proper label for a user query. Thus, CoCoP is founded on the strengths of LLMs’ capabilities in code completion and in-context learning. The effectiveness of our method is evaluated across various classification datasets using LLaMA2 and CodeLLaMA models (Roziere et al., 2023), revealing superior performance compared to the few-shot learning method in classification tasks. Also, our experiments indicate that LLMs designed for code-related tasks (code models) demonstrate superior performance with this method compared to other LLMs. The source code of our proposed method will be available to the public upon the acceptance of the paper. Our research contributions include: • Introduced a novel text classification method using LLMs leveraging their code completion capability. • Outperformed the traditional few-shot learning method in text classification with LLMs. • Achieved comparable or superior performance using smaller code models like 7B and 13B model size compared to larger models such as the 70B model size across various text classification benchmarks through our approach. The rest of this paper is organized as follows. Section 2 describes our proposed method. Next, Section 3 shows results of CoCoP performance and impact of each part of it. Section 4 describes related works. Limitations and future work are discussed in Section 6. Finally, Section 5 concludes the paper."
https://arxiv.org/html/2411.09689v1,LLM Hallucination Reasoning with Zero-shot Knowledge Test,"LLM hallucination, where LLMs occasionally generate unfaithful text, poses significant challenges for their practical applications. Most existing detection methods rely on external knowledge, LLM fine-tuning, or hallucination-labeled datasets, and they do not distinguish between different types of hallucinations, which are crucial for improving detection performance. We introduce a new task, Hallucination Reasoning, which classifies LLM-generated text into one of three categories: aligned, misaligned, and fabricated. Our novel zero-shot method assesses whether LLM has enough knowledge about a given prompt and text. Our experiments conducted on new datasets demonstrate the effectiveness of our method in hallucination reasoning and underscore its importance for enhancing detection performance.","Large language models (LLMs) have shown remarkable ability in generating text on various topics [35, 32]. However, they often produce hallucinations — incorrect or unverifiable content — that pose significant risks to their practical applications [2]. Detecting these hallucinations is crucial for ensuring reliability [12] yet challenging due to the plausible appearance of the hallucinated text [37]. Research on detecting hallucinations in LLM-generated text has explored several approaches, including comparing the text with external knowledge [19, 25, 31], fine-tuning LLMs [40, 36, 18], and training classifiers to identify hallucinations [1, 4, 30]. However, these methods require external knowledge, LLM fine-tuning, or supervised training with hallucination-labeled data. To address these limitations, there has been growing interest in source-free, zero-shot methods that analyze LLM outputs directly. These methods encompass consistency checks [22], uncertainty estimation [42, 5, 16, 3, 38], and prompting LLMs to assess the correctness of the text [6, 43]. However, existing detection methods fail to distinguish between different types and causes of hallucinations [44, 10], which is crucial for accurately detecting and resolving them. To be specific, LLM-prompting methods may randomly guess the correctness of text when the LLM lacks relevant knowledge, while most uncertainty-based methods cannot identify errors caused by the inherent randomness of the LLM [29, 7]. Differentiating the underlying causes of hallucinations enables more accurate detection and can even suggest potential solutions: if the LLM lacks knowledge, external knowledge can be provided; otherwise, responses can simply be regenerated. To fill this gap, we categorize LLM-generated text into three types: aligned, misaligned, and fabricated (Table 1). Misaligned text arises from sampling randomness or dependencies on previous tokens [29, 7, 41], while fabricated text is generated when the LLM lacks relevant knowledge [10, 44]. Based on this categorization, we propose a new task, hallucination reasoning, which aims to classify LLM-generated text into one of these three types. We contribute: • New hallucination reasoning task for better understanding and detection of hallucinations (Sec. 3, Table 1). Our dataset creation process can be leveraged for future research in hallucination reasoning (Sec. 4). • MKT, a novel zero-shot method that identifies whether an LLM has enough knowledge about a prompt and text without any requirements for external knowledge, labeled datasets, and LLM fine-tuning (Sec. 3.1, Fig. 1). • Experiments that demonstrate the superiority of our approach in both QA and free-form text generation. Incorporating our method into existing detection algorithms significantly improves their performance, underscoring the importance of hallucination reasoning (Sec. 4)."
https://arxiv.org/html/2411.09642v1,"On the Limits of Language Generation:
Trade-Offs Between Hallucination and Mode Collapse","Specifying all desirable properties of a language model is challenging, but certain requirements seem essential for any good model. Given samples drawn from an unknown language, the trained model should (1) produce valid strings that have not been seen in the training data, and (2) be expressive enough to capture the full richness of the language. Otherwise, if the language model outputs invalid strings, it “hallucinates,” and if it fails to capture the full range of the language, it suffers from “mode collapse.” In this paper, we ask whether it is possible for a language model to meet both of these requirements.We investigate this question within a statistical setting of language generation, building on the seminal works of \citet[Inf. Control]gold1967language, \citet[STOC]angluin1979finding, and \citet[Tech. Report]angluin1988identifying. In this setting, the language model is presented with randomly sampled strings from a distribution supported on an unknown language K, which is only known to belong to a possibly infinite collection of candidate languages. The goal of the model is to generate unseen strings from this target language. We say that the language model generates from K with consistency and breadth if, as the size of the training set increases, the set of strings it can output converges to the set of all unseen strings in K.[NeurIPS]kleinberg2024language posed an open question of whether consistency and breadth in language generation are both possible. We answer this question negatively: for a large class of language models – including next-token-prediction-based models – this is impossible for most collections of candidate languages. This contrasts with the recent positive result of \citet[NeurIPS]kleinberg2024language, which demonstrated that consistent generation, without requiring breadth, is possible for any countable collection of candidate languages. Our finding highlights that generation with breadth is fundamentally different from generation without breadth.As a byproduct of our result, we also examine how many samples are required for generation with or without breadth, establishing near-tight bounds on the “learning curves” for generation in the statistical framework of \citet*[STOC]bousquet2021theory.Finally, our results also give some hope for consistent generation with breadth: it is achievable for any countable collection of languages when negative examples – in the form of strings outside of K – are available in addition to strings inside of K. This suggests that feedback in post-training, which encodes negative examples, can be crucial in reducing hallucinations while also limiting mode collapse.","Language acquisition is a fundamental mystery across multiple scientific fields, ranging from Biology and Neuroscience to Sociology \citepbresnan2007syntactic,saffran1996statistical,clark2014distributional,mahowald2024dissociating. Theoretical Computer Scientists have been fascinated by language since the early days of the field: in the 1950s, \citetturing1950computing introduced his famous test using language as an interface to cognition, \citetshannon1951prediction studied statistics of printed English aiming at understanding its entropy and the extent to which it could be compressed, and \citetmandelbrot1953informational designed a statistical model to capture connections between language and the brain. Over the years, language modeling has advanced through simple models, such as the word n-gram model introduced by \citetshannon1951redundancy and widely used in natural language processing \citepbrown1992class. In the early 2000s, neural networks achieved a significant breakthrough in the field \citepbengio2000neural, leading to fascinating deep learning systems \citepmikolov2010recurrent,goldberg2016primer,lecun2015deep built using traditional architectures like Recurrent Neural Networks \citeprumelhart1986learning and Long Short-Term Memory \citephochreiter1997long. In 2017, the field of language modeling was revolutionized by the introduction of the Transformer architecture \citepsutskever2014sequence,bahdanau2014neural,vaswani2017attention, which led to the development of Large Language Models (LLMs). The achievements of LLMs have been groundbreaking; recent models can perform well on tasks far beyond natural language processing \citepbubeck2023sparks,touvron2023llama. Despite their impressive performance, their extensive use has revealed that LLMs exhibit various bizarre behaviors even in seemingly mundane tasks \citepborji2023categorical. Perhaps the most well-known issue with current LLMs is hallucinations: the models generate false but plausible-sounding text with surprising frequency \citepzhang2023siren,ji2023survey.111We stress that LLMs outputting wrong facts based on errors in training data (e.g., “The Earth is flat”) or miscalculations (e.g., “1+1 = 3”) do not constitute hallucinations. A hallucination is a plausible but false text with unclear origin (e.g., “Barack Obama was the president of the US and was born on January 1, 1958”). Such hallucinations, highlighted by popular media \citepweise2023ai, could significantly impact safety, reliability, and user trust as the adoption of these systems extends to new tasks \citephendrycks2021unsolved,amodei2016concrete. The importance of this problem, among other concerns, led both the US \citepbiden2023executive and the EU \citepsatariano2023eu to issue calls for safeguards against misleading outputs generated by LLMs. In this direction, designing LLMs that generate responses consistent with the ground truth is an effort that has gained a lot of attention from Machine Learning (ML) practitioners \citepandriopoulos2023augmenting,gunasekar2023textbooks,wei2022chain,huang2023survey,feng2024don,kang2024unfamiliar,ji2023survey, policymakers \citepbiden2023executive,satariano2023eu,satariano2023nations, and theorists \citephanneke2018actively,kalai2024calibrated,kleinberg2024language. If the sole goal is to avoid hallucinations, then, of course, one could simply limit the range of outputs generated by the language model. As an extreme example, consider a language model that only outputs “I am a language model” and, therefore, never hallucinates. However, modern LLMs do not just aim to generate a few valid outputs; their goal is to obtain the ability to express a wide range of plausible outputs, thus capturing the richness of human language. The key challenge lies in avoiding hallucinations while achieving breadth. The problem of achieving consistent generation with breadth is not new in the ML community, dating back at least to the era of Generative Adversarial Networks (GANs) \citepgoodfellow2020generative. In this line of work, mode collapse \citepgoodfellow2020generative is the analog of lack of breadth; it refers to the phenomenon where the GAN assigns non-zero mass only to a few modes of the true data distribution, thus producing a limited variety of samples and becoming repetitive \citeparjovsky2017towards,bau2019seeing,shmelkov2018good. The starting point of our work is exactly this puzzling tension between consistent generation and breadth in language generation. We start with a mathematical specification inspired by classical work on learning theory, tracing back to the seminal work of \citetangluin1988identifying, and the recent formulation of \citetkleinberg2024language: the domain \euscr{X} is a countable collection of strings, and there is an unknown target language K which is a subset of this domain. We know that the true language lies within a collection of possibly infinite but countably many languages \euscr{L}=\{L_{1},L_{2},\dots\}. There exists an unknown distribution \euscr{P} over strings in K\in\euscr{L} that satisfies \operatorname{supp}(\euscr{P})=K; any distribution with this property is said to be valid for K. The algorithm observes i.i.d. samples from \euscr{P} and aims to learn how to generate unseen strings from the target language K – this, at a high level, is the language generation problem. Intuitively, the target language K is capturing “facts” of the world; everything that belongs to K is correct, whereas everything outside of K is unambiguously incorrect and can be thought of as a “hallucination.” Observe that K has to be infinite for the problem to be well-defined as, otherwise, at some point, the algorithm will see all possible strings of K and, from then on, would have no unseen strings to generate from. Let us explore language generation further, with the immediate aim of quantifying an algorithm’s progress toward becoming a useful generator. Consider a generating algorithm \mathpzc{G}_{n}222Formally, a generating algorithm is a sequence of mappings (\mathpzc{G}_{n})_{n\in\mathbb{N}}: for each n, it is a computable mapping from a training dataset of size n to a (computable) distribution (i.e., a sampling algorithm) over \euscr{X}. We will use the notation (\mathpzc{G}_{n})_{n} to refer to the generating algorithm and the notation \mathpzc{G}_{n} or simply \mathpzc{G} for the induced distribution (generator) after training; hence when we write x\sim\mathpzc{G}_{n} or \operatorname{supp}(\mathpzc{G}_{n}), we refer to the distribution obtained after training. that is trained on a set S of n i.i.d. examples from \euscr{P}. To quantify the inconsistency of \mathpzc{G}_{n}, we need an objective. As discussed above, this objective should penalize \mathpzc{G}_{n} for outputting strings outside of K and for repeating examples already seen in the training data S.333When we require generating algorithm to achieve breadth, it is not important to enforce that the support does not contain S. We will elaborate after the formal statement of Definition 4. For a target language K and a model \mathpzc{G}_{n} trained on S, we consider the following generation error \mathrm{gen\_er}(\mathpzc{G}_{n})\coloneqq\Pr_{S\sim\euscr{P}^{n}}[% \operatorname{supp}(\mathpzc{G}_{n})\supset K\setminus S]\,. (1) In words, a model errs according to \mathrm{gen\_er}(\cdot) if it either hallucinates by outputting strings from \euscr{X}\setminus K or if it outputs something already contained in the training set S. This is inspired by the notion of generation considered by \citetkleinberg2024language; they call an algorithm a consistent generator if its support becomes a subset of K\setminus S after seeing finitely many training examples S. We relax this definition and call an algorithm a consistent generator for the collection \euscr{L} if its error, as defined in Equation 1, asymptotically goes to zero for any valid distribution \euscr{P}. Let us now review how prior work has approached issues with language generation algorithms – foremost, hallucination. Under the above statistical setting, \citetkalai2024calibrated made important progress showing that calibrated models must hallucinate by lower bounding the hallucination rate by the model’s calibration. For a detailed comparison with our work, we refer to Section 1.5. Closer to our paper, the work of \citetkleinberg2024language explored language generators that must not hallucinate, i.e., they must be consistent. They studied language generation in an online setting where the data are not drawn from \euscr{P} but are given as a stream to the learner, i.e., as an adversarial enumeration of the strings of the true language K. In their setting, \mathpzc{G}_{n} is said to generate in the limit from K if, after some finite time n_{0} in the enumeration of K, \mathpzc{G}_{n} is able to generate new unseen strings from K for all subsequent times n\geq n_{0}. They showed that there exists an algorithm that can generate in the limit from every countable list of candidate languages. This result is surprising because it contrasts with strong negative results for the well-studied problem of language identification in the limit (where one wants to identify K in the limit and not simply generate from it;444 Very briefly, a language collection \euscr{L}=\{L_{1},L_{2},\dots\} is called identifiable in the limit if there exists an algorithm (\euscr{A}_{n}\colon\euscr{X}^{n}\to\mathbb{N})_{n} such that for any K\in\euscr{L} and any enumeration x_{1},x_{2},\dots of the strings of K appearing as a stream to (\euscr{A}_{n}), there is a finite time n_{0}\in\mathbb{N} after which the algorithm predicts the correct index of the true language, i.e., L_{\euscr{A}_{n}(x_{1},\dots,x_{n})}=K for any n\geq n_{0}. see also Definition 9). The family of languages identifiable in the limit is very limited: the results of \citetgold1967language,angluin1979finding showed that language identification is a very difficult problem and most collections of languages are non-identifiable (in fact, there is a tight characterization due to \citetangluin1980inductive which we state in Definition 10). Hence, the algorithm of \citetkleinberg2024language shows that language generation in the limit is much more tractable than identification. We note that while their algorithm operates in a non-statistical setting, it will be an important building block for our results. \citet kleinberg2024language observed that their algorithm eventually becomes a consistent generator but suffers from mode collapse: initially, it generates with breadth while being inconsistent with the target language; later on, as a larger part of the stream is seen, it starts sacrificing breadth in order to generate valid outputs. This behavior led them to leave the existence of a consistent generator that achieves breadth as an interesting open question. In this work, we will formally introduce a notion of breadth for language generation in our statistical setting (Section 1.1.1). For now, we mention that our definition roots in the notion of mode collapse from Generative Adversarial Networks (GANs) \citepgoodfellow2020generative,arjovsky2017towards and, roughly speaking, states that an algorithm (\mathpzc{G}_{n}) generates with breadth from K if the probability that its support contains all the unseen examples from the target language goes to 1, as the training samples from a valid distribution go to infinity. Now it is a good point to contrast breadth with consistency: consistent generators aim at avoiding any elements outside of K while generators achieving breadth try to cover all unseen elements of K. The question of \citetkleinberg2024language is asking whether the equilibrium condition that the support of the generator exactly matches the unseen elements of K can eventually be achieved by some algorithm. This is the main question we aim to address in this paper. Is it possible to achieve consistent language generation with breadth or is there some inherent trade-off between consistency and breadth? 1.1 Informal Results Our main results confirm the tension between consistent generation and breadth for language models, conjectured by \citetkleinberg2024language, in a strong way: informally, we show that A language model that generates with breadth must be inconsistent, i.e., it must hallucinate. We focus on the probabilistic setting of \citetangluin1988identifying which we have already introduced informally. En route to our results in the probabilistic setting, we also obtain results in the online setting of \citetgold1967language, \citetangluin1979finding, and \citetkleinberg2024language, as we will see later. To facilitate a formal discussion of our contributions, we need to introduce some further definitions. 1.1.1 Setup and Definitions A generating (or learning) algorithm is a sequence of computable mappings (\mathpzc{G}_{n})=(\mathpzc{G}_{n})_{n\in\mathbb{N}} from samples S\subseteq\euscr{X}^{n} to generators, which are simply distributions over the domain \euscr{X}. More formally, a generating algorithm is a sequence of mappings from samples to Turing machines that generate samples from an (explicitly or implicitly) defined distribution over strings. In the statistical setting we consider, the learner observes samples from an unknown distribution which is valid for some unknown language K in the collection \euscr{L}=\{L_{1},L_{2},\dots\}. Definition 1 (Valid Distribution \citepangluin1988identifying). A distribution \euscr{P} over a countable domain \euscr{X} is valid with respect to a countable language collection \euscr{L} if its support is the same as some language K\in\euscr{L}. In this case, when we want to be specific about the language that \euscr{P} draws samples from, we say \euscr{P} is valid for K. If the collection \euscr{L} is clear from context, we will simply say that \euscr{P} is valid. Based on this definition and building on the model studied by \citetkleinberg2024language, we give the following adaptation for consistent generation from a collection \euscr{L} in the statistical setting. Definition 2 (Consistency). A generating algorithm (\mathpzc{G}_{n}) for a language collection \euscr{L} is consistent if for any valid distribution \euscr{P}, it holds that \lim_{n\to\infty}\mathrm{gen\_er}(\mathpzc{G}_{n})=0. Otherwise, the algorithm is said to be inconsistent. Hence, an algorithm is said to be consistent if the generators it produces by training on any valid distribution \euscr{P} converge to generating examples from the unseen part of \euscr{P}. Some of our results explore when asymptotic consistency is achievable. However, the main focus of our work is on understanding the rates at which consistency (and other desirable properties) can be attained – if possible at all. In particular, we want to study the rate at which the generation error \mathrm{gen\_er}(\mathpzc{G}_{n}) decreases as the number of samples n goes to infinity – that is, we want to study the learning curve of consistent generation (and other tasks that we introduce later in this section). Bousquet, Hanneke, Moran, van Handel, and Yehudayoff \citepbousquet2021theory characterized learning curves for binary classification, formalizing the universal rates framework, earlier explored by \citetschuurmans1997characterizing and \citetantos1996strong. To this end, we borrow their definition of universal rates. Definition 3 (Informal, Universal Rates; \citepbousquet2021theory, see Definition 12). A generating algorithm (\mathpzc{G}_{n}) has rate R(\cdot), where \lim_{n\rightarrow\infty}R(n)=0, for a language collection \euscr{L} if \forall\euscr{P}\in\mathrm{Val}(\euscr{L})~{}~{}\exists C,c>0\quad\text{such % that}\quad\mathrm{gen\_er}(\mathpzc{G}_{n})\leq C\cdot R(c\cdot n)\quad\forall n% \in\mathbb{N}\,, where \mathrm{Val}(\euscr{L}) is the class of valid (realizable) distributions for \euscr{L}. Observe that these learning curves are distribution-dependent since the constants c and C are allowed to depend on \euscr{P}. This difference turns out to be crucial and can, sometimes, lead to significant differences between universal rates and the corresponding distribution-independent rates \citepbousquet2021theory. Among different universal rates, exponential universal rates are of specific interest as they are often the best possible rate, as we will see later. We say that the algorithm (\mathpzc{G}_{n}) generates with an exponential universal rate if R(n)=\exp(-n) in the above definition. Next, we turn to language generation with breadth. Definition 4 (Breadth). A generating algorithm (\mathpzc{G}_{n}) for a language collection \euscr{L} is said to achieve breadth if, for any valid distribution \euscr{P}, it holds that \lim_{n\to\infty}\Pr[\operatorname{supp}(\mathpzc{G}_{n})\supseteq K\setminus S% _{n}]=1, where S_{n} is the dataset used to train \mathpzc{G}_{n}, i.i.d. from \euscr{P}. Otherwise, the algorithm suffers from mode collapse. Definition 4 is inspired by the literature on GANs (see e.g., \citepgoodfellow2020generative,arjovsky2017towards). For instance, consider the work of \citetarjovsky2017towards, which studies distributions \mathpzc{G} and \euscr{P} induced by the generator and nature, respectively, and says that mode collapse occurs when the KL divergence \operatornamewithlimits{\mathsf{KL}}\left(\euscr{P}\|\mathpzc{G}\right)% \coloneqq\int\log\left(\nicefrac{{\euscr{P}(x)}}{{\mathpzc{G}(x)}}\right)~{}{% \rm d}\euscr{P}(x)\to\infty. In particular, mode collapse happens when there is some string x\in\operatorname{supp}(\euscr{P}) for which \mathpzc{G}(x)=0. In other words, the generator has breadth when \operatorname{supp}(\mathpzc{G})\cup S_{n}\supseteq\operatorname{supp}(\euscr{% P}), which recovers our definition for breadth by noting that \operatorname{supp}(\euscr{P})=K since \euscr{P} is valid for K and that, to be compatible with the definition of consistency (Definition 2), we bar a generator from repeating strings it has already seen. (It is worth mentioning that one can modify the definition of breadth to require \operatorname{supp}(\mathpzc{G}_{n})\supseteq K without changing any of our results; see Remark 2.) We also note that the definition of consistency we use can also be derived in an analogous fashion by requiring the reverse KL divergence (i.e., \operatornamewithlimits{\mathsf{KL}}\left(\mathpzc{G}\|\euscr{P}\right)) to be finite. Putting the definitions of consistency and breadth together implies that an algorithm generates with consistency and breadth if, eventually, its support matches the set of unseen strings in K, i.e., K\setminus S_{n} at the n-th step. After presenting our main results, in Section 1.3, we discuss relaxations of this notion of consistent generation with breadth. A last ingredient for our results concerns the decidability of a folklore Theoretical Computer Science problem, which we call the membership oracle problem, that has motivated extensive work in formal languages and complexity theory \citepsipser2012introduction,soare1999recursively. A generator \mathpzc{G}, which is the output of some generating algorithm, corresponds to some Turing machine, as is standard in the language inference literature, that samples according to a distribution over \euscr{X} \citepangluin1979finding,blum1975toward,angluin1983inductive,adleman1991inductive. Definition 5 (Membership Oracle Problem). Given a generator \mathpzc{G}, the membership oracle problem for \mathpzc{G}, denoted as \mathsf{MOP}(\mathpzc{G}), is defined as follows: given the description of \mathpzc{G} and a string x, output Yes if x\in\operatorname{supp}(\mathpzc{G}) and output No otherwise. This problem is, in general, undecidable due to a reduction to the halting problem (Section A); nevertheless, its decidability depends on the structure of the Turing machine as we will see shortly. The above definition naturally extends to generating algorithms. Definition 6 (MOP for Generating Algorithms). The membership oracle problem is decidable for a generating algorithm (\mathpzc{G}_{n}) if, for any n\in\mathbb{N} and any S\subseteq\euscr{X}^{n}, \mathsf{MOP}(\cdot) is decidable for the induced generator \mathpzc{G}=\mathpzc{G}_{n}(S). We note that the above definitions implicitly assume that the generator \mathpzc{G}_{n}(S) depends only on the randomness of S; we could extend this by allowing \mathpzc{G}_{n}(S) to be a distribution over generators. 1.1.2 Main Results We now have all the ingredients to state our first result, which establishes that, for all generating algorithms for which \mathsf{MOP}(\cdot) is decidable, (consistent) generation with breadth is as hard as language identification in the statistical setting. As in Definition 3, we will say that the generating algorithm (\mathpzc{G}_{n}) generates with breadth from \euscr{L} at some rate R(\cdot) if, for any K\in\euscr{L}, valid distribution \euscr{P}, and n\in\mathbb{N}, \operatornamewithlimits{\mathbb{E}}_{S\sim\euscr{P}^{n}}\mathds{1}\left\{% \operatorname{supp}(\mathpzc{G}_{n})\neq K\setminus S\right\}\leq C\cdot R(c% \cdot n)\,, for some distribution-dependent constants C,c>0. If no rate R(\cdot) satisfying \lim_{n\to\infty}R(n)=0 exists, we will say that (\mathpzc{G}_{n}) does not generate with breadth at any rate. Informal Theorem 1 (see Theorem 3.3). For every language collection \euscr{L} that is not identifiable in the limit, no generating algorithm (\mathpzc{G}_{n}), for which \mathsf{MOP}{}(\cdot) is decidable, can generate from \euscr{L} with breadth at any rate. Recall that the family of languages non-identifiable in the limit is quite broad. Based on the results of \citetgold1967language,angluin1979finding,angluin1980inductive on the problem of language identification in the limit, our impossibility result holds for most interesting collections of languages. For Informal Theorem 1 to be valuable and meaningful though, we further need to show that there exists an algorithm that generates without breadth for the collections of languages for which our impossibility result is true. Our next result states that this is indeed possible: there exists an algorithm that generates with (almost) exponential universal rates for any countable language collection \euscr{L}. Informal Theorem 2 (see Theorem 3.3). For every language collection \euscr{L} that is not identifiable in the limit, there exists a generating algorithm (\mathpzc{G}_{n}), for which \mathsf{MOP}(\cdot) is decidable, that generates (possibly) without breadth from \euscr{L} at exponential rates. Further, if \euscr{L} is identifiable in the limit, then there exists a generating algorithm (\mathpzc{G}_{n}), for which \mathsf{MOP}(\cdot) is decidable, that generates with breadth from \euscr{L} at (almost) exponential rates. Informal Theorem 2 shows that any countable collection of languages not only admits a consistent generator in the limit under an adversarial enumeration of the target language (as shown by \citetkleinberg2024language), but the statistical rate at which consistency (as per Definition 2) is achieved is exponential in the number of samples. Further, for identifiable collections of languages, we give an algorithm that generates with breadth at an (almost) exponential rate. The combination of Informal Theorem 1 and 2, reveals a strong separation between generation with and without breadth for any generating algorithm for which \mathsf{MOP}(\cdot) is decidable. What is missing is an answer to: how large is the class of generators for which the membership oracle problem \mathsf{MOP}(\cdot) is decidable? It turns out there is a very broad class of language generators for which this is the case and which also captures modern LLMs, as we show next. A Family of Generators for Which \mathsf{MOP}{}(\cdot) Is Decidable. Motivated by the structure of modern language models \citepbahl1983maximum,brown1990statistical,touvron2023llama,bubeck2023sparks,achiam2023gpt, we consider a family of iterative generators. A generator is said to be iterative if it generates text one alphabet or “token” at a time (see Definition 14). To generate each token, the generator can perform an arbitrary (but finite) amount of computation and, possibly, use randomness. For this to make sense, one has to imagine strings of \euscr{X} as strings over some finite alphabet \Sigma. This holds without loss of generality as \euscr{X} is countably infinite and, hence, there is a one-to-one mapping from \euscr{X} to strings over \Sigma (due to which \euscr{X} can be thought of as a set of strings over \Sigma).555 In a bit more detail, since \euscr{X} and \Sigma^{*} are countably infinite, they have enumerations x_{1},x_{2},\dots and s_{1},s_{2},\dots. Therefore, given any string s_{i}\in\Sigma^{*} generated by an iterative generator, one can map it to a string x_{i}\in\euscr{X}, thereby getting a generator for \euscr{X}. We show that for any iterative generator, the membership oracle problem is decidable and our Informal Theorem 1 is applicable. Informal Theorem 3 (see Theorem 3.4). For any iterative generator \mathpzc{G}, \mathsf{MOP}(\mathpzc{G}) is decidable. Observe that this family of next-token generators is very general. First, it captures existing large language models: for instance, to simulate an LLM L, we define the next-token predictor as a Turing machine that simulates L on the provided string until L generates one new token. Next, it also captures systems where an LLM can interact with another Generative AI model or algorithmic system (such as a diffusion model or a code interpreter) – as these auxiliary systems can also be simulated by the generator. Given this, it becomes evident that this class of generators for which \mathsf{MOP}{}(\cdot) is decidable is fairly large and interesting. Implications for the Gold-Angluin Model. We repeat that all the aforementioned results hold in the statistical setting. En route to obtaining our results in this setting (Informal Theorems 1 and 2), we show several connections to the online setting of \citetgold1967language, angluin1979finding,angluin1980inductive,kleinberg2024language, which lead to the following result. Informal Theorem 4 (see Theorem 3.5). For any language collection \euscr{L} that is not identifiable in the limit, no generating algorithm (\mathpzc{G}_{n}), for which \mathsf{MOP}{}(\cdot) is decidable, can generate from \euscr{L} with breadth in the limit. To be more concrete, a generating algorithm generates with breadth in the limit if its support is eventually K\setminus S_{n}, where S_{n} is the set of the first n positive examples (i.e., examples that belong to K). We emphasize that Informal Theorem 4 is in a similar spirit as our Informal Theorem 1, but holds in the online model instead of the statistical model discussed earlier. In particular, Informal Theorem 4 combined with the algorithm of \citetkleinberg2024language give a separation between consistent generation with and without breadth in the Gold-Angluin model. Further, as explained before, this result applies to any iterative generator due to Informal Theorem 3. Moreover, as \mathsf{MOP}{}(\cdot) is decidable for the generating algorithm of \citetkleinberg2024language (since its support contains a singleton element x which can be computed by running their algorithm), the above result, in particular, shows that the algorithm of \citetkleinberg2024language cannot generate with breadth in the limit from any non-identifiable collection. Organization of Rest of the Introduction. We proceed with an exposition of our techniques in order to obtain our main results presented above. In Section 1.3, we relax the definitions of consistency and breadth and give more “robust” trade-offs between hallucination and breadth. Next, in Section 1.4, we give a list of open problems for future work. Finally, Section 1.5 contains an extensive overview of related works. 1.2 Technical Overview In this section, we present the technical tools we develop to obtain our main results. A Natural Strategy to Prove Informal Theorem 1. At first glance, there seems to be a natural strategy to prove Informal Theorem 1: assume that there exists a consistent generating algorithm with breadth \mathpzc{G}=(\mathpzc{G}_{n}) for some non-identifiable collection \euscr{L} in the statistical setting and then show that this implies identification in the statistical setting, which would contradict the fact that \euscr{L} is non-identifiable. To implement this strategy one needs a method to utilize \mathpzc{G}, along with the positive samples from the target language K, for identification. This raises the question: what additional power can \mathpzc{G} give that the positive samples do not already provide? Initial Attempts to Implement the Strategy. Indeed, if one uses no additional properties of \mathpzc{G}, then its outputs provide no more information than an adversarial enumeration of K. To develop some intuition, we begin by considering some properties of the generator and explaining why they are insufficient to enable identification. 1. \mathpzc{G} is non-adaptive. First, one may want to utilize the fact that the generator \mathpzc{G}{} is fixed and, hence, the samples it outputs cannot adapt to the specific algorithm being used based on the outputs of the algorithm. Hence, it will probably provide an algorithm-independent enumeration of the true language. However, this is not helpful in general since there exist simple non-identifiable language collections that remain non-identifiable for many enumerations of the target language. 2. \mathpzc{G} samples from a fixed distribution. Another property one may want to leverage is the stochasticity of the generator: \mathpzc{G} samples its outputs from a fixed distribution (which is valid for K). However, even this does not enable the identification of non-identifiable collections due to a result by \citetangluin1988identifying. Angluin shows that even if the positive examples are i.i.d. from a valid distribution and do not appear as an adversarial enumeration (as in \citetgold1967language), this does not enable identification of any collection \euscr{L} that is non-identifiable in the limit. (We prove a stronger version of this result in Lemma 5.5.) 3. \mathpzc{G} samples from a simple distribution. Moreover, the difficulty in the above negative result is not the complexity of the encoded distribution: it holds even when \mathpzc{G} samples from a distribution that is computable by a Turing machine. At this point, it is not clear how to utilize access to a generator \mathpzc{G} which generates with breadth from K. Next, we present a strong form of access to the generator \mathpzc{G} that is useful for identification. 4. Access to Subset Queries “\operatorname{supp}(\mathpzc{G})\subseteq L_{i}” and “L_{i}\subseteq L_{j}”. For one of their algorithms, \citetkleinberg2024language utilize a subset oracle that answers queries of the form “is L_{i}\subseteq L_{j}?”. (In general, this oracle is not guaranteed to be computable). One can imagine an extension of this oracle that, given an index i and description of the generator \mathpzc{G}, outputs whether \operatorname{supp}(G)\subseteq L_{i}. The existence of this oracle turns out to be sufficient to identify K, as we explain next: After a finite amount of time, \citetkleinberg2024language’s algorithm creates a list of “critical” languages C_{1},C_{2},\dots, of the following form (see Theorem 4.1 in \citetkleinberg2024language) C_{1}\supseteq C_{2}\supseteq\dots\supseteq\left(C_{i}\coloneqq K\right)% \supseteq C_{i+1}\supseteq\dots\,. In words, this list has two properties (1) K appears in this list, say, at C_{i}=K for some i<\infty and (2) each language C_{j} in the list is a subset of the preceding language C_{j-1}. Given this list and the aforementioned subset oracle, one can easily identify the index of K as the largest j for which \operatorname{supp}(\mathpzc{G})=K\subseteq C_{j}. This assumption allows to identify any collection in the limit given access to a consistent generation \mathpzc{G} with breadth. However, this type of access is not very practical since it is not clear when such an oracle is implementable. Our Approach. Our first idea is that a much weaker form of access to \mathpzc{G} – membership oracle to \operatorname{supp}(\mathpzc{G}) – is sufficient for identification. This is where the membership oracle problem \mathsf{MOP}{}(\cdot) (Definition 5) appears in the proof. In fact, given this idea, it is not difficult to show that with that type of access, we can go from a generator with breadth in the online setting to an identification algorithm in the online setting; and, hence, get Informal Theorem 4. However, our focus is the statistical setting where there are several additional challenges in using the membership oracle to \operatorname{supp}(\mathpzc{G}). A. Need for Universal Rates for Generation and Identification. The key issue is the following: In the statistical setting, if we assume that we have a generator with breadth at rate R(\cdot), then we can hope to show an implication that we can get an identification algorithm at rate R(\cdot). However, this need not imply a contradiction to the identifiability of \euscr{L} as in the online setting. This is because, even though \euscr{L} is non-identifiable in the online setting, it may become identifiable at some rate R^{\prime}(\cdot) in the statistical setting. Indeed, this is the case in binary classification, where there are simple hypothesis classes (such as thresholds over reals) that are not learnable in Littlestone’s online setting \citeplittlestone1988learning but become learnable (at a universal – and uniform – linear rate) in the statistical setting; in fact, any hypothesis class is learnable in the statistical setting under universal rates, since there is a Bayes consistent algorithm, under benign assumptions \citepbousquet2021theory. Hence, to get a contradiction, we first need to understand the taxonomy of universal rates for generation and identification. We remark here that both the learning task (e.g., classification, regression, identification, and generation) and loss function used in the problem are pivotal for the landscape of rates that one gets; for instance, with the zero-one loss for binary classification one gets a trichotomy of rates \citepbousquet2021theory, but with the L_{1}-loss for regression, one gets infinitely many rates \citepattias2024universal. To overcome the above challenge, we provide statistical rates for identification and generation. We start with identification. We show that if \euscr{L} is identifiable in the limit in the adversarial Gold-Angluin setting with positive examples \citepgold1967language,angluin1980inductive, then it is identifiable under Definition 12 with (almost) exponential (universal) rates. This is the less technical part of the proof so we will give a high-level approach. B. Identification in the Limit \implies Identification at (Almost) Exponential Rates. Our idea is reminiscent of \citet*bousquet2021theory and requires splitting the input dataset into multiple batches whose size is carefully selected, running the online algorithm on each batch, and then taking a majority vote over the outputs of the algorithm. We remark that there are some technical issues which require further care, compared to \citetbousquet2021theory. First, unlike the setting of \citetbousquet2021theory, we only see positive examples and we get no feedback about our guesses. Thus, we cannot use their approach to “estimate” a time after which the learner will stop making mistakes. Moreover, when we run the learners on multiple batches, it can be the case that different batches output different indices of languages that correspond to K (since the target language can appear at multiple positions in the countable collection \euscr{L}). Thus, taking a majority vote over these indices might not work. Nevertheless, we manage to handle these issues and get almost exponential rates for collections that satisfy Angluin’s criterion for identification in the limit \citepangluin1979finding. A bit more concretely, to circumvent the first issue, our approach is to “guess” the right batch size, and this guess needs to be an increasing function of n – this is why we get almost exponential rates instead of exactly exponential rates (Lemma 5.5). The second issue is more subtle. At a high level, we use a voting scheme where the output of every batch \widehat{L}_{i} gives a “vote” to every language L\in\euscr{L} such that \widehat{L}_{i}=L, and we predict the lowest-indexed language that is voted by at least half of the batches. In its current form, this scheme is not computable, nevertheless, we show that it can be modified so that it becomes computable (Lemma 5.4). The more interesting half of establishing universal rates for identification is the lower bound showing that if a collection is not identifiable in the limit, it is also not identifiable in the statistical setting at any rate R(\cdot) such that \lim_{n\to\infty}R(n)=0. C. Impossible to Identify in the Limit \implies Impossible to Identify at Any Rate. Recall that the statistical setting was studied by \citetangluin1988identifying. \citetangluin1988identifying showed that every learner, with probability at least \nicefrac{{1}}{{3,}} does not converge to outputting a (stable) index of the target language in an infinite stream of examples drawn from a valid distribution. In other words, with probability at least \nicefrac{{1}}{{3,}} the algorithm will either stabilize to an index that does not correspond to the target language or it will not stabilize to any index. Notice that this does not rule out algorithms that output different indices of the target language, for all but finitely many n\in\mathbb{N}. The first step towards establishing our desired lower bound is to strengthen Angluin’s result: we show that any learning algorithm, with probability at least \nicefrac{{1}}{{3}}, outputs indices that do not correspond to the target language infinitely often. More formally, let us consider an identification algorithm h_{n}, which maps a training set of n examples x_{1},\dots,x_{n} to an index h_{n}(x_{1},\dots,x_{n}) so that L_{h_{n}(x_{1},\dots,x_{n})} is the n-th prediction for the true language. The aforementioned lower bound means that666 Informally, \limsup of a sequence of events captures the events that occur infinitely often. For instance, \Pr[\limsup_{n\to\infty}\mathscr{E}_{n}] represents the probability that infinitely many of the events \mathscr{E}_{1},\mathscr{E}_{2},\dots occur. On the other hand, \limsup_{n\to\infty}\Pr[\mathscr{E}_{n}], roughly speaking, denotes the largest value that the probabilities \Pr[\mathscr{E}_{1}],\Pr[\mathscr{E}_{2}],\dots,\dots approach infinitely often as n\to\infty. \Pr_{\left\{x_{i}\colon i\in\mathbb{N}\right\}\sim\euscr{P}^{\infty}}\left[% \limsup_{n\rightarrow\infty}\left\{L_{h_{n}\left(x_{1},\ldots,x_{n}\right)}% \neq K\right\}\right]\geq\frac{1}{3}\,, where X\sim\euscr{P}^{\infty} corresponds to an infinite i.i.d. draw from \euscr{P}. One may be tempted to conclude that this implies that with probability \nicefrac{{1}}{{3}} we cannot identify the target language (in the statistical setting). However, the quantity we wish to bound away from 0 to derive the desired lower bound is \limsup_{n\rightarrow\infty}\Pr_{x_{1},\ldots,x_{n}\sim\euscr{P}^{n}}\left[% \left\{L_{h_{n}\left(x_{1},\ldots,x_{n}\right)}\neq K\right\}\right]\,. It is well-known that for any sequence of events \{\mathscr{E}_{n}\}_{n\in\mathbb{N}}, \Pr\left[\limsup_{n\rightarrow\infty}\mathscr{E}_{n}\right]\geq\limsup_{n% \rightarrow\infty}\Pr[\mathscr{E}_{n}]\,. This, however, is not sufficient to deduce the result we need; we need the opposite inequality. Hence, Angluin’s guarantee does not suffice to get our lower bound. In order to show our result, we use a boosting argument (Lemma 5.8): if there exists a learner h_{n} whose probability of misidentification \Pr_{x_{1},\dots,x_{n}\sim\euscr{P}^{n}}\left[L_{h_{n}\left(x_{1},\dots,x_{n}% \right)}\neq K\right] converges to a number strictly less that \nicefrac{{1}}{{2}}, then we can convert it to a learner whose error rate decreases (almost) exponentially quickly. This (almost) exponential rate, in particular, implies that \sum_{n=1}^{\infty}\Pr_{x_{1},\ldots,x_{n}\sim\euscr{P}^{n}}\left[L_{h_{n}% \left(x_{1},\ldots,x_{n}\right)}\neq K\right]<\infty\,. This, crucially, enables us to use the Borel-Cantelli lemma (see Lemma E.1) which gives us that \Pr\left[\limsup_{n\rightarrow\infty}\left\{L_{h_{n}\left(x_{1},\ldots,x_{n}% \right)}\neq K\right\}\right]=0 , and, thus, a contradiction to Section 1.2. This implies the desired impossibility result. As consequence of the above results, we get a dichotomy for universal identification rates: Informal Theorem 5 (see Theorem 3.1). For any language collection \euscr{L} that is identifiable in the limit and for any g(n)=o(n), there exists a learner that identifies \euscr{L} at rate \exp(-g(n)). Otherwise, \euscr{L} is not identifiable at any rate. We remark that if we have access to subset queries for \euscr{L}, we can show that there exists an algorithm that achieves exactly exponential rates, for all identifiable collections (see Proposition 3.8). Next, we move to understanding universal rates for language generation. D. Universal Rates for Generation (Possibly Lacking Breadth) Without Boosting. One might suspect that a similar batching argument would give us exponential rates for generation: just run the online algorithm of \citetkleinberg2024language multiple times and aggregate. The issue is that aggregation for generation is different than prediction: for prediction, it is clear how to implement majority vote as a boosting technique; for generation, it is unclear how to aggregate different generated strings which is, typically, necessary to obtain a boosting algorithm. One immediate attempt is to take majority votes over the strings that each batch outputs; unfortunately, even if the majority of them are generating from the target language, they might be outputting different strings, thus, even a few batches outputting the same invalid strings are enough to fool our aggregation rule. Another tempting approach is to mimic the strategy we used to aggregate different indices of the target language in the identification setting: we go over every output of the batches and we let them give a vote to each of the languages in \euscr{L} they belong to.777The astute reader might realize that, as stated, this strategy is not computable – as we explain, even if one could implement it, this aggregation scheme does not work. It is not hard to see that every batch whose output corresponds to a valid generator will vote for the target language. Unfortunately, it will also vote for all supersets of the target language. This is exactly the heart of the difficulty of identification: telling apart supersets of the target language from the target language, which is colloquially called overgeneralization. Taking it to the extreme, imagine that the first language of the collection contains all the strings, i.e., L_{1}=\euscr{X}. Then, all the batches will vote for L_{1}. This is problematic for two reasons: generating a fresh string from the majority-voted language is as good as random guessing, and choosing a string among the ones that voted for the majority-voted language is as good as picking one of the outputs of all batches uniformly at random. Perhaps surprisingly, it turns out that a much simpler approach works: we show that the algorithm of \citetkleinberg2024language directly enjoys exponential rates in the statistical setting, without the use of batching and boosting. This observation is based on a sufficient condition that allows one to use an algorithm that works “in the limit” to obtain exponential rates in the statistical setting, without any modification (see Lemma 5.11). Informal Theorem 6 (see Theorem 3.2). For any countable language collection \euscr{L} there exists a generating algorithm that generates from \euscr{L} at an (optimal) exponential rate. This pair of results for identification (Informal Theorem 5) and generation (Informal Theorem 6) allow us to get Informal Theorem 1 and 2. The idea for Informal Theorem 1 is that we will use the algorithm \mathpzc{G} that generates with breadth at some rate R(\cdot) for an arbitrary non-identifiable collection \euscr{L} and membership oracle access to \mathpzc{G} in order to get an identification algorithm for \euscr{L} with some rate R^{\prime}(\cdot) such that \lim_{n\to\infty}R^{\prime}(n)=0. This is a contradiction since Informal Theorem 5 shows that \euscr{L} admits no rate in the universal setting. Finally, Informal Theorem 2 follows almost immediately from our universal rates result for generation. 1.3 Additional Results With Relaxation of Consistency and Breadth Next, we study a relaxation of consistent generation with breadth, which we call unambiguous generation, and ask: is there a generator that unambiguously generates from a non-identifiable collection? In this section, we will allow the generator to repeat examples in the training data. Like all of our results with breadth, this choice is not crucial, and all of the results have analogs where the generator does not repeat training examples (see Remark 2). We make this choice for simplicity. We show that unambiguous generation (which we define later in this section) from non-identifiable collections is impossible for any generator \mathpzc{G} for which \mathsf{MOP}{}(\mathpzc{G}) is decidable and that satisfies the natural property that \mathpzc{G} “stabilizes” after seeing sufficiently many examples: Definition 7 (Stability). A generating algorithm (\mathpzc{G}_{n}) is stable for a language collection \euscr{L} if for any target language K\in\euscr{L} and for any enumeration of K, there is some finite n^{*}\in\mathbb{N} such that for all n,n^{\prime}\geq n^{*}, it holds that \operatorname{supp}(\mathpzc{G}_{n})=\operatorname{supp}(\mathpzc{G}_{n^{% \prime}}). We make some initial remarks about stable generators. First, any generator \mathpzc{G} that is consistent and achieves breadth is also stable, since after some finite time its support, union the training set, becomes K and remains so. (Here, whether \mathpzc{G} repeats training examples or not is not crucial – the two types of generators are interchangeable; see Remark 2.) Second, this notion of stability can be seen as trying to capture practical heuristics such as learning rate schedules and early stopping that reduce the amount of changes to the generator as more and more samples are seen. Moreover, the original work of \citetgold1967language also requires the identifier to stabilize to a consistent guess, and, more recently, the stability property of learning algorithms was explored in the PEC learning setting of \citetmalliaris2022unstable. Having defined stability, we proceed to discuss relaxations of generation with breadth. Intuitively, consistent generation with breadth requires the generator to eventually stop making mistakes – where a mistake is any element x that \mathpzc{G} incorrectly includes (if x\not\in K or x is part of the training samples) or excludes (if x\in K) from its support. We now relax this and only require that, eventually, the generator \mathpzc{G} makes finitely many mistakes. Observe that this is a non-trivial requirement because the languages contain infinitely many strings and, so, at the start, \mathpzc{G} is expected to make infinitely many mistakes. A valuable observation is that it is possible for two languages L_{1} and L_{2} to only differ in finitely many strings even if each contains infinitely many strings. With this observation, it is not too hard to see that the aforementioned requirement is too weak to capture a reasonable notion of generation from the target language K. Indeed, it would allow generators that, given examples from K, perpetually generate outputs (with breadth) from a language L that is not the actual target language – which is a severe form of hallucination. Hence, to create a meaningful model, we must impose some further restrictions on the mistakes of the generator \mathpzc{G}. The above example motivates that, at the least, the generator \mathpzc{G} should be “closer” to generating from K than some language L\neq K with L\in\euscr{L}. We call such a generator unambiguous. Definition 8 (Unambiguous Generator). A generating algorithm \mathpzc{G}=(\mathpzc{G}_{n}) is unambiguous for a language collection \euscr{L} if, for any K\in\euscr{L} and every enumeration of K, its support eventually becomes closer to K than to any other language L\neq K in \euscr{L} in terms of the symmetric difference metric, i.e., there exists some n^{*}\in\mathbb{N} such that for all n\geq n^{*} it holds that \left|\operatorname{supp}(\mathpzc{G}_{n})\triangle K\right|<\min_{L\in\euscr{% L}\colon L\neq K}\left|\operatorname{supp}(\mathpzc{G}_{n})\triangle L\right|, where recall that for two sets S and T, S\triangle T\coloneqq\left(S\setminus T\right)\cup\left(T\setminus S\right). Figure 1: An Unambiguous Generator That neither Has Consistency nor Breadth. In this example, the language collection \euscr{L} has two languages L and K, where K denotes the target language. The red curve denotes L, the dashed green curve denotes K, and the blue curve denotes the support of \operatorname{supp}(\mathpzc{G}_{n}). The generator \mathpzc{G}_{n} hallucinates since \operatorname{supp}(\mathpzc{G}_{n})\setminus K\neq\emptyset and does not achieve breadth for the target K since B=K\setminus\operatorname{supp}(\mathpzc{G}_{n}) is non-empty. Nevertheless, this generator is unambiguous as \left|\operatorname{supp}(\mathpzc{G}_{n})\setminus K\right|+\left|B\right|<% \left|\operatorname{supp}(\mathpzc{G}_{n})\setminus L\right|+\left|A\right|. Here, we pause to observe that this notion of generation is a significant relaxation of generation with breadth that we considered earlier (Definition 4). Not only does it allow the generator to hallucinate certain strings not in the target K and omit strings actually in K for arbitrarily long, the number of hallucinations and omissions can be arbitrarily large, depending on the structure of the language collection \euscr{L}. Surprisingly, we show that even this very weak notion of “consistent generation with breadth” is not achievable by a large class of generators. Informal Theorem 7 (see Theorem 3.6). For every language collection \euscr{L} that is not identifiable in the limit, no stable generating algorithm \left(\mathpzc{G}_{n}\right) for which \mathsf{MOP}{}(\cdot) is decidable, can generate unambiguously from \euscr{L} at any rate. Thus, under mild conditions, no stable algorithm can generate unambiguously from a non-identifiable collection. Moreover, we also prove an analog of Informal Theorem 7 in the online setting (see Theorem 3.7), which extends our earlier result for generation with breadth in the online setting (Informal Theorem 4). This raises several questions regarding unambiguous generation, which we leave as interesting open problems (see Section 1.4). Note that while this impossibility result has a benign requirement that the generator is stable, it already considerably extends our main result Informal Theorem 1, since any generator that achieves breadth must be stable – otherwise, its support cannot settle on the target language K. (Note that while Informal Theorem 1 requires the generator to not repeat training examples, any generator that repeats training examples can be converted into one that does not repeat training examples and vice-versa; see Remark 2.) 1.4 Takeaways, Discussion, and Open Problems We believe that a key takeaway of our results is that the question of \citetkleinberg2024language seems to open an avenue towards a formal modern theory of language generation bridging learning theory and traditional TCS fields, like complexity theory and formal languages. As we explain in the subsequent technical overview, our tools contribute to this direction by connecting classical lines of work on identification of formal languages tracing back to \citetgold1967language, \citetangluin1979finding,angluin1980inductive,angluin1988identifying and computability theory \citepsipser2012introduction,soare1999recursively, to modern learning paradigms such as learning curves \citepbousquet2021theory and language generation \citepkleinberg2024language,kalai2024calibrated. Next, we emphasize that our impossibility result (Theorem 3.3) is not a dead end for language generation. Instead, it illustrates the need for additional human feedback during the post-training process – which provides additional information over positive samples alone – to achieve effective language models. Indeed, if both positive and negative examples are available, then generation with breadth is achievable for all countable collections of languages.888This follows from the work of \citetgold1967language, which showed that any countable collection of languages can be identified with such feedback. Using appropriate batching and boosting, we show that this identification algorithm (which works in the limit) can be converted to a generation algorithm with breadth that achieves an exponential rate. Concretely, Theorem 3.11 shows how to identify at an exponential rate and Proposition 6.5 shows how to convert this to a generation algorithm. In other words, our results can be seen as further theoretical evidence of the benefits of post-training with human feedback, highlighting its importance in developing language models that achieve both consistency and breadth, and adding to prior theoretical results from \citetkalai2024calibrated. Further, we underline that even though we focus on a prompt-less generation setting \citepkalai2024calibrated,kleinberg2024language, most of our results immediately extend to a prompted setting using the approach of \citetkleinberg2024language. Remarks and Open Questions. We now state a few remarks regarding our results and pose some interesting open questions. First, as a byproduct of our results, we establish almost tight rates for identification and generation with positive examples (see Section 3.1 and Section 3.4 for formal statements and discussion). Obtaining tight rates for these tasks is an interesting problem. Next, our impossibility results capture a large class of language-generating algorithms but do not completely forbid consistent generation with breadth. An immediate open question is how much further we can extend the class of generating algorithms for which the impossibility result in Informal Theorem 1 holds. Open Question 1. Is there a class of generative algorithms for which the induced generators can be modeled as Turing machines and which achieve breadth and consistency for all countable collections of languages? Further, we also proved a more robust version of our main result (Informal Theorem 1), namely, Informal Theorem 7, which showed that no algorithm from a large class of generators can generate while making a “small” number of hallucinations or omissions (also see Section 3.3 for another robust version of Informal Theorem 1). It is interesting to understand if one can prove a more robust version of Informal Theorem 1. To this end, we propose the following problem. Open Question 2. What is the Pareto frontier of an approximate notion of breadth and consistency? In other words, if we fix a collection of languages and allow the generator to hallucinate at some given rate, what is the minimal fraction of the mass from the target language that this generator has to miss? Next, to the best of our knowledge, it is not possible to test if a language collection is identifiable in the limit (without access to a strong oracle); this, for instance, becomes evident by inspecting Angluin’s criterion for identifiable collections (see Definition 10). Hence, we would like to know the following: Open Question 3. Is there a best-of-both-worlds algorithm between consistent generation and generation with breadth, i.e., is there an algorithm that will always generate in the limit from the target language consistently but, whenever identification is possible, it will also achieve breadth? We make some initial progress on this question by showing that the algorithm proposed by \citetkleinberg2024language already achieves this best-of-both worlds guarantee, provided it has access to a subset oracle for \euscr{L} that answers queries of the form “is L_{i}\subseteq L_{j}?” (see Section B.2). Finally, our algorithm that achieves (almost) exponential rates for identification uses an algorithm for identification in the limit as a black box. However, our algorithm that achieves exponential rates for generation makes use of certain specific properties of the algorithm of \citetkleinberg2024language. Thus, we ask the following question. Open Question 4. Is there a black-box transformation from an algorithm that generates in the limit in the online setting to an algorithm that generates with exactly exponential rates in the statistical setting? 1.5 Further Related Works Our setting is based on the statistical formulation of \citetangluin1988identifying, who studied identification from stochastic examples in the limit. However, \citetangluin1988identifying does not provide any learning rates which is one of the main aspects of our work. In terms of techniques, our inspiration for the statistical rates comes from universal learning, initiated by \citet*bousquet2021theory and studied in \citetbousquet2021theory,kalavasis2022multiclass,hanneke2022universal,hanneke2023universal,attias2024universal,bousquet2023fine. However, as we have already explained there are various differences between our setting and our techniques (we provide a more extensive and self-contained discussion in Section D). Our work connects various disjoint strands of research and we discuss each one of them below. Theory on Hallucinations. In terms of rigorous evidence about hallucinations in LLMs, we have already mentioned the work of \citetkalai2024calibrated at the start of Section 1. The result of \citetkalai2024calibrated is that calibrated999The exact definition of calibration is not important for this work: a language model is calibrated if, roughly speaking, the strings that the model assigns probability mass p, appear in a p fraction of the true distribution \citepdawid1982well. language models must hallucinate. The fascinating implication of this result is that one can lower bound the rate of hallucination, i.e., the quantity \operatornamewithlimits{\mathbb{E}}_{S\sim\euscr{P}^{n},~{}x\sim\mathpzc{G}_{n% }}\mathds{1}\left\{x\notin K\right\}, by the extent of a model’s calibration. Their intuition is that the root of hallucinations are rare patterns in the training data. Informally, their main result (under assumptions on K and \euscr{P}) is that for any trained model \mathpzc{G}_{n} with n samples, the hallucination rate \operatornamewithlimits{\mathbb{E}}_{S\sim\euscr{P}^{n},~{}x\sim\mathpzc{G}_{n% }}\mathds{1}\left\{x\notin K\right\}\geq\widehat{R}-\mathrm{Mis}_{\euscr{P}}(% \mathpzc{G}_{n})-\nicefrac{{1}}{{\sqrt{n}}}, where \widehat{R} is the fraction of facts that only appear once in the training data and \mathrm{Mis}_{\euscr{P}}(\mathpzc{G}_{n}) is the amount of miscalibration of the model. Hence, if the model is calibrated, i.e., \mathrm{Mis}_{\euscr{P}}(\mathpzc{G}_{n})\approx 0, the hallucination rate is lower bounded by the rare facts’ rate. Compared to our work, their goal is to show a quantitative lower bound, which is obtained under assumptions on the training distribution \euscr{P} and the fact that the model is calibrated. Our goal is different: we want to understand whether a model can achieve breadth while avoiding hallucinations building on the recent work of \citetkleinberg2024language. We also refer the reader to \citetkalai2024calibrated for an extensive overview of applied works on hallucinations. \citet peng2024limitations use communication complexity to prove that the transformer layer is incapable of composing functions if the domains of the functions are large enough. This work could also be seen as rigorous evidence about the hallucinations of LLMs since function composition is a fundamental task for reasoning \citepguan2024mitigating. The work of \citetxu2024hallucination is also studying hallucinations of LLMs. They define hallucination as a failure to identify the target function which belongs to an uncountable collection of functions. This is significantly stronger than the definition we and prior works \citepkalai2024calibrated,kleinberg2024language have considered (making their impossibility results significantly easier to prove). Their main result is that all LLMs must hallucinate. This is easy to see: consider an LLM learning to predict the next element in a sequence of 0s and 1s, after observing only a finite prefix of the enumeration, it has no way of knowing the next element in the order (since they allow both continuations) and, hence, the target sequence cannot be identified. Finally, the work of \citetaithal2024understanding, which is mainly empirical, aims to explain hallucinations on the other important family of generative models, namely diffusion-based models, via mode interpolation which, in theory, relies on difficulties in approximating non-smooth parts of the score function. Language Learning. In our results, we make no implicit assumption about the architecture of our models; this is in accordance with the works of \citetsolomonoff1964formal,gold1967language,angluin1982inference,angluin1983inductive,angluin1988identifying,pitt1989probabilistic,kleinberg2024language. However, there are various works aiming at understanding language learning capabilities of specific architectures, e.g., \citephahn2020theoretical,elman1990finding,gers2001lstm,bhattamishra2020ability,hewitt2020rnns,merrill2019sequential,merrill2023parallelism,yao2021self,ebrahimi2020can. For instance, \citetliu2022transformers show that low-depth transformers can represent the computations of any finite-state automaton, while \citetsanford2024representational identify a particular mathematical problem that cannot be computed by single-layer multi-head transformers. The aforementioned works share some similarities with us in the sense that they focus on whether models can be trained to generate or recognize strings in a fixed formal language. \citetakyurek2024context study in-context language learning: the language model is prompted with a finite collection of strings from an unknown regular language (which changes across different tasks), and must infer the distribution over strings corresponding to the full language. In a similar spirit, \citetedelman2024evolution study in-context learning of Markov chains. Other related works are those of \citethahn2023theory,xie2021explanation that study conditions under which in-context learning can arise for language learning. \citet allen2023physics design context-free grammars and empirically study the consistent generation (accuracy) and breadth (diversity) of GPT models on these synthetic examples. In comparison to this work, we provide a theoretical treatment of the trade-off between consistency and breadth under a very abstract model, studied by \citetgold1967language,angluin1979finding,angluin1988identifying, kleinberg2024language. Our results indicate that, even in a very idealized framework, achieving (perfect) consistency and breadth is impossible. We view the empirical findings of \citetallen2023physics as an exciting indication that, in the real world (or more concretely in controlled experiments on “small” models and synthetic datasets), a balance between (imperfect) consistency and breadth is possible and modern LLMs can achieve it. Further understanding how much consistency and breadth one can achieve at the same time theoretically is an exciting direction. Finally, in a concurrent and independent work, \citetli2024generationlenslearningtheory also study language generation, interpreting it in a learning-theoretic setting reminiscent of the PAC framework and the online learning setting of \citetlittlestone1988learning. They propose “non-uniform generatability” – which relaxes “uniform generatability” \citepkleinberg2024language – and characterize the collections for which uniform and non-uniform generatability are achievable in the Gold-Angluin model; in particular, unlike \citetkleinberg2024language they also allow the collection \euscr{L} to contain uncountably many languages. These dimensions are analogs to the Littlestone dimension (and its extension to the non-uniform setting \citeplu2023non), which only holds for finite collections of languages. Moreover, they show the proposed dimension is incomparable to the VC dimension. Finally, they give analogous characterizations in the “prompted generation” setting, extending some of the results of \citetkleinberg2024language. Our work is orthogonal to theirs: first, we study trade-offs between generating with and without breadth – both in a statistical setting and the Gold-Angluin model – and, second, we study the “learning curves” for generation and identification in the framework of \citetbousquet2021theory. Probably Eventually Correct Learning. As we mentioned Gold’s model is a predecessor of the famous PAC model of \citetvapnik2013nature and \citetvaliant1984theory. A natural question is whether there is a conceptual meeting point for the two works. Is there a notion of “PAC learning in the limit?” The answer to this question is affirmative and comes from the field of algorithmic stability (see e.g., \citepalon2022private,moran2023bayesian,kalavasis2023statistical,bun2023stability,chase2023stability and the references therein), studied in the context of binary classification \citepmalliaris2022unstable. \citet malliaris2022unstable introduce the Probably Eventually Correct (PEC) model of learning. Here we fix a collection \euscr{L}=\{L_{1},L_{2},\dots\} of languages and a distribution \euscr{P} over positive and negative labeled examples (in contrast to the standard identification setting of Gold). PEC learning focuses on distributions \euscr{P} realizable by the collection \euscr{L} in the sense of \citetbousquet2021theory (see Section D). An algorithm is said to PEC learn \euscr{L} if for any realizable distribution \euscr{P}, with probability 1 over i.i.d. samples \left\{(x_{i},y_{i})\colon i\in\mathbb{N}\right\} drawn from \euscr{P}, there exists time t^{*}\in\mathbb{N} such that for all t\geq t^{*}, given \left\{(x_{i},y_{i})\colon 1\leq i\leq t\right\}, the algorithm outputs an L_{t}\in\euscr{L} such that \Pr_{(x,y)\sim\euscr{P}}[L_{t}(x)\neq y]=0\,. Malliaris and Moran give a combinatorial characterization of the collections of languages that are PEC learnable: a collection of languages \euscr{L} is PEC learnable if and only if it does not shatter an infinite Littlestone tree. We stress that, when the learner has access to positive and negative examples, the absence of an infinite Littlestone tree does not characterize identification in our setting. This is in stark contrast with binary classification. In particular, in Section D, we show that there exists a set of languages that have an infinite Littlestone tree, hence not learnable in the online setting of \citetbousquet2021theory, but it allows for identification in the limit with positive and negative examples. In fact, the collection we use in Example 3 is identifiable in the limit even with just positive examples. This already sets the stage for a starkly different landscape of optimal learning rates between the setting of \citetbousquet2021theory and \citetangluin1988identifying, as we will see in Section 3.1. As we said before, the online model of \citetgold1967language and the classical online setting of \citetlittlestone1988learning have various differences. \citetlu2023non studies non-uniform online learning in order to bridge the gaps between the inductive inference model of \citetgold1967language and classical online learning. In this setting, the adversary is oblivious and fixes the true language K in advance (as in Gold’s model). At each round, an example from K is revealed, the learner makes a prediction but then she observes feedback. The model is non-uniform in the sense that the mistake bound depends on K. Learning from Positive Examples. Learning from positive examples occurs very frequently in real-world applications and has been extensively studied. A lot of work has been done on learning from positive examples in Gold’s model of learning in the limit \citepgold1967language,angluin1980inductive,angluin1988identifying,berwick1986learning,shinohara1989inductive,zeugmann2005guided. Apart from that, an extension of Valiant’s PAC model has been also studied \citepnatarajan1987learning,denis1998pac. \citetnatarajan1987learning considered the setting where the learner only has access to positive examples and showed that even very simple classes such as halfspaces in two dimensions are not learnable from positive examples alone. \citetdenis1998pac relaxed this requirement: they study a setting where the learner has access to both positively labeled examples but also to unlabeled examples \citepdenis2005learning. At the heart of virtually all of the results in this line of work is the use of unlabeled samples in order to generate negative examples. When the original distribution is uniform, better algorithms are known: \citetde2014learning gave efficient learning algorithms for DNFs and LTFs, \citetfrieze1996learning,anderson2013efficient gave efficient learning algorithms for learning d-dimensional simplices. On the other side, \citeteldan2011polynomial,goyal2009learning give lower bounds for learning with positive examples. Recently, interest in learning from positive examples has sparked from work on truncated statistics (e.g., \citepdaskalakis2018efficient,daskalakis2019computationally,Kontonis2019EfficientTS,fotakis2020efficient,daskalakis2021statistical,de2023testing,de2024detecting,plevrakis2021learning,de2024detecting,diakonikolas2024statistical,lee2024efficient). \citetKontonis2019EfficientTS show how to learn concept classes of bounded Gaussian surface area from positive Gaussian examples and \citetlee2024efficient generalize this to show how to learn concept classes approximable by polynomials in the L_{2}-norm from positive examples. However, all these works focus on computationally efficient learning/testing while we focus on statistical consistency of identification and generation without any restrictions on computation time."
https://arxiv.org/html/2411.09607v1,Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the AutoNuggetizer Framework,"This report provides an initial look at partial results from the TREC 2024 Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation as a barrier to continued progress in information access (and more broadly, natural language processing and artificial intelligence), and it is our hope that we can contribute to tackling the many challenges in this space. The central hypothesis we explore in this work is that the nugget evaluation methodology, originally developed for the TREC Question Answering Track in 2003, provides a solid foundation for evaluating RAG systems. As such, our efforts have focused on “refactoring” this methodology, specifically applying large language models to both automatically create nuggets and to automatically assign nuggets to system answers. We call this the AutoNuggetizer framework. Within the TREC setup, we are able to calibrate our fully automatic process against a manual process whereby nuggets are created by human assessors semi-manually and then assigned manually to system answers. Based on initial results across 21 topics from 45 runs, we observe a strong correlation between scores derived from a fully automatic nugget evaluation and a (mostly) manual nugget evaluation by human assessors. This suggests that our fully automatic evaluation process can be used to guide future iterations of RAG systems.","This report examines the partial results from the TREC 2024 Retrieval-Augment Generation (RAG) Track. There is, obviously, tremendous excitement and interest in RAG, but we feel that the evaluation of RAG output remains deficient from many perspectives. Furthermore, the lack of standardized evaluations presents a barrier to continued progress in information access, and more broadly, natural language processing and artificial intelligence. We hope to harness the institutional knowledge and resources provided by the National Institute of Standards and Technology (NIST) via the Text Retrieval Conference (TREC) to tackle these challenges. Now in its 33rd year, TREC has led the way in many aspects of evaluation in information retrieval (IR), natural language processing (NLP), and beyond, producing many innovations that the community (both researchers as well as practitioners) take for granted today. We hope that these efforts will lay similar foundations for standardization of RAG evaluation in the future. Our central hypothesis is that the nugget evaluation methodology (Voorhees, 2003) provides a solid foundation for evaluating RAG systems. This methodology was originally articulated more than two decades ago in the context of the TREC Question Answering (QA) Track for evaluating answers to free-form “definition questions”. This matches our RAG setting, where for a given query, there are claims or facts (i.e., nuggets) from different documents that are combined into a fluent and cohesive natural language response. Given this starting point, our collective efforts have focused on “refactoring” the original nugget evaluation methodology in light of large language models (LLMs). Specifically, we leverage LLMs to both automatically create nuggets and automatically assign nuggets to system-generated answers. We implement our nugget evaluation methodology in the AutoNuggetizer framework. While this is not the first attempt to automate nugget evaluations (Lin and Demner-Fushman, 2005, 2006), the advent of LLMs provides opportunities that did not exist before. The biggest advantage of our approach is minimal reinvention of the wheel, in that we are able to leverage the existing body of work that has gone into exploring the nugget evaluation methodology, e.g., (Lin and Zhang, 2007; Dang and Lin, 2007). For aspects that are not directly impacted by our use of LLMs, we can continue to assert findings from the literature without needing to carefully revalidate those claims again. Furthermore, and unique to the TREC setup, we calibrate our fully automatic process against a (mostly) manual process whereby nuggets are created by NIST assessors semi-manually and assigned manually to system-generated answers by the same assessors. It is important to qualify that this report focuses only on answer content (in terms of nuggets), and does not take into account support, or the requirement that the content be grounded by references into the underlying document collection. Thus, we do not consider possible LLM hallucinations at all. We will examine support in a separate report. To provide the tl;dr — Based on initial results across 21 topics from 45 runs, we observe a strong correlation between scores derived from a fully automatic nugget evaluation and a (mostly) manual nugget evaluation by NIST assessors. This suggests that our fully automatic evaluation process can be used to guide future iterations of RAG systems. This report attempts to provide the experimental results to support the above assertion.111We apologize in advance to our many colleagues who have done excellent work on RAG evaluation. Given the pressures of the strict TREC timeline, we did not have the opportunity to survey the field and acknowledge previous work in this report. Instead, we have decided to focus exclusively on our nugget evaluation methodology. A thorough discussion of the advantages and disadvantages of alternative approaches will be provided in a subsequent follow-up. Query: how did african rulers contribute to the triangle trade Answer: African rulers played a significant role in the triangular trade by capturing and supplying slaves to European traders. They actively participated in the trade by capturing individuals from inland areas and transporting them to the coast, where they were sold to European traders. This involvement was driven by the economic benefits they received, such as gold, guns, and other goods from Europe. Many African rulers and merchants developed new trade routes to avoid those who refused to cooperate, ensuring a steady supply of captives for the transatlantic slave trade. The trade was highly profitable for African rulers, who received a variety of European goods in exchange for slaves. This economic incentive led to the expansion of slave trade networks and the growth of states based on slavery, such as the Kingdom of Dahomey, which became one of the most prosperous nations due to its involvement in the slave trade. The triangular trade system involved three stages: the export of manufactured goods from Europe to Africa, the transportation of enslaved Africans to the Americas, and the return of goods such as sugar, coffee, and tobacco to Europe. African rulers were crucial in the first stage, where they exchanged captives for European goods, which were often luxury items or weapons that helped them gain military and political advantages over their rivals. Despite some African rulers voicing opposition to the practice, the lure of profits ensured their continued participation in the trade. The involvement of African rulers was essential for the success of the transatlantic slave trade, as European traders relied on them to capture and supply slaves from the African interior. This collaboration was fundamental to the operation of the triangular trade, as European traders were generally confined to coastal areas and depended on African intermediaries to procure slaves. In summary, African rulers contributed to the triangular trade by capturing and supplying slaves to European traders, driven by the economic benefits they received in exchange. Their involvement was crucial for the success and expansion of the transatlantic slave trade. Table 1: Sample answer from the TREC 2024 RAG Track for topic 2024-35227. Note that here we have purposely omitted citations in the answer since this report does not cover support."
https://arxiv.org/html/2411.09595v1,LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models,"This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. A primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce Llama-Mesh, a novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct a supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in a text-based format, effectively unifying the 3D and text modalities. Llama-Mesh achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance. ††footnotemark: 11footnotetext: Work completed during NVIDIA internship.","Large Language Models (LLMs) [52, 5] have demonstrated remarkable capabilities in understanding and generating human-like text, achieving success in applications such as conversational agents, code generation, and visual content reasoning [1, 16, 30]. Despite these advances, their generative abilities have primarily been limited to the textual content, restricting their utility for broader tasks. Our work seeks to extend LLMs into a new modality—3D mesh generation—unlocking significant potential for fields like computer graphics, engineering, robotics, and virtual/augmented reality. By enabling LLMs to generate 3D meshes from textual descriptions, we unify language understanding with 3D content creation, expanding the functional scope of LLMs. This approach paves the way for more intuitive and efficient workflows in 3D content creation driven by language-based instructions. However, integrating a new modality into an LLM is challenging, particularly in the tokenization process for processing the new modality. To the best of our knowledge, there have been no attempts to unify 3D mesh and text generation in a single framework. Some studies explored unifying image and text generation. Among these works [37, 55], a common approach is to train a new tokenizer such as a vector-quantized variational autoencoder (VQ-VAE) [51, 17] to encode the new modality into discrete tokens, which are used in training. However, this requires vocabulary expansion, increasing the adaptation’s learning cost. Additionally, this method introduces information loss during the auto-encoding process. To tackle these challenges, we introduce Llama-Mesh, a novel framework that enables large language models (LLMs) to generate 3D meshes by representing them as plain text. Our approach uses the OBJ file format, a widely adopted text-based standard for 3D models comprising vertex coordinates and face definitions, as shown in Figure 4. By treating these numerical values as a sequence of text, we convert 3D meshes into a format that LLMs can process directly, avoiding modifications to the tokenizer or the vocabulary, thus minimizing additional training overhead. This design capitalizes on the extensive knowledge embedded in pretrained LLMs. Figure 6 shows pretrained LLMs demonstrate a native ability to represent 3D structures in text — a capability our framework harnesses. We construct a supervised fine-tuning (SFT) dataset that includes text-3D pairs and interleaved text-3D dialogues. We fine-tune a pretrained LLaMA-3.1-8B-Instruct [16] model on our curated dataset. We find that LLMs can acquire complex spatial knowledge by learning the numerical values of meshes in textual format. After fine-tuning, our model demonstrates the ability to (1) generate 3D meshes given text prompts, (2) produce interleaved outputs of text and 3D meshes in a conversational setup, and (3) describe meshes in natural language. Llama-Mesh is the first successful effort to empower an LLM to generate 3D content with language, unifying the 3D and text modalities in a single large model. It achieves mesh generation quality comparable to models trained from scratch while maintaining strong text generation abilities. Figure 3: Gallery of generations from Llama-Mesh. We can generate high-quality and diverse meshes with artist-like created topology."
https://arxiv.org/html/2411.09510v2,Communication Compression for Tensor Parallel LLM Inference,"Large Language Models (LLMs) have pushed the frontier of artificial intelligence but are comprised of hundreds of billions of parameters and operations. For faster inference latency, LLMs are deployed on multiple hardware accelerators through various Model Parallelism strategies. Our paper looks into the details on one such strategy - Tensor Parallel - and proposes to reduce latency by compressing inter-accelerator communication. We leverage fine grained quantization techniques to compress selected activations by 3.5 - 4.5x. Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) with negligible model performance degradation.","Large Language Models (LLMs) have become essential across various applications due to their exceptional performance. As model performance tends to improve with increased parameter counts, LLMs have been significantly scaled in recent years, with contemporary models now reaching 500B+ parameters [Chowdhery et al., 2023]. Deploying such large models for inference presents major challenges [Pope et al., 2022]. Tensor Parallel [Shoeybi et al., 2020] addresses this by splitting layers on multiple accelerators, enabling the execution of extremely large models and significantly reducing latency. However, Tensor Parallel demands accumulation of results from accelerators, as shown in Figure 1, and can lead to data communication bottlenecks [Zhuang et al., 2024, Agrawal et al., 2024], especially during the first auto-regressive inference step (the prefill phase). One approach to mitigate these bottlenecks, and thus reduce model latency even further, is to quantize activations before communication, which reduces the time needed to accumulate results from accelerators in a Tensor Parallel group. However, the presence of outliers [Dettmers et al., 2022, Lin et al., 2023] complicates this strategy, necessitating fine-grained quantization approaches. We leverage such approaches proposed by Rouhani et al. [2023] to compress activations and demonstrate the potency of communication compression by measuring time-to-first-token (TTFT) in realistic inference scenarios using different inference hardware setups. We find that for hardware setups which have slower inter-accelerator bandwidths, the TTFT can be improved by 3.5 - 4.5x with negligible degradation of model performance."
https://arxiv.org/html/2411.09436v1,"Robot Tasks with Fuzzy Time Requirements
from Natural Language Instructions","Natural language allows robot programming to be accessible to everyone. However, the inherent fuzziness in natural language poses challenges for inflexible, traditional robot systems. We focus on instructions with fuzzy time requirements (e.g., “start in a few minutes”). Building on previous robotics research, we introduce fuzzy skills. These define an execution by the robot with so-called satisfaction functions representing vague execution time requirements. Such functions express a user’s satisfaction over potential starting times for skill execution. When the robot handles multiple fuzzy skills, the satisfaction function provides a temporal tolerance window for execution, thus, enabling optimal scheduling based on satisfaction. We generalized such functions based on individual user expectations with a user study. The participants rated their satisfaction with an instruction’s execution at various times. Our investigations reveal that trapezoidal functions best approximate the users’ satisfaction. Additionally, the results suggest that users are more lenient if the execution is specified further into the future.","Automating household tasks and production in small and medium enterprises is attracting considerable attention in robotics (e.g., [1, 2]). Robot programming is still mostly relegated to specialized robotics experts, resulting in high costs and slow adaptation to new situations [2]. One response to this is to increase the accessibility of robot programming [3, 4], for example, with natural language [5]. Natural language contains inherent fuzziness that reduces the user’s cognitive load. However, this contrasts with the rigid parameters (like concrete execution times) demanded by traditional robot systems [6]. For example, the instruction “Prepare some food in about ten minutes!” (Fig. 1) contains fuzziness regarding parameters (“some food”) and execution time (“about ten minutes”). To handle fuzzy parameters, fuzzy-logic [7] is commonly used to deduce exact parameters (e.g., weight in grams) for the operating robot system [8, 9]. For time-dependent parameters, fuzzy-logic is extended to temporal fuzzy logic [10]. However, previous research focused hardly on the user’s perception of fuzziness regarding execution time. For instance in Fig. 1, there is no immediate loss if the robot prepares the food five minutes earlier or later. Nevertheless, the user may be dissatisfied if the operation is not performed around the specified time, which depends on the instruction and the context. In this example, execution after 15 minutes leads to higher user satisfaction than after 20 minutes. Figure 1: Natural language instructions are inherently fuzzy, requiring an interpretation within the context of the instruction and instructor. Here, the user’s satisfaction varies over time based on the start of the task execution. This paper investigates instructions with fuzzy time requirements. An instruction describes a fuzzy skill that encodes the manipulation of an object by the robot (in continuation of [11, 12]). If the user instructs only one fuzzy skill, it can be executed at maximum satisfaction by default. However, suppose the user instructs several fuzzy skills combined into one superordinate plan, the fuzzy task. In that case, the task execution may require compromises, i.e., the robot must perform some skills at a suboptimal time. For example, if the user issues another command that should also start in ten minutes (Fig. 1), rigid robot systems lack the knowledge of which operation to prefer. The satisfaction function provides this required knowledge – enabling the scheduling to maximize overall satisfaction. Precisely identifying the expectations of an individual user before execution is challenging. In addition to the given instruction itself, other aspects could also influence satisfaction, e.g., the context of the scenario, the user’s previous experience, or the expected abilities of the actor. Another challenge is that several users with (partly) divergent expectations may instruct the robot system. One response to this could be the creation of user profiles. However, this does not account for frequent user changes or public scenarios with previously unknown users. Hence, we aim to provide general statements about deriving the satisfaction functions from the instructions and context. We present two central contributions: (i) We formalize fuzzy tasks, their inference from language, and the scheduling (Section III-A and Section III-B). (ii) We deduce an overall satisfaction function from individual user satisfaction (Section III-C). On this basis, we examine fuzzy time requirements regarding their modeling, the difference between human and robot actors, and the influence of time until the required execution starts (Section IV). For this, we exploit subjective satisfaction data gathered with an online user study."
https://arxiv.org/html/2411.09389v1,Less is More: Unseen Domain Fake News Detection via Causal Propagation Substructures,"The spread of fake news on social media poses significant threats to individuals and society. Text-based and graph-based models have been employed for fake news detection by analyzing news content and propagation networks, showing promising results in specific scenarios. However, these data-driven models heavily rely on pre-existing in-distribution data for training, limiting their performance when confronted with fake news from emerging or previously unseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news is a challenging yet critical task. In this paper, we introduce the Causal Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to enhance zero-shot fake news detection by extracting causal substructures from propagation graphs using in-distribution data and generalizing this approach to OOD data. The model employs a graph neural network-based mask generation process to identify dominant nodes and edges within the propagation graph, using these substructures for fake news prediction. Additionally, CSDA’s performance is further improved through contrastive learning in few-shot scenarios, where a limited amount of OOD data is available for training. Extensive experiments on public social media datasets demonstrate that CSDA effectively handles OOD fake news detection, achieving a 7%\sim16% accuracy improvement over other state-of-the-art models.","The popularity of social media has enabled rapid news dissemination, for both true and fake news. Given the potential impact of fake news, robust fake news detection methods are needed to debunk such news in a timely manner. In real-world scenarios, out-of-distribution news from unseen domains emerges over time. This brings substantial challenges to fake news detection models. Graph-based fake news detection methods using graph neural networks (GNN) have garnered much attention recently for modelling news propagation patterns (Gong et al. 2023a). Despite their success, existing GNN-based methods are generally built on the assumption that both training and testing data are independently sampled from an identical data distribution (i.i.d.), which often does not hold true nor reflect the real challenges of fake news detection (Li et al. 2022). Emerging and hitherto unseen fake news and their associated propagation graphs can and do appear. From an empirical perspective, these methods focus on minimising the average training error and incorporating correlations within the training data (which is considered to be in-distribution) to improve fake news detection accuracy (Liu et al. 2021). However, real-world graph-based fake news data is often mixed with biased domain-specific information in the training data. The detection model may thus learn these domain-specific biases resulting in misclassification of cross-domain news items (Li et al. 2022). To detect fake news across different domains (e.g., sports and politics), some early studies (Ma, Gao, and Wong 2018; Bian et al. 2020) focus on capturing content-independent propagation patterns. However, it has been shown (Min et al. 2022) that not only the news contents but also the propagation patterns can vary across different news domains. More recent approaches (Li et al. 2023; Lin et al. 2022) collect and manually label a small dataset from emerging news domains. They utilise domain adaptation methods to adapt the trained models to the emerging domains in a few-shot manner. However, these approaches require labelled data from emerging domains which is not always available and could be expensive and time-consuming. To address the limitations above, we focus on extracting causal subgraphs from news propagation graphs to eliminate potential domain biases. The patterns of such subgraphs are learnt for fake news detection in emerging domains. News from an emerging domain is considered as the out-of-distribution (OOD) data, and we generalise our model trained on in-distribution data to OOD data by capturing causal subgraphs in an unsupervised manner. From a causal analysis perspective, each propagation graph is composed of causal subgraph and biased subgraph which are initially entangled. Our intuition is that not all nodes in the propagation graph of a given news item are helpful for fake news detection. Instead, only some causal subgraphs of the propagation graph carry critical clues that can be used to identify fake news, as illustrated in Fig. 1 with an example. If we can identify and capture such causal subgraphs, we can improve fake news detection accuracy and subsequently improve the way we generalise the model to OOD data. Figure 1: Illustration of the causal subgraphs and the Structure Causal Models (SCMs). In the SCMs, the grey and white variables represent unobserved and observed variables. Further explanations on SCMs are given in Preliminaries. Based on this intuition, a cross-domain model – the Causal Subgraph Oriented Domain Adaptive Fake News Detection (CSDA) model, is proposed. This model extracts subgraphs from propagation graphs and performs detection based on the subgraphs. In CSDA, a binary mask is learned for each node and each edge of the propagation graph of a news item to classify them into causal or biased elements. For the subgraph formed by each type of element, a graph encoder and a multilayer perceptron (MLP) classifier together encode the subgraphs and classify the news item according to the subgraph embeddings. In the training process, we utilise a data augmentation strategy by concatenating the causal subgraph embedding and the permuted biased subgraph embedding. We then train CSDA with both embeddings to enhance the effectiveness of causal subgraph learning. In the testing process, only the causal branch of the CSDA model is utilised to predict news veracity. Following recent works (Li et al. 2023; Lin et al. 2022), we also consider a scenario where limited OOD data becomes available through manual labelling. In this scenario, CSDA’s performance on OOD data is further enhanced with a supervised contrastive learning-based approach and achieves state-of-the-art (SOTA) classification accuracy. In summary, our contributions include: • We propose a zero-shot cross-domain fake news detection model named CSDA based on extracting causal subgraphs related to news propagation patterns. • We further explore a few-shot scenario in cross-domain fake news detection where a small number of OOD examples are available, and we utilise contrastive learning to enhance CSDA’s cross-domain performance. • Extensive experiments are conducted on four real datasets. The results confirm the effectiveness of CSDA in cross-domain fake news detection, outperforming SOTA models by 7.69\sim 16.00\% in terms of accuracy."
https://arxiv.org/html/2411.09339v1,Re-Parameterization of Lightweight Transformer for On-Device Speech Emotion Recognition,"With the increasing implementation of machine learning models on edge or Internet-of-Things (IoT) devices, deploying advanced models on resource-constrained IoT devices remains challenging. Transformer models, a currently dominant neural architecture, have achieved great success in broad domains but their complexity hinders its deployment on IoT devices with limited computation capability and storage size. Although many model compression approaches have been explored, they often suffer from notorious performance degradation. To address this issue, we introduce a new method, namely Transformer Re-parameterization, to boost the performance of lightweight Transformer models. It consists of two processes: the High-Rank Factorization (HRF) process in the training stage and the de-High-Rank Factorization (deHRF) process in the inference stage. In the former process, we insert an additional linear layer before the Feed-Forward Network (FFN) of the lightweight Transformer. It is supposed that the inserted HRF layers can enhance the model learning capability. In the later process, the auxiliary HRF layer will be merged together with the following FFN layer into one linear layer and thus recover the original structure of the lightweight model. To examine the effectiveness of the proposed method, we evaluate it on three widely used Transformer variants, i. e., ConvTransformer, Conformer, and SpeechFormer networks, in the application of speech emotion recognition on the IEMOCAP, M3ED and DAIC-WOZ datasets. Experimental results show that our proposed method consistently improves the performance of lightweight Transformers, even making them comparable to large models. The proposed re-parameterization approach enables advanced Transformer models to be deployed on resource-constrained IoT devices.","With the proliferation of Internet-of-Things (IoT) devices, deploying artificial intelligence (AI) models on IoT devices becomes imperative to enable real-time intelligent services while preserving privacy [1, 2, 3, 4]. Speech emotion recognition (SER) is a typical application that needs to be deployed on IoT devices, such as smart home assistants and healthcare wearables, due to its privacy and real-time requirements [5, 6, 7, 8]. After the inception of Transformer [9], it has received tremendous success in natural language processing (NLP), speech processing, and computer vision, and continuously achieves state-of-the-art (SOTA) performance in diverse applications, such as emotion recognition, language translation, dialogue systems, and speech recognition [10, 11, 12]. Such great success is not only due to the considerable increase of training data and the optimization of Transformer architecture, but also largely attributed to the remarkable expansion of model size [10, 13]. For example, the Transformer decoder-based language model of GPT-3 has 175 billion parameters [14] and the model Pangu-\Sigma has even more than one Trillion parameters [15]. The contribution of model size to the model performance has been comprehensively and empirically investigated in NLP [16]. Meanwhile, in many scenarios, it is needed to deploy models on IoT devices because of the privacy and security, and low-latency requirements [17, 18, 19, 20]. For example, the tasks of emotion recognition and health screening and diagnosis, just to name a few, highly relate to users’ personal information, and require keeping the data on their own devices to protect their privacy and security. Besides, other tasks such as command recognition, face detection, and autonomous driving system are always on and require real-time decision-making. However, these IoT devices often lack sufficient storage memory, computational resources, energy, and network bandwidth. The contradiction between the complexity (e. g., model size and computational operations) of Transformer models and the resource-constrained IoT devices has crucially hindered the on-device deployment of Transformer-style models. To deal with this challenge, plenty of model compression and optimization approaches have been introduced to date [21, 22], including pruning [23], knowledge distillation [24], and matrix decomposition [25]. However, with these model compression approaches, especially when compacting the model into a tiny size one, the performance of the lightweight Transformer often significantly degrades due to the reduction of its learning capability [17, 26]. To address this issue, we propose a novel approach, namely Transformer re-parameterization, to boost the performance of lightweight Transformer models without any increase of model size and computational operations. This Transformer re-parameterization approach comprises two processes, i. e., the High-Rank Factorization (HRF) process in the training stage and the de-High-Rank Factorization (deHRF) process in the inference stage. Specifically, in the HRF process, we insert an expanded linear layer before a feedforward network (FFN) of the Transformer. By doing this, it can endow the model with a better learning capability in the training stage due to the increase of model parameters. This process can be algebraically reversed. Thus, in the inference stage, the inserted linear layer will be removed by merging it together with its followed dense layer into one new dense layer with a mathematical calculation. By this means, the original lightweight Transformer architecture will be reconstructed without adding new parameters and computational operations in the inference stage whilst maintaining the boosted model performance. We evaluate the introduced approach in SER due to its broad applications, such as human-machine interaction, call centers, and mental health tracking [27, 12]. The present work is highly motivated by the ‘scaling law’ of deep models [28, 29], where the model performance has a power-law relationship with the model size. This is not only empirically demonstrated via various tasks and models [28, 29], but also mathematically explained through the theory that an interpolated/over-parameterized classifier is more generalized to the unseen data than a small model [30]. In this work, we focus on the Transformer models and investigate their different modules, including the queries, keys, and values (QKV) for attention mechanisms, Projection, FFN, classification head (CLS), and their combinations. To the best of our knowledge, this is the first work for enhancing the lightweight Transformers via the lossless model re-parameterization strategy. Our major contributions can be summarized as follows: • We, for the first time, introduced a novel re-parameter-ization strategy for enhancing the lightweight Transformers, to the best of our knowledge. It empowers lightweight Transformers with better learning capability in the training process whilst no performance loss in the inference stage, without the price of model size and computational operation. • We comprehensively investigated the proposed re-parameterization strategy in different modules of Transformer, including QKV for attention mechanisms, projection, FFN, CLS, and their combinations. • We extensively evaluate the effectiveness and generalizability of our approach in the context of SER, a domain with a strong imperative for on-device deployment due to privacy concerns. The remainder of this paper is organized as follows. We first introduce related work in Section II, and then formalize the problem statement and elaborately describe the proposed method in Section III. After that, the experimental setups and experimental results are given in Section IV. Finally, we draw the conclusion in Section V."
https://arxiv.org/html/2411.09259v1,Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey,"The rapid evolution of multimodal foundation models has led to significant advancements in cross-modal understanding and generation across diverse modalities, including text, images, audio, and video. However, these models remain susceptible to jailbreak attacks, which can bypass built-in safety mechanisms and induce the production of potentially harmful content. Consequently, understanding the methods of jailbreak attacks and existing defense mechanisms is essential to ensure the safe deployment of multimodal generative models in real-world scenarios, particularly in security-sensitive applications. To provide comprehensive insight into this topic, this survey reviews jailbreak and defense in multimodal generative models. First, given the generalized lifecycle of multimodal jailbreak, we systematically explore attacks and corresponding defense strategies across four levels: input, encoder, generator, and output. Based on this analysis, we present a detailed taxonomy of attack methods, defense mechanisms, and evaluation frameworks specific to multimodal generative models. Additionally, we cover a wide range of input-output configurations, including modalities such as Any-to-Text, Any-to-Vision, and Any-to-Any within generative systems. Finally, we highlight current research challenges and propose potential directions for future research. The open-source repository corresponding to this work can be found at https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.","In recent years, multimodal generative models have made significant advancements in both understanding and generation [1, 2]. For multimodal understanding, Multimodal Large Language Models (MLLMs) [3, 4, 5] have demonstrated notable capabilities in Any-to-Text comprehension, excelling in tasks such as visual, audio, and video question-answering [6, 7, 8]. For multimodal generation, denoising diffusion probabilistic models (DDPMs) [9] have achieved impressive performance in Any-to-Vision generation [10, 11, 12, 13]. Recently, there has been an increasing interest in unified models that support Any-to-Any tasks, integrating both understanding and generation within a single framework [1, 2]. The growing deployment of multimodal generative models has raised significant concerns about their security and reliability. Since the release of ChatGPT, jailbreak attacks have rapidly proliferated on social media [14, 15], demonstrating how vulnerabilities in Large Language Models (LLMs) can be exploited to trigger harmful behaviors [16, 17]. Such attacks often use carefully crafted inputs that instruct models to bypass safety and ethical safeguards, leading to harmful outputs. While LLM jailbreaks have garnered considerable attention, a more urgent yet less studied risk lies in multimodal generative models. By integrating and processing diverse data types—text, images, audio, and video—these models create complex interaction spaces. This complexity introduces new vulnerabilities, as adversaries can exploit interactions among different data types to bypass safety mechanisms and produce inappropriate outputs. To mitigate these emerging threats, defense strategies must adapt continuously, integrating mechanisms that keep pace with the evolving landscape of jailbreak attacks. Figure 1: Illustrated examples of jailbreak attacks on multimodal generative models to induce harmful outputs across various modalities, including harmful text via the Jailbreak in Pieces [18], harmful images via the MMA-diffusion [19], harmful videos via the T2VSafetyBench [20] and harmful audio via the Voice Jailbreak [21]. Figure 2: Jailbreak attack and defense against multimodal generative models. Existing reviews on jailbreak methods [22, 23, 24, 25] have primarily concentrated on content output within a specific modality, addressing tasks either Any-to-Text [26, 23] or Any-to-Image [24, 25]. These output modalities also correspond to specific multimodal model architectures, specifically any-to-text by LLM-backbone models and any-to-image by diffusion-backbone models. While these surveys have made valuable contributions, they lack a unified framework that spans a broad range of modalities (i.e., text, images, audio, and video) within different generative systems, as shown in Fig. 1. To address this gap, our survey introduces the first unified framework which systematically summarizes jailbreak attacks and defense mechanisms across various input-output modalities and different generative structures. Specifically, we break down the lifecycle of multimodal jailbreak and abstract four discrete levels – input, encoder, generator, and output. This structured approach helps bridge the gaps between different models, each of which may have unique architecture but share common vulnerabilities within these four key levels. We outline the four general steps (as shown in Fig. 2) to devise jailbreak attack and defense techniques: 1. Input Level: Attackers and defenders operate solely on the input data. Attackers modify inputs to execute attacks, while defenders incorporate protective cues to enhance detection. 2. Encoder Level: With access to the encoder, attackers optimize adversarial inputs to inject malicious information into the encoding process, while defenders work to prevent harmful information from being encoded within the latent space. 3. Generator111We refer to the entire generative model as the “Generator” without loss of generality. Level: With full access to the generative models, attackers leverage inference information, such as activations and gradients, and fine-tune models to increase adversarial effectiveness, while defenders use these techniques to strengthen model robustness. 4. Output Level: With the output from the generative model, attackers can iteratively refine adversarial inputs, while defenders can apply post-processing techniques to enhance detection. Note that we encompass a broader range of input-output modality configurations, including text, image, audio, and video, alongside multiple types of multimodal generative models such as Any-to-Text, Any-to-Vision, and Any-to-Any models. Meanwhile, we conduct a comparative analysis of various evaluation datasets and metrics used for benchmarking, along with insightful observations and suggestions for future research directions. By highlighting the landscape of jailbreak attacks against multimodal generative models, our survey enhances the understanding of security challenges and provides direction for developing effective defenses. We aim to equip researchers, practitioners, and policymakers with valuable insights to safeguard foundation models against malicious exploitation. In summary, our key contributions are as follows: • Through a comprehensive review of existing attack methodologies (see TABLE III) and defense strategies (see TABLE IV), we abstract and summarize a general categorization for launching and defending jailbreak against multimodal generative models, comprising four distinct stages (see Fig. 2). • We present a comprehensive and systematic review of attack, defense, and evaluation strategies across various input-output modalities and different model structures. • We thoroughly discuss the limitations, challenges, and future directions for real-world applications, facilitating future research in this domain. The remaining paper is organized as follows. We first provide a brief introduction to the preliminaries in Section II, which cover essential topics and concepts for the proper understanding of this work. In Section III and Section IV, we summarize existing approaches for jailbreak attacks and defense strategies, based on the stages of interaction with generative models respectively, including input-level, encoder-level, generator-level, and output-level. Section V introduces the commonly used datasets and evaluation metrics. Moreover, we provide discussions and future research opportunities in Section VI. Finally, we conclude this review in Section VII."
https://arxiv.org/html/2411.09018v1,"Bridging the Visual Gap: Fine-Tuning Multimodal Models 
with Knowledge-Adapted Captions","Recent research increasingly focuses on training vision-language models (VLMs) with long, detailed image captions. However, small-scale VLMs often struggle to balance the richness of these captions with the risk of hallucinating content during fine-tuning. In this paper, we explore how well VLMs adapt to such captions. To quantify caption quality, we propose Decomposed NLI (DNLI), an evaluation framework that breaks down generated captions into individual propositions, assessing each in isolation. This fine-grained analysis reveals a critical balance between capturing descriptive details and preventing hallucinations. Our findings show that simply reducing caption complexity or employing standard data curation techniques does not effectively resolve this issue. To tackle this challenge, we introduce Knowledge Adapted (KnowAda) fine-tuning, a data-centric approach that automatically adapts training data with the model’s existing knowledge and visual understanding. KnowAda minimizes hallucinations while preserving high descriptiveness. We validate this approach across several small-scale VLMs (up to 7B parameters) and dense caption datasets, demonstrating that KnowAda effectively balances hallucination reduction and descriptiveness. Our results show that KnowAda outperforms various baselines in both automatic metrics and human evaluations. We will release our code and models.","Fine-tuning pretrained multimodal models for generating dense image captions is common in both research and practical applications, such as assisting visually impaired individuals. Recent work has focused on creating high-quality, descriptive captions through human annotations Onoe et al. (2024); Garg et al. (2024); Deitke et al. (2024) and synthetic generations from models like GPT-4 Chen et al. (2023a) and Gemini Singla et al. (2024). These datasets help fine-tune models to produce detailed descriptions in specific styles, differing from the zero-shot captions of pretrained models. However, smaller multimodal models (e.g., up to 7 billion parameters), crucial for real-time applications, often struggle with capturing visual details during fine-tuning, leading to hallucinations. Consider a fine-tuning dataset for dense captioning, like the one in Figure 1 from DOCCI Onoe et al. (2024), alongside a pretrained vision-language model trained on tasks like captioning, VQA, and OCR. If, for instance, the model has only encountered low-resolution images during pretraining, it may struggle to identify fine details, such as the drawing on the purple van or the hotel name in the background, which require higher resolution. This issue extends beyond resolution to other visual challenges in modern VLMs Tong et al. (2024); Wu et al. (2024); Zhang et al. (2024a). We hypothesize that fine-tuning the model on overly complex captions may increase hallucinations, as the model is compelled to predict details it cannot accurately perceive or understand. Recent works on large language models (LLMs) have shown that fine-tuning primarily adapts pre-existing factual knowledge for specific tasks, with the majority of this knowledge being encoded during the pretraining phase Geva et al. (2020); Meng et al. (2022); Roberts et al. (2020). Rather than acquiring new information, fine-tuning typically activates and refines pretrained knowledge, which remains largely stable throughout the process Zhou et al. (2024b). Furthermore, Gekhman et al. (2024) demonstrated that fine-tuning on content not grounded in a model’s pre-existing factual-knowledge can lead to an increase in hallucinations. Motivated by these findings, we demonstrate that similar challenges extend beyond factual knowledge, affecting multimodal models when fine-tuned on information that is not rooted in their pretrained visual understanding. To better adapt pretrained models to dense caption datasets, we introduce KnowAda, a model-specific adaptation technique that simplifies complex details in dense captions. The KnowAda pipeline, shown in Figure 2, automatically identifies visual knowledge gaps between a pretrained VLM and an image-caption pair by generating questions related to the image’s visual content. It then modifies the captions to align with the model’s visual knowledge and capabilities, producing adapted captions that enhance fine-grained control while balancing a low hallucination rate with high descriptiveness. To evaluate dense image captions, two factors are crucial: descriptiveness, which captures the image’s details, and hallucination rate, which measures factual accuracy. Traditional metrics, which focus on similarity to reference captions Papineni et al. (2002); Banerjee and Lavie (2005); Zhang et al. (2019); Reimers (2019) or CLIP-based alignment Sarto et al. (2023); Radford et al. (2021), struggle with long captions. These metrics penalize valid variations in phrasing and miss the distinction between factual accuracy and token overlap or semantic similarity, making them inadequate for detecting hallucinations in detailed captions. Existing hallucination metrics focus on short captions or object-level errors Rohrbach et al. (2018); Ben-Kish et al. (2023). In response, we propose Decomposed NLI (DNLI) a new evaluation framework that decomposes captions into propositions and measures their entailment with the ground truth description. DNLI provides a more reliable assessment of both descriptiveness and accuracy and highly aligns with human scores. Our results show that training with KnowAda captions offers a favorable balance between descriptiveness and fidelity when fine-tuning LLaVA-7B, outperforming other data-centric baselines. To demonstrate the consistency of KnowAda across different models and datasets, we fine-tuned several multimodal models, ranging from 2 billion to 7 billion parameters, on two different dense captioning datasets. Across all models, KnowAda consistently reduced hallucinations compared to training with the original captions, as confirmed by both automatic and human evaluations. Stated explicitly, our contributions are: (I) We show that small-to-medium-scale VLMs underperform when fine-tuned directly on dense caption datasets, exhibiting increased hallucinations and reduced descriptive accuracy; (II) To address this we propose KnowAda, a model-dependent augmentation method that fills knowledge gaps in captions, reducing hallucinations while preserving high descriptive accuracy; (III) We demonstrate the effectiveness of KnowAda through extensive experiments, supported by both quantitative metrics and qualitative human evaluations; (IV) We introduce DNLI, a novel evaluation framework for dense captions that offers a more fine-grained analysis and shows strong correlation with human annotations. Figure 3: Dense captioning descriptiveness precision-recall results for LLaVA-7B fine-tuned with DOCCI captions, adapted using different methods. “Trimmed” refers to naive removal of sentences, while “Gemini” involves prompting Gemini to simplify the caption by removing difficult details of varying degrees. KnowAda consistently achieves the best precision-recall balance. The original dataset corresponds to KnowAda with a threshold of T=100\%, where no information is classified as unknown. Figure 4: DNLI Evaluation. Given a generated description by a VLM, we decompose it to atomic propositions. Then, we classify each proposition to either entailed, contradicted or neutral, conditioned on the ground-truth description. Finally, we calculate the descriptiveness and contradiction based on the number of entailed and contradicted propositions. Contradiction \downarrow Descriptiveness \uparrow Model FT Captions Precision Recall Precision Recall # Words Auto Human Auto Human Auto Human Auto Human PaliGemma Synthetic 38.9 19 30 15.5 47.9 81 39.2 67.8 72 PaliGemma Synthetic KA 32.4 18.4 20.8 13 55.2 81.6 36.4 61.2 54 TinyLLaVA Synthetic 38.1 52.1 49.1 45 35.1 47.9 26.3 40.8 71 TinyLLaVA Synthetic KA 40.2 34 40.2 22.2 48.1 66 25.4 40.9 51 LLaVA-7B Synthetic 39.1 19.3 39.1 16.2 47.3 80.7 39.7 65.3 79 LLaVA-7B Synthetic KA 31 15.8 31 9.3 58.4 84.2 34.5 47.4 54 PaliGemma Human 41.6 20.4 24.6 14.9 46.7 79.6 24.6 43.2 62 PaliGemma Human KA 38.3 18.3 22.2 13.6 49.4 81.7 28.7 38.9 65 TinyLLaVA Human 53 51.8 39.5 38.8 31.9 51.8 22.4 31.4 100 TinyLLaVA Human KA 42.6 34.5 19.6 12.5 46.9 65.5 19.1 22.5 53 LLaVA-7B Human 47.2 33.4 39.7 33.2 39.4 66.6 33.7 48.1 109 LLaVA-7B Human KA 33.7 17.1 16.7 11.2 56.9 82.9 25.8 31.8 55 Table 1: Dense captioning results over the test sets of DOCCI when fine-tuning on original human-annotated captions, synthetic captions, and KnowAda-adapted captions (denoted as KA) with a threshold of 20%. “Automatic (Auto)” refers to model-based NLI evaluation, while “Human” refers to evaluations based on human labeling."
https://arxiv.org/html/2411.09009v1,"Cut Your Losses 
in Large-Vocabulary Language Models","As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.https://github.com/apple/ml-cross-entropy","Progress in large language models (LLMs) has been fueled in part by an increase in parameter count, context length, and vocabulary size (the number of tokens that can be used to represent the input). As LLMs grew, so did the associated infrastructure. Large mini-batch gradient descent (Goyal et al., 2017) combined with data-parallelism (Hillis & Steele, 1986) enabled the harnessing of increasing computational power. ZeRO (Rajbhandari et al., 2020) broke the dependence between the number of GPUs and the memory used for model parameters, gradients, and optimizer state. Activation checkpointing (Chen et al., 2016) reduced the amount of memory used for activations, supporting the development of deeper models. FlashAttention (Dao et al., 2022) reduced the memory used in self-attention from O(N^{2}) to O(N), thereby supporting longer context windows. These improvements gradually shifted the memory consumption of LLM training to one single layer – the cross-entropy loss, whose memory footprint grows with the product of vocabulary size and number of tokens per batch. The cross-entropy loss is responsible for up to 90\% of the memory footprint of modern LLM training (see Fig. 1(a)). The problem grows only more acute with time, since even the largest contemporary vocabularies (e.g., 256K tokens) may benefit from further expansion (Tao et al., 2024). We propose a cross-entropy implementation, Cut Cross-Entropy (CCE), that has a negligible memory footprint and scales to arbitrarily large vocabularies. Our key insight is that computation of the loss and its gradient only depends on a single log-probability, that of the ground-truth label. With an arithmetic reformulation, we decompose the cross-entropy loss into an index matrix multiplication over a single ground-truth label and a log-sum-exp operation over all vocabulary entries for each token. Each operation has small and well-defined inputs – the network embeddings and classifier matrix – and a single scalar output per token. Both operations do, however, rely on a large intermediate logit matrix that computes the score for each token and potential vocabulary entry. We show that there is no need to materialize this logit matrix in GPU memory. Instead, we compute logits as needed in SRAM in a series of custom CUDA kernels. The result is a cross-entropy computation that has negligible memory footprint, with no detrimental effect on latency or convergence. See Fig. 1(b) for a breakdown of memory savings and consequent batch size increases afforded by CCE. (a) Regular cross-entropy (b) Cut cross-entropy (ours) Figure 1: Memory use and maximum attainable batch size (in millions of tokens) for a variety of frontier models on a 16-GPU (80 GB each) fully-sharded data-parallel setup (Rajbhandari et al., 2020) with activation checkpointing (Chen et al., 2016) and a mixed-precision 16-bit (fp16/bf16) AdamW optimizer (Kingma & Ba, 2015; Loshchilov & Hutter, 2019). For each model, we break its memory use down into weights and optimizer states, activation checkpoints, and the log-probabilities computed by the cross-entropy loss layer. Our Cut Cross-Entropy (CCE) enables increasing the batch size by 1.5x (Llama 2 13B) to 10x (GPT 2, Gemma 2 2B), with no sacrifice in speed or convergence. Exact values in Table A3."
https://arxiv.org/html/2411.09003v1,Refusal in LLMs is an Affine Function,We propose affine concept editing (ACE) as an approach for steering language models’ behavior by intervening directly in activations. We begin with an affine decomposition of model activation vectors and show that prior methods for steering model behavior correspond to subsets of terms of this decomposition. We then provide a derivation of ACE and test it on refusal using Llama 3 8B and Hermes Eagle RWKV v5. ACE ultimately combines affine subspace projection and activation addition to reliably control the model’s refusal responses across prompt types. We evaluate the results using LLM-based scoring on a collection of harmful and harmless prompts. Our experiments demonstrate that ACE consistently achieves more precise control over model behavior and generalizes to models where directional ablation via affine subspace projection alone produces incoherent outputs. Code for reproducing our results is available at https://github.com/EleutherAI/steering-llama3.,"According to the linear representation hypothesis (Elhage et al., 2022; Park et al., 2023), models represent high-level concepts as linear directions in activation space. This idea has led to the development of several techniques designed to change the behavior of large language models (LLMs) by performing linear modifications of residual stream activations. Directional ablation and activation addition are two such techniques that have both been shown to be effective. In this paper, we examine a unification and generalization of directional ablation and activation addition, which we call affine concept editing. We also test this on a recurrent neural network from the RWKV v5 family introduced by Peng et al. (2024). In this paper, we point out an ambiguity that has existed in most work on the linear representation hypothesis thus far. Specifically, the notion of “linear representation” or “direction” may either refer to a linear function – that is, a function that does not have a constant term – or an affine function, with a constant (bias) term that may or may not be zero. We show that this distinction matters in practice: steering methods that assume concepts are encoded linearly can sometimes cause a model to devolve into outputting nonsense, while a generalized affine method steers the model successfully. Figure 1: The effect of ACE (right) on several activation vectors (green circles), contrasted with the effect of CAA (left) and directional ablation (center). Class means are shown for presence (r^{+}) and absence (r^{-}) of the steered behavior. 1.1 Linear and affine decomposition Linear decomposition. Given an activation vector {\bm{v}} and a nonzero vector {\bm{r}}, we can decompose {\bm{v}} linearly into its orthogonal projections parallel to {\bm{r}} and perpendicular to {\bm{r}}: {\bm{v}}=\text{proj}_{\bm{r}}^{\perp}({\bm{v}})+\text{proj}_{\bm{r}}^{% \parallel}({\bm{v}}) Because \text{proj}_{\bm{r}}^{\parallel}({\bm{v}}) is parallel to {\bm{r}}, we can write it as a multiple of {\bm{r}}: {\bm{v}}=\text{proj}_{\bm{r}}^{\perp}({\bm{v}})+\alpha{\bm{r}} If we interpret {\bm{r}} as a direction corresponding to some behavior or concept, then we can interpret \alpha as the degree to which that behavior is represented in the activation vector {\bm{v}}. Note that \alpha is a linear function of {\bm{v}}: \alpha=\frac{{\bm{r}}}{\|{\bm{r}}\|^{2}}\cdot{\bm{v}} This framework suggests some possible steering interventions. For example, we may want to shift \alpha by a fixed amount \Delta\alpha, to increase or decrease the behavior, \displaystyle{\bm{v}}^{\prime} \displaystyle=\text{proj}_{\bm{r}}^{\perp}({\bm{v}})+(\alpha+\Delta\alpha){\bm% {r}}={\bm{v}}+\Delta\alpha{\bm{r}} Or set \alpha\mapsto 0 to “remove” it: \displaystyle{\bm{v}}^{\prime}=\text{proj}_{\bm{r}}^{\perp}({\bm{v}})={\bm{v}}% -\text{proj}_{\bm{r}}^{\parallel}({\bm{v}}). Or, by combining these, set \alpha to a new value \alpha^{\prime}, regardless of its original value: \displaystyle{\bm{v}}^{\prime} \displaystyle={\bm{v}}-\text{proj}_{\bm{r}}^{\parallel}({\bm{v}})+\alpha^{% \prime}{\bm{r}} As we will see, these correspond to existing steering methods in the literature. Affine decomposition. We argue that this framework is not sufficiently general. In particular, it assumes without justification that the origin (zero vector) is a meaningful “default value” for activations, corresponding to a given behavior/concept being “absent”. In practice, this assumption fails for multiple reasons; the typical level of most behaviors in a network is not “none”, and typical activations are often far from the origin. We extend the framework by allowing our decomposition to depend on both a vector {\bm{r}} and a point of reference {\bm{v}}_{0}, corresponding to some standardized degree of behavior. We now decompose {\bm{v}} as \displaystyle{\bm{v}} \displaystyle={\bm{v}}_{0}+\Delta{\bm{v}}={\bm{v}}_{0}+\text{proj}_{\bm{r}}^{% \perp}(\Delta{\bm{v}})+\text{proj}_{\bm{r}}^{\parallel}(\Delta{\bm{v}}) where \Delta{\bm{v}} is the displacement vector ({\bm{v}}-{\bm{v}}_{0}) from {\bm{v}}_{0} to {\bm{v}}. We again write this in terms of \alpha as {\bm{v}}={\bm{v}}_{0}+\text{proj}_{\bm{r}}^{\perp}(\Delta{\bm{v}})+\alpha{\bm{% r}} Note that {\bm{v}}_{0} determines the zero-point for \alpha: it is zero when {\bm{v}}={\bm{v}}_{0}, and not (in general) when {\bm{v}}=\bm{0}. In other words, \alpha is now an affine function of {\bm{v}}, with a linear term and a constant term: \displaystyle\alpha \displaystyle=\frac{{\bm{r}}}{\|{\bm{r}}\|^{2}}\cdot\Delta{\bm{v}}=\frac{{\bm{% r}}}{\|{\bm{r}}\|^{2}}\cdot{\bm{v}}-\frac{{\bm{r}}\cdot{\bm{v}}_{0}}{\|{\bm{r}% }\|^{2}} This also suggests a different set of steering tools. In particular, the operation that zeros out this version of \alpha is now given by \displaystyle{\bm{v}}^{\prime} \displaystyle={\bm{v}}_{0}+\text{proj}_{\bm{r}}^{\perp}(\Delta{\bm{v}})={\bm{v% }}-\text{proj}_{\bm{r}}^{\parallel}({\bm{v}})+\text{proj}_{\bm{r}}^{\parallel}% ({\bm{v}}_{0}) And the operation that sets it to a new value \alpha^{\prime} is given by \displaystyle{\bm{v}}^{\prime} \displaystyle={\bm{v}}-\text{proj}_{\bm{r}}^{\parallel}({\bm{v}})+\text{proj}_% {\bm{r}}^{\parallel}({\bm{v}}_{0})+\alpha^{\prime}{\bm{r}} Note that the {\bm{r}}-parallel component of {\bm{v}}_{0} determines the zero-point for \alpha. We can make this more explicit by defining \alpha_{0} such that \alpha_{0}{\bm{r}}=\text{proj}_{\bm{r}}^{\parallel}({\bm{v}}_{0}) and therefore {\bm{v}}^{\prime}={\bm{v}}-\text{proj}_{\bm{r}}^{\parallel}({\bm{v}})+\alpha_{% 0}{\bm{r}}+\alpha^{\prime}{\bm{r}} (1) This representation, derived by thinking about an affine decomposition of {\bm{v}}, realizes a generalization of two previously described steering methods. In the rest of this paper, we show that with sensible choices for \alpha_{0} and \|r\|\neq 0, this more complete, affine treatment of steering simultaneously gives more deterministic control over the refusal behavior and solves the problem of certain models generating nonsense when steered. 1.2 Standardized Steering In the context of steering, we define standardization as the deterministic control of a behavior with respect to the steering parameter. In other words, we say that a steering technique is “standardized” with respect to a parameter if fixing the parameter fixes the degree to which a behavior is exhibited, regardless of input prompt. A technique is not standardized if the degree of behavior depends upon the input prompt itself along with the parameter. Furthermore, we require that for a steering strategy to be considered standardized that setting the steering parameter to 1 should induce the behavior and setting it to 0 should induce the null-behavior. Tables 1 and 2 give examples of standardized versus non-standardized steering. In the context of refusal, we consider the behavior to be refusal and the null-behavior to be non-refusal. We feel that this choice is intuitive, but one could perform the same analysis having made the opposite choice. Standardization may or may not be desirable in any given application. If the goal is simply to bias a model toward a certain behavior rather than totally control it, then non-standardized techniques may be preferred. However, if the desire is to totally control the behavior—as often with refusal—then a standardized technique is required. Here, we show that contrastive activation addition is not a standardized method of steering refusal whereas directional ablation fails to generalize across models. We also show that ACE helps to simultaneously both of these shortcomings, being more standardized than CAA and more general across models than directional ablation."
https://arxiv.org/html/2411.08968v1,Sparse Upcycling: Inference Inefficient Finetuning,"Small, highly trained, open-source large language models are widely used due to their inference efficiency, but further improving their quality remains a challenge. Sparse upcycling is a promising approach that transforms a pretrained dense model into a Mixture-of-Experts (MoE) architecture, increasing the model’s parameter count and quality. In this work, we compare the effectiveness of sparse upcycling against continued pretraining (CPT) across different model sizes, compute budgets, and pretraining durations. Our experiments show that sparse upcycling can achieve better quality, with improvements of over 20% relative to CPT in certain scenarios. However, this comes with a significant inference cost, leading to 40% slowdowns in high-demand inference settings for larger models. Our findings highlight the trade-off between model quality and inference efficiency, offering insights for practitioners seeking to balance model quality and deployment constraints.","Recent advancements have made several small, open-source large language models (LLMs) widely available to practitioners, such as Llama 3 8B (Dubey et al., 2024), Gemma 2B, and Gemma 9B (Team et al., 2024). These dense decoder-only models are designed for inference efficiency: smaller than their flagship counterparts (often 70B+ parameters), but trained on trillions of tokens for improved quality. Despite their high quality, there remains a persistent demand for further improvements, particularly in downstream task performance. One common approach to improve model quality is continued pretraining (CPT), where the model is further trained on additional data, typically on a different dataset than the original pretraining data (Gupta et al., 2023). While CPT can improve model quality, it is limited by the original model’s dense architecture and parameter count. An alternative and increasingly popular approach is sparse upcycling (Komatsuzaki et al., 2023), which increases a model’s parameter count by converting a dense model into a Mixture-of-Experts (MoE) model. MoE architectures dynamically activate only a subset of weights (experts), allowing the model to expand its parameter count without proportionally scaling up training and inference FLOPs (Shazeer et al., 2017). Sparse upcycling a dense model into an MoE architecture has the potential to improve model quality, but it introduces a trade-off: sparse upcycled models are significantly larger than their dense counterparts, resulting in higher inference costs and limiting their utility for large-scale real-world deployments. Sparse upcycling thus conflicts with the growing trend to deploy smaller models specifically to decrease the cost of inference (Sardana et al., 2024). In this work, we ask: What is the trade-off between model quality and inference efficiency for sparse upcycling? We compare sparse upcycling to dense CPT across varying model sizes, compute budgets, and pretraining durations. By exploring these trade-offs, we provide insights for practitioners on how to balance performance gains with the practical costs of deploying LLMs."
https://arxiv.org/html/2411.08884v1,Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play,"As Large Language Models (LLMs) become more prevalent, concerns about their safety, ethics, and potential biases have risen. Systematically evaluating LLMs’ risk decision-making tendencies and attitudes, particularly in the ethical domain, has become crucial. This study innovatively applies the Domain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and proposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess LLMs’ ethical risk attitudes in depth. We further propose an novel approach integrating risk scales and role-playing to quantitatively evaluate systematic biases in LLMs. Through systematic evaluation and analysis of multiple mainstream LLMs, we assessed the ""risk personalities"" of LLMs across multiple domains, with a particular focus on the ethical domain, and revealed and quantified LLMs’ systematic biases towards different groups. This research helps understand LLMs’ risk decision-making and ensure their safe and reliable application. Our approach provides a tool for identifying and mitigating biases, contributing to fairer and more trustworthy AI systems. The code and data are available.","Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human language, showing significant potential across various domains. The outstanding performance of LLMs has sparked hope that they might be the AGI of our era Bubeck et al. (2023). As LLMs become more widely adopted, ranging from everyday use to specialized fields, the need for comprehensive evaluation, especially regarding safety, ethics, and biases, becomes increasingly urgent Chang et al. (2024). Currently, the widespread popularity of LLMs has led to the development of numerous evaluation benchmarks, tasks and metrics that examine LLMs from different angles Chang et al. (2024). Evaluations of LLMs’ risk attitudes are crucial for ensuring their safe and reliable application, especially in critical decisions such as health and finance Chang et al. (2024), particularly in modes where the LLM acts as an agent Zhao et al. (2024). In the field, while several benchmarks have been proposed to explore LLMs’ propensity to engage in harmful activities Yuan et al. (2024); Wang et al. (2023); Perez et al. (2022); Shi and Xiong (2024), such work remains relatively scarce. Compared to prior work, we not only innovatively apply interdisciplinary tools to evaluate LLMs’ risk attitudes across multiple domains and deeply assess ethical risk decision-making, but we are also, to our knowledge, the first to evaluate the biases within LLMs based on the risk analysis and role-play. Moreover, our work fills the gap in research AI psychology and cognitive science. Figure 1: Examine LLMs’ risk attitudes through various risk events across multiple domains (such as social and financial domains). What are the things they accept, and what are the things they have zero tolerance for? In this paper, We explore four key questions: (1) Do LLMs exhibit stable and measurable risk attitudes? (2) Are there specific differences or consistent patterns in risk attitudes across multiple domains among different LLMs? (3) What are LLMs’ risk propensities in the ethical domain, and how do these impact LLM safety? (4) Do LLMs exhibit systematic biases in their perception of risk attitudes and ethical levels for different social groups? We innovatively applies risk attitude assessment tools from human psychology, cognitive science and behavioral economics to AI systems, conducting a systematic, standardized, and quantitative evaluation of risk preferences in mainstream LLMs. Studies have shown that using standard psychometric inventories for LLMs is feasible and effective (Pellert et al., 2023; Li et al., 2024). We have chosen the DOSPERT Weber et al. (2002), widely used in social science researches Farnham et al. (2018); Shou and Olney (2020), as our assessment tool. DOSPERT places risk assessment within specific contexts across different domains, allowing for a comprehensive evaluation. DOSPERT has been widely validated and applied across different age groups and cultural backgrounds Blais and Weber (2006). In summary, we find that DOSPERT provides a promising framework for multi-dimensional analysis of LLMs’ ""risk personality"". Applying DOSPERT to LLMs represents an innovative interdisciplinary attempt. Futhermore, given that risk scores in the ethical domain are directly related to the safety of LLMs, we propose EDRAS to comprehensively and specifically evaluate LLMs’ risk attitudes in ethical domain. However, our research does not stop at simply assessing the risk propensities of LLMs. In the field of AI, systematic biases and stereotypes in algorithms have been a significant concern Mehrabi et al. (2021); Blodgett et al. (2020). These not only potentially exacerbate the spread of biases and promote social inequality but may also cause harm to certain groups. How to detect and quantify potential biases in LLMs, which may contain racial, gender, or other biases due to training from human text, is a crucial issue Demszky et al. (2023). We have discovered a novel approach using risk scale and role-play as a bridge to detect and quantify bias in LLMs, indirectly reflecting LLMs’ differential views on various social identities, occupations, ethnicities, and genders. The main contributions of this study include: • We verified that specific LLM possess differentiated and relatively stable risk propensities through multiple tests. • We conducted domain-specific risk propensities evaluations on multiple mainstream LLMs, explored specific differences and consistent patterns in LLMs’ risk attitudes. • We propose a novel EDRAS to further delve into the assessment of LLMs’ ethical decision-making risk attitudes. • We designed different role hypotheses, using risk scales to quantitatively explore what LLMs consider to be the different risk attitudes for various social groups, and discuss potential systematic biases. Models \mathbf{\mu_{5}} \mathbf{M_{5}} \mathbf{{max}_{5}} \mathbf{{min}_{5}} \mathbf{\Sigma^{\mathbf{2}}_{\mathbf{5}}} \mathbf{\Sigma_{5}} \mathbf{\Gamma_{5}} \mathbf{\kappa_{5}} Claude 3.5 Sonnet 38.80 38.40 40.00 38.00 0.51 0.72 0.63 -1.05 ERNIE 3.5 47.36 47.20 48.80 45.20 1.83 1.35 -0.35 -1.18 moonshot-v1 49.04 49.20 50.40 46.80 1.83 1.35 -0.52 -1.08 GPT-4o mini 52.24 52.00 53.60 50.80 1.38 1.18 0.11 -1.71 GPT-3.5 Turbo 55.20 55.20 55.60 54.80 0.13 0.36 0.00 -1.75 Llama 3.2-3b 60.88 61.20 64.00 58.00 4.95 2.23 0.02 -1.47 Table 1: In basic DOSPERT, the scores obtained include the mean \mu, median M, maximum \mathrm{max}, minimum \mathrm{min}, variance \Sigma^{2}, standard deviation \Sigma, skewness \Gamma, and kurtosis \kappa. These scores are calculated by dividing the raw scores by the total possible score of 250. The \mathrm{max} and \mathrm{min} of the various models differ slightly, and the \Sigma^{2} and \Sigma are small, indicating a relatively high stability in scores. This suggests that the LLMs have a relatively stable risk trait. Additionally, the scores among the LLMs also show stable differences, reflecting distinct ""risk personalities."""
https://arxiv.org/html/2411.08870v1,The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models,"Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision-language models (VLMs) via continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining (DAPT) improves performance on downstream medical tasks, such as answering medical licensing exam questions. In this paper, we compare ten public “medical” LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-/few-shot prompting and supervised fine-tuning regimes for medical question-answering (QA).111This working paper substantially extends the results of Jeong et al. (2024) to include closed-ended tasks based on clinical notes in addition to medical-exam-style QA, as well as a comparison of performance when using medical versus general-domain models as an initialization for supervised fine-tuning. For instance, across all tasks and model pairs we consider in the 3-shot setting, medical LLMs only outperform their base models in 22.7% of cases, reach a (statistical) tie in 36.8% of cases, and are significantly worse than their base models in the remaining 40.5% of cases. Our conclusions are based on (i) comparing each medical model head-to-head, directly against the corresponding base model; (ii) optimizing the prompts for each model separately in zero-/few-shot prompting; and (iii) accounting for statistical uncertainty in comparisons. While these basic practices are not consistently adopted in the literature, our ablations show that they substantially impact conclusions. Meanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs can show performance improvements, but the benefits do not carry over to tasks based on clinical notes. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies.","1 Introduction Recent advances in autoregressive large language models (LLMs) and vision-language models (VLMs) have attracted interest from practitioners in medicine, where these models hold great potential to transform various aspects of clinical practice (e.g., medical diagnosis, information retrieval from clinical documents, patient triaging) (Fries et al., 2022a; Moor et al., 2023a). State-of-the-art performance on various medical benchmarks is typically achieved by massive-scale closed-source models, such as GPT-4 (OpenAI, 2023a, b), Med-Gemini (Saab et al., 2024; Yang et al., 2024), and Med-PaLM (Singhal et al., 2023a, b; Tu et al., 2024), often performing on par with humans on medical licensing exams and open-ended consumer health question-answering (QA) tasks. However, the general lack of transparency in these models, high API usage costs, and patient data privacy concerns make their integration into routine clinical workflows challenging (Marks and Haupt, 2023). To address such concerns, recent works have proposed cheaper, open-source alternatives through domain-adaptive pretraining (DAPT; Gururangan et al., 2020), where a pretrained open-source general-domain model—such as Llama (Touvron et al., 2023a, b; Meta, 2024) or Mistral (Jiang et al., 2023) in the language space, and LLaVA (Liu et al., 2023) or Open-Flamingo (Awadalla et al., 2023) in the vision-language space—is continually pretrained on biomedical (image-)text corpora from public sources such as PubMed and medical textbooks. While some prior works show that medical models pretrained from scratch only using domain-specific corpora can outperform those trained via DAPT, both in the context of BERT-style encoder-only models (Devlin et al., 2019; Gu et al., 2021; Yang et al., 2022) and decoder models (Taylor et al., 2022; Luo et al., 2022; Hernandez et al., 2023; Bolton et al., 2024), the DAPT approach has become common practice, resulting in a trend where the release of a more capable general-domain model is typically followed by the release of its medical counterpart. Despite the widespread adoption of medical DAPT, the claimed improvements in performance are worth scrutinizing. While the story is intuitive, more recent base models often already exhibit strong off-the-shelf performance on medical benchmarks without any adaptation. For instance, as of the time of writing, the general-domain Llama-3-8B (Meta, 2024) outperforms other medically specialized models such as MediTron-70B (Chen et al., 2023) and BioMistral-7B (Labrak et al., 2024) on the Open Medical LLM Leaderboard (Pal et al., 2024), which evaluates each model on standard medical QA benchmark datasets such as MedQA (Jin et al., 2020) and MedMCQA (Pal et al., 2022). Moreover, given the general lack of transparency about the pretraining corpora used to train the general-domain model in the first place, it is possible that they may already be trained on relevant medical text. Perhaps more concerning is the lack of apples-to-apples comparisons in the literature. First, medical models resulting from DAPT are often only compared against other baselines with different architectures and model scale (e.g., Clinical-Camel-70B (Toma et al., 2023) vs. GPT-4 (OpenAI, 2023a)). Second, even for the same model scale, models are often evaluated under inconsistent evaluation setups (e.g., MediTron-70B (Chen et al., 2023) fine-tuned on MedQA with gradient updates vs. Clinical-Camel-70B (Toma et al., 2023) few-shot prompted on MedQA). Third, the common practice of using a single, fixed prompting setup (e.g., prompt format, choice of few-shot examples) for all models under evaluation also warrants concern, as LLM/VLM behavior is extremely sensitive to such design decisions (Jiang et al., 2020; Zhao et al., 2021; Ceballos-Arroyo et al., 2024b), and the “optimal” choice of such details rarely correlates between different models (Sclar et al., 2024). All of these issues can confound the interpretation of results, when evaluating the performance benefits from medical DAPT. (a) (b) Figure 1: Medical LLMs and VLMs trained via domain-adaptive pretraining (DAPT) show limited improvement over their general-domain counterparts in zero-/few-shot prompting and supervised fine-tuning regimes. (a) Overview of our head-to-head evaluation approach for each pair of general-domain (blue) and medically adapted LLM/VLM (red). (b) Win, tie, and loss rates (%) of medical models vs. their corresponding base models across all (model pair, QA dataset) combinations. Win rate refers to the proportion of (model pair, QA dataset) combinations where a medical model shows a statistically significant improvement. In this paper, we perform an apples-to-apples comparison that addresses these concerns, comparing ten medical LLMs and two medical VLMs against their general-domain base models on various medical (visual) QA tasks (see Figure 1(a)). We compare several pairs of general-domain and medically adapted LLMs/VLMs (see Table 1), whose only differences lie in medical DAPT (i.e., one model is the base model, from which the other is derived via medical DAPT). For each pair, we compare their downstream performances from (i) zero-/few-shot prompting (Radford et al., 2019; Brown et al., 2020) and (ii) supervised fine-tuning. For the former, we follow the approach considered in Jeong et al. (2024) and compare the performances after independently selecting the “best” prompt format and few-shot examples for each model based on the validation set (Section 3.1). For the latter, we compare the performances after fine-tuning each model on the training set of each downstream QA dataset, using the best hyperparameters selected via grid search (Section 3.2). In both cases, we use the percentile bootstrap to assess whether the perceived improvements in performance from medical DAPT are attributable to chance. Table 1: Summary of open-source autoregressive VLM and LLM pairs used for evaluation. For Med42, we only list the top-five adaptation datasets with the highest mixture ratios. Model Class General Domain Medical Domain Medical Adaptation Corpora LLM Llama-3-70B-Instruct (Meta, 2024) Med42-v2-70B (Christophe et al., 2024b) Medical QA Datasets (e.g., MedQA, MedMCQA) Medical Instruction 120k (Altaf, 2023) OpenGPT (OpenChat) (Wang et al., 2024) StackExchange (Lambert et al., 2023) Medical Flashcards (Han et al., 2023) Llama-3-70B-Instruct (Meta, 2024) OpenBioLLM-70B (Pal and Sankarasubbu, 2024) Undisclosed Llama-2-70B (Touvron et al., 2023b) MediTron-70B (Chen et al., 2023) Clinical Practice Guidelines (e.g., CDC, WHO) PubMed Articles (S2ORC; Lo et al., 2020) Llama-2-70B (Touvron et al., 2023b) Clinical-Camel-70B (Toma et al., 2023) ShareGPT 20k PubMed Articles Published Before 2021 Random 4k Subset of MedQA (Jin et al., 2020) Llama-2-70B (Touvron et al., 2023b) Med42-v1-70B (Christophe et al., 2024a) Medical QA Datasets (e.g., MedQA, MedMCQA) OpenGPT (OpenChat) (Wang et al., 2024) StackExchange (Lambert et al., 2023) Medical Flashcards (Han et al., 2023) CORD-19 (Wang et al., 2020) Llama-3-8B-Instruct (Meta, 2024) Med42-v2-8B (Christophe et al., 2024b) Medical QA Datasets (e.g., MedQA, MedMCQA) Medical Instruction 120k (Altaf, 2023) OpenGPT (OpenChat) (Wang et al., 2024) StackExchange (Lambert et al., 2023) Medical Flashcards (Han et al., 2023) Llama-3-8B (Meta, 2024) OpenBioLLM-8B (Pal and Sankarasubbu, 2024) Undisclosed Llama-2-7B (Touvron et al., 2023b) MediTron-7B (Chen et al., 2023) Clinical Practice Guidelines (e.g., CDC, WHO) PubMed Articles (S2ORC; Lo et al., 2020) Mistral-7B-Instruct-v0.1 (Jiang et al., 2023) BioMistral-7B (Labrak et al., 2024) PubMed Articles (PMC Open Access Subset) Llama-2-7B-Chat (Touvron et al., 2023b) BioMedGPT-LM-7B (Luo et al., 2023) PubMed Articles (S2ORC; Lo et al., 2020) VLM LLaVA-v0-7B (Liu et al., 2023) LLaVA-Med-7B (Li et al., 2023a) PubMed Articles (PMC-15M; Zhang et al., 2023) Open-Flamingo-9B (Awadalla et al., 2023) Med-Flamingo-9B (Moor et al., 2023b) Medical Textbooks (MTB; Moor et al., 2023b) PubMed Articles (PMC-OA; Lin et al., 2023) Overall, we find that both medical LLMs and VLMs show limited improvement over their base models, across all medical QA tasks and evaluation settings that we consider (Figure 1(b)). In the zero-/few-shot prompting regime (Section 4), we find that all medical VLMs and the majority of medical LLMs fail to consistently outperform their base models across all datasets, including QA tasks focused on assessing medical knowledge and those based on real-world clinical notes (e.g., discharge summaries). In the supervised fine-tuning regime (Section 5), we find that the medical LLMs overall do show statistically significant improvements on medical knowledge QA tasks but not on the clinical note QA tasks, while the medical VLMs show little to no improvement on all of the visual medical QA tasks. Our findings also suggest that rigorous pairwise comparison of models, including tests for statistical significance, is essential to drawing reliable conclusions about the performance benefits from medical DAPT. Our main contributions can be summarized as follows: 1. We provide a comprehensive head-to-head comparison between state-of-the-art general-domain LLMs/VLMs and their medical DAPT counterparts on various medical QA benchmarks, to investigate the effectiveness of DAPT for medical specialization. 2. We find that when the prompts are optimized for each medical and general-domain model independently, the majority of medical models fail to improve over their general-domain counterparts in the zero-/few-shot prompting regime (Section 4.1). 3. We show that using a single, fixed prompt format and choice of few-shot examples for all models without statistical testing can lead to overly optimistic conclusions about the benefits from medical DAPT in the zero-/few-shot prompting regime (Section 4.2). 4. We find that in the supervised fine-tuning regime, all medical VLMs fail to show improvement, while medical LLMs show improvement on textual medical knowledge QA tasks but not on tasks based on clinical notes (Section 5)."
https://arxiv.org/html/2411.08868v1,CamemBERT 2.0: A Smarter French Language Model Aged to Perfection,"French language models, such as CamemBERT, have been widely adopted across industries for natural language processing (NLP) tasks, with models like CamemBERT seeing over 4 million downloads per month. However, these models face challenges due to temporal concept drift, where outdated training data leads to a decline in performance, especially when encountering new topics and terminology. This issue emphasizes the need for updated models that reflect current linguistic trends. In this paper, we introduce two new versions of the CamemBERT base model—CamemBERTav2 and CamemBERTv2—designed to address these challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use of the Replaced Token Detection (RTD) objective for better contextual understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked Language Modeling (MLM) objective. Both models are trained on a significantly larger and more recent dataset with longer context length and an updated tokenizer that enhances tokenization performance for French. We evaluate the performance of these models on both general-domain NLP tasks and domain-specific applications, such as medical field tasks, demonstrating their versatility and effectiveness across a range of use cases. Our results show that these updated models vastly outperform their predecessors, making them valuable tools for modern NLP systems. All our new models, as well as intermediate checkpoints, are made openly available on Huggingface https://huggingface.co/almanach?search_models=camembert+v2.","In recent years, French language models such as CamemBERT Martin et al. (2020) have become integral to businesses leveraging natural language processing (NLP) to boost productivity and efficiency. Since its release, CamemBERT has gained widespread adoption, receiving over 4 million downloads each month, and continues to be actively used by the NLP community. A notable example is ENEDIS, which used CamemBERT to automate the dispatch of 100,000 customer requests per day across 1,500 operators, significantly reducing manual workload and achieving a return on investment of approximately €3M per year Gemignani et al. (2023); Akani et al. (2023). However, a significant challenge that models like CamemBERT, developed in 2019, face is temporal concept drift Loureiro et al. (2022); Agarwal and Nenkova (2022); Jin et al. (2022) This phenomenon occurs when the data a model was originally trained on becomes outdated, leading to a decline in performance as new topics, events, and terminology emerge. For instance, when CamemBERT was trained, discussions around COVID-19, public health restrictions, and associated changes in language usage were not present in the training data. As a result, models like CamemBERT, which have not been updated, struggle to understand or generate accurate responses to these newer concepts. Temporal drift impacts their ability to remain relevant in evolving real-world applications, highlighting the need for continuous updates to keep the models aligned with current linguistic and contextual trends. Given these challenges, it is essential to develop and deploy updated encoder models that can better serve modern NLP applications. In response, we aim to provide state-of-the-art models and fine-tuned versions for a range of common NLP tasks, including Named Entity Recognition (NER), Question Answering (QA), Natural Language Inference (NLI), and Part-of-Speech (POS) tagging. These updated models will play a major role in creating fast, efficient, and reliable AI systems. In this paper, we present two new versions of the CamemBERT base model: CamemBERTav2 and CamemBERTv2. CamemBERTav2 is built on the DeBERTaV3 He et al. (2021a) architecture as an update to the CamemBERTa model Antoun et al. (2023), using the Replaced Token Detection (RTD) training objective for enhanced context and positional representation, while CamemBERTv2 is based on the RoBERTa Liu (2019) architecture, trained using the Masked Language Modeling (MLM) objective. Both models benefit from training on a much larger and more recent dataset, coupled with an updated tokenizer designed to better capture the nuances of the French language, and to support modern token requirements by adding new lines, tabulation and emojies to the vocabulary. This ensures improved tokenization performance across various NLP tasks. To evaluate the performance of these new models, we conducted extensive tests on both general-domain NLP tasks and domain-specific applications, such as those in the medical field. This dual approach demonstrates the versatility of our models, highlighting their capacity to excel in diverse use cases, including highly specialized areas where domain-specific knowledge is necessary. The contributions of this paper are as follows: • We present CamemBERTav2 and CamemBERTv2 and trained on a larger, up-to-date dataset with an enhanced tokenizer to better capture the complexities of the French language. • We evaluate the models on both general-domain NLP tasks and domain-specific applications, particularly in the medical field, to demonstrate their robustness and versatility. • We are releasing all model artifacts, including intermediate checkpoints and fine-tuned models, enabling the community to further experiment, fine-tune, and deploy these models across various applications.111https://huggingface.co/almanach?search_models=camembert+v2 These contributions aim to advance French language modeling and provide the community with state-of-the-art tools for diverse NLP tasks."
https://arxiv.org/html/2411.08785v1,Zero-shot Cross-lingual Transfer Learning with Multiple Source and Target Languages for Information Extraction: Language Selection and Adversarial Training,"The majority of previous researches addressing multi-lingual IE are limited to zero-shot cross-lingual single-transfer (one-to-one) setting, with high-resource languages predominantly as source training data. As a result, these works provide little understanding and benefit for the realistic goal of developing a multi-lingual IE system that can generalize to as many languages as possible. Our study aims to fill this gap by providing a detailed analysis on Cross-Lingual Multi-Transferability (many-to-many transfer learning), for the recent IE corpora that cover a diverse set of languages. Specifically, we first determine the correlation between single-transfer performance and a wide range of linguistic-based distances. From the obtained insights, a combined language distance metric can be developed that is not only highly correlated but also robust across different tasks and model scales. Next, we investigate the more general zero-shot multi-lingual transfer settings where multiple languages are involved in the training and evaluation processes. Language clustering based on the newly defined distance can provide directions for achieving the optimal cost-performance trade-off in data (languages) selection problem. Finally, a relational-transfer setting is proposed to further incorporate multi-lingual unlabeled data based on adversarial training using the relation induced from the above linguistic distance. Experimental results on two practical multi-lingual IE tasks demonstrate our method significantly outperforms baselines across tasks and languages simultaneously. Additionally, by carefully designing the multi-lingual training to utilize data from relevant languages, we can achieve a substantial boost in generalization ability with reasonable labor cost for the additional data collection.","The objective of Information extraction (IE) is to identify and extract structure information, such as entities, relations, and events, from natural unstructured text. IE plays an important role in various downstream applications, including Question Answering, Knowledge Graph Construction, News Analysis, etc. Solving IE tasks pose significant challenges for NLP models as they often require the understanding of complex features of natural languages. For example, to extract relations within a sentence, models first need to learn specialized structures of the corresponding language to identify entities mentioned in the given text. Next, a deep understanding of context is required to correctly classify the relations between these entities. These challenges are further exacerbated in multilingual settings, where datasets are collected from multiple languages, each of which contains language-specific characteristics and structures. The rapid development around English-based datasets has pushed machine performance to be on par with human ability in English tasks, prompting recent works to explore NLP research in other languages Liang et al. (2020); Ruder et al. (2021). However, despite advanced large-scale architectures and high English results, current models notably under-perform in new languages, especially those that are considered low-resource and lack high-quality datasets for fine-tuning. Cross-lingual Transfer, as a result, becomes one of the most important directions in the field. Given a particular task, the goal of Cross-lingual Transfer is to train multilingual models over high-resource source languages that can solve textual tasks in new target languages despite the shifts in linguistic origin. Currently, the most popular and practical approach for IE involves Zero-Shot Cross-Lingual (ZSCL) transfer Conneau et al. (2020); Goyal et al. (2021). These methods fine-tune Transformer-based multilingual Language Models (mLMs), which were pre-trained using unlabeled text from hundreds of languages, for downstream tasks using high-resource source-language labeled datasets (predominantly English). The resulting models are directly used for evaluation on the corresponding tasks in target languages. Studies have shown, however, performance of these multi-lingual models varies substantially across languages and tasks. Several factors have been attributed to this phenomenon, ranging from data-dependent statistic(e.g. dataset size, word overlap) Malkin et al. (2022), to data-independent features (e.g. phylogenetic and typological features) Lin et al. (2019); Dolicki and Spanakis (2021). Based on these previous observations, we believe that there is a deeper connection between cross-lingual transfer ability and the relations in the linguistic landscape. Unraveling this correlation can provide tremendous practical implications for IE. First, it serves as a guide for data collection process to achieve optimal cost-performance trade-off by gathering training samples from appropriate source languages for a target language. Furthermore, the modeling process can also be tailored such that the learned representations explicitly capture the linguistic relations to improve generalization across languages. Previous papers following the above direction define the problem as a Performance Prediction task. In Lin et al. (2019); Dolicki and Spanakis (2021); Srinivasan et al. (2021), a regression model is trained to take linguistic features of the source-target language pairs as input to predict a trained model performance scores on target languages. Despite high prediction accuracy, these works are insufficient for the following two reasons. First, they place too much emphasis on the accuracy of the regression model, which is trained for a specific architecture on a particular task. As the training configuration varies widely in practice, the results obtained from the performance prediction models may become unreliable and not applicable in general. Another reason is that previous work is only limited to the setting of single-transfer between two languages, in which only one source language (predominately English) is utilized. Current advances in translation model and data gathering process have enabled the creation of datasets in many languages, thus multiple source languages should be considered. Intuitively, additional training data from more languages can help improve model’s generalization on downstream tasks, and learning from text in multiple languages may have a positive effect on zero-shot transfer. We believe that multi-transfer setting is the next important step for cross-lingual transfer, both to improve model performance across languages and to provide a more complete picture of multi-linguality in machine learning. In this paper, we focus on what has been missing in previous works by aiming to answer the following three major research questions: Q1: How do the relations in the linguistic landscape affect an IE model’s cross-lingual transfer ability? We use URIEL Typological Database Littell et al. (2017) to extract phylogenetic and typological properties of each language. These properties, represented as multi-dimensional binary features, are used to compute the pair-wise linguistic distances (or equivalently the similarity scores) between languages. We compare the correlation between these scores and model single-transfer performance. A source language with a high correlation value would imply that we can infer its ability to transfer to different languages using only linguistic relations, without the need to actually fine-tune models. Q2: Can we implicitly leverage these linguistic features as dataset-independent knowledge to efficiently address the more general multi-transfer setting? While many-to-many cross-lingual transfer has the potential to significantly improve single-transfer performance, it would also require gathering data from multiple languages. Given a set of languages and their linguistic features as the only prior information, we aim to find the optimal subset of source languages to gather labeled data for zero-shot cross-lingual multi-transfer to target languages. The goal is to achieve the best cost-performance trade-off on all languages, without having to fine-tune the mLMs on an exponential number of possible language combinations. Q3: Can we explicitly integrate these linguistic relations in the learning process to effectively improve multi-transfer performance? We then investigate further into the possibility of directly embedding the linguistic relations in the fine-tuning process. The hypothesis is that, by capturing these connections, the multi-lingual representations would be able to adaptatively generalize to not only languages that are closely related to source languages, but also distant languages that share little similarity with the available training data. The following observations are obtained from our quantitative experiments and qualitative analysis, through 3 levels of transfer settings: 1) Single-transfer (ZSCL-S) - Only 1 labeled source language available. It is possible to achieve a high degree of correlation between model ZSCL performances and linguistic relations, using a combination of syntax, inventory, and phonology features from URIEL. However, in contrast to prior works which only focus on syntactic transfer when fine-tuning, our combined metric places the least importance on the syntax feature. This implies that previous researches are suboptimal and incomplete, prompting further investigations into the problem. 2) Multi-transfer (ZSCL-M) - Multiple labeled source language available. We first cluster languages based on the combined metric above. Then, by selecting source languages following the guidance from the resulting clusters, we observe significant improvements in ZSML performances over the naive method of randomly picking source languages. In other words, with only the prior linguistic knowledge, we can efficiently choose a suitable small subset of languages for labeled data annotations, to fine-tune a MMLM to perform best on a given set of languages. 3) Relational-transfer (ZSCL-R) - ZSCL-M with additional multi-lingual unlabeled data. We leverage unlabeled data from all available languages and their linguistic relations as inputs to graph-relational adversarial learning framework Xu et al. (2022b), a generalization of adversarial language adaption Chen et al. (2018) that can only perform strict uniform alignment for pair-wise transfer. By conditioning the multi-lingual representation flexibly on the connections expressed by the corresponding language relational-graph, we achieve a considerable increase in transfer performances across every language. This is only at the small cost of collecting additional unlabeled data from other languages. SMILER % MINION % ita 19.71 eng 39.76 fra 16.25 pol 13.7 deu 13.75 tur 13.7 por 11.54 nld 10.38 eng 9.57 spa 9.99 kor 5 por 4.59 pol 4.5 swe 4.59 spa 2.95 ara 2.49 rus 1.71 hin 4.58 swe 1.2 kor 4.58 fas 0.7 jpn 4.5 ukr 0.26 Table 1: Percentage distributions of training data in each task for every language, which are separated into high, medium, and low resource categories. The shared languages are color-coded, with red indicating that the language belongs to a different category between the two tasks, whereas green indicates otherwise. This study involves a total of 17 languages including: arabic (ara), german (deu), english (eng), farsia (fas), french (fra), hindi (hin), italian (ita), japanese (jpn), korean (kor), dutch (nld), polish (pol), portuguese (por), russian (rus), spanish (spa), swedish (swe), turkish (tur), and ukrainian (ukr). Figure 1: The pairwise Pearson correlation for all computed language distances."
https://arxiv.org/html/2411.08752v1,Multi-Perspective Stance Detection,"Subjective NLP tasks usually rely on human annotations provided by multiple annotators, whose judgments may vary due to their diverse backgrounds and life experiences. Traditional methods often aggregate multiple annotations into a single ground truth, disregarding the diversity in perspectives that arises from annotator disagreement. In this preliminary study, we examine the effect of including multiple annotations on model accuracy in classification. Our methodology investigates the performance of perspective- aware classification models in stance detection task and further inspects if annotator disagreement affects the model confidence. The results show that multi-perspective approach yields better classification performance outperforming the baseline which uses the single label. This entails that designing more inclusive perspective-aware AI models is not only an essential first step in implementing responsible and ethical AI, but it can also achieve superior results than using the traditional approaches.","Large language models (LLMs) have revolutionized natural language processing (NLP) field by outperforming state-of-the-art approaches in NLP tasks. In the context of supervised learning settings, which require labeled data, it is well-known that human annotators may provide diverse perspectives via annotations due to the intricacies of thought, varied life experiences, and diverse educational backgrounds [1, 2]. This inevitably leads to disagreement especially in the subjective tasks such as toxicity, argumentation mining, stance and hate speech detection in which multiple perspectives may be equally valid, and a unique ‘ground truth’ label may not exist. To validate this assumption, we intend to revisit the task of stance detection through involving multiple annotations, i.e. diverse perspectives of multiple annotators, instead of a single ground truth. In this preliminary study, our contributions are two-fold: (i) we investigate if establishing a more inclusive ethically-aware LLM can provide better classification performance, and (ii) further explore if the model confidence is affected by annotator disagreement. The main concern related to the traditional aggregation procedures, which reduce multiple annotations into a single label using standard approaches of disagreement resolution through majority voting, is oversimplifying the real-world complications by assuming the presence of a single ground truth [3, 4, 5]. Due to these concerns, a novel approach called perspectivism has recently been proposed for gathering annotated data in NLP [6]. This new paradigm promotes the use of disaggregated datasets, containing different human judgments, especially for subjective tasks. This approach holds the potential of fostering the development of responsible and ethical AI systems in which fairness is promoted by incorporating the perspectives of diverse annotators. These perspectives arise not only from different backgrounds but also from different ideas, ensuring a more comprehensive and equitable representation. Figure 1: Baseline vs. Multi-perspective Approach in Model Finetuning. Baseline relies on aggregated label via majority voting (i.e. Majority label), while Multi-perspective uses each annotator’s individual label (i.e. Annotator label)."
https://arxiv.org/html/2411.08745v1,Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers,"A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing latent representations (latents) during a word translation task in transformer-based LLMs. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean over latents across different languages does not impair and instead improves the models’ performance in translating the concept. Our results provide evidence for the existence of language-agnostic concept representations within the investigated models.111This work has been previously published under the title ”How Do Llamas Process Multilingual Text? A Latent Exploration through Activation Patching” at the ICML 2024 mechanistic interpretability workshop https://openreview.net/forum?id=0ku2hIm4BS.Code for reproducing our experiments is available at https://github.com/Butanium/llm-lang-agnostic.","The emergence of the field of mechanistic interpretability has led to the conception of powerful tools Carter et al. (2019); Nostalgebraist (2020); Schubert et al. (2020); Belrose et al. (2023); Cunningham et al. (2023); Kramár et al. (2024); Marks et al. (2024); O’Neill and Bui (2024); Tufanov et al. (2024) for the investigation of the inner workings of deep neural networks such as large language models (LLMs) Vaswani et al. (2017); Radford et al. (2019); Touvron et al. (2023) with the ultimate goal of reverse engineering the algorithms encoded in their weights. As a result, researchers today are often able to open up a “black box” neural network, and with near surgical precision pinpoint where a certain input-output behaviour comes from Wang et al. (2022); Conmy et al. (2023); Nanda et al. (2023); Zhong et al. (2024); Furuta et al. (2024). This provides a unique opportunity to examine how multilingual concepts are represented and processed within LLMs, potentially revealing insights into language biases and concept formation. In particular, approaches based on activation patching Variengien and Winsor (2023); Ghandeharioun et al. (2024); Chen et al. (2024) in which activations are patched from one forward pass into another one while observing the output (c.f. Fig. 2) present a simple, yet effective way to inspect the representations learned and computations performed by LLMs. Summary of contributions. In this work, we leverage activation patching to understand how LLMs process multilingual text, with a particular focus on whether they use a language-agnostic concept space, as theorized by Wendler et al. (2024). We focus on autoregressive, decoder-only transformers as they are the most common type of LLMs. In order to do so, we design multiple patching experiments leveraging pairs of translation prompts with differing expected predicted concept and language. 1. First, we perform an activation patching analysis of Llama 2 7B Touvron et al. (2023), patching at the last token position. We demonstrate that the model processes translation tasks by first resolving output language, then the concept to be translated. 2. We propose two competing hypotheses about how transformers solve the translation task during their forward pass: H1 where language and concepts are represented independently, and H2 where they are inherently entangled. We argue that if language and concepts are independent (H1), averaging the latent representation of a concept across languages should still allow the model to make sense of and utilize this representation. Conversely, if language and concepts are entangled (H2), this mean representation would be an incoherent mixture of language-specific concepts that the model cannot effectively use. 3. To test these hypotheses, we use a novel activation patching setup depicted in Figure 1 which forces Llama 2 7B to translate this mean representation across languages. We find that using the mean concept representation across languages improves Llama 2 7B’s performance on a word translation task, supporting H1. 4. Finally, in App. C we show that our observations generalize to a diverse set of transformer models varying in size, architecture, and training data, including Llama 2 70B, Llama 3 8B Dubey et al. (2024), Mistral 7B Jiang et al. (2023), Qwen 1.5 7B Bai et al. (2023) and Aya 23 8B Aryabumi et al. (2024). Our results align with the theory for Llama 2 outlined by Wendler et al. (2024) and with previous results on BERT models Devlin et al. (2018) by Wu et al. (2019); Pires et al. (2019). However, our analysis goes beyond the observational approach using the logit lens by Wendler et al. (2024) and cross-lingual embedding similarity analysis by Wu et al. (2019) and Pires et al. (2019). Using activation patching, we perform causal interventions that allow us to draw stronger conclusions about the computations performed and representations used within the models."
https://arxiv.org/html/2411.08733v2,"Dynamic Rewarding with Prompt Optimization Enables 
Tuning-free Self-Alignment of Language Models","Aligning Large Language Models (LLMs) traditionally relies on costly training and human preference annotations. Self-alignment seeks to reduce these expenses by enabling models to align themselves. To further lower costs and achieve alignment without any expensive tuning or annotations, we introduce a new tuning-free approach for self-alignment, Dynamic Rewarding with Prompt Optimization (DRPO). Our approach leverages a search-based optimization framework that allows LLMs to iteratively self-improve and craft the optimal alignment instructions, all without additional training or human intervention. The core of DRPO is a dynamic rewarding mechanism, which identifies and rectifies model-specific alignment weaknesses, allowing LLMs to adapt efficiently to diverse alignment challenges. Empirical evaluations on eight recent LLMs, both open- and closed-sourced, demonstrate that DRPO significantly enhances alignment performance, with base models outperforming their SFT/RLHF-tuned counterparts. Moreover, the prompts automatically optimized by DRPO surpass those curated by human experts, further validating the effectiveness of our approach. Our findings highlight the great potential of current LLMs to achieve adaptive self-alignment through inference-time optimization, complementing tuning-based alignment methods.111Code available: https://github.com/Singla17/DRPO","Aligning Large Language Models (LLMs, Brown et al. 2020; Chowdhery et al. 2023; Touvron et al. 2023a; Achiam et al. 2023) with human ethical standards and practical expectations is extremely crucial to prevent unintended consequences and ensure AI’s positive contribution to society. Traditional alignment methods, such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) Bai et al. (2022b); Ouyang et al. (2022), are resource-intensive and require extensive human oversight, limiting their scalability and practicality. As LLMs grow more complex and widely adopted, the demand for cost-effective, annotation-efficient, and rapidly adaptable alignment strategies becomes increasingly urgent. Figure 1: Comparison of DRPO with other LLM alignment paradigms. DRPO combines the benefits of self-alignment and tuning-free alignment, enabling self-improvement and high cost-efficiency without requiring human supervision or additional model training. Self-alignment aims to improve LLM alignment by leveraging the models themselves; for example, by replacing human feedback with model-generated feedback Lee et al. (2023), synthesizing preference data Kim et al. (2023); Sun et al. (2024), or self-critique Bai et al. (2022b). Despite these advancements, such methods still demand significant resources, including the costly and unstable RLHF tuning, as well as some level of human supervision, such as carefully curated alignment rules or in-context learning (ICL) prompts Sun et al. (2024). On the other hand, as shown in Figure 1, a recent line of research focuses on tuning-free alignment, which prioritizes extreme efficiency without incurring any tuning cost. These approaches include techniques like decoding-based alignment Li et al. (2023c); Wang et al. (2024b) or ICL alignment Han (2023); Lin et al. (2024a); Zhao et al. (2024). However, these tuning-free methods are often static (e.g., relying on fixed prompts or reward functions) and thus lack the flexibility to adapt and self-improve for better alignment. To marry the strengths of both paradigms, in this paper, we propose DRPO, Dynamic Rewarding with Prompt Optimization, a novel tuning-free approach for LLM self-alignment. DRPO draws inspiration from two key insights from recent alignment research. First, the superficial alignment hypothesis Zhou et al. (2024) suggests that LLMs can be effectively aligned through lightweight tuning or even simple prompting Lin et al. (2024a); Zhao et al. (2024). Second, reward models in RLHF often generalize poorly to out-of-distribution samples Burns et al. (2023), whereas LLMs, known for their superior generalization capabilities, can provide more effective rewards and feedback for alignment purposes. Building on these insights, DRPO is constructed atop a search-based prompt optimization (PO) framework Pryzant et al. (2023); Hao et al. (2023); Wang et al. (2023), which enables LLMs to self-correct and automatically craft detailed alignment instructions. This steers model behavior more effectively, without relying on any use of human preferences or model training. The core novelty of DRPO lies in its dynamic rewarding mechanism, integrated with the optimization framework. This mechanism allows LLM-based rewards to be dynamically adjusted based on specific queries, helping to identify and address the model’s alignment blind spots. For example, if an LLM with outdated knowledge pretends to answer a question requiring the latest news, its “knowledge limitation” reward will be low, and the alignment prompt will be updated accordingly. We apply this novel method to automatically craft both the system prompt and responses in ICL examples, which have proven highly effective in improving alignment. We conducted comprehensive experiments on 8 recent LLMs using the standard alignment benchmark, just-eval-instruct, composed of questions from multiple alignment datasets. Our results show that DRPO can effectively align both base and SFT/RLHF tuned models. Notably, DRPO significantly enhances base models, enabling them to outperform their SFT/RLHF-tuned counterparts. DRPO can further improve SFT/RLHF-tuned models, highlighting its compatibility with other tuning-based alignment techniques. Additionally, our automatically optimized prompts substantially outperform those curated by human experts. Figure 2: Comparison of DRPO with other alignment methods, including RLHF and URIAL Lin et al. (2024a). DRPO consistently outperforms both baselines across multiple LLMs. Note that we do not have access to gpt-3.5-turbo base model; hence, both DRPO and URIAL are directly applied to its RLHF-tuned version."
https://arxiv.org/html/2411.08726v1,Analyst Reports and Stock Performance: Evidence from the Chinese Market,"This article applies natural language processing (NLP) to extract and quantify textual information to predict stock performance. Using an extensive dataset of Chinese analyst reports and employing a customized BERT deep learning model for Chinese text, this study categorizes the sentiment of the reports as positive, neutral, or negative. The findings underscore the predictive capacity of this sentiment indicator for stock volatility, excess returns, and trading volume. Specifically, analyst reports with strong positive sentiment will increase excess return and intraday volatility, and vice versa, reports with strong negative sentiment also increase volatility and trading volume, but decrease future excess return. The magnitude of this effect is greater for positive sentiment reports than for negative sentiment reports. This article contributes to the empirical literature on sentiment analysis and the response of the stock market to news in the Chinese stock market.Keywords: Chinese Stock Market; Sentiment Analysis; Natural Language Processing; BERTJEL: C45, G14, G15","In the era of information, text plays a crucial role in shaping financial market behavior, as it serves as a medium of communication, conveying thoughts, emotions, and experiences. Investors and analysts rely on various text sources, including news, social media, and professional reports, to gather information and make informed financial decisions. As a result, the impact of textual information on the financial sector has gained significant attention, leading to the emergence of a cross-disciplinary research field that combines Natural Language Processing (NLP) with financial analysis for the evaluation of securities. The analysis of text reports for investment decision making poses several challenges. First, these reports are typically written objectively and unemotionally, making it difficult to extract sentiment and provide clear investment advice. Overcoming this challenge requires finding effective methods to accurately identify the emotions conveyed in reports. Second, research reports could have a bias toward presenting a positive outlook for companies, particularly on the Chinese market due to incentives and market conditions (see Wu et al. (2018)). Therefore, distinguishing a real positive sentiment or a rather negative sentiment from a large number of reports becomes difficult, as it necessitates information on the analyst to control for her incentives. Lastly, evaluating the validity of information in research reports is another challenge, considering that data can be sensitive to time and quickly become outdated. To make informed investment decisions, investors need ways to assess the accuracy and relevance of the data in the report articles. In light of these challenges, this study aims to quantitatively extract sentiment from investment recommendation reports on the Chinese stock market and examine the impact of these sentiments on stock performance. Specifically, we investigate whether the sentiments expressed in the reports influence future excess returns, volatility, and trading volume. To achieve this objective, we employ the BERT (Bidirectional Encoder Representations from Transformers ) language model to analyze sentiment in a dataset comprising 62735 Chinese financial analyst reports. Then we establish a relationship between the sentiment expressed in the reports and the behavior of the stocks with regression analysis. Our findings indicate several notable insights. First, we observe a positive relationship between the sentiment scores of the research reports and the excess returns of the stocks the next day. Additionally, strong sentiment, both negative and positive, leads to increased intraday volatility of stocks the following day. Furthermore, we find that strong positive sentiment is associated with higher trading volume on the next day, whereas the same effect is not necessarily observed for negative sentiment. Our work uses a new dataset of collected analysts’ reports that, to the best of our knowledge, have never been collected and used before in scholarly research. Similarly as in other research work, it uses a deep learning model to extract sentiment from text. Our contribution consists in providing empirical evidence of sentiment effects arising from analyst reports and impacting stock market performance in China. Although the impact of implied sentiment measures derived from different methodologies has been applied to study its effect on the Chinese financial market such as in Wu et al. (2018) or in Liang et al. (2020), this work is the first, to our knowledge, that uses a language model to train and predict sentiment in a text corpus and investigate its effect on the performance of the Chinese financial market. The impact of direct investment recommendation by Chinese analyst has been studied Jiang et al. (2014) finding significant investment performance effects of these recommendations. Our work uses a more extensive dataset and provides additional empirical evidence of information content of analysts’ reports. Our findings shed light on the importance of sentiment analysis in the financial domain and provide valuable implications for investment decision-making. The remaining structure of the article is as follows. Section 2 reviews the research literature on sentiment analysis and NLP application in finance. Section 3 introduces our methodology for the analysis of sentiment and regression models. Section 4 contains the results of the analysis. Finally, Section 5 concludes our work."
https://arxiv.org/html/2411.08724v1,QCG-Rerank: Chunks Graph Rerank with Query Expansion in Retrieval-Augmented LLMs for Tourism Domain,"Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in Large Language Models (LLMs) by integrating information retrieval techniques. However, in the tourism domain, since the query is usually brief and the content in the database is diverse, existing RAG may contain a significant amount of irrelevant or contradictory information contents after retrieval. To address this challenge, we propose the QCG-Rerank model. This model first performs an initial retrieval to obtain candidate chunks and then enhances semantics by extracting critical information to expand the original query. Next, we utilize the expanded query and candidate chunks to calculate similarity scores as the initial transition probability and construct the chunks graph. Subsequently, We iteratively compute the transition probabilities based on an initial estimate until convergence. The chunks with the highest score are selected and input into the LLMs to generate responses. We evaluate the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets. The experimental results demonstrate the effectiveness and superiority of the QCG-Rerank method. 11footnotetext: These authors contributed equally to this work. 22footnotetext: Corresponding author","Recently, LLMs, such as Llama[1], ChatGPT[2], and ChatGLM[3], have strong generative capabilities and are widely applied in question answering[4] and code generation[5][6]. LLMs are trained on large-scale corpora to learn deep semantic information, enabling them to generate corresponding responses based on queries. However, the generated responses sometimes contain inconsistencies with facts, leading to the “hallucinations” phenomenon[7]. In the tourism domain, LLMs often produce outdated addresses and route information, making the hallucination phenomenon particularly obvious[8]. To mitigate the hallucination issues of LLMs in the tourism domain, we use RAG to retrieve query-related contents in tourism and input them into LLMs. Subsequently, we use the matched contents to limit the scope of LLMs’ responses, ensuring that the generated output is more consistent with the user’s query[9]. Due to its outstanding performance, RAG has been widely applied in law[10][11] and medicine[12]. Fig. 1 illustrates the detailed processes LLMs with RAG and without RAG. Given the concise nature of queries in the travel domain and the inconsistent quality and length of candidate contents, similarity-based retrieval methods with travel documents often result in retrieving chunks irrelevant to or contradictory to the user’s query. Consequently, this misalignment can cause LLMs to summarize incorrect information, adversely affecting the model’s overall performance. Figure 1: The process of without RAG and with RAG. To address this issue, Wang et al.[13] utilized LLMs to generate pseudo-documents for the original query and concatenated them to expand the semantic complexity of the query. The enhanced query is then used for retrieval to improve the matching accuracy of relevant documents. Given that the documents retrieved in the initial stage based on similarity may have low semantic relevance or contradict the original query, Xiao et al.[14] proposed a rerank method based on Bge-embedding. This method aims to refine the initial retrieval by adding an additional rerank step that leverages semantic understanding to identify the relevant contents. The reranked contents are then summarized using the capabilities of LLMs to provide more accurate responses. Youdao[15] has developed bilingual and cross-lingual embedding (BCEmbedding) in English and Chinese. It includes two main components: the EmbeddingModel and the RerankerModel. The EmbeddingModel is designed to produce semantic vectors essential for enhancing semantic searches and facilitating question-answering processes. Meanwhile, the RerankerModel is adept at improving the quality of search outcomes and performing ranking tasks with precision. To mitigate the hallucination problem in RAG in the travel domain, it is crucial to mine information deeply from brief queries and retrieve relevant contents accurately. In this paper, we propose Chunks Graph Rerank with Query Expansion(QCG-Rerank). We first perform an initial retrieval using the query to obtain candidate contents. Next, we extract critical information from the query and duplicate it to expand its semantic complexity to improve matching reliability. Given that the initial retrieval results may contain overlapping contents with minimal differences in similarity, our reranking stage involves calculating the semantic similarity between the updated query and the candidate chunks. These similarity scores serve as initial transition probabilities to construct a chunks graph. After several iterations, the transition probabilities stabilize, and we choose the top-ranked chunks to input into LLMs, which summarize the relevant contents and generate the final output. This approach ensures that the chunks input into the large model are highly relevant to the query, thereby enhancing the reliability of the responses generated by the LLMs in the travel domain. We evaluate the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets. The experimental results demonstrate the effectiveness and superiority of the QCG-Rerank method. In summary, the contributions of our paper are as follows: 1. We extract critical information from the query and duplicate it to enrich the semantic complexity of the original query, thereby enhancing the relevance of the retrieved contents. 2. We propose a chunks graph reranking method that constructs a graph based on the similarities between the updated query and candidate chunks. Identifying the significant chunks within this graph enhances the accuracy of the reranking stage. 3. We evaluate QCG-Rerank on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets. The experimental results highlight the superior performance and accuracy of QCG-Rerank."
https://arxiv.org/html/2411.08708v1,Are  Needed for Document-Level Event Extraction?,"Most existing work on event extraction has focused on sentence-level texts and presumes the identification of a trigger-span — a word or phrase in the input that evokes the occurrence of an event of interest. Event arguments are then extracted with respect to the trigger. Indeed, triggers are treated as integral to, and trigger detection as an essential component of, event extraction. In this paper, we provide the first investigation of the role of triggers for the more difficult and much less studied task of document-level event extraction. We analyze their usefulness in multiple end-to-end and pipelined neural event extraction models for three document-level event extraction datasets, measuring performance using triggers of varying quality (human-annotated, LLM-generated, keyword-based, and random). Our research shows that trigger effectiveness varies based on the extraction task’s characteristics and data quality, with basic, automatically-generated triggers serving as a viable alternative to human-annotated ones. Furthermore, providing detailed event descriptions to the extraction model helps maintain robust performance even when trigger quality degrades. Perhaps surprisingly, we also find that the mere existence of trigger input, even random ones, is important for prompt-based LLM approaches to the task.","Figure 1: Document-level event extraction example from MUC. Two events should be extracted from the given text, an Attack (doc on left) and a Kidnapping (doc on right). Note that only one mention (among many coreferent ones) per event argument is shown. Some event arguments are empty and some might have multiple fillers. The (duplicated) document also depicts event triggers obtained from different annotation sources (labeled with pink background); more details in Section 3. There is a long history of research in Natural Language Processing (NLP) on the topic of event extraction (Grishman, 2019). The vast majority of that research focuses on sentence-level event extraction (Ji and Grishman, 2008; Li et al., 2013; Yang et al., 2019; Wadden et al., 2019, i.a.): given a text and a predefined set of event types E, each e\in E with its own set of arguments A_{e} (also known as roles), find all role-fillers of these events in the text, identifying for each (1) the trigger span t that denotes the occurrence of the event, (2) the event type e, and (3) a span of text s for each instantiated event argument a\in A_{e} (also called role fillers) mentioned within the sentence containing t. Prior work treats triggers as integral to accurate event extraction (M’hamdi et al., 2019; Tong et al., 2020; Lin et al., 2022). Specifically, trigger detection is typically the first step of event extraction as it informs event type categorization and provides a lexical anchor for argument extraction. Thus, trigger spans are marked during the data annotation process for sentence-level event extraction (Doddington et al., 2004). In recent years, however, there has been increasing interest in document-level event extraction: given a document d, a set of event types E, and their associated arguments A_{e\in E}, produce one event output template for each relevant event described in d with event arguments recognized at the entity level, i.e. via a single representative mention.111Other names for the task include template filling Jurafsky and Martin (2008), generalized template extraction (Chen et al., 2023), N-ary relation extraction Jain et al. (2020). Document-level event extraction is important because the vast majority of real-world news and narrative texts describe multiple complex interacting events. Li et al. (2021), for example, find that the arguments of close to 40% of events in their Wikipedia-based dataset appear in sentences other than the event trigger sentence. Unfortunately, the document-level task setting poses significant challenges (Das et al., 2022). Documents can contain zero events or discuss multiple events of the same or similar types. Events and event arguments are likely to be mentioned many times throughout the document, requiring coreference resolution . Arguments can be shared across events. Information about different events can be intermingled. For example, in Figure 1, “Heyndal”, and “Teneda” are arguments for both the Attack and Kidnapping events; and their multiple mentions in the text must be recognized as coreferent so that only one mention for each is included in each of the output events. Additionally, the arguments for each event are spread across multiple sentences. Note that event trigger-span identification is not required as part of the output; indeed, most datasets for the task do not include trigger-span annotations Soboroff (2023); Jain et al. (2020); Pavlick et al. (2016). On one hand, this makes sense: there is not generally a single trigger associated with each event, and the existence of intermingled event descriptions means that an event trigger may be associated with more than one event. Also, it is known that event individuation — the problem of distinguishing distinct events — is difficult even for people Gantt et al. (2023). On the other hand, event triggers, which are by definition lexical indicators of an event, should help with event type identification. In cases where they denote a (verb or noun) predicate for the event, they will serve as an anchor to localize argument extraction Ebner et al. (2020). Finally, consecutive triggers denoting different event types will be cues to segment the text into event-specific chunks. In this paper, we provide the first investigation of the usefulness of triggers for document-level event extraction. As described in Section 3, we treat Triggers as ""rationales"" that indicate the presence of an event and guide the system in finding the argument of the event. We analyze the usefulness of trigger rationales in multiple end-to-end and pipelined neural event extraction models for three document-level event extraction datasets. In particular, we measure performance when using a range of event triggers of varying quality (human-annotated, LLM-generated, keyword-based, and random) to perform event extraction. Additionally, we compare the results to in-context learning prompting baselines. We make available publicly all trigger annotations employed in our experiments222GitHub Link (not available during anonymous review) including our manually annotated trigger-spans for the classic MUC dataset. We find that the usefulness of triggers depends, in part, on characteristics of the extraction task and its data, i.e. the density of relevant events in each document and the degree to which event arguments are localized with respect to the trigger. We also find that lower-quality, automatically identified triggers can be used as an alternative to human annotations. This is good news given the expense of obtaining manual annotations. In addition, we show that performance robustness in the face of degraded trigger quality can be achieved by making descriptive event information available to the extraction model. Perhaps surprisingly, we also find that the mere availability of triggers, even random ones, is important for prompt-based LLM approaches to the task. See Section 4.3 for access to our public data and code. Figure 2: Approach Overview: We examine four state-of-the art event extraction systems (e.g. TANL, Degree, GTT, GenIE). We run these models in two architectures, when possible, — pipeline and end-to-end (E2E). We also include comparisons to GPT-4o and GPT-4o-mini prompted via in-context few-shot learning (not shown). Models differ in the task-specific information document only (TANL, GTT), document+instructions (TANL +GenIE), document+instructions+template (Degree)."
https://arxiv.org/html/2411.08610v1,Dynamic Subset Tuning: Expanding the Operational Range of Parameter-Efficient Training for Large Language Models,"We propose a novel parameter-efficient training (PET) method for large language models that adapts models to downstream tasks by optimizing a small subset of the existing model parameters. Unlike prior methods, this subset is not fixed in location but rather which parameters are modified evolves over the course of training. This dynamic parameter selection can yield good performance with many fewer parameters than extant methods. Our method enables a seamless scaling of the subset size across an arbitrary proportion of the total model size, while popular PET approaches like prompt tuning and LoRA cover only a small part of this spectrum. We match or outperform prompt tuning and LoRA in most cases on a variety of NLP tasks (MT, QA, GSM8K, SuperGLUE) for a given parameter budget across different model families and sizes.","Large language models (LLMs) such as GPT-4 (OpenAI, 2023) and PaLM (Anil et al., 2023; Chowdhery et al., 2022) have demonstrated remarkable performance on various natural language processing (NLP) tasks. A common paradigm for adapting LLMs to specific NLP tasks is fine-tuning the pre-trained model on a downstream task dataset. Effective regularization is paramount since fine-tuning is often prone to overfitting due to the large model size and the small number of examples in the fine-tuning dataset. Furthermore, storing a full model version for each task becomes prohibitive as the number of tasks grows. Parameter-efficient training (PET) (Ding et al., 2022) is a family of approaches that regularizes fine-tuning by allowing only a small number of parameters to change, thus reducing the storage overhead for each task drastically. Popular techniques such as prompt tuning (Lester et al., 2021) and LoRA (Hu et al., 2022) fall into this category. A recent line of work in PET keeps the internal structure of the seed model intact by identifying a small subset of parameters that is optimized in fine-tuning while freezing all others. Ding et al. (2022) refer to this as specification-based delta tuning. Typically, these approaches specify the free parameter set once either before or after model adaptation. The subset is chosen based on either heuristics (Ben-Zaken et al., 2022), Fisher information (Xu et al., 2021; Sung et al., 2021), or the fine-tuning training signal (Mallya et al., 2018; Guo et al., 2021; Ansell et al., 2022; Panigrahi et al., 2023). In this work, we propose a novel algorithm called dynamic subset tuning (DST) that efficiently re-selects the set of free parameters in each training step without a substantial slowdown in training. Our method is more flexible than prompt tuning and LoRA as it allows us to precisely specify the fine-tuning parameter budget as a fraction of the model size. Our method often matches or outperforms prompt tuning and LoRA with comparable parameter budget on various NLP tasks, and covers a much wider range of subset sizes. In particular, it enables the use of PET with many fewer free parameters (down to 0.00001% of the network) than most prior work, allowing DST to make use of even very small training sets."
https://arxiv.org/html/2411.08553v1,CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs,"Large language models (LLMs) have demonstrated remarkable performance in diverse tasks using zero-shot and few-shot prompting. Even though their capabilities of data synthesis have been studied well in recent years, the generated data suffers from a lack of diversity, less adherence to the prompt, and potential biases that creep into the data from the generator model. In this work, we tackle the challenge of generating datasets with high diversity, upon which a student model is trained for downstream tasks. Taking the route of decoding-time guidance-based approaches, we propose CorrSynth, which generates data that is more diverse and faithful to the input prompt using a correlated sampling strategy. Further, our method overcomes the complexity drawbacks of some other guidance-based techniques like classifier-based guidance. With extensive experiments, we show the effectiveness of our approach and substantiate our claims. In particular, we perform intrinsic evaluation to show the improvements in diversity. Our experiments show that CorrSynth improves both student metrics and intrinsic metrics upon competitive baselines across four datasets, showing the innate advantage of our method.","Pretrained language models (LLMs) (Devlin et al., 2019) have achieved strong performance on text classification with a large amount of task-specific training data. However, in real world scenarios, collecting labeled data can be challenging due to expense and need for domain expertise. Recently, several works have focused on generating texts using versatile LLMs such as GPT-4 (Achiam et al., 2023), Claude (Bai et al., 2022), Mistral (Jiang et al., 2023), Mixtal (Jiang et al., 2024) and subsequently distill a student model on the synthetically generated data (West et al., 2022). However, generated datasets suffer from a lack of diversity (Yu et al., 2023a) and regurgitate the biases of the teacher LLMs, which proliferate into the student model. Although prior works have utilized retrieval augmented generation for diverse dataset synthesis (Divekar and Durrett, 2024), here we focus on the more fundamental challenge of improving or controlling generations given a prompt and context. Figure 1: CorrSynth introduces anti-correlation between examples, compared to few-shot generation. In particular, we focus on synthetic data generation for supervised text classification tasks and take the route of decoding time guidance based approaches (Sanchez et al., 2023; O’Brien and Lewis, 2023; Li et al., 2023; Chuang et al., 2023), which aim to tackle the challenge of improving diversity and faithfulness to target class in these generated datasets. Motivated by recent works on Classifier Free Guidance (CFG) (Sanchez et al., 2023), we introduce a novel guidance based strategy, CorrSynth. In CorrSynth, generations are kept faithful to the synthesis instruction, while introducing greater diversity and similarity to human text. CorrSynth is a correlated sampling approach which generates multiple sequences in parallel with strong inter-dependence between them. The main idea is as follows: when generating an instance of a particular class and sampling the next token, we contrast its logits with logits corresponding to partially generated instances from other classes. This is a simple but crucial change compared to CFG: in CorrSynth, the contrasting logits for a class/label are obtained from generations corresponding to other labels, whereas in CFG, the contrasting logits are obtained feeding back the generation for the current label into the LLM with prompts corresponding to other labels. To synthesize a K–class classification dataset, this requires K–times fewer forward passes compared to CFG. Furthermore, we can smoothly trade-off diversity and improve class-separability by introducing contrasts between logits from the same or different classes. In summary, our contributions are: (1) we develop a general correlated sampling approach, CorrSynth, that can generate multiple correlated sequences in parallel from an LLM, by explicitly introducing contrasts between parallel generations during the sampling of each token, (2) we apply this to classification dataset synthesis, with the goal of improving diversity of synthetic generations, (3) we demonstrate how our method overcomes the limitations of CFG and controllable synthesis in regards to diversity and label-separation, (4) we benchmark our approach on tasks ranging from humor detection, sentiment analysis and topic classification in regional news headlines. Our intrinsic analysis find that CorrSynth generates datasets with higher representation of tail entities, lexical diversity and similarity to human text, and distillation accuracy of student models, compared to four state of the art baselines."
https://arxiv.org/html/2411.08534v1,Neural Topic Modeling with large language models in the loop,"Topic modeling is a fundamental task in natural language processing, allowing the discovery of latent thematic structures in text corpora. While Large Language Models (LLMs) have demonstrated promising capabilities in topic discovery, their direct application to topic modeling suffers from issues such as incomplete topic coverage, misalignment of topics, and inefficiency. To address these limitations, we propose LLM-ITL, a novel LLM-in-the-loop framework that integrates LLMs with many existing Neural Topic Models (NTMs). In LLM-ITL, global topics and document representations are learned through the NTM, while an LLM refines the topics via a confidence-weighted Optimal Transport (OT)-based alignment objective. This process enhances the interpretability and coherence of the learned topics, while maintaining the efficiency of NTMs. Extensive experiments demonstrate that LLM-ITL can help NTMs significantly improve their topic interpretability while maintaining the quality of document representation.","Topic modeling is an essential task in natural language processing that uncovers hidden thematic structures within large text collections in an unsupervised way. The ability to automatically extract topics has proven to be invaluable across a range of disciplines, such as bioinformatics (Liu et al., 2016), marketing research (Reisenbichler & Reutterer, 2019), and information retrieval (Yi & Allan, 2009). Topic models are usually based on probabilistic frameworks that generate a set of interpretable global topics, each represented as a distribution over vocabulary terms. These topics are then used to represent individual documents as mixtures of topics, providing a structured and interpretable view of the corpus. Recently, research on topic modeling has shifted from classical Bayesian methods such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to Neural Topic Models (NTMs) (Zhao et al., 2021; Churchill & Singh, 2022; Wu et al., 2024) that use deep neural networks to model document-topic distributions, enabling more expressive and flexible representations compared to their probabilistic counterparts. While Large Language Models (LLMs) (OpenAI, 2022; Touvron et al., 2023a; b) have redefined the landscape of natural language processing, topic models continue to hold their place as valuable tools for text analysis. Specifically, LLMs can provide a fine-grained understanding of a document; however, given a large collection of domain-specific documents, topic models are more suitable to obtain a clear global view of the topics in a more interpretable way with much less computational cost. Unsurprisingly, it has been a trending research direction to use LLMs to improve topic modeling (Rijcken et al., 2023; Wang et al., 2023; Pham et al., 2023; Mu et al., 2024; Doi et al., 2024; Chang et al., 2024). Despite the promising performance of these initial studies, most existing methods involve prompting LLMs to generate topics for each document in the corpus, which may lead to several limitations. As LLMs are asked to focus on a document individually, they may be unable to cover all the topics across all the documents in the corpus (Doi et al., 2024), which is critical in topic modeling. Moreover, although LLMs excel at capturing local context, they usually struggle with long documents with multiple interrelated topics, which may evolve or shift throughout the text. With their limited window of focus, LLMs may miss key topics of a document that are necessary to fully understand its content. Finally, it is computationally expensive as LLMs have to do inference for documents in the corpus; thus, existing methods usually scale poorly with large datasets. To overcome the aforementioned limitations, we propose LLM-ITL, a framework that integrates LLMs into NTMs and enhances the overall quality and interpretability of the learned topics, while maintaining computational efficiency. Specifically, to enhance the interpretability of the topics learned by an NTM, we introduce an LLM-based refinement step. The representative words for each topic, as generated by the NTM, are provided to the LLM, which suggests improved words that better capture the semantic meaning of the topic. The refinement process is guided by a novel plug-in objective based on Optimal Transport (OT), which ensures that the topics learned by the NTM align closely with the LLM’s refinements. Additionally, to mitigate potential hallucinations from the LLM (i.e., the generation of inaccurate or irrelevant suggestions), we introduce a confidence-weighted mechanism that adjusts the influence of the LLM’s suggestions based on their confidence scores. Our proposed LLM-ITL framework offers the following key contributions: • Improved balance between topic coherence and document representation quality: With the LLM’s refined topics and OT-based alignment, the topics generated are more interpretable and semantically coherent. At the same time, LLM-ITL ensures that the document-topic distributions, as learned by the NTM, remain high-quality and reflective of the document’s content. • Efficiency and scalability: Unlike most existing LLM-based approaches that rely on document-level LLM analysis, LLM-ITL uses LLMs at the word level, significantly reducing computational overhead for large datasets. • Flexibility: LLM-ITL is a modular framework that can integrate with a variety of NTMs and LLMs, offering flexibility in model selection depending on the application and computational constraints. • State-of-the-art performance: Extensive experimental results on multiple benchmark datasets show that LLM-ITL achieves state-of-the-art performance in both topic coherence and document representation quality."
https://arxiv.org/html/2411.08516v1,Tree-of-Table: Unleashing the Power of LLMs for Enhanced Large-Scale Table Understanding,"The ubiquity and value of tables as semi-structured data across various domains necessitate advanced methods for understanding their complexity and vast amounts of information. Despite the impressive capabilities of large language models (LLMs) in advancing the natural language understanding frontier, their application to large-scale tabular data presents significant challenges, specifically regarding table size and complex intricate relationships. Existing works have shown promise with small-scale tables but often flounder when tasked with the complex reasoning required by larger, interconnected tables found in real-world scenarios. To address this gap, we introduce ""Tree-of-Table"", a novel approach designed to enhance LLMs’ reasoning capabilities over large and complex tables. Our method employs Table Condensation and Decomposition to distill and reorganize relevant data into a manageable format, followed by the construction of a hierarchical Table-Tree that facilitates tree-structured reasoning. Through a meticulous Table-Tree Execution process, we systematically unravel the tree-structured reasoning chain to derive the solutions. Experiments across diverse datasets, including WikiTQ, TableFact, FeTaQA, and BIRD, demonstrate that Tree-of-Table sets a new benchmark with superior performance, showcasing remarkable efficiency and generalization capabilities in large-scale table reasoning.","Figure 1: Comparison of (a) Generic Reasoning, (b) Chain-of-Table (Wang et al., 2024), and the proposed (c) Tree-of-Table methods when confronted with large-scale relational tables. Generic Reasoning often struggles with the increased context and complexity, leading to inefficient processing and potential loss of critical information. Chain-of-Table, while more structured with linear thought chain, still faces challenges with the scale and intricacy of data. In contrast, Tree-of-Table showcases a structured and hierarchical reasoning process that adeptly handles large-scale tables, significantly enhancing comprehension and efficiency compared to previous methods, particularly in managing the complexity of expansive tabular data. Tables, as a pivotal form of semi-structured data, ubiquitously underpin numerous aspects of daily life and professional domains, ranging from open data repositories and web pages to critical applications in financial analysis, risk management, health monitoring, and business reporting (Cafarella et al., 2008). The advent of large language models (LLMs) (OpenAI, 2023; Chen, 2023; Jiang et al., 2023; Imani et al., 2023; Anil et al., 2023; Zhu et al., 2024a; Valmeekam et al., 2022; Zhu et al., 2024c) has opened new vistas for understanding and reasoning with tabular data, marking a significant stride in the realm of natural language understanding (Nahid & Rafiei, 2024; Chen et al., 2024; Sui et al., 2024b; a; Ye et al., 2023; Cheng et al., 2022; Jin & Lu, 2023). This intersection is not only instrumental in enhancing the comprehension of tables but also vital for powering a plethora of downstream tasks such as table-based fact verification (Chen et al., 2019) and question answering (Li et al., 2024). Unlike their unstructured text counterparts, tables provide a dense, structured format through the interaction of rows and columns, offering a rich source of information. However, the same structural characteristics pose unique challenges for language models, as they necessitate advanced levels of reasoning over both the textual and numerical data contained within. Given the increasing reliance on tables for data representation and the complexities involved in their interpretation, investigating the integration of LLMs for improved large-scale table understanding has emerged as an essential and compelling research avenue, drawing heightened interest from the global academic and industrial research communities. Existing methods for table understanding have shown substantial progress in comprehending small-scale tables (Cheng et al., 2022; Ye et al., 2023; Wang et al., 2024). However, these approaches often falter when applied to the more complex and larger tables frequently encountered in real-world scenarios. This gap between academia and practical applications stems from a variety of limitations inherent in current methodologies. One significant challenge is the limited contextual capacity of today’s language models. As tables increase in size, the amount of information that must be processed and understood grows exponentially due to the intricate interactions between rows and columns. This complexity makes it difficult for models to capture and reason about all the necessary information in one go, significantly impeding their understanding capabilities. When faced with complex question-answering logic that spans lengthy chains, pinpointing, extracting, and comprehending key table information becomes an immense challenge. To address these issues, two main approaches have generally been adopted, as shown in Figure 1. The first involves using only the schema information of tables and employing program-aided methods, such as generating SQL-based answers from questions (Rajkumar et al., 2022b; a; Shi et al., 2020; Pönighaus, 1995; Katsogiannis-Meimarakis & Koutrika, 2023). While this approach avoids directly inputting entire tables, the resulting SQL statements can be lengthy and prone to errors, leading to suboptimal performance. The second strategy involves decomposing tables into multiple sub-tables. Methods like Dater (Ye et al., 2023) attempt to manage larger tables by initially inputting the entire table before breaking it down, which is impractical. The Chain-of-Table (Wang et al., 2024) draws inspiration from the chain-of-thought principle (Wei et al., 2022), performing implicit sub-table extraction. Yet, even this approach is limited to understanding smaller tables. Additionally, traditional table understanding datasets like WikiTQ (Pasupat & Liang, 2015) and TableFact (Chen et al., 2019), which are relatively small, severely restrict the exploration of large-scale table understanding. Fortunately, the introduction of the BIRD (Li et al., 2024) dataset, considered the largest and most complex table understanding dataset to date, highlights the pressing need for improvements. Despite this, due to the reasons mentioned above, existing large language models still exhibit low accuracy on comprehensive, large-scale table datasets like BIRD (Li et al., 2024), signaling a clear necessity for methodological innovations in this area. Addressing these concerns, we propose ""Tree-of-Table"", a novel paradigm crafted to optimize LLMs for the task of large-scale table understanding, as shown in Figure 1. By condensing and decomposing tables, our approach distills and systematizes the critical information into a tree-structured model that resonates with the stepwise reasoning employed by humans. This tree acts as a roadmap, guiding the LLM through the complexities of the table in a logical and organized manner. It provides a structured approach where each node serves a purpose, simplifying the interaction between the LLM and the tabular data. The efficacy of our Tree-of-Table methodology is emphatically validated through rigorous testing across a selection of datasets (including WikiTQ (Pasupat & Liang, 2015), TableFact (Chen et al., 2019), FeTaQA (Nan et al., 2022), and BIRD (Li et al., 2024)) with each presenting its own unique challenges. Consistently achieving top-tier results, Tree-of-Table demonstrates not just its capacity to navigate the intricacies of table reasoning but also its potential to set a new benchmark in the field."
https://arxiv.org/html/2411.08504v2,Towards Objective and Unbiased Decision Assessments with LLM-Enhanced Hierarchical Attention Networks,"How objective and unbiased are we while making decisions? This work investigates cognitive bias identification in high-stake decision making process by human experts, questioning its effectiveness in real-world settings, such as candidates assessments for university admission. We begin with a statistical analysis assessing correlations among different decision points among in the current process, which discovers discrepancies that imply cognitive bias and inconsistency in decisions. This motivates our exploration of bias-aware AI-augmented workflow that surpass human judgment. We propose BGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding, Gated Residual Connections and Multi-Head Attention. Using it as a backbone model, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow, which simulate real-world decision-making. In our experiments, both the proposed model and the agentic workflow significantly improves on both human judgment and alternative models, validated with real-world data. Source code is available at: https://github.com/junhua/bgm-han.","Field Format Sample Entry GCEA [GCEA] School:{school}, UAS:{score}; Grades:{subject grade pairs} [GCEA]: School:HCI, UAS:90.0; Grades:H1 PROJECT WORK A, H1 GENERAL PAPER A, H2 ECONOMICS A, H2 CHEMISTRY A, … GCEO [GCEO] {subject-grade pairs} [GCEO] HIGHER CHINESE B3, ENGLISH A1, ELEMENTARY MATHEMATICS A2, … Leadership [Leadership] {activity}, Level:{role}, Year:{year}, Category:{type}, Participation:{level}. (Multiple entries) [Leadership] Mind Sports Club, Level:Captain, Year:2023, Category:Sports, Participation:Executive Committee PIQ [PIQ{n}]{essay text}, where n = 1,2,3,4,5 [PIQ1] My first visit to the university in Primary 5 for a Learning Journey left me with a deep impression… OfferType Offered / Not Offered Offered Analysis Structured text (profile analysis by LLM) This candidate demonstrates a strong academic profile, particularly in STEM, with straight As… Table 1. Data Format and Sample Entry Decision making in high-stake scenarios is often done by human experts leveraging their domain expertise and experience to optimize decision quality (Alur et al., 2024). However, subjectivity and cognitive biases, such as anchoring bias (Haag et al., 2024) and confirmation bias (Echterhoff and et al., 2024), are often difficult to detect and avoid (Kahneman and Tversky, 2023). Mitigating decision biases is critical to ensure long-term sustainable outcomes and fairness to stakeholders, especially in high-stakes environments (Ghai and Mueller, 2022). Recent studies propose artificial intelligence (AI) systems to enhance manual decision processes, such as fairness-aware AI systems guiding decision-makers toward more impartial choices (Yang and et al., 2024), Explainable AI frameworks that identify potential biases and improve judgment accuracy (Haag et al., 2024), and a human-AI collaborative to audit and mitigate social biases (Ghai and Mueller, 2022). Despite the promising recent work in augmenting human expertise and mitigating various biases, the overall advancements are underwhelming due to several reasons. Firstly, the complexity and context-specific nature of cognitive biases make it challenging for AI systems to accurately detect and mitigate (Kahneman and Tversky, 2023). Secondly, the limited interpretability of AI models can reduce trust in high-stakes settings (Haag et al., 2024). Lastly, real world scenarios often rely on sensitive or proprietary data. The lack of accessible data impose tremendous challenges in such research activities (Smith et al., 2024). In this regard, this work aims to contribute to AI-augmented, bias-aware decision making in a real world setting, such as university admissions. Current processes involve complex, semi-structured data that requires nuanced assessment across multiple dimensions. Through statistical analysis, we identified non-trivial discrepancies between human evaluations and final outcomes, suggesting inconsistencies and cognitive biases. This motivates our approach, which seeks not only to automate but also to enhance decision consistency and reduce subjective influences through structured AI interventions. We propose BGM-HAN an enhanced Hierarchical Attention Network with Byte-pair Encoded, Gated Residual Connetions and Multihead Attention, which takes an leverage hierarchical learning approach to better capture and interpret multi-level semi-structured data. Using BGM-HAN as backbone, we introduce a Shortlist-Analyze-Recommend (SAR) agentic workflow to simulate existing human decision processes. In our experiments, the proposed models outperform different categories of baseline models. While comparing to current human evaluation, our proposed workflow introduces over 9.6% improvement in F1-score and accuracy. The promising results uncovers potentials in integrating hierarchical learning with LLM-augmentation to perform automated decision making with implicit fairness and consistency in high-stakes, real-world decision-making environments. Contributions. In summary, this paper makes the following novel contributions: Firstly, we propose a statistical approach to identify cognitive bias and inconsistency in a real-world decision making process, i.e., university admission assessments. Secondly, we propose a hierarchical learning model, BGM-HAN, that effectively represents multi-level semi-structured data and experimentally outperforms baseline models of multiple categories. Lastly, we propose an agentic workflow, \mathcal{W}_{SAR}, that mimics existing human decision process. \mathcal{W}_{SAR} mitigates inconsistency and cognitive bias across different decision makers, and empirically outperforms human evaluation by over 9.6% in F1-score and accuracy for in decision recommendation."
https://arxiv.org/html/2411.08432v1,One STEP at a time: Language Agents are Stepwise Planners,"Language agents have shown promising adaptability in dynamic environments to perform complex tasks. However, despite the versatile knowledge embedded in large language models, these agents still fall short when it comes to tasks that require planning. We introduce STEP, a novel framework designed to efficiently learn from previous experiences to enhance the planning capabilities of language agents in future steps. Concretely, STEP functions through four interconnected components. First, the Planner takes on the task, breaks it down into subtasks and provides relevant insights. Then the Executor generates action candidates, while the Evaluator ensures the actions align with learned rules from previous experiences. Lastly, Memory stores experiences to inform future decisions. In the ScienceWorld Wang et al. (2022) benchmark, our results show that STEP consistently outperforms state-of-the-art models, achieving an overall score of 67.4 and successfully completing 12 out of 18 tasks. These findings highlight STEP’s potential as a framework for enhancing planning capabilities in language agents, paving the way for more sophisticated task-solving in dynamic environments. 111Project page with code: https://github.com/minhtuong201/step.git","Autonomous agents that incorporate Large Language Models (LLMs) as integral cognitive systems Sumers et al. (2023) have demonstrated significant capabilities in addressing a diverse range of interactive tasks e.g., mathematical problems Cobbe et al. (2021); Hendrycks et al. (2021), programming challenges Zhuo et al. (2024); Jimenez et al. (2024), and logical reasoning Tafjord et al. (2021); Saparov and He (2023). Nonetheless, their performance tends to diminish in dynamic scenarios, such as Web navigation Zhou et al. (2024b); Yao et al. (2023a) and Open-ended environments Wang et al. (2022); Shridhar et al. (2021), which require robust reasoning capabilities of the agents. A key contributing factor to language agents’ efficiency in long tasks is the notion of memory Sumers et al. (2023). The recent approaches Majumder et al. (2023); Zhao et al. (2023a) guided the agent to store reflections on their experience of solving a task Shinn et al. (2023) in memory, and then to retrieve these to improve future attempts Yao et al. (2023c). The use of verbal refinements rather than updating model parameters, these techniques are more flexible than conventional Reinforcement Learning (RL) methods. However, this memory module often lacks a retrieval mechanism. Additionally, complex tasks that cannot be solved in a single attempt also require effective planning and goal decomposition mechanisms (we will explain this in more detail later in this paper). In this paper, we take a close look at the memory utilization and planning capabilities of the language agent. More concretely, we propose STEP - a novel framework for Stepwise Planning which consists of a Planner, an Executor, and an Evaluator. Upon receiving a task from the environment, the Planner decomposes it into manageable subtasks and retrieves relevant information from the Memory. After receiving messages, the Executor then generates action candidates, which are subsequently evaluated by the Evaluator Madaan et al. (2023). Once an action is approved and sent back to the environment, the agent receives an observation and determines whether the subtask requires refinement. After completing an episode, the agent generates learning insights Majumder et al. (2023), which are stored in the Memory for future attempts. A key aspect is that the Planner not only breaks down tasks but also dynamically distils relevant insights from previous attempts to enhance the current task trace. This iterative process maximizes the efficiency of the memory system, ensuring continuous learning and adaptation. We evaluate STEP within ScienceWorld Wang et al. (2022) - a dynamic, text-based environment designed to simulate complex scientific tasks. Our results demonstrate that STEP consistently outperforms state-of-the-art (SOTA) models, achieving an overall score of 67.4. The model successfully completes 12 out of 18 tasks, ranking first in 11 tasks."
https://arxiv.org/html/2411.08397v1,"CLaSP: Learning Concepts for Time-Series Signals from
Natural Language Supervision","This paper proposes a foundation model called “CLaSP” that can search time series signals using natural language that describes the characteristics of the signals as queries. Previous efforts to represent time series signal data in natural language have had challenges in designing a conventional class of time series signal characteristics, formulating their quantification, and creating a dictionary of synonyms. To overcome these limitations, the proposed method introduces a neural network based on contrastive learning. This network is first trained using the datasets TRUCE and SUSHI, which consist of time series signals and their corresponding natural language descriptions. Previous studies have proposed vocabularies that data analysts use to describe signal characteristics, and SUSHI was designed to cover these terms. We believe that a neural network trained on these datasets will enable data analysts to search using natural language vocabulary. Furthermore, our method does not require a dictionary of predefined synonyms, and it leverages common sense knowledge embedded in a large-scale language model (LLM). Experimental results demonstrate that CLaSP enables natural language search of time series signal data and can accurately learn the points at which signal data changes.","In the realm of industrial machinery, it is critically important to diagnose machine conditions using time-series signals, which are garnered as sensor data. Data scientists frequently grapple with the substantial task of identifying signals with specific characteristics within their datasets to devise algorithms for diagnosing machine conditions. One of the fundamental queries a data scientist might raise is, “Are there examples of this pattern in my dataset?” [1]. For such tasks, the ability to search for time-series signals from databases using natural language queries that denote signal characteristics is invaluable. This search technique for time-series signals, which employs natural language as a query, cannot be accomplished by dozens of existing query-by-sketching (QbS) [2, 3] or hundreds of query-by-example (QbE) [4] systems for time series [5]. In this paper, we propose a novel technique for searching time-series signals using natural language as a query. This approach has been previously suggested by Imani et al. [6]. However, it introduced the challenge of meticulously designing in advance the classes of time-series signal characteristics, the formulas to quantify them, and a dictionary of synonyms. To overcome the limitations of the conventional method proposed by Imani et al. [6], we introduce a neural network that enables time-series signal-natural language search based on contrastive learning. This approach draws upon the concept of multimodal learning, leveraging a dataset comprising pairs of time-series signals and their corresponding natural language descriptions, and capitalizing on the common-sense concepts embedded in large-scale language models (LLMs). Figure 1: Overview of our approach: CLaSP trains a signal encoder and a text (annotations) encoder using contrastive learning so that the model predicts the correct combinations in a set of signal data and annotations. In the evaluation, we verify whether the model can search for signal data that match the input query by using natural language text that was not used during training (Left half of the figure). During the search, the model outputs the optimal solution based on the cosine similarity between the embedding of the input query and the embedding of each searchable data set(Right half of the figure)."
https://arxiv.org/html/2411.08384v1,"Interpretable Syntactic Representations 
Enable Hierarchical Word Vectors","The distributed representations currently used are dense and uninterpretable, leading to interpretations that themselves are relative, overcomplete, and hard to interpret. We propose a method that transforms these word vectors into reduced syntactic representations. The resulting representations are compact and interpretable allowing better visualization and comparison of the word vectors and we successively demonstrate that the drawn interpretations are in line with human judgment. The syntactic representations are then used to create hierarchical word vectors using an incremental learning approach similar to the hierarchical aspect of human learning. As these representations are drawn from pre-trained vectors, the generation process and learning approach are computationally efficient. Most importantly, we find out that syntactic representations provide a plausible interpretation of the vectors and subsequent hierarchical vectors outperform the original vectors in benchmark tests.","Distributed representation of words present words as dense vectors in a continuous vector space. These vectors, generated using unsupervised methods such as Skipgram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), have proven to be very useful in downstream NLP tasks such as parsing (Bansal et al., 2014), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013). Although semi-supervised models like BERT (Devlin et al., 2018) and ELMO (Peters et al., 2018) have demonstrated better performance in downstream tasks, the unsupervised methods are still popular as they are able to derive representations using a raw, unannotated corpora without the need of massive amounts of compute. However, the major problem with nearly all of these methods lie within the word representations themselves as their use can be termed as a black-box approach. These dense representations comprise of coordinates which by themselves have no meaningful interpretation to humans, for instance, we are unable to distinguish what a ”high” value along a certain coordinate signifies compared to a ”low” value along the same direction. Additionally, this phenomenon also restricts the ability to compare multiple words against each other with respect to individual coordinates. It would appear that the numerical values of a word’s representation are meaningful only with respect to representation of other words, under very specific conditions. So, is it possible to create a representation that is not only comprised of coordinates that are meaningful to humans, but is also made of individual coordinates that enable humans to distinguish between words? Ideally such a representation would be of reduced size and capable of capturing the meaning of the word along absolute coordinates i.e coordinates whose values are meaningful to humans and can be compared against other coordinates. A common absolute that encompasses all words in the vocabulary are the eight parts of speech, namely noun, verb, adjective, adverb, pronoun, preposition, conjunction and interjection. The parts of speech explain the role of a particular word within the given context and with a coarse grained approach, all words can be classified as at least one part of speech. This work, precisely describes the representation of words in a vector space where each coordinate corresponds to one of the eight parts of speech, derived from the pre-trained word vectors via post processing. Such representation would be able to capture the absolute syntactic meaning of a word. For instance, the syntactic representation for the words ”right” and ”I” can be visualized in figure 1. (a) Syntactic Representation for right (b) Syntactic Representation for I Figure 1: Examples of Syntactic Representations These Syntactic Representations are continuous and highly reduced transformations of the original representations. These representations isolate the syntactic regularities existing within the large original word vectors and when used together in a incremental learning approach as Hierarchical Vectors, outperforms the original vectors in benchmark tests. Hierarchical Vectors utilize the basics of human learning by emulating the hierarchical aspect of learning. As human learning begins with a simple introduction, branches out to tougher topics and further extends to more complex studies in a hierarchical pattern, the incremental approach used in our work intends to emulate such pattern in word vectors. For this, we have generated unigram Syntactic Representations which act as the foundation of learning and further extend to pre-trained vectors as composite vectors. For any downstream evaluation, the model was trained using a composite representation built using the syntactic representation followed by the original vector. Our work introduces a novel approach focusing on isolating syntactic regularities within word vectors, deliberately avoiding state-of-the-art contextual information. Consequently, a notable limitation emerges as the model struggles with handling polysemy in words. However, this limitation aligns with the primary aim of this research, laying the groundwork for a fundamental shift in model interpretability and black-box nature of language models. Our methodology, instead of challenging the state-of-the-art directly, aims to use isolated regularities instead of automatically learned hidden representations, thereby questioning the status-quo. This approach aims to bridge the gap between performance and opacity of language models. In the subsequent sections, we elaborate on our methodology and present experimental results illustrating the effectiveness of our approach."
https://arxiv.org/html/2411.08348v1,Refining Translations with LLMs: A Constraint-Aware Iterative Prompting Approach,"Large language models (LLMs) have demonstrated remarkable proficiency in machine translation (MT), even without specific training on the languages in question. However, translating rare words in low-resource or domain-specific contexts remains challenging for LLMs. To address this issue, we propose a multi-step prompt chain that enhances translation faithfulness by prioritizing key terms crucial for semantic accuracy. Our method first identifies these keywords and retrieves their translations from a bilingual dictionary, integrating them into the LLM’s context using Retrieval-Augmented Generation (RAG). We further mitigate potential output hallucinations caused by long prompts through an iterative self-checking mechanism, where the LLM refines its translations based on lexical and semantic constraints. Experiments using Llama and Qwen as base models on the FLORES-200 and WMT datasets demonstrate significant improvements over baselines, highlighting the effectiveness of our approach in enhancing translation faithfulness and robustness, particularly in low-resource scenarios.","The rapid advancement of large language models (LLMs) has profoundly impacted the field of machine translation (MT). Recent studies have demonstrated the remarkable capacity of LLMs to perform zero-shot and few-shot translation tasks, achieving impressive results even without explicit training on parallel corpora Zhu et al. (2024a, b). This inherent multilingual capability, arising from vast pre-training on diverse textual data, presents new opportunities for low-resource MT, a domain traditionally hindered by data scarcity Robinson et al. (2023). However, despite their promising potential, LLMs still struggle to consistently achieve the faithfulness and precision required for high-quality translation, especially when dealing with rare or specialized terminology Cui et al. (2024). LLMs are typically pre-trained on data that exhibits a significant skew towards high-resource languages, leading to an under-representation of linguistic nuances and vocabulary associated with low-resource languages. This bias results in a performance gap when translating between or into low-resource languages Ghazvininejad et al. (2023). Rare words often exhibit a higher degree of polysemy, making it challenging for LLMs to accurately discern the correct translation based on context. This ambiguity can lead to semantically inaccurate or misleading translations Robinson et al. (2023). Several studies have explored the integration of external knowledge into the LLM-based MT pipeline. One prominent approach involves incorporating bilingual dictionaries or lexical constraints into the prompting process Ghazvininejad et al. (2023); Lu et al. (2023). While these efforts have shown promise in improving translation accuracy, they often rely on simply appending pre-translated terms to the prompt, neglecting a more nuanced analysis of the source sentence. Furthermore, existing methods often overlook the potential for leveraging LLMs themselves as agents for both translation and refinement. To address this issue, we propose a multi-step prompt chain that aims to enhance the faithfulness and robustness of LLM-based MT. In details, we first identifies and extracts keywords in the source sentence critical to translation quality. Then, we apply retrieval-augmented generation (RAG) Lewis et al. (2020a) to embed translations of these keywords, retrieved from a bilingual dictionary, into the LLM’s context. Finally, we refine translations through iterative revisions by employing LLMs as constraint-aware translators and revisers. Through comprehensive experiments on the FLORES-200 benchmark dataset for low-resource languages NLLB Team et al. (2022) and contamination-free WMT datasets Arthur et al. (2016); Duan et al. (2020); Zhong and Chiang (2020), we demonstrate that our method significantly outperforms existing approaches, achieving state-of-the-art results on multiple language pairs. Our main contributions are three-fold: • The proposed method allows for the effective identification of keywords that are crucial to the quality of translation, while simultaneously filtering out less relevant words that do not contribute to accuracy. Furthermore, the method ensures adherence to the constraints that are typically associated with translation. • The proposed approach employs Retrieval-Augmented Generation (RAG) to improve translation quality. By incorporating keywords retrieved from bilingual dictionaries into the LLM’s context window and applying a post-translation self-checking mechanism, the method minimizes the potential for misunderstandings and optimizes. • We further advanced machine translation tasks employing LLMs with 7B parameters by leveraging prompt-based techniques, obviating the necessity for fine-tuning. This approach yielded notable improvements in translation performance."
https://arxiv.org/html/2411.08344v1,Bangla Grammatical Error Detection Leveraging Transformer-based Token Classification,"Bangla is the seventh most spoken language by a total number of speakers in the world, and yet the development of an automated grammar checker in this language is an understudied problem. Bangla grammatical error detection is a task of detecting sub-strings of a Bangla text that contain grammatical, punctuation, or spelling errors, which is crucial for developing an automated Bangla typing assistant.Our approach involves breaking down the task as a token classification problem and utilizing state-of-the-art transformer-based models. Finally, we combine the output of these models and apply rule-based post-processing to generate a more reliable and comprehensive result. Our system is evaluated on a dataset consisting of over 25,000 texts from various sources. Our best model achieves a Levenshtein distance score of 1.04. Finally, we provide a detailed analysis of different components of our system.","Typing assistance has become increasingly important in today’s digital age, especially with the rise of grammar-checking systems. Users expect typing assistance tools to not only detect and correct grammatical errors but also provide suggestions to improve their writing skills. With the help of predictive text, spell checkers, and auto-correct features, typing assistance tools can help users to type accurately and efficiently, reducing the time needed to correct errors. These features can also help users to expand their vocabulary and improve their grammar, leading to better writing skills and effective communication. With the rise of AI and machine learning, typing assistance tools such as Grammarly are becoming more advanced, helping users to improve their writing skills and communicate effectively. Errors in Bangla text can be due to spelling, punctuation, or grammar. The spelling error itself can occur in various forms Bijoy et al. (2022) shown in Table 1. Examples of errors similar to ERRANT Bryant et al. (2017) error classes are shown in Table 2. Alam et al. (2020) approaches the punctuation correction problem only considering commas, periods, and question marks for ASR applications. Table 1: Types of Spelling Errors Table 2: Errors by ERRANT Classification"
https://arxiv.org/html/2411.08324v1,"Are LLMs Prescient? A Continuous Evaluation 
using Daily News as the Oracle","Many existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to the emergence of new models and training data. These benchmarks also fall short in assessing how LLM performance changes over time, as they consist of static questions without a temporal dimension. To address these limitations, we propose using future event prediction as a continuous evaluation method to assess LLMs’ temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict “future” event outcomes. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy, the performance degradation pattern persists, highlighting the need for continuous model updates.","Traditional Large Language Model (LLM) benchmarks are often static, and do not reflect real-world information that evolves over time. This presents two significant challenges. First, as LLMs are updated, there is a risk that the static benchmarks become outdated and more vulnerable to data leakage, where their content might end up in the training data of newer models. This undermines the reliability of performance assessments on these benchmarks (Sainz et al.,, 2023; Xu et al.,, 2024; McIntosh et al.,, 2024; Li and Flanigan,, 2024). Second, the static benchmarks often lack the temporal information to track the model’s performance variations over time (McIntosh et al.,, 2024). This creates a need for evaluation methods that stay relevant over time and incorporate temporal dynamics. Daily news provides a natural setting for continuous evaluation of LLMs. Since the world is constantly changing, a benchmark designed around forecasting the next day’s news will never be out of date by construction. In addition to enabling continuous evaluation, forecasting is itself a longstanding challenge with significant implications across various domains, including healthcare, finance, and policymaking (Tetlock and Gardner,, 2016; Dempsey et al.,, 2017; Gillingham et al.,, 2018; Lopez-Lira and Tang,, 2023). While human experts have traditionally made such forecasts, machine learning models, particularly LLMs, have emerged as promising alternatives due to their capability to learn from vast and diverse corpora (Halawi et al.,, 2024; Ye et al.,, 2024; Yan et al.,, 2023). Several recent forecasting question-answer (QA) datasets have been developed (Jin et al.,, 2021; Zou et al.,, 2022; Zhang et al.,, 2024), however, they are limited in either size, scope, or do not continuously keep pace with the rapidly changing world. More critically, the extent to which LLMs’ predictive abilities change over time remains underexplored. In this work, we propose Daily Oracle—a continuous evaluation benchmark that uses automatically generated QA pairs from daily news to assess how the future prediction capabilities of LLMs evolve over time. The QA pairs are generated on a daily basis, consisting of True/False (TF) and Multiple Choice (MC) questions across various categories such as business, politics, and arts. Unlike traditional reading comprehension tasks, these QA pairs are designed to challenge LLMs to predict future events based on their own existing knowledge, effectively evaluating their temporal generalization and forecasting abilities. We continuously evaluate various LLMs, both with and without access to a limited archive of news articles. Our experiments reveal that LLMs experience an average performance decline of 20.14% on True/False (TF) questions and 23.26% on Multiple Choice (MC) questions between January 2020 and September 2024, with degradation becoming more pronounced before and after the models’ knowledge cutoff dates. Although models utilizing Retrieval Augmented Generation (RAG) (Lewis et al.,, 2020) can demonstrate improved prediction performance, the downward trend persists, suggesting an ongoing challenge in maintaining temporal generalization. Overall, our benchmark highlights the challenges posed by outdated pertaining data in LLMs, and underscores the necessity for continuous model updating to keep up with the constantly evolving stream of real-world information. To summarize, our key contributions are two-fold: • Continuous Forecasting Evaluation Benchmark: We present Daily Oracle, the largest and most up-to-date forecasting dataset, composed of automatically generated QA pairs. This benchmark continuously evaluates LLMs’ temporal generalization and future prediction abilities using daily news, ensuring relevance over time and offering a challenging evaluation framework. • Empirical Findings on Performance Degradation: Our experiments demonstrate a consistent decline in LLM prediction accuracy over time, even when supplemented with recent “open-book” information. The persistent degradation highlights the challenges posed by outdated pretraining data in LLMs and underscores the necessity for continuous model updating."
https://arxiv.org/html/2411.08302v1,R3HF: Reward Redistribution for Enhancing Reinforcement Learning from Human Feedback,"Reinforcement learning from human feedback (RLHF) provides a paradigm for aligning large language models (LLMs) with human preferences. This involves the initial training of a reward model based on pairwise human feedback. The reward model is subsequently utilized in reinforcement learning to assess the scores of each generated sentence as a whole, further guiding the optimization of LLMs. However, current approaches have a significant shortcoming: They allocate a single, sparse, and delayed reward to an entire sequence of output. This may overlook some significant individual contributions of each token towards the desired outcome. To overcome this limitation, our paper proposes a novel reward redistribution method called R3HF, which facilitates a more fine-grained, token-level reward allocation. Specifically, our method treats the reward prediction task of the reward model as a regression problem. As a result, the redistributed rewards are computed by evaluating the specific contribution of each token to the reward model’s output. This detailed approach improves the model’s understanding of language nuances, leading to more precise enhancements in its performance. Our method is crafted to integrate seamlessly with most current techniques while incurring minimal computational costs. Through comprehensive experiments across diverse datasets and tasks, we have verified the effectiveness and superiority of our approach111Warning: The Appendix contains example data that may be offensive or harmful..","Large language models (LLMs) have showcased remarkable adaptabilities across various tasks, with applications spanning fields like psychology [12], education [49, 19], and medical support [46, 27]. However, as LLMs become increasingly sophisticated, the complexity of their decision-making processes and outputs also escalates, introducing potential risks such as the propagation of bias [14, 48], generation of misinformation [24, 29], and potential harm [16, 15]. This underscores the critical need for effective alignment [35, 50, 26, 11] of LLMs. Such alignment aims to guide the models to better comprehend and prioritize human preferences, ensuring their operations are in tune with human values and ethics. Reinforcement learning from human feedback (RLHF) [10, 29, 5] is an advanced paradigm that incorporates human feedback into LLM training. This approach typically unfolds in three primary stages, which is shown in Figure 1 (left). The initial stage involves supervised fine-tuning (SFT) applied to the target domain. Subsequently, the second stage develops and trains a reward model on data that reflect human preferences. The final stage is dedicated to refining the language model using reinforcement learning algorithms with the learned reward model. Though RLHF technology has demonstrated its effectiveness in various scenarios, it also presents a significant drawback that hampers the training efficiency of the model. Traditional reward models typically assess the overall effectiveness of an entire generated sequence, assigning a score only after delivering the final token, with the other tokens receiving a score of zero. This reward structure, being both sparse and delayed, challenges the model in recognizing the impact of individual tokens. An intuitive example is illustrated in Figure 1(right). Consider a question-answering task with the prompt, “Was Walt Disney the original creator of Mickey Mouse? <end>” and the generated response, “Yes, Walter Elias Disney was indeed the original creator of Mickey Mouse.” The reward model assigns a positive evaluation score of 0.8. However, when treating the entire sentence as an episode, traditional methods only allocate a score of 0.8 to the “<end>” token, potentially hindering the efficient optimization of LLMs. Meanwhile, the initial tokens in a sequence can significantly influence the subsequent generation, a nuance that current methodologies often struggle to accommodate effectively. In the example, the word “Yes” is the most crucial token in the generated sequence that influences the overall score, yet it receives a reward of zero. This highlights the urgent need for methodologies that better recognize and reward the contribution of each token. Figure 1: Left: The training paradigm of reinforcement learning from human feedback typically encompasses three stages. Our proposed method is applied in the final stage, where we redistribute the holistic rewards at the terminal time-step to provide a fine-grained and immediate reward for each generated token. This approach aims to more effectively guide the optimization of Large Language Models (LLMs). Right: An example of reward redistribution, where the sum of the fine-grained rewards is equivalent to the original sparse reward. To address this shortcoming, in this paper, we introduce a novel approach: Reward Redistribution for enhancing Reinforcement learning from Human Feedback (R3HF). The foundational concept of our method involves assigning individual credits to each token within the generated sentences, thus providing a nuanced signal for optimization tailored to LLMs. As the example in Figure 1(right), since “Yes” is the most crucial token according to the reward model, it receives the highest reward signal. Similarly, other tokens receive different rewards, which can be either positive or negative. The sum of the rewards for all tokens in the sequence is equivalent to the original overall reward score. To achieve this goal, our approach is operationalized through the framework of Sequence-Markov Decision process (SDPs) [4], wherein the allocation of rewards is not constrained by the Markov property. Concretely, we conceptualize the reward model as a regression model, predicting each sequence-wide return from the entire state-action sequence. By adopting this perspective, we allocate to each token a portion of credit relative to its incremental impact on the reward model compared to the preceding time-step. The credits can be inferred through temporally differentiated computation, providing more granular guidance over the language generation process. Compared to state-of-the-art RLHF approaches, our R3HF offers the following advantages: (1) Learning Efficiency. By providing token-level rewards, our method significantly enhances learning by offering immediate and relevant information. This approach avoids the limitations of delayed rewards that may be less informative. Consequently, it facilitates more accurate fine-tuning of language models, leading to considerable advancements in language generation that are more closely aligned with human feedback. (2) Independence from Human Labeling. The redistributed rewards do not depend on extensive human labeling of data. The reward model itself dynamically assigns value to each token based on its contribution to the overall sequence, thus reducing the need for labor-intensive labeling efforts. This feature facilitates the rapid incorporation of feedback without being bottlenecked by the pace of human annotation, streamlining the training process. (3) Seamless Integration. Our method is designed for easy application across most mainstream RLHF paradigms, requiring only minimal modification that involves simple recomputation of rewards. This compatibility ensures that existing RLHF methods can be effortlessly enhanced with our token-level reward redistribution technique, boosting their effectiveness without necessitating extensive overhaul or complex re-engineering. To evaluate the efficacy of our approach, we have conducted a series of experiments across a diverse set of tasks, including summarization, question-answering, and harmfulness mitigation&helpfulness enhancement. Moreover, we have applied our reward distribution technique to various established RLHF methods. The empirical results from these experiments indicate that by training with such fine-grained feedback, our method consistently exhibits improved performance across all tested tasks."
https://arxiv.org/html/2411.08278v2,Knowledge Bases in Support of Large Language Models for Processing Web News,"Large Language Models (LLMs) have received considerable interest in wide applications lately. During pre-training via massive datasets, such a model implicitly memorizes the factual knowledge of trained datasets in its hidden parameters. However, knowledge held implicitly in parameters often makes its use by downstream applications ineffective due to the lack of common-sense reasoning. In this article, we introduce a general framework that permits to build knowledge bases with an aid of LLMs, tailored for processing Web news. The framework applies a rule-based News Information Extractor (NewsIE) to news items for extracting their relational tuples, referred to as knowledge bases, which are then graph-convoluted with the implicit knowledge facts of news items obtained by LLMs, for their classification. It involves two lightweight components: 1) NewsIE: for extracting the structural information of every news item, in the form of relational tuples; 2) BERTGraph: for graph-convoluting the implicit knowledge facts with relational tuples extracted by NewsIE. We have evaluated our framework under different news-related datasets for news category classification, with promising experimental results.","Figure 1. Illustration of Knowledge Base (left) and Large Language Model (right), with the former typically storing structured knowledge explicitly and the latter holding unstructured knowledge implicitly. Large Language Models have garnered widespread enthusiasm lately as conversational agents for diverse applications. Noticeably, ChatGPT (Schulman et. al., 2022) takes queries and creates responses, Google’s Bard (Manyika, 2023) can produce creative replies due to its coding and multilingual capabilities, ChatSonic (Coombe, 2023) generates update-to-date replies with the aid of Google Search for accurate and informative content creation, among others. Meanwhile, a simple and yet powerful language representation model, known as Bidirectional Encoder Representations from Transformers (BERT)(Kenton and Toutanova, 2019), was introduced. As an LLM family member, BERT comprises a stack of transformer encoders and is pre-trained via unlabeled texts. Most LLMs are fine-turnable using small amounts of domain-specific data for improving the performance of the given domain tasks. In particular, the BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks (Kenton and Toutanova, 2019), such as question answering and language inference, with little changes to the models. Conventional LLMs usually take massive amounts of unstructured text data for pre-training, before made them available for general applications. Although a pre-trained LLM can produce replies to its inputs (such as Web news items), it does not inherently capture structural and relational information of tokens existing in its every input, presenting an opportunity for improving the quality of replies. To this end, we resort to knowledge bases (KBs), which are created by pre-processing Web news items individually to extract structural and relational information of token in each item, such as the triggers, the arguments, the temporal relations, etc. This KB creation is undertaken in the rule-based manner automatically without requiring any relation-specific training data. Pre-processing news items extracts a number of relational tuples (Arg1, Pred, Arg2) per item, realized by the News Information Extractor (NewsIE) we have developed. The KBs created out of news items make it possible to produce better replies to inputs (i.e., news items) that are fed to a given LLM for processing, by complementing the LLM outputs. This results from the fact that LLM outputs contain no structural information while the KBs hold structural and relational information of individual news items. In this article, we consider the framework that lets KBs support LLMs for improving Web news processing performance. The framework consists of an LLM and a graph convolutional network (GCN) (Defferrard et al., 2016), which is inputted with the LLM output and KBs for graph-convolutive operations to let KBs complement the LLM output. The GCN serve to convolute relational tuples with the implicit knowledge facts obtained by the LLM, which is fine-tuned by Web news items for improving the performance of news processing. The fine-tuned LLM takes raw Web news themselves directly as its input. While any LLM may be adopted to form the framework, we use BERT (Kenton and Toutanova, 2019) as an example LLM in this article, realized our BERTGraph. Without making any change to BERT for use in BERTGraph, our framework is in contrast to prior designs, like KG-BERT (Yao et al., 2019) and K-BERT (Liu et al., 2020), which change the BERT input format, and Relphormer (Bi et al., 2022), which modifies the BERT encoder structure. However, any change to the input format or the encoder structure of BERT destroys its original embedding, requiring prohibitively expensive LLM model re-training, making our BERTGraph more favorable in practice and able to fully utilize implicit knowledge learned by BERT. More details of the BERTGraph framework can be found in Subsection 3.1.1 For the BERT output and KBs to be compatible with the GCN input, a text-to-graph adapter is devised, as stated in Subsection 3.1.2. We have implemented BERTGraph for experimentally evaluating its news category classification performance under three publicly available Web news datasets, N24News (Wang et al., 2021a), SNOPES (Snopes, 2023), and politifact (Politifact, 2023). Our evaluation results demonstrate that BERTGraph outperforms its BERT counterpart for evaluated news datasets, in terms of all performance metrics but the precision of politifact, with 0.27 versus 0.28. The remainder of this paper is organized as follows. Section 2 presents related background and prior work. Section 3 overviews the proposed BERTGraph design, with its two key components detailed in sequence. Section 4 describes the implementation of BERTGraph and then provides experimental results under different news datasets for news category classification. Section 5 concludes this paper."
https://arxiv.org/html/2411.08248v1,Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach,"Deep learning underpins most of the currently advanced natural language processing (NLP) tasks such as textual classification, neural machine translation (NMT), abstractive summarization and question-answering (QA). However, the robustness of the models, particularly QA models, against adversarial attacks is a critical concern that remains insufficiently explored. This paper introduces QA-Attack (Question Answering Attack), a novel word-level adversarial strategy that fools QA models. Our attention-based attack exploits the customized attention mechanism and deletion ranking strategy to identify and target specific words within contextual passages. It creates deceptive inputs by carefully choosing and substituting synonyms, preserving grammatical integrity while misleading the model to produce incorrect responses. Our approach demonstrates versatility across various question types, particularly when dealing with extensive long textual inputs. Extensive experiments on multiple benchmark datasets demonstrate that QA-Attack successfully deceives baseline QA models and surpasses existing adversarial techniques regarding success rate, semantics changes, BLEU score, fluency and grammar error rate.","Question-answering (QA) models, a key task within Sequence-to-Sequence (Seq2Seq) frameworks, aim to enhance computers’ ability to process and respond to natural language queries. As these models have evolved, they have been widely adopted in real-world applications such as customer service chatbots[40], search engines [69], and information retrieval in fields like medicine [19] and law [35]. However, despite the significant progress in deep learning and natural language processing (NLP), these models remain vulnerable to adversarial examples, leading to misinformation, privacy breaches, and flawed decision-making in critical areas [23, 8, 15, 52]. This highlights the importance of understanding how adversarial examples are generated from the attackers’ perspective and potential defense mechanisms — an area that remains under-explored. QA models are expected to comprehend given texts and questions, providing accurate and contextually relevant answers [50]. These models primarily address two types of questions: Informative Queries and Boolean Queries. The Informative Queries typically begin with interrogative words such as “who,” “what,” “where,” “when,” “why,” or “how,” requiring detailed and specific information from the provided context. Although models like T5 [43], LongT5 [14], and BART [27], which follow an encoder-decoder structure, have demonstrated strong performance, they still suffer from the maliciously crafted adversarial examples. Initially, studies like “Trick Me If You Can” [57] primarily relied on human annotators to construct effective adversarial question-answering examples. This methodology, however, inherently constrained scalability and increased resource demands. As research progressed, automated approaches for attacking textual classifiers in QA models emerged. Gradient-based methods, as employed in RobustQA [63], UAT [56], and HotFlip [10], were developed to identify and modify the most influential words affecting model answers. Building upon a deeper understanding of QA tasks, subsequent studies explored more targeted strategies. For instance, Position Bias [24], TASA [5], and Entropy Maximization [49] investigated the manipulation of sentence locations and the analysis of answer sentences to identify vulnerable parts of the context. These approaches refined the attack methods by applying modifications through paraphrasing or replacing original sentences, thus enhancing the effectiveness of adversarial examples. However, these methods encounter two primary challenges: 1) None of these attack methods is suitable for both “informative queries” and “boolean queries”. 2) Constraining the search space for optimal vulnerable words to answer-related sentences compromises attack effectiveness; meanwhile, targeting entire sentences proves inefficient [17]. In addition, Boolean Queries seek a simple binary “Yes” or “No” answer. Models like BERT [7], RoBERTa [70], and GPT variants [22, 3, 1, 51], which excel at sentence-level understanding and token classification, are widely used for Boolean QA tasks. These models leverage their deep contextual understanding of language to accurately determine whether a given statement is true or false, making them state-of-the-art baselines for the task. Researchers have proposed various approaches to target boolean classifiers in the context of Boolean Queries attacks. Attacks like [31, 12, 18, 66, 46], which involve adding, relocating, or replacing words, are based on the influence that each word has on the prediction. They retrieve word importance by the output confidence to the level or with gradient. However, gradient calculation is computationally intensive and ineffective when dealing with long context input, and knowing victim models’ internal information is unrealistic in practice. We present QA-Attack, an adversarial attack framework tailored for both Informative Queries and Boolean Queries in QA models. QA-Attack uses a Hybrid Ranking Fusion (HRF) algorithm that integrates two methods: Attention-based Ranking (ABR) and Removal-based Ranking (RBR). ABR identifies important words by analyzing the attention weights during question processing, while RBR evaluates word significance by observing changes in the model’s output when specific words are removed. The HRF algorithm combines these insights to locate vulnerable tokens, which are replaced with carefully selected synonyms to generate adversarial examples. These examples mislead the QA system while preserving the input’s meaning. This unified attack method improves both performance and stealth, ensuring realistic applicability for both types of queries. In summary, our work makes the following key contributions: • We present QA-Attack with a Hybrid Ranking Fusion (HRF) algorithm designed to target question-answering models. This novel approach integrates attention and removal ranking techniques, accurately locating vulnerable words and fooling the QA model with a high success rate. • Our QA-Attack can effectively target multiple types of questions. This adaptability allows our method to exploit vulnerabilities across diverse question formats, which significantly broadens the scope of potential attacks in various real-world scenarios. • QA-Attack generates adversarial examples by implementing subtle word-level changes that preserve both linguistic and semantic integrity while minimizing the extent of alterations, and we conduct extensive experiments on multiple datasets and victim models to thoroughly evaluate our method’s effectiveness in attacking QA models. The rest of this paper is structured as follows. We first review QA system baselines and adversarial attacks for QA models in Section 2. Then we detail our proposed method in Section 3. We evaluate the performance of the proposed method through extensive empirical analysis in Section 4. We conclude the paper with suggestions for future work in Section 5."
https://arxiv.org/html/2411.08243v1,Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset,"In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset’s content through both manual and automated evaluation; (2) experiments demonstrating the dataset’s impact on models’ safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs.Warning: This paper contains model outputs that may be considered offensive.","Learning from Human Feedback (LHF) has gained in popularity in recent years as a strategy to mitigate the harms of large language models (LLMs) while preserving their utility. LHF consists of having human annotators assign relative preferences between model outputs, then training a model to generate outputs which are more likely to be preferred by the annotators. Although the first instances of LHF were in robotics Christiano et al. (2017) and text summarization Stiennon et al. (2020), it has been adopted as a harm reduction strategy by companies like Anthropic Bai et al. (2022), Google DeepMind Gemini Team et al. (2024) and OpenAI (OpenAI, 2023). Among the guiding principles for aligning LLMs, Askell et al. (2021) define an AI system as aligned if it is helpful, honest, and harmless (HHH). Helpfulness refers to the LLM’s capacity to perform a task or answer a question. Honesty pertains to factual accuracy and truthfulness. Harmlessness encompasses a range of considerations. Following the adoption of the HHH principles, recent studies have shifted towards a more integrated approach, recognizing these challenges as part of a broader “safety bundle” of misaligned behaviors (Ouyang et al., 2022a; Bai et al., 2022; Touvron et al., 2023). Anthropic’s Helpful and Harmless dataset Bai et al. (2022) serves as a prototypical example of this trend—a human preferences dataset designed for safety mitigation, cited over 1000 times and used to train more than 200 models.***https://huggingface.co/models?dataset=dataset:Anthropic/hh-rlhf This presents a shift in paradigm, as earlier research in responsible natural language processing (NLP) focused on distinct methods for identifying, quantifying, and mitigating issues such as stereotypical bias (Chu et al., 2024), toxicity (Gehman et al., 2020), and privacy leakage (Huang et al., 2022). Despite the widespread adoption of the HH dataset as a benchmark for models’ safety alignment, it is not clear to what extent this dataset truly succeeds at making LLMs more harmless and more helpful. In fact, recent work shows that models trained with LHF with HH preferences are more likely to exhibit safety failures associated with these preferences and the trade-offs they create (i.e. a safer model is less likely to respond to a query, making it less helpful, and vice versa) (Röttger et al., 2023; Bianchi et al., 2024; Chehbouni et al., 2024) or to showcase superficial safety alignment (Zhou et al., 2023a; Lin et al., 2024) — meaning that the model’s alignment predominantly improves the style of its outputs rather than influencing its underlying knowledge. Furthermore, while various work has looked at the shortcomings of open-source corpora (Gehman et al., 2020; Dodge et al., 2021; Birhane et al., 2023), to the best of our knowledge, none has investigated the content of preference datasets. In this work, we conduct a comprehensive audit to examine how the dataset embodies the HH principles and explore the connection between the conceptualization of these principles and the safety failures reported in the literature. We investigate the following three research questions by evaluating the quality of the dataset and its effectiveness for safety mitigation: (RQ1) What’s in the HH dataset? We provide a thorough audit of the HH dataset through an exploratory analysis as well as a human evaluation and show its various shortcomings, including a failure to conceptualize harmlessness and quality issues (§3); (RQ2) What can models learn from the HH dataset? We train models using different variations of the HH dataset and evaluate them for safety (Röttger et al., 2023). Our analysis reveals that using LHF with the HH dataset can lead to disparate exaggerated safety behaviors across demographic groups (§4); and (RQ3) How did the HH dataset impact the community? We conduct a survey of the 100 most influential papers that reference the original study and offer insights into how the dataset has been adopted by the broader research community as well as how some inherent limitations of the dataset are framed as an inevitable trade-off between helpfulness and harmlessness (§5). Through this multidimensional audit, we showcase the limitations of the “safety bundle” introduced by the HHH principles and operationalized by the HH dataset. In light of these findings, we highlight the need for more nuanced and context-sensitive approaches for safety mitigation as well as a shift in how these considerations are addressed by the community."
https://arxiv.org/html/2411.08147v1,Large Language Models Can Self-Improve in Long-context Reasoning,"Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose SeaLong, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of SeaLong, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, SeaLong achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.111The repository can be accessed at https://github.com/SihengLi99/SEALONG.","Prompt Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct HotpotQA MuSiQue 2WikiMQA HotpotQA MuSiQue 2WikiMQA Default 55.5 33.0 66.0 60.0 54.0 77.0 Direct answer 49.0 28.5 55.0 61.5 51.5 74.0 Think step-by-step (Kojima et al., 2022) 62.5 50.5 77.5 75.5 62.5 85.0 Fact-and-reflection (Zhao et al., 2024b) 67.0 49.0 76.5 78.0 62.0 84.0 Plan-and-solve (Wang et al., 2023a) 64.0 49.5 82.0 74.0 68.5 85.5 Table 1: Comparison of various prompting methods. The best result is highlighted in bold. Large language models (LLMs) with long-context processing capabilities have spurred a range of novel applications, such as repository-level coding assistance (Jimenez et al., 2024), multi-document analysis (Wang et al., 2024b) and autonomous agents (Ma et al., 2024). Delivering high-quality service in these domains requires LLMs to reason effectively over long contexts, necessitating the retrieval of essential details and integration of dispersed information throughout the reasoning process. While recent advancements have enabled LLMs to attain near-perfect accuracy on the needle-in-a-haystack (NIAH; Kamradt (2023); Li et al. (2024b)) task (Hsieh et al., 2024; Yen et al., 2024b; Dubey et al., 2024), which involves locating evidence within vast amounts of irrelevant text, substantial performance declines persist on tasks that require reasoning over long contexts (Levy et al., 2024; Hsieh et al., 2024; Vodrahalli et al., 2024; Yen et al., 2024b; Li et al., 2024a), limiting their applicability in real-word scenarios. To address this limitation, recent studies have investigated fine-tuning LLMs to improve long-context reasoning, with effective data synthesis as a primary challenge. Two main approaches have emerged: one relies on human annotations (Chen et al., 2024b; Li et al., 2024c, a), which are expensive and difficult to scale; the other leverages expert models, such as GPT-4o (Hurst et al., 2024), for data synthesis. For example, Bai et al. (2024); Zhang et al. (2024c, b) apply self-instruct techniques (Wang et al., 2023b) to create long-context instruction-following data. Despite substantial progress, the dependence on pre-existing expert models limits the potential for achieving more advanced capabilities. This work investigates whether LLMs can self-improve in long-context reasoning. Drawing on evidence of LLMs’ near-perfect long-context retrieval and strong reasoning abilities in general domains (Bubeck et al., 2023; Zhong et al., 2024), we hypothesize that LLMs have untapped potential in long-context reasoning. Our preliminary studies show that refined prompting strategies achieve notable improvements over both default prompting methods and direct answer requests. Furthermore, scaling the number of sampled outputs reveals a marked performance gap between the optimal outputs and those derived via greedy search. These results suggest that LLMs hold substantial potential to advance in long-context reasoning. Inspired by these observations, we propose a Self-improving method for rEAsoning over LONG-contexts (SeaLong). This involves first sampling multiple reasoning trajectories from the LLM, then scoring each based on Minimum Bayes Risk (MBR) (Bickel and Doksum, 1977), which prioritizes outputs that are more consistent with others. This idea is intuitive, as reasoning trajectories that deviate from the majority are more likely to be hallucinations (Manakul et al., 2023; Farquhar et al., 2024). Following this, we can either conduct supervised fine-tuning using high-scoring outputs or apply preference optimization by utilizing both high-scoring and low-scoring outputs. We apply SeaLong to several leading LLMs and conduct evaluations on multiple long-context reasoning tasks (Bai et al., 2023; Yang et al., 2018; Trivedi et al., 2022; Ho et al., 2020; Dasigi et al., 2021). The results reveal that LLMs can self-improve in long-context reasoning. Specifically, SeaLong raises the score of Llama-3.1-8B-Instruct (Dubey et al., 2024) from 50.8 to 55.0. Additionally, SeaLong enables Qwen-2.5-14B-Instruct (Yang et al., 2024a) to outperform its 32B variant (54.7 vs. 53.1). In comparison to previous synthetic data, SeaLong demonstrate notable improvement without requiring human or expert model annotation. We hope that SeaLong can pave the way for self-improving approaches in long-context scenarios, supporting the continual advancement of LLM capabilities."
https://arxiv.org/html/2411.08790v1,Can sparse autoencoders be used to decompose and interpret steering vectors?,"Steering vectors are a promising approach to control the behaviour of large language models. However, their underlying mechanisms remain poorly understood. While sparse autoencoders (SAEs) may offer a potential method to interpret steering vectors, recent findings show that SAE-reconstructed vectors often lack the steering properties of the original vectors. This paper investigates why directly applying SAEs to steering vectors yields misleading decompositions, identifying two reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in feature directions, which SAEs are not designed to accommodate. These limitations hinder the direct use of SAEs for interpreting steering vectors.111Code is available at https://github.com/HarryMayne/SV_interpretability","As language models advance, there is growing interest in methods to steer their behaviours toward desirable characteristics [1]. Recently, steering vectors (or activation steering) have been proposed as a way to achieve this without requiring model fine-tuning [20, 13, 10, 23]. Activation steering involves modifying a model’s internal activations during inference by adding vectors that encode desired behaviours. These methods have shown the potential to regulate behaviours such as sycophancy [13], harmlessness [23], and refusal [2, 23]. Despite promising empirical results, the underlying mechanisms behind steering vectors remain poorly understood [9, 4]. Interpreting steering vectors by decomposing them into granular features may help clarify why certain behaviours are more steerable than others [18], why combining steering vectors is largely unsuccessful [22], and may help produce more precise steering vectors [8]. Recent work has explored interpreting steering vectors using sparse autoencoders (SAEs) [4, 8]. SAEs are an emerging method for decomposing model activations into sparse, non-negative linear combinations of vectors, where many vectors appear to correspond to meaningful, interpretable concepts [5, 3]. Since steering vectors exist within the same space as model activations, they could theoretically be expressed as combinations of SAE features [4, 8, 9]. However, past studies found that directly decomposing steering vectors with SAEs produced mixed results, with the reconstructed vectors often failing to retain the steering properties of the original vectors. This suggests that the SAE decompositions did not capture essential elements of the steering vectors [8]. Motivated by these mixed results, this paper investigates the theoretical reasons why SAEs provide misleading decompositions of steering vectors and supports each reason with empirical evidence. We identify two main reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in SAE feature directions, which SAEs are not designed to accommodate. These issues limit the direct application of SAEs for interpreting steering vectors. Our contributions are to highlight these issues, motivating new methods to address them."
https://arxiv.org/html/2411.08671v1,Theoretical Analysis of Byte-Pair Encoding,"Byte-Pair Encoding (BPE) is a widely used method for subword tokenization, with origins in grammar-based text compression. It is employed in a variety of language processing tasks such as machine translation or large language model (LLM) pretraining, to create a token dictionary of a prescribed size. Most evaluations of BPE to date are empirical, and the reasons for its good practical performance are not well understood.In this paper we focus on the optimization problem underlying BPE: finding a pair encoding that achieves optimal compression utility. We show that this problem is APX-complete, indicating that it is unlikely to admit a polynomial-time approximation scheme. This answers, in a stronger form, a question recently raised by Zouhar et al. [ZMG+23].On the positive side, we show that BPE approximates the compression utility of the optimal pair encoding to a worst-case factor between 0.333 and 0.625. Our results aim to explain the ongoing success of BPE and are, to our knowledge, the first rigorous guarantees on its compression utility that hold for all inputs.","A common step in the modern NLP application pipeline is tokenization: given an input text, the task is to partition it into tokens, i.e., frequently occurring consecutive groups of symbols. The main goal is to identify semantically meaningful units (words or subwords) in order to facilitate higher level tasks (e.g., translation or text generation) [MAS+21, AFT+23]. As this goal is difficult to directly optimize for, tokenization is usually solved heuristically, or formulated as a different but closely related task: data compression. Indeed, the dictionary-encoding of tokens reduces text length; the amount of compression is easy to measure, and was found to be a good predictor of the quality of tokenization for downstream tasks, e.g., for translation accuracy [Gal19]. It is thus natural to study tokenization with the proxy optimization goal of compression utility. Byte-Pair Encoding (BPE), introduced by Gage in 1994 [Gag94], is a commonly used heuristic for tokenization. It proceeds by repeatedly identifying the most frequently occurring pair of symbols and replacing all occurrences of this pair with a new symbol, thereby shortening the text. The new symbols, together with the pairs they replace, are stored in a lookup-table, which allows the reconstruction of the original text. In typical applications, the number of new symbols (and thus the size of the lookup-table) is fixed upfront, and the goal is to achieve the best compression within this budget. The symbols of the resulting (shortened) text correspond to the tokens of the input. Figure 1 shows an example of the encoding of a text by BPE. BPE has become a de-facto standard in NLP applications, widely employed in machine translation [Sen15, XZG+21, DGMH+19, Gal19, GVBS23] and in the preprocessing stage of training large language models [BMR+20, LLSZ21, TLI+23, RWC+19, LSFA+23, WGC+23]111See also https://github.com/google/sentencepiece and https://github.com/openai/tiktoken.. Besides its effectiveness, the popularity of BPE is likely due to its simplicity and computational efficiency, when compared with more sophisticated or linguistically motivated methods (e.g., see [BD20, SRZ+24]). A careful implementation of BPE has a total runtime that is linear in the input length, for an arbitrary number of replacement rounds. In addition, a BPE-encoded representation can support efficient random access and pattern-matching on the original text, which is important in some applications [SKF+99]. Given the popularity and good empirical performance of BPE, there is a surprising lack of rigorous guarantees for the quality of its output. In this paper we study the problem of compressing a text (a string s over some alphabet \Sigma) by successive encoding of pairs (strings of length two). Adopting the framework of approximation algorithms [WS11] we study how well BPE, as a natural greedy heuristic, approximates this problem. Our optimization goal is to maximize compression utility, i.e., the reduction in text length, within k pair-encoding rounds, where s and k are given as the input (we precisely define this problem – optimal pair encoding – later in this section). The problem formulation we use closely resembles the one recently introduced by Zouhar et al. [ZMG+23] for the same task. This abstract setting presents a challenging algorithm design problem of independent interest and allows a clean theoretical analysis of BPE. Note however, that we necessarily ignore some practical aspects and optimizations of BPE-variants (e.g., special treatment of whitespace and punctuation or language-specific rules [RWC+19, AFT+23]). An algorithm \mathscr{A} for optimal pair encoding has approximation ratio \upalpha\leq 1, if the compression utility of \mathscr{A} is at least \upalpha times the optimum for all inputs (s,k). The greedy step of BPE is locally optimal, and thus, for k=1 it achieves optimal compression. For k>1, however, simple examples show that BPE may not achieve optimal compression (see Figure 1). As our main complexity-result, we show that optimal pair encoding is APX-complete. This means (informally) that no polynomial-time algorithm can approximate it to a factor arbitrarily close to 1, unless P=NP. On the positive side, we show that BPE achieves an approximation ratio \upalpha, with 0.333<\upalpha\leq 0.625. We note that previously no constant-approximation guarantee was known for BPE or other algorithms. The question of whether optimal pair encoding is NP-complete was raised recently by Zouhar et al. [ZMG+23]; our result settles this question in a stronger form. \setcapindent 0em \displaystyle\mathtt{aabaaaba\rightarrow XbXaba\rightarrow YXaba\rightarrow Zaba} \displaystyle\mathtt{aabaaaba\rightarrow aXaaXa\rightarrow YaYa\rightarrow ZZ} Figure 1: Input s=\mathtt{aabaaaba} encoded by BPE merge sequence (\mathtt{aa\rightarrow X,\,Xb\rightarrow Y,\,YX\rightarrow Z)} (above). An optimal encoding by the merge sequence (\mathtt{ab\rightarrow X,\,aX\rightarrow Y,\,Ya\rightarrow Z}) (below). Before precisely stating our results, we review some further related work and give a formal definition of the problem and the algorithms that we study. Related work. BPE has its origins in text compression, in particular, the class of grammar-based compression methods or macro schemes, e.g., see [SS82, KY00, CLL+05, Loh12] for surveys. (The encoding obtained by BPE can be seen as a restricted kind of context-free grammar or string straight-line program.) A compression method closely related to BPE is Re-Pair [LM00]. Re-Pair differs from BPE in that its number of replacement rounds k is not fixed; instead, it performs replacements as long as they achieve a saving (i.e., as long as some pair appears in at least two disjoint copies). Re-Pair is widely used, e.g., in bioinformatics, and several variants and practical improvements of it have been proposed [KVOB24, GJ17, GIM+19, FTN+19]. The central question of grammar-based compression is to find a minimal grammar that generates a given text. This task is known to be NP-hard, as well as hard to approximate [SS82, CLL+05] (by some constant factor). The best known approximation ratio for the grammar-based compression of an input of length n is O(\log{n}) [Ryt03, Jeż14]. The approximation ratio of Re-Pair is O((n/\log{n})^{2/3}) and \Omega(\log{n}/\log\log{n}) [CLL+05, BHH+20]. Note that these results relate the size of the obtained grammar to that of the minimal grammar, where the latter can be of a more general kind than what Re-Pair (or BPE) can produce. Navarro and Russo [NR08] show a different kind of theoretical guarantee for Re-Pair, namely that its cost approximates the order-t entropy of the input, for a certain range of t and alphabet size. Furuya et al. [FTN+19] bound the gap between runs of Re-Pair with different tie-breaking. These results use a different cost measure (compressed length versus reduction, which will be discussed in deail later) and are not directly comparable to ours; nevertheless, a construction from [FTN+19] is also useful in our context, as shown below. Closest to our work is the recent paper of Zouhar et al. [ZMG+23], which initiated the formal study of BPE and optimal pair encoding that we also largely follow in this paper, apart from small notational differences and a more general problem formulation. Using the theory of submodular functions, they relate the approximation ratio of BPE to a certain parameter (total backward curvature) that depends on the unknown optimum. Zouhar et al. also observe an empirical bound on this quantity, however, without giving any worst-case guarantees. Problem definition. We consider strings (sequences of symbols) over some alphabet \Sigma and denote concatenation of strings a and b by a\cdot b, omitting the \cdot when clear from context. We denote the length of a string s by |s|, the i-th symbol of s by s[i], and the substring s[i]\cdot s[{i+1}]\cdots s[j] by s[i:j]. We thus have s=s[{1:|s|}]. A replacement rule is a function \operatorname{\mathsf{replace}}_{x\rightarrow y} that transforms a string s by replacing all occurrences of the string x in s with the string y. Formally, \operatorname{\mathsf{replace}}_{x\rightarrow y}(s)=s if s does not contain x, and otherwise \operatorname{\mathsf{replace}}_{x\rightarrow y}(s)=s[{1:i}]\cdot y\cdot% \operatorname{\mathsf{replace}}_{x\rightarrow y}(s[{i+|x|+1:|s|}]), where i is the smallest index for which s[{i+1:i+|x|}]=x. A sequence of replacement rules \mathscr{R}=(\mathscr{R}_{1},\dots,\mathscr{R}_{k}) with \mathscr{R}_{i}=\operatorname{\mathsf{replace}}_{a_{i}b_{i}\rightarrow c_{i}}, where a_{i},b_{i},c_{i} are symbols, is called a merge sequence of length k. Denoting s^{\prime}=\left(\mathscr{R}_{k}\circ\cdots\circ\mathscr{R}_{1}\right)(s), where \circ is the function composition, we refer to |s^{\prime}| as the compressed length, and |s|-|s^{\prime}| as the utility of \mathscr{R} for s. In words, s^{\prime} is obtained from s by applying the sequence of replacement rules \mathscr{R}_{1},\dots,\mathscr{R}_{k}. We refer to the i-th step, i.e., the application of \mathscr{R}_{i} as the i-th merge. We sometimes use the term full merge to emphasize that no copies of a_{i}b_{i} remain after the operation. Given the resulting encoded (compressed) string s^{\prime}, we can recover s by applying the sequence of reverse transformations \mathscr{R}^{\prime}=(\mathscr{R}^{\prime}_{k},\dots,\mathscr{R}^{\prime}_{1}) to s^{\prime}, with \mathscr{R}^{\prime}_{i}=\operatorname{\mathsf{replace}}_{c_{i}\rightarrow a_{% i}b_{i}}. Notice that the symbols c_{i}, for all i\in[k], can be assumed w.l.o.g., to be new, i.e., not appearing in s or in \left(\mathscr{R}_{j}\circ\cdots\circ\mathscr{R}_{1}\right)(s) for j<i. Indeed, if c_{i} already appears in the string, then the replacement a_{i}b_{i}\rightarrow c_{i} may not be unambiguously reversible. We can now formulate our main optimization problems. Given a string s and an integer k>0, find a merge sequence \mathscr{R} of length k, of maximal utility for s (or equivalently, of minimal compressed length). We denote this optimal utility as \operatorname{\mathsf{OPT}}^{m}(s,k), and call the task of computing it the optimal merge sequence (OMS) problem.222Apart from slightly different notation that is more convenient for our arguments, the OMS problem is identical to the problem defined by Zouhar et al. [ZMG+23]. Note that maximizing compression utility and minimizing compressed length are equivalent for exact computation, but not necessarily for approximability. We also define a more general optimization problem where we do not require to replace every occurrence of a pair. Formally, a partial replacement rule \mathscr{R}_{i}^{*}=\operatorname{\mathsf{replace}}^{*}_{a_{i}b_{i}\rightarrow c% _{i}} can be any function that satisfies \operatorname{\mathsf{replace}}_{c_{i}\rightarrow a_{i}b_{i}}(\operatorname{% \mathsf{replace}}^{*}_{a_{i}b_{i}\rightarrow c_{i}}(s))=s for all s. In words, \mathscr{R}_{i}^{*} replaces some occurrences of a_{i}b_{i} with c_{i}. A sequence of partial replacement functions \mathscr{R}^{*}=(\mathscr{R}_{1}^{*},\dots,\mathscr{R}_{k}^{*}) is a partial merge sequence. Denoting s^{\prime}=\left(\mathscr{R}_{k}^{*}\circ\cdots\circ\mathscr{R}_{1}^{*}\right)% (s), we define utility and compressed length of \mathscr{R}^{*} analogously to merge sequences. Notice that s can be recovered from s^{\prime} identically to the case of merge sequences. The optimal pair encoding (OPE) problem asks, given a string s and an integer k>0, to find a partial merge sequence \mathscr{R}^{*} of length k, of maximal utility for s. We denote this optimal utility as \operatorname{\mathsf{OPT}}(s,k). While the OMS problem is perhaps more natural, OPE is more general, and as shown in Figure 2, it can indeed be stronger (i.e., it is sometimes worth not merging every occurrence of a pair). Most of our results in this paper apply to both problems. \setcapindent 0em \displaystyle\mathtt{abcd\,|\,bc\,|\,bcda\,|\,cd\,|\,cdab\,|\,da\,|\,dabc\,|\,ab} \displaystyle\mathtt{~{}\rightarrow~{}XY\,|\,Z\,|\,ZT\,|\,Y\,|\,YX\,|\,T\,|\,% TZ\,|\,X} \displaystyle\mathtt{abcd\,|\,bc\,|\,bcda\,|\,cd\,|\,cdab\,|\,da\,|\,dabc\,|\,ab} \displaystyle\mathtt{~{}\rightarrow~{}XZ\,|\,Y\,|\,YT\,|\,Z\,|\,ZX\,|\,T\,|\,% dXc\,|\,X} Figure 2: Input s=\mathtt{abcd\,|\,bc\,|\,bcda\,|\,cd\,|\,cdab\,|\,da\,|\,dabc\,|\,ab}, where | denotes a distinct symbol for each occurrence. An optimal OPE encoding of s (above) with utility \operatorname{\mathsf{OPT}}(s,4)=12. An optimal OMS encoding of s (below) with utility \operatorname{\mathsf{OPT}}^{m}(s,4)=11. The OMS solution is obtained via the merge sequence (\mathtt{ab\rightarrow X,~{}bc\rightarrow Y,~{}cd\rightarrow Z,~{}da% \rightarrow T}). Byte-pair encoding (BPE). BPE solves both the OPE and OMS problem as follows. Starting with the input string s, it performs k locally optimal full merge steps, always choosing a pair whose replacement maximizes compression utility. Formally, for input (s,k), we output \mathscr{R}=(\mathscr{R}_{1},\dots,\mathscr{R}_{k}), where \mathscr{R}_{i}=\operatorname{\mathsf{replace}}_{a_{i}b_{i}\rightarrow c_{i}}. Denoting s^{(0)}=s, and s^{(i)}=\mathscr{R}_{i}(s^{(i-1)}) for i\in[k], each c_{i} is a new symbol, i.e., not occurring in s^{(j)} with j<i, and for i=1,\dots,k, the pair a_{i}b_{i} is chosen so that |\mathscr{R}_{i}(s^{(i-1)})| is minimal. With careful data structuring, identifying a_{i}b_{i} and performing \mathscr{R}_{i} can be done in linear total time over all k merge steps, e.g., see [SKF+99]. In this paper, we ignore such implementation details and focus on the total utility of BPE, i.e., |s|-|s^{\prime}|, where s^{\prime}=s^{(k)}. We denote this quantity as \operatorname{\mathsf{BPE}}(s,k). Note that clearly \operatorname{\mathsf{BPE}}(s,k)\leq\operatorname{\mathsf{OPT}}^{m}(s,k)\leq% \operatorname{\mathsf{OPT}}(s,k). We remark that a number of choices allow for small variation in the definition of BPE (and partly of \operatorname{\mathsf{OPT}}^{m}): (1) when choosing a pair to replace, in case of a tie in utility, we pick the pair that appears first; (2) the utility of a chosen pair equals its number of occurrences, except for the case of overlapping pairs (e.g., the pair \mathtt{aa} appears twice in \mathtt{aaa}, but only one of its copies can be replaced) – one could also decide based on the number of occurrences; and (3) in case of such overlapping pairs, we do the replacements left-to-right, e.g., \mathtt{aaa\rightarrow Xa}, whereas \mathtt{aaa\rightarrow aX} would also be a valid choice. Overall, the effect of these design decisions appears negligible. Our results hold regardless of the tie-breaking strategy for (1). As for (2) and (3), the implementation we chose appears better motivated than the alternatives, but our results can easily be adapted to the other variants; see also [LM00, ZMG+23] for discussion. Our results. As defined, OPE and OMS are natural string compression problems (maximizing compression utility), and BPE is a straightforward greedy heuristic for both. Surprisingly, no worst-case guarantee is known for BPE or for any other algorithm solving OPE or OMS. Zouhar et al. [ZMG+23] formulated the OMS problem (under slightly different terminology), and raised the question whether its exact decision problem is NP-hard. Our first result, shown in § 2 answers this question in a stronger form. We show that both OMS and OPE are in fact APX-complete, ruling out the existence of a polynomial time approximation scheme (PTAS), unless P=NP. Theorem 1.1. OPE and OMS are APX-complete. The fact that the number k of merge-steps is part of the input is crucial; for fixed values of k a polynomial-time exact algorithm is easy to derive. The APX-hardness also holds for the problem of minimizing compressed length, as well as for some other variants of the problem, as discussed in § 2. As for BPE, we analyze its approximation ratio for compression utility, showing in § 3: Theorem 1.2. BPE approximates OPE with a ratio of \upalpha, where 0.333<\upalpha\leq 0.625. Closing this gap is an intriguing open question. Note that the result also implies an approximation of OMS with the same or better ratio, as well as that \operatorname{\mathsf{OPT}} and \operatorname{\mathsf{OPT}}^{m} are within a constant factor of each other. Unlike the hardness result, this guarantee does not transfer to the dual optimization problem of minimizing compressed length. In particular, we show in § 4 that BPE cannot achieve a constant approximation for this measure. Theorem 1.3. The approximation ratio of BPE for compression length in OPE or OMS is \Omega(n). While our main focus is on the BPE algorithm, we find the OPE optimization problem of independent interest. We give in § 5 a simple algorithm we call EvenOdd, that achieves an approximation ratio of 0.5. We stress that despite this guarantee, on most inputs BPE likely behaves better than EvenOdd, which should be seen as a proof of concept. Theorem 1.4. EvenOdd is a 0.5-approximation for OPE. The following four sections are dedicated to the proofs of Theorems 1.1–1.4. In § 6 we conclude with a list of open questions."
https://arxiv.org/html/2411.08599v1,XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL,"To tackle the challenges of large language model performance in natural language to SQL tasks, we introduce XiYan-SQL, an innovative framework that employs a multi-generator ensemble strategy to improve candidate generation. We introduce M-Schema, a semi-structured schema representation method designed to enhance the understanding of database structures. To enhance the quality and diversity of generated candidate SQL queries, XiYan-SQL integrates the significant potential of in-context learning (ICL) with the precise control of supervised fine-tuning. On one hand, we propose a series of training strategies to fine-tune models to generate high-quality candidates with diverse preferences. On the other hand, we implement the ICL approach with an example selection method based on named entity recognition to prevent overemphasis on entities. The refiner optimizes each candidate by correcting logical or syntactical errors. To address the challenge of identifying the best candidate, we fine-tune a selection model to distinguish nuances of candidate SQL queries. The experimental results on multiple dialect datasets demonstrate the robustness of XiYan-SQL in addressing challenges across different scenarios. Overall, our proposed XiYan-SQL achieves the state-of-the-art execution accuracy of 89.65% on the Spider test set, 69.86% on SQL-Eval, 41.20% on NL2GQL, and a competitive score of 72.23% on the Bird development benchmark. The proposed framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods.Keywords LLM, Text-to-SQL, NL2SQL","The ability to convert natural language queries into structured query language (SQL) through natural language to SQL (NL2SQL) technology represents a significant advancement in making complex datasets more accessible. It greatly facilitates both non-expert and advanced users in extracting valuable insights from extensive data repositories [2, 15, 24, 27, 6, 10, 13, 29, 20, 19, 23, 22]. Recent advancements in large language models (LLMs) have significantly enhanced the efficacy and accuracy of NL2SQL applications. There are generally two approaches for NL2SQL solutions based on LLMs: prompt engineering [3, 5, 17, 18], and supervised fine-tuning (SFT) [9]. Prompt engineering leverages the intrinsic capabilities of the model by optimizing prompts to generate diverse SQL queries. Prompt engineering has demonstrated promising results in NL2SQL using zero-shot [3] or few-shot prompting [28, 5, 18]. This type of approach typically employs closed-source models with enormous parameters, such as GPT-4 [1] and Gemini 1.5 [26], which present significant potential and powerful generalization capability. However, most methods rely on multi-path generation and selecting the best option utilizing self-consistency, resulting in significant inference overheads. Approaches based on SFT seek to fine-tune models with much smaller parameter sizes on the NL2SQL task to produce more controllable SQL queries, such as CodeS [9]. Nevertheless, due to their limited parameters, these methods struggle to perform complex NL2SQL reasoning and transfer to databases within a new domain. In this technical report, we propose XiYan-SQL, a novel NL2SQL framework that employs a multi-generator ensemble strategy to enhance candidate generation. XiYan-SQL combines prompt engineering and the SFT method to generate candidate SQL queries with high quality and diversity. To enhance high quality, we take advantage of the high controllability of SFT and utilize a range of training strategies to specifically fine-tune models to generate candidates with different preferences. We introduce a two-stage multi-task training approach, which first activates the model’s fundamental SQL generation capabilities, and subsequently transitions to a model with enhanced semantic understanding and diverse stylistic preferences. To enhance diversity of generated candidates and capability of generating complex SQL queries, we utilize in-context learning to prompt LLMs. We propose to extract the skeleton of the questions by masking the named entities with common special tokens and using skeleton similarity to select and organize useful examples. Then, each generator is followed by a refiner to correct logical or syntactical error based on execution results or error information. Finally, a selection agent is required to select the best option. Most existing works use self-consistency, but the most consistent result is not always the correct case. So we propose to fine-tune a model to understand and identify the subtle differences among candidates and pick the final response. Additionally, to enhance LLMs for better understanding of the database schema, we propose a new schema representation method named M-Schema. Inspired by MAC-SQL Schema [28], M-Schema presents the hierarchical structure between databases, tables, and columns in a semi-structured form. We revised MAC-SQL Schema by adding data types and resulting in a more compact and clear format. We conduct experiments to compare the impact of different schema representations on NL2SQL performance. In comparison to DDL Schema and MAC-SQL Schema, LLMs using M-Schema demonstrate superior performance. We present comprehensive evaluations on both relational and non-relational databases, specifically focusing on prominent systems such as SQLite, PostgreSQL, and nGQL. XiYan-SQL demonstrates remarkable performance across a range of benchmarks, achieving the state-of-the-art performance on the Spider [32], SQL-Eval, and NL2GQL [33] datasets with 89.65%, 69.86%, and 41.20% execution accuracy, respectively. In the context of the more challenging Bird [10] benchmark, XiYan-SQL also achieves a competitive score of 72.23%. The impressive results achieved on various challenging NL2SQL benchmarks not only validate the effectiveness of our approach but also demonstrate its significant potential for broader applications in NL2SQL translation tasks. XiYan-SQL can be accessed from https://bailian.console.aliyun.com/xiyan. We also release the source code for connecting to the database and building M-Schema at https://github.com/XGenerationLab/M-Schema."
https://arxiv.org/html/2411.08275v1,"A Large-Scale Study of Relevance Assessments with
Large Language Models: An Initial Look","The application of large language models to provide relevance assessments presents exciting opportunities to advance information retrieval, natural language processing, and beyond, but to date many unknowns remain. This paper reports on the results of a large-scale evaluation (the TREC 2024 RAG Track) where four different relevance assessment approaches were deployed in situ: the “standard” fully manual process that NIST has implemented for decades and three different alternatives that take advantage of LLMs to different extents using the open-source UMBRELA tool. This setup allows us to correlate system rankings induced by the different approaches to characterize tradeoffs between cost and quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system rankings induced by automatically generated relevance assessments from UMBRELA correlate highly with those induced by fully manual assessments across a diverse set of 77 runs from 19 teams. Our results suggest that automatically generated UMBRELA judgments can replace fully manual judgments to accurately capture run-level effectiveness. Surprisingly, we find that LLM assistance does not appear to increase correlation with fully manual assessments, suggesting that costs associated with human-in-the-loop processes do not bring obvious tangible benefits. Overall, human assessors appear to be stricter than UMBRELA in applying relevance criteria. Our work validates the use of LLMs in academic TREC-style evaluations and provides the foundation for future studies.","Relevance assessments are critical for evaluating information access systems, providing guidance for model training in information retrieval (IR), natural language processing (NLP), and beyond. Acquiring relevance assessments from humans, of course, is an expensive proposition, not only in terms of compensation for the assessors, but also overhead in articulating clear guidelines, coordinating large-scale annotation efforts, ensuring consistent quality, etc. The advent of large language models (LLMs) provides an opportunity to automate these assessments, potentially accelerating improvements in retrieval methods (Faggioli et al., 2023)—although see Soboroff (2024) for a contrary position. Most recently, Thomas et al. (2024) revealed that LLMs have been used to provide relevance assessments at Bing since late 2022. While the authors quite clearly assert that LLMs can accurately predict searcher preferences in the context of Bing, they began from the industry perspective and retrospectively “backported” their proposed fully automatic technique to an old TREC collection for validation. In this work, we adopt a complementary strategy, exploring different applications of LLMs to generate relevance assessments in situ, directly deployed in a large-scale retrieval-augmented generation (RAG) evaluation organized by the National Institute of Standards and Technology (NIST) as part of the Text Retrieval Conference (TREC) series. Now in its 33rd year, TREC is widely acknowledged as representing the “gold standard” in academic IR evaluations. We examined three different evaluation approaches that vary in levels of LLM involvement, which are compared against fully manual relevance assessments. Our study used the UMBRELA tool (Upadhyay et al., 2024), which has been previously validated as a successful reproduction of Thomas et al. (2024). Given this setup, we explored the following research questions: RQ1 To what extent can automatically generated relevance assessments from LLMs replace NIST assessors? Specifically, we examined three scenarios involving different levels of LLM involvement: fully automatic, manual post-editing, and manual filtering. RQ2 Different levels of LLM involvement lead to different tradeoffs between cost and quality in relevance assessments. Can we characterize these tradeoffs? RQ3 Are there any quantitative or qualitative differences between human versus LLM judgments? Are these differences affected by different levels of LLM involvement? In this work, we analyzed a diverse set of 77 runs from 19 teams that contributed to the TREC 2024 RAG Track. Our findings can be summarized as follows: • For RQ1, we find that system rankings induced by automatically generated relevance assessments using UMBRELA correlate highly with those induced by manual assessments in terms of nDCG@20, nDCG@100, and Recall@100. • For RQ2, we find that, surprisingly, LLM assistance does not appear to increase correlation with fully manual assessments. Thus, the additional costs associated with hybrid human–LLM solutions do not appear to have obvious tangible benefits. • For RQ3, analyses suggest that assessors apply stricter relevance criteria than UMBRELA. We find cases where the LLM draws inferences that humans would consider unwarranted, and vice versa, but overall, the first case is more common. The contribution of this work is, to our knowledge, the first large-scale study of different in situ approaches to using automatically generated LLM assessments, where system rankings induced by these approaches are correlated against reference rankings generated by fully manual assessments. This represents a first step outside the industry setting to validate methods for automatically generating relevance assessments using LLMs. In this context, our work contributes to a long tradition of meta-evaluation in the information retrieval literature, evaluations that evaluate evaluation methodologies."
https://arxiv.org/html/2411.08165v1,"Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion","The Knowledge Graph Completion (KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these limitations, we propose \text{KGR}^{3}, a context-enriched framework for KGC. \text{KGR}^{3} is composed of three modules. Firstly, the Retrieval module gathers supporting triples from the KG, collects plausible candidate answers from a base embedding model, and retrieves context for each related entity. Then, the Reasoning module employs a large language model to generate potential answers for each query triple. Finally, the Re-ranking module combines candidate answers from the two modules mentioned above, and fine-tunes an LLM to provide the best answer. Extensive experiments on widely used datasets demonstrate that \text{KGR}^{3} consistently improves various KGC methods. Specifically, the best variant of \text{KGR}^{3} achieves absolute Hits@1 improvements of 12.3\% and 5.6\% on the FB15k237 and WN18RR datasets. The code is anonymously available at https://anonymous.4open.science/r/RRR-KGC-CBA3/.","Knowledge Graphs (KGs) are graph-structured knowledge bases (KBs) that organize factual knowledge as triples in the form of (head entity, relation, tail entity). Recently, KGs have become a crucial foundation for various downstream applications, such as recommendation systems Wang et al. (2019), question answering Sun et al. (2024), and sentiment analysis Wang and Shu (2023). Nevertheless, mainstream KGs such as Freebase Bollacker et al. (2008) and Wordnet Miller (1995) suffer from serious incomplete issues. This problem highlights the importance of the Knowledge Graph Completion (KGC) task, which aims to predict the missing entity from an incomplete triple. Figure 1: Limitations of existing embedding-based (top) and LLM-based (bottom) KGC methods. Existing KGC methods can be roughly categorized into embedding-based methods Bordes et al. (2013); Yang et al. (2015); Sun et al. (2019); Cao et al. (2022) and text-based methods Yao et al. (2019); Wang et al. (2021, 2022). Embedding-based methods implicitly learn rules based on relation patterns observed in triples and make predictions based on the likelihood of these patterns occurring Wu et al. (2023). For example, from triple (X,\textit{works in},Y) and (Y,\textit{city of},Z), it is very likely to deduce that (X,\textit{citizen of},Z). However, these methods ignore the contextual semantics supporting these triples, leading to conclusions that do not align with the facts. Text-based methods employ pre-trained language models (PLMs) to embed entities and relations with their labels and descriptions. However, these methods still cannot surpass the latest embedding-based counterparts Ge et al. (2023) due to the substantial semantic gap between structural KG triples and natural language sentences. Large language models (LLMs), trained by extensive corpora, demonstrate emergent semantic understanding and in-context learning (ICL) capabilities. Recent studies Wei et al. (2023); Liu et al. (2024) have proposed utilizing LLMs for the KGC task, as these models harbor general knowledge that can be leveraged to mitigate information scarcity for long-tail entities. However, the application of LLMs in KGC tasks encounters several limitations. Firstly, if the pre-training corpora of the LLMs lack adequate contextual information on specific entities, the LLMs may produce hallucinated or biased responses. Secondly, the structured nature of KG triples limits the ability of LLMs to effectively capture and leverage contextual information from the graph structure. These shortcomings necessitate a strong reliance on a considerable amount of in-context demonstrations Wei et al. (2023) or external structured embeddings Liu et al. (2024), which inevitably limit the performance and generality of existing approaches. Considering the aforementioned challenges, we propose a context-enriched KGC framework named \textbf{KGR}^{\mathbf{3}}, which consists of three modules: Retrieval, Reasoning, and Re-ranking. Given a query triple, the retrieval module gathers semantically relevant supporting triples with the same relation and similar entities, and extracts plausible candidate answers from a base KGC model. To ensure that the LLM attains a fundamental understanding of the retrieved information, this module also collects and augments relevant contextual information to entities in supporting triples and the candidate answer list. Then, the reasoning module exploits the semantic understanding capability of pre-trained LLM to suggest several potential answers based on in-context demonstrations and the description of the known entity. Finally, the re-ranking module fine-tunes the LLM to select out the corrupted entity of the training triple from a set of candidate entities, enabling it to process structured knowledge. \textbf{KGR}^{\mathbf{3}} possesses strong plug-and-play capability, making it compatible with all base KGC methods without costly re-training. During inference, the re-ranking module integrates the candidate answers derived from the base KGC model and the reasoning module, and then instructs the LLM to output the entity label that best completes the query triple. We validate the proposed framework on two conventional KGC datasets: FB15k237 and WN18RR. The extensive experiments show that \text{KGR}^{3} significantly and consistently outperforms all baseline methods with different types of base KGC models and backbone LLMs, showing its superiority. Notably, the best variant of \text{KGR}^{3} achieves state-of-the-art performance with absolute Hits@1 improvements of 12.3\% and 5.6\% on the two datasets. Our contributions are summarized as follows: • We propose a novel \text{KGR}^{3} framework for the KGC task, which systematically retrieves relevant supporting contexts, conducts semantic reasoning, and re-ranks candidate answers. • We notice the semantic gap between KG triples and natural language sentences, and seamlessly bridge this gap with entity contexts. • We conduct extensive experiments and ablation studies to evaluate the effectiveness of the \text{KGR}^{3} framework, and discuss the importance of incorporating entity contexts and LLMs."
https://arxiv.org/html/2411.08019v1,Language Models as Causal Effect Generators,"We present a framework for large language model (LLM) based data generation with controllable causal structure. In particular, we define a procedure for turning any language model and any directed acyclic graph (DAG) into a sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM is a causal model with user-defined structure and LLM-defined structural equations. We characterize how an SD-SCM allows sampling from observational, interventional, and counterfactual distributions according to the desired causal structure. We then leverage this procedure to propose a new type of benchmark for causal inference methods, generating individual-level counterfactual data without needing to manually specify functional relationships between variables. We create an example benchmark consisting of thousands of datasets, and test a suite of popular estimation methods on these datasets for average, conditional average, and individual treatment effect estimation, both with and without hidden confounding. Apart from generating data, the same procedure also allows us to test for the presence of a causal effect that might be encoded in an LLM. This procedure can underpin auditing LLMs for misinformation, discrimination, or otherwise undesirable behavior. We believe SD-SCMs can serve as a useful tool in any application that would benefit from sequential data with controllable causal structure.","All causal inference methods have to deal with the same fundamental underlying problem: we can only observe one state of the world at a given moment in time. If we wish to reason about what caused what, we — in some way or other — will have to reason about a hypothetical world (or worlds) that we didn’t observe. This issue is often referred to more formally as The Fundamental Problem of Causal Inference [28]: a given individual can only receive one treatment at a time, so we can only ever observe one of their potential outcomes (or counterfactuals). Overcoming this fundamental problem not only makes causal inference a challenge to perform, but also a challenge to evaluate with realistic data. Our main focus — data generation with known causal structure — is thus an important tool both for the development and the evaluation of methods that deal with causality. A well-established strategy for causal effect estimation uses randomization of a treatment to estimate average treatment effects (ATEs) [50], capturing the effect of a treatment on average across many individuals. Because ATEs are feasible to isolate with proper study design, many realistic datasets exist with ‘true’ or at least widely-accepted ATEs [38, 15, 30]. If, instead, inferences about individual treatment effects (ITEs) are desired, making additional modeling assumptions can allow us to reason about counterfactuals at an individual level; unfortunately, so long as counterfactuals remain unobservable, these assumptions remain untestable. This means that there is no easy way to collect realistic data for ITEs.111It is common in literature on individual-level causal inference to draw no distinction between ITEs and conditional average treatment effects (CATEs) [55], but we emphasize these two quantities as distinct: the CATE is the conditional expectation of the ITE, which typically will not explain all ITE variation [39]. In particular, the counterfactual outcome \tilde{y} and/or treatment assignment \tilde{t} must always be manually generated, even if covariates \tilde{x} are based on real data (see, e.g., [42]). Our method is aimed at using LLMs to provide this type of data in a flexible manner. (a) (b) Figure 1: (a) A sequence-driven structural causal model (SD-SCM) uses a language model to sample data according to any desired causal relationships between variables (blue solid arrows represent an example set of possible DAGs). Any variables whose values are sampled from the language model will potentially share the language model as a common cause (red dashed arrows), unless sampled manually, e.g., uniformly. (b) However, the effects of red dashed arrows are not necessarily significant. For example, the variable weather is a strong confounder (top) or strong collider (bottom), depending on which DAG is chosen. All structural equations are determined implicitly by whatever has already been encoded in the language model (see Section 3.3 for the full example). Contribution. We define a procedure for turning any language model and any DAG into a sequence-driven structural causal model (SD-SCM). We characterize in Section 3 how an SD-SCM provides access to observational, interventional, and counterfactual distributions over sequential data according to the desired causal structure, where structural equations are defined implicitly via language model (see Figure 1). In Section 4, we use SD-SCMs to create an example benchmark for causal effect estimation and test a suite of popular estimation methods across ATE, CATE, and ITE estimation. Section 5 demonstrates how this same technique can underpin the auditing of language models for (un)desirable causal effects. All benchmark datasets as well as code for SD-SCM-based data generation is available on GitHub.222https://github.com/lbynum/sequence-driven-scms We believe SD-SCMs can be used across many applications as a general tool for imposing causal structure on sequentially generated data."
https://arxiv.org/html/2411.08010v1,ExpressivityArena: Can LLMs Express Information Implicitly?,"While Large Language Models (LLMs) have demonstrated remarkable performance in certain dimensions, their ability to express implicit language cues that human use for effective communication remains unclear. This paper presents ExpressivityArena, a Python library for measuring the implicit communication abilities of LLMs. We provide a comprehensive framework to evaluate expressivity of arbitrary LLMs and explore its practical implications. To this end, we refine the definition and measurements of “expressivity,” and use our framework in a set of small experiments. These experiments test LLMs in creative and logical tasks such as poetry, coding, and emotion-based responses. They are then evaluated by an automated grader, through ExpressivityArena, which we verify to be the most pragmatic for testing expressivity. Building on these experiments, we deepen our understanding of the expressivity of LLMs by assessing their ability to remain expressive in conversations. Our findings indicate that LLMs are capable of generating and understanding expressive content, however, with some limitations. These insights will inform the future development and deployment of expressive LLMs. We provide the code for ExpressivityArena alongside our paper.","Large Language Models (LLMs) OpenAI (2023); Touvron et al. (2023) are disrupting many domains where human communication is essential, including education OpenAI (2023), customer support Radford et al. (2019), legal services Chern et al. (2024), and healthcare Bubeck et al. (2023). Increasing parameter count in LLMs has resulted in better performance in a multitude of downstream tasks such as language translation, text summarizing, and question-answering Devlin et al. (2018); Brown et al. (2020). This performance is typically measured in terms of the number of errors OpenAI (2023), contextual understanding Brown et al. (2020), versatility Bai et al. (2024), problem-solving skills Bubeck et al. (2023), etc. Given that much of human communication is implicit Knepper et al. (2017), expressivity may represent an important aspect of creating “human-like” output in models, improving output quality and user trust in many applications Huang et al. (2024). In order for LLMs to generate text to communicate in a natural way, it is critical that they convey both explicit information and implicit information. In this context, we define expressivity as the implicit communication of information Apresyan (2018). For instance, in a conversation about a movie, explicit information would be “I thought the movie went on far too long” while implicit information may be expressed as “I kept checking my watch during the movie.” The fact remains the same: the speaker thought the movie was too long, but the second statement requires a level of interpretation. Expressivity may come through various metaphors, lexical choices, etc. in daily communication, and may take the form of a different speech act entirely. Aside from emotions, the speaker may also indicate other information about themselves. Word choice, such as slang, may implicitly communicate one’s regional background, level of education, or other identities Green (2016). Figure 1: ExpressivityArena tests LLMs on their ability to implicitly express information. Expressivity is an indispensable facet of human communication, and without it, LLMs will not be able to communicate as naturally, potentially undermining users’ trust in those models. Additionally, the ability to express and control implicit communication—such as certain emotions, tone, or style—in a model’s output opens up new use cases. This capability is particularly useful when maintaining a specific mood, writing style, or conversation flow is important. For instance, it enables LLMs to engage in more fluid and natural conversations by adjusting their responses to match the desired emotional tone or stylistic preferences. This study aims to quantitatively measure the expressivity of the state-of-the-art LLMs. To this end, we focus on the following research questions (RQs): • RQ1: Are LLMs capable of exhibiting expressivity? • RQ2: Can LLMs remain expressive through the course of a conversation? In order to answer these questions, we present ExpressivityArena, a framework to evaluate expressivity of LLMs. First, we set up a grader to objectively evaluate Wimsatt et al. (1946); Benedetto et al. (2013) outputs generated by various LLMs. Since evaluating thousands of LLM-generated outputs is an onerous task for humans, following recent success of using AI evaluators Lee et al. (2023); Bai et al. (2024); Chern et al. (2024), we also use independent LLMs as evaluators. Unlike human whose performance drift over time due to fatigue Boksem et al. (2005) when reading long texts, machine learning models maintain consistency of evaluation. We first established the validity of the grader with a human study. We then conducted experiments for tasks with varying degrees of expressivity—poetry generation and code generation—to answer the first question. We found that LLMs have wildly varying degrees of expressivity, and that models tended to be less expressive while generating code than while generating poetry, suggesting that models perform worse in low-expressivity domains. We then tested if models were able to maintain expressivity throughout the course of a simulated conversation, testing the expression of emotions and professions. We found that models became less expressive over time when expressing emotions, but became more expressive over the course of a conversation when expressing professions."
https://arxiv.org/html/2411.07990v1,Derivational Morphology Reveals Analogical Generalization in Large Language Models,"What mechanisms underlie linguistic generalization in large language models (LLMs)? This question has attracted considerable attention, with most studies analyzing the extent to which the language skills of LLMs resemble rules. As of yet, it is not known whether linguistic generalization in LLMs could equally well be explained as the result of analogical processes, which can be formalized as similarity operations on stored exemplars. A key shortcoming of prior research is its focus on linguistic phenomena with a high degree of regularity, for which rule-based and analogical approaches make the same predictions. Here, we instead examine derivational morphology, specifically English adjective nominalization, which displays notable variability. We introduce a new method for investigating linguistic generalization in LLMs: we fit cognitive models that instantiate rule-based and analogical learning to the LLM training data and compare their predictions with those of the LLM, allowing us to draw direct conclusions regarding underlying mechanisms. We compare the performance of GPT-J on a set of nonce adjectives with that of a high-performing rule-based model and a competitive analogical model. As expected, both models explain the predictions of GPT-J equally well for adjective classes with regular nominalization patterns. However, for adjective classes with variable nominalization patterns, the analogical model provides a much better match. Furthermore, GPT-J’s behavior is sensitive to the individual word frequencies, even for regular complex forms — a behavior that is consistent with an analogical account of regular forms but not a rule-based one. These findings refute the hypothesis that GPT-J’s linguistic generalization on adjective nominalization involves rules, suggesting similarity operations on stored exemplars as the underlying mechanism. Finally, we highlight a difference between linguistic generalization in humans and LLMs: while humans generalize based on types, LLMs generalize based on tokens, which we show has detrimental effects on their predictions. Overall, our study suggests that analogical processes play a bigger role in the linguistic generalization of LLMs than previously thought.","Results Table 2: Comparison with cognitive models. The table shows real and nonce examples for the four examined adjective classes, the counts of corresponding derivatives in the Pile as well as the results of rule-based and exemplar-based analogy models evaluated against GPT-J. The evaluation measure is accuracy. We highlight the highest accuracy value (i.e., the best-matching cognitive model) in each row in boldface — for the two adjective classes where there is a winner (i.e., -ive and -ous), this is the token-based GCM model. We highlight accuracy values that are significantly (p<.05) worse than the highest accuracy value in each row with a †. Examples Counts MGL GCM Regularity Suffix Real Nonce -ity -ness Type Token Type Token High -able available tegornable 11,081 1,034 .893 .893 .893 .893 -ish selfish friquish 0 1,502 .997 .997 .997 .997 Low -ive sensitive cormasive 4,508 2,438 †.658 .662 †.622 .688 -ous luminous momogorous 1,372 2,450 †.657 †.613 †.610 .703 Generalization to Nonce Words We compare the linguistic generalization behavior of GPT-J with that of two high-performing cognitive models (Materials and Methods, Cognitive Models): the Minimal Generalization Learner [MGL; 73, 74] and the Generalized Context Model [GCM; 22, 23, 75]. The MGL is a rule-based model that we have selected because it undertakes to capture detailed patterns of variation that earlier, simpler models did not capture. The GCM is an exemplar-based analogy model that was previously applied to variability in inflectional morphology [28, 31]. The comparison will give us some evidence about the underlying mechanism; if, for example, GPT-J agrees consistently with the MGL over the GCM, this would suggest that it uses a similar (i.e., rule-based) mechanism under the hood. The MGL and GCM models can be fit to either word types or word tokens. The inventory of word types corresponds to the list of words in a mental lexicon; only the existence of a word in the language is taken into account, and not its frequency in the training data. In an inventory of word tokens, each occurrence of a word in the training data is treated as a separate instance, with the result that more frequent words have more instances than less frequent words. We consider both settings, because the contrast between behaviors governed by type frequencies and those governed by token frequencies is a major theme in cognitive research on the lexicon. We focus on English adjective nominalization and examine four adjective classes (i.e., sets of adjectives ending in the same suffix), two of which clearly prefer -ity or -ness, and two of which are less regular while still showing an overall tendency towards one of the two suffixes (Table 2). We train the cognitive models on all adjective-derivative pairs that meet the following three criteria: (i) the adjective belongs to one of the four adjective classes in question; (ii) the derivative ends in -ity or -ness; (iii) both the adjective and the derivative occur in the Pile. For evaluation, we use UniPseudo [76] to generate 50 nonce adjectives for each of the four adjective classes (Materials and Methods, Nonce Adjectives). We check that both the generated nonce adjective and the two corresponding derivatives have a frequency of zero in the Pile, i.e., they have never been seen by either the cognitive models or GPT-J, thus providing an ideal test set for probing linguistic generalization. We then feed all nonce adjectives into the cognitive models and determine which of the two competing derivatives they prefer. For GPT-J, we measure the probability that it assigns to the two derivatives resulting from adding -ity and -ness to the adjectives (Materials and Methods, GPT-J). Specifically, we use GPT-J’s autoregressive language modeling head to compute the log probabilities for the tokens into which the derivatives are split by the tokenizer and sum them. We take the derivative with the higher total log probability as the preferred one. Since prior research has shown that varying prompts (i.e., the texts used to elicit LLM responses) can heavily affect LLM behavior [77], we repeat this procedure with 12 different prompts (Supporting Information). If not stated otherwise, the presented results are averaged over prompts. (a) Type-based MGL (b) Token-based MGL (c) Type-based GCM (d) Token-based GCM (e) GPT-J Figure 1: Distribution of preferred nominalization type (specifically, ratio of -ness derivatives) for unseen nonce adjectives, for rule-based models (a, b), exemplar-based models (c, d), and GPT-J (e). Models based on types are shown on the left (a, c), and models based on tokens are shown on the right (b, d). The ratio is computed as the number of -ness predictions divided by the total number of predictions (i.e., -ness and -ity predictions). As shown in Figure 1, both MGL and GCM — in the type-based as well as the token-based setting — make completely consistent predictions for the two adjective classes that strongly prefer one affix. They always predict -ity for -able and -ness for -ish. Thus, both cognitive models reproduce the regular behavior that characterizes these two adjective classes. GPT-J also predicts -ity for -able, and it predicts -ness for -ish in all but two cases for just one of the prompts (turgeishity and prienishity). GPT-J is nearly as successful in capturing the regular cases as MGL and GCM, and these in turn match the predictions of GPT-J equally well (Table 2, upper panel). Thus, the regular adjective classes do not tell us whether GPT-J is more like a rule-base model or an analogical model. Moving to the two adjective classes that show more variability between -ity and -ness (i.e., -ive and -ous), both MGL and GCM generate variable outcomes with a higher rate of -ness for -ous than for -ity. However, the predictions differ substantially in detail: the cognitive models (in the type-based as well as the token-based setting) agree in only 54% of the adjective types (not shown in the figure). Crucially, the cognitive model that matches the predictions of GPT-J on these two adjectives classes best is the token-based GCM model (Table 2, lower panel). As a concrete example, we consider the nonce adjective pepulative. The MGL models map pepulative to a rule that prescribes -ity following -tive, which in the type-based as well as the token-based setting has the highest confidence of all competing rules and is hence selected by both MGL models. The GCM models, by contrast, are more strongly influenced by local similarity effects. While overall there are a larger number of -ity derivatives in the neighborhood of pepulative (e.g., for adjectives ending in -lative there are 88 derivatives with -ity vs. 27 with -ness), many of the adjectives particularly close to pepulative have -ness derivatives with a high token frequency (e.g., manipulativeness has a token frequency of 1,544 vs. 26 for manipulativity). This difference is reflected by the GCM models, where the type-based model predicts -ity, but the token-based model predicts -ness. GPT-J, on the other hand, prefers -ness for this example and hence matches the behavior of the token-based GCM model. Our results show that the generalization behavior of LLMs on linguistic phenomena with a high degree of variability is best explained as a result of analogical mechanisms. This new finding is in line with the observation that LLMs store a considerable amount of their training data in their model weights [33, 34, 35, 19], and it further suggests that these stored data actively contribute to the language skills displayed by LLMs. Our results are consistent with a model that generalizes all adjective nominalizations by analogy; they eliminate the possibility that all nominalizations are generated by rules. However, there remains the possibility that LLMs effectively use analogies in cases of variation and apply rules for adjective classes with a high degree of regularity. This possibility is suggested by earlier theories of inflectional morphology, proposing dual-mechanism models in which regular plurals and past tenses are created by rules, while irregular forms involve analogies [36, 37, 38]. To address this possibility, it is necessary to look into frequency effects for individual words, as discussed in prior work [78, 79]. We will do so in the next sections. Predictions for Seen Words According to cognitive theories, analogies are based on remembered examples. If the mechanism underlying GPT-J’s behavior is analogical, it must implicitly remember a large number of examples. As the first step in evaluating this inference, we ask how well GPT-J’s behavior matches the statistics of its training data. Accurately matching the training data, derivative by derivative, would imply that the distributed representations in GPT-J encode information about individual derivatives. We extend the four adjective classes examined so far and include six other adjective classes that can be nominalized with either -ity or -ness: -al, -ar, -ed, -ic, -ing, and -less. We can divide the ten adjective classes into four groups with similar degrees of competition between -ity and -ness (see Supporting Information for details): • -ed, -ing, -ish, -less (r-ness): this group exhibits the highest degree of regularity and almost always takes -ness. • -able, -al, -ar, -ic (r-ity): this group also exhibits a high degree of regularity (although somewhat lower than in the case of r-ness), with a strong tendency toward -ity. • -ous (v-ness): this adjective class exhibits a high degree of variability, with a slight tendency toward -ness. • -ive (v-ity): this adjective class also exhibits a high degree of variability, with a slight tendency toward -ity. We ask whether GPT-J treats adjectives from these four groups differently, and whether differences between the more regular and more variable ones correspond to differences in the training data. We draw upon the Pile and extract all derivatives ending in -ity and -ness whose bases belong to one of the 10 adjective classes. To decrease noise, we only extract derivatives whose bases also occur in the Pile and apply several filtering heuristics, such as excluding words with non-alphabetic characters (see Supporting Information). To include all productively formed derivatives, we do not impose a frequency threshold on the derivatives. (a) The Pile (b) GPT-J Figure 2: Ratio of bases preferring -ness in the Pile (a) and GPT-J’s predictions with one example prompt (b). Results are similar for the other prompts. The suffixes of the base (i.e., adjective classes) are grouped by degree of competition between -ity and -ness. The overall setup of probing GPT-J is identical to the comparison with the cognitive models: we measure the probability that GPT-J assigns to the two derivatives resulting from adding -ity and -ness to the adjectives, using the same set of prompts. Following this procedure, we evaluate GPT-J on all 48,995 bases from the Pile. If not stated otherwise, results are again averaged across prompts. Figure 2 compares, for each adjective class, the ratio of bases for which GPT-J prefers -ness compared to -ity with the statistics from the Pile. We find that the two distributions are very similar: almost no competition for the bases in r-ness (i.e., -ed, -ing, -ish, -less), little competition for the bases in r-ity (i.e., -able, -al, -ar, -ic), and strong competition for v-ness (i.e., -ous) and v-ity (i.e., -ive). The tendency towards -ity and -ness is also exactly as predicted based on the training data — the average correlation between the class-level -ity/-ness ratios in the training data (Figure 2(a)) and GPT-J predictions (Figure 2(b)) is 0.995 (\pm0.004; p< 0.001 for all prompts), measured using Pearson’s r. For multiple comparisons, p-values are corrected using the Holm-Bonferroni method [80]. Table 3: Match between preferred derivatives in the training data and derivatives preferred by GPT-J. Adjective Class Suffix Accuracy r-ness -ed .986\pm.007 -ing .989\pm.014 -ish .995\pm.004 -less .999\pm.001 r-ity -able .896\pm.082 -al .884\pm.073 -ar .896\pm.060 -ic .867\pm.090 v-ness -ous .788\pm.038 v-ity -ive .842\pm.012 Furthermore, GPT-J matches the training data statistics even on the level of individual bases: across all bases, the accuracy of GPT-J’s preference for one of the two derivatives compared against the training data (considered here as the ground truth) is 89.5% (\pm4.8%); the derivative preferred by GPT-J is generally the derivative that is more likely in the training data. Table 3 shows that there is variation between individual adjective classes, with bases in r-ness (-ed, -ing, -ish, -less) having above 95% accuracy, bases in r-ity (-able, -al, -ar, -ic) having above 85% accuracy, and bases in v-ity (-ive) and v-ness (-ous) having below 85% accuracy, but the general level of agreement is very high. Thus, GPT-J’s morphological preferences closely mirror the statistics of the data it was trained on. The fact that GPT-J very consistently prefers the derivative with the higher frequency in the training data, even in cases such as adjectives ending in -ive where the suffix alone is a bad predictor of -ity vs. -ness, suggests that it stores many derivatives in its model weights. This is again in line with an analogical mechanism. However, it is still possible that some of the high-regularity adjective classes (e.g., -ish) are handled by a rule, as suggested by dual-mechanism approaches. Next, we will disentangle these two hypotheses. Frequency Effects and Analogical Pressure To test whether at least part of GPT-J’s behavior on adjective nominalization can be explained by rules, we analyze the extent (on a log probability scale) to which GPT-J prefers the observed nominalized form over the alternative, non-observed nominalized form. We consider only cases for which just one outcome of nominalization is attested in the Pile. The score for the unattested alternative thus represents the latent competition from the new form that might be created by rule or analogy. The difference between the two scores can be viewed as reflecting GPT-J’s confidence in its decision to use a form that it has encountered during training. A large difference indicates high confidence, while a small difference reflects low confidence. For each adjective class, we create two sets: one in which the attested derivative has a low frequency in the Pile, f\in(0,10], and one in which the attested derivative has a high frequency in the Pile, f\in(100,\infty). If an adjective class is handled by a rule, the difference in frequency between the two sets should not affect GPT-J’s confidence to predict the attested derivative. This is because rule-based theories abstract away from individual words; once a rule has been acquired, regular complex forms are assumed to be generated on the fly, much like complex sentence structures are, rather than being stored in memory. Rule-based theories predict that only memorized exceptions to rules will exhibit frequency effects. Many researchers might suggest the following default rule for nominalization (taking NOM to be the underlying morpheme that may be spelled out as -ness or -ity): \text{NOM}\rightarrow\text{{-ness}} (1) Under this assumption, all forms in -ity would be memorized exceptions. Statistical theories of rule-formation, such as MGL, would induce the following narrower rule for r-ness, which has extremely strong statistical support (see Supporting Information for details): \text{NOM}\rightarrow\text{{-ness}}\>/\>\left\{\begin{aligned} &\text{{-ed}}\\ &\text{{-ing}}\\ &\text{{-ish}}\\ &\text{{-less}}\end{aligned}\right\}\>\underline{\hskip 11.38092pt} (2) The following subregularity for r-ity is also a strong candidate for status as a minor rule [81], given the low rate of exceptions if the structural description is met. \text{NOM}\rightarrow\text{{-ity}}\>/\>\left\{\begin{aligned} &\text{{-able}}% \\ &\text{{-al}}\\ &\text{{-ar}}\\ &\text{{-ic}}\end{aligned}\right\}\>\underline{\hskip 11.38092pt} (3) Empirically, the MGL model trained on the Pile contains versions of all of these rules. With respect to GPT-J, we have already seen that the most regular outcomes can be generated by analogy, but could they instead be generated by rule? We can address this question by probing whether the cases that fall under rules 1, 2, or 3 exhibit word frequency effects on GPT-J’s confidence in its decision. If a rule is active for a group of cases, its outputs should be exempt from word frequency effects since the outputs — and hence their individual frequencies — are not stored. Figure 3: Impact of word frequency on GPT-J’s confidence in its choice. x-axis: Log probability difference between the attested and unattested choices for low-frequency derivatives with f\in(0,10]. We have converted the log probabilities from base e to base 10 for better readability. y-axis: Relative increase in confidence for high-frequency derivatives with f\in(100,\infty). Each dot corresponds to GPT-J’s predictions for an adjective class given a specific prompt. Dots are colored by degree of competition between -ity and -ness. We added LOWESS lines for r-ness and r-ity. Dots at y=0\% indicate the expected behavior if r-ness and r-ity were handled by rule. Figure 3 shows GPT-J’s confidence (i.e., the natural log probability difference between the attested and the unattested derivative) for low-frequency derivatives with f\in(0,10] vs. the relative increase in confidence for high-frequency derivatives with f\in(100,\infty) for all adjective classes, divided into the four regularity-based groups. The relative increase in confidence is positive for all adjective classes and for all prompts, indicating that GPT-J is always more confident in its decision for the frequent than the rare derivatives, even for the r-ness class. This indicates that the model has stored distributed representations for all the derivatives, contrary to the predictions of the dual-mechanism model. Put differently, none of the adjective classes are handled by rule. Given that the cognitive models considered above (i.e., MGL and GCM) generate no examples of -ishity (reflecting the non-occurrence of such examples in the training data), it is striking that GPT-J exhibited any uncertainty at all about these cases. Recall, however, that it made actual errors as well (see Generalization to Nonce Words). This outcome can be attributed to the model paying some attention to parts of the word that precede the suffix of the base (e.g., -ish), even though these parts have no linguistic relevance to the form of the nominalization. Note that transformer-based language models such as GPT-J lack the ability to pay zero attention to any part of the input. Overall, the examined adjective classes exhibit a downward slope in Figure 3, which is also reflected by the r-ness and r-ity groups individually (indicated by trendlines). Thus, the more confident the model was in its decisions for the low-frequency group, the smaller the effect of word frequency on confidence. This finding is difficult to explain under the assumption of rules, but we will show that it is perfectly in line with analogy as the underlying generalization mechanism. More specifically, we will attribute the downward slope to variation in the analogical pressure for the different adjective classes. It is well established that frequent words have more robust representations in memory than rare words. This generalization plays a central role in our understanding of how analogy functions in morphology. During language production, analogical processes supply a complex word form if the form has never been encountered during learning. But, they also exert pressure on known forms stored in memory. The better the complex form has been learned as such, the less susceptible it is to these pressures and the more quickly and reliably it is retrieved in its remembered form. This mechanism operating over multiple generations is the one by which frequent forms can retain exceptional inflectional patterns while rare forms are regularized [82, 83]; for example, English say, take, come still retain their exceptional past tense forms (said, took, came) while previously irregular rare verbs, such as writhe, flay, spew do not [84]. A Bayesian model that formalizes analogical pressure, including the frequency effect just described, is developed in [85]. For the most regular r-ness adjective classes, there is little analogical pressure on any specific target form because there are few if any examples that have the suffix followed by -ity. GPT-J has high confidence in its answer, and the weak analogical pressure comes from a few forms that happen to have similarities in the stem. Word frequency, it appears, has a relatively small effect on the confidence when the confidence is already high. Lower confidence levels for the low-frequency forms towards the left of the graph represent cases in which the neighborhood of the target form is more heterogeneous. With increased competition, we observe that the effect of word frequency becomes greater. We quantify this by calculating the Shannon entropy of the distribution over -ity and -ness as the preferred form in the Pile, and using Pearson’s r as a measure of the correlation with the confidence increase for high-frequency derivatives. At r^{2}=0.75, p<0.001, this correlation is highly significant. Thus, the heterogeneity of the exemplar neighborhood for an adjective class predicts the extent to which GPT-J’s confidence relies on frequency — a finding that is exactly in line with the predictions of models assuming analogical pressure [e.g., 85] while being completely at odds with rule-based approaches, which do not assume such frequency effects to begin with. The left side of Figure 3 exhibits more variability than the right side. We believe that this variability is caused by local neighborhood effects and the interaction of these effects with the prompting mechanism. Recall from the discussion of the nonce word pepulative that analogical models are sensitive not merely to the overall statistics for the two competing nominalizations, but also to the similarity and frequency of the most similar neighbors (see Generalization to Nonce Words). These localized effects — which for the case of attested derivatives would also include semantic similarity — create a lumpy prediction landscape whose properties we do not try to quantify here. Meanwhile, the prompting mechanism is known to influence the focus and bias of the underlying transformer model [86]. Slightly different prompts direct the focus towards different parts of the lumpy landscape, and would hence produce noise in the datapoints for Figure 3. To sum up, our analysis suggests that GPT-J learns adjective nominalization by implicitly storing derivatives in its model weights. In cases where the exemplar neighborhood is highly homogeneous, GPT-J produces highly regular outputs, but nonetheless reveals evidence of analogical pressure in its confidence scores. While regular, or rule-like, behavior of LLMs has been observed before [e.g., 18], our results contextualize this finding in important ways, suggesting that rule-like behavior forms the end of a gradient characterized by varying levels of regularity. This result is not consistent with assuming a qualitative difference between forms derived by rule and stored exemplars. It is exactly in line with the predictions of exemplar-based analogy models [e.g., 28, 31, 32]. Human Use of Word Types Versus Tokens We have established that GPT-J relies on token-level analogical generalization. In contrast, previous studies have concluded that humans generalize over word types [87, 88, 89]: their propensity to generalize a word formation pattern depends on the number of distinct word types in the individual’s mental lexicon that support the pattern (referred to as the size of the lexical gang). This points to a difference between the morphological processing in humans and LLMs. We will now investigate this difference in greater detail, by comparing the predictions of GPT-J to judgments made by humans. Judgments of Nonce Words First, we make a direct comparison to GPT-J’s behavior for nonce words. 22 native English speaker volunteer annotators indicated their preference for the -ity versus the -ness derivative of each nonce adjective in our study (Supporting Information). Because GPT-J is not a state-of-the-art model, we also introduce an additional comparison, by asking whether a more recent model is more human-like in its judgments. Specifically, we evaluate GPT-4 [3] on the same set of adjectives (see Materials and Methods, GPT-4 for implementation details). If GPT-4 displays more human-like judgments than GPT-J, then the trend of improving LLMs through larger training sets and bigger model sizes will have paid off in this domain. Table 4: Human evaluation. The table shows the results of rule-based and exemplar-based analogy models as well as GPT-J and GPT-4 evaluated against human annotations. The measure is accuracy. MGL GCM LLMs Suffix Type Token Type Token GPT-J GPT-4 -able 1.000 1.000 1.000 1.000 .893 .960 -ish 1.000 1.000 1.000 1.000 .997 1.000 -ive .720 .680 .760 .700 .632 .440 -ous .560 .520 .640 .520 .503 .400 In Table 4, we take the derivative (with -ity or -ness) more often selected by humans as the ground truth. The table gives the accuracy of GPT-J, GPT-4, as well as the cognitive models considered above (i.e., MGL and GCM), measured against this human response. The type-based GCM model overall matches the human behavior best. While all cognitive models perfectly reproduce the homogeneous behavior for -able and -ish, the type-based GCM model better matches the human predictions for -ive and -ous, as reflected by large gaps compared to the second best model, type-level MGL (-ive: 4%, -ous: 8%). The token-based variants of MGL and GCM match the human behavior substantially worse than the type-based variants, which is exactly in line with what has been suggested in prior work [88, 74]. Moving to the results for GPT-J, it turns out to match the human responses worse than any of the cognitive models, for all four adjective classes. The gap compared to the best cognitive model, type-based GCM, is considerable, especially for -ive and -ous, amounting to roughly 13% in both cases. The picture is overall even worse for GPT-4. While the predictions for the high-regularity classes are good (almost always -ity for -able and -ness for -ish, like all other models), the match with humans is more than 10% worse than GPT-J for both -ive and -ous. Why do GPT-J and GPT-4 match the human behavior so much worse than the much simpler type-based GCM? The key factor, we argue, is that both of these LLMs are driven by the token frequencies of the words in the training data. Just as for GPT-J, the token-based GCM and MGL models match the behavior of GPT-4 better than the type-based models (see Supporting Information). Token-oriented behavior is desirable in that it results in highly realistic implicit knowledge of individual words, as we have seen above. However, humans step back from the frequencies of individual words when making generalizations about possible words. LLMs seem to lack the ability to do this. Furthermore, our results suggest that GPT-4’s over-reliance on token frequency is if anything worse than that of GPT-J. Thus GPT-4’s morphological generalization behavior seems to be even less human-like than that of GPT-J. This finding is reminiscent of recently reported ‘inverse scaling effects’, more specifically the tendency of larger LLMs to rely even more strongly on prior statistics from the training data than smaller models do [90]. Since the performance of the best model, the type-based GCM, leaves room for improvement, we can ask why its performance was not better. The single biggest discrepancy was that the GCM selected -ness after -ous more than the humans did. Our analysis does not deal with the possibility that some people may consider the affix -osity to be a unified affix bundle, along the lines suggested in [91, 92]; this would enhance its availability. Given that the GCM was fit to all word pairs attested in the Pile, the analysis also failed to allow for differences amongst human mental lexicons (whether in the entries themselves, or in the extent to which morphological relations between entries have been adduced). Other studies have found considerable variability amongst human participants in the area of derivational morphology in general, and in preference for -ity over -ness specifically [93, 94, 95, 96, 97]. In this context, it is noteworthy that all participants of our annotation study held at least a college degree (see Supporting Information for details), whereas much of the Pile consists of web data such as informal discussions on Reddit or StackExchange [72]. There is thus the possibility that there was a misalignment between the sociolect most strongly represented in the Pile and the ideolects sampled as part of our annotation study. Finally, the GCM works on the basis of word forms only, and has no way of taking into account similarities in meaning that also play a role in shaping morphological systems. In contrast, LLMs like GPT-J and GPT-4 do have the ability to consider similarities in meaning, but any advantage they may gain from their semantics appears to be more than offset by the drawbacks of their reliance on token frequencies. Familiarity of Complex Words Our results on nominalizations indicate that GPT-J and GPT-4 do not have a mental lexicon in the sense that humans do, in that they lack the ability to step back from word tokens and generalize over word types. Here, we present a brief demonstration that this observation pertains to morphologically complex words more generally, and not just to nominalizations. For this demonstration, we draw on the Hoosier Lexicon, a dataset of 19,320 English words that includes word frequencies and familiarity ratings on a seven-point Likert Scale [98]. An important finding of the original study was a dissociation between word frequency and rated familiarity; one might expect the two to be highly correlated, however some infrequent words are judged as much more familiar than their frequency would suggest. Needle et al. [99] identify morphological structure as an important factor contributing to this dissociation. A word like precancellation, with a recognizable prefix, stem, and affix seems familiar even though it is rare, on the strength of the familiarity of its parts. We analyze the n=2,835 words in the Hoosier lexicon that have a frequency of less than 10,000 in the Pile (corresponding to a frequency of roughly 1 in 50,000,000 words or less). Leveraging the CELEX dictionary [54] and methodology from prior work [100, 101], we use affix-stripping to identify the n=1,005 words that exemplify derivational morphology by virtue of being parsable as a simpler word plus any combination of affixes. n=1,830 words cannot be parsed in this way, and we consider them to be simplex words (see Supporting Information). For human judgments, we take the familiarity ratings reported by Nusbaum et al. [98]. We estimate the ‘familiarity’ that GPT-J assigns to a word as the log probability that it assigns in the context of neutral prompts (see Materials and Methods, Vocabulary Test for details). Comparing log probabilities to human familiarity ratings is justified because the probabilities assigned to words by language models are known to correlate with psycholinguistic measures of lexical access [e.g., reading times; 102], which for humans are impacted by familiarity to a larger extent than frequency [103]. (a) Humans (b) GPT-J Figure 4: Impact of morphological decomposability of words on their familiarity as rated by human annotators (a) and the log probability assigned to them by GPT-J (b). Results for humans are displayed in Figure 4(a). The average familiarity of words with a morphological parse (n=1,005) is significantly higher than the average familiarity of words with no morphological parse (n=1,830), t(2120.2)=19.2, p<.001 by a Welch’s t-test. This confirms the results reported by Needle et al. [99]. Due to this important factor, the correlation between familiarity and log frequency in the entire Hoosier lexicon proves to be modest according to a linear regression, F(1,19318)=11251.2, R^{2}=0.368, p<.001. For GPT-J, on the other hand, words with a morphological parse do not have any advantage (Figure 4(b)); quite the opposite, the estimated familiarity of words with a morphological parse is significantly lower than the estimated familiarity of words with no morphological parse for GPT-J, t(2285.9)=-4.9, p<.001. This outcome can be explained by the fact that the correlation between the log frequencies and the log probabilities assigned to the words by GPT-J is very high, F(1,19318)=58553.5, R^{2}=0.752, p<.001, and the target words without a parse have somewhat higher average frequency (m=4285.1) than those having a parse (m=4093.7). Experimental studies on wordlikeness judgments [99] and on speech perception [104, 105, 106] show that humans continually monitor for known words inside rare or novel words. This means that their type-level lexical representations are exploited during processing, and can cause rare words to seem familiar. GPT-J does not rely on this type-level mechanism and hence lacks the dissociation between frequency and familiarity that is caused by morphological structure."
https://arxiv.org/html/2411.07917v1,CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and Classification of Crypto Posts,"The rapid growth of social media has resulted in an large volume of user-generated content, particularly in niche domains such as cryptocurrency. This task focuses on developing robust classification models to accurately categorize cryptocurrency-related social media posts into predefined classes, including but not limited to objective, positive, negative, etc. Additionally, the task requires participants to identify the most relevant answers from a set of posts in response to specific questions. By leveraging advanced LLMs, this research aims to enhance the understanding and filtering of cryptocurrency discourse, thereby facilitating more informed decision-making in this volatile sector. We have used a prompt-based technique to solve the classification task for reddit posts and twitter posts. Also, we have used 64-shot technique along with prompts on GPT-4-Turbo model to determine whether a answer is relevant to a question or not.","We have seen the emergence of cryptocurrencies has generated substantial interest and discourse across various social media platforms [1]. With millions of users expressing their opinions, sharing information, and speculating on the future of digital assets, the need for effective tools to process and analyze this data has become increasingly critical [2]. However, the diverse nature of social media content, characterized by informal language, varying tones, and mixed sentiments, presents significant challenges for traditional text classification methods [3]. This task is designed to address these challenges by inviting participants to develop machine learning models capable of accurately classifying cryptocurrency-related social media posts into one of eight categories, such as objective, positive, negative, and others. Beyond simple classification, the task also extends to the identification of relevant answers to specific questions from a corpus of posts, a function that mirrors real-world applications where users seek precise information in a sea of data [4]. By focusing on the cryptocurrency domain, this task [5] not only contributes to the broader field of sentiment analysis and text classification but also offers practical implications for stakeholders in the financial sector. Improved classification and information retrieval from social media can lead to better market insights, more effective communication strategies, and enhanced user experiences in digital finance. This study aims to showcase the potential of natural language processing techniques in transforming raw social media data into actionable intelligence in the context of cryptocurrency [6]. In our approach to solving the classification task for Reddit and Twitter posts, we employed a prompt-based technique [7], which has proven to be effective in leveraging the strengths of large language models for text classification. Specifically, we utilized carefully designed prompts to guide the model in understanding the context and nuances of social media posts related to cryptocurrency. This approach enabled us to harness the power of GPT-4-Turbo [8, 9] to classify posts into predefined categories, such as objective, positive, negative, etc. To enhance the model’s ability to determine the relevance of answers to specific questions, we implemented a 64-shot technique [10] in conjunction with our prompt-based method to determine whether a answer is relevant to a question or not. This allowed the model to learn from a variety of examples, thereby improving its accuracy in identifying whether a given answer is pertinent to the corresponding question."
https://arxiv.org/html/2411.07892v1,"Mapping the Podcast Ecosystem with the
Structured Podcast Research Corpus","Podcasts provide highly diverse content to a massive listener base through a unique on-demand modality. However, limited data has prevented large-scale computational analysis of the podcast ecosystem. To fill this gap, we introduce a massive dataset of over 1.1M podcast transcripts that is largely comprehensive of all English language podcasts available through public RSS feeds from May and June of 2020. This data is not limited to text, but rather includes audio features and speaker turns for a subset of 370K episodes, and speaker role inferences and other metadata for all 1.1M episodes. Using this data, we also conduct a foundational investigation into the content, structure, and responsiveness of this ecosystem. Together, our data and analyses open the door to continued computational research of this popular and impactful medium.","Over the past two decades, podcasts have emerged as an important part of the modern media landscape (Aufderheide et al., 2020). With its low barrier to entry, this medium attracts many independent voices, but has also been embraced by professional media and celebrities, collectively supported by billions of dollars in advertising Wirtschafter (2023). However, despite significant evidence highlighting the wide-scale distribution and real-world impact of podcasts (Bottomley, 2015), there is only limited understanding of this medium and even fewer methods for analyzing podcasts, in part due to technical hurdles and limited data availability. In this work, we present the Structured Podcast Research Corpus (SPoRC), the first large-scale dataset of podcasts for NLP and computational social science research, and use it to analyze and document the podcast ecosystem at scale. Access to large-scale text data has been fundamental to enabling new research directions. Past resources for obtaining data from Reddit (Baumgartner et al., 2020), Twitter (Pfeffer et al., 2023), and Amazon (Ni et al., 2019), have all led to multiple concomitant advances in modeling, applications, and understanding of human behavior. Podcasts, however, introduce unique challenges as a source of data. As an audio medium, the text for podcast episodes is inaccessible in its original form; further, unlike structured media from single platforms, podcasts are distributed by multiple sources and varied metadata exist for each episode. Here, we introduce SPoRC to overcome these challenges. SPoRC is a thick sample of 1.1M podcast episodes from May–June 2020, each associated with transcripts, hosting platform, inferred host and guest names, and other metadata. Using a highly-parallelized pipeline for the collection and processing of podcasts, we introduce new data and models for extracting hosts and guests from podcast transcripts, enabling studies of the social network. We further use these identities to link specific turns of dialog to their speakers, where possible. In sum, our paper makes the following three contributions. First, we introduce SPoRC and the computational infrastructure and models needed to create it (§3). Second, we present the first large-scale characterization of the content and structure of the podcast ecosystem; these analyses reveal the topical distribution discussed on podcasts, and the community network structure created by guest co-appearances (§4). Third, we analyze the responsiveness of podcasts, examining the timing and extent of the impact of a major media event (§5), finding varied responses across different segments of the medium. Overall, our results point to new fundamental questions for further research into phenomena such as community identity, information diffusion, and incidental news exposure in podcasts. With the release of SPoRC, these and other lines of inquiry can be quantitatively studied at scale. Our code and data are made available for non-commercial use and can be found at https://github.com/blitt2018/SPoRC_data."
https://arxiv.org/html/2411.07870v2,Trustful LLMs: Customizing and Grounding Text Generation with Knowledge Bases and Dual Decoders,"Although people are impressed by the content generation skills of large language models, the use of LLMs, such as ChatGPT, is limited by the domain grounding of the content. The correctness and groundedness of the generated content need to be based on a verified context, such as results from Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to a customized domain is that the generated responses are often incomplete, or the additions are not verified and may even be hallucinated. Prior studies on hallucination detection have focused on evaluation metrics, which are not easily adaptable to dynamic domains and can be vulnerable to attacks like jail-breaking. In this work, we propose 1) a post-processing algorithm that leverages knowledge triplets in RAG context to correct hallucinations and 2) a dual-decoder model that fuses RAG context to guide the generation process.","Adapting an LLM to a specific domain is challenging for several reasons: 1) Pre-trained LLMs cover general knowledge and cannot access private data (even during fine-tuning) due to privacy, copyright, and policy constraints. 2) The grounding of generated texts can change depending on specific contexts, such as domain or timestamp. Recent studies mostly focus on detecting hallucinations and using multiple LLMs when hallucinations occur. 3) Business logic and structured data, such as databases and private knowledge bases, are required when integrating customized LLMs into production systems and presenting them to customers or users. We offer two methods for correcting hallucinations (beyond merely detecting them Wan et al. (2024); Li et al. (2023a); Ji et al. (2023)): 1) Applying post-processing to generated texts using knowledge triplets, and 2) Proposing guided generation via Dual Decoders. Inspired by common practices like Retrieval-Augmented Generation (RAG) Li et al. (2024), which retrieves relevant grounding context and feeds it to an LLM for text generation, we address hallucinations in generated texts from two aspects: 1) Post-editing based on knowledge graphs extracted from the context, and 2) Infusing guided context that contains important knowledge triplets into a generic LLM. Our proposed methods also provide reasoning and create consistent results from generative LLMs, benefiting from both the generation and extraction capabilities of decoder-only LLMs and the groundedness of RAG via the second decoder on the guidance Le et al. (2020); Wang et al. (2022b). In this work, we elaborate on our real-world commercial application scenario of using LLMs to support customers with Microsoft product inquiries in copilots, where groundedness is key to success. Pre-trained LLMs often lack the relevant knowledge or cannot adapt promptly to changes in the product database updates. Different variants of large language models (LLMs), such as Phi-3.5 Abdin et al. (2024), ChatGPT Mohamadi et al. (2023), LLama-3 Dubey et al. (2024), and Gemma Team (2024), are proficient at producing fluent outputs for diverse user queries. Despite their human-like fluency in generating text across a wide range of prompts, large language models suffer from hallucinations (see examples in Figures 2, 3, 4), where parts or the entirety of the generated text lack faithfulness, factuality, or reasoning, yet are presented with a confident tone Ji et al., 2023. To mitigate and correct hallucinations, we leverage guided text generation. Grounding guidance Socher et al. (2013); Nickel et al. (2011); Lin et al. (2015); Wang et al. (2014); Bordes et al. (2013); Wang et al. (2022a); Grover and Leskovec (2016), such as knowledge graphs (KGs), has been shown to significantly improve the reliability and factuality of LLMs in recent studies, e.g., KELM Agarwal et al. (2020); Lu et al. (2021), SKILL Moiseev et al. (2022), K-DLM Zou et al. (2023), KEPLET Li et al. (2023b), and LUKE-Graph Foolad and Kiani (2023). Knowledge graphs typically consist of factual information represented explicitly in a semi-structured format, generally as [subject entity, relation, object entity] triples, e.g., (Bill Gates, was, the CEO of Microsoft) Han et al. (2019); Gardner et al. (2017). We collect and maintain such knowledge triplets and grounded context offline for RAG. Our contributions are as follows. 1) We correct hallucinations and out-of-domain outputs in generated texts from LLMs by leveraging a graph algorithm and provide reasoning using knowledge triplets extracted from both the guided context and the generated texts. 2) We propose a dual-decoder model that fuses guided context with natural language generation models, in which the decoders share the weights of a pre-trained LLM. 3) The proposed algorithm and model reduce the constraints on the maximum output length, in addition to correcting hallucinations, by returning or generating only outputs related to the prompt and the guided context."
https://arxiv.org/html/2411.07858v1,Verbosity \neq Veracity: Demystify Verbosity Compensation Behavior of Large Language Models,"When unsure about an answer, humans often respond with more words than necessary, hoping that part of the response will be correct. We observe a similar behavior in large language models (LLMs), which we term “Verbosity Compensation” (VC). VC is harmful because it confuses the user understanding, leading to low efficiency, and influences the LLM services by increasing the latency and cost of generating useless tokens. In this paper, we present the first work that defines and analyzes Verbosity Compensation, explores its causes, and proposes a simple mitigating approach. We define Verbosity Compensation as the behavior of generating responses that can be compressed without information loss when prompted to write concisely. Our experiments, conducted on five datasets of knowledge and reasoning-based QA tasks with 14 newly developed LLMs, reveal three conclusions. 1) We reveal a pervasive presence of Verbosity Compensation across all models and all datasets. Notably, GPT-4 exhibits a VC frequency of 50.40%. 2) We reveal the large performance gap between verbose and concise responses, with a notable difference of 27.61% on the Qasper dataset. We also demonstrate that this difference does not naturally diminish as LLM capability increases. Both 1) and 2) highlight the urgent need to mitigate the frequency of VC behavior and disentangle verbosity with veracity. We propose a simple yet effective cascade algorithm that replaces the verbose responses with the other model-generated responses. The results show that our approach effectively alleviates the VC of the Mistral model from 63.81% to 16.16% on the Qasper dataset. 3) We also find that verbose responses exhibit higher uncertainty across all five datasets, suggesting a strong connection between verbosity and model uncertainty. Our dataset and code are available at https://github.com/psunlpgroup/VerbosityLLM.","Throw everything at the wall and see what sticks. — American Proverb “Verbosity compensation” (VC) refers to a situation where someone is using excessively wordy language, attempting to make up for a lack of substance or clear communication by trying to appear more knowledgeable or persuasive, even if they are not adding significant meaning to the message. VC is widely studied in liguistic (Juola, 2008), education (Strunk Jr & White, 2007; Demir, 2019), and psychology (Brookshire & McNeil, 2014). Interestingly, we find that LLMs also exhibit Verbosity Compensation behaviors. Figure 1 shows a motivating example. In the first response, LLM generates a concise answer that is correct with low uncertainty. In the second and third responses, LLM tries to guess the correct answer by repeating the question, and producing ambiguity, leading to a verbose response with low performance and high uncertainty. Figure 1: An illustration of comparison between concise and verbose responses. In the first response, LLM generates a concise answer, while in the second and third responses, LLM performs repeating, and ambiguity, leading to a verbose response with low performance and high uncertainty. In LLMs, the verbosity compensation behaviors are harmful. The reasons are twofold. First, for the users, the responses with verbosity compensation will lead to confusion and inefficiency (Fowler, 1927; Oppenheimer, 2006). Second, for the servers, the verbosity leads to unnecessary higher costs and higher latency. Besides, the performance of the model should not correlated to the length or format of responses because diverse users favor different lengths of responses. If verbose responses attain higher or lower performance than concise responses, verbosity compensation also leads to bias of users of different length preferences. In this paper, we present the first study that defines, analyzes, and mitigates VC behavior. First, we define Verbosity Compensation (VC) as the behavior of generating responses that can be compressed without information loss when prompted to write concisely. To analyze the VC behavior systematically, we propose a benchmark by unifying four existing long-context question-answering datasets, and a reasoning-based language understanding dataset. We benchmark 14 newly proposed LLMs on five datasets and conduct a comprehensive analysis of the results. Next, we demystify the VC behavior in steps. We first obtain a fine-grained understanding of this behavior by manually inspecting the model-generated results and summarizing the pattern of VC by classifying them. Although we find that different models and datasets exhibit diverse distribution over types of VC behavior, we can categorize VC into five distinct types, including repeating questions, enumerating, ambiguity, verbose details, and verbose format. Next, we combine all these verbosity types and produce statistics of the frequency of LLM-generating VC responses, the result reveals a pervasive presence of verbosity compensation (VC) across all models and all datasets. Notably, GPT-4 exhibits a VC frequency of 50.40%. The high-frequency result amplifies the harmful influence of verbose responses. We further analyze the impact of VC on model performance. We define metrics that compute the difference between verbose and concise responses and apply these metrics to models. Our results indicate that verbose responses exhibit significantly different compared to concise ones, with a notable drop of 27.61% on the Qasper dataset, highlighting the urgent need to disentangle verbosity with veracity. Next, we investigate the underlying causes of VC. We use perplexity and Laplacian scores for open and closed-source models to measure the uncertainty of model responses. We find that verbose responses exhibit higher uncertainty across all five datasets, suggesting a strong connection between verbosity and model uncertainty. Last but not least, to mitigate VC in LLMs, we propose a simple yet effective cascade algorithm that replaces verbose responses with responses of larger LLMs. Experiments demonstrate the efficacy of our cascade algorithm through tests on three model combinations: Gemma to Gemini, Mistral to GPT-4, and Llama to Claude. The results show that our approach effectively alleviates the VC of the Mistral model from 63.81% to 16.16% on the Qasper dataset. We also modify the cascade algorithm for model routing and obtain significant improvement over the random selecting baseline. To summarize, our contribution is threefold: • We define the verbosity compensation behavior of LLMs and propose a benchmark for evaluating the verbosity compensation phenomenon. We found LLMs suffer from significant verbosity. • We conduct a comprehensive analysis of the verbosity compensation and connect verbosity compensation behavior to 1) model performance and 2) model uncertainty. • We propose a simple but effective cascade approach to mitigate verbosity compensation in LLMs, and our extensive experiments show it is highly effective."
https://arxiv.org/html/2411.07854v1,: Advancing Neural Text Generation for Portuguese,"Significant advances have been made in natural language processing in recent years. However, our current deep learning approach to language modeling requires substantial resources in terms of data and computation. One of the side effects of this data-hungry paradigm is the current schism between languages, separating those considered high-resource, where most of the development happens and resources are available, and the low-resource ones, which struggle to attain the same level of performance and autonomy. This study aims to introduce a new set of resources to stimulate the future development of neural text generation in Portuguese. In this work, we document the development of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting to 200 billion tokens. Via this corpus, we trained a series of decoder-transformers named Tucano. Our models perform equal or superior to other Portuguese and multilingual language models of similar size in several Portuguese benchmarks. The evaluation of our models also reveals that model performance on many currently available benchmarks used by the Portuguese NLP community has little to no correlation with the scaling of token ingestion during training, highlighting the limitations of such evaluations when it comes to the assessment of Portuguese generative language models. All derivatives of our study are openly released on GitHub111 github.com/Nkluge-correa/Tucano and Hugging Face222 huggingface.co/TucanoBR.","Over almost a decade, the deep learning paradigm has been the de facto mode of operation for many of the sub-fields involved in artificial intelligence research lecun2015deep ; goodfellow2016deep . Natural Language Processing (NLP) is a canonical depiction of the success story of deep learning nadkarni2011natural ; deng2018deep ; otter2020survey , where neural network approaches to machine learning have become the engine that powers many aspects of our current age of intelligent automation, with breakthroughs like word embeddings mikolov2013linguistic ; mikolov2013efficient and the transformer neural architecture vaswani2017attention being at the heart of this revolution. Another aspect of this developmental movement is using the self-supervised learning approach as an intermediate step to many language modeling tasks geiping2023cookbook . In essence, self-supervised learning is a training methodology for machine learning systems, where we leverage the vastness of available unlabeled data at our disposition to create pretraining tasks where labeling can happen on the fly. This results in systems with useful and downstream-applicable representations tied to the domain they were trained on hastie2009overview ; misra2020self ; geiping2023cookbook . This training approach has been responsible for some of the early breakthroughs of the field bengio2000neural ; mikolov2013efficient ; sutskever2014sequence ; bahdanau2014neural , which have now morphed into our standard training recipe for foundation models bommasani2021opportunities . Nonetheless, while the statement ""leverage the vastness of available unlabeled data at our disposition to create pretraining tasks"" can be true for languages like English or Chinese, where datasets can reach the 10^{13} tokens mark dolma ; young2024yi ; chen2023chinesewebtext ; penedo2024finewebdatasetsdecantingweb , and models are trained way past what scaling laws prescribe as compute-optimal zhang2024tinyllama ; parmar2024nemotron ; xue2024openmoe , the same cannot be said about the crushing majority of more than 7000 languages spoken around the world today rodrigues2023advancing ; lopes2024gl ; cohere2024gap . Hence, the prospect of training language models at the scale required to match what is done in such high-resource languages (even when compared to the state-of-the-art from 5 years ago)333Models like GPT-3 (2020) brown2020language were trained only on 300B tokens, which is still something not reproduced in a monolingual setting for most low-resource languages. is a far-fetched goal for most low-resource languages gandhe2014neural ; adams2017cross ; cruz2019evaluating ; muennighoff2023scaling ; correa2024teenytinyllama . To overcome this linguistic shortcoming, one of the approaches found in the literature is the development of multilingual models and datasets singh2024ayadatasetopenaccesscollection ; ustun2024ayamodelinstructionfinetuned ; aryabumi2024aya ; srivastava2024lolaopensourcemassively . In these models, the self-supervised pretraining stage is conducted with various languages. Models like mBERT devlin2018bert , mT5 xue2020mt5 , XLM-RoBERTa conneau2020unsupervised , mGPT shliazhko2022mgpt , XGLM lin2021few , BLOOM workshop2022bloom , PolyLM wei2023polylm , Aya ustun2024ayamodelinstructionfinetuned , and Llama 3 dubey2024llama are examples of this approach. On the other hand, the development of monolingual language models has also been explored and, at many times, shown to be a more successful approach to the multilingual one, like in the case of Finish virtanen2019multilingual , French martin-etal-2020-camembert , Catalan armengol2021multilingual , Chinese sun2021ernie , and Portuguese souza2020bertimbau ; rodrigues2023advancing ; correa2024teenytinyllama . Besides, as already pointed out by other works correa2024teenytinyllama , if based on raw pretraining instead of a fine-tuning approach, the monolingual approach can help developers escape the computational (i.e., models that are too expensive to run) and legal constraints (i.e., models that are restricted in terms of their licensing) of working with an already established foundation. However, advances in developing low-resource monolingual language models, such as those for Portuguese, remain limited, small in scale, undocumented, lacking standardization, and often reliant on repurposing models trained behind closed doors,444This is particularly true for the European and Brazilian variants, with other variants (e.g., Angolan Portuguese) even less represented or entirely absent. as will be discussed in the next section. These deficits also make it challenging to compare language models and evaluation benchmarks. At the same time, the effectiveness of the currently available benchmarks for Portuguese is also untested. In this work, we aim to address these challenges and build on existing studies to improve the status of generative language modeling research and development for Portuguese. In summary, our study offers the following advancements to the Portuguese NLP community: 1. The concatenation of a larger and more high-quality dataset for Portuguese language modeling (GigaVerbo). 2. The development of learned filters and datasets to improve text pre-processing for Portuguese. 3. Pushing self-supervised pretraining beyond the 500B tokens mark for Portuguese monolingual models. 4. The development of new, low-resource, efficient, and effective open-source foundation models for Portuguese (Tucano). 5. A critical assessment and comparison of currently available benchmarks for Portuguese language models. In Section 2, we review the current status of Portuguese Large Language Model (LLM) research and development, documenting the trends and deficits in the field. Section 3 describes the pretraining corpus used in this work. Section 4 and 5 contain the definition of our chosen tokenizer and the parameter space of different models we trained. In Section 6, we discuss the training and evaluation of our models. We also employ a simple alignment strategy to our more capable models, as will be discussed in Section 7. In Section 8, we present the results of our evaluation harness. Finally, Sections 9 and 10 provide an outlook for future studies and conclusion of our work."
https://arxiv.org/html/2411.07850v1,IAE: Irony-based Adversarial Examples for Sentiment Analysis Systems,"Adversarial examples, which are inputs deliberately perturbed with imperceptible changes to induce model errors, have raised serious concerns for the reliability and security of deep neural networks (DNNs). While adversarial attacks have been extensively studied in continuous data domains such as images, the discrete nature of text presents unique challenges. In this paper, we propose Irony-based Adversarial Examples (IAE), a method that transforms straightforward sentences into ironic ones to create adversarial text. This approach exploits the rhetorical device of irony, where the intended meaning is opposite to the literal interpretation, requiring a deeper understanding of context to detect. The IAE method is particularly challenging due to the need to accurately locate evaluation words, substitute them with appropriate collocations, and expand the text with suitable ironic elements while maintaining semantic coherence. Our research makes the following key contributions: (1) We introduce IAE, a strategy for generating textual adversarial examples using irony. This method does not rely on pre-existing irony corpora, making it a versatile tool for creating adversarial text in various NLP tasks. (2) We demonstrate that the performance of several state-of-the-art deep learning models on sentiment analysis tasks significantly deteriorates when subjected to IAE attacks. This finding underscores the susceptibility of current NLP systems to adversarial manipulation through irony. (3) We compare the impact of IAE on human judgment versus NLP systems, revealing that humans are less susceptible to the effects of irony in text.","Adversarial examples [1], crafted by adding imperceptible tiny perturbations to origin inputs maliciously, cause deep neural networks (DNNs) to fail blatantly. The secure issue, namely adversarial attack, is being widely concerned among researchers as soon as it was proposed. Extensive research has revealed that adversarial examples widely exist in many fields, e.g., computer vision (CV) [2], natural language processing (NLP) [3] and automatic speech recognition (ASR) [4]. Textual data is not as continuous as images which are capable of being perturbed imperceptibly with pixel noise. Instead, it is impossible to craft a factual imperceptible perturbation on a text due to its discrete nature. Furthermore, the grammar and semantics may be broken easily by changing even a character. The textual adversarial attack is confronted with greater challenges compared with images. A variety of textual adversarial attack models has been proposed in many NLP tasks, incorporating machine translation [5], question-answering system [3], sentiment analysis [6], et al. Spelling mistake [7], visually similar characters substitution [8], synonyms substitution [9] and sentence paraphrasing [10] are typical textual adversarial attack methods ranging from word-level to sentence-level while categorized by attacking granularity. However, there are still a few issues while assuming those methods in practical situations: 1) Subtle spelling mistakes can be recovered easily with spelling error correction [11]. 2) Words out of vocabulary may arise attention and alertness while exceeding averages in a text. 3) Word substitution and sentence paraphrasing may cause grammar to be broken or semantics deviated. Therefore, we consider a textual adversarial attacking method more practically. TABLE I: Examples of straightforward and ironic text. Straightforward 1: {CJK}UTF8gbsn他真是个糟糕的守门员，让对方进了六个球。 He is a really terrible goalkeeper, allowing the other side to score six goals. \hdashline[3pt/3pt] Ironic 1: {CJK}UTF8gbsn他真是个有天赋的守门员，让对方进了六个球。 He is a really talented goalkeeper, allowing the other side to score six goals. Straightforward 2: {CJK}UTF8gbsn那个男人真恶心，在公共场所随地吐痰。 That man is totally disgusting, spiting everywhere in public. \hdashline[3pt/3pt] Ironic 2(a): {CJK}UTF8gbsn那个男人真美味，在共场所随地吐痰。 That man is totally delicious, spiting everywhere in public. \hdashline[3pt/3pt] Ironic 2(b): {CJK}UTF8gbsn那个男人真优雅，在共场所随地吐痰。 That man is totally elegant, spiting everywhere in public. \hdashline[3pt/3pt] Ironic 2(c): {CJK}UTF8gbsn那个男人真优雅，在共场所随地吐痰。真是值得称赞啊。 That man is totally elegant, spiting everywhere in public. It is really praiseworthy. The irony is a kind of rhetorical device expressing a strong emotion referring to the opposite of literal meaning and needs to understand the actual meaning from context. Detecting irony is challenging while implementing it the model needs to have human-level language understanding ability. As far as we know, there are no studies considering converting text from straightforward to ironic as a method of generating textual adversarial examples orienting the NLP task of sentiment analysis presently. The cruxes of converting a text from straightforward into ironic are to turn the polarity of the evaluation words and make an ironic expansion appropriately when necessary. Specifically, there are at least three challenges here: 1) locating evaluation words, 2) substituting evaluation words with correct collocation, and 3) expanding text with appropriate ironic evaluation. Without loss of generality, we consider Chinese irony-based adversarial examples in this paper. As shown in Table I, Chinese words {CJK}UTF8gbsn“糟糕” is an evaluation to {CJK}UTF8gbsn“守门员” in first sentence, where {CJK}UTF8gbsn“糟糕” means “terrible” and {CJK}UTF8gbsn“守门员” means “goalkeeper”. It is necessary to locate the words {CJK}UTF8gbsn“糟糕” as an evaluation disclosing negative emotion and then substitute {CJK}UTF8gbsn“糟糕” with {CJK}UTF8gbsn“有天赋” which means “talented”. Humans are in capable of understanding the second sentence still exhibiting negative emotion with strong language comprehending ability, although the evaluation words {CJK}UTF8gbsn“有天赋” is an absolutely positive evaluation literally. Besides, it ought to be notice the substitution needs to consider collocation relation instead of substituting with antonym simply. For example, {CJK}UTF8gbsn“美味” is one of antonyms for {CJK}UTF8gbsn“恶心”, where {CJK}UTF8gbsn“美味” means “delicious” and {CJK}UTF8gbsn“恶心” means “disgusting”, but {CJK}UTF8gbsn“美味” is not supposed to collocate with {CJK}UTF8gbsn“男人”, which means “man”, referring to the context in fourth sentence, and it is supposed to be substituted with {CJK}UTF8gbsn“优雅” instead, which means “elegant”, as shown in fifth sentence. Furthermore, the whole sentence needs to be semantically smooth while to expand it with an ironic evaluation when necessary, as shown in sixth sentence. In this paper, we present a textual adversarial attacking method orienting the NLP task of sentiment analysis by rewriting a straightforward sentence into an ironic sentence, namely IAE (Irony-based Adversarial Examples). To the best of our knowledge, we are the first to use irony for textual adversarial examples generation. We summarize our major contributions as follows: • We propose IAE, a strategy based on the concept of a rhetorical device called irony for generating textual adversarial examples, which does not need to prepare irony corpus. • We show that the performance of various deep learning models substantially drops for sentiment analysis tasks when attacked by IAE. • We show that humans are only mildly or not at all affected by irony in contrast to NLP systems. II Literature review Our work connects to two strands of literature: textual adversarial examples and irony generation. Textual Adversarial Examples Existing textual adversarial attack models can be categorized into character-level, word-level, and sentence-level according to the perturbation levels of their adversarial examples. Character-level attacks disrupt the process of converting natural language text into numerical representations that computers can process, thereby causing model decision shifts. The manifestation of character-level attacks varies across different linguistic environments. In English, character-level attacks often exploit visual perturbations, such as inserting[12], deleting, swapping, and modifying[8] letters within words to create artificially constructed spelling errors. In the Chinese context, handwriting errors on paper do not occur in electronic input based on input methods. Therefore, character-level attacks in the Chinese environment often manifest as the use of homophones for substitution[13, 14] or visual decomposition of characters[15]. Word-level adversarial attacks achieve a shift in the semantic vector of the sample by perturbing the input sample at the word level, causing it to cross the decision boundary and thus leading to incorrect model outputs. Word substitution, as the core method of this strategy, includes various word replacement means such as word vector similarity[16], synonyms[17], and language model scoring[18]. Word-level adversarial attacks do not break the grammatical rules of the text and retain the original semantics to the greatest extent, thus performing better in terms of adversarial text quality and attack success rate. Coupled with the use of language models for control, it also ensures the fluency and smoothness of adversarial texts. Among them, text attacks based on synonym substitution have strong semantic retention and grammatical coherence, belonging to the most threatening category of text adversarial attacks, which have attracted widespread attention from researchers. Sentence-level adversarial attacks treat the entire original input sentence as the object of perturbation, carefully reconstructing the text content, that is, generating adversarial text that has the same semantics as the original input but causes the victim model to make decision errors. Common sentence-level adversarial attack methods include encoding and then re-decoding[19], adding irrelevant sentences[20], paraphrasing[21], etc. Irony Generation The field of irony generation, particularly within the Chinese linguistic context, remains largely unexplored, with limited research and development dedicated to this area. Zhu et al. [22] proposed a novel method that integrates reinforcement learning with style transfer techniques to generate ironic text. Their approach relies on a carefully designed reward system to guide the model towards producing text that effectively conveys irony. This method demonstrates the potential of combining advanced machine learning techniques with stylistic adjustments to achieve the nuanced expression of irony. Veale et al. [23] took a different route by exploring knowledge-based systems and shallow linguistic techniques, which they term ”mere re-generation,” for irony generation. This approach leverages existing knowledge structures and simple linguistic manipulations to introduce ironic elements into the text. While this method may not delve deeply into the complexities of language, it offers a more straightforward and potentially more accessible avenue for irony generation. In the closely related domain of sarcasm, Mishra et al. [24] presented a framework that utilizes reinforced neural sequence-to-sequence learning coupled with information retrieval strategies for sarcasm generation. To the best of our knowledge, our work represents the first instance of leveraging irony for the generation of textual adversarial examples. This application of irony in adversarial machine learning is groundbreaking, as it introduces a new dimension to the field of natural language processing security. It serves as a testament to the importance of understanding and incorporating advanced linguistic features, such as irony, into machine learning models to enhance their resilience against adversarial attacks. III Problem statement We assume access to a corpus of labeled sentences D=\{(s_{1},p_{1}),...,(s_{n},p_{n})\}, where s_{i} is a sentence and p_{i}\in L, the set of possible emotional polarity, i.e., L = {positive, negative}. We define s^{p}=(c,e,d), a sentence with emotional polarity p, where c is the central word of the sentence, e is the evaluation word that evaluating the central word c, and d is the detailed description of the evaluation. On this basis, we define emotional sentence s^{p} as a straightforward sentence or an ironic sentence while the evaluation e have emotional polarity p^{\prime}, while collocating with c, and p=p^{\prime}, or p\neq p^{\prime}. Generally, the irony is a negative sentence exhibiting positive evaluation. Thus, our goal is to build a model that takes as input sentence s, a negative emotional sentence exhibiting negative evaluation e^{\textrm{neg}}, and outputs a sentence s^{\prime} that retains the negative emotional polarity while exhibiting positive evaluation e^{\textrm{pos}}. Note that the concept of evaluation word we use is not equivalent to the sentiment word while sentiment word is an adjective with a clear emotional polarity. The emotional polarity of an evaluation word should be determined by the central word with which the evaluation word collocates. IV Approach Figure 1: An overview of our proposed IAE generator. In this section, we detail our irony-based textual adversarial attacking method, incorporating three parts: 1) an extractor of collocations between nouns and adjectives, 2) a strategy for evaluation word substitution, and 3) a strategy for ironic evaluation sentence generation. An overview of our IAE generator is shown in Fig. 1. Generally, it takes straightforward text as inputs and outputs ironic text. First, the central word and relevant evaluation word will be located, and then the evaluation word will be substituted with an opposite evaluation word among all possible alternatives, Finally, an appropriate ironic evaluation sentence, determined by local model, will be appended to the text for strengthening the effect of irony. Next, we describe the details of each component of IAE generator. IV-A Collocation extractor We design a collocations extractor to establish noun-adjective collocations tables, which also reveals probable emotional polarity between a noun with all collocated adjectives, as a library of alternatives for evaluation word substitution (see section IV-B). A host of observations were made on Chinese corpus with part-of-speech tagging and dependency parsing, and we found the noun-adjective collocations in a Chinese sentence are supposed to form the following two kinds of dependencies: 1) a subject-verb structure, or 2) an attributive structure (see examples in Table II). Note that the results of dependency parsing in Chinese may be different from English due to the differences in the two kinds of syntax rules. e.g., the words “weather” and “good” are supposed to form a subject-predicative in English instead of a subject-verb. Then we can extract plenty of collocations from a large corpus through the observations above, but the next key question is how to determine the emotional polarity of each noun-adjective collocation. Although we can use advanced sentiment analysis models to determine the overall emotional polarity of the sentence from which a noun-adjective collocation extracted, it is no guarantee the emotional polarity of a noun-adjective collocation will be consistent with the whole sentence. But, intuitively, the emotional polarity of a collocation should probably be positive if it mostly appears in sentences with a positive overall emotional polarity rather than negative. Hence, the polarity of collocation can be inferred by the following formulas: x=\frac{\textrm{Freq}_{\textrm{pos}}}{\textrm{Freq}_{\textrm{neg}}} (1) F(x)=\left\{\begin{array}[]{l}\textrm{positive}\ \ \ ,x>1\\ \textrm{negative}\ \ ,x<1\\ \textrm{manually},x=1\\ \end{array} (2) Where \textrm{Freq}_{\textrm{pos}} and \textrm{Freq}_{\textrm{neg}} are the frequencies of a collocation appearing in sentences with an emotional polarity of positive or negative respectively. The emotional polarity of a collocation is supposed to be positive when x>1, or negative when x<1, or decided manually when the result of x happens to be 1. Therefore, the noun-adjective collocations table, denoted as T, can be established by collecting collocations by dependency parsing and inferring their emotional polarities by counting and comparing the numbers of each emotional polarity of the sentences in which they occur. IV-B Evaluation word substitution The strategy for evaluation word substitution is the most important procedure to convert a straightforward sentence s to an ironic sentence s^{\prime} while s^{\prime} has the evaluation word e with emotional polarity p^{\prime}, which is opposite to the emotional polarity p of the whole sentence. Next, we describe our evaluation word substitution step by step. Locating. At the very beginning, the pairs of central word and relevant evaluation word are located by using part-of-speech tagging and dependency parsing together, which is similar to the strategy of extracting noun-adjective collocation (see secttion IV-A). Retrieving. The alternatives are retrieved among the table T by using central word c as an index. The whole procedure will terminate and return a general evaluation word (e.g., {CJK}UTF8gbsn“不错”, which is analog to “fine” in English) as the result while the central word does not exist or none of the positive evaluation words are retrieved. Determining. To determine what alternative evaluation word to substitute original, our strategy is to evaluate the quality (i.e., probability of sentence) of all alternative sentences S^{\prime} by N-gram language model while combining any possible collocation of central word and alternative evaluation word. Formally, for any s^{\prime}\in S^{\prime}, the probability is calculated by the following formula: P(s)=\prod\frac{\textrm{count}(w_{i-1}w_{i},D)+\delta}{\textrm{count}(w_{i},D)% +\delta} (3) where w_{i} is the i-th word in s^{\prime}, w_{i-1}w_{i} is the sequence composed of w_{i-1} and w_{i} sequentially, \textrm{count}(.,D) denotes the numbers of times a word or a sequence appears in D, and \delta is an additive smoothing parameter for the situation that some words are just not appearing in D. In practice, the smoothing parameter \delta can be set to 1 empirically. The alternative sentence s^{\prime} with the highest probability among S^{\prime} will be determined as the result of evaluation word substitution. TABLE II: Examples of sentences containing noun-adjective collocations and dependencies. Sentences Collocations Dependencies {CJK}UTF8gbsn天气这么好，应该出去透透空气。 The weather is so good for enjoying fresh air. {CJK}UTF8gbsn天气，好 weather, good subject-verb {CJK}UTF8gbsn那个男人真帅。 That man is so handsome. {CJK}UTF8gbsn男人，帅 man, handsome subject-verb {CJK}UTF8gbsn优美的音乐可以给人们带来享受。 Beautiful music can bring people enjoyment. {CJK}UTF8gbsn音乐，优美 music, beautiful attributive {CJK}UTF8gbsn她招待我们吃了一顿可口的午餐。 She served us a delicious lunch. {CJK}UTF8gbsn午餐，可口 lunch, delicious attributive IV-C Ironic evaluation appending Reversing the result of sentiment analysis by substituting the evaluation alone is often difficult while the context still exhibits original emotional polarity. But this problem can be solved by appending an evaluation, which is opposite to the polarity of real emotion for strengthening the ironic effect. It is easy to construct positive evaluations by composing positive adjectives and other grammatical constituents according to sentence patterns. However, the problems are how to choose an evaluation and how to guarantee the semantic smoothness of the whole sentence after evaluation appending. Our strategy is to construct general positive evaluations, which can collocate with almost objects and guarantee the semantic smoothness, as much as possible, and then to determine an evaluation appending to s^{\prime}. Inspired by the substitute black box attack (SBA) [25] which is utilizing the transferability of adversarial examples, we consider training the local model to substitute the victim model, then testing each alternative on the local model, and finally selecting the evaluation while the local model outputs a wrong prediction after appending. For the case that there is no effective adversarial example on the local model, we consider choosing the longest one. After determining the ironic evaluation, which is supposed to append to the sentence s^{\prime}, the final IAE is generated completely. V Experiments and Results In this section, we conduct comprehensive experiments to evaluate our IAE on the tasks of sentiment analysis. V-A Datasets and Victim Models We evaluate our IAE on the public reviews of Meituan111Meituan is a platform for ordering takeaway, which contains positive and negative user reviews and Amazon. The five-star and one-star reviews are taken as positive and negative text respectively. Because our IAE is only applicable to the examples with negative emotional polarity, we randomly select 500 examples with negative emotional polarity from each dataset as the test set, and then we divide the remaining examples into two balanced parts for training local and victim models. Details of the datasets are shown in Table III, where “Class #” refers to the number of labels, “Max. #W” means maximum length of sentences (number of words), “Min. #W” means minimum length of sentences (number of words), and “Avg. #W” means average length of sentences (number of words), “P. #” and “N. #” signify the number of text exhibiting positive and negative emotional polarity respectively. Besides, for comprehensive noun-adjective collocations extracting (see section IV-A), we collected 30111 nouns and 114383 related collocations from serveral Chinese corpus, including reviews on Meituan and Amazon, Sina weibo222a twitter like Chinese online platform comments, and online News corpus. For each noun, there are 1115 collocations at most and 1 collocation at least, with an average of 3.7. We choose three popular models for text classification, namely TextCNN [26], Bidirectional LSTM (BiLSTM) [27] and a fine-tuned BERT [28], used for evaluating our IAE. TextCNN has three convolutional filters of different kernel sizes (3, 4, 5), and their outputs are concatenated, pooled and fed to a fully-connected layer followed by an output layer. BiLSTM is composed of a 128-dimenional bidirectional LSTM layer, a dropout layer using a drop rate of 0.5, and an output layer. BERT is obtained by fine-tuning the Chinese BERT-Base model with 12-layer, 768-hidden, and 12-heads released by Google. The optimizer, learning rate, and loss function of all models are set to adam, 0.01, and cross-entropy respectively. Besides, we implement Chinese word segmentation, part of speech tagging, and dependency parsing using the third-party library released by Harbin Institute of technology [29]. V-B Baseline methods We implement two baseline methods based on important word substitution and compared them with ours for proving the contribution of this work. The two baseline methods are 1) visual-based substitution [8], which means substitute important words with visual similar chart, and 2) homonym-based substitution [13], which means substitute important words with others pronounced the same way but have different meanings. The important words refer to the words in the input text that make the most contribution to the model decision and the calculation algorithm of important words adopts [30]. TABLE III: Statistics for the datasets. Dataset Class # Max. #W Min. #W Avg. #W P. # N. # Meituan 2 237 2 18.96 6000 6500 Amazon 2 858 2 23.71 6000 6500 TABLE IV: Performance of victim models under attacking of IAE and two baseline methods on Meituan review dataset. Dataset Method Local Model Victim model WMD TextCNN BidLSTM Bert Meituan Origin N/A 0.885 0.894 0.934 N/A Visual-based TextCNN 0.312 0.702 0.880 1.497 BidLSTM 0.260 0.670 0.892 1.787 Bert 0.610 0.818 0.860 0.384 Homonym-based TextCNN 0.326 0.694 0.854 1.551 BidLSTM 0.304 0.698 0.856 1.784 Bert 0.606 0.832 0.846 0.411 Ours TextCNN 0.464 0.204 0.456 1.197 BidLSTM 0.324 0.416 0.542 1.041 Bert 0.700 0.876 0.370 0.808 Amazon Origin N/A 0.900 0.936 0.944 N/A Visual-based TextCNN 0.564 0.664 0.870 2.570 BidLSTM 0.346 0.400 0.914 2.643 Bert 0.860 0.830 0.836 2.480 Homonym-based TextCNN 0.648 0.702 0.844 2.559 BidLSTM 0.544 0.550 0.870 2.605 Bert 0.842 0.826 0.834 2.472 Ours TextCNN 0.338 0.322 0.602 2.344 BidLSTM 0.720 0.728 0.538 2.341 Bert 0.880 0.886 0.670 2.379 TABLE V: Human evaluation of emotional correctness and grammar smoothness. Dataset Emotional Correctness Grammar Smoothness Origin IAE Origin IAE Meituan 91 86 4.25 3.50 Amazon 90 84 4.50 3.75 V-C Attack performance The attack performance results of our IAE and two baseline methods are shown in Table IV. Note that only examples labeled with negative are used for test as the adversarial attack based on irony is only applicable to the examples with negative emotional polarity. We observe the adversarial examples generated by our irony-based attack cause the victim models to fail more seriously than the baseline methods in most conditions in most conditions. Specifically, the visual-based and homonym-based attack can hardly fool Bert models while our method can cause the accuracy of Bert from 89.8% to 37.0% at most, besides, the Word Mover’s Distances [31] between our IAE and clean examples are always smaller than those between adversarial examples generated by baseline methods and clean examples. V-D Human evaluation We ask 4 students with native Chinese language skill to evaluate the emotional correctness and semantic smoothness of successful IAE generated from Meituan and Amazon reviews. Specifically, we randomly select 100 IAE and 100 clean examples, and every student needs to evaluate the mixture of them. For evaluating emotional correctness, each student evaluates the true emotional polarity of each example and it is annotated as positive (negative) if two or more students evaluate a example as positive (negative). An extra human evaluator would participate in the evaluation if there are equal numbers of different evaluation on emotional polarity. For evaluating semantic smoothness, each student scores the semantic smoothness of each example with Likert scale ranging from 1 to 5 while 1 and 5 mean the semantics of a example is completely confused or fluent separately. We summarized the evaluation of all students and averaged the semantic smoothness of the IAE and the clean examples respectively. The results are shown in Table V and it shows that a lightly lower emotional correctness and semantic smoothness in IAE than clean examples, but the emotional correctness and semantic smoothness of IAE still reach 86 and 3.75 respectively. VI Discussion We studied how to regard irony as a textual adversarial perturbation in Chinese and it proved effective in sentiment analysis. There are differences between Chinese and other languages in grammar and habits, however, irony, as a rhetorical device in almost all languages, could be utilized as a general way of textual adversarial perturbation. The experiment of training the local model for generating effective adversarial examples also reveals some properties of transferability. First, the transfers between two models are non-symmetric. As we can see, the accuracy of victim model BERT is 54.2\% when generated IAE from local model BidLSTM, however, the accuracy of victim model BidLSTM is 87.6\% when generated IAE from local model BERT while testing on Meituan reviews dataset. It is similar to the findings in the study of the transferability of image adversarial examples [32], even though we focus on the text field. Second, the adversarial examples generated from the high-accuracy models may be less transferable. As we can see, BERT is the most accurate model among all models we use, however, the adversarial examples generated from BERT hardly mislead other models. We also found there are three major types of weaknesses in our methods, which affect the attacking performances. For analysis of the weaknesses, we sampled 100 failed IAE which mislead the victim model unsuccessfully or lose original sentiment. We found that 26% of the failures are due to the long length of input text which is more than 50 Chinese characters, 38% of the failures are due to the weak correlation between evaluative sentence and context, 29% of the failures are due to the imperfection of part-of-speech tagging and dependency parsing tools, and the remaining 7% of the failures have no significant type. The first type of failure is due to the obvious fact that the longer the text, the more negative content it contains, so it is difficult to change the label of model prediction by substituting an evaluation word or appending a generally positive evaluation sentence. The second type of failure is due to the weak correlation between the evaluative sentence and the context description. For example, for the sentence “{CJK}UTF8gbsn菜真的很难吃，还是去其他店吃好些” (The food is really unpalatable, and it’s better to go to another restaurant), where the context is not a correlational detail description to the evaluation of food, it is inappropriate to substitute the negative evaluation word “{CJK}UTF8gbsn难吃” (unpalatable) to a positive word “{CJK}UTF8gbsn好吃” (delicious) otherwise the emotional polarity of the text will change completely. The third type of failure is due to the dependency analysis tools, which is unable to analyze the dependency correctly all the time, while it is necessary to locate the evaluation word with part-of-speech tagging and dependency parsing. VII Conclusion and future work In this paper, we have introduced Irony-based Adversarial Examples (IAE), a novel method for generating adversarial text by transforming straightforward sentences into ironic ones. Our research has made several significant contributions to the field of adversarial attack. Firstly, we have introduced IAE as a strategy for generating textual adversarial examples that leverages irony. This method is innovative in that it does not depend on pre-existing irony corpora, thereby offering a flexible instrument for creating adversarial text across a spectrum of NLP tasks. Secondly, we have demonstrated empirically that the performance of several deep learning models on sentiment analysis tasks is markedly compromised when confronted with IAE attacks. This result highlights the vulnerability of current NLP systems to adversarial manipulations facilitated through irony. Thirdly, we have compared the effects of IAE on human judgment versus NLP systems, revealing a notable difference in susceptibility. Our findings indicate that humans are relatively more resilient to the influence of irony in text, contrasting with the performance of NLP models. Our future work will focus on enhancing the performance of IAE in longer texts and improving its generalization capabilities across different languages. This will involve addressing the complexities associated with maintaining ironic integrity over extended passages and adapting to the nuances of various linguistic contexts. Additionally, we are intrigued by the prospect of integrating more rhetorical devices into textual adversarial perturbations, beyond irony. For instance, exploring the use of metaphors to disrupt machine reading comprehension presents an exciting avenue for further research. By expanding the repertoire of rhetorical strategies employed in adversarial text generation, we aim to deepen our understanding of the interplay between language, context, and machine learning models, ultimately contributing to the development of more robust and nuanced NLP systems. References [1] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6572 [2] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep neural networks,” IEEE Trans. Evol. Comput., vol. 23, no. 5, pp. 828–841, 2019. [Online]. Available: https://doi.org/10.1109/TEVC.2019.2890858 [3] R. Jia and P. Liang, “Adversarial examples for evaluating reading comprehension systems,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, M. Palmer, R. Hwa, and S. Riedel, Eds. Association for Computational Linguistics, 2017, pp. 2021–2031. [Online]. Available: https://doi.org/10.18653/v1/d17-1215 [4] Q. Wang, B. Zheng, Q. Li, C. Shen, and Z. Ba, “Towards query-efficient adversarial attacks against automatic speech recognition systems,” IEEE Trans. Inf. Forensics Secur., vol. 16, pp. 896–908, 2021. [Online]. Available: https://doi.org/10.1109/TIFS.2020.3026543 [5] Y. Belinkov and Y. Bisk, “Synthetic and natural noise both break neural machine translation,” in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [Online]. Available: https://openreview.net/forum?id=BJ8vJebC- [6] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,” in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 8018–8025. [Online]. Available: https://aaai.org/ojs/index.php/AAAI/article/view/6311 [7] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, “Hotflip: White-box adversarial examples for text classification,” in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, I. Gurevych and Y. Miyao, Eds. Association for Computational Linguistics, 2018, pp. 31–36. [Online]. Available: https://www.aclweb.org/anthology/P18-2006/ [8] S. Eger, G. G. Sahin, A. Rücklé, J. Lee, C. Schulz, M. Mesgar, K. Swarnkar, E. Simpson, and I. Gurevych, “Text processing like humans do: Visually attacking and shielding NLP systems,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 1634–1647. [Online]. Available: https://doi.org/10.18653/v1/n19-1165 [9] S. Ren, Y. Deng, K. He, and W. Che, “Generating natural language adversarial examples through probability weighted word saliency,” in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. Màrquez, Eds. Association for Computational Linguistics, 2019, pp. 1085–1097. [Online]. Available: https://doi.org/10.18653/v1/p19-1103 [10] M. Iyyer, J. Wieting, K. Gimpel, and L. Zettlemoyer, “Adversarial example generation with syntactically controlled paraphrase networks,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 1875–1885. [Online]. Available: https://doi.org/10.18653/v1/n18-1170 [11] D. Pruthi, B. Dhingra, and Z. C. Lipton, “Combating adversarial misspellings with robust word recognition,” in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. Màrquez, Eds. Association for Computational Linguistics, 2019, pp. 5582–5591. [Online]. Available: https://doi.org/10.18653/v1/p19-1561 [12] B. Formento, C. S. Foo, L. A. Tuan, and S. K. Ng, “Using punctuation as an adversarial attack on deep learning-based NLP systems: An empirical study,” in Findings of the Association for Computational Linguistics: EACL 2023, A. Vlachos and I. Augenstein, Eds. Dubrovnik, Croatia: Association for Computational Linguistics, 2023, pp. 1–34. [13] W. Wang, R. Wang, L. Wang, and B. Tang, “Adversarial examples generation approach for tendency classification on chinese texts,” Ruan Jian Xue Bao/J. Softw., vol. 30, pp. 2415–2427, 2019. [Online]. Available: https://doi.org/10.13328/j.cnki.jos.005765 [14] N. Cheng, G. Chang, H. Gao, G. Pei, and Y. Zhang, “Wordchange: Adversarial examples generation approach for chinese text classification,” IEEE Access, vol. 8, pp. 79 561–79 572, 2020. [Online]. Available: https://doi.org/10.1109/ACCESS.2020.2988786 [15] H. Ou, L. Yu, S. Tian, and X. Chen, “Chinese adversarial examples generation approach with multi-strategy based on semantic,” Knowl. Inf. Syst., vol. 64, no. 4, pp. 1101–1119, 2022. [16] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is BERT really robust? A strong baseline for natural language attack on text classification and entailment,” in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 8018–8025. [17] S. Ren, Y. Deng, K. He, and W. Che, “Generating natural language adversarial examples through probability weighted word saliency,” in Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, 2019, pp. 1085–1097. [18] H. Zhang, H. Zhou, N. Miao, and L. Li, “Generating fluent adversarial examples for natural languages,” in Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. Màrquez, Eds. Association for Computational Linguistics, 2019, pp. 5564–5569. [Online]. Available: https://doi.org/10.18653/v1/p19-1559 [19] W. Han, L. Zhang, Y. Jiang, and K. Tu, “Adversarial attack and defense of structured prediction models,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Association for Computational Linguistics, 2020, pp. 2327–2338. [20] B. Liang, H. Li, M. Su, P. Bian, X. Li, and W. Shi, “Deep text classification can be fooled,” in Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, J. Lang, Ed. Stockholm, Sweden: IJCAI, 2018, pp. 4208–4215. [Online]. Available: https://doi.org/10.24963/ijcai.2018/585 [21] Y. Xu, X. Zhong, A. Jimeno Yepes, and J. H. Lau, “Grey-box adversarial attack and defence for sentiment classification,” in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, Eds. Online: Association for Computational Linguistics, Jun. 2021. [22] M. Zhu, Z. Yu, and X. Wan, “A neural approach to irony generation,” CoRR, vol. abs/1909.06200, 2019. [Online]. Available: http://arxiv.org/abs/1909.06200 [23] T. Veale, “A massive sarcastic robot: What a great idea! two approaches to the computational generation of irony,” in Proceedings of the Ninth International Conference on Computational Creativity, ICCC 2018, Salamanca, Spain, June 25-29, 2018, F. Pachet, A. Jordanous, and C. León, Eds. Association for Computational Creativity (ACC), 2018, pp. 120–127. [24] A. Mishra, T. Tater, and K. Sankaranarayanan, “A modular architecture for unsupervised sarcasm generation,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds. Association for Computational Linguistics, 2019, pp. 6143–6153. [Online]. Available: https://doi.org/10.18653/v1/D19-1636 [25] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical black-box attacks against machine learning,” in Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates, April 2-6, 2017, R. Karri, O. Sinanoglu, A. Sadeghi, and X. Yi, Eds. ACM, 2017, pp. 506–519. [Online]. Available: https://doi.org/10.1145/3052973.3053009 [26] Y. Kim, “Convolutional neural networks for sentence classification,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, A. Moschitti, B. Pang, and W. Daelemans, Eds. ACL, 2014, pp. 1746–1751. [Online]. Available: https://doi.org/10.3115/v1/d14-1181 [27] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes, “Supervised learning of universal sentence representations from natural language inference data,” in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, M. Palmer, R. Hwa, and S. Riedel, Eds. Association for Computational Linguistics, 2017, pp. 670–680. [Online]. Available: https://doi.org/10.18653/v1/d17-1070 [28] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of deep bidirectional transformers for language understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4171–4186. [Online]. Available: https://doi.org/10.18653/v1/n19-1423 [29] W. Che, Y. Feng, L. Qin, and T. Liu, “N-LTP: A open-source neural chinese language technology platform with pretrained models,” CoRR, vol. abs/2009.11616, 2020. [Online]. Available: https://arxiv.org/abs/2009.11616 [30] J. Li, S. Ji, T. Du, B. Li, and T. Wang, “Textbugger: Generating adversarial text against real-world applications,” in 26th Annual Network and Distributed System Security Symposium, NDSS 2019, San Diego, California, USA, February 24-27, 2019. The Internet Society, 2019. [Online]. Available: https://www.ndss-symposium.org/ndss-paper/textbugger-generating-adversarial-text-against-real-world-applications/ [31] M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger, “From word embeddings to document distances,” in Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, ser. JMLR Workshop and Conference Proceedings, F. R. Bach and D. M. Blei, Eds., vol. 37. JMLR.org, 2015, pp. 957–966. [Online]. Available: http://proceedings.mlr.press/v37/kusnerb15.html [32] L. Wu and Z. Zhu, “Towards understanding and improving the transferability of adversarial examples in deep neural networks,” in Proceedings of The 12th Asian Conference on Machine Learning, ACML 2020, 18-20 November 2020, Bangkok, Thailand, ser. Proceedings of Machine Learning Research, S. J. Pan and M. Sugiyama, Eds., vol. 129. PMLR, 2020, pp. 837–850. [Online]. Available: http://proceedings.mlr.press/v129/wu20a.html Xiaoyin Yi received the M.S. degree in computer science from the Chongqing University of Posts and Telecommunications, China. She is currently a teacher with Chongqing College of Mobile Communication and pursuing the Ph.D. degree with Chongqing University of Posts and Telecommunications, China. Her research interests include cybersecurity within AI and AI security. Jiacheng Huang received the M.S. degree in instructional technology from the Hubei Normal University, China. He is currently pursuing the Ph.D. degree with Chongqing University of Posts and Telecommunications, China. His research interests include cybersecurity within AI, natural language processing, and AI security. \EOD"
https://arxiv.org/html/2411.07845v1,"Ethical Concern Identification in NLP:
A Corpus of ACL Anthology Ethics Statements","What ethical concerns, if any, do LLM researchers have? We introduce EthiCon, a corpus of 1,580 ethical concern statements extracted from scientific papers published in the ACL Anthology. We extract ethical concern keywords from the statements and show promising results in automating the concern identification process. Through a survey (N=200), we compare the ethical concerns of the corpus to the concerns listed by the general public and professionals in the field. Finally, we compare our retrieved ethical concerns with existing taxonomies pointing to gaps and future research directions.","Researchers are often asked to subscribe to ethical guidelines, e.g., the European Code of Conduct for Research Integrity ALLEA (2017) – or the ACM Code of Ethics111See https://www.aclweb.org/portal/content/acl-code-ethics for ACL’s guidelines. for publishing their work in the Association for Computational Linguistics (ACL). In addition, authors are often encouraged to write a so-called ethics statement, addressing the broader implications of their work or any ethical considerations. We ask: What ethical concerns are raised in such statements, and how do they compare with public perceptions? Is there a gap between academic and public concerns? Figure 1: Visualizing top 60 concerns in ACL ethics statements, reflecting term frequencies. As natural language processing technologies become more prevalent, understanding the ethical concerns raised by professionals will enable us to compare them with public concerns, helping to identify gaps and overlaps that can inform frameworks and solutions to existing and emerging problems. For this reason, we create EthiCon, an annotated corpus of ethics statements from the Proceedings of the 60th and 61st Annual Meeting of the Association for Computational Linguistics Muresan et al. (2022); Rogers et al. (2023). Our aim is twofold: to map out the concerns of the NLP community as they appear on the ethics statements, and to trace gaps and overlaps between NLP professionals and the general public. Our results show that laypeople express different ethical concerns than professionals, focusing on socio-economic and human-computer interaction issues, along with miscellaneous concerns like existential risks. This highlights the need for increased dialogue between researchers and the public to address these varying perspectives and an updated taxonomy covering both existing and emerging issues. Contributions. We provide a corpus of 1,580 ethics statements from the ACL Anthology. We identify the main issues that NLP researchers flag as ethically concerning in their work and show how LLMs could automate this process. Through a survey, we compare how laypeople and NLP professionals perceive the ethical concerns surrounding natural language processing. Lastly, we provide a comparison of the ACL and the survey ethical concerns to existing taxonomies of risks posed by Language Models. Finding a way to better automate the process will enable the comparison of ethical concerns across time and technical innovation and provide a better understanding of the impact of NLP research within the field and beyond. Figure 2: Examples from the identified categories of ethical concern statements."
https://arxiv.org/html/2411.07843v1,Chain Association-based Attacking and Shielding Natural Language Processing Systems,"Association as a gift enables people do not have to mention something in completely straightforward words and allows others to understand what they intend to refer to. In this paper, we propose a chain association-based adversarial attack against natural language processing systems, utilizing the comprehension gap between humans and machines. We first generate a chain association graph for Chinese characters based on the association paradigm for building search space of potential adversarial examples. Then, we introduce an discrete particle swarm optimization algorithm to search for the optimal adversarial examples. We conduct comprehensive experiments and show that advanced natural language processing models and applications, including large language models, are vulnerable to our attack, while humans appear good at understanding the perturbed text. We also explore two methods, including adversarial training and associative graph-based recovery, to shield systems from chain association-based attack. Since a few examples that use some derogatory terms, this paper contains materials that may be offensive or upsetting to some people.","In past years, many studies has shown that the adversarial examples can cause decision-making errors in natural language processing (NLP) systems Formento et al. (2023); Ou et al. (2022), even in large language models Wang et al. (2024) (LLMs). Howerver, existing adversarial attacks only consider the attack strategies in a direct way while ignoring the complexity of textual adversarial attacks in reality. For example, Chinese words {CJK}UTF8gbsn“幼稚”, which means “naive”, is an adjective with emotional tendency, but it will not be recognized as an emotional word by an emotion analysis system after being substituted with {CJK}UTF8gbsn“拿衣服”, which is a verb object phrase meaning to “take clothes”. The relation between {CJK}UTF8gbsn“幼稚” and {CJK}UTF8gbsn“拿衣服” is not a simple single-layer mapping, but a multi-layer mapping. Specifically, we associate {CJK}UTF8gbsn“幼稚” with “naive” first, which is the English translation of {CJK}UTF8gbsn“幼稚”, and then further associate it with {CJK}UTF8gbsn“拿衣服”, which is one of the Mandarin transliterations of “naive”. Note that the above is only a simple example of word substitution based on chain association while the associative ability of human being is complex. English has analogous cases. Take “screw”, a polysemy referring to “a metal object like a nail” or “having sex with someone”. Attackers online can replace “screw” with its related emoji to form offensive phrases like “{\mathchoice{\leavevmode@ifvmode\lower 0.0pt\tf@size*1/10\hbox{% \includegraphics[height=0.0pt]{screw.png}}}{\leavevmode@ifvmode\lower 0.0pt% \tf@size*1/10\hbox{\includegraphics[height=0.0pt]{screw.png}}}{% \leavevmode@ifvmode\lower 0.0pt\sf@size*1/10\hbox{\includegraphics[height=0.0% pt]{screw.png}}}{\leavevmode@ifvmode\lower 0.0pt\ssf@size*1/10\hbox{% \includegraphics[height=0.0pt]{screw.png}}}} you”, which utilizes human associations about the corresponding word of emoji and its polysemy. Note that this example is merely used as an analogy to explain our idea, in fact, this work only focuses on Chinese adversarial examples. In this work, we investigate to what extent advanced Chinese NLP systems are sensitive to chain association-based attack and explore various shielding techniques. Specifically, we first generate a chain association graph for Chinese characters based on the association paradigm for building search space of potential adversarial examples. Then, we regard generating adversarial examples as a problem of combinatorial optimization and introduce an discrete particle swarm optimization algorithm to search for the optimal adversarial examples. We show that advanced NLP models and applications are extremely vulnerable to our attack. To our best knowledge, we are the first to introduce chain association in adversarial attack. Furthermore, we also explore two methods to protect NLP systems from our attacks."
https://arxiv.org/html/2411.07820v2,Expanding Horizons in Short Text Analysis: Integrating LLMs and VAEs for Enhanced Topic Modeling,"We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel approach designed to bridge the pre-retrieval information gap in Retrieval-Augmented Generation (RAG) systems through query optimization tailored to meet the specific knowledge requirements of Large Language Models (LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR framework begins by extracting parametric knowledge from LLMs, followed by using a specialized query optimizer for refining these queries. This process ensures the retrieval of only the most pertinent information essential for generating accurate responses. Moreover, to enhance flexibility and reduce computational costs, we propose a trainable scheme for our pipeline that utilizes a smaller, tunable model as the query optimizer, which is refined through knowledge distillation from a larger teacher model. Our evaluations on various question-answering (QA) datasets and with different retrieval systems show that ERRR consistently outperforms existing baselines, proving to be a versatile and cost-effective module for improving the utility and accuracy of RAG systems.","In recent years, the field of natural language processing has witnessed remarkable advancements recently, largely attributable to the proliferation of Large Language Models (LLMs). These LLMs, characterized by their extensive scale training on vast corpora, have demonstrated unparalleled capability of understanding human text and producing high-quality responses kaplan2020scaling; clark2022unified. Moreover, they showcase exceptional practicality and scalability across a spectrum of downstream NLP tasks, such as conversational response generation, text summarization, or content recommendation, under only few-shot or zero-shot settings wu2023survey. However, despite their potential, a major limitation of these LLMs is their struggle to accurately capture the dynamic nature of information updates. Since LLMs are pre-trained on static corpora of data they face difficulties when dealing with the latest or less commonly known information that falls outside their initial training scope. This limitation leads to generating responses that contain outdated or incorrect information, or in some cases, entirely fictitious content— a problem often termed as ""hallucination"" lee2018hallucinations. In response to this pressing challenge, the concept of Retrieval-Augmented Generation (RAG) RAG has emerged as a promising technique to enhance the functionality and reliability of LLMs. Retrieval augmentation addresses the shortcomings of original LLMs by incorporating external knowledge (i.e., non-parametric knowledge) sources through some information retrieval system and leveraging the in-context learning ability of LLM readers to generate more contextually relevant and accurate responses. For instance, in a conversational setting where a user interacts with an LLM like ChatGPT gpt to inquire about the latest news events. ChatGPT, being reliant on static pre-training knowledge, may struggle to offer real-time updates in response to the user query. RAG mitigates this information gap by retrieving pertinent news articles or data related to the query from external databases. By augmenting the original user question with retrieved knowledge, the final LLM reader is more likely to generate a contextually relevant answer. While previous research has underscored the efficacy of retrieval augmentation in mitigating hallucination, it introduces its own set of challenges. One such critical challenge in the RAG systems is the existence of a gap between the information retrievable using the given retriever using original user input queries and the information required for generating optimal responses ragsurvey. Consider a scenario where an external document collection comprises three distinct passages labelled Passage A, B, and C, each containing unique knowledge components denoted as knowledge x, y, and z, respectively. Notably, all three passages include keywords related to Knowledge z, which the user seeks to obtain. However, if the user enters a poorly formulated query, there exists a possibility that Passage A or Passage B may be retrieved instead of the ideal source, Passage C, thereby limiting the LLM reader’s capacity to derive an optimal response. This disparity is referred to as the pre-retrieval gap in RAG and poses a fundamental obstacle to achieving optimal text generation outcomes. In the quest to bridge this pre-retrieval information gap ragsurvey within the existing RAG systems, the Rewrite-Retrieve-Read (RRR) framework rrr introduces a pivotal paradigm shift by incorporating the concept of query rewriting from the realm of traditional information retrieval into the RAG domain. RRR proposes the integration of a specialized Query Rewriter module into the native RAG system to align user queries with both the retriever and the LLM reader, thereby optimizing the retrieval process and narrowing the pre-retrieval gap between user queries and the information sought by the LLM reader, i.e. the LLM that generates the final responses. This query optimization technique has also seen widespread adoption in more recent RAG systems stepback; ragsurvey. However, while RRR demonstrates notable advancements, only the original query is passed to the rewrite module, which focuses more on rephrasing queries or expanding the search scope. This framework falls short in tailoring optimization strategies for queries based on the specific needs of the LLM reader, overlooking the need for retrieving the most relevant knowledge essential for accurate generation. To this end, we propose Extract-Refine-Retrieve-Read (ERRR), a simple but effective framework designed for retrieval augmentation systems. The ERRR framework is crafted to bridge the pre-retrieval information gap through tailored query optimization and aims to resolve the inherent limitations of RRR by enabling retrieval based on the specific information needs of the LLM reader. Specifically, it initiates by extracting parametric knowledge from LLMs and employs a specialized query optimizer that refines user queries. This refinement either complements or validates the extracted parametric knowledge, ensuring that only essential information is retrieved for generating accurate responses, and minimizing the retrieval of extraneous information that could degrade the quality of the output. Additionally, given that many advanced LLMs, such as ChatGPT gpt, function as black-box systems accessible only through inference APIs, our study introduces a trainable ERRR scheme to enhance customizability for different types of queries. This scheme incorporates a smaller, tunable language model as the query optimizer, which not only helps to lower the expenses associated with query optimization for parametric knowledge refinement but also allows for more adaptable customization tailored to each knowledge source. Our methods undergo thorough evaluation on multiple question-answering (QA) datasets (HotpotQA hotpotqa, AmbigNQ ambigqa, PopQA popqa). All frameworks and baselines evaluated are implemented using GPT-3.5-Turbo gpt3 or T5-Large t5 as the query optimizer, with GPT-3.5-Turbo serving as both the final reader and the source of parametric knowledge. Evaluation is conducted on the Brave Web Search Engine 111https://brave.com/search/api/ and a local dense retriever, Dense Passage Retrieval (DPR) DPR, alongside a static open-source Wikipedia corpus. Our evaluations demonstrate that ERRR consistently boosts the performance of retrieval-augmented LLMs across all tested QA datasets and retrieval systems when compared to the RRR framework. Furthermore, the outcomes highlight the adaptability and versatility of ERRR, showcasing its effectiveness across diverse settings and data sources. In summary, our key contributions are as follows: (i) We propose Extract-Refine-Retrieve-Read (ERRR), a retrieval augmentation framework designed to boost the effectiveness of RAG systems by optimizing queries to meet the specific knowledge needs of LLMs. (ii) We reveal that ERRR has a high degree of adaptability across diverse settings and data sources, as well as across various retrieval systems. (iii) We introduce a trainable scheme for ERRR, which not only achieves higher performance but also effectively reduces computational costs."
https://arxiv.org/html/2411.07773v1,Likelihood as a Performance Gauge for Retrieval-Augmented Generation,"Recent work finds that retrieval-augmented generation with large language models is prone to be influenced by the order of retrieved documents in the context. However, the lack of in-depth analysis limits the use of this phenomenon for prompt engineering in practice. In this study, we posit that likelihoods serve as an effective gauge for language model performance. Through experiments on two question-answering datasets with a variety of state-of-the-art language models, we reveal correlations between answer accuracy and the likelihood of the question at both the corpus level and the instance level. In addition, we find that question likelihood can also indicate the position of the task-relevant information in the context. Based on these findings, we propose two methods that use question likelihood as a gauge for selecting and constructing prompts that lead to better performance. We demonstrate their effectiveness with experiments. In addition, our likelihood-based methods are efficient, as they only need to compute the likelihood of the input, requiring much fewer language model passes than heuristic prompt engineering methods that require generating responses. Our analysis deepens our understanding of how input prompts affect model performance and provides a promising direction for efficient prompt optimization.111Our code is available at https://github.com/lyutyuh/poptimizer.","Figure 1: A prompt with higher question likelihood tends to lead to a better answer. Prompt designing is crucial for large language models (LMs) when tackling downstream tasks with retrieval-augmented generation (RAG, Lewis et al., 2020). Well-designed prompts can boost LMs’ performance and lead them to generate responses that better meet users’ expectations (Gao et al., 2021; Izacard et al., 2024; Liu et al., 2024; Schulhoff et al., 2024; Ma et al., 2024, inter alia). Typically, under the RAG framework, a prompt consists of three major components—an instruction defining a task and providing general guidance, a specific question222Formally, in an input prompt, we refer to the segment that directly conveys the question or query expected to be solved by the LM’s output as a question. In our experiments, this contains the entire question sentence, including the punctuation. of the task, and a context comprising a set of documents retrieved by retrievers from some external source (Karpukhin et al., 2020; Ni et al., 2022). Much previous work has explored empirical approaches of prompt engineering, such as manually designing prompts that mimic human reasoning Wei et al. (2023); Yao et al. (2023). Recently, Liu et al. (2024) have shown that LM performance is substantially affected by the order of the retrieved documents in the context: Namely, the answer accuracy peaks when the gold document333In factual QA tasks, the document containing the ground truth answer as a substring is referred to as a “gold document”. is placed at the beginning or the end of the context. While extensive experimental results have been presented for validating the existence of such a phenomenon, there is a lack of insights into the underlying mechanism driving it, limiting its applicability in prompt designing and optimization for real-world applications. Figure 2: Besides answer accuracy (Liu et al., 2024), we find that the likelihood of question simultaneously fluctuates in a U-shaped curve as the gold document position within the context changes. The log-likelihood is computed per token in the question, with LLaMA-3-8B. In this work, we contend that the likelihood assigned by the LM to a question preceded by a given retrieval-augmented context can provide useful information on forecasting the LM’s accuracy in answering that question. For verification, we conduct comprehensive experiments on two QA benchmarks (NQ-Open and ELI5) with a variety of state-of-the-art open LMs (LLaMA-2, LLaMA-3, LLaMA-3.1, Mistral-v0.3, MPT). We focus on three functional components in the input prompt and LM output—namely the context, the question, and the gold answer—and analyze their log-likelihood.444In information theory, the negative log-likelihood is also called surprisal as it quantifies how “surprising” a particular outcome is to the estimator. At the corpus level, we find that LMs can better respond to questions with higher log-likelihoods; at the instance level, contexts with favorable document orders that lead to higher likelihoods of a particular question are also more likely to elicit better answers as shown in Figure 1. Besides, we find that changing the position of relevant information in the input context simultaneously affects question likelihood and answer accuracy, as illustrated in Figure 2. Thus, question likelihood can be deemed both a performance gauge and a strong indicator of the position of useful task-relevant information in the input context. Based on these findings, we propose a promising direction for prompt optimization with two specific methods. The first directly takes the prompt that leads to the highest question likelihood during random document shuffling. The other uses question likelihood as a gauge for task-relevant information and reorders the documents within the context to get a better prompt. Experiments show that our methods improve answer accuracy on the two datasets for both instruction-tuned and base models. In addition to effectiveness, our method is efficient because it only employs the encoding function of pretrained LMs and computes the likelihood for each token in the prompt. Because LM encoding can be parallelized, the computation time for encoding can be vastly shorter than decoding an LM response.555Encoding, also known as prefill phase or prompt phase, requires significantly fewer LM passes than decoding. Previous work (Kwon et al., 2023; Zhong et al., 2024) reports that the throughput of processing input prompts, measured by the number of tokens processed per second, can be up to three orders of magnitude larger than that of completion generation. To our knowledge, our work is the first to present in-depth analyses of the relation between question likelihood and model performance under the RAG framework. Our contributions in this paper are summarized as follows: • We hypothesize and prove that question likelihood positively correlates with answer accuracy at corpus level on NQ-Open and ELI5. • We also demonstrate a strong instance-level correlation and verify its generality on the two datasets, based on our hypothesis. • We reveal that question likelihood is an indicator of the position of task-relevant information in the context. • We validate the effectiveness and efficiency of using question likelihood as a gauge for prompt optimization and demonstrate that likelihood-based prompt optimization is a promising direction for future study."
https://arxiv.org/html/2411.07656v1,Mitigating Bias in Queer Representation within Large Language Models: A Collaborative Agent Approach,"Large Language Models (LLMs) often perpetuate biases in pronoun usage, leading to misrepresentation or exclusion of queer individuals. This paper addresses the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns (""he,"" ""she"") when inclusive language is needed to accurately represent all identities. We introduce a collaborative agent pipeline designed to mitigate these biases by analyzing and optimizing pronoun usage for inclusivity. Our multi-agent framework includes specialized agents for both bias detection and correction. Experimental evaluations using the Tango dataset—a benchmark focused on gender pronoun usage—demonstrate that our approach significantly improves inclusive pronoun classification, achieving a 32.6 percentage point increase over GPT-4o in correctly disagreeing with inappropriate traditionally gendered pronouns (\chi^{2}=38.57,p<0.0001). These results accentuate the potential of agent-driven frameworks in enhancing fairness and inclusivity in AI-generated content, demonstrating their efficacy in reducing biases and promoting socially responsible AI.","The advancement of Large Language Models (LLMs) has significantly advanced natural language processing (NLP), enabling machines to generate human-like text and perform complex language tasks with notable proficiency [3, 9]. However, LLMs often inherit and amplify societal biases present in their training data, leading to the marginalization of underrepresented groups [4, 2]. Among these groups, the queer community faces unique challenges in AI representation, particularly concerning pronoun usage and gender identity [16, 6]. Existing bias mitigation techniques, such as data augmentation [17], debiasing algorithms, and fairness-aware machine learning models [8, 7] primarily focus on broader demographic categories like binary gender and race. These methods often fail to address the variation of queer identities, which involve the fluidity and diversity of gender expressions and the evolving language used [10, 16, 1]. Pronouns such as ""they,"" ""xe,"" ""ey,"" and ""fae"" are used by non-binary and transgender individuals but are often underrepresented or misinterpreted by LLMs [5]. Misgendering and exclusionary language can lead to perpetuating discrimination against queer individuals [11, 15]. Therefore, addressing queer bias in LLMs requires specialized approaches that account for the complexities of gender identity and pronoun usage. In this paper, we address the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns when inclusive language is needed. We introduce a collaborative agent pipeline designed to reduce biases in pronoun usage, thereby improving the representation of queer individuals in AI-generated content. Our multi-agent framework includes specialized agents for bias detection and optimization, focusing on pronoun inclusivity."
https://arxiv.org/html/2411.07611v1,Multimodal Clinical Reasoning through Knowledge-augmented Rationale Generation,"Clinical rationales play a pivotal role in accurate disease diagnosis; however, many models predominantly use discriminative methods and overlook the importance of generating supportive rationales. Rationale distillation is a process that transfers knowledge from large language models (LLMs) to smaller language models (SLMs), thereby enhancing the latter’s ability to break down complex tasks. Despite its benefits, rationale distillation alone is inadequate for addressing domain knowledge limitations in tasks requiring specialized expertise, such as disease diagnosis. Effectively embedding domain knowledge in SLMs poses a significant challenge. While current LLMs are primarily geared toward processing textual data, multimodal LLMs that incorporate time series data, especially electronic health records (EHRs), are still evolving. To tackle these limitations, we introduce ClinRaGen, an SLM optimized for multimodal rationale generation in disease diagnosis. ClinRaGen incorporates a unique knowledge-augmented attention mechanism to merge domain knowledge with time series EHR data, utilizing a stepwise rationale distillation strategy to produce both textual and time series-based clinical rationales. Our evaluations show that ClinRaGen markedly improves the SLM’s capability to interpret multimodal EHR data and generate accurate clinical rationales, supporting more reliable disease diagnosis, advancing LLM applications in healthcare, and narrowing the performance divide between LLMs and SLMs.","The widespread adoption of electronic health records (EHRs) in healthcare has significantly advanced deep learning techniques. EHRs include a variety of data types, such as medical notes, laboratory (lab) test results, and chest X-ray (CXR) images. These datasets are extensively employed in various healthcare applications, including disease diagnosis, mortality prediction, and drug discovery Niu et al. (2024a); Xu et al. (2018); Laghuvarapu et al. (2024). Our research focuses on using multimodal EHRs, particularly medical notes and lab test results, for disease diagnosis. Usually, disease diagnosis is treated as a classification problem. However, with the development of pre-trained language models, generative methods have gained popularity. These methods excel in identifying clinical relationships between EHR inputs and diagnoses, often surpassing most classification models (Niu et al., 2024a). Despite their effectiveness, these generative approaches lack a clear and intuitive rationale for interpreting diagnosed results, which is essential for trustworthy healthcare. The increasing use of prompt learning (Li and Liang, 2021; Niu et al., 2024b), chain-of-thought (COT) Wei et al. (2022), and instruction tuning (Wei et al., 2021; Chung et al., 2024) has significantly advanced the capabilities of Large Language Models (LLMs) in natural language processing (NLP). These models excel across various tasks and support their outputs with rationales (Touvron et al., 2023; Achiam et al., 2023). In healthcare, advanced medical LLMs such as MedPaLM (Singhal et al., 2023), a 540B model trained on diverse medical Q&A datasets and MEDITRON (Chen et al., 2023), which utilizes LLaMA2 for medical reasoning, have shown notable performance improvements in domain-specific applications. Similarly, ALPACARE (Zhang et al., 2023), built on LLaMA2 and trained through instruction tuning with a specialized dataset created by GPT-4, demonstrates the potential of targeted training in enhancing the effectiveness of medical LLMs. However, tuning such large-scale LLMs for specific tasks often faces practical challenges due to their substantial computation demands. Moreover, EHRs are typical multimodal data; these models only improve medical Q&A accuracy on the single EHR modality. They do not significantly advance the generation of multimodal clinical rationales, especially when time series EHR analysis is included. To improve practical applications of LLMs in downstream tasks, a novel distillation method for small language models (SLMs) called “Distilling Step-by-Step” has been developed (Hsieh et al., 2023) and adapted for healthcare research (Kwon et al., 2024; Kang et al., 2024). Despite these innovations, diagnosing diseases requires extensive medical expertise. Without detailed pre-training on specialized datasets and domain knowledge support, SLMs often struggle to provide accurate diagnoses and dependable clinical rationales, even with advanced rationale distillation techniques (Kang et al., 2024). In response, KARD (Kang et al., 2024) has developed an additional knowledge retriever that extracts pertinent information from medical queries to support clinical rationale generation. However, relying on a comprehensive knowledge base can reduce the efficiency advantages of SLMs and also not directly enhance the SLM rationale generation abilities. Furthermore, this distillation approach still faces challenges in processing multimodal EHRs and producing multimodal rationale effectively. To address these research gaps, we introduce ClinRaGen, a multimodal EHR understanding and disease diagnosis model that simultaneously provides supportive multimodal clinical rationales enhanced by clinical domain knowledge augmentation. We utilize the advanced capabilities of LLMs to infuse concise domain-specific medical knowledge into SLMs. Instead of limiting ourselves to single modality-based rationale distillation from LLMs (Kang et al., 2024; Kwon et al., 2024), we introduce a sequential multimodal rationale distillation paradigm with our model ClinRaGen to understand multimodal EHRs (textual medical notes and time series lab test results). Specifically, to better utilize external domain knowledge and effectively interpret these two modalities, we developed a knowledge-augmented attention module, which encodes time series lab test results into understandable representations for SLMs. The multimodal inputs will directly enrich the SLMs’ capabilities for more accurate disease diagnosis and clinical rationale generation. Our comprehensive evaluation on two pubic medical datasets shows that SLMs can effectively interpret multimodal EHR data and generate accurate disease diagnosis and multimodal rationales with efficacy comparable to LLMs. Our main contributions are summarized as follows: • ClinRaGen represents a state-of-the-art multimodal EHR understanding framework designed to deliver accurate disease diagnoses through consistent multimodal clinical rationale by leveraging the external domain knowledge and the power of LLMs. • We introduce a novel knowledge-augmented attention mechanism to encode time series EHRs in a SLM-understandable manner and a sequential multimodal rationale distillation paradigm, effectively integrating multimodal EHRs and improving their representation in generated rationales. • Our evaluations on the MIMIC-III (Johnson et al., 2016) and MIMIC-IV (Johnson et al., 2023) datasets demonstrate that ClinRaGen effectively interprets multimodal EHR data and produces comparable clinical rationales with LLMs, bridging the gap between LLMs and SLMs in healthcare applications."
https://arxiv.org/html/2411.07598v1,"Problem-Oriented Segmentation and Retrieval: 
Case Study on Tutoring Conversations","Many open-ended conversations (e.g., tutoring lessons or business meetings) revolve around pre-defined reference materials, like worksheets or meeting bullets. To provide a framework for studying such conversation structure, we introduce Problem-Oriented Segmentation & Retrieval (POSR)111Pronounced as “poser” (\textipa/\textprimstresspoz\textschwar/), a perplexing problem., the task of jointly breaking down conversations into segments and linking each segment to the relevant reference item. As a case study, we apply POSR to education where effectively structuring lessons around problems is critical yet difficult. We present LessonLink, the first dataset of real-world tutoring lessons, featuring 3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT® math problems. We define and evaluate several joint and independent approaches for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT), and large language models (LLMs) methods. Our results highlight that modeling POSR as one joint task is essential: POSR methods outperform independent segmentation and retrieval pipelines by up to +76\% on joint metrics and surpass traditional segmentation methods by up to +78% on segmentation metrics. We demonstrate POSR’s practical impact on downstream education applications, deriving new insights on the language and time use in real-world lesson structures.222Our code and dataset are open-sourced at https://github.com/rosewang2008/posr.","Across education, business, and science, many open-ended conversations like meetings or tutoring sessions are designed to address a set of pre-defined topics. As a prominent example, educators often shape their lessons around worksheet problems. Structuring lessons effectively is critical but challenging, as educators must allocate the right amount of time to different problems, while addressing different student learning needs (Haynes, 2010; Henderson, 1997; Panasuk and Todd, 2005). However, many novices or educators teaching large groups of students struggle with lesson structuring and often run out of time (Stradling and Saunders, 1993; Pozas et al., 2020; Deunk et al., 2018; Takaoglu, 2017; Hejji Alanazi, 2019). Figure 1: Problem-Oriented Segmentation and Retrieval (POSR) provides a framework for studying conversation structure around reference materials. For example, while conversations i,j discuss the same worksheet, POSR reveals that conversation i covers fewer problems than j but spends more time per problem. Providing evidence-based insights on lesson structuring is a key step towards addressing this challenge. These insights provide educators feedback on their teaching (Fishman et al., 2003; Kraft et al., 2018; Lomos et al., 2011; Desimone, 2009), tutoring platforms on training priorities (Hilliger et al., 2020; Gottipati and Shankararaman, 2018; Hilliger et al., 2022) and curriculum developers on material design (O’Donnell, 2008; Fullan and Pomfret, 1977). Unfortunately, obtaining insights on lesson structures at scale is challenging. The study of conversation structure around reference materials draws on concepts from two, typically distinct natural language processing (NLP) tasks: discourse segmentation to identify segments in the conversations and information retrieval (IR) to retrieve the relevant reference material for each segment. While each task has rich literature, studying them jointly reveals real-world challenges that existing works bypass. For example, discourse segmentation methods assume that conversations share the same structure (Ritter et al., 2010; Hearst and Plaunt, 1993; Chen and Yang, 2020), but education conversations have unique structures as teachers adapt their lessons to different needs. While prior IR work has studied supporting natural-language queries over conversations (Sanderson et al., 2010; Oard et al., 2004; Chelba et al., 2008), the reverse task of using open-ended conversation segments as queries for retrieving domain-specific reference materials has not received similar attention. To address these gaps, we make several key contributions. We define the Problem-Oriented Segmentation and Retrieval (POSR) task for jointly segmenting conversations and linking segments to relevant reference materials, such as worksheet problems (Figure 1). Unlike segmentation or retrieval alone, the joint POSR task reflects the realistic opportunities and challenges presented by knowing the potential reference topics (from the reference materials) for conversation segments. POSR provides a general framework for studying conversation structure around reference materials. As a case study, we apply POSR to the education setting. We contribute LessonLink, a novel dataset of real-world tutoring lessons featuring 3,500 segments, 116 SAT® math problems, and over 24,300 minutes of instruction. Our open-source dataset consists of real tutoring conversations paired with SAT® math worksheets, each conversation lasting about 1.5 hr long. Each conversation is segmented and each segment is linked with one of the 116 problems. To the best of our knowledge, this is the first dataset to include real-world conversations of unique structures linked with reference materials like worksheets. Evaluating POSR is challenging: Existing segmentation metrics do not measure time-weighed errors and existing metrics fail to reflect the subtle ways in which segmentation and retrieval errors interact. To address this, we contribute time-aware segmentation metrics adapted from standard line-based metrics (e.g., WindowDiff from Pevzner and Hearst (2002)) and introduce the Segmentation and Retrieval Score (SRS) to jointly measure segmentation and retrieval accuracy as the proportion of conversation where the retrieved item matches the ground truth. We define and evaluate a suite of segmentation, retrieval and POSR methods on LessonLink, including traditional segmentation methods like TextTiling (Hearst, 1997), popular IR methods like ColBERT (Khattab and Zaharia, 2020) and long-context large language models (LLMs) like Claude and GPT-4 Anthropic (2024); OpenAI (2024). Our results highlight the importance of POSR’s joint approach: POSR methods outperform independent segmentation and retrieval pipelines by up to +76\% on SRS metrics and traditional segmentation methods by up to +78% on segmentation metrics. However, several challenges remain. In domains with high privacy risks like education, companies are often unwilling to share data long-term due to privacy concerns. Moreover, while LLMs achieve strong POSR performance, their high API costs on long texts raise scalability concerns. Our findings motivate the need for more cost-effective, open-sourced methods that can deliver high accuracy on joint reasoning tasks like POSR. Finally, to further highlight the utility of POSR to real-world scenarios, we describe two novel applications of POSR to illustrate its potential for impacting evidence-based practices in education. First, through a linguistic analysis, we discover that tutors who spend more time on problems provide richer conceptual explanations. Tutors who spend less time provide procedural explanations. Second, POSR quantifies wide variability in how long tutors spend on the same problem. These examples point to opportunities for improving language and time-management practices."
https://arxiv.org/html/2411.07533v1,Large Language Models as Neurolinguistic Subjects: Identifying Internal Representations for Form and Meaning,"This study investigates the linguistic understanding of Large Language Models (LLMs) regarding signifier (form) and signified (meaning) by distinguishing two LLM assessment paradigms: psycholinguistic and neurolinguistic. Traditional psycholinguistic evaluations often reflect statistical biases that may misrepresent LLMs’ true linguistic capabilities. We introduce a neurolinguistic approach, utilizing a novel method that combines minimal pair and diagnostic probing to analyze activation patterns across model layers. This method allows for a detailed examination of how LLMs represent form and meaning, and whether these representations are consistent across languages. Our contributions are three-fold: (1) We compare neurolinguistic and psycholinguistic methods, revealing distinct patterns in LLM assessment; (2) We demonstrate that LLMs exhibit higher competence in form compared to meaning, with the latter largely correlated to the former; (3) We present new conceptual minimal pair datasets for Chinese (COMPS-ZH) and German (COMPS-DE), complementing existing English datasets.","Large Language Models (LLMs) have demonstrated remarkable reasoning, linguistic, arithmetic, and other cognitive abilities. The advent of LLMs has reignited cross-disciplinary discussions about what sorts of behavior are “intelligence”, even if the intelligence exhibited by LLMs may differ from human intelligence Sejnowski (2023). LLMs have drawn the attention of researchers from various fields, including linguistics, cognitive science, computer science and neuroscience, who investigate how LLMs develop and exhibit these capabilities. Figure 1: Illustration of LLMs prcocessing the same signified (meaning) across different signifiers (forms). Figure 2: Psycholinguistic vs. Neurolinguistic Paradigm. Both direct probability measurement and metalinguistic prompting can be considered as psycholinguistic methods, while minimal pair probing He et al. (2024) and other diagnostic probing are neurolinguistic. There is currently a heated debate about whether LLMs understand human language or whether their performance is simply the product of complex statistical relationships Mitchell and Krakauer (2023). Some assert that LLMs have demonstrated the potential for artificial general intelligence Bubeck et al. (2023). According to Manning (2022), optimism grows that emerging knowledge-imbued systems have exhibited a degree of general intelligence. Others remain skeptical Bender and Koller (2020). Browning and LeCun (2022) argue that LLMs are limited to a superficial understanding. Bender et al. (2021) suggests that LLMs cannot possess true understanding because they lack mental models of the world and experiential knowledge. A central aspect of this debate concerns the nature of LLMs’ linguistic representations. Using the semiotic framework of language proposed by De Saussure (1989), which distinguishes between the signifier (form) and the signified (meaning), we can inquire into the extent to which LLMs comprehend the form and meaning, and how do form and meaning intertwist with each other. Is LLM’s understanding of language meaning merely a statistical outcome based on their grasp of language form? When different languages express a shared concept with distinct forms, do LLMs create similar representations for these variations? How can we better understand the representations of form and meaning in these systems that support the observed patterns of performance? The underlying processes remain unclear due to the opaque nature of neural networks. Therefore, we need appropriate methods to assess their true linguistic understanding. Drawing inspirations from cognitive study on human language processing, we propose that assessment of LLMs can be divided into two primary paradigms: psycholinguistic and neurolinguistic. As illustrated in Figure 2, psycholinguistic paradigm measures the model’s output probabilities, directly reflecting the model’s behavior and performance. Neurolinguistic paradigm delves into the internal representations of LLMs. While prior studies have treated language models as psycholinguistic subjects Futrell et al. (2019), this research is the first to propose viewing LLMs as neurolinguistic subjects as well. When treating LLMs as psycholinguistic subjects, their responses often lean towards signifiers (form) due to their reliance on statistical correlations. This enables LLMs to produce structurally coherent language but not necessarily semantically accurate responses, as their “understanding” is shaped by statistical patterns rather than true conceptual processing Harnad (1990); Bender and Koller (2020); Nie et al. (2024), which makes psycholinguistic evaluation results might not accurately reflect the true linguistic understanding of LLMs. In contrast, examining LLMs as neurolinguistic subjects focuses on internal representations, which may transcend surface-level statistical biases. To achieve neurolinguistic assessing, we adapted the decoding probing method presented by He et al. (2024), which we refer to as “minimal pair probing” to distinguish it from conventional diagnostic probing Belinkov and Glass (2019); Belinkov (2022). Using this method, we can examine how LLMs represent both form and meaning in a fine-grained, layer-by-layer manner. In order to address questions about whether LLMs maintain consistent underlying representations of the same concept when form changes across multiple languages, we also create a multilingual minimal pair dataset (COMPS-ZH for Chinese and COMPS-DE for German). By evaluating LLMs in both psycholinguistic and neurolinguistic paradigm, our results reveal:1) Psycholinguistic and neurolinguistic results reveal very different patterns, suggesting both paradigms are necessary for a comprehensive understanding of LLMs. 2) LLMs demonstrate easier, earlier and better competence in language form than meaning. 3) As linguistic form varies across different languages, the LLMs’ understanding of meaning appears to shift accordingly with the meaning competence being linearly correlated to form. Results together suggest that signifier and signified in LLMs might not be independent to each other. Maintaining a conceptual representation might rely on statistical correlations based on linguistic form."
https://arxiv.org/html/2411.07527v1,Prompt-enhanced Network for Hateful Meme Classification,"The dynamic expansion of social media has led to an inundation of hateful memes on media platforms, accentuating the growing need for efficient identification and removal. Acknowledging the constraints of conventional multimodal hateful meme classification, which heavily depends on external knowledge and poses the risk of including irrelevant or redundant content, we developed Pen—a prompt-enhanced network framework based on the prompt learning approach. Specifically, after constructing the sequence through the prompt method and encoding it with a language model, we performed region information global extraction on the encoded sequence for multi-view perception. By capturing global information about inference instances and demonstrations, Pen facilitates category selection by fully leveraging sequence information. This approach significantly improves model classification accuracy. Additionally, to bolster the model’s reasoning capabilities in the feature space, we introduced prompt-aware contrastive learning into the framework to improve the quality of sample feature distributions. Through extensive ablation experiments on two public datasets, we evaluate the effectiveness of the Pen framework, concurrently comparing it with state-of-the-art model baselines. Our research findings highlight that Pen surpasses manual prompt methods, showcasing superior generalization and classification accuracy in hateful meme classification tasks. Our code is available at https://github.com/juszzi/Pen.","With the evolution of the internet, social media has emerged as the primary mode of communication, information sharing, and expressing opinions. The rise of social media has introduced a new multimodal entity – memes, comprised of images and short texts. While this form has gained popularity on social media networks, it has also become a tool for some users to disseminate hate speech, causing serious harm to vulnerable groups Piccoli et al. (2024). Due to the rapid dissemination of hateful memes, there is an urgent need to develop accurate classification methods. Figure 1 illustrates examples of hateful and non-hateful memes. To address this issue, early efforts emphasized the alignment and fusion across modalities to recognize hateful memes in social media Zhu (2020); Muennighoff (2020); Velioglu and Rose (2020). Recognizing the need for intricate reasoning and contextual background knowledge in determining hatred in memes, attempts were made to enhance model classification accuracy by integrating external tools Zhou et al. (2021) or incorporating additional external knowledge Lee et al. (2021) within the visual language model framework. Building upon this foundation, subsequent research began considering a modality transformation perspective. Cao et al. transformed meme images into image captions, employing prompting methods and introducing external knowledge to guide Pre-trained language models (PLM) in predictions Cao et al. (2022). In the latest work, building upon their prior work Cao et al. (2022), Cao et al. enhanced the quality of image Captions. They employed zero-shot visual question answering (VQA) with pre-trained vision-language models (PVLMs) for generating image captions Cao et al. (2023). This enhancement led to superior image caption quality, achieving a state-of-the-art results in the current domain. However, recent strategies for classifying hateful memes tend to emphasize improving model performance through the incorporation of additional external knowledge, potentially neglecting issues related to irrelevant or redundant content within such knowledgeLee et al. (2021); Blaier et al. (2021); Cao et al. (2022); Fang et al. (2022). For instance, incorporating image entity recognition information Lee et al. (2021) may introduce entities that are unrelated or redundant to hateful memes, thereby adding irrelevant details that could interfere with the model’s classification judgment. While some studies utilize prompting methods to guide PLM in leveraging external knowledge Cao et al. (2022), this approach predominantly focuses on the data processing stage. It enhances the contextual learning capabilities of language models for classification by introducing prompt template tokens and demonstrations of different categories to the original sequence. However, it does not comprehensively address the training conditions of the sequence in the feature space. Hence, our focus lies in extracting valuable information through a simple and effective network mechanism, enabling the PLM to adaptively select pertinent information for hateful meme classification. Existing prompt method guides PLM in classification by providing demonstrations corresponding to each label. Given the demonstrations for each label, there should be specific feature-level connections in the feature space between the contextual information of inference instances and the contextual information corresponding to the demonstrations of their correct labels. Building upon this concept, we extend prompt method into the feature space, introducing a novel framework called the Prompt-enhanced network for hateful meme classification (Pen). In this framework, we initially process the sequences of the input PLM with prompts, followed by region segmentation. We extract global information features from both inference instance and demonstration regions, incorporating the prompt-enhanced multi-view perception module. This module perceives the global information features of inference instances and demonstrations from multiple views to make hate emotion judgments, enhancing the model’s classification accuracy by effectively utilizing contextual information in input sequences. To better capture the relationships between hate and non-hate in the feature space, we introduce contrastive learning and adapt it to our framework, forming prompt-aware contrastive learning. This adaptation enhances the quality of the feature distribution for samples. In summary, the primary contributions of this paper are as follows: • We propose a model framework named Pen, which extends the prompt method into the feature space. By incorporating multi-view perception of inference instances and demonstrations in the feature space, Pen enhances hate classification accuracy, thereby improving the utilization of sequences. • We propose a contrasting learning method compatible with manual prompting to align and differentiate sample features used for hate judgment. This method sharpens the features of samples from different categories, thereby improving the accuracy of the classifier. • Through extensive ablation experiments conducted on two publicly available datasets, we validate the effectiveness of the prompt-enhanced framework, demonstrating its superiority over state-of-the-art baselines."
https://arxiv.org/html/2411.07521v2,Fair Summarization: Bridging Quality and Diversity in Extractive Summaries,"Fairness in multi-document summarization of user-generated content remains a critical challenge in natural language processing (NLP). Existing summarization methods often fail to ensure equitable representation across different social groups, leading to biased outputs. In this paper, we introduce two novel methods for fair extractive summarization: FairExtract, a clustering-based approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints. We evaluate these methods using Divsumm summarization dataset of White-aligned, Hispanic, and African-American dialect tweets and compare them against relevant baselines. The results obtained using a comprehensive set of summarization quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well as a fairness metric F, demonstrate that FairExtract and FairGPT achieve superior fairness while maintaining competitive summarization quality. Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that integrate quality and fairness into a single evaluation framework, offering a more nuanced understanding of the trade-offs between these objectives. This work highlights the importance of fairness in summarization and sets a benchmark for future research in fairness-aware NLP models.","Multi-document summarization, which condenses multiple documents into a concise summary, is a fundamental task in natural language processing (NLP). Summarization methods are typically either extractive, selecting the most important sentences, or abstractive, where the content is rephrased. Early research focused on summarizing formal text sources such as news articles. However, with the rise of social media, attention has shifted to summarizing user-generated content, which is diverse in style and language. Social media platforms bring together users from varied backgrounds, introducing linguistic diversity through informal language, slang, and emojis. This diversity raises the challenge of ensuring fairness in summarization ensuring balanced representation of various social groups. In social media, where public opinion is shaped, fair summaries are essential to include different perspectives and avoid underrepresentation of one or more social groups. In the context of social media, where millions of users contribute diverse perspectives, ensuring representation of this diversity in summaries becomes crucial. Social media platforms encompass a wide range of voices, including those from historically underrepresented or marginalized groups, making it essential that summarization methods capture this diversity fairly. Without proper representation, certain voices might be excluded or misrepresented, leading to biased summaries that skew public discourse (Binns,, 2017; Hutchinson and Mitchell,, 2018). The need for fairness in summarization is further heightened by the fact that user-generated content is often informal and marked by dialectal variations, requiring models to go beyond traditional summarization approaches (Pitsilis et al.,, 2018). Therefore, ensuring that all groups—across race, gender, and linguistic diversity—are fairly represented is critical for generating balanced summaries that reflect the diversity of public opinion (Dash et al.,, 2018). Despite advancements, bias remains a concern in automated summarization (Dash et al.,, 2019; Jung et al.,, 2019; Keswani and Celis,, 2021; Olabisi et al.,, 2022) as most existing summarization methods focus on quality but fall short in optimizing fairness. This gap leads to the key question: if a summarization method is optimized for fairness, how does it affect the overall summary quality? Previous studies suggest a trade-off between fairness and quality (Jung et al.,, 2019). Improving fairness can sometimes lower quality. While existing algorithms have made strides in balancing the two, none achieve perfect fairness. ChatGPT-EXT (Zhang et al.,, 2023) FairGPT (Ours) If you see on the news something about the Chicago Kitchen Clown Bandits then it will be referring me my friend Eten and I. Turns out not all White Castles are the same. Why do you push me away Chicago?! I mean I’m from Chicago. I’ll cheer for the Bears, but I’m a bigger 49ers fan. Is this new wave of Chicago Rap gonna be like the Hyphy movement? Don’t talk shot about Chicago, or those big shoulders will plow right into your little Boston ass. Nothing makes me happier than seeing the Bulls win #ChicagoBasketball #Bullieve. Don’t talk shot about Chicago, or those big shoulders will plow right into your little Boston ass. Nothing makes me happier than seeing the Bulls win #ChicagoBasketball #Bullieve. Truuu we tryna find sum to do too.. I dnt wanna b n Chicago if ain’t nobody here. Turns out not all White Castles are the same. Why do you push me away Chicago?! I mean I’m from Chicago. I’ll cheer for the Bears, but I’m a bigger 49ers fan. Is this new wave of Chicago Rap gonna be like the Hyphy movement? Table 1: Comparison of summaries by ChatGPT-EXT and FairGPT. Tweets from different groups are highlighted: Group 1 (e.g., White-aligned) and Group 2 (e.g., African-American). In this paper, we address two research questions: 1. How does achieving perfectly fair summaries affect overall quality? 2. How well do current methods perform when considering both fairness and quality? To illustrate the performance of fairness-aware summarization models, we compare summaries generated by ChatGPT-EXT (Zhang et al.,, 2023) and our proposed FairGPT model on a sample instance from Divsumm dataset (Olabisi et al.,, 2022). As shown in Table 1, FairGPT ensures equal representation of tweets from different groups, while ChatGPT-EXT shows a slight imbalance. We make the following contributions: • We propose FairExtract, a fair clustering-based extractive summarization method that achieves perfect fairness and is evaluated against baseline models using standard and composite quality-fairness metrics. • We develop FairGPT, a large language model-based extractive summarization method that enforces fairness through equal representation and accurate content extraction using the longest common subsequence. • We introduce composite metrics combining normalized quality scores with fairness, providing a comprehensive analysis of the quality-fairness trade-off in summarization models."
https://arxiv.org/html/2411.07494v1,Rapid Response: Mitigating LLM Jailbreaks with a Few Examples,"As large language models (LLMs) grow more powerful, ensuring their safety against misuse becomes crucial. While researchers have focused on developing robust defenses, no method has yet achieved complete invulnerability to attacks. We propose an alternative approach: instead of seeking perfect adversarial robustness, we develop rapid response techniques to look to block whole classes of jailbreaks after observing only a handful of attacks. To study this setting, we develop RapidResponseBench, a benchmark that measures a defense’s robustness against various jailbreak strategies after adapting to a few observed examples. We evaluate five rapid response methods, all of which use jailbreak proliferation, where we automatically generate additional jailbreaks similar to the examples observed. Our strongest method, which fine-tunes an input classifier to block proliferated jailbreaks, reduces attack success rate by a factor greater than 240 on an in-distribution set of jailbreaks and a factor greater than 15 on an out-of-distribution set, having observed just one example of each jailbreaking strategy. Moreover, further studies suggest that the quality of proliferation model and number of proliferated examples play an key role in the effectiveness of this defense. Overall, our results highlight the potential of responding rapidly to novel jailbreaks to limit LLM misuse.","**footnotetext: Work started during MATS. 1 Anthropic 2 New York University 3 MATS. Correspondence to {alwin,mrinank}@anthropic.com As Large Language Models (LLMs) become more capable, they pose greater misuse risks. Indeed, the potential for catastrophic misuse of LLMs has motivated AI labs to make public commitments to developing safeguards to minimize the risk of such misuse (Anthropic, 2023; OpenAI, 2023). Additionally, such concerns have motivated substantial effort from the research community to defend against jailbreaks, which are techniques that extract harmful information from LLMs trained to be helpful, harmless, and honest (Bai et al., 2022b; Xie et al., 2023; Xu et al., 2024). Despite ongoing research, ensuring that large language models (LLMs) are robustly resistant to jailbreaking remains an unsolved challenge (Hendrycks et al., 2021; Ziegler et al., 2022). Even state-of-the-art methods that substantially improve robustness, such as representation rerouting (Zou et al., 2024), have been publicly broken within hours of release. The situation could worryingly parallel that of adversarial robustness in computer vision, where new defenses are often defeated by attacks available before their development with proper tuning (Tramer et al., 2020). Indeed, in computer vision, a decade of work and thousands of papers have yielded “limited progress” (Carlini, 2024). If we cannot design AI systems that are robust to persistent jailbreaking attempts, how can we safely deploy highly capable LLMs? In this work, we thus propose Jailbreak Rapid Response as an alternative paradigm for mitigating LLM misuse (Fig. 1). Traditional approaches aim to develop highly robust static systems that resist all possible jailbreaks. In contrast, jailbreak rapid response emphasizes effectively monitoring for novel jailbreaks and quickly defending against those jailbreaks after observing them. Figure 1: Comparison of traditional robustness and rapid response for mitigating LLM jailbreaking. Traditional adversarial robustness aims to develop a highly robust static system that resists all possible jailbreak attempts. However, even state-of-the-art defenses are often quickly defeated by persistent attackers. In contrast, rapid response emphasizes effective monitoring to quickly detect novel jailbreaks, and then rapidly adapting the system to defend against detected attacks. To assess the feasibility of jailbreak rapid response, we introduce a new benchmark: RapidResponseBench. Our benchmark measures the effectiveness of different rapid response techniques in protecting against novel jailbreak attacks. The benchmark includes six jailbreaking attack strategies. For each strategy, we allow a jailbreak defense method to observe a few successful instances of the attack and measure the attack success rate (ASR) of new attempts as the number of observed jailbreak examples increases. We also test out-of-distribution (OOD) variants of each attack strategy, to simulate real-world jailbreakers adapting existing attacks to new defenses. Moreover, we measure the refusal rate on benign queries as the system adapts to novel jailbreaks on WildChat (Zhao et al., 2024). This allows us to evaluate how well rapid response techniques generalize to novel jailbreak attempts, and further how these defenses affect the refusal rate on benign queries. We then evaluate five baseline rapid response techniques using RapidResponseBench. We apply these techniques to input-guarded language models, which check the input for potential jailbreaking attempts before processing it. Our approach uses jailbreak proliferation, a data augmentation method that generates many similar examples from a small set of observed jailbreaks. In particular, we find that fine-tuning an input-guarded language model on this proliferated data reduces the attack success rate (ASR) by an average of 99.6% on in-distribution attacks and 93.6% on out-of-distribution attacks across various models, using only one example from each jailbreak attack category. This shows the effectiveness of our rapid response techniques in mitigating jailbreaking attempts having observed only a small number of attacks using a given jailbreaking strategy. Following this, we conduct an analysis to better understand the impact of different components on the effectiveness of jailbreak rapid response. We vary the number of observed jailbreak examples, the language model used for generating additional jailbreak examples (proliferation), and the number of generated examples per observed jailbreak. We find that while most defenses improve when observing more jailbreak examples, the strongest defense is the one whose performance scales best as more resources are invested in jailbreak proliferation. Increasing the capability of the proliferation model yields only modest gains in jailbreak defense, but generating more examples per observed jailbreak has a dramatic positive impact. These results highlight the importance of proliferation in rapid response and suggest further improvements could be made with improved proliferation. Having demonstrated the promise of jailbreak rapid response on RapidResponseBench, we then consider different factors that affect whether rapid response is an appropriate strategy for mitigating real-world catastrophic misuse. In particular, we highlight the role of timely jailbreak identification and response, the quality of the rapid response method, and the misuse threat model. While frontier AI labs can influence some of these factors, details of the threat model are harder to influence. As such, further research is needed to understand precisely how LLM misuse occurs. Overall, our work highlights jailbreak rapid response as a potentially promising new paradigm for mitigating misuse risks from large language models. With further research to better understand threat models, improve real-time jailbreak detection, and improve rapid response and proliferation methods, this approach offers a promising alternative to static adversarial defense. Our benchmark is open source and we hope others improve upon our baseline results.111https://github.com/rapidresponsebench/rapidresponsebench"
https://arxiv.org/html/2411.07474v1,Controlled Evaluation of Syntactic Knowledge in Multilingual Language Models,"Language models (LMs) are capable of acquiring elements of human-like syntactic knowledge. Targeted syntactic evaluation tests have been employed to measure how well they form generalizations about syntactic phenomena in high-resource languages such as English. However, we still lack a thorough understanding of LMs’ capacity for syntactic generalizations in low-resource languages, which are responsible for much of the diversity of syntactic patterns worldwide. In this study, we develop targeted syntactic evaluation tests for three low-resource languages (Basque, Hindi, and Swahili) and use them to evaluate five families of open-access multilingual Transformer LMs. We find that some syntactic tasks prove relatively easy for LMs while others (agreement in sentences containing indirect objects in Basque, agreement across a prepositional phrase in Swahili) are challenging. We additionally uncover issues with publicly available Transformers, including a bias toward the habitual aspect in Hindi in multilingual BERT and underperformance compared to similar-sized models in XGLM4.5B.","There is a substantial body of work dedicated to evaluating the linguistic knowledge of language models. Popular evaluation methodologies include: • probing, i.e., predicting linguistic properties from a network’s internal activations Giulianelli et al. (2018); • classifying sentences as grammatically acceptable or unacceptable Warstadt et al. (2019); • targeted syntactic evaluation (TSE), a method based on comparing LM-assigned probabilities of minimally different sequences Marvin and Linzen (2018). To date, most of the research investigating the linguistic knowledge of LMs has concentrated on high-resource languages such as English (Lin et al., 2019; Warstadt et al., 2020; Hu et al., 2020), German (Mueller et al., 2020; Zaczynska et al., 2020), Spanish (Pérez-Mayos et al., 2021; Bel et al., 2024), Italian (Trotta et al., 2021; Miaschi et al., 2022), Chinese (Wang et al., 2021; Xiang et al., 2021; Zheng and Liu, 2023), and Japanese (Futrell et al., 2018; Someya and Oseki, 2023; Someya et al., 2024). However, efforts have also been made to include less prominent languages. Notably, Torroba Hennigen et al. (2020) and Stanczak et al. (2022) probed masked LMs for morphosyntactic attributes of words across 36 and 43 languages, respectively. Acceptability benchmarks have been developed for North Germanic languages by Volodina et al. (2021), Jentoft and Samuel (2023), Nielsen (2023), and Zhang et al. (2024). TSE has been applied to Hebrew Gulordava et al. (2018), Norwegian Kobzeva et al. (2023), and Indonesian and Tamil Leong et al. (2023). We believe that evaluating LMs’ linguistic knowledge across a diverse range of languages is crucial for developing a comprehensive picture of how they form linguistic generalizations. Assessing ‘off-the-shelf’ LMs in lower-resourced languages offers a further benefit of diagnosing limitations and challenges these models may face due to issues like insufficient training data, model biases, or difficulties in capturing particular linguistic features. We recognize TSE’s advantage of focusing on one combination of linguistic phenomenon and sentence structure at a time, which enables a fine-grained analysis of how performance depends on the structure and complexity of input sentences. Therefore, we conduct three TSE case studies benchmarking publicly available Transformer LMs on distinctive morphosyntactic phenomena in low-resource languages—auxiliary verb agreement in Basque, split ergativity in Hindi, and noun class agreement in Swahili. We find that LMs mostly do well on agreement in Basque, with errors linked to the presence of an indirect object in a sentence, and almost always succeed in selecting the correct aspectual form of the verb based on the presence or absence of the ergative clitic in Hindi, with the exception of multilingual BERT, which prefers the habitual aspect regardless of its grammaticality. However, LMs struggle to agree predicates with the noun class of their subjects in Swahili. Performance on our tests has a positive relationship with model size, but XGLM4.5B systematically underperforms similar-sized models for reasons possibly including the lack of low-resource upsampling and the ‘curse of multilinguality’. Syntactically complex attractor phrases weaken performance in Swahili but not in Hindi."
https://arxiv.org/html/2411.07466v1,: A Challenging Mention Resolution Benchmark for LLMs,"Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models’ referential understanding. To address this, we introduce \dataset, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. \datasetfeatures long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained analysis of model performance. We evaluate both closed- and open-source LLMs on \datasetand observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones. We observe that pronominal mentions, which have limited surface information, are typically much harder for models to resolve than nominal mentions. Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy, highlighting the strong referential capabilities of state-of-the-art LLMs while also indicating room for further improvement.111Code for the paper is available at https://github.com/KawshikManikantan/IdentifyMe","Figure 1: Sample instance from the validation set of \dataset. The mention of interest is highlighted in the text. The answer options include frequently occurring entities in the text, and None of the Above. Coreference resolution (CR) is a fundamental task for text comprehension. While LLMs have made tremendous strides on a wide array of NLP tasks Brown et al. (2020), their performance on CR has been relatively underwhelming, with models struggling at even mention detection Le and Ritter (2023); Manikantan et al. (2024). Through extensive analysis, recent work by Gan et al. (2024) has identified that LLMs’ excellent referential understanding is underestimated in the typical CR setup due to the span-based output format being ill-suited for LLMs. They suggest adapting CR datasets and task metrics to support LLM evaluations. Along these lines, we create the \datasetbenchmark for mention resolution in an MCQ format, commonly used for LLM evaluations Hendrycks et al. (2021). To construct the benchmark, we use annotations from two long-text coreference benchmarks, namely LitBank Bamman et al. (2020) and FantasyCoref Han et al. (2021). To make the benchmark challenging, we restrict it to pronominal and nominal mentions and apply some heuristics for each mention type to filter out easily resolvable cases (Section 2.1). Each MCQ instance consists of text marked with the mention of interest, and the choices consist of frequently occurring entities in the text and the None of the Above (NoA) option. Figure 1 shows an example in \dataset, derived from LitBank. We evaluate both closed- and open-source models. On average, among the mention types, LLMs perform worse on pronominal mentions, which have limited surface information, than nominal mentions. The instances where None of the Above is the correct answer prove particularly challenging for all the models, with open-source models seeing a drop of more than 50%. With nested mentions, LLMs tend to frequently confuse between entities with overlapping mentions (e.g., his mother). The highest-scoring model GPT-4o scores 81.9% on \dataset, highlighting the strong performance of frontier LLMs while also indicating scope for further improvement in referential capabilities."
https://arxiv.org/html/2411.07457v1,: Decoding Prompts Reduces Hallucinations when Large Language Models Meet False Premises,"While large language models (LLMs) have demonstrated increasing power, they have also called upon studies on their hallucinated outputs that deviate from factually correct statements. In this paper, we focus on one important scenario of false premises, where LLMs are distracted by misaligned claims although the model possesses the required factual knowledge to answer original questions accurately. Inspired by the observation that entropy of the false-premise prompt is closely related to its likelihood to elicit hallucination generation, we propose a new prompting algorithm, named DecoPrompt, to mitigate hallucination. DecoPrompt leverages LLMs to “decode” the false-premise prompts without really eliciting hallucination output from LLMs. We perform experiments on two datasets, demonstrating that DecoPrompt can reduce hallucinations effectively on outputs from different LLMs. Moreover, DecoPrompt exhibits cross-model transferability, which facilitates its applications to scenarios such as LLMs of large sizes or unavailable model logits 111Codes are available at https://github.com/xunannancy/DecoPrompt.","Figure 1: Examples of false-premise questions from existing and our work and responses from Llama2-13b-chat. We focus on topics that LLMs already possess the required world knowledge to better study resilience of LLMs against false premises. In spite of providing fluent and coherence responses to user questions (Touvron et al., 2023a; Achiam et al., 2023; Team et al., 2023), large language models (LLMs) tend to hallucinate outputs that deviate from 1) the source input provided by users, e.g., input documents from machine translation (Guerreiro et al., 2023) and summarization (Shi et al., 2023b), 2) previously generated information by themselves (Liu et al., 2023; Shi et al., 2023a), or 3) established world knowledge (Chuang et al., 2023). Considering the comprehensive training and broad versatility of LLMs, it remains an open question for hallucination evaluation and mitigation (Zhang et al., 2023b; Huang et al., 2023). Previous research (Vu et al., 2023; Varshney et al., 2023) shows that current LLMs struggle on false premises, which include questions whose premises are factually incorrect and thus have to be rebutted. We show one such question and hallucination example at the top of Figure 1. Unfortunately, popular strategies such as few-shot prompting (Brown et al., 2020) and Chain-of-Thought (Wei et al., 2022) increase hallucination, while adding an explicit false-premise check (Vu et al., 2023) in the prompt hurts performance on questions with valid premises. To further unveil the mystery of LLM hallucinations against false premises, we focus on one important scenario where LLMs are distracted by misaligned claims while the original question can be answered accurately relying on factual knowledge learned during pre-training and stored implicitly within the model parameters. Taking the fictitious ‘‘ZeroGravity Boots’’ at the bottom of Figure 1 as an example: although the studied LLM can identify the factual incorrectness in the user’s question, it still generates a hallucinated response when the fictitious description is intended to denote a fabricated person. In addition, in this paper, we concentrate on questions that LLMs can answer correctly without any distraction. This allows us to investigate resilience of LLMs against false premises without interference from other counterfactors such as lacking relevant knowledge (Zheng et al., 2023b; Wu et al., 2023) or internalizing false knowledge (Dziri et al., 2022; Penedo et al., 2023), as compared in Figure 1. To mitigate hallucination from false premises, we propose a new prompting algorithm, named DecoPrompt, which pays particular attention to the entropy-based uncertainty of false-premise user questions, based on the observation that the entropy of the false-premise prompt is closely related to its likelihood of resulting in hallucination generation (see Figure 2). DecoPrompt is directly inspired from prior hallucination mitigation practices of filtering out or rectifying model outputs with high logit-based uncertainty (e.g., token-level probability or entropy) estimation (Xiong et al., 2023; Pezeshkpour, 2023; Varshney et al., 2023). Unlike generation-time mitigation strategies such as contrastive decoding (Shi et al., 2023b; Chuang et al., 2023) or output uncertainty exploitation (Pezeshkpour, 2023; Varshney et al., 2023), DecoPrompt leverages LLMs to “decode” the false-premise prompts without really eliciting hallucination output from LLMs. Figure 2: Entropy-based uncertainty distribution along with resulted generation hallucination percentages. Each dot represents a cluster of false-premise questions with the instance counts annotated that share a similar level of uncertainty. We evaluate LLMs of different sizes from two families on two false-premise datasets: Fictitious consists of fictitious descriptions intended to denote fabricated person names and Authorship mismatches celebrities to irrelevant books. Though recognizing that the fictitious descriptions are factually incorrect and aware of the actual authors of queried books, we observe that studied LLMs still suffer from high hallucinations when prompted with misaligned false-premise prompts. Fortunately, the proposed DecoPrompt helps reduce hallucinations effectively on both datasets and exhibits strong transferability, which addresses challenges such as prohibitively costly calculation of entropy for LLMs of large sizes or inaccessible logits from proprietary LLMs."
https://arxiv.org/html/2411.07417v1,"Untangling Hate Speech Definitions: 
A Semantic Componential Analysis Across Cultures and Domains","Hate speech relies heavily on cultural influences, leading to varying individual interpretations. For that reason, we propose a Semantic Componential Analysis (SCA) framework for a cross-cultural and cross-domain analysis of hate speech definitions. We create the first dataset of definitions derived from five domains: online dictionaries, research papers, Wikipedia articles, legislation, and online platforms, which are later analyzed into semantic components. Our analysis reveals that the components differ from definition to definition, yet many domains borrow definitions from one another without taking into account the target culture. We conduct zero-shot model experiments using our proposed dataset, employing three popular open-sourced LLMs to understand the impact of different definitions on hate speech detection. Our findings indicate that LLMs are sensitive to definitions: responses for hate speech detection change according to the complexity of definitions used in the prompt.","The infeasibility of formulating a universally accepted definition for hate speech and other related concepts (such as toxic language, cyberbullying, and misogyny) is a much discussed topic that permeates not only Natural Language Processing (NLP) research Fortuna et al. (2020); Khurana et al. (2022); Pachinger et al. (2023); Korre et al. (2023); Nghiem et al. (2024) but also expands into the legal and social science fields Maussen and Grillo (2014); Flick (2020); Zufall et al. (2022); Guillén-Nieto (2023). The lack of a clear definition due to cultural diversity hinders the development of models, as it is unclear what criteria they should be trained to detect. For instance, consider two definitions, A and B, where only A covers sexual orientation and political opinion criteria. The statement “Collectivists are Faggots” should be labeled as hate speech according to A, and as not hate according to B since B lacks the above-mentioned criteria. Cultural perspectives influence how hate speech is perceived; datasets consist of statements produced by individuals within a culture, so the biases reflect, to some extent, the values, norms, and ethics of that culture Bagga and Piper (2020); Hershcovich et al. (2022). Since most NLP research focuses on English-language data Søgaard (2022), this cultural dimension is often overlooked, resulting in biases that favor English-speaking cultures. Current NLP approaches are not adequately equipped to address the cultural dependency of hate speech. Existing monolingual hate speech classifiers often lack cultural awareness Lee et al. (2024). Prevailing hate speech taxonomies tend to focus more on legal or academic definitions rather than incorporating cultural dimensions, a gap that can prove detrimental, as hate speech per se and hate speech regulation might influence societal discourse, relationships, and cultural norms, potentially shaping how people interact and express themselves Hietanen and Eddebo (2023). Figure 1: HateDefCon creation pipeline. Inspired by the compositionality principle Hinzen et al. (2012), we introduce a component-annotated resource for hate speech definitions, the HateDefCon dataset. Figure 1 shows our HateDefCon creation pipeline. We propose a Semantic Componential Analysis (SCA), in which we define a hate speech definitional component as a fundamental element or criterion used to define what constitutes hate speech in terms of target, intention/purpose, and act/means. We define definitional hate speech domains as the contexts where hate speech definitions emerge. This study focuses on five such domains: legislation, Wikipedia, online dictionaries, research papers, and conduct policies from online platforms or technological companies. In addition, we look at the cultural representation of hate speech. We use culture, as the term that encompasses language, ideas, beliefs, customs, codes, institutions, tools, techniques, among other elements.111https://www.britannica.com/topic/culture We highlight the need for cross-cultural and cross-domain approaches to define hate speech and argue that such definitions should be context-specific, be it cultural, legal, or academic. We advocate for grounding hate speech definitions within these particular domains. This is in line with best practices for tackling subjective tasks Rottger et al. (2022), in which the guideline - or the definition - chosen should consider the downstream use, i.e. the context. In this work, we focus on three research questions: (1) What are the differences among various definitions of hate speech? (2) What is the diversity of these definitions? (3) How do definitions with different components affect LLM predictions? Our contributions are both theoretical and practical. On the theoretical front, our cross-cultural and cross-domain analysis of definitions shows significant variation in components, ranging from broad definitions to highly specific ones. Even among the more detailed definitions, which address aspects like the target of hate speech, the intent, and the methods of expressing it, there are differences in their components. On the practical side, we assess whether LLMs respond better to certain definitions, potentially revealing underlying biases. Our results reveal that definitions vary in their components while domains also borrow definitions from one another. When research is culturally specific, borrowing from other domains can be problematic, as it may introduce elements that do not align with the intended cultural context. The contribution of this paper is threefold: • We propose a semantically informed framework and use it to create HateDefCon, the first resource for hate speech definitions. • We conduct a cross-domain and cross-cultural analysis of hate speech definitions and their components. • We assess whether LLMs exhibit a preference for certain definitions, potentially revealing underlying biases."
https://arxiv.org/html/2411.07407v1,Using Generative AI and Multi-Agents to Provide Automatic Feedback,"[Abstract]This study investigates the use of generative AI and multi-agent systems to provide automatic feedback in educational contexts, particularly for student constructed responses in science assessments. The research addresses a key gap in the field by exploring how multi-agent systems, called AutoFeedback, can improve the quality of GenAI-generated feedback, overcoming known issues such as over-praise and over-inference that are common in single-agent large language models (LLMs). The study developed a multi-agent system consisting of two AI agents: one for generating feedback and another for validating and refining it. The system was tested on a dataset of 240 student responses, and its performance was compared to that of a single-agent LLM. Results showed that AutoFeedback significantly reduced the occurrence of over-praise and over-inference errors, providing more accurate and pedagogically sound feedback. The findings suggest that multi-agent systems can offer a more reliable solution for generating automated feedback in educational settings, highlighting their potential for scalable and personalized learning support. These results have important implications for educators and researchers seeking to leverage AI in formative assessments, offering a pathway to more effective feedback mechanisms that enhance student learning outcomes.","Automatic feedback strengthens teachers’ capacity by offering tailored, real-time information to support student learning, with the potential to improve students’ learning experience and outcomes \parencitehahn2021systematic. Automatic feedback is especially valuable in online learning environments \parencitechauhan2014massive, cavalcanti2021automatic and classroom formative assessment practices, as it addresses the challenges that instructors have limited time to provide individualized feedback simultaneously to students with diverse needs. Research has consistently reported the positive effects of automatic feedback, including increased student engagement, improved learning outcomes, and reduced teachers’ bias \parencitehahn2021systematic,kochmar2020automated, razzaq2020effect. With the development of Artificial Intelligence (AI) techniques, particularly generative AI (GenAI) such as ChatGPT, many argue that teachers can leverage these tools to automatically generate natural-sounding and context-specific feedback for more complex tasks, which holds the potential to significantly advance student learning \parenciteguo2024artificial,du2024harnessing,fokides2024comparing,guo2024resist. However, GenAI-powered automatic feedback also demonstrated limitations\parencitejansen2024comparing,scarlatos2024improving. Among the many drawbacks of GenAI-generated feedback, over-praise and over-inference can be detrimental to student learning (Authors, under review). Over-praise occurs when the feedback is excessively positive, even if the students’ responses are incorrect or meaningless. Over-inference refers to a misalignment between the student’s actual performance and the intepretation in the feedback, making the feedback inaccurately matched to the student’s work. These two issues can convey misleading information that undermines students’ learning, decreasing learning motivation and fostering inauthentic self-esteem, ultimately impairing opportunities to improve learning \parencitelee2017understanding,irvin2one. To address the issues of over-praise and over-inference, this study developed a multi-agent feedback system (AutoFeedback) to generate quality automatic feedback for students’ written responses in science assessments, with one agent to generate and another to examine and revise feedback. The two agents in the multi-agent system interact with each other, playing different roles and serving varying functions. Compared to the single agent, multi-agents are more powerful for addressing complex problems due to the scalability, robustness, reliability as a whole, and specialization and heterogeneity per agent. Therefore, this approach holds the potential to help automatically provide better feedback. We tested AutoFeedback’s performance with 240 students’ constructed responses and compared the feedback quality with that of a single agent. The study answers two questions: 1) How frequently does a single GenAI agent generate feedback with over-praise and over-inference? 2) To what degree does the multi-agent system AutoFeedback improve feedback in terms of reducing over-praise and over-inference?"
https://arxiv.org/html/2411.07404v1,"Controllable Context Sensitivity
and the Knob Behind It","When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context ( Paris is in England) and a question ( Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model’s performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior.","Language models are often prompted with a query and preceding context, e.g., in settings of in-context learning, retrieval-augmented generation, or document analysis. In such scenarios, the language model needs to integrate information from both the context and its prior knowledge stored in its parameters. In some cases, we may prefer the model to rely more on the context, e.g., to avoid hallucinating responses that may be based on outdated prior knowledge (Zhang et al., 2023); however, in other cases, we may prefer the model to rely more on its prior knowledge, e.g., to avoid being misled by misinformation provided in the context (Hong et al., 2024). As a motivating example, consider a document analysis setting in which a language model is asked to help understand an opinion article in a newspaper. It might first be asked to summarize, e.g., What is the main argument of this article?. In this case, the model should rely heavily on the context, i.e., the text of the article. Then, one might ask: What are some criticisms of this argument?. To answer this, the model ought to be skeptical; an opinion article may be written very authoritatively as if its arguments are established fact, or it may make some misleading claims to support its argument. To answer this critically, the language model must draw more upon its prior knowledge of the issue and related opinions than blindly following the context. More broadly, because the degree of context sensitivity depends highly on the use case, it would be desirable to be able to specify how much and whether the model should be influenced by the context versus its prior knowledge. Studies on the tension between context and prior knowledge have primarily focused on the setting of knowledge conflicts (Longpre et al., 2021), in which a given context directly contradicts information assumed to be in a model’s prior knowledge about a given query. For example, a language model trained on a sufficient amount of data should be able to reply to the query What’s the capital of France? with Paris. However, if the context The capital of France is London. is prepended to the query, the model needs to decide whether to respond based on the context ( London) or its prior knowledge ( Paris). Prior studies (Longpre et al., 2021; Du et al., 2024; Monea et al., 2024; Ortu et al., 2024) have shown that models will prefer drawing from context for some questions and prior knowledge from others; however, the exact manner by means of which the model selects among the sources is not well-understood. To this question of how, we hypothesize that there is a simple fundamental mechanism within the language model that facilitates the binary decision of whether to rely on the context or the prior knowledge. To guide our search for such a mechanism, we design and execute a structured recipe. First, we create the controllable context sensitivity (CCS) task which augments the standard knowledge-conflict setting with an intent, such as Ignore the context or Listen to the context. By disambiguating whether the model should follow context or prior knowledge through a simple addition to the prompt, we are able to identify and evaluate its behavior in both modes for the same context–query pair. We adapt models for this task using fine-tuning and in-context learning, then evaluate them on in-domain and out-of-domain test sets to assess whether they have developed a deeper ability to choose between context and prior knowledge beyond surface-level heuristics. In our case study on the Llama-3.1-8B family (Dubey et al., 2024), we find that both fine-tuning and in-context learning are moderately effective, with models excelling on in-domain test sets and significantly improving over zero shot baselines on out-of-domain test sets. Armed with models that can perform the CCS task reasonably well, we then explore the mechanisms that facilitate their behavior in this task. We hypothesize that for a model to solve this task, it must execute at least three high-level steps (not in any particular order): extracting an answer from prior knowledge, extracting an answer from the context, and deciding to answer with the context answer or the prior answer. We then seek to identify layers that may contain the model’s computations that are aligned with each step. To do so, we develop an algorithm that uses tools from mechanistic interpretability to find a targeted subset of layers at which activation patching (Meng et al., 2022) can switch a model from preferring the answer in the context to preferring the answer in its prior knowledge and vice versa. Then, building on ideas from distributed alignment search (Geiger et al., 2024), we identify a knob for the model’s decision between following context or prior in the form of a one-dimensional subspace. Despite locating such a knob on an instruction-tuned model fine-tuned on this task that states explicit intents, we show that it is even effective on non-finetuned and base models of the same family for prompts that do not state the intent. Furthermore, we show strong evidence that for models good at the CCS task, the two intents correspond to two distinct values in that subspace, while bad models fail to exhibit this distinction. We repeat this process for Gemma-2 9B and Mistral-v0.3 7B to find a similar story. Our results suggest that, across many types of large language models, one fundamental mechanism represented by a value in a one-dimensional subspace facilitates their ability to decide between following the context or its prior knowledge. These findings move toward developing more robust language models with controllable levels of reliance on context and prior knowledge. They further highlight how investigating models at a mechanistic level can yield high-quality interventions to control a model’s behavior."
https://arxiv.org/html/2411.07398v1,Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical Concern-related App Reviews,"With the increasing proliferation of mobile applications in our everyday experiences, the concerns surrounding ethics have surged significantly. Users generally communicate their feedback, report issues, and suggest new functionalities in application (app) reviews, frequently emphasizing safety, privacy, and accountability concerns. Incorporating these reviews is essential to developing successful products. However, app reviews related to ethical concerns generally use domain-specific language and are expressed using a more varied vocabulary. Thus making automated ethical concern-related app review extraction a challenging and time-consuming effort.This study proposes a novel Natural Language Processing (NLP) based approach that combines Natural Language Inference (NLI), which provides a deep comprehension of language nuances, and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract ethical concern-related app reviews at scale. Utilizing 43,647 app reviews from the mental health domain, the proposed methodology 1) Evaluates four NLI models to extract potential privacy reviews and compares the results of domain-specific privacy hypotheses with generic privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to privacy concerns; and 3) Uses the best NLI and LLM models further to extract new privacy reviews from the dataset. Results show that the DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the classification of app reviews. Then, using NLI+LLM, an additional 1,008 new privacy-related reviews were extracted that were not identified through the keyword-based approach in previous research, thus demonstrating the effectiveness of the proposed approach.","Mobile applications are created with specific user goals in focus [1]. A user goal can be defined as any conceptual aim the given system should fulfill [2]. For instance, Sharing Economy applications (like Uber and Airbnb) aim to enhance social capital and stimulate economic development in resource-limited areas [3]. In contrast, the goal of Health&Fitness applications is to encourage healthy habits among both children and adults [4]. However, due to intense market rivalry, the app development cycle often aims to produce functional applications within brief intervals (such as days or weeks), leading developers to stray from their initial objectives frequently. These divergences frequently bring forth ethical concerns such as declining mental health, bias, privacy violations, and manipulation [5, 6, 7, 8]. Applications that fail to sufficiently consider their users’ ethical concerns are often labeled as untrustworthy or even deserted by their users [9]. Thus, for applications to endure the market’s scrutiny, developers continuously keep track of user feedback through ratings and reviews found in app marketplaces (like Google Play Store). They typically analyze user feedback to gather insights on bug reports, feature suggestions, connectivity issues, resource consumption challenges (e.g., battery life), and interface problems [10, 11, 12, 13]. Numerous studies have investigated user perspectives on ethical concerns within software applications. Research conducted by Besmer et al. [14] and Nema et al. [15] underscores users’ concerns regarding privacy breaches and data security measures in mobile applications. The emergence of discriminatory algorithms and the potential for bias in software functionalities are also significant areas of concern, as highlighted by the findings of Tushev et al. [16] and Olson et al. [17]. Furthermore, manipulative design tactics that coerce users or take advantage of psychological weaknesses are increasingly worrisome, as noted by Olson et al. [18]. However, these investigations largely depend on keyword-based sampling from app reviews, which limits the ethical issues users address to a predetermined set of terms. To overcome this limitation, Harkous et al. [19] suggest using the NLI method. However, they rely on a set of generic privacy hypotheses (derived from generic privacy concepts) overlooking the fact that users’ ethical concerns are domain-dependent [1]. For instance, individuals using ridesharing services (e.g., Uber and Lyft) may raise concerns about the constant tracking of their location, while those utilizing financial platforms (e.g., Robinhood and Coinbase) might express concerns regarding the sharing of their social security or banking details with the application. Additionally, NLI with generic hypotheses identifies a high number of false positives (FP) that require further manual analysis to identify ethical concern-related reviews [19]. To address these challenges, in this paper, we propose a novel Natural Language Processing (NLP) based hybrid approach that combines Natural Language Inference (NLI) and a decoder-only Large Language Model (LLM) to mine ethical concern-related app reviews at scale. We use NLI with domain-specific hypotheses to determine potential ethical concern-related reviews and further process these reviews using LLMs to extract ethical concern-related app reviews. The main contributions of this study can be summarized as follows. • To the best of our knowledge, this is the first hybrid approach that utilizes NLI and LLM along with domain-specific privacy hypotheses to extract ethical concern-related app reviews. NLI+LLM demonstrated better results compared to generic privacy hypotheses utilized by Harkous et al. [19]. • We develop domain-specific hypotheses based on the Mental Health (domain-specific) privacy concepts provided by Iwaya et al. [20]. • We demonstrate that our proposed hybrid approach (NLI+LLM) can extract concern-related reviews that do not contain predefined wordings used in the keyword-based method in Ebrahimi et al [1]. • We open source our source code and dataset111https://github.com/AakashSorathiya/CHyMER of 1,008 privacy-related reviews (results from our study) that remained unidentified by the previous Ebrahimi et al’s [1] study which used a keyword-based approach. The rest of the paper is organized as follows. To determine the research gaps, Section II discusses related work. Section III presents the motivation for our research through examples. We define our research questions (RQs) and explain preliminaries in Section IV and V, respectively. In Section VI, we describe the dataset and explain our methodology in Section VII. Section VIII shows and discusses the results of our investigation. Section IX lists various threats to the validity of our investigation and Section X presents concluding remarks and future directions."
https://arxiv.org/html/2411.07396v1,Toward Optimal Search and Retrieval for RAG,"Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.","Retrieval-augmented generation (RAG) (1) is gaining popularity due to its ability to address some of the challenges with using Large Language Models (LLMs), including hallucinations (2) and outdated training data (1; 3). RAG pipelines are made up of two disparate components: a retriever, which identifies documents relevant to a query from a given corpus, and a reader, which is typically an LLM prompted with a query, the text of the retrieved documents, and instructions to use this context to generate its response. However, it is unclear how a RAG pipeline’s performance on downstream tasks can be attributed to each of these components (1; 2). In this work, we study the contributions of retrieval to downstream performance.111https://www.github.com/intellabs/rag-retrieval-study. For this purpose, we evaluate pipelines with separately trained retriever and LLM components, as training retrieval-augmented models end-to-end is both more resource-intensive and obfuscates the contribution of the retriever itself. We aim to address questions that will enable practitioners to design retrieval systems tailored for use in RAG pipelines. For example, what are the weaknesses of the typical search and retrieval setup in RAG systems? Which search hyperparameters matter for RAG task performance? We choose to evaluate RAG pipeline performance on both standard QA and attributed QA. In attributed QA, the model is instructed to cite supporting documents provided in the prompt when making factual claims (4; 5). This task is interesting for its potential to boost the trustworthiness and verifiability of generated text (6). We make four contributions: (1) We show how both QA performance and citation metrics vary with more retrieved documents, adding new data to a small literature on attributed QA with RAG. (2) We describe how RAG task performance is affected when fewer gold documents are included in the context. (3) We show that saving retrieval time by decreasing approximate nearest neighbor (ANN) search accuracy in the retriever has only a minor effect on task performance. (4) We show that injecting noise into retrieval results in performance degradation. We find no setting that improves above the gold ceiling, contrary to a prior report (7)."
https://arxiv.org/html/2411.07387v1,Isochrony-Controlled Speech-to-Text Translation: A study on translating from Sino-Tibetan to Indo-European Languages,"End-to-end speech translation (ST), which translates source language speech directly into target language text, has garnered significant attention in recent years. Many ST applications require strict length control to ensure that the translation duration matches the length of the source audio, including both speech and pause segments. Previous methods often controlled the number of words or characters generated by the Machine Translation model to approximate the source sentence’s length without considering the isochrony of pauses and speech segments, as duration can vary between languages. To address this, we present improvements to the duration alignment component of our sequence-to-sequence ST model. Our method controls translation length by predicting the duration of speech and pauses in conjunction with the translation process. This is achieved by providing timing information to the decoder, ensuring it tracks the remaining duration for speech and pauses while generating the translation. The evaluation on the Zh-En test set of CoVoST 2, demonstrates that the proposed Isochrony-Controlled ST achieves 0.92 speech overlap and 8.9 BLEU, which has only a 1.4 BLEU drop compared to the ST baseline.","Speech-to-text Translation (ST) is a technology that converts speech input in one language into text output in another language [1, 2, 3, 4]. This technology has numerous real-world applications, such as simultaneous translation for international conferences [4, 5, 6], enhancing accessibility [7, 8] and communication across language barriers [9, 10]. Additionally, ST can be integrated with Text-to-Speech Synthesis (TTS) [11, 12, 13] to form a core component of cascaded Speech-to-Speech Translation systems [14, 15], which are particularly useful in video dubbing applications [16, 17]. In these applications, controlling the prosody alignment and duration of the generated translation is crucial [18, 19]. In ST tasks, proper duration control is crucial to maintain synchronization between the source audio and the translated text or speech, ensuring that the translation is delivered within the same timeframe as the original speech. This synchronization is essential for applications like video dubbing, where the translated speech must match the timing of the speaker’s lip movements [20], and for captioning, where subtitles need to appear in sync with the audio [21, 22]. Therefore, precise control over prosody and duration enhances the overall quality and usability of ST in various multimedia applications [23, 24]. Conventional Speech-to-Text Translation (ST) methods typically involve a two-step process where Automatic Speech Recognition (ASR) [25, 26] and Machine Translation (MT) [27, 28, 29] are cascaded together. In these approaches, the responsibility for maintaining isochrony and the duration alignment between the source speech and the translated text falls on the MT module [30]. Their goal is to ensure that the translated text mirrors the timing of the original speech as closely as possible. In [16, 31], the authors attempt to achieve isochrony in Machine Translation by using isometry, which means generating translations that match the number of characters in the source text. However, this approach has its own limitations. Research [32] has shown that isometry is only weakly correlated with isochrony, meaning that matching character counts does not necessarily result in translations that align well with the timing of the source speech. An alternative approach, as discussed in [33], involves predicting the duration of each translated word in conjunction with generating the word sequences. By interleaving word duration predictions with the translated words, this method aims to achieve more precise control over the timing of the translation. This approach allows the translation to maintain better synchronization with the source speech, improving the overall quality and naturalness of the translated output. Expanding on this idea, authors of [30] explored target factors and auxiliary counters to predict duration sequences jointly with target language phoneme sequences. In their approach, target factors enabled the estimation of the phonemes and duration sequence separately while still ensuring that they are conditioned on each other. However, the aforementioned approaches control the duration of the translated speech in the MT module in the cascaded ST approach, and their performance might be negatively affected by the error propagation of the ASR and Voice Activity Detection (VAD) modules. In this study, we aim to conduct Isochrony-Controlled Speech-to-Text Translation for Sino-Tibetan languages into English. The Sino-Tibetan and Indo-European language families are distinct, with the former predominantly featuring tonal, monosyllabic languages with simpler grammar, and the latter comprising non-tonal, polysyllabic languages with more complex grammar, which presents challenges in speech translation [34]. Additionally, the conciseness of Sino-Tibetan languages like Chinese, which are often two-thirds the length of their Indo-European counterparts such as English in written form, poses further difficulties in achieving isochrony in translation [35]. We propose a sequence-to-sequence ST system and specific pre-training data collection to ensure competitive performance on CoVoST2 [36] benchmark. Additionally, we propose to integrate embedded utterance timing information into the decoder to condition the translation onto the duration modeling for both speech and pause segments. Our evaluations on real and synthetic data shows in terms of both isochrony and translation quality over our sequence-to-sequence ST baseline. The remainder of this paper is as follows. In section 2, the proposed method is described. Section 3 explains the details of the data preparation and experiment setup. Finally, section 4 provides the analysis and conclusions of the proposed method."
https://arxiv.org/html/2411.07381v1,BeeManc at the PLABA Track of TAC-2024: RoBERTa for task 1 LLaMA3.1 and GPT-4o for task 2,"This report is the system description of the BeeManc team for shared task Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024. This report contains two sections corresponding to the two sub-tasks in PLABA 2024. In task one, we applied fine-tuned ReBERTa-Base models to identify and classify the difficult terms, jargon and acronyms in the biomedical abstracts and reported the F1 score. Due to time constraints, we didn’t finish the replacement task. In task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot prompts to complete the abstract adaptation and reported the scores in BLEU, SARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024 on Task 1A and 1B, our much smaller fine-tuned RoBERTa-Base model ranked 3rd and 2nd respectively on the two sub-task, and the 1st on averaged F1 scores across the two tasks from 9 evaluated systems. Our share our fine-tuned models and related resources at https://github.com/HECTA-UoM/PLABA2024","1 Background Health literacy, or the ability of individuals to comprehend and apply health information for informed decision-making, is one of the central focuses of the Healthy People 2030 framework in the US. Even though biomedical information is highly accessible online, patients and caregivers often struggle with language barriers, even when the content is presented in their native language. The shared task PLABA aims to harness advances in deep learning to empower the automatic simplification of complex scientific texts into language that is more understandable for patients and caregivers. Despite substantial obstacles to effective implementation, the goal of the PLABA track is to improve health literacy by translating biomedical abstracts into plain language, making them more accessible and understandable to the general public 111https://bionlp.nlm.nih.gov/plaba2024/. Following our participation on the PLABA 2023 shared task using large language models (LLMs) such as ChatGPT, BioGPT, and Flan-T5, and Control Mechanisms Li et al. (2024), in this work, we introduce our system participation to the PLABA 2024. Instead of end-to-end biomedical abstract simplification as in PLABA-2023, in this year, PLABA-2024 introduced more granular-steps, including Term Replacement for Task-1 and Complete Abstract Adaption for Task-2, which we will describe in detail for our methodologies via fine-tuning RoBERTa-Base model for Task-1 and prompting LLMs (Llama and ChatGPT) for Task-2."
https://arxiv.org/html/2411.07343v1,Multi-head Span-based Detector for AI-generated Fragments in Scientific Papers,"This paper describes a system designed to distinguish between AI-generated and human-written scientific excerpts in the DAGPap24 competition hosted within the Fourth Workshop on Scientific Document Processing. In this competition the task is to find artificially generated token-level text fragments in documents of a scientific domain. Our work focuses on the use of a multi-task learning architecture with two heads. The application of this approach is justified by the specificity of the task, where class spans are continuous over several hundred characters. We considered different encoder variations to obtain a state vector for each token in the sequence, as well as a variation in splitting fragments into tokens to further feed into the input of a transform-based encoder. This approach allows us to achieve a 9% quality improvement relative to the baseline solution score on the development set (from 0.86 to 0.95) using the average macro F_{1}-score, as well as a score of 0.96 on a closed test part of the dataset from the competition.","Modern advances in the field of text generation models provide high quality artificial texts that are hardly distinguishable from human-written texts at fluent reading. Text generation systems such as Llama Touvron et al. (2023), ChatGPT Ray (2023), Mistral Jiang et al. (2023) are increasing the list of successfully solved problems in updated benchmarks Li et al. (2023) with the release of each newer version of the product. Although the progress of such models is impressive, it poses new challenges for scientists, as the development of these systems implies the emergence and spread of generated fragments in texts of the scientific domain. In the field of Natural Language Processing there are already a large number of methodologies for detecting texts generated by machine learning models Jawahar et al. (2020), including approaches for the scientific domain Liyanage and Buscaldi (2023); Gritsay et al. (2024). Therefore, the improvement of artificial text detection techniques occurs simultaneously with the improvement of text generation methods. In order to prevent the gap between quality generation and precision of detection from growing, it is necessary to periodically update and modernise existing detection approaches with newly generated fragments. Indeed, increasing the appearance of generated fragments in scientific papers carries with it a potential increase in plagiarism Bakhteev et al. (2022), fake studies Bakhteev et al. (2020) and missinformation. The DAGPap24 competition allowed us to perform a further cycle of updates and make a reliable AI-generated scientific text detection system. The challenge is to build a detection system that is robust to generated fragments from fundamentally different frameworks and diverse scientific domains. In this paper, we present a solution that was developed for the DAGPap24 competition by our team. We propose a method to find artificially generated fragments at the token level using a multi-task architecture. Our model has two classifiers for each token, that are trained jointly and allow the model to have a higher generalisation capability and to process the text sequences arriving at the input faster. We introduce the aforementioned architecture in this paper and conduct experiments to tune its components to obtain the best result."
https://arxiv.org/html/2411.07336v1,: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models,"Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs’ algorithmic abilities under simple lexical or semantic variations. To this end, we present the SetLexSem Challenge, a synthetic benchmark that evaluates the performance of LLMs on set operations. SetLexSem assesses the robustness of LLMs’ instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SetLexSem, we find that they exhibit poor robustness to variation in both operation and operands. We show – via the framework’s systematic sampling of set members along lexical and semantic dimensions – that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of ""deceptive"" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them independently. The code for reproducing the results of this paper, and for generating the SetLexSem Challenge dataset, is available at https://github.com/amazon-science/SetLexSem-Challenge.","Figure 1: To evaluate the robustness of LLMs to semantic variation in set members, we create “deceptive” sets. To construct such sets, we sample a pair of hypernyms (e.g. “mammal” and “vehicle”) and, from them, a set of their hyponyms in three conditions: (1) with the hyponyms as sampled, (2) with half of the set members swapped, and (3) randomly sampled. LLMs exhibit a unique failure mode under the second condition (swapped) and the mean and variance in accuracy of the first condition (not swapped) is better than that of the random baseline. See Figure 7 for results. Transformer models (Vaswani et al. [2017]) were initially devised and used for traditional natural language processing tasks, such as machine translation, natural language inference, or question answering (Vaswani et al. [2017], Devlin et al. [2019], Wang et al. [2018], Rajpurkar et al. [2018]). More recently, auto-regressive Transformers pre-trained on Internet-scale datasets and fine-tuned to conform to human preferences on a curated set of instructions (Ouyang et al. [2022]) – colloquially, large language models (LLMs) – have been shown to exhibit impressive performance on some analytical tasks, such as mathematics (Cobbe et al. [2021], Hendrycks et al. [2021a]), reasoning (Dua et al. [2019]), and computer programming (Chen et al. [2021]). The increasing adoption of these models requires that we carefully interpret and interrogate their behaviors and the datasets on which they are evaluated. Careful evaluation exposes weaknesses and irregularities that inform users about the quality of models they may adopt. It can also inform model designers about the limitations of current architectures and requirements of future ones. We should be clear-eyed about the limitations of existing datasets. Multiple choice tasks are not uncommon among them. A multiple-choice task setup constrains the complexity of the problem posed to the system being evaluated. More concerning is that zero-shot task performance of an LLM tends to be better on datasets that existed before it was trained than on those that were released after [Li and Flanigan, 2024]. This suggests that datasets or tasks may be leaking into the LLM training procedure. The ways this might happen are numerous. A dataset – its training set, test set, or even both – might be included in the LLM training set (dataset leakage). Or a proprietary instruction dataset that contains tasks similar to those in a public dataset might be created and used during the instruction tuning process (task leakage). Zhang et al. [2024] found that overlap between training and evaluation data is reported for only 30% of LLMs. Synthetic benchmarks may address these problems, even if imperfectly. A synthetic dataset can require an LLM to perform a procedure of complexity greater than answering a multiple choice question and it can control the complexity of the procedure itself. The task leakage problem is more challenging, but a synthetic dataset may at least circumvent dataset leakage by supporting regeneration with different parameters. We present SetLexSem, a synthetic set theory benchmark that controls both the difficulty of the task and the objects on which a task is performed. SetLexSem focuses on set theoretical operations because sets can comprise objects of unconstrained type. The members of a set can be anything that can be named or described. Other fields of mathematics are constrained in their operands. Arithmetic works on numbers, and geometry on shapes, and while logic can operate on things that can be named, it is constrained in their relationships. We have thus focused on set operations because of their flexibility with respect to operands, particularly because they enable a systematic and simultaneous testing of language models’ analytical task performance under controlled variation of the task operands. A truly intelligent system should exhibit System 2 thinking [Kahneman, 2011], which implies performing tasks consistently regardless of incidental features. Consequently, the figure of merit of SetLexSem is the variance of accuracy, not average accuracy. The more robust the system is, the less variance it should exhibit as incidental features of a task vary. We categorize incidental features throughout this paper as follows: • Analytical Task performance should not vary with either computational complexity of the task – as shown in Table 1 – or the task’s scale (i.e. the size of the operands A and B). • Lexico-semantic Task performance should not vary as the lexical or semantic aspects of the sets A and B are varied. SetLexSem is a benchmark of the robustness of a system – robustness to variations in task complexity and to variations in task content. The results presented here indicate that current LLMs are not robust – in a System 2 sense – along any of the dimensions that the benchmark evaluates. Further, the results show that LLM are particularly not robust to easy-to-create semantic groupings of ""deceptive"" sets, as shown in Figure 1. This latter result has, we believe, significant implications for the design of any future model that aspires to achieve System 2 robustness. Table 1: The set operations evaluated in SetLexSem. Performing them requires composing simple logic \land (\lor) and membership \in (\notin) functions. Operation Notation Definition Union A\cup B \{x:x\in A\lor x\in B\} Intersection A\cap B \{x:x\in A\land x\in B\} Difference A\setminus B \{x:x\in A\land x\notin B\} Symmetric difference A\triangle B \{x:(x\in A\land x\notin B)\lor(x\notin A\land x\in B)\}"
https://arxiv.org/html/2411.07320v1,"Richer Output for Richer Countries:
Uncovering Geographical 
Disparities in Generated Stories and Travel Recommendations","While a large body of work inspects language models for biases concerning gender, race, occupation and religion, biases of geographical nature are relatively less explored. Some recent studies benchmark the degree to which large language models encode geospatial knowledge. However, the impact of the encoded geographical knowledge (or lack thereof) on real-world applications has not been documented. In this work, we examine large language models for two common scenarios that require geographical knowledge: (a) travel recommendations and (b) geo-anchored story generation. Specifically, we study four popular language models, and across about 100K travel requests, and 200K story generations, we observe that travel recommendations corresponding to poorer countries are less unique with fewer location references, and stories from these regions more often convey emotions of hardship and sadness compared to those from wealthier nations. 111Upon publication, we will publicly release the data and code required to reproduce our evaluation.","Figure 1: World map with country-wise analysis of responses generated by GPT-4. Left: Average count of geographical entities mentioned in generated stories (correlated with the GDP per capita with Pearson r = 0.5). Right: Uniqueness scores for travel recommendations (Pearson r = 0.4 with GDP per capita). Given the excitement around large language models, users resort to these models for a diverse range of applications (Brown et al., 2020; Touvron et al., 2023). Based on our analysis of ShareGPT,222https://sharegpt.com/ a collection of user interactions with ChatGPT, 1.7% of queries are about travel recommendations, whereas 1.5% concern story generation. Such use cases make one wonder whether the generated travel itinerary for Mumbai is just as informative compared to New York City? Or is a generated story of a girl growing up in Nairobi just as relatable compared to another story based in Seattle? For these applications to be broadly useful, it is important that there are no (or few) geographical disparities. Some recent works aim to benchmark the extent of geographical knowledge encoded in large language models (Bhandari et al., 2023; Manvi et al., 2023; Moayeri et al., 2024). Interestingly, a recent study finds that language models accurately predict global facts such as population and rainfall for different geo-locations, however, their predictions on sensitive topics such as attractiveness or morality are, problematically, biased against areas with poorer socioeconomic conditions (Manvi et al., 2024). Similar in spirit, our work aims to quantitatively study model responses for two real-world scenarios that require geographical knowledge. In this work, we analyze over 300K responses from 4 language models, corresponding to requests for travel recommendations and geo-anchored story generations. These requests span over 150K locations across the globe. We quantify the informativeness and uniqueness of model responses, in addition to the emotions they express. We then compare these quantities with the socioeconomic indicators of different locations. Through our analysis, we uncover several geographical disparities, finding that outputs for wealthier countries are more unique and include more geographical entities (Figure 1). For Sub-Saharan African countries, we notice considerably less unique outputs compared to the North American region, with the average difference being about 40% across all models. Further, stories about poorer countries express considerably more hardship and sadness, with 65% more narratives depicting hardship for low-income countries compared to high-income countries. Despite many large corporations claiming to be egalitarian, e.g., OpenAI aims to develop intelligence that benefits all of humanity (OpenAI, 2024), many findings, including ones from this study, demonstrate how current models perpetuate western hegemony in the generated content Schwöbel et al. (2023); Bender et al. (2021), and more serious effort is needed to ensure that models serve the diverse population across the globe."
https://arxiv.org/html/2411.07268v2,Target-driven Attack for Large Language Models,"Current large language models (LLM) provide a strong foundation for large-scale user-oriented natural language tasks. Many users can easily inject adversarial text or instructions through the user interface, thus causing LLM model security challenges like the language model not giving the correct answer. Although there is currently a large amount of research on black-box attacks, most of these black-box attacks use random and heuristic strategies. It is unclear how these strategies relate to the success rate of attacks and thus effectively improve model robustness. To solve this problem, we propose our target-driven black-box attack method to maximize the KL divergence between the conditional probabilities of the clean text and the attack text to redefine the attack’s goal. We transform the distance maximization problem into two convex optimization problems based on the attack goal to solve the attack text and estimate the covariance. Furthermore, the projected gradient descent algorithm solves the vector corresponding to the attack text. Our target-driven black-box attack approach includes two attack strategies: token manipulation and misinformation attack. Experimental results on multiple Large Language Models and datasets demonstrate the effectiveness of our attack method.","As large language models (LLMs) [21, 6] continue to advance in architecture and functionality, their integration into complex systems requires a thorough review of their security properties. Since the use of most LLMs relies on interface interaction, it is difficult to avoid the hidden danger of generative adversarial attacks. Therefore, it is significant to study adversarial attacks on large language models to improve the security and robustness of LLMs [28]. Previous attacks on LLMs mainly include white-box attacks and black-box attacks. White-box attacks assume that the attacker has full access to the model weights, architecture, and training workflow so that the attacker can obtain the gradient signal. The main method is gradient-based attacks, for example, Guo et al. [11] proposed a gradient-based distributed attack (GBDA), which, on the one hand, uses the Gumbel-Softmax approximation technique to make the adversarial loss optimization differentiable and, on the other hand, uses BERTScore and perplexity to enhance perceptibility and fluency. These methods can only attack open-source large language models but are powerless against more widely used closed-source LLMs such as ChatGPT. The attacker in a black-box attack can only observe the inputs and outputs of the model. They can provide input and receive feedback but cannot access any additional information about the model. Black box attacks [10] can be divided into letter level, word level, sentence level, and multi-layer level according to the attack granularity. Many black-box attacks use word replacement [13] to identify the most critical words in the text and replace them, or use some simple and general text enhancement methods [20], including synonym replacement, random insertion, random exchange, or random deletion. Black-box strategies cannot know the internal structure of large models, so most attack methods use heuristic strategies, where it is not clear how these heuristic strategies are related to the success rate of the attack, so they cannot effectively improve the success rate of the attack strategies. In our work, we assume that the model satisfies the conditional probability distribution p(y|x) and p(y|z) under the condition of clean text representation x and attack text representation z respectively, then maximize the KL divergence between the two distributions. Based on the above assumptions, we prove that maximizing the KL divergence is equivalent to maximizing the Mahalanobis distance between x and z. Subsequently, we transform the maximization problem of Mahalanobis distance into a convex optimization problem and estimate the variance of Mahalanobis distance through another convex optimization problem. Finally, the embedding corresponding to the optimal text is obtained by the projected gradient descent algorithm, which serves as a guide for us to generate disturbance information for the downstream task of cohesion. Experimental results on two black box attack strategies of token manipulation and misinformation attack verify the effectiveness of the proposed method. Our main contributions are summarized as follows: • We propose a new objective function maximizing the KL divergence of two conditional probabilities to guide the attack algorithm to achieve the best attack effect. • We’ve theoretically demonstrated that maximizing the KL divergence between normal and attack text is approximately equivalent to maximizing their Mahalanobis distance. This relationship clarifies how these statistical measures distinguish between normal and attack text in security analysis. • Based on the above theorem, we transformed the original problem into a convex optimization problem and obtained a vector representation of the attack text through projected gradient descent. Then we designed two new black-box attack methods based on token manipulation and misinformation attack strategies. Experimental results verify the effectiveness of our method."
https://arxiv.org/html/2411.08003v1,Can adversarial attacks by large language models be attributed?,"Attributing outputs from Large Language Models (LLMs) in adversarial settings—such as cyberattacks and disinformation—presents significant challenges that are likely to grow in importance. We investigate this attribution problem using formal language theory, specifically language identification in the limit as introduced by Gold and extended by Angluin. By modeling LLM outputs as formal languages, we analyze whether finite text samples can uniquely pinpoint the originating model. Our results show that due to the non-identifiability of certain language classes, under some mild assumptions about overlapping outputs from fine-tuned models it is theoretically impossible to attribute outputs to specific LLMs with certainty. This holds also when accounting for expressivity limitations of Transformer architectures. Even with direct model access or comprehensive monitoring, significant computational hurdles impede attribution efforts. These findings highlight an urgent need for proactive measures to mitigate risks posed by adversarial LLM use as their influence continues to expand.","References [1] Dana Angluin. Inductive inference of formal languages from positive data. Information and Control, 45(2):117–135, 1980. [2] Umar Anwar, Aziz Saparov, Julia Rando, Daniel Paleka, Michael Turpin, Pete Hase, et al. Foundational challenges in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932, 2024. [3] Robert Axelrod and Radoslav Iliev. Timing of cyber conflict. Proceedings of the National Academy of Sciences, 111(4):1298–1303, 2014. [4] Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020. [5] Alexander Bick, Adam Blandin, and David J. Deming. The rapid adoption of generative ai. Working Paper w32966, National Bureau of Economic Research, Cambridge, MA, September 2024. [6] Rishi Bommasani, Dilara Soylu, Thomas I. Liao, Kathleen A. Creel, and Percy Liang. Ecosystem graphs: The social footprint of foundation models. March 2023. [7] Manuel Cebrian. A time-critical crowdsourced computational search for the origins of covid-19. Nature Electronics, 4(7):450–451, 2021. [8] Nicholas A Christakis and James H Fowler. Connected: The surprising power of our social networks and how they shape our lives. Little, Brown Spark, 2009. [9] Brian Edwards, Allen Furnas, Steve Forrest, and Robert Axelrod. Strategic aspects of cyberattack, attribution, and blame. Proceedings of the National Academy of Sciences, 114(11):2825–2830, 2017. [10] E.M. Gold. Language identification in the limit. Information and Control, 10(5):447–474, 1967. [11] Kent Johnson. Gold’s theorem and cognitive science. Philosophy of Science, 71(4):571–592, 2004. [12] Jon Kleinberg and Sendhil Mullainathan. Language generation in the limit. arXiv preprint arXiv:2404.06757, 2024. [13] William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah Smith. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. arXiv preprint arXiv:2010.09697, 2020. [14] Mark Ed Newman, Albert-László Ed Barabási, and Duncan J Watts. The structure and dynamics of networks. Princeton university press, 2006. [15] Oak Ridge Leadership Computing Facility. Frontier supercomputer. [16] Bin Peng, Srikumar Narayanan, and Christos Papadimitriou. On limitations of the transformer architecture. arXiv preprint arXiv:2309.06863, 2023. [17] Nicole Perlroth. This is how they tell me the world ends: The cyberweapons arms race. Bloomsbury publishing, 2021. [18] Iyad Rahwan, Manuel Cebrian, Nicholas Obradovich, Josh Bongard, Jean-François Bonnefon, Cynthia Breazeal, Joshua W Crandall, Nicholas A Christakis, Iain D Couzin, Michael O Jackson, et al. Machine behaviour. Nature, 568(7753):477–486, 2019. [19] Yuval Shavit, Shuchi Agarwal, Miles Brundage, Saurabh Adler, Courtney O’Keefe, Riley Campbell, and David G Robinson. Practices for governing agentic ai systems. Research Paper, OpenAI, December 2023, 2023. [20] Lutz Strobl, William Merrill, Gregory Weiss, Daniel Chiang, and Dana Angluin. What formal languages can transformers express? a survey. Transactions of the Association for Computational Linguistics, 12:543–561, 2024. [21] Fabio Urbina, Filippa Lentzos, Cédric Invernizzi, and Sean Ekins. Dual use of artificial-intelligence-powered drug discovery. Nature Machine Intelligence, 4(3):189–191, 2022. [22] Marcin Waniek, Petter Holme, Manuel Cebrian, and Talal Rahwan. Social diffusion sources can escape detection. Iscience, 25(9):104956, 2022. [23] Marcin Waniek, Petter Holme, Katayoun Farrahi, Rémi Emonet, Manuel Cebrian, and Talal Rahwan. Trading contact tracing efficiency for finding patient zero. Scientific reports, 12(1):22582, 2022. [24] Jiajia Xu, Alice Smith, Liam Johnson, and Kevin Lee. Autoattacker: A large language model guided system to implement automatic cyber-attacks. arXiv preprint arXiv:2403.01038, 2024."
https://arxiv.org/html/2411.07975v1,JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation,"We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.","(a) Benchmark Performances. (b) Visual Generation Results. Figure 1: Multimodal understanding and image generation with JanusFlow. JanusFlow surpasses the state-of-the-art unified multimodal models and several task-specific understanding models on visual understanding benchmarks. It is also capable of generating high-quality images. The resolution of the images is 384\times 384. Large language models (LLMs) have demonstrated remarkable capabilities in learning diverse knowledge and generalizing to new scenarios [88, 68, 1, 8, 7]. Leveraging these capabilities, researchers have developed sophisticated models specialized in image comprehension [57, 56, 47, 2, 15, 49] and text-to-image generation [77, 74, 71, 23]. The field has recently shifted toward creating unified systems capable of handling both tasks simultaneously. One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models [25, 26, 84, 27, 19]. However, this approach introduces architectural complexity and potentially constrains the model’s capabilities through maintaining separate LLM and generative components. Alternative approaches [85, 95, 96, 103, 93] propose training a single LLM for both tasks, typically incorporating either diffusion models [32, 80] or vector-quantized autoregressive models [22, 83]. Our approach builds upon recent breakthroughs in rectified flow models [60, 55, 3, 23, 61], which provide a simple framework for generative modeling while delivering exceptional empirical performance [23, 45, 36]. Building on these advances, we propose JanusFlow, a powerful unified multimodal model that seamlessly integrates rectified flow with LLM architecture. Following a minimalist design principle, our architecture requires only a lightweight encoder and decoder to adapt the LLM for rectified flow operations. To optimize JanusFlow’s performance, we implement two key strategies: First, we maintain separate vision encoders for understanding and generation tasks, preventing task interference and thus enhancing comprehension capabilities. Second, we align the intermediate representations between generation and understanding modules during training, strengthening semantic coherence in the generation process. JanusFlow shows state-of-the-art performances in both multimodal comprehension and text-to-image generation compared to existing unified approaches, and even outperforms several specialized methods. Specifically, on text-to-image generation benchmarks, MJHQ FID-30k [48], GenEval [28] and DPG-Bench [34], JanusFlow achieves scores of 9.51, 0.63 and 80.09\%, surpassing established text-to-image models including SDv1.5 [75] and SDXL [71]. In multimodal comprehension benchmarks, JanusFlow attains scores of 74.9, 70.5 and 60.3 on MMBench [62], SeedBench [46], and GQA [35], respectively, exceeding specialized models such as LLaVA-v1.5 [56] and Qwen-VL-Chat [4]. Notably, these results are achieved with a compact LLM architecture with only 1.3B parameters."
https://arxiv.org/html/2411.07772v1,Automatic Album Sequencing,"Album sequencing is a critical part of the album production process. Recently, a data-driven approach was proposed that sequences general collections of independent media by extracting the narrative essence of the items in the collections. While this approach implies an album sequencing technique, it is not widely accessible to a less technical audience, requiring advanced knowledge of machine learning techniques to use. To address this, we introduce a new user-friendly web-based tool that allows a less technical audience to upload music tracks, execute this technique in one click, and subsequently presents the result in a clean visualization to the user. To both increase the number of templates available to the user and address shortcomings of previous work, we also introduce a new direct transformer-based album sequencing method. We find that our more direct method outperforms a random baseline but does not reach the same performance as the narrative essence approach. Both methods are included in our web-based user interface, and this—alongside a full copy of our implementation—is publicly available at https://github.com/dylanashley/automatic-album-sequencing","Album sequencing is the process of taking a music album and ordering it so that listening to it in that order produces a desired emotional response in the listener. Despite its importance in producing an impactful music album, album sequencing has received comparatively little attention from the artificial intelligence community. Our previous research [1] introduced a way to compress different kinds of media down into an ultra-low dimensional representation. This representation captures their relevancy to the overarching story induced by the ordering of the collection they belonged to, i.e., their narrative essence. This is accomplished by using neural networks and contrastive learning [2, 3]. Then, evolutionary algorithms are used to learn a set of template curves and a novel curve-fitting algorithm to fit the narrative essence of new media collections to these template curves. The above was principally done with music albums from the FMA dataset [4], though it is shown that this applies to other forms of media as well. There are two key issues with our previous work. First, our previous method requires knowledge of advanced machine learning techniques, making it inaccessible to many people who perform album sequencing. Second, it requires a complex pipeline with (1) a neural network to extract the narrative essence followed by (2) a separate evolutionary algorithm to learn a set of templates and then (3) a fitting algorithm to produce a final ordering. This is a highly complex and particularly problematic setup that does not allow information like the genre of an album to flow between the narrative essence and the final ordering, resulting in genre-agnostic templates. Here, we address both of the aforementioned issues. To address the latter issue, we introduce a new approach that replaces the full pipeline with a single Transformer [5, 6, 7]. While this does not outperform the more complicated pipeline proposed in our previous work, the new simpler pipeline still outperforms a random baseline, making it useful for automatic album sequencing. Next, to address the former issue, we implement and release a dedicated user-friendly web-based interface that allows a less technically inclined user to run both the narrative essence-based and the new simplified album sequencing approaches on the user’s own music. We release this interface alongside a complete implementation of our approach publicly at https://github.com/dylanashley/automatic-album-sequencing In summary, our contributions are as follows: (1) We introduce a new direct method to perform automatic album sequencing. (2) We show that, despite the simpler pipelineethod outperforms a random baseline. (3) We release a web-based user interface tool that makes automatic album sequencing accessible to a less technical audience."
https://arxiv.org/html/2411.07618v1,Direct Preference Optimization Using Sparse Feature-Level Constraints,"The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often experience computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves an above 5% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.","Aligning large language models (LLMs) with human values and practical objectives is a critical challenge in AI development (Wang et al., 2023). Post-training methods, including fine-tuning (Wei et al., 2022; Chung et al., 2024) and alignment strategies (Tunstall et al., 2023), have played a significant role in refining LLM behavior. Among these, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022) has emerged as a leading technique, integrating human feedback to guide models towards producing valuable and useful outputs. Despite its success, RLHF involves complex mechanisms such as reward modeling and policy gradients, which introduce significant training complexity and computational cost (Zheng et al., 2023b; Rafailov et al., 2024). To address these limitations, Direct Preference Optimization (DPO) (Rafailov et al., 2024) has been proposed as a more efficient alternative. Unlike reward-based methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), DPO directly adjusts the model’s output probabilities based on human preferences, reducing training complexity and computational cost. DPO-like approaches can offer a more stable and faster alignment process by bypassing the challenges associated with reward models and policy updates, making it a compelling solution for efficient LLM alignment since DPO uses a reference model to stabilize post-training. Recent advancements in DPO focus on mainly two directions: efficiency i.e., further simplifying the constraints of DPO, and controllability i.e., keeping the balance between alignment and generation diversity. In terms of simplicity, methods like SimPO (Meng et al., 2024) and Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024) eliminate the need for a reference model by using the average log probability of sequences as an implicit normalizer, thereby reducing memory usage and computational demands. However, DPO’s performance is sensitive to the strength of constraints from the reference policy (Liu et al., 2024), and these reference-free alignment approaches (Hong et al., 2024; Meng et al., 2024) can compromise control, resulting in unstable training. In terms of controllability, Token-level Direct Preference Optimization (TDPO) (Zeng et al., 2024) introduces token-level rewards and sequential Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951) to tackle issues related to linguistic coherence, diversity, and stability. However, it comes at the cost of increased computational complexity, introducing an additional sequential KL and depending on reference models, complicating the loss computation. Figure 1: Left. The DPO objective loss function and its two main improvement directions: SimPO and TDPO. SimPO focuses on simplifying the reference model, while TDPO concentrates on controlling the alignment process to enhance generation diversity. Right. The pipeline of FPO consists of sparse autoencoders and the feature-level MSE constraints. A natural hypothesis arises: “Is there a method that can strike the right balance between efficiency and controllability?” In response, we propose FPO, Feature-level Constrained Direct Preference Optimization (See Figure 1), introducing an efficient and controllable method for constraining the model at the feature level. Here a feature refers to a salient piece of information for the model decision (Huben et al., 2024). Intuitively, adjusting the model using feature-level preferences allows fine-grained adjustment that minimizes the side impact, by avoiding the negative influence of spurious features in course-grained control such as token level regularization (Zeng et al., 2024). To achieve that, we derive the FPO objective by contrasting SimPO and DPO, showing the constraint term that SimPO misses. We then add such a term by introducing the feature-level constraints as an alternative to the costly sequential KL (Zeng et al., 2024). We use Sparse Autoencoders (SAEs) (Huben et al., 2024), which generate representations where only a few features are active, enhancing computational efficiency (See Figure 2 Right). Furthermore, regularization in the coefficient space promotes sparsity, stability, and uniqueness in the model’s representations. Since SAEs produce sparse representations, only a few dozen out of 16,000 features are active at any given time (Lieberum et al., 2024). Compared to SimPO, FPO is as efficient in memory and time complexity, yet has improved controllability due to feature-level constraints; compared to constraint-based methods like TDPO, FPO matches the computational and memory efficiency of methods such as SimPO, and has potentially improved performance as feature-level control can give stronger generalization than token-level control. A contrast between FPO, DPO, SimPO and TDPO is shown in Figure 1. Our experiments demonstrate that FPO consistently outperforms state-of-the-art methods based on different sizes of backbone LLMs, achieving up to 5% absolute improvements in win rate (See Table 2) based on AlpacaEval-2 and Arena-Hard benchmarks, up to 0.5 scores on MT-Bench and competitive output diversity. By constraining the shifts of these features during the training process, we can achieve results that meet or even exceed the effectiveness of sequential KL, at a significantly lower computational cost (17.6% reductions compared to TDPO2 as shown in Figure 4 Left). Additionally, we introduce detailed ablation studies to show that our method maintains a stable performance over different temperatures and the selection of SAE layers. Overall, we show that FPO enjoys the efficiency of SimPO by using the offline reference control, while also the constraint quality of sequential KL by using the sparse feature-level constraints. To our knowledge, this is the first approach that integrates sparse feature-level constraints into LLM alignment. By incorporating sparse autoencoders with token-level DPO, FPO makes practically meaningful and theoretically solid improvements over existing preference optimization methods along three dimensions: simplicity of implementation, efficiency, and generation diversity."
https://arxiv.org/html/2411.07602v1,Circuit Complexity Bounds for RoPE-based Transformer Architecture,"Characterizing the express power of the Transformer architecture is critical to understanding its capacity limits and scaling law. Recent works provide the circuit complexity bounds to Transformer-like architecture. On the other hand, Rotary Position Embedding (\mathsf{RoPE}) has emerged as a crucial technique in modern large language models, offering superior performance in capturing positional information compared to traditional position embeddings, which shows great potential in application prospects, particularly for the long context scenario. Empirical evidence also suggests that \mathsf{RoPE}-based Transformer architectures demonstrate greater generalization capabilities compared to conventional Transformer models. In this work, we establish a tighter circuit complexity bound for Transformers with \mathsf{RoPE} attention. Our key contribution is that we show that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \mathrm{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the arithmetic problem or the Boolean formula value problem. This result significantly demonstrates the fundamental limitation of the expressivity of the \mathsf{RoPE}-based Transformer architecture, although it achieves giant empirical success. Our theoretical framework not only establishes tighter complexity bounds but also may instruct further work on the \mathsf{RoPE}-based Transformer.","Recently, Large Language Models (LLMs), such as GPT-4 [1], Claude [3], Llama [55], and more recently, OpenAI’s o1 [60], have exhibited remarkable potential to revolutionize numerous facets of daily life, including conversational AI [40], AI agents [75, 21], search capabilities [60], and AI assistants [38, 23], among others. One of the most significant emergent capabilities of LLMs is their proficiency in handling long-context information, which is essential for effectively processing complex documents such as academic papers, official reports, and legal texts. LLMs also have demonstrated exceptional capabilities in tackling long-context tasks, such as zero-shot summarization [14, 79] and sustaining coherent, extended conversations [76, 56]. The o1 model from OpenAI [60] represents a major advancement in this field. By leveraging Chain-of-Thought (CoT) reasoning [74, 37] and incorporating Retrieval Augmented Generation (RAG) [48, 26], it showcases a level of expertise comparable to PhD-level problem solving, with both techniques heavily relying on extensive contextual understanding. Large Language Models (LLMs) are primarily built upon the Transformer architecture [69], which uses the self-attention mechanism as its core component. Given this foundational structure, an important question arises: what computational primitives can the components of the Transformer implement, and what problems can the entire system solve collectively? These questions are crucial for interpreting Transformers in a principled manner, understanding the potential limitations of their reasoning capabilities, and fostering trust in deployed Transformer-based systems. To address the aforementioned questions and to investigate the expressiveness of transformers, prior research has made significant strides. Recent studies, such as [58], have established two key results concerning both non-uniform and \mathsf{L}-uniform settings: first, any depth-d transformer with c\log n-precision can be simulated by a threshold circuit family with constant depth; second, such a transformer can also be simulated by a \mathsf{L}-uniform threshold circuit family of constant depth. Further advancing these findings, [57] demonstrate that \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} circuits are capable of simulating softmax-attention transformers. Building on this foundation, [16] refine these results by increasing the accuracy of approximation. They enhance the precision for softmax-attention transformers from O(\log n) to O(\operatorname{poly}(n)), confirming that these transformers fall within the \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} class. Additionally, they show that a softmax-attention transformer with an absolute error bound of 2^{-O(\operatorname{poly}(n))} is also contained within \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0}. On the other hand, first introduced by [61], Rotation Position Embedding (\mathsf{RoPE}) enhances Transformers by encoding both absolute and relative positional information through a rotation matrix, enabling greater sequence length flexibility, improved attention mechanism efficiency, and better performance on long-text tasks, exemplified by \mathsf{RoPE}-based language models that can summarize an entire book in a single pass. Due to these advantageous properties, \mathsf{RoPE} has been widely adopted in numerous empirical studies [20, 9, 12]. However, despite its considerable success, the underlying mechanisms of \mathsf{RoPE} remain largely unknown, posing an intriguing mystery in the field. A natural question arises: Does \mathsf{RoPE} enhance the expressiveness of the Transformer-based Large Language Model? This work aims to address this question from the perspective of circuit complexity, taking a step forward in theoretically understanding the underlying mechanisms of \mathsf{RoPE}. We present a rigorous theoretical investigation of \mathsf{RoPE}-based Transformers, establishing fundamental limits on their computational power. Our core approach involved a systematic examination of the circuit complexity for each component of the \mathsf{RoPE}-based architecture, from the basic trigonometric functions to the complete attention mechanism. Ultimately, we prove that these models can be simulated using uniform \mathsf{TC}^{0} circuits. Furthermore, we show that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, \mathsf{RoPE}-based Transformers with \operatorname{poly}(n)-precision, O(1) layers, and a hidden dimension d\leq O(n) are unable to solve either arithmetic formula evaluation or Boolean formula value problems. This finding is significant because it uncovers fundamental expressivity limitations of \mathsf{RoPE}-based architectures, even though they have shown empirical success in modern language models. Beyond Merrill and Sabharwal [58, 57] and Chiang [16], our contribution are summarized as follows: • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, constant-depth, \operatorname{poly}(n)-size can be simulated by a \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} circuit family (Theorem 4.8). • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the arithmetic formula evaluation problems (Theorem 5.8). • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the Boolean formula value problem (Theorem 5.9). Roadmap. In Section 2, we review the related work. In Section 3, we introduce some important computation concepts and Transformer definitions essential for the subsequent sections. In Section 4, we give the circuit complexity result of \mathsf{RoPE}-based Transformers. In Section 5, we give our hardness results. In Section 6, we summarizes our theoritical results."
https://arxiv.org/html/2411.07595v1,Entropy Controllable Direct Preference Optimization,"In the post-training of large language models (LLMs), Reinforcement Learning from Human Feedback (RLHF) is an effective approach to achieve generation aligned with human preferences. Direct Preference Optimization (DPO) allows for policy training with a simple binary cross-entropy loss without a reward model. The objective of DPO is regularized by reverse KL divergence that encourages mode-seeking fitting to the reference policy. Nonetheless, we indicate that minimizing reverse KL divergence could fail to capture a mode of the reference distribution, which may hurt the policy’s performance. Based on this observation, we propose a simple modification to DPO, H-DPO, which allows for control over the entropy of the resulting policy, enhancing the distribution’s sharpness and thereby enabling mode-seeking fitting more effectively. In our experiments, we show that H-DPO outperformed DPO across various tasks, demonstrating superior results in pass@k evaluations for mathematical tasks. Moreover, H-DPO is simple to implement, requiring only minor modifications to the loss calculation of DPO, which makes it highly practical and promising for wide-ranging applications in the training of LLMs.","Large language models (LLMs) have exhibited remarkable performance across various tasks (OpenAI et al., 2023; Dubey et al., 2024). However, large datasets often include data created for various purposes, and the models trained on these datasets are not always suitable for users’ specific needs. Additionally, some datasets include malicious text and code related to cyberattacks, posing risks of misuse by humans or the AI itself (Bender et al., 2021; Bai et al., 2022; Ji et al., 2023; Shevlane et al., 2023). Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022) is an effective approach to make an LLM follow human instructions and suppressing undesired outputs. In RLHF, a reward model is trained based on data evaluated according to human preferences. The LLM then learns to maximize rewards, aligning its outputs with human preferences. To prevent significant deviation from the original model, regularization using reverse KL divergence is added to the reward maximization process, and RL algorithms such as PPO (Schulman et al., 2017) are employed. However, RLHF has issues such as high computational costs, the reliance on a learned reward model, and the inherent instability and hyperparameter sensitivity of RL algorithms. To address these problems, Direct Policy Optimization (DPO) (Rafailov et al., 2023) has emerged and is now widely used. DPO proposes a loss function that directly optimizes the policy through a change of variables, eliminating the need for the reward model and allowing training with a simple binary cross-entropy loss. While more stable and lightweight than RLHF, DPO can optimize the same objective function as RLHF, which involves reward maximization and regularization with the reverse KL divergence. Other types of divergences have also been proposed to prevent deviation from the original model (Wang et al., 2024a), but reverse KL divergence, which enables mode-seeking estimation, is generally preferred for performance. (a) \min_{\pi}D_{\text{KL}}(\pi||\pi_{\text{ref}}) (b) \min_{\pi}D_{\alpha}(\pi||\pi_{\text{ref}}) Figure 1: For a Gaussian mixture model \pi_{\text{ref}}, \hat{\pi} that minimizes D_{\text{KL}} (left) and \hat{\pi} that minimizes D_{\alpha}=-\alpha H(\pi)+H(\pi,\pi_{\text{ref}}) with \alpha=0.6 (right). Using D_{\alpha} results in successful mode-seeking estimation. We point out that minimizing reverse KL divergence can cause the mode of the fitted distribution to fail to capture the mode of the target distribution. As shown in Figure 1, consider fitting a unimodal distribution to a multimodal distribution. We call the way of fitting a distribution mode-seeking when one of the modes of target distribution is captured by the fitted model as shown in the right side of Figure 1, and mode-covering when all the modes are covered as shown in the left side of Figure 1. In the case of mode-seeking, the fitted distribution discards other modes of the target distribution, resulting in smaller variance than the target distribution. However, reverse KL minimization can fail at mode-seeking fitting due to its nature of preserving variance, as illustrated in the left side of Figure 1. To enable variance reduction and encourage mode-seeking estimation, we generalize the loss function of DPO, named H-DPO, which allows for controlling the distribution’s entropy H(\pi) by modifying the regularization term. H-DPO can adjust the entropy of generations of the LLM during training using the hyperparameter \alpha in Equation 9 introduced later. By setting \alpha less than 1, it encourages the entropy to be reduced so that achieves mode-seeking fitting more successfully. The right side of Figure 1 demonstrates that our regularizer D_{\alpha}, a modification to the reverse KL, enables mode-seeking fitting even in cases where reverse KL fails, as shown on the left. Using our proposed loss with \alpha<1, the estimated policy distribution is expected to be sharper or more deterministic, which we consider a beneficial feature rather than a problem. Traditional LLMs use a softmax function with a temperature parameter to represent distributions over raw outputs, where the temperature is set to 1 during training. When LLMs are evaluated, a lower value such as 0.6 often performs better (Xu et al., 2022; OpenAI et al., 2023; Zhu et al., 2024). This post-training sharpening lacks guarantees of optimality for the objective function. In contrast, our proposed method trains the language model using an objective function aimed at sharpening the distribution, ensuring that this sharper distribution aligns with the objective function. Our main contribution is the alignment method H-DPO, which allows controlling entropy and encourages mode-seeking fitting more than DPO. The implementation of H-DPO is simple, requiring minimal modifications to DPO. Experiments included alignment based on Mistral-7B (Jiang et al., 2023) with the Zephyr framework (Tunstall et al., ; Tunstall et al., 2023). Compared to DPO, our proposed method allows for more diverse generations without losing performance, and shows superior accuracy and coverage across various tasks."
https://arxiv.org/html/2411.07546v1,"Contrastive Language Prompting to Ease 
False Positives in Medical Anomaly Detection","A pre-trained visual-language model, contrastive language-image pre-training (CLIP), successfully accomplishes various downstream tasks with text prompts, such as finding images or localizing regions within the image. Despite CLIP’s strong multi-modal data capabilities, it remains limited in specialized environments, such as medical applications. For this purpose, many CLIP variants—i.e., BioMedCLIP, and MedCLIP-SAMv2—have emerged, but false positives related to normal regions persist. Thus, we aim to present a simple yet important goal of reducing false positives in medical anomaly detection. We introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both positive and negative text prompts. This straightforward approach identifies potential lesion regions by visual attention to the positive prompts in the given image. To reduce false positives, we attenuate attention on normal regions using negative prompts. Extensive experiments with the BMAD dataset, including six biomedical benchmarks, demonstrate that CLAP method enhances anomaly detection performance. Our future plans include developing an automated fine prompting method for more practical usage.","Input Ground-truth A_{\textit{positive}} A_{\textit{negative}} A_{\textit{CLAP}} (Ours) Normal Brain MRI False positives True negatives Chest X-ray False positives True negatives Abnormal Brain MRI False positives True negatives Chest X-ray False positives True negatives Fig. 1: Generated attention maps by leveraging BiomedCLIP [1]. A_{\textit{positive}} and A_{\textit{negative}} are the attention maps obtained using positive or negative prompts only. A_{\textit{CLAP}} shows results of our proposal, dubbed Contrastive LAnguage Prompting (CLAP). CLAP leverages both positive and negative prompts. The negative prompts are used to attenuate false positive attention of normal regions. Fig. 2: Schematic diagram of our method. Existing positive prompt methods only utilize positive prompts. In this situation, the false positive attention issue remains. In comparison, our method CLAP successfully suppresses false positives by additionally exploiting negative prompts, shown in (a). After getting the attention map of CLAP, we employ the existing UAD model EAR [2]. We only replace the saliency map for mosaic obfuscation with an attention map from CLAP, shown in (b). Recent advancements in multi-modal models, particularly visual-language models (VLMs), have revolutionized various downstream tasks, such as image retrieval, captioning, and object localization. Among these, Contrastive Language–Image Pre-training (CLIP) [3] has demonstrated remarkable performance by leveraging natural language prompts to interpret visual data. This capability enables CLIP to handle a wide array of tasks without specialized fine-tuning. However, its application to highly specialized domains, such as medical imaging, has uncovered limitations. In the medical field, accurate anomaly detection is crucial for early diagnosis and treatment. Nevertheless, general-purpose models like CLIP often struggle with the intricacies of medical images, which contain subtle and features unique to medical imaging essential for identifying pathological regions. To improve performance in biomedical domains, various adaptations of CLIP, such as BiomedCLIP [1] and MedCLIP-SAMv2 [4], have been proposed to improve performance in biomedical domains. They shows surprising enhancement of medical reasoning compared to ordinary models, but the issue of false positives—incorrectly identifying normal regions as anomalous—remains prevalent. These false positives can lead to unnecessary medical procedures, increasing the burden on healthcare systems and potentially harming patients. To address this issue, we propose a novel method called Contrastive LAnguage Prompting (CLAP), which introduces a more refined way of leveraging natural language prompts for medical anomaly detection. By leveraging both positive and negative prompts, our method aims to find out lesions accurately with CLIP attention. Positive prompts guide the CLIP attention toward potential lesion regions, while negative prompts help attenuate the attention on normal regions, thereby reducing the occurrence of strong attention to false positives. This approach not only provides a more improved understanding of the medical image but also aligns with the demands for reliability in medical diagnostics by artificial intelligence. We can just determine whether disease or not based on the CLIP attention. Toward a more accurate diagnosis, we employ an unsupervised anomaly detection (UAD) method that features a reconstruction-by-inpainting strategy [2]. For this, we obfuscate strong attention regions, over \mu+0.674\sigma (Q3) [5] valued regions, by considering suspected disease regions. Then, we attempt to reconstruct obfuscated regions into normal patterns by U-Net which is trained with normal samples only. Finally, we determine the final disease based on the reconstruction error obtained. To evaluate the legitimacy of our proposal, we perform extensive experiments using BMAD dataset [6]. This dataset provides six benchmarks for five anatomies. Visual comparisons demonstrate that CLAP successfully overcomes issues of strong attention in non-lesion regions. In addition, we improved UAD performance compared to existing methods. Through this work, we aim to bridge the gap between general-purpose VLMs and the specific needs of medical anomaly detection. We conclude by discussing the potential for automating language prompt construction to further improve the usability of this approach in real-world clinical settings."
https://arxiv.org/html/2411.07528v1,Logs are All You Need in Security,"Large and Small Language Models (LMs) are typically pretrained using extensive volumes of text, which are sourced from publicly accessible platforms such as Wikipedia, Book Corpus, or through web scraping. These models, due to their exposure to a wide range of language data, exhibit impressive generalization capabilities and can perform a multitude of tasks simultaneously. However, they often fall short when it comes to domain-specific tasks due to their broad training data. This paper introduces SecEncoder, a specialized small language model that is pretrained using security logs. SecEncoder is designed to address the domain-specific limitations of general LMs by focusing on the unique language and patterns found in security logs. Experimental results indicate that SecEncoder outperforms other LMs, such as BERT-large, DeBERTa-v3-large and OpenAI’s Embedding (text-embedding-ada-002) models, which are pretrained mainly on natural language, across various tasks. Furthermore, although SecEncoder is primarily pretrained on log data, it outperforms models pretrained on natural language for a range of tasks beyond log analysis, such as incident prioritization and threat intelligence document retrieval. This suggests that domain-specific pretraining with logs can significantly enhance the performance of LMs in security. These findings pave the way for future research into security-specific LMs and their potential applications.","Transformers [65] are a breakthrough AI architecture that have facilitated the development of various Language Models (LMs) [27, 3]. These models can harness huge amounts of data to perform diverse tasks across language and other modalities such as audio, image and video. Some examples of LMs are BERT, RoBERTa, GPT, Gemini and PaLM [48, 19, 23, 17, 24, 25, 4, 5]. LMs can differ in their size, data sources, learning objectives, and are trained on large collections of text, such as Wikipedia, books, news articles, code, social media posts or web scraping. Certain language models, such as encoder-only models, are designed to encode the semantic and syntactic information of natural language into high-dimensional vectors, enabling them to perform downstream tasks like search, classification, summarization, translation, and question answering. Decoder-only models, on the other hand, are capable of generating natural language text by sampling from their probability distributions, producing coherent and fluent outputs. Some models, such as GPT-o and Gemini 1.5, extend these capabilities to multimodal content generation, creating images, audio, and video by utilizing a shared latent space across different modalities. Language models (LMs) have achieved state-of-the-art results across a wide range of benchmarks in natural language processing, coding, mathematics, and reasoning. They have also demonstrated impressive abilities to generate realistic and creative content, including stories, poems, songs, and jokes. These advancements in LMs have unlocked new opportunities and introduced challenges for AI research and applications, spanning areas such as natural language understanding, natural language generation, multimodal fusion, and knowledge extraction. However, LMs are not ideally suited to address domain-specific challenges, such as specialized vocabulary, terminology, knowledge, and logic, due to their design for broad and diverse applications. Previous studies highlight these limitations across several domains. In Biomedicine [31, 49], LMs struggle to capture complex relationships and semantics of biomedical entities and concepts. In Finance [66], LMs underperform compared to domain-specific counterparts. In Medicine [40], LMs lack alignment with clinical utility. In Security [38, 21], LMs fall short in domain-specific security knowledge. Similarly, in Software [33], LMs face challenges in interpreting and making sense of operational logs. Despite these limitations, the potential of Artificial Intelligence (AI), particularly generative AI, continues to attract significant interest from the security community [11, 2, 6]. These generative models hold potential as valuable tools for security professionals, serving as copilots to navigate the complexities of security tasks such as identifying phishing emails, crafting detections, or analyzing and summarizing incidents. However, the field of security presents unique challenges that can only be partially addressed by generative models. A notable challenge is the need for security professionals to handle a variety of data types beyond natural language texts, including logs and telemetries. These data are often heterogeneous, noisy, and voluminous, necessitating efficient and scalable processing and analysis methods. In this paper, we present SecEncoder, a small language model that is trained with security logs. SecEncoder is an encoder-only model, pretrained on a large corpus of security logs, which capture various events and activities related to security incidents and operations. SecEncoder aims to demonstrate the feasibility and utility of training a domain-specific language model on security logs at scale, and to provide a versatile and powerful model that can be applied to various security use cases. We evaluate SecEncoder on both intrinsic and extrinsic tasks, such as log analysis, anomaly detection, log search and incident classification. Our main contributions are: • We pretrain a security-specific small language model from scratch on a large and diverse corpus of security logs, which capture various events and activities related to security incidents and operations. We aim to train a versatile and powerful model that can generalize to various type of security use cases. • We evaluate SecEncoder using both intrinsic and extrinsic measures, using both internal and publicly available benchmarks. We also compare SecEncoder to the other LMs, such as BERT, DeBERTa and OpenAI’s embedding model (text-embeddings-ada-002), and show that SecEncoder outperforms the best results on most of the tasks, and also exhibits some unique and novel capabilities. • We present four real-world use cases for SecEncoder. Notably, some of these use cases such as incident classification and threat intelligence document retrieval demonstrate that, despite SecEncoder being primarily trained on logs, it can effectively generalize to other data modalities without specific training on them. This finding suggests that logs could serve as valuable data sources for pretraining language models across domains beyond security. • We discuss the limitations and future directions for SecEncoder, focusing on areas such as data quality and diversity, as well as improvements in robustness and inference speed. The remainder of this paper is structured as follows: Section 2 discusses related work, while Section 3 introduces the overall architecture and design. Section 4 details the various experiments conducted for testing and evaluation. Section 5 explains multiple real world use cases of SecEncoder and the corresponding results. Section 6 delves into limitations of SecEncoder and discusses future work, and finally, Section 7 provides the conclusion."
https://arxiv.org/html/2411.07516v1,SparrowVQE: Visual Question Explanation for Course Content Understanding,"Visual Question Answering (VQA) research seeks to create AI systems to answer natural language questions in images, yet VQA methods often yield overly simplistic and short answers. This paper aims to advance the field by introducing Visual Question Explanation (VQE), which enhances the ability of VQA to provide detailed explanations rather than brief responses and address the need for more complex interaction with visual content. We first created an MLVQE dataset from a 14-week streamed video machine learning course, including 885 slide images, 110,407 words of transcripts, and 9,416 designed question-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3 billion parameters multimodal model. We trained our model with a three-stage training mechanism consisting of multimodal pre-training (slide images and transcripts feature alignment), instruction tuning (tuning the pre-trained model with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide image and QA pairs). Eventually, our SparrowVQE can understand and connect visual information using the SigLIP model with transcripts using the Phi-2 language model with an MLP adapter. Experimental results demonstrate that our SparrowVQE achieves better performance in our developed MLVQE dataset and outperforms state-of-the-art methods in the other five benchmark VQA datasets. The source code is available at https://github.com/YoushanZhang/SparrowVQE.","Visual Question Answering (VQA) is an interdisciplinary problem that combines computer vision with natural language processing in answering questions regarding images, aiming to recognize and localize objects and information presented in a context. It makes positive differences in various applications, such as aiding visually impaired people, supporting educational tools, and developing user interfaces of human-computer interaction [1]. Figure 1: Our SparrowVQE matches the performance of 7B models in numerous visual language tasks, standing out from general-purpose text and visual language models. A major challenge for the development of VQA is the large diversity of questions that can be asked, from simple identifying tasks, such as “What is in the picture?”, to complex queries requiring sophisticated comprehension and inferential reasoning of the relationships and stories in the visual content. Applying VQA in educational settings, specifically machine learning (ML) lectures, can be more complex, as ML lectures often include complex diagrams, mathematical formulas, and dense textual information. Traditional educational resources often fail to provide the engagement and context-aware assistance necessary for effective learning. This gap results in difficulties in bridging theoretical knowledge with practical understanding in the educational context. Recently, several education chatbots or VQA systems were developed to improve students’ learning experience in different education levels [2, 3, 4, 5, 6, 7]. However, the models’ performance varies with preliminary evaluation and mostly produce short answers instead of detailed explanation of questions. The educational VQAs still face the problem of limited training data and over-simplistic answers produced. In this paper, we propose an MLVQE dataset for model training in the machine learning setting, specifically to achieve automatic teaching. We also propose a SparrowVQE model to enrich the VQA application in education. It caters to ML learners by allowing them to ask questions about visual content directly. Our work directly contributes to the improvement of effectiveness and personalizing of learning experiences. We improve our model’s efficiency in interpreting slide-text/transcript pairs, making it outperform state-of-the-art models in VQA tasks in different contents, as shown in Fig. 1. Our model has the potential to be applied in other educational domains that require training with more datasets."
https://arxiv.org/html/2411.07464v1,BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks,"Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4. Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting. With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls. Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning. With 94.2% reduction in the cost (from $0.931 per run cost averaged over all tasks for GPT-4 single agent system to $0.054), our system is able to yield better average success rate of 32.95% as compared to GPT-4 single-agent system yielding 22.72% success rate averaged over all the tasks of MLAgentBench.","Although recent advances have shown that Large Language Models (LLMs) are adept at handling a vast array of applications ranging from natural language (Fang et al., 2024; Huang and Chang, 2023; Zhu et al., 2023; Yi et al., 2024) to code-related tasks (Zheng et al., 2024; Zan et al., 2023; Zhang et al., 2024a), this capability does not often translate to more complicated and nuanced tasks (Yeadon et al., 2024). Most code-related efforts involving LLMs (Guo et al., 2024; Huang et al., 2024; Zhong et al., 2024) are based on tasks such as HumanEval (Chen et al., 2021) and MBXP (Athiwaratkun et al., 2023), that have a relatively easier level of complexity that is far from what is experienced by data scientists. However, real-world engineering challenges demand nuanced problem-solving and intricate planning, often involving multiple rounds of strategizing, experimentation, and recalibration. LLM agent systems excel in simulating this iterative process, since they comprise of an environment containing code files, description files and data files and a pre-defined action space allowing interaction with the environment. This demonstrates their capability to address intricate engineering challenges effectively. (Zhang et al., 2024b). Transitioning to codifying Machine Learning (ML) applications brings its own challenges since they often involve training models on datasets, tuning hyperparameters, devising ways to improve performance, etc. These applications are not straightforward and require a deep understanding of the underlying algorithms and techniques along with specific libraries used for implementation of plans. Although there exist AutoML-based approaches for automating such tasks (He et al., 2021; Salehin et al., 2024), these offer limited flexibility since they typically operate within predefined constraints and search spaces in the form of possible configurations of architectures and/ or hyper-parameters, which may limit their ability to explore solutions out-of-distribution of the search space. While works such as ChatDev (Qian et al., 2023) and MetaGPT (Hong et al., 2023) have explored the capabilities of LLM Agents in a software development environment, there is a notable scarcity of research on utilizing LLM Agents for solving ML tasks. Recent works like MLCopilot (Zhang et al., 2024c) introduce an assistant for solving ML tasks. However, these architectures are limited in the types of problems they can address and must strictly follow task description formats that do not align with real-world scenarios. Additionally, such assistants only suggest solutions, leaving the actual burden of implementation to the user. To the best of our knowledge, MLAgentBench (Huang et al., 2023) is the only significant benchmark addressing ML problem solving capabilities of LLM Agents directly dealing with code. Although they get good performance on some tasks in their benchmark, they focus on single-agent systems using expensive LLMs such as GPT-4, which costs approximately $0.52-$2.9 per run, depending on the task. For the experiments they conduct, they go for 8 runs per task for 15+ tasks, leading to very high experimental cost of approximately $200+. With such larger models becoming increasingly expensive to use, there is a natural incentive to develop no-cost or low-cost systems using smaller, open-source models and making them equally capable for niche tasks. However, existing agent creation frameworks like AutoGen (Wu et al., 2023) do not prioritize cost-reduction. Replacing single-agent systems using expensive LLMs with single-agent smaller, open-source LLMs may not serve the purpose. Our initial experiments with replacing all LLM calls in Huang et al. (2023) for auto-generating codes for ML tasks, with no or low-cost LLMs, namely, Gemini-Pro (Team et al., 2023)111gemini-pro 1.0 API from https://ai.google.dev/tutorials/python_quickstart. The rate limit for the free or no-cost version is sufficient for conducting our experiments, however, we also include costs for a no-cost version with pay-as-you-go pricing, CodeLlama (Rozière et al., 2024)222https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf and Mixtral(Jiang et al., 2024)333Mixtral-8x7B-v0.1 https://huggingface.co/mistralai/Mixtral-8x7B-v0.1, yield very poor results for all of the tasks in a single-agent setting. In real-world setting any complicated tasks are rarely tackled by a single individual alone, especially when all the individuals do not possess the required expertise to perform the task. Instead, teams of engineers collaborate, with each member having a unique role (persona) and contributing unique expertise and skills to achieve the target with collective efforts. Past works on LLM agents have simulated this real-world setting by designing multi-agent frameworks (Li et al., 2024; Shen et al., 2024), combining LLM experts (Wang et al., 2023; Ding et al., 2024) and defining cascades (Chen et al., 2023; Yue et al., 2024; Zhang et al., 2023) for tasks such as code generation, reasoning, question answering, etc. Cascades refer to the chaining of LLMs in a progressive fashion, where, a weaker LLM is invoked first and if the response is not satisfactory then stronger LLMs are invoked. However, to the best of our knowledge multi-agent frameworks with open-source LLMs as agents have not be explored for engineering of ML tasks. In this paper, we address the gap of utilizing LLMs for solving ML tasks by proposing a system that leverages - (i) Multi-LLM Agents as a combination of experts using profiling, (ii) LLM Cascades, (iii) Efficient retrieval of relevant past observations, and (iv) our novel occasional ask-the-expert calls to GPT-4 444gpt-4-0125-preview https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo for planning. Our approach aims to bridge the divide between capabilities of cheaper LLMs and the requirements of complex ML tasks, offering a more cost-efficient and scalable solution. Through empirical analysis, we validate the following claims: • Our best performing multi-agent system using no-cost or low-cost versions of Gemini-Pro as the base LLM, is able to perform tasks at a fraction of the cost (on an average average $0.054 for no-cost and $0.120 for low-cost version per run per ML task in MLAgentBench Dataset) as compared to benchmarked single-agent GPT4 system presented in Huang et al. (2023) (on an average $0.931 per run per task) • With 94.2% and 87.1% cost reduction for the no-cost and low-cost Gemini-Pro versions, our best performing multi-agent system is able to yield better success rate of 32.95% averaged for all the tasks in MLAgentBench as compared to the GPT4 based single-agent system yielding 22.72% average success rate for all tasks. • Our best performing multi-agent system is able to achieve equal or better performance for 45.45% of tasks when compared to the GPT4-based Single-Agent system in Huang et al. (2023), whereas it yields comparable performance for other tasks"
https://arxiv.org/html/2411.07279v1,"The Surprising Effectiveness of 
Test-Time Training for Abstract Reasoning","Language models have shown impressive performance on tasks within their training distribution, but often struggle with novel problems requiring complex reasoning. We investigate the effectiveness of test-time training (TTT)—updating model parameters temporarily during inference using a loss derived from input data—as a mechanism for improving models’ reasoning capabilities, using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through systematic experimentation, we identify three crucial components for successful TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and augmentations (3) per-instance training. TTT significantly improves performance on ARC tasks, achieving up to 6\times improvement in accuracy compared to base fine-tuned models; applying TTT to an 8B-parameter language model, we achieve 53% accuracy on the ARC’s public validation set, improving the state-of-the-art by nearly 25% for public and purely neural approaches. By ensembling our method with recent program generation approaches, we get SoTA public validation accuracy of 61.875%, matching the average human score. Our findings suggest that explicit symbolic search is not the only path to improved abstract reasoning in neural language models; additional test-time applied to continued training on few-shot examples can also be extremely effective.","Large-scale neural language models (LMs) excel at performing tasks that occur in their training data, and often elementary variations or compositions of those tasks (Brown et al., 2020; Todd et al., 2024). Given natural language task specifications or a small number of examples, LMs often successfully infer the desired task and produce an appropriate output. But can LMs also solve new problems, involving non-trivial reasoning, planning, or string manipulation of a kind very different from their pre-training data? This question is central to understanding the novel skill acquisition capabilities of current AI systems, which has been proposed as a key measure of intelligence (Chollet, 2019). For complex and novel tasks, it is often difficult to obtain a correct answer simply by sampling from an LM (Wu et al., 2023). However, a significant finding in recent years has been that LM performance can be substantially improved by augmenting LM decoding with additional test-time computation. Methods in this category include chain-of-thought prompting (Wei et al., 2022), sampling with majority voting (self-consistency; Wang et al., 2022), code execution (Brown et al., 2024; Snell et al., 2024; Damani et al., 2024), and search (Yao et al., 2024). One scaling strategy that has gained recent attention is test-time training (TTT), in which models are updated through explicit gradient steps based on test-time inputs (Krause et al., 2018; 2019). This method differs from standard fine-tuning as it operates in an extremely low-data regime—typically via an unsupervised objective on a single input, or a supervised objective applied to one or two in-context labeled examples. Modern versions of this approach was proposed for vision models by Sun et al. (2020), and also applied to sequence models by Gandelsman et al. (2022). The design space for TTT approaches is large, and there is currently a limited understanding of which design choices are most effective for LMs (and specifically for novel-task learning). In this paper, we systematically study the impact of various TTT design choices, as well as its interaction with pre-training and sampling schemes. We evaluate these methods in the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), a collection of extremely challenging few-shot visual reasoning problems. ARC is an ideal benchmark for testing the limits of LM generalization as it presents novel tasks, in a novel format, requiring nontrivial search and inference capabilities. Current language models perform poorly on ARC. Most successful approaches have relied on program synthesis techniques (Butt et al., 2024; Ainooson et al., 2023; Huang et al., 2023), though recently Cole et al. (2024) reported promising results using TTT on the benchmark. We identify several crucial ingredients for effective application of TTT to few-shot learning: (1) initial fine-tuning on synthetic tasks similar to those encountered at test time, (2) an augmented, leave-one-out task generation strategy for constructing the test-time dataset, (3) per-instance adapter training and (4) a self-consistency (Wang et al., 2022) approach under invertible transformations. With careful choices of these components, TTT can significantly improve LM performance on ARC—increasing accuracy by up to a factor of six over a 1B model, and achieving state-of-the-art results for published, purely neural models on the ARC task with a 8B model. Indeed, our results show that when equipped with test-time training, ordinary LMs can match or exceed the performance of many neuro-symbolic approaches on ARC. Our main contributions111Our implementation can be found at this link. are: 1. We identify and systematically analyze the key components needed for test-time training on ARC tasks, with a a novel test time training data generation and self-consistency component. 2. We achieve state-of-the-art results among published neural approaches on the ARC validation set: • 53% accuracy on the public validation set with a 8B parameter model. • 61.875% accuracy when ensembled with program synthesis approaches, comparable to average human performance on the dataset. 3. We demonstrate that tasks that could only be solved by program synthesis previously can be solved with fully neural approaches equipped with our TTT framework. These results challenge the assumption that symbolic components are strictly necessary for solving such complex tasks. Instead, they suggest that the critical factor in solving novel reasoning problems may be the allocation of proper computational resources during test time, perhaps independently of whether these resources are deployed through symbolic or neural mechanisms."
https://arxiv.org/html/2411.07264v1,Multi-Document Financial Question Answering using LLMs,"We propose two new methods for multi-document financial question answering. First, a method that uses semantic tagging, and then, queries the index to get the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that uses semantic tagging, and, retrieves knowledge graph triples from a graph database, as context. KG_RAG uses knowledge graphs constructed using a small model that is fine-tuned using knowledge distillation using a large teacher model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet, NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of questions in the data consists of 111 complex questions including many esoteric questions that are difficult to answer and the answers are not completely obvious. As evaluation metrics, we use overall scores as well as segmented scores for measurement including the faithfulness, relevance, correctness, similarity, an LLM based overall score and the rouge scores as well as a similarity of embeddings. We find that both methods outperform plain RAG significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.","LLMs are now increasingly used for various linguistics tasks like information extraction [1] [2] [3], language understanding [4] [5] , named entity recognition [6] [7] [8], text summarization (including query based summarization) [9] [10] [11], translation [12] [13], speech synthesis [14], question answering [15] [16] [17] [18], code generation/understanding [19], and other computational linguistics tasks. In this work, we focus our attention on financial question answering. Specifically multi-document financial question answering. We use 10k reports from six companies for three years. This is different from asking questions about one document specific to a year, domain and organization, which is a significantly easier problem. Our method, which uses semantic tagging and knowledge graphs can scale to a very large corpus of documents including multiple publication dates, multiple domains, and multiple industries. Financial question answering using LLMs has been recently studied [20] [21] and many attempts have been successful and widely used. There are a few benchmark datasets for financial linguistics [22] [23] [16] [17] but most offer single document question answering. In this paper, we propose two new methods, RAG_SEM and KG_RAG for multi-document question answering. We perform experiments using 18 10k reports, spanning three years and 111 questions. Multi-document question answering has been studied recently (including methods based on knowledge graphs) [24] but the exploration of this subject has been limited. Knowledge graphs are very suitable for multi-document question answering because of the fine-grained semantics that they model. Works such as [10] could be explored for this purpose. Specialized models fine-tuned on large financial text corpuses have been shared for the general public to consume [25] [26] [21]. However, their use has been limited due to the risk of hallucinations, that increases for models trained using transfer learning [27]. We do not fine-tune models that generate answers, but we do fine-tune a KG generator, for speed and cost saving. In this work, we introduce two new methods for multi-document financial question answering. Multi-document question answering is difficult because of the fact that the context (RAG) is generated from all documents rather than a document specific to a domain, industry or organization. This makes it challenging as the synthesizer LLM may start giving wrong answers when asked a question about a specific entity. For example, if the question is ""How are companies’ revenues in the finance industry?"" might retrieve information about closely related topics, like economics, business, investing, insurance or lending. This may impact the overall effectiveness of the synthesized response. Figure 1: Segmented Comparison of the Three Methods of Financial Question Answering Figure 2: Comparison of the Three Methods of Financial Question Answering Table 1: Experiment Results (18 10K Reports and 111 Questions) Metric RAG RAG_SEM KG_RAG Relevance 80% 84% 85% Correctness 77% 81% 83% Faithfulness 77% 82% 83% Similarity 73% 78% 80% LLM Score 89% 92% 92% Embeddings 89% 92% 92% rouge1 52.9% 53.8% 53.8% rouge2 19% 19.5% 19.5% rougeL 23.2% 23.8% 23.8%"
https://arxiv.org/html/2411.07240v1,"UTMath: Math Evaluation with Unit Test 
via Reasoning-to-Coding Thoughts","The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and adaptability. This paper introduces the UTMath Benchmark, which robustly evaluates the models through extensive unit tests. It consists of 1,053 problems across 9 mathematical domains, with over 68 test cases per problem. We propose an innovative evaluation framework inspired by unit testing in software development, focusing on both accuracy and reliability of results. Furthermore, we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to perform explicit reasoning before generating code, leading to generating more advanced solution and improved performance. Furthermore, we are releasing not only the UTMath benchmark but also the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning.","The pursuit of AGI necessitates strong mathematical reasoning capabilities, making the evaluation of such abilities a crucial area of research Zhou et al. (2024a). Recent advancements in LLMs have demonstrated remarkable proficiency in solving complex mathematical problems, achieving amazing performance on various datasets of Math Word Problems (MWPs), such as GSM8K Cobbe et al. (2021), MATH Hendrycks et al. (2021), TheoremQA Chen et al. (2023). Figure 1: Comparing UTMath with MATH. In the left, GPT-4o easily solved a question but failed on a similar question with a different number. The right figure illustrates our benchmark, where each problem includes multiple test cases, and a solution must pass all test cases to be strictly considered correct. We also propose a new prompting method RCoT, where the LLM first reasons through the problem and then generates code. However, classic benchmarks exhibit several limitations that impede the accurate and comprehensive assessment of these models’ capabilities Ahn et al. (2024). First, these benchmarks test models on narrowly defined problems with some specific numbers, which do not adequately assess adaptability to similar but varied scenarios as shown in Fig. 1. Second, their evaluation relies on predetermined rules or the method of LLM-as-a-Judge( Dubois et al. (2024); Zheng et al. (2023)) that usually failed with capricious responses of LLMs. For example, an accurate answer need to be extracted to exactly match the fianl answer in the dataset GSM8K, TheoremQA, and MATH dataset. Furthermore, these benchmarks focus more on final answers than on the underlying reasoning steps. While recent work has made great progress in developing new benchmarks for evaluating the mathematical reasoning of LLMs, many of these approaches still fall short of addressing the fundamental limitations of earlier datasets. For instance, benchmarks like GSM-HARD Gao et al. (2023), GSM-IC Shi et al. (2023), GSM-Plus Li et al. (2024a), MetaMath Yu et al. (2023) have extended the dataset of GSM8K or MATH with some perturbation such as substitution, reversing, distractor insertion. These efforts, while valuable, are characterized by limited coverage and high costs. In this context, our work seeks to bridge these gaps by proposing a solid and robust benchmark that accurately evaluates the mathematical capabilities of LLMs.Drawing inspiration from evaluation methods in software development, where solution correctness is determined through comprehensive unit testing, a solution is deemed correct if it passes all tests, as this demonstrates that its logic is sufficiently robust to generalize across a broad spectrum of scenarios. Similarly, we propose designing a comprehensive set of unit tests for mathematical problems to rigorously assess the reasoning processes of LLMs. If a solution passes all unit tests across a group of similar problems, it suggests that the reasoning behind the solution is more reliable and trustworthy. Specifically, we introduce the UTMath, a novel benchmark derived from the On-Line Encyclopedia of Integer Sequences (OEIS) OEIS Foundation Inc. (2024). The benchmark consists of 1,053 problems spanning 9 mathematical domains, such as Number Theory and Geometry. Each problem is accompanied by more than 68 test cases that provide a set of inputs and their corresponding outputs. In terms of evaluation methodology, our benchmark requires models to derive a general solution for a class of problems, typically represented in the form of code. Compared to solving a problem defined by specific numbers, developing such a general solution is substantially more challenging, requiring higher levels of intelligence and reasoning ability. However, we observed that when the model is required to perform reasoning and coding in a single response, as in “Program of Thoughts (PoT)” Chen et al. (2022), it consistently tends to produce simpler, more straightforward solutions. We surmise that this tendency may be influenced by the distribution of coding data. To address this, we introduced the “Reasoning-to-Coding of Thoughts (RCoT)” approach, which requires the LLM to perform mathematical reasoning in the first turn without any coding instruction, then writing code based on that reasoning. Compared to PoT, RCoT shifts the code distribution towards mathematics in the first turn, prompting more reasoning steps. Figure 2: RCoT significantly improves the efficiency and effectiveness of the solution. It indicates that our RCoT proves to be more effective, suggesting that it encourages the model to reason critically and find more efficient solutions. We conducted a comprehensive study with 8 LLMs. Some of our key findings are summarized as follows: (1) The best model, GPT-4o, only solves 26.93% problem in our benchmark, demonstrate the difficulty of our benchmarks. (2) Modern LLMs perform poorly in Graph Theory, Group Theory, Geometry and Topology (Fig. 4). (2) With RCoT, 7 of 8 evaluated LLMs generated more efficient solutions, with most models achieving higher scores (Fig. 2). (3) RCoT can significantly improve the pass@k performance of LLMs (§ 5.4). (4) The quality of reasoning significantly impacts the accuracy and efficiency of the model’s final solution (§ 5.5). More interesting findings can be found in § 5. We hope our findings contribute to a deeper understanding of current reasoning ability of LLMs and the further development of models."
https://arxiv.org/html/2411.07238v1,"OpenThaiGPT 1.5: A Thai-Centric Open Source 
Large Language Model","OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5, finetuned on over 2,000,000 Thai instruction pairs. This report provides an engineering perspective on the model’s development, capabilities, and performance. We discuss the model’s architecture, training process, and key features, including multi-turn conversation support, Retrieval Augmented Generation (RAG) compatibility, and tool-calling functionality. Benchmark results demonstrate OpenThaiGPT 1.5’s state-of-the-art performance on various Thai language tasks, outperforming other open-source Thai language models. We also address practical considerations such as GPU memory requirements and deployment strategies.","1 Model Architecture and Training 1.1 Base Model OpenThaiGPT 1.5 is built upon the Qwen v2.5 architecture [5], leveraging its advanced capabilities as a foundation for Thai language modeling. The model is available in two sizes: 7 billion and 72 billion parameters, catering to different computational resource constraints and performance requirements. The 7B model was finetuned from Qwen/Qwen2.5-7B-Instruct on Huggingface, and the 72B model was finetuned from Qwen/Qwen2.5-72B-Instruct. Both base models have a vocabulary size of 152,064 and a maximum input length of 32,768. Inspection of the tokenizers and initial experimentation revealed that the Qwen 2.5 models already support the Thai language. For this reason, as well as due to the limitation of our computing, we opted not to perform any continued pretraining of the model with Thai data and start with instruction finetuning. 1.2 Finetuning Process The model underwent extensive finetuning on a diverse dataset of over 2,000,000 Thai instruction pairs. This process was crucial in adapting the base model to the nuances of the Thai language and culture, enabling its effectiveness in handling Thai-specific domain questions. We used the NeMo111https://github.com/NVIDIA/NeMo framework by Nvidia for finetuning. The 7B and 72B proposed models were trained using the LoRa technique [4]. The LoRa adaptor size r was 64, and \alpha was 128. The learning rate was 1e^{-4}. The global batch size was 32. We used 10% of the finetuning data as a validation set and restored the checkpoint with the lowest validation loss after training. Both models were trained on a server with 8x H100 GPUs, which was kindly provided to us by SiamAI222https://siam.ai/. 1.2.1 Datasets The finetuning dataset for OpenThaiGPT 1.5 comprises several high-quality, diverse Thai language datasets: • Thai Wiki Summary Dataset: A collection of 3,000 cleaned rows generated from Wikipedia, focusing on summarization and information synthesis tasks. • Thai QA Multi-turn Answer Dataset: This dataset contains 11,992 cleaned rows of multi-turn question-answering conversations, enhancing the model’s ability to maintain context and generate coherent responses across multiple interactions. • Additional Proprietary Datasets: Supplementary over 30 datasets covering various domains and task types, further enriching the model’s knowledge and capabilities. • Synthetic Data: We used Llama 3.1 405B[3] to generate synthetic data by prompting it to create variations of an existing instruction, generate counterfactual questions to existing facts, and to form questions about Wikipedia articles. • LLM as a Judge: After generating synthetic data, we used another instance of Llama 3.1 405B[3] to review the quality of the generated data. We used in-context learning to inform the model of examples of low and high-quality instruction-answer pairs. We filtered the synthetic data to retain only instruction-answer pairs that the model judged high-quality. • English Data: We would like the model to be English-Thai bilingual. Therefore, to alleviate catastrophic forgetting of English, we mixed in about 20% of English data from various datasets on Huggingface such as: yahma/alpaca-cleaned, OpenAssistant/oasst1, and (a subset of) Open-Orca/OpenOrca. These datasets were carefully curated and processed to ensure high-quality training data, contributing to the model’s robust performance across a wide range of Thai language tasks and domains. 1.3 Alignment and Safety We created a safety net for Reinforcement Learning from Human Feedback (RLHF) [6] to ensure the model does not generate or respond to rude or socially sensitive topics. Our approach began with compiling a list of impolite words and sensitive subjects to which the model should refuse to answer. We manually wrote examples that included these words and addressed sensitive issues. Additionally, we employed techniques such as Easy Data Augmentation [11] to enhance our dataset, and we also experimented with jailbreaking [12] some open-weight LLMs to generate more of this data locally and off-line. In total, we constructed around 5,000 records of alignment data and aligned the model using Direct Preference Optimization (DPO) [9], accessible through the model aligner module of NeMo."
https://arxiv.org/html/2411.07237v1,":
Taking the Guesswork Out of Language Model Evaluations","Language model users often issue queries that lack specification, where the context under which a query was issued—such as the user’s identity, the query’s intent, and the criteria for a response to be useful—is not explicit. For instance, a good response to a subjective query like “What book should I read next?” would depend on the user’s preferences, and a good response to an open-ended query like “How do antibiotics work against bacteria?” would depend on the user’s expertise. This makes evaluation of responses to such queries an ill-posed task, as evaluators may make arbitrary judgments about the response quality. To remedy this, we present contextualized evaluations, a protocol that synthetically constructs context surrounding an underspecified query and provides it during evaluation. We find that the presence of context can 1) alter conclusions drawn from evaluation, even flipping win rates between model pairs, 2) nudge evaluators to make fewer judgments based on surface-level criteria, like style, and 3) provide new insights about model behavior across diverse contexts. Specifically, our procedure uncovers an implicit bias towards WEIRD contexts in models’ “default” responses and we find that models are not equally sensitive to following different contexts, even when they are provided in prompts.","Users of language models often issue queries that are underspecified Spärck-Jones et al. (2007); Clarke et al. (2009); Ziegler et al. (2019); Keyvan and Huang (2022); Herlihy et al. (2024), but common evaluation practices for language models do not account for this. Consider an evaluator presented with a language model’s response to an underspecified query such as “Is coffee good for you?” (Figure 1). A language model might respond with an explanation about benefits like antioxidants and mental alertness, but this output would be unacceptable to users with certain health conditions. Can an evaluator make an informed judgment about response quality without further details about the intended user’s preferences, background or criteria for the response to be useful? Figure 1: Queries issued by users to language models are often underspecified and can lead to arbitrary evaluation judgments of response quality. We present contextualized evaluations, where queries are supplemented with surrounding context during evaluation. \rotatebox[origin={c}]{90.0}{{Degree of underspecification}}\left\uparrow% \begin{tabular}[]{l@{~~}p{7cm}p{7cm}c}\lx@intercol\hfil{Query Type}\hfil% \lx@intercol &\lx@intercol\hfil{Description}\hfil\lx@intercol &\lx@intercol% \hfil{Example}\hfil\lx@intercol &\lx@intercol\hfil{Frequency}\hfil\lx@intercol% \\ \hline\cr\hline\cr Incomplete\hfil~{}~{}&Missing essential information needed % to provide a response (e.g., unresolved coreference).&\emph{What is the best % team in the league?}&\pagecolor{Lavender!30}18.27\%\\ Ambiguous\hfil~{}~{}&Can be interpreted in multiple ways (e.g., ambiguous word% sense).&\emph{What is a transformer?}&\pagecolor{Lavender!5}1.87\%\\ Subjective\hfil~{}~{}&Answers based on opinions or personal values (e.g., ``% best'', ``worst'').&\emph{Who is the greatest philosopher from the 20th % century?}&\pagecolor{Lavender!30}18.69\%\\ Open-ended\hfil~{}~{}&Allows for multiple possible detailed responses (e.g., `% `explain how'', ``describe why'').&\emph{Can you summarize recent work on % mRNAs?}&\pagecolor{Lavender!70}76.17\%\\ \hline\cr Closed-ended\hfil~{}~{}&Requires specific, concise answer with % little room for interpretation.&\emph{What is the capital of France?}&% \pagecolor{Lavender!40}27.46\%\\ \hline\cr\hline\cr\end{tabular}\right.\rotatebox[origin={c}]{90.0}{} Table 1: Queries from five LM benchmark datasets, categorized based on the amount of underspecification. Queries can present multiple types (e.g., “What is the best team in the league?” is both incomplete and subjective). In this work, we consider the role of context in the evaluation of language model responses to underspecified queries. We propose contextualized evaluations, a protocol to synthetically generate and incorporate diverse contexts (represented as follow-up question-answer pairs) to underspecified queries. By applying this procedure to queries from widely-used language model benchmark datasets, we investigate three main research questions: First, we investigate whether providing context to evaluators has a substantial effect on the conclusions drawn from evaluation. We sample responses from pairs of language models and collect pairwise preference judgments from both model-based as well as human evaluators, in context-agnostic (only the query and model outputs) and context-aware settings (additionally with follow-up questions and answers to clarify the query). Our experiments show not only that inclusion of context during evaluation can significantly improve agreement between evaluators (3-10% absolute), but also that context-aware evaluation can even flip win rates between model pairs. This raises concern for the reliability of findings produced from context-agnostic evaluations using today’s language model benchmarks, which we find to be full of underspecified queries (Table 1). Next, we ask whether context changes the criteria used by evaluators for making judgments. Without context, evaluators may make arbitrary judgments which reward surface-level adequacy of a response Park et al. (2024); Chiang et al. (2024). To evaluate this, we collect free-text justifications from both model and human evaluators. We find that context-aware evaluation can decrease the frequency with which evaluators make judgments using surface-level properties like style as opposed to other properties like response relevance (by 5-7% absolute). Finally, we investigate whether context helps us learn more about the ability of models to adapt to different user contexts. In our work, we use contexts to study biases exhibited by “default” model responses, i.e., those generated without context (§5), to underspecified queries. We find strong evidence of a bias where default model responses are better aligned with WEIRD (Western, Educated, Industrialized, Rich and Democratic) contexts Henrich et al. (2010). Context can also allow us to directly evaluate the instruction-following and personalization abilities of models. We show this by evaluating sensitivity of models to different contextual attributes, where we find considerable disparity in the ability of models to adapt to different contexts, even when they’re provided in prompts (§6). Figure 2: Types of common contextual attributes which can be lacking in underspecified queries, along with examples of follow-up questions for each attribute. In summary, our findings suggest that underspecification can have a significant impact on the conclusions and insights drawn from evaluation. To address this, we propose contextualized evaluations, a simple and broadly applicable solution that involves synthesizing relevant context and injecting it into existing evaluation protocols (§2.2). We show that context can increase agreement between evaluators and substantially modify model win rates based on pairwise preference judgements (§4). Further, we show that context can enable us to gather more insights about model behavior, such as identifying contexts that align more closely with default model responses (§5) and assessing model sensitivity to different user contexts (§6). Our work provides a plug-and-play recipe for incorporating context into future language model evaluations."
https://arxiv.org/html/2411.07218v1,TreeCoders: Trees of Transformers,"In this paper, we introduce TreeCoders, a novel family of transformer trees. We moved away from traditional linear transformers to complete k-ary trees. Transformer blocks serve as nodes and generic classifiers learn to select the best child and route the sequence of tokens to a specific leaf. The selectors, moved outside the transformer blocks, allow for the use of a variety of architecture without further modifications. Furthermore, our proposed architecture supports sparse node activation due to the logarithmic complexity of a tree search. We validate our idea by testing a series of decoder-only tree transformers achieving competitive results across a diverse range of language datasets. Our study demonstrates that the proposed tree transformer model outperforms a size-equivalent linear transformer model 76% of the time over a wide range of tree architectures. Furthermore, our proposed model naturally lends itself to distributed implementation.","Transformers have proved their effectiveness ever since their introduction [Vaswani17]. One development has been a rapid increase in the size of the models and the amount of data they consume, leading to models such as GPT-3 [Gpt3] towering at 175 billion parameters. Those large and dense models impose heavy hardware requirements and longer inference time. Better management of those resources would be greatly beneficial. One possible solution to reduce inference time has been to promote sparsity in networks through the Mixture-of-Expert approach [Jacobs1991AdaptiveMO][Jordan1993HierarchicalMO]. A tree is sparse by design, as shown in Figure 1 and, therefore, an attractive candidate for scaling future models. Large language models are increasingly expected to perform on a variety of tasks and datasets. Textual data varies greatly in terms of structure, intent, content, formality, language, and organization. Nevertheless, the underlying language apparatus of these contents can be hierarchically exploited based on their common features. This is our second motivation to investigate moving from a linear structure to a tree-like one. The final motivation is the inherently explainable nature of decision trees. Bringing about explainability to large language models would be a formidable step forward in itself but should also help with fine-tuning, safety, and alignment. ((a)) A sequence going through one path in the tree, from root decoder A to leaf decoder K.ABDHIEJKCFKLGMN ((b)) Total number of nodes in the tree. Height Branching Factor 1 2 3 4 1 2 3 4 5 2 3 7 13 21 3 4 15 40 85 4 5 31 121 341 5 6 63 364 1365 ((c)) Percentage of parameters used by a token. Height Branching Factor 1 2 3 4 1 100.0 66.7 50.0 40.0 2 100.0 42.9 23.1 14.3 3 100.0 26.7 10.0 4.7 4 100.0 16.1 4.1 1.5 5 100.0 9.5 1.6 0.4 Figure 1: A tree structure allows for a sparse activation of the network. The sparsity will also grow with the tree"
https://arxiv.org/html/2411.07191v1,The Super Weight in Large Language Models,"Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM’s ability to generate text – increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs111Code is available in https://github.com/mengxiayu/LLMSuperWeight..","Large Language Models (LLMs) have been growing in size and capability at an unprecedented rate, enabling them to capture increasingly complex linguistic patterns across a wide range of tasks. However, with this increase in model scale, new and unexpected behaviors have emerged. Dettmers et al. (2022) discovered that once LLMs reach a certain scale, a small set of hidden state features contains outliers of exceptionally large magnitude. These outliers account for a small percentage of all activations but are crucial for preserving the compressed model’s quality (Dettmers et al., 2022; Xiao et al., 2023; Wei et al., 2023; Shao et al., 2024). However, not all outliers are equally important. In this paper, we study a tiny yet important set of outliers in LLMs, termed super weights. In Llama-7B, pruning the super weight, a single scalar, completely destroys the model’s ability to generate text; the average accuracy of zero-shot downstream tasks effectively plummets to zero. Conversely, pruning the other top 7,000 outliers, including outliers that are larger than the super weight, affects no more than a few percentage points. Intriguingly, super weights behave similarly across model families and sizes. For one, the super weight is always found in the mlp.down_proj weight, always in an early layer. We also find that the super weight amplifies input activation inliers to ultimately produce the exceptionally large magnitude activation observed by Sun et al. (2024) – we term this the super activation. This super activation persists throughout the model at exactly the same magnitude and position regardless of the prompt, and we find this is uniquely enabled by skip connections. Finally, super weights suppress stopword likelihood. Taken together, pruning the super weight destroys quality by dampening the super activation and shifting almost all logit probability mass to stopwords. Both super weights and super activations, which we collectively refer to as super outliers, are critical to model quality. Fortunately, there are no more than a handful of scalar super outliers per tensor; in light of this, we revisit round-to-nearest quantization, equipped only with the ability to hold out and restore super outliers. This yields a data-free, hardware-friendly method. For activation quantization, we find this technique competitive with SmoothQuant; for weight quantization, we can scale round-to-nearest to much larger block sizes with higher quality. Our contributions are summarized as follows. 1. Super Weights: We discover a tiny subset of outliers in LLMs, at most six scalars, that are disproportionately important; pruning these super weights destroys model quality. 2. Identifying Super Weights: We present a data-free way to identify super weights using only a single forward pass and provide an index of super weights for existing, open LLMs. 3. Super Activations: We analyze how super weights influence inference and relate them to the activation outliers observed in prior work. 4. Compression: By preserving super outliers, we show that round-to-nearest quantization increases effectiveness noticeably; preserving super outliers improves compression quality. Figure 1: Super Weight Phenemenon. We discover that pruning a single, special scalar, which we call the super weight, can completely destroy a Large Language Model’s ability to generate text. On the left, the original Llama-7B, which contains a super weight, produces a reasonable completion. On the right, after pruning the super weight, Llama-7B generates complete gibberish. As we show below, this qualitative observation has quantitative impact too: zero-shot accuracy drops to guessing and perplexity increases by orders of magnitude."
https://arxiv.org/html/2411.07176v2,More Expressive Attention with Negative Weights,"We propose a novel attention mechanism, named Cog Attention, that enables attention weights to be negative for enhanced expressiveness, which stems from two key factors: (1) Cog Attention can shift the token deletion and copying function from a static OV matrix to dynamic QK inner products, with the OV matrix now focusing more on refinement or modification. The attention head can simultaneously delete, copy, or retain tokens by assigning them negative, positive, or minimal attention weights, respectively. As a result, a single attention head becomes more flexible and expressive. (2) Cog Attention improves the model’s robustness against representational collapse, which can occur when earlier tokens are over-squashed into later positions, leading to homogeneous representations. Negative weights reduce effective information paths from earlier to later tokens, helping to mitigate this issue. We develop Transformer-like models which use Cog Attention as attention modules, including decoder-only models for language modeling and U-ViT diffusion models for image generation. Experiments show that models using Cog Attention exhibit superior performance compared to those employing traditional softmax attention modules. Our approach suggests a promising research direction for rethinking and breaking the entrenched constraints of traditional softmax attention, such as the requirement for non-negative weights. Our code is available at https://github.com/trestad/CogAttn.","The Transformer architecture (Vaswani et al., 2017) has achieved success across numerous applications, such as language modeling (Brown et al., 2020) and image generation (Dosovitskiy et al., 2021). A crucial factor contributing to its success is the softmax attention mechanism (Bahdanau et al., 2015). Figure 1: In the Indirect Object Identification (IOI) task (Wang et al., 2023), a language model should identify the indirect object (IO) from a context that includes both IO and a subject (S). Subfigures (a) and (b) show how Cog Attention and softmax attention process a single sequence through a process of elimination, but in different ways: a softmax attention head with a deletion-function OV matrix eliminates all attended tokens. While the IO token receives less attention than S, it is also deleted. In contrast, Cog Attention shifts functions like deletion or copying from a static OV matrix to dynamic query-key inner products, allowing the head to assign negative weights to S tokens for elimination while preserving IOs. Subfigures (c) and (d) show the attention of these two heads on names across the entire dataset vs the direction of their output embeddings. Cog Attention exhibits a weaker correlation between the attention weights assigned to IOs and the extent of their deletion. For further details, please see Section 3. Softmax ensures non-negative attention weights, but we argue that it limits the expressiveness of the attention mechanism. Figure 1 shows one possible way that negative attention weights enhance the model’s expressiveness using the same number of parameters: in a softmax attention head, the QK matrix (Elhage et al., 2021) determines the relevant tokens for attention, while the OV matrix governs the processing of these attended tokens (e.g., deletion or copying). Suppose a softmax attention head has an OV matrix capable of deleting tokens that the QK matrix attends to; since attention weights must be non-positive, a useful token is also somewhat deleted. By allowing negative attention weights, however, deletion or copying can be expressed through the sign of the attention weight and accomplished during the weighted summation of value vectors. This functional shift also allows the OV matrix to focus more on higher-level tasks, such as refinement or modification, rather than solely handling contextual deletions or copies. Consequently, negative attention weights eliminate irrelevant tokens while preserving useful ones, mitigating the risk of “friendly fire” on useful tokens. Despite the potential benefits of incorporating negative weights in attention mechanisms, this question has been rarely explored. Apart from the common belief that attention weights should naturally be non-negative, introducing negative weights can lead to challenges such as training instability, numerical overflow, and difficulties in attention normalization due to issues like division by zero, etc. In this paper, we propose a novel attention mechanism named Cog Attention111The name is derived from the attention pattern, which resembles cogs. See Figure 10. that enables negative weights. Cog Attention exhibits superior properties from a mechanistic interpretability perspective and surpasses softmax attention in various applications without introducing any additional parameters. In Section 3, we provide mechanistic evidence for the expressiveness of Cog Attention: (1) We identify attention heads that share the same working mechanism as exemplified above (Figures 1(b) and (d)), which shift the contextual process from the static OV matrix to dynamic QK inner products, with the OV matrix focusing more on refinement or modification. Irrelevant tokens are assigned negative weights for elimination, while other tokens are well preserved at the same time. This demonstrates Cog Attention’s enhanced flexibility and expressiveness compared to softmax attention. (2) We demonstrate that models using Cog Attention exhibit improved robustness against representational collapse (Liu et al., 2020; Xie et al., 2023). Representational collapse refers to the phenomenon where representations become homogeneous in the later positions of a sequence within deep Transformer models. Barbero et al. (2024) contended that this issue arises because earlier tokens are “over-squashed” into later positions as the layer goes deeper. The negative weights in Cog Attention reduce the effective information paths from earlier tokens to later positions, thereby alleviating over-squashing and, consequently, mitigating representational collapse. In Section 4, we develop Transformer-like models that use Cog Attention as attention modules and evaluate their performance across various tasks. Specifically, we train decoder-only language models for language modeling, and U-ViT diffusion models (Bao et al., 2023) for both unconditional and text-conditioned image generation. Our results show that across a wide range of tasks, these models equipped with Cog Attention achieve improved performance over the vanilla Transformer architecture using softmax attention."
https://arxiv.org/html/2411.07175v1,"Continual Memorization of Factoids in 
Large Language Models","Large language models (LLMs) can absorb a massive amount of knowledge through pretraining, but pretraining is inefficient for acquiring long-tailed or specialized facts. Therefore, fine-tuning on specialized or new knowledge that reflects changes in the world has become popular, though it risks disrupting the model’s original capabilities. We study this fragility in the context of continual memorization, where the model is trained on a small set of long-tail factoids (subject-relation-object associations) and must retain these factoids after multiple stages of subsequent training on other datasets. Continual memorization focuses on the specific challenge of retaining long-tail factoids, whereas general continual learning aims to maintain the LLM’s capabilities across a wide range of generic tasks (e.g., reasoning, commonsense knowledge). Through extensive experiments, we show that LLMs suffer from forgetting across a wide range of subsequent tasks, and simple replay techniques do not fully prevent forgetting, especially when the factoid datasets are trained in the later stages. We posit that there are two ways to alleviate forgetting: 1) protect the memorization process as the model learns the factoids, or 2) reduce interference from training in later stages. With this insight, we develop an effective mitigation strategy: REMIX (Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic data sampled from pretraining corpora or even randomly generated word sequences during each stage, despite being unrelated to the memorized factoids in the first stage. REMIX can recover performance from severe forgetting, often outperforming replay-based methods that have access to the factoids from the first stage. We then analyze how REMIX alters the learning process and find that successful forgetting prevention is associated with a pattern: the model stores factoids in earlier layers than usual and diversifies the set of layers that store these factoids. The efficacy of REMIX invites further investigation into the underlying dynamics of memorization and forgetting, opening exciting possibilities for future research.","Figure 1: The continual memorization setting. In stage 1 (red box), a pretrained model is trained to convergence on a factoid dataset D_{A} to yield the fine-tuned model \theta_{A}. In stage 2, model \theta_{A} is further trained on either a factoid dataset or a non-factoid dataset (blue box) to obtain model \theta_{B}. The final model \theta_{B} is evaluated on the training examples D_{A} in stage 1. REMIX mixes either random word sequences or pretraining data (denoted by D_{M}) into training during stages 1 and 2 to prevent forgetting. We describe a two-stage setting in this figure and use it as the basis for most of our experiments, but the general setting naturally extends to multiple stages. Large language models (LLMs) have shown a remarkable ability to absorb a massive amount of knowledge through large-scale pretraining (Petroni et al., 2019; AlKhamissi et al., 2022; Cohen et al., 2023). Despite their familiarity with common knowledge, they still struggle to capture the long tail (Kandpal et al., 2023). Recent work explains that during the pretraining phase, each piece of knowledge requires many exposures and diverse manifestations to be properly acquired (Allen-Zhu & Li, 2024a; b; Chang et al., 2024). A straightforward alternative is to finetune the model on a small, domain-specific dataset. However, finetuning on long-tail knowledge can cause unintentional harm by decreasing factuality and exacerbating hallucination (Kang et al., 2024; Gekhman et al., 2024; Zhang et al., 2024; Ghosal et al., 2024). In this regard, the finetuning process bears some similarity to continual learning (McCloskey & Cohen, 1989; Ratcliff, 1990), where one tries to successively train a model on a series of tasks without forgetting earlier ones. Prior research on continual learning in LLMs focuses on general capabilities such as reasoning (Luo et al., 2023a), or broad proxies like the language modeling loss over a general corpus (Yıldız et al., 2024). In this work, we focus on the challenges unique to the continual learning of factoids – atomic facts representable as subject-object relations. We formalize this setting as continual memorization (Figure 1), where a model is first trained on a small collection of factoids (stage 1), and then must retain their knowledge after training on other datasets (stage 2). We first study how different tasks affect forgetting when placed in the second stage. We find that the effect is maximum for factoid datasets (i.e., datasets consisting of factoids) and that it is less pronounced for non-factoid datasets such as those involving coding, math, or chat abilities. Even more worryingly, we find that typical replay methods, which typically work well for general continual learning, fail to prevent model forgetting when the second stage involves a factoid dataset. How can an LLM prevent the forgetting of factoids? We intuit that this question may be approached in two ways: 1) teach the model to protect learned knowledge better in the first stage, or 2) reducing the interference of the second stage by manipulating the data distribution. Based on this hypothesis, we propose REMIX (Random and Generic Data Mixing), which combines both approaches. First, REMIX mixes random or generic data into the factoids in the first stage. While surprising at first glance, including a broad range of mixed data teaches the model to diversify where it stores the knowledge – as we show in later analysis of REMIX. This diversification allows it to better protect learned knowledge. In the second stage, jointly learning the mixing data and the stage 2 data avoids overfitting to a narrow distribution, alleviating the negative interference on the learned factoids. Our experiments demonstrate that REMIX is highly effective at helping the model retain learned factoids: in the most severe case, REMIX increases post-phase 2 accuracy from 13.5\% to 53.2\%. In comparison, replay can only reach 41.6\% despite using 10\% of the factoids from stage 1. These benefits are seen consistently across several choices of factoid and non-factoid tasks in stage 2. We finally perform a careful analysis of REMIX through Logit Lens (nostalgebraist, 2020) and ablation studies. We find that REMIX teaches the model to both store facts in relatively earlier layers (as opposed to the unmixed case) and diversify their storage to many layers. We summarize our contributions as follows: • We formalize the setting of continual memorization, identify its unique challenge of memorizing factoid data, and demonstrate that it cannot be easily addressed with replay. • We propose REMIX, a simple strategy that does not require access to the factoids from prior stages; we establish through experiments that REMIX helps models remember factoids better – often increasing accuracy by as much as 3\times. • Through careful analysis and ablation studies, we find that REMIX operates by teaching the model to protect factoids via diversification and by reducing the negative interference from the later training stages."
https://arxiv.org/html/2411.07152v1,HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals,"Task-Oriented Dialogue (TOD) systems assist users in completing tasks through natural language interactions, often relying on a single-layered workflow structure for slot-filling in public tasks, such as hotel bookings. However, in enterprise environments, which involve rich domain-specific knowledge, TOD systems face challenges due to task complexity and the lack of standardized documentation. In this work, we introduce HierTOD, an enterprise TOD system driven by hierarchical goals and can support composite workflows. By focusing on goal-driven interactions, our system serves a more proactive role, facilitating mixed-initiative dialogue and improving task completion. Equipped with components for natural language understanding, composite goal retriever, dialogue management, and response generation, backed by a well-organized data service with domain knowledge base and retrieval engine, HierTOD delivers efficient task assistance. Furthermore, our system implementation unifies two TOD paradigms: slot-filling for information collection and step-by-step guidance for task execution. Our human study demonstrates the effectiveness and helpfulness of HierTOD in performing both paradigms.","Task-oriented dialogue systems aim to help users accomplish specific goals by executing tasks through natural language interactions. Significant advancements have been made in developing systems that support public domain tasks (Andreas et al., 2020; Peng et al., 2021; Su et al., 2022), such as booking hotels or reserving restaurants. These tasks typically feature straightforward, single-layered workflows with well-defined steps, intents, and information extraction requirements. However, in enterprise environments rich with domain-specific knowledge, TOD systems face unique challenges due to the complexity and lack of standardized documentation of tasks. Enterprise tasks often involve multi-layered structures composed of numerous interconnected subtasks, as illustrated in Figure 1. These tasks are rarely formally documented, and their execution heavily relies on the implicit knowledge of human experts. When users interact with a dialogue system in such contexts, their utterances can pertain only to atomic tasks defined at the leaf nodes of a complex task hierarchy. The overarching structure and sequence of these tasks remain internalized within the user’s expertise, making it difficult for dialogue systems to fully comprehend and assist effectively. Figure 1: Composite workflows to execute tasks for hierarchical goal completion. A high-level goal consists of multiple sub-goals that transition between each other as the conversation progresses. Each sub-goal can include atomic goals such as product knowledge QA, operational insights QA, or navigation instructions. Consider, for instance, an enterprise marketing platform equipped with audience segmentation functionality. Users within an organization may independently create many client profile segments for various projects. Over time, this practice can result in the proliferation of duplicate segments, increasing platform resource costs and reducing business efficiency. To mitigate this, users perform data hygiene processes to clean up and consolidate these duplicate segments. However, the formal steps for this process are typically undocumented, and practices vary across different organizations. With guidance from subject matter experts, the complex task of identifying and removing duplicate segments can be distilled into a high-level goal comprising four sequential steps: {mdframed} [backgroundcolor=cyan!10] Goal: Data Hygiene for Audience Segments • Step 1: Detect duplicate segments by definition or outcome. • Step 2: List segment references by relevant business entities. • Step 3: Remove or unlink non-essential segment references and relink to essential ones when necessary. • Step 4: Delete non-essential segments. In practice, different users may prioritize certain business entities over others and follow various paths through this task hierarchy when interacting with the dialogue system. Their intents correspond to atomic actions represented as nodes within a task graph, and the sequences connecting these nodes can vary significantly. A dialogue system with a deep understanding of these high-level business goals and the complex structure of such tasks can significantly enhance user experience. It can improve query comprehension, disambiguate user intents more effectively, proactively suggest relevant goals, and provide personalized responses that align with individual user needs. To this ends, we introduce HierTOD, an enterprise TOD system driven by hierarchical goals to facilitate the generation of more proactive and effective dialogues with users. For example, when a user inquires about detecting duplicate segments (as described in the first step of the previous case), HierTOD can proactively suggest transitioning to the high-level goal of conducting data hygiene for audience segments. Our system comprises several modules, including natural language understanding, composite goal retriever, dialogue management, and response generation, supported by a well-organized data service with a domain knowledge base and retrieval engine. Furthermore, existing TOD systems typically follow one of two paradigms, which are often developed separately. The first is slot-filling for information collection (Yang et al., 2021; Hu et al., 2022; Hudeček and Dušek, 2023), where users provide details and direct the system to perform specific tasks, such as making reservations. The second paradigm is step-by-step guidance, designed to assist users in executing real-world tasks by providing accurate information and step-by-step instructions. For instance, Amazon’s Alexa Prize Taskbot (Gottardi et al., 2022; Agichtein et al., 2023) helps users complete tasks such as cooking a dish or following a DIY tutorial, guiding them through the process with detailed instructions (Mo et al., 2023). In this work, HierTOD implements a unified framework that supports both paradigms, delivering comprehensive and efficient task assistance. Our contributions include: (1) Developing a TOD system to support composite workflows in enterprise environments. (2) Introducing a goal-driven approach to dialogues, making the system more proactive and enabling mixed-initiative interactions for improved task completion. (3) Implementing a unified framework that integrates two TOD paradigms: slot-filling for information collection and step-by-step guidance for task execution. (4) Conducting a human study to verify the effectiveness and helpfulness of our dialogue system."
https://arxiv.org/html/2411.07142v1,"Greenback Bears and Fiscal Hawks:
Finance is a Jungle and Text Embeddings Must Adapt","Financial documents are filled with specialized terminology, arcane jargon, and curious acronyms that pose challenges for general-purpose text embeddings. Yet, few text embeddings specialized for finance have been reported in the literature, perhaps in part due to a lack of public datasets and benchmarks. We present BAM embeddings, a set of text embeddings finetuned on a carefully constructed dataset of 14.3M query-passage pairs. Demonstrating the benefits of domain-specific training, BAM embeddings achieve Recall@1 of 62.8% on a held-out test set, vs. only 39.2% for the best general-purpose text embedding from OpenAI. Further, BAM embeddings increase question answering accuracy by 8% on FinanceBench and show increased sensitivity to the finance-specific elements that are found in detailed, forward-looking and company and date-specific queries. To support further research we describe our approach in detail, quantify the importance of hard negative mining and dataset scale.","Portfolio managers and analysts have access to millions of financial documents. Text embeddings are a key component of the information retrieval and retrieval-augmented generation (RAG) systems Lewis et al. (2020) that can help extract insights from this mass of information. However, the financial domain poses unique challenges for text embeddings. Financial documents are filled with specialized terminology (‘par value’, ‘stagflation’), jargon (‘Chinese wall’), curious acronyms (‘CAGR’, ‘DCF’, ‘VIX’), and technical terms and company names that collide with ordinary words (‘short’, ‘forward’, ‘spread’, ‘Apple’, ‘Stripe’). Is a CDO similar to a CFO? And what is a Greenback bear111A Greenback bear is an investor who believes the US dollar will decline in value. A fiscal hawk argues for a reduction in government spending.? Despite their importance, few text embeddings specialized for finance have been reported in the literature. To address this gap, we present BAM embeddings, a set of text embeddings optimized for financial document retrieval. BAM embeddings are based on Multilingual-E5 Wang et al. (2024b), further finetuned on a carefully filtered, clean dataset of 14.3M query-passage pairs (6B tokens) constructed from 2.8M financial documents. While we cannot release our dataset, we describe in detail our data curation and query generation strategy, finetuning process, and approach to deployment in a high-priority application. On a held-out set of 447K query-passage pairs, BAM embeddings achieve Recall@1 of 62.8%, far surpassing the Multilingual-E5 base model (34.3%) as well as large closed-source models (e.g., OpenAI’s 3072-dim text-embedding-3-large model, 39.2%). Quantitatively, we show that hard negative mining (+5.3%) and data scale (+4.5%) are critical to achieving this performance. Deploying BAM embeddings in an application alongside traditional lexical search (Okapi BM25), we find that BAM embeddings outperform lexical search over all query lengths. Notably, vector search with BAM embeddings improves as queries become longer and more detailed, while lexical search degrades. Finally, we evaluate BAM embeddings in a public RAG benchmark using FinanceBench Islam et al. (2023). Replacing OpenAI’s ada-002 embeddings with ours increases question answering correctness by 8%. Qualitatively, we observe that after finetuning, embeddings are more sensitive to company names, tickers and financial metrics, leading to improved performance detailed, forward-looking, and company or date-specific queries."
https://arxiv.org/html/2411.07140v2,Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models,"New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.","A significant challenge in AI development is to ensure language models generate factually accurate responses. Current frontier models sometimes produce false outputs or answers that are not substantiated by evidence. This is the problem known as “hallucinations”, which greatly hinders the extensive use of general AI technologies, such as large language models (LLMs). Besides, it is difficult to evaluate the factuality abilities of the existing LLMs. For example, LLMs usually generate lengthy responses containing numerous factual claims. Recently, to address the aforementioned evaluation problem, OpenAI has released the SimpleQA benchmark (Wei et al., 2024) with 4,326 concise and fact-seeking questions, which makes measuring factuality simple and reliable. However, the SimpleQA benchmark primarily targets the English language, resulting in a limited understanding of LLMs’ capabilities in other languages. Moreover, inspired by several recent Chinese LLM benchmarks (e.g., C-Eval (Huang et al., 2023), CMMLU (Li et al., 2023b)), to evaluate the factuality abilities of LLMs in Chinese, we present the Chinese SimpleQA benchmark111https://openstellarteam.github.io/ChineseSimpleQA/, which consists of 3000 high-quality questions spanning 6 major topics, ranging from humanities to science and engineering, as shown in Figure 1. Specifically, the distinct main features of our proposed Chinese SimpleQA dataset are as follows: Figure 1: Overview of Chinese SimpleQA. “Chinese Cul.” and “ETAS” represent “Chinese Culture” and “Engineering, Technology, and Applied Sciences”, respectively. • Chinese: Our Chinese SimpleQA focuses on the Chinese language, which provides a comprehensive evaluation of the factuality abilities of existing LLMs in Chinese. • Diverse: Chinese SimpleQA covers 6 topics (i.e., “Chinese Culture”, “Humanities”, “Engineering, Technology, and Applied Sciences”, “Life, Art, and Culture”, “Society”, and “Natural Science”), and these topic includes 99 fine-grained subtopics in total, which demonstrates the diversity of our Chinese SimpleQA. • High-quality: We conduct a comprehensive and rigorous quality control process to ensure the quality and accuracy of our Chinese SimpleQA. • Static: Following SimpleQA, to preserve the evergreen property of Chinese SimpleQA, all reference answers would not change over time. • Easy-to-evaluate: Following SimpleQA, as the questions and answers are very short, the grading procedure is fast to run via existing LLMs (e.g., OpenAI API). Moreover, we perform a comprehensive evaluation and analysis of existing LLMs on Chinese SimpleQA, and several insightful findings are as follows: • Chinese SimpleQA is challenging. Only o1-preview and Doubao-pro-32k achieve the passing score (63.8% and 61.9% on the correct metric), and there is a long way to improve for many closed-source and open-source LLMs. • Larger models lead to better results. Based on the results of the Qwen2.5 series, InternLM series, Yi-1.5 series, etc, we observe that better performance is obtained when the model is larger. • Larger models are more calibrated. We observe that o1-preview is more calibrated than o1-mini, and GPT-4o is more calibrated than GPT-4o-mini. • RAG matters. When introducing the RAG strategy into existing LLMs, the performance gaps between different LLMs decrease a lot. For example, for GPT-4o and Qwen2.5-3B, the performance gap decreases from 42.4% to 9.3% within using RAG. • Alignment tax exists. Existing alignment or post-training strategies usually decrease the factuality of language models. • Rankings of SimpleQA and Chinese SimpleQA are different. The performance of several LLMs focusing on Chinese (Doubao-pro-32k, and GLM-4-Plus) is close to the high-performance o1-preview. In particular, in the “Chinese Culture” topic, these Chinese community LLMs are significantly better than GPT or o1 series models."
https://arxiv.org/html/2411.07130v1,Retrieval or Global Context Understanding? On Many-Shot In-Context Learning for Long-Context Evaluation,"Language models (LMs) have demonstrated an improved capacity to handle long-context information, yet existing long-context benchmarks primarily measure LMs’ retrieval abilities with extended inputs, e.g., pinpointing a short phrase from long-form text. Therefore, they may fall short when evaluating models’ global context understanding capacity, such as synthesizing and reasoning over content across input to generate the response. In this paper, we study long-context language model (LCLM) evaluation through many-shot in-context learning (ICL). Concretely, we identify the skills each ICL task requires, and examine models’ long-context capabilities on them. We ask the first question: What types of ICL tasks benefit from additional demonstrations, and are these tasks effective at evaluating LCLMs? We find that classification and summarization tasks show notable performance improvements with additional demonstrations, while translation and reasoning tasks do not exhibit clear trends. This suggests the classification tasks predominantly test models’ retrieval skills. Next, we ask: To what extent does each task require retrieval skills versus global context understanding from LCLMs? We develop metrics to categorize ICL tasks into two groups: (i) retrieval tasks that require strong retrieval ability to pinpoint relevant examples, and (ii) global context understanding tasks that necessitate a deeper comprehension of the full input. We find that not all datasets can effectively evaluate these long-context capabilities. To address this gap, we introduce a new many-shot ICL benchmark, ManyICLBench, designed to characterize LCLMs’ retrieval and global context understanding capabilities separately. We benchmark 11 open-weight LCLMs using ManyICLBench. We find that while state-of-the-art models demonstrate satisfactory performance up to 64k tokens in retrieval tasks, many models experience significant performance drops at only 16k tokens in global context understanding tasks.111Data and code are available at https://github.com/launchnlp/ManyICLBench","Long-context language models (LCLMs) have revolutionized the way users interact with language models by extending the context size from 2K to 128K or even 1M tokens (Team et al., 2024a; GLM et al., 2024; Dubey et al., 2024), which unlock challenging applications, such as long- and multi-document summarization, multi-turn dialogue, and code repository comprehension. Despite the recent progress in building LCLMs, existing benchmarks primarily evaluate these models’ retrieval capabilities (Liu et al., 2023; Hsieh et al., 2024). From synthetic tasks such as Needle-in-A-Haystack (Kamradt, 2023) and RULER benchmark (Hsieh et al., 2024) to real-world challenges like long-novel QA (Karpinska et al., 2024), the majority of benchmarks assess how well LCLMs retrieve specific pieces of information from extensive contexts. As a result, evaluating models’ global understanding of the full context remains lacking. To fill the gap, Li et al. (2024) introduce LongICLBench, which uses many-shot ICL classification tasks to evaluate models’ long-context performance, arguing that these tasks require the comprehension of the entire input. A few other works have also explored many-shot ICL for long-context models (Agarwal et al., 2024; Bertsch et al., 2024). Yet, they have mainly relied on classification tasks (Li et al., 2024; Bertsch et al., 2024), which are insufficient to distinguish which skills LCLMs require to perform well on many-shot ICL classification tasks. Recently, Agarwal et al. (2024) study non-classification ICL tasks but only on Gemini 1.5 Pro. In this work, we want to conduct a comprehensive study on many-shot ICL across a wide range of models, with a goal of identifying tasks that benefit from additional demonstrations and explore their utility in evaluating long-context models. Moreover, we seek to determine the extent to which these tasks rely on retrieval versus global context understanding. RQ1: Which tasks benefit from many-shot ICL? First, we investigate ICL tasks that are used in prior work, including classification, summarization, and reasoning, under many-shot settings with context lengths from 1k to 128k (Agarwal et al., 2024). We find that classification and summarization tasks show strong positive correlation between context lengths and model performance. Our findings indicate that translation and reasoning tasks such as ARC (Clark et al., 2018) and FLORES-200 (Team et al., 2022) do not gain much performance with an increasing number of demonstrations. Science and symbolic reasoning tasks exhibit inconsistent trends between context lengths and model performance. This variance in performance is mainly attributed to the specific nature of tasks, where more demonstrations do not boost the models’ task understanding. Interestingly, math tasks benefit from additional demonstrations only when step-by-step solutions are derived and using strong LCLMs. RQ2: What skill does each task primarily measure? We then analyze the retrieval and global context understanding skills necessary for each ICL task. We use the ratio between the performance change of removing dissimilar examples and the change of removing similar examples. A high ratio means a more pronounced drop in performance upon removing similar examples, which indicates the task’s heavy reliance on retrieval capabilities. Our analysis indicates that existing many-shot ICL classification tasks (Li et al., 2024) predominantly assess retrieval abilities rather than global context understanding. This leads us to categorize tasks into retrieval and non-retrieval groups. Subsequently, we explore whether non-retrieval tasks genuinely benefit from additional demonstrations and assess models’ global context understanding skills. By comparing the performance of models with unique demonstrations versus duplicated examples on non-retrieval tasks, we aim to determine if duplicating examples adversely affects performance compared to adding new examples. If this is the case, it signifies that unique demonstrations provide additional beneficial information, reinforcing the notion that these tasks require global context understanding. Using this method, we identify a subset of non-retrieval tasks that evaluate models’ comprehension of global content. Following the categorization, we propose a new many-shot ICL benchmark, ManyICLBench, designed for evaluating long-context models and advocate for the inclusion of many-shot ICL tasks as effective evaluation candidates. Importantly, on ManyICLBench, models are tested to either retrieve the most similar demonstrations or assimilate all demonstrations to enhance their understanding of the task (Lin & Lee, 2024; Bertsch et al., 2024). Therefore, ManyICLBench evaluates both retrieval skills and global context understanding, thus providing a holistic assessment of long-context models’ capabilities. In summary, we make the following contributions in this paper: • Investigate whether ICL tasks benefit from additional demonstrations and assess their suitability for evaluating LCLMs with a context length up to 128k tokens. • Develop methods to characterize the primary skills evaluated by ICL tasks, where we focus on distinguishing between retrieval capabilities and global context understanding. • Construct a many-shot ICL benchmark, named ManyICLBench, designed for evaluating LCLMs on both retrieval and global context understanding, while excluding irrelevant datasets previously used in LCLM evaluation. • Benchmark 11 widely-used state-of-the-art LCLMs on ManyICLBench to assess their performance comprehensively."
https://arxiv.org/html/2411.07127v1,Benchmarking LLMs’ Judgments with No Gold Standard,"We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.","High-quality and reliable Large Language Model (LLM) benchmarks can effectively guide research, encourage innovation, monitor their advancement, and inform users of which model to use for their purpose. The importance of the final goal is underscored by the over 900k models currently available on Hugging Face, an online platform for open-source LLMs111https://huggingface.co/docs/hub/en/index. Various benchmarks are proposed for evaluating LLMs’ ability in different aspects, including ARC (Chollet, 2019), HellaSwag (Zellers et al., 2019), Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020), MT Bench (Zheng et al., 2023a), GSM8K (Cobbe et al., 2021), TruthfulQA (Lin et al., 2021), Natural Questions (Kwiatkowski et al., 2019), etc. Most benchmarks are based on multiple-choice questions or other questions with objective gold standard answers, since it is easy to verify the LLMs’ outputs. While they provide valuable evaluation for LLMs, open-ended tasks, e.g. providing judgment about a research paper, encompass a broader array of skills and require both objective and subjective reasoning. In addition, concern has been raised about data contamination (also called data leakage), where the training data contains information about the tasks in the benchmarks, as the LLMs are pre-trained on massive online data, which is also the source of some of the benchmark tasks. LLMs can show unreliably good performance due to data contamination (Sainz et al., 2023; Golchin and Surdeanu, 2023; Oren et al., 2023). In contrast, with open-ended questions, LLMs can be asked to provide judgments about newly created content, e.g. the latest academic papers, for which LLMs have yet to index human evaluations or responses. However, it is not clear how to automate the evaluation of subjective response quality. An added challenge is that there is no gold standard quality response with which to compare. We would like an evaluator to have two properties. First, it should be accurate and be sensitive to the semantic content response. As current LLMs have already shown strong language ability, we want to focus on evaluating the semantic informativeness of candidate responses instead of their syntax or style. Second, the evaluator should be manipulation-resistant-we should not be able to manipulate a response in a trivial fashion to consistently increase evaluation scores. This is important because otherwise one cannot determine whether a high evaluation score indicates that the LLM output a high-quality response or merely results from manipulation designed to achieve an artificially inflated score. Given these gaps in previous research, our research question is: Can we develop accurate, manipulation-resistant, and automated evaluation metrics for textual responses with no gold standard reference to compare with? A straightforward method may be to use another LLM as an oracle examiner directly to provide the evaluation, which has shown effectiveness in evaluating open-response QA (Bai et al., 2024), chatbot (Zheng et al., 2023b), etc. However, LLM examiners are susceptible to certain manipulations. In our experiment, elongating all responses by adding the same fixed sentences can significantly increase the GPT-4o LM examiner’s score. Interestingly, previous research has shown that even human evaluations can be manipulated by adding meaningless text (Goldberg et al., 2023). Meanwhile, other automated natural language generation (NLG) evaluation metrics, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), BARTScore (Yuan et al., 2021) and GPTScore (Fu et al., 2023), rely on comparison with a gold standard reference. 1.1. Our Contributions We propose the Generative Estimator for Mutual information (GEM) which uses the estimated Shannon mutual information (MI) between a set of candidate responses and a set of peer reference responses, which need not be of gold standard quality. The mutual information measures the amount of information revealed about the reference responses by obtaining the candidate responses. By artificially removing stylistic and syntactical information, our approach measures how much semantic information the candidate responses can reveal about the reference responses, which is related to the concept of semantic coverage (Yuan et al., 2021; Nenkova and Passonneau, 2004). We additionally propose a variant of our method, Generative Estimator for Mutual Information with Synopsis (GEM-S), which estimates the mutual information conditional on a synopsis of the task, e.g. the abstract of the paper. This prevents a candidate response from receiving a high score based solely on superficial information. Consequently, the score emphasizes the additional semantic information gained from the candidate responses. For implementation, we utilize a generative language model to estimate the conditional distribution between two text responses. Thus, the GEM can be categorized as an LLM-probability-based metric, with techniques similar to BARTScore (Yuan et al., 2021) and GPTScore (Fu et al., 2023), from which our metric inherits the effectiveness of evaluating objective tasks with a gold standard. Our work creates a bridge between the information theoretical framework in the literature of information elicitation without verification (Kong and Schoenebeck, 2019; Lu et al., 2024) and the NLG evaluation problem. Though GEM and GEM-S resemble the GPPM and GSPPM mechanisms in Lu et al. (2024), with manipulation resistance aligned to their incentive compatibility, we make a necessary change to make the score more suitable for the NLG evaluation problem. Specifically, for incentive compatibility, the score can rely solely on accuracy of predicting the peer reference, while we use mutual information to capture the gain in that accuracy for evaluation purposes. Results Overview The results of experiments validate GEM’s accuracy and resistance to manipulation, and compare it with many different NLG evaluation metrics. 1. Positive Correlation with Human Annotation (Section 4.1) On a human-annotated dataset, the GEM metrics, especially GEM-S, achieve a significant positive correlation with human-labeled quality scores and demonstrate competitive performance with GPT4-o Examiner while outperforming all other baseline metrics. 2. Better Sensitivity to Degradations (Section 4.2) Compared to various baseline metrics, GEM and GEM-S are the only metrics that demonstrate significant sensitivity to all semantic degradations in our experiment, by effectively penalizing degraded responses. 3. Better Robustness against Manipulations (Section 4.3) GEM and GEM-S are the only metrics that exhibit no significant score increases after meaningless elongation (Figure 4) and GPT-4o/Llama-3.1 rephrase, whereas LMExaminer show vulnerabilities by significantly increasing scores after all manipulations. Building on GEM and GEM-S, we introduce the GRE-bench (Generating Review Evaluation Benchmark) to evaluate LLMs’ peer review capabilities with data from open-access research papers and peer reviews. With the continuous influx of new data each year, GRE-bench can mitigate data contamination issues. In addition, since GRE-bench is based on GEM or GEM-S, it inherits their nice properties demonstrated in the experiments above. We run GRE-bench for various state-of-the-art LLMs on the ICLR 2023 dataset, and present the results in Section 5. We find a strong correlation between parameter size and GRE-bench score within LLM families, which further validates the effectiveness of GEM and GEM-S. 1.2. Related Work LLMs in academic peer review. Given the success of LLMs, they have the potential to assist with peer reviews when used appropriately (Robertson, 2023; Kuznetsov et al., 2024). Liang et al. (2023) use a survey to evaluate GPT-4-generated reviews, and find that the GPT-generated reviews are thought helpful by more than 50% of participants. However, there have been concerns regarding issues such as hallucinations (Donker, 2023) and inconsistent performance (Liu and Shah, 2023). While our study uses the peer review scenario to evaluate LLM capabilities, it also provides a quantitative comparison for the effectiveness of various LLMs in generating informative reviews. Information Elicitation. Both information elicitation and machine learning employ a principal-agent model (Ali and Silvey, 1966). Specifically, the principal aims to elicit information from the agent, e.g. the probability of rain tomorrow. To incentivize the agent to provide truthful and informative reports, payment mechanisms are employed that reward truthful and informative reports more than untruthful or uninformative ones. When the information is verifiable, e.g., we will eventually know whether it rains tomorrow, proper scoring rules (Good, 1952; Brier, 1950; Hendrickson and Buehler, 1971) can be applied, which are similar to the loss functions in supervised learning. When the information is subjective and unverifiable, Miller et al. (2005) propose the peer prediction mechanism, suggesting rewarding data by how well it predicts a peer’s subjective report which is judged by a proper scoring rule. Prelec (2004) propose the Bayesian Truth Serum mechanism. Kong and Schoenebeck (2019) propose an information-theoretic framework, suggesting paying the agents according to the f-mutual information between their reports. The framework provides a unified theoretical view of previous mechanisms in Dasgupta and Ghosh (2013), and Prelec (2004). Lu et al. (2024) first generalize the peer prediction to elicit informative textual responses with LLMs, providing the foundation for our GEM metrics. Most information elicitation mechanisms theoretically ensure that truthful and informative reports yield higher expected scores. However, high ex-post correlation between scores and report qualities is critical for practical application, as highlighted by Burrell and Schoenebeck (2021); Xu et al. (2024). In our study, to validate GEM’s effectiveness in evaluating textual reports, we measure both the ex-post correlation with human-annotated quality scores and expected score change after several degradation and manipulation strategies. Estimation of Mutual Information. Kong and Schoenebeck (2018b) bridges information elicitation without verification and co-training, by formulating an optimization problem that maximizes the mutual information between two Bayesian predictors with two data sources. Cao et al. (2019); Xu et al. (2019) apply this information theoretical approach in co-training and de-noising respectively. Concurrently and independently, Belghazi et al. (2018) and Hjelm et al. (2018) propose neural network-based methods for estimating mutual information, which can be used as an optimization goal in unsupervised and self-supervised learning settings."
https://arxiv.org/html/2411.07122v1,": Sparse Conditioned Autoencoders for
Concept Detection and Steering in LLMs","Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, but their output may not be aligned with the user or even produce harmful content. This paper presents a novel approach to detect and steer concepts such as toxicity before generation. We introduce the Sparse Conditioned Autoencoder (Scar), a single trained module that extends the otherwise untouched LLM. Scar ensures full steerability, towards and away from concepts (e.g., toxic content), without compromising the quality of the model’s text generation on standard evaluation benchmarks. We demonstrate the effective application of our approach through a variety of concepts, including toxicity, safety, and writing style alignment. As such, this work establishes a robust framework for controlling LLM generations, ensuring their ethical and safe deployment in real-world applications.111Code available at https://github.com/ml-research/SCAR","Large Language Models (LLMs) have become central to numerous natural language processing (NLP) tasks due to their ability to generate coherent and contextually relevant text [zhao_survey_2023, chang_survey_2024, wei_emergent_2022]. However, deploying these in real-world applications presents distinct challenges [kasneci_chatgpt_2023, solaiman2024evaluatingsocialimpactgenerative, Friedrich2022RevisionTI]. LLMs mainly behave as opaque systems, limiting the understanding and interpretability of their output. As such, they are prone to generate toxic, biased, or otherwise harmful content. Anticipating and controlling the generation of these texts remains a challenge despite the potentially serious consequences. Recent studies have systematically demonstrated the prevalence of bias and toxicity in LLMs [bommasani2021opportunities, weidinger2021ethical, liang2023holistic]. These works have led to the creation of evaluation datasets [gehman2020realtoxicityprompts, tedeschi2024alert] and tools to identify toxic content [noauthor_perspective_nodate, inan2023llama, helff2024llavaguard]. The dominant technique to mitigate the generation of unwanted text is fine-tuning on dedicated datasets [ouyang24training, rafailov_direct_2024]. Although these approaches have shown promise in mitigating toxicity, they can still be circumvented [wei_jailbroken_2023], are computationally expensive, and often do not generalize to unseen use cases. In addition, these methods encode static guardrails into the model and do not offer flexibility or steerability. More flexible techniques have been proposed in recent work [turner_activation_2024, dathathri_plug_2020, pei_preadd_2023], but suffer from other limitations. They often require backward [dathathri_plug_2020] or multiple forward passes [pei_preadd_2023], severely impacting latency and computational requirements at deployment. A further shortcoming of all of these methods is their inherent inability to detect toxic content. To remedy these issues, we propose Sparse Conditioned Autoencoders (Scar). We built on sparse autoencoders (SAEs) that have shown promising results in producing inspectable and steerable representations of LLM activations [gao_scaling_2024, cunningham_sparse_2023, templeton_scaling_2024]. However, SAEs do not guarantee that a desired feature—like toxicity—will be included nor disentangled in the latent space. Furthermore, SAEs still require manual labor or additional models to identify semantic features in the first place [rajamanoharan_improving_2024, bricken_towards_2023, rajamanoharan_jumping_2024]. Scar closes this gap by introducing a latent conditioning mechanism that ensures the isolation of desired features in defined latent dimensions. Specifically, we make the following contributions. 1) We formally define Scar and introduce a novel conditional loss function. 2) Subsequently, we empirically demonstrate Scar’s effectiveness and efficiency in producing inspectable representations to detect concepts. 3) Lastly, we provide empirical results for Scar’s usability in steering the generation of toxic content with no measurable effect on overall model performance."
https://arxiv.org/html/2411.07111v1,"Building a Taiwanese Mandarin Spoken Language Model: 
A First Attempt","This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.","This is a technical report on the practice of training large language models (LLMs) for spoken Taiwanese Mandarin. The content is based on the final project of a course and serves as a record of the work completed thus far. As a work-in-progress paper, the current version is not yet ready for conference submission. However, we believe that some ideas and findings from this project may inspire future work. Therefore, we have made the report publicly available. If you find the ideas or findings in this report useful for your project, please cite this technical report. 1.1 Goal of the Project The project aims to train a spoken LLM that can engage in real-time speech-to-speech conversations. The model takes a system prompt as input and, based on that prompt, facilitates seamless spoken interactions. Ultimately, the aim is to create an experience where conversing with the AI feels as natural as talking to a real person on Google Meet. In terms of interaction, there are several goals we aim to achieve: • Speech-to-speech, multi-turn conversations • Mandarin conversation with a Taiwanese accent • The model should be capable of seamless, full-duplex communication with humans. This means the model can process and respond in real time, allowing for interruptions and simultaneous speaking and listening. In the past, spoken dialogue has been achieved through a cascade framework, which combines automatic speech recognition (ASR), LLM, and text-to-speech (TTS) to facilitate speech-to-speech interaction. Here, we aim to build an end-to-end model. Since the model is end-to-end, we hope it can accomplish several tasks that a cascade model cannot. For example, it should be able to understand input audio beyond just the text, including prosody and speaker characteristics, and respond appropriately (Lin et al., 2024). It should also recognize non-verbal vocalizations (e.g., laughter, crying) and respond with the corresponding non-verbal sounds. Compared to a cascade model, an end-to-end model has the potential to interpret environmental sounds. However, we did not implement or thoroughly evaluate all of these capabilities due to time constraints, so we are unsure how well the model currently performs in these areas. Nonetheless, the end-to-end model has this potential, and we can extend the current work to incorporate these abilities in the future. Although GPT-4’s voice mode can achieve seamless interaction with speech, it is not open-sourced, so we cannot know how it is achieved. We hope this technical report can shed light on the development of spoken LLMs in the future, helping to accelerate the creation of open-source spoken LLMs for the community. Our model will soon be open-sourced. The open-source model most similar to our goal is Moshi (Défossez et al., 2024), but it primarily supports English and cannot understand or generate Mandarin. 1.2 Technical Contributions in This Project We believe this technical report may contain some technological contributions, though we have not fully verified them. Below, we briefly mention the potential contributions. For the model side, we demonstrate the potential to achieve real-time full-duplex machine-human speech communication using a typical decoder-only transformer architecture in Section 2. Existing speech-language models for generation, such as LLaMA-Omni (Fang et al., 2024), Mini-Omni (Xie & Wu, 2024), and Moshi (Défossez et al., 2024), require modifications to the standard transformer model. In contrast, we generate audio based on the standard decoder-only transformer architecture. Traditionally, full-duplex communication is achieved using a dual-channel system – one channel for listening and one for speaking (Nguyen et al., 2023; Ma et al., 2024). However, we show that even a typical transformer architecture can achieve full-duplex communication. Since our model uses the same architecture as text LLM, we can directly use a text-based LLM for initialization and fine-tune it from there. Ideally, the speech model will inherit the text model’s capabilities. Since it is possible to collect much more text data than speech data, initializing the speech model from a text model and leveraging the capabilities of a text LLM is critical to the success of a spoken LLM. Additionally, because we do not modify the network architecture of the text LLM, the model can benefit from the wide range of frameworks and toolkits designed to enhance text LLMs. This also means it can be easily deployed and accelerated. The data preparation and model training pipeline will be described in Section 3 and 4. We originally intended to use a large amount of dialogue data collected from the Internet. However, we found that using real dialogues from the Internet negatively impacted the model’s performance. Therefore, we opted to use synthesized data instead. First, we used a text-based LLM to generate dialogues, then applied a TTS model to vocalize the generated dialogues. To ensure that the TTS output sounded natural with a Taiwanese accent, we used a TTS model developed by another project111The technical report of the project will be available soon. Our goal is for the model to generate realistic dialogue that aligns with the emotional content of the conversation. For example, when a character says, “I feel sad,” the speech should sound appropriately sad. Typical TTS systems, especially commercial APIs, often struggle to capture such nuances, so we rely on our own TTS models. To create more realistic dialogues, we even generate dialogues with interruptions, which will be further explained in the following discussion. Even with a well-trained model, numerous implementation challenges remain in achieving real-time conversation. All the difficulties, solutions, and tips will be further discussed in Section 5. In Section 6, we will discuss how to evaluate spoken LLMs. Currently, there is no standard method for evaluating spoken LLMs. Commonly used benchmarks, such as Dynamic-SUPERB (Huang et al., 2024a; b), are instruction-following-based evaluations. However, we also want to assess how well the model can engage in communication, which no existing benchmark adequately measures. To address this, we developed a platform to facilitate communication between two chatbots. We also created an interface for users to interact with the bot, which simulates a smooth, natural conversation."
https://arxiv.org/html/2411.07107v1,Training Neural Networks as Recognizers of Formal Languages,"Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. However, when empirically testing these bounds, existing work often leaves a discrepancy between experiments and the formal claims they are meant to support. The problem is that formal language theory pertains specifically to recognizers: machines that receive a string as input and classify whether it belongs to a language. On the other hand, it is common to instead use proxy tasks that are similar in only an informal sense, such as language modeling or sequence-to-sequence transduction. We correct this mismatch by training and evaluating neural networks directly as binary classifiers of strings, using a general method that can be applied to a wide variety of languages. As part of this, we extend an algorithm recently proposed by Snæbjarnarson et al. (2024) to do length-controlled sampling of strings from regular languages, with much better asymptotic time complexity than previous methods. We provide results on a variety of languages across the Chomsky hierarchy for three neural architectures: a simple RNN, an LSTM, and a causally-masked transformer. We find that the RNN and LSTM often outperform the transformer, and that auxiliary training objectives such as language modeling can help, although no single objective uniformly improves performance across languages and architectures. Our contributions will facilitate theoretically sound empirical testing of language recognition claims in future work. We have released our datasets as a benchmark called FLaRe111https://github.com/rycolab/flare (Formal Language Recognition), along with our code.222https://github.com/rycolab/neural-network-recognizers","Neural network-based AI systems, including large language models (LLMs), have been hailed for their emergent reasoning abilities. What exactly are these abilities? The precise scope of what has emerged is hard to pin down. Fortunately, formal language theory gives us a vocabulary for ascribing hard limits to the kinds of computations neural networks can perform, enabling a much-needed formal characterization. For example, results from formal language theory allow us to know with certainty that a transformer LM (with no extra chain-of-thought timesteps) cannot determine whether two regular expressions with repetition operators are equivalent; a transformer LM runs in quadratic time, but the aforementioned problem provably requires exponential time (Sipser, 2013). A long line of research has attempted to precisely describe the class of problems neural architectures can solve in terms of formal languages. It consists of two parts: formal results that mathematically prove language class bounds, often under simplifying assumptions; and empirical results that, in complementary fashion, provide evidence of these bounds under real settings. Simplifying assumptions for the transformer architecture have included the absence of layer normalization, the use of hard attention, or the use of special positional encodings (see Strobl et al. (2024b) for a survey). Formal expressivity results also typically do not comment on whether solutions are practically reachable through training, even though the bias imposed by the training algorithm may render the set of solvable problems much smaller than suggested by formal expressivity results. Empirical results are therefore important for validating formal results under unsimplified conditions. The purpose of this paper is to reconcile a subtle but important disconnect between empirical results and claims about computational power. Formal language theory deals in recognizers: machines that receive a string as input and classify whether it is a member of a language. The Chomsky hierarchy and the classes P and NP are defined in terms of this paradigm. Consider, for example, the recent study by Delétang et al. (2023): its core claims are about the Chomsky hierarchy, but the supporting experiments train language models and evaluate them as string-to-string functions. While the experiments do validate the authors’ claims about a memory-based hierarchy in an informal sense, formally, the experiments do not demonstrate claims about the Chomsky hierarchy of languages, but an analogous hierarchy of functions. There are multiple ways to fix this mismatch: one could change the theoretical claims to those of a hierarchy of language models (Icard, 2020; Borenstein et al., 2024) or string-to-string functions (Strobl et al., 2024a); or one could change the experiments to recognition to match the Chomsky hierarchy. In this paper, we explore the latter approach. We propose an experimental setup for training neural networks as recognizers of formal languages that only requires language-specific algorithms for positive sampling and membership testing. In contrast to most prior work, we generate adversarial negative examples without requiring language-specific rules (cf. Weiss et al., 2018b; Someya et al., 2024; Bhattamishra et al., 2024). Like Delétang et al. (2023), we focus on length generalization, and we include two sets of experiments that carefully distinguish between tests of inductive bias and expressivity. We extend recent work by Snæbjarnarson et al. (2024) to implement a scalable algorithm for length-controlled sampling from finite automata, with a preprocessing step that is asymptotically faster than a standard approach by a factor of {{O}({n_{\mathrm{max}}}^{2})}, where {n_{\mathrm{max}}} is the maximum string length. In order to explore the effectiveness of other training objectives used in past work while remaining within the recognition paradigm, we experiment with auxiliary loss terms in a multi-task learning setup. Although they do aid certain architectures on specific tasks, they do not have a consistent effect across architectures and languages, and a simple binary cross-entropy objective is often very effective. We compare three architectures: a simple RNN, an LSTM, and a causally-masked transformer. We experiment on a variety of formal languages across the Chomsky hierarchy and show that the transformer often underperforms the RNN and LSTM. We also see evidence of recent claims that transformers prefer low-sensitivity Boolean functions (Hahn & Rofin, 2024). We have publicly released our datasets as a benchmark called FLaRe (Formal Language Recognition), along with our code."
https://arxiv.org/html/2411.07075v1,Transformer verbatim in-context retrieval across time and scale,"To predict upcoming text, language models must in some cases retrieve in-context information verbatim. In this report, we investigated how the ability of language models to retrieve arbitrary in-context nouns developed during training (across time) and as language models trained on the same dataset increase in size (across scale). We then asked whether learning of in-context retrieval correlates with learning of more challenging zero-shot benchmarks. Furthermore, inspired by semantic effects in human short-term memory, we evaluated the retrieval with respect to a major semantic component of target nouns, namely whether they denote a concrete or abstract entity, as rated by humans. We show that verbatim in-context retrieval developed in a sudden transition early in the training process, after about 1% of the training tokens. This was observed across model sizes (from 14M and up to 12B parameters), and the transition occurred slightly later for the two smallest models. We further found that the development of verbatim in-context retrieval is positively correlated with the learning of zero-shot benchmarks. Around the transition point, all models showed the advantage of retrieving concrete nouns as opposed to abstract nouns. In all but two smallest models, the advantage dissipated away toward the end of training.","In language models (LMs), successful prediction of upcoming words depends on in-context information. For example, when given the context prompt “The novel’s plot and symbolism are centered around three objects: a centipede, a parachute, and a waterfall. The first and most important object in the list is the ___”, an LM must retrieve the noun (centipede) out of all in-context tokens to correctly predict the continuation. In human cognitive science, this ability to flexibly retrieve items from recent context is known as short-term memory and is believed to be the core computation underlying human cognition (Baddeley, 2003). Figure 1: Overview of the approach and experiments. Recently, Armeni et al. (2022) showed that a transformer language model (GPT-2, Brown et al., 2020) develops such flexible short-term memory — it was able to retrieve the identity and ordering of lists of arbitrary nouns from recent context (Fig. 1, A), even though retrieval of arbitrary in-context information is not the explicit objective of LMs (as opposed to dedicated models of short-term memory, e.g. Oberauer et al. 2018). Yet, studying retrieval in a single fully-trained model on arbitrary nouns neglects three further dimensions of the capacity: how it is learned, how learning of this dedicated capacity relates to models’ learning of other tasks, and the semantics of retrieved nouns. First, studying learning trajectories of LM capacities offers complementary insights to studying only performance of fully-trained models (e.g. Chen et al., 2024). Previous work on LM learning trajectories showed that transformers learn next-token prediction by undergoing a sudden transition (“phase change”) early during training, which coincides with the development of attention heads that attend to repeated tokens (Olsson et al., 2022). Does verbatim retrieval follow a similar learning trajectory? Second, the ability to retrieve and predict in-context tokens verbatim (i.e. identity-based matching) can be viewed as a rudimentary form of the more flexible zero-shot learning, where the relevant in-context information is not necessarily given verbatim and must possibly be retrieved based on fuzzy, similarity-based matching Olsson et al. (2022). How does successful learning of verbatim retrieval relate to LM’s zero-shot performance on more challenging benchmark tasks? Third, while the successful retrieval of arbitrary nouns underscores the flexibility of transformer short-term memory, this approach neglects that the lexicon of natural language is not a set of unorganized, arbitrary words — instead, it has semantic structure. Two prominent semantic categories are concrete and abstract nouns. Concrete nouns (e.g. “hammer”) have sensory referents, whereas abstract nouns (e.g. “justice”) do not have a straightforward sensory component. Word concreteness affects human cognitive processing. Children typically acquire concrete words, especially nouns, earlier than abstract words (Gleitman et al., 2005). In certain short-term memory paradigms, humans are better at recalling concrete than abstract words (Taylor et al., 2019). Importantly, the two word categories differ also in their distributional properties: concrete words occur in a semantically narrower range of contexts compared to abstract words (Schulte im Walde and Frassinelli, 2022). Is the transformer retrieval affected by whether nouns refer to concrete vs. abstract entities? To address these questions, we evaluated verbatim in-context retrieval on the Pythia suite of language models (Biderman et al., 2023). Leveraging the fact that the suite includes pretrained LMs ranging from 14M to 12B parameters in scale and their intermediate training checkpoints across the entire learning epoch, we evaluated how retrieval develops over the course of training and across model sizes (Fig. 1, B and C). Additionally, the Pythia suite contains zero-shot evaluations on various benchmarks for each LM checkpoint. To test how in-context retrieval relates to LM’s zero-shot performance, we correlated the learning trajectory of the retrieval against the learning trajectories on zero-shot benchmarks (Fig. 1, D). Finally, to test the role of noun semantics for in-context retrieval, we evaluated how noun concreteness, as rated by human participants Brysbaert et al. (2014), affected retrieval over the course of training (Fig. 1, E). The main contributions of the current work are: a) In all models, verbatim retrieval developed in a sudden transition early during training, after about 1% training tokens elapsed, and remained constant during the rest of training, b) learning of verbatim retrieval was positively correlated with learning of zero-shot task performance, and c) around the transition point, LMs showed an advantage to retrieve concrete rather than abstract nouns. This advantage almost entirely diminished towards the end of training."
https://arxiv.org/html/2411.07070v2,On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models,"The pretraining and fine-tuning approach has become the leading technique for various NLP applications. However, recent studies reveal that fine-tuning data, due to their sensitive nature, domain-specific characteristics, and identifiability, pose significant privacy concerns. To help develop more privacy-resilient fine-tuning models, we introduce a novel active privacy auditing framework, dubbed Parsing, designed to identify and quantify privacy leakage risks during the supervised fine-tuning (SFT) of language models (LMs). The framework leverages improved white-box membership inference attacks (MIAs) as the core technology, utilizing novel learning objectives and a two-stage pipeline to monitor the privacy of the LMs’ fine-tuning process, maximizing the exposure of privacy risks. Additionally, we have improved the effectiveness of MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our research aims to provide the SFT community of LMs with a reliable, ready-to-use privacy auditing tool, and to offer valuable insights into safeguarding privacy during the fine-tuning process. Experimental results confirm the framework’s efficiency across various models and tasks, emphasizing notable privacy concerns in the fine-tuning process. Project code available for https://anonymous.4open.science/r/PARSING-4817/","Concerns regarding the privacy of training data pose a significant challenge in AI security, especially in sensitive domains like healthcare [34] and finance [55], where privacy issues are particularly pronounced. The study of privacy attacks and defense mechanisms for large language models (LLMs) is still in its early stages. In the past, research has primarily concentrated on extracting training data from pre-trained language models (PLMs) [5, 27, 24], leading to the development of novel theories regarding model memorization in LLMs [7] and mechanisms of data leakage [26]. Compared to the extensive and diverse datasets used during the pre-training stage, the training data in the fine-tuning phase presents much greater privacy risks. On one side, the constrained datasets employed for fine-tuning usually do not match the number of model parameters, resulting in an over-reliance on a limited set of data samples [12, 8, 56]. This reliance not only greatly impacts the model’s ability to generalize, but also raises the likelihood of disclosing sensitive details from the training data. On the other side, fine-tuning datasets are frequently derived from particular real-world sectors and usually contain more intricate information. While improving performance on targeted tasks, this specificity also significantly increases the cost of privacy breaches. Figure 1: An example of privacy auditing results for fine-tuning a model on different tasks. Quantify the level of privacy leakage using two carefully designed metrics. Thorough privacy evaluations greatly enhance user trust in models and their creators, serving as a crucial element in the sustainable advancement of technology. The emergence of advanced privacy attack algorithms [9, 26] have increased the demand for innovative research on privacy protection and auditing. We believe that an effective privacy audit should be able to present the quantified results of privacy risks in a simple and intuitive manner, as shown in Figure 1. MIAs [50, 20, 36, 16], which form the basis for many other types of attack and privacy audit frameworks [38, 6, 40, 51], are employed as a critical benchmark to assess the privacy boundaries of models. These attacks provide a direct measure for assessing privacy risks, where the attack metrics may mirror the model’s privacy level. Notably, the dynamic and intricate nature of the fine-tuning process arises both theoretical and practical challenges for privacy analysis under MIAs, hindering their successful application in deployed LMs. Additionally, existing privacy auditing methods primarily focus on the passive detection phase, where they assess privacy leakage after model training is completed. However, these methods often struggle to effectively address the dynamic changes occurring during the fine-tuning process, potentially overlooking critical privacy risks. To shed new light on developing more privacy-robust LMs, the following natural questions are explored as, Can the model fine-tuner actively identify privacy risks during the fine-tuning process? Can we quantify the privacy risks in this process using MIAs? What are the characteristics of privacy leakage during the process of LMs? As a result, it is necessary to develop more thorough methods to fill this technological gap. We note that non-members and members show noticeable variations in numerical data such as loss and gradient norms at the final layer, as well as in implicit data such as intermediate module outputs and gradients during fine-tuning. We have developed an effective methodological framework Parsing (Privacy Auditing on Risk of Supervised fine-tunING) to tackle these distinctions, highlighting and quantifying the privacy risks associated with the fine-tuning of LMs. Contributions. In this paper, we make the following contributions to the study of privacy in LMs. • We introduce a novel privacy auditing framework applied during the fine-tuning phase of LMs, which has the potential to be employed in the foundation model designs. The Parsing framework aims to identify and quantify the risks of privacy leakage inherent in the fine-tuning process and seeks to help achieve a balance between privacy and utility. We offer a detailed explanation of the structure and operational mechanisms of the framework. • We propose an active two-stage white-box MIA method targeting LMs, applicable to various models including the GPT series and Llama series. By introducing a new methodological architecture and learning objectives, the method first optimizes the membership representation of samples, thereby enhancing their feature representation capability. This improvement significantly increases the effectiveness of MIAs on complex models. • We evaluate Parsing on a range of models and diverse text tasks, and benchmarked it against existing studies. Empirical validation has shown that our framework is effective in detecting and quantifying privacy risks during the fine-tuning of LMs. Moreover, we also conduct a systematic analysis of the key factors leading to privacy vulnerabilities during the fine-tuning process, including task complexity, model size, text length, and so forth, in order to propose corresponding privacy protection strategies."
https://arxiv.org/html/2411.07037v1,LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios,"As Large Language Models (LLMs) continue to advance in natural language processing (NLP), their ability to stably follow instructions in long-context inputs has become crucial for real-world applications. While existing benchmarks assess various LLM capabilities, they rarely focus on instruction-following in long-context scenarios or stability on different inputs. In response, we introduce the Long-context Instruction-Following Benchmark (LIFBench), a scalable dataset designed to evaluate LLMs’ instruction-following capabilities and stability across long contexts. LIFBench comprises three long-context scenarios and eleven diverse tasks, supported by 2,766 instructions generated through an automated expansion method across three dimensions: length, expression, and variables. For evaluation, we propose LIFEval, a rubric-based assessment framework that provides precise, automated scoring of complex LLM responses without relying on LLM-assisted evaluations or human judgments. This approach facilitates a comprehensive analysis of model performance and stability across various perspectives. We conduct extensive experiments on 20 notable LLMs across six length intervals, analyzing their instruction-following capabilities and stability. Our work contributes LIFBench and LIFEval as robust tools for assessing LLM performance in complex, long-context settings, providing insights that can inform future LLM development.","Benchmark Long. Inst. Stab. Unlim. ZeroSCROLLS (2023) ✓ ✗ ✗ ✗ BAMBOO (2024) ✓ ✗ ✗ ✗ Longbench (2023) ✓ ✗ ✗ ✗ \infty Bench (2024c) ✓ ✗ ✗ ✗ RULER (2024) ✓ ✗ ✗ ✓ IFEval (2023a) ✗ ✓ ✗ ✗ FollowBench (2023) ✗ ✓ ✗ ✗ InfoBench (2024b) ✗ ✓ ✗ ✗ CELLO (2024) ✗ ✓ ✗ ✗ LIFBench (Ours) ✓ ✓ ✓ ✓ Table 1: A comparison of our LIFBench with some relevant datasets. We summarize their focus, including Long-context scenarios, Instruction-Following, and model stability. ‘Unlim.’ denotes whether the data length can be unlimited. . Figure 1: The framework of LIFBench. Bold denotes the scenario description D; normal denotes the context X; italics denotes instruction I, where the red indicate the instruction variables var, and the remaining black parts correspond to the instruction template tpl. As Large Language Models (LLMs) continue to expand their impact across practical applications Achiam et al. (2023); Chowdhery et al. (2023); Brown (2020), their performance in natural language processing (NLP) tasks has reached new heights, spanning text generation Que et al. (2024); Tan et al. (2024); Zhang et al. (2024b), complex reasoning Parmar et al. (2024), and problem-solving Lu et al. (2023); Li et al. (2024a). In real-world production scenarios, stable instruction-following ability in long-context inputs are crucial, as these capabilities enable LLMs to better meet user needs and accurately complete more complex and diverse tasks. These needs have driven the creation of a number of benchmarks to accurately assess the LLMs’ capabilities. On the one hand, Shaham et al. (2023); An et al. (2023); Dong et al. (2024) collated traditional NLP datasets, forming comprehensive benchmarks containing long data. Subsequently, considering the power of LLMs on open-ended tasks, Bai et al. (2023); Zhang et al. (2024c); Hsieh et al. (2024) has further introduced some synthetic tasks to better assess the understanding and problem-solving capabilities in long contexts. On the other hand, some studies have gone beyond the limitations of specific downstream tasks, crafted more elaborate and systematic instruction datasets, which aim to assess a model’s ability to follow complex instructions Zhou et al. (2023a); He et al. (2024); Qin et al. (2024b) or satisfying diversiform constraints Jiang et al. (2023); Wen et al. (2024). However, two issues remain unresolved: (1) What is the instruction-following capability of LLMs in long-context scenarios? (2) How to evaluate the impact of some input factors (e.g., the expression of instruction, the variable in the input) on LLMs’ stability? Based on these issues, we introduce the Long-context Instruction Following Benchmark (LIFBench), a scalable benchmark for evaluating LLM instruction-following capability and stability in long-context scenarios. The framework of our benchmark is shown in Figure 1. Considering realistic applications of LLM, we construct three long-context scenarios based on the granularity of information to be processed. On this basis, eleven delicate tasks are designed, which can reflect various dimensions of instruction-following capabilities. We manually crafted templates for all tasks, and introduced an automated instruction expansion method from three dimensions (length, expression, and variables), resulting in a total of 2,766 instructions. The dataset’s scalability allows our benchmark to grow extensively and support testing with arbitrarily long context lengths. For evaluation, traditional metrics for downstream tasks are not applicable to complex instruction-following scenarios Honovich et al. (2023). Moreover, many studies rely on GPT-4 for automated, open-ended assessment, but this approach encounters limitations due to notable gaps between GPT-4 and human experts Qin et al. (2024b); Jiang et al. (2023), as well as potential bias problems Wang et al. (2023). To address these challenges, we propose LIFEval, a framework for evaluating complex LLM responses. Through rubric-based scoring and an automated assessment process, LIFEval enables precise, detailed distinctions in response quality without relying on LLMs or human evaluators, providing insights into models’ fundamental capabilities and stability from various perspectives. Overall, our contributions are as follows: • We introduce LIFBench, a benchmark designed to evaluate instruction-following capabilities in long-context scenarios, containing 11 carefully designed tasks across three scenarios. • We develop methods for dataset expansion across three perspectives, enabling scalable data generation with up to 2,766 instructions, adaptable to arbitrary context lengths. • We propose LIFEval, an evaluation framework for accurately, rapidly, and comprehensively assessing the quality and stability of LLMs’ complex responses. • We conduct extensive experiments across six length intervals, which evaluates and analyzes the instruction-following capabilities and stability of 20 famous LLMs, including both open-source and closed-source models."
https://arxiv.org/html/2411.07019v1,UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction,"Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, existing link prediction models are usually designed for one specific type of facts, making it difficult to generalize to other fact representations. To overcome this limitation, we propose a Unified Hierarchical Representation learning framework (UniHR) for unified knowledge graph link prediction. It consists of a unified Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing the semantic information within individual facts and enriching the structural information between facts. Experimental results across 7 datasets from 3 types of KGs demonstrate that our UniHR outperforms baselines designed for one specific kind of KG, indicating strong generalization capability of HiDR form and the effectiveness of HiSL module. Code and data are available at https://github.com/Lza12a/UniHR.","Large-scale knowledge graphs (KGs) such as WordNet Miller (1995), Freebase Bollacker et al. (2008), and Wikidata Vrandečić and Krötzsch (2014) have been widely applied in many areas like question answering Kaiser et al. (2021), recommendation systems Guo et al. (2020), and natural language processing Annervaz et al. (2018). However, the presence of missing facts within these KGs inevitably limit their applications. Therefore, the link prediction task has been introduced to predict missing elements within factual data. Current link prediction methods mainly focus on facts in the form of triple \left(head\,entity,relation,tail\,entity\right). Figure 1: A special KG consists of triple-based fact, hyper-relational fact, nested fact and temporal fact. Despite the simplicity and unity of triple-based representation, it is difficult to adequately express complex facts, such as “Oppenheimer is educated at Harvard University for a bachelor degree in chemistry"" shown in Figure 1. Therefore, existing researches Wang et al. (2021); Xiong et al. (2024); Xu et al. (2019) contribute to focusing on semantically richer facts. Figure 1 illustrates three specific types of facts: hyper-relational fact ((Oppenheimer, educated at, Harvard University), degree: bachelor, major: chemistry), temporal fact (Oppenheimer, honored with, Fermi Prize, 1963), nested fact ((Oppenheimer, born in, New York), imply, (Oppenheimer, nationality, The United States)). These forms of facts allow for expression of complex semantics and revelation of relationships between facts, extending beyond the triple-based representation. Thus in recent years, Hyper-relational KGs (HKG) Chung et al. (2023), Temporal KGs (TKG) Xu et al. (2023a), and Nested factual KGs (NKG) Xiong et al. (2024) attract wide research interests. Recent studies have demonstrated the effectiveness of various embedding strategies for these beyond-triple representations Xiong et al. (2023). However, these methods are usually designed for specific representation forms, e.g., StarE Galkin et al. (2020) customizes graph neural network to implement message passing on hyper-relational facts, For nested factual KGs, BiVE Chung and Whang (2023) connects two levels of facts throgh a simple linear layer. In addition, GeomE+ Xu et al. (2023a) et al. temporal KG embedding methods contain time-aware scoring functions to adapt timestamps. Although these methods perform well on specific type of facts, it is evident that such customized methods are difficult to generalize to other types of KGs. Therefore, establishing a unified representation learning method for multiple types of KGs is worth to investigate. To overcome the challenges mentioned above, we propose a Unified Hierarchical Representation learning method (UniHR), which includes a Hierarchical Data Representation (HiDR) module and a Hierarchical Structure Learning (HiSL) module as the graph encoder. HiDR module standardizes hyper-relational facts, nested factual facts, and temporal facts into the form of triples without loss of information. Furthermore, HiSL module captures local semantic information during intra-fact message passing and then utilizes inter-fact message passing to enrich the global view of nodes to obtain better node embeddings based on HiDR form. Finally, the updated embeddings are fed into decoders for link prediction. Experimental results demonstrate that our UniHR achieves state-of-the-art performance on HKG and NKG datasets, and competitive performance on TKG datasets, revealing strong generalization capability of HiDR form and effectiveness of HiSL module. Our contributions can be summarized as follows: 1. We emphasize the value of investigating unified KG representation method, including unified symbolic representation and unfied representation learning method for different KGs. 2. We propose the first unified KG representation learning framework UniHR, across different types of KGs, including a hierarchical data representation module and a hierarchical structure learning module. 3. We conduct link prediction experiments on 7 datasets across 3 types of KGs. Compared to methods designed for one kind of KG, UniHR achieves the best or competitive results, verifying strong generalization capability."
https://arxiv.org/html/2411.06989v1,Token2Wave,"This paper provides an in-depth analysis of Token2Wave, a novel token representation method derived from the Wave Network, designed to capture both global and local semantics of input text through wave-inspired complex vectors. In Token2Wave, each token is represented with a magnitude component, capturing the global semantics of the entire input text, and a phase component, encoding the relationships between individual tokens and the global semantics. Building on prior research that demonstrated the effectiveness of wave-like operations, such as interference and modulation, during forward propagation, this study investigates the convergence behavior, backpropagation characteristics, and embedding independence within the Token2Wave framework. A detailed computational complexity analysis shows that Token2Wave can significantly reduce video memory usage and training time compared to BERT. Gradient comparisons for the [CLS] token, total input text, and classifier parameters further highlight Token2Wave’s unique characteristics. This research offers new insights into wave-based token representations, demonstrating their potential to enable efficient and computationally friendly language model architectures.","Currently, there are two types of token embedding methods. The fixed token embedding, such as Skip-gram and Continuous Bag of Words (CBOW) [1], assign the same embedding vector to each token, which cannot adapt to the dynamic meanings of tokens in varying contexts. The context-dependent embedding, on the other hand, generates different embeddings for the same token depending on its contexts. Many current Natural Language Processing (NLP) methods, such as the Transformer [2], use the attention mechanism to update token embeddings by measuring relationships between tokens with dot products. However, attention only infers global semantics indirectly through pairwise relationships rather than directly capturing the overall meaning of the text. In our previous work [3], we introduced the Wave Network, a language model based on a new token representation method called Token2Wave. Token2Wave uses complex vector token representations to represent both the global and local semantics of each token with two parts: a magnitude vector representing the global semantics of the input text, and a phase vector capturing the relationships between individual tokens and global semantics. The complex vector token representations enables wave-like operations, such as interference and modulation for efficient updates. While the previous work focused on constructing token representations as waves and their forward propagation in text classification tasks, the current study delves into the architectural and functional aspects of the Wave Network. Here, we present a thorough analysis of the convergence performance, gradient behaviors of the network components (e.g., [CLS] embedding, overall input embedding, classifier), and the independence level among embedding dimensions. By focusing on these aspects, we aim to provide deeper insights into the theoretical details of the Wave Network and its potential effectiveness in various NLP tasks."
https://arxiv.org/html/2411.06950v1,Sniff AI: Is My ‘Spicy’ Your ‘Spicy’? Exploring LLM’s Perceptual Alignment with Human Smell Experiences,"Aligning AI with human intent is important, yet perceptual alignment—how AI interprets what we see, hear, or smell—remains underexplored. This work focuses on olfaction, human smell experiences. We conducted a user study with 40 participants to investigate how well AI can interpret human descriptions of scents. Participants performed ""sniff and describe"" interactive tasks, with our designed AI system attempting to guess what scent the participants were experiencing based on their descriptions. These tasks evaluated the Large Language Model’s (LLMs) contextual understanding and representation of scent relationships within its internal states - high-dimensional embedding space. Both quantitative and qualitative methods were used to evaluate the AI system’s performance. Results indicated limited perceptual alignment, with biases towards certain scents, like lemon and peppermint, and continued failing to identify others, like rosemary. We discuss these findings in light of human-AI alignment advancements, highlighting the limitations and opportunities for enhancing HCI systems with multisensory experience integration.","Aligning Artificial intelligence (AI) behaviour with human preference is critical for the future of AI. An important yet often overlooked aspect of this alignment is the perceptual alignment. Perceptual alignment refers to the agreement between AI assessments and human subjective judgments across different sensory modalities, such as vision, hearing, taste, touch, and smell [1, 2, 3]. It enables AI to better understand the physical world as humans experience it, ensuring that AI applications are reliable and beneficial in real-world settings. For example, consider autonomous vehicles: if the ""AI eye"" misinterprets data from sensors such as cameras and fails to recognize obstacles or pedestrians, it poses significant safety risks [4]. Beyond safety considerations, perceptual alignment plays a critical role in everyday AI applications [3], whereas olfactory alignment remains completely unexplored. Imagine a future where AI assistants are capable of controlling environmental factors like lighting and scents based on user requests. For instance, instead of asking ""Alexa, play uplifting workout music"", you can ask Alexa to ""spice up my workout session"" or ""help me remember my holiday to Madrid"". Here, the challenge for AI goes beyond playing music and ventures into ""AI sniff"", to select the ideal scent aligned with human descriptions. This poses the question of how AI would understand and interpret scents in a way that resonates with our personal sensory experiences. Or simply: Is my ""spicy"" AI’s ""spicy""? AI, or mostly Large Language Models (LLMs), lack human-like perceptions; rather, they process human provided language inputs using algorithms that function on binary systems to analyse information [5, 6, 7, 8, 9]. We place a special emphasis on LLMs as they are being increasingly viewed as the interface for human interaction, with agentic AI systems and general alignment being one of its core research domains [10, 11]. LLMs, or any neural network-based learning systems, represent concepts and ideas in what is commonly known as an embedding space – a learned internal high-dimensional vector space [12, 13]. In this space, semantically similar items cluster closely together, and the semantic differences between items are preserved [14]. This naturally lends us a hand in analyzing how closely aligned humans and AI are. If two items are deemed similar within the AI system’s embedding space, humans should also perceive them as similar, if human-AI alignment exists. Our work then exploits exactly this approach and focuses on analyzing the model’s embedding space. We leverage LLM-based embedding models to develop an AI system capable of performing the ""Human sniff and describe and AI guesses"" task. Here, the LLM encoder translates human language descriptors of scents into the embedding space, and the system makes then scent suggestions based on the semantic similarities measured in this vector space. Given the recent advancements in LLMs’ ability to interpret human language, an intriguing question arises: can LLMs effectively understand scents based on user descriptions? For instance, will both LLMs and humans agree on Jasmine and Ylang-Ylang being perceptually similar? To investigate this question, we conducted an in-person user study with 40 participants, where participants engaged in interactive tasks where an AI system had to guess what scent they were experiencing. These participants, who were non-domain experts, were specifically chosen to reflect common perceptions of scents as conceptualized by Henning’s Odour Prism [15], the Fragrance Wheel [16], and attributes such as the fresh, citrusy, and zesty qualities typically associated with lemon. We analysed the system’s performance using both quantitative and qualitative methods to capture the participant feedback. Our findings suggest that scent-related semantics are represented in the embedding space, though to a limited extent. There is some degree of perceptual alignment, but it was biased toward certain scents and characteristics. For instance, sometimes the AI believed the human description of ""an aromatic plant that probably you use for like stew or for like chicken and it’s very green and is fresh"" refers to eucalyptus rather than rosemary. Occasionally, participants were surprised by certain emergent behaviours; for instance, the AI correctly identified a ""intense masculine scent"" as oakmoss. We discuss these findings in light of recent advancements of LLMs and efforts towards improved human-AI alignment, with the opportunity to enhance HCI systems with multisensory experience integration."
https://arxiv.org/html/2411.06946v1,Cancer-Answer: Empowering Cancer Care with Advanced Large Language Models,"Gastrointestinal (GI) tract cancers account for a substantial portion of the global cancer burden, where early diagnosis is critical for improved management and patient outcomes. The complex aetiologies and overlapping symptoms across GI cancers often delay diagnosis, leading to suboptimal treatment strategies. Cancer-related queries are crucial for timely diagnosis, treatment, and patient education, as access to accurate, comprehensive information can significantly influence outcomes. However, the complexity of cancer as a disease, combined with the vast amount of available data, makes it difficult for clinicians and patients to quickly find precise answers. To address these challenges, we leverage large language models (LLMs) such as GPT-3.5 Turbo to generate accurate, contextually relevant responses to cancer-related queries. Pre-trained with medical data, these models provide timely, actionable insights that support informed decision-making in cancer diagnosis and care, ultimately improving patient outcomes. We calculate two metrics: A1 (which represents the fraction of entities present in the model-generated answer compared to the gold standard) and A2 (which represents the linguistic correctness and meaningfulness of the model-generated answer with respect to the gold standard), achieving maximum values of 0.546 and 0.881, respectively.","Gastrointestinal (GI) tract cancers [1, 2, 3], represent a significant portion of cancer-related morbidity and mortality worldwide, encompassing malignancies of the esophagus, stomach, liver, pancreas, and intestines. Early detection and accurate diagnosis are paramount for improving prognosis and patient survival. However, these cancers present a unique set of challenges due to their complex aetiologies and overlapping symptoms, which often result in delayed diagnosis and misclassification. Differentiating between various GI tract cancers remains a formidable task for clinicians, who must navigate a wide array of symptoms that can mimic benign conditions or other malignancies. In this context, cancer-related queries are crucial for timely diagnosis, treatment, and patient education, as access to accurate and comprehensive information can significantly impact outcomes [4]. The sheer volume of data and complexity of cancer as a disease make it difficult for clinicians and patients to quickly access precise answers. Traditional diagnostic methods [5, 6], including imaging, endoscopy, and histopathological examination, although valuable, sometimes fall short in providing rapid and precise differentiation of these cancer types. As a result, delays in diagnosis can compromise the effectiveness of treatment, leading to suboptimal patient outcomes. The need for more advanced, data-driven diagnostic tools has never been greater. In recent years, the advent of large language models (LLMs) [7, 8, 9] such as GPT-3.5 Turbo has opened new possibilities in medical diagnostics and decision support. These models, when prompted appropriately, have demonstrated remarkable potential in generating human-like text and answering complex queries. Leveraging LLMs for medical diagnostics offers a promising approach to addressing the diagnostic challenges posed by GI tract cancers. In this work, we explore the use of prompted LLMs to generate answers to medical queries, with a particular focus on their applicability to differentiating between GI tract cancers. By harnessing the power of these models, we aim to offer new insights into how artificial intelligence can assist clinicians in making more timely and accurate diagnoses, ultimately improving patient outcomes. We explore prompted large language models (LLMs), such as GPT-3.5 Turbo [10], by designing and using specific prompts to generate relevant and coherent answers for various medical queries. These prompts guide the model to focus on producing medically accurate, context-appropriate responses based on the input. By leveraging the capabilities of GPT-3.5 Turbo, we aim to harness its vast knowledge base and advanced natural language understanding to assist in addressing a range of medical-related questions effectively. The use of prompts ensures that the model responds in a structured manner, providing meaningful information that aligns with the specific medical context of the queries. Metrices A1 and A2 represent distinct components or processes within the evaluated system, and their performance metrics offer valuable information about the operational efficiency and potential areas for enhancement. By examining the results from multiple runs, we aim to identify trends, improvements, or inconsistencies that could impact the overall effectiveness of the system. This evaluation not only helps in understanding the current performance but also guides future adjustments and optimizations."
https://arxiv.org/html/2411.06899v1,"LongSafetyBench: Long-Context LLMs 
Struggle with Safety Issues","With the development of large language models (LLMs), the sequence length of these models continues to increase, drawing significant attention to long-context language models. However, the evaluation of these models has been primarily limited to their capabilities, with a lack of research focusing on their safety. Existing work, such as ManyShotJailbreak, has to some extent demonstrated that long-context language models can exhibit safety concerns. However, the methods used are limited and lack comprehensiveness. In response, we introduce LongSafetyBench, the first benchmark designed to objectively and comprehensively evaluate the safety of long-context models. LongSafetyBench consists of 10 task categories, with an average length of 41,889 words. After testing eight long-context language models on LongSafetyBench, we found that existing models generally exhibit insufficient safety capabilities. The proportion of safe responses from most mainstream long-context LLMs is below 50%. Moreover, models’ safety performance in long-context scenarios does not always align with that in short-context scenarios. Further investigation revealed that long-context models tend to overlook harmful content within lengthy texts. We also proposed a simple yet effective solution, allowing open-source models to achieve performance comparable to that of top-tier closed-source models. We believe that LongSafetyBench can serve as a valuable benchmark for evaluating the safety capabilities of long-context language models. We hope that our work will encourage the broader community to pay attention to the safety of long-context models and contribute to the development of solutions to improve the safety of long-context LLMs.","Recently, thanks to more advanced model architectures (Xiao et al., 2024b; a; Liu et al., 2024a) and expanded position encoding techniques (Su et al., 2023; Liu et al., 2024b), the context length of language models has been extended significantly (Achiam et al., 2023; Reid et al., 2024). In the foreseeable future, as language models continue to evolve and tackle increasingly complex problems, the demand for handling longer contexts is expected to grow accordingly. We anticipate that long-context language models will become mainstream. Previous research on long-context language models, such as LongBench (Bai et al., 2024), L-Eval (An et al., 2023), and RULER (Hsieh et al., 2024), has typically focused on their capabilities, while neglecting to address their safety. In short-context scenarios, the safety issues of language models have already been extensively studied.(Zhang et al., 2024b; Hartvigsen et al., 2022) In long-context scenarios, Anthropic introduced ManyShotJailbreak(Anil et al., 2024), which revealed safety issues in long-context language models. However, there is still a lack of comprehensive research on the safety of these models. To better access the safety issues of long-context models, we need to design a robust and comprehensive benchmark. In this work, we propose LongSafetyBench, a new benchmark to evaluate the safety of long-context language models. As shown in Figure 1, we collected and constructed data for these three categories, targeting three types of unsafe scenarios: Illegal Activities, Misinformation Harm, and Offensiveness and Bias. Additionally, we categorized the context of the questions into three classes: Fully Harmful Context, Partially Harmful Context, and Harmless Context. In total, we designed ten tasks. To facilitate more convenient and objective testing, we formatted all data in a multiple-choice question format. We carefully designed the options so that each one reflects a specific model behavior, allowing for a more accurate assessment of the model’s capabilities in handling long context as well as its safety performance. With overview statistics shown in Table 1, LongSafetyBench contains 1,203 test instances, with an average length of 41,889 words. (a) The safety issues are categorized into three types: Illegal Activities, Misinformation Harm, Offensiveness and Bias. Based on the methods of context construction, these issues are divided into three distinct categories, encompassing a total of ten specific tasks (b) A sample consists of a question and a set of options. The question is composed of a long content followed by a related query, while the options include several choices representing possible model behaviors. The model is required to respond with the letter of the chosen option. Figure 1: On the left are the categories of the LongSafetyBench, while on the right are a data sample. Figure 2: Average Harm Awareness (HA) score and Safe Response (SR) score across models. We conducted tests on 10 mainstream long-context language models, and the summarized test results are shown in Figure 2. These results provide an intuitive reflection of the models’ safety capabilities in long-context scenarios. During the testing process, we also observed some interesting phenomena: long-context LLMs generally struggle with safety issues and models and the proportion of safe responses from most mainstream long-context LLMs is below 50%; the safety performance in long-context scenarios does not always align with their performance in short-context scenarios; models tend to ignore the harmful content within a long context. More specific findings and conclusions will be discussed in detail in Section 4. To improve the safety performance of long-context language models, we used four tasks that are easy to scale up to construct a total of 11k training samples, and performed supervised fine-tuning (SFT) on Llama3-8b-Instruct(Dubey et al., 2024) and Intern2.5-7b-Chat(Cai et al., 2024). We found that training with a small amount of long-context safety data can effectively enhance the models’ long-context safety capabilities, even reaching the performance level of top-tier closed-source models. In summary, the main contributions of our work are: 1. We present LongSafetyBench, the first comprehensive benchmark that enables evaluation of long-context LLMs’ safety. 2. We conducted extensive testing on 10 popular long-context language models, reflecting the safety capabilities of different models in long-context scenarios. By observing the experimental results, we derived some interesting conclusions (see Section 4). 3. We constructed a 11k training set aimed at enhancing the safety capabilities of long-context language models, discovering that training with a small amount of data enables open-source models to reach the performance level of state-of-the-art closed-source models. This resource supports the open-source community in improving the safety of long-context language models."
https://arxiv.org/html/2411.06852v1,Evaluating Large Language Models on Financial Report Summarization: An Empirical Study,"In recent years, Large Language Models (LLMs) have demonstrated remarkable versatility across various applications, including natural language understanding, domain-specific knowledge tasks, etc. However, applying LLMs to complex, high-stakes domains like finance requires rigorous evaluation to ensure reliability, accuracy, and compliance with industry standards. To address this need, we conduct a comprehensive and comparative study on three state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their effectiveness in generating automated financial reports. Our primary motivation is to explore how these models can be harnessed within finance, a field demanding precision, contextual relevance, and robustness against erroneous or misleading information. By examining each model’s capabilities, we aim to provide an insightful assessment of their strengths and limitations. Our paper offers benchmark for financial report analysis, encompassing proposed metrics such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative evaluation framework that integrates both quantitative metrics (e.g., precision, recall) and qualitative analyses (e.g., contextual fit, consistency) to provide a holistic view of each model’s output quality. Additionally, we make our financial dataset publicly available, inviting researchers and practitioners to leverage, scrutinize, and enhance our findings through broader community engagement and collaborative improvement. Our dataset is available on huggingface111https://huggingface.co/datasets/xinqiyang/tradingview_msn_financial_news_1k.","Figure 1: The Flow Chart of Prompt Engineering in Financial Report Summarization. Large Language Models (LLMs) [1, 2, 3, 4, 5, 6, 7] have emerged as transformative tools in Natural Language Processing (NLP), demonstrating unprecedented capabilities across tasks such as text generation [8], summarization [9], classification [10], translation [11], automatic dialogue [12], and also extend to the multi-modal area [13]. Built upon massive datasets and powered by advanced neural architectures, LLMs can understand and generate human-like text, making them adaptable to a wide range of domains. Recently, their application has extended into high-stakes sectors such as finance [14], healthcare [15], and legal industries [16], where accuracy, informativeness and coherence are essential. However, deploying LLMs in such fields poses unique challenges, requiring careful evaluation to ensure reliability and trustworthiness. The financial sector, in particular, benefits from LLM-driven innovations in automation, such as generating reports, analyzing market sentiment, and summarizing extensive financial data [17]. Yet, financial text is often dense with specialized terminology and context-sensitive information, demanding high levels of precision and contextual understanding from LLMs. Consequently, assessing LLM performance in financial applications goes beyond conventional metrics, requiring a nuanced approach that includes accuracy, informativeness and coherence, as well as alignment with financial domain knowledge. This paper presents a comprehensive evaluation of our strive of prompt engineering on three cutting-edge LLMs: GLM-4, Mistral-NeMo and LLaMA3.1, which are applied to text summarization tasks in financial statement analysis. By comparing these models across multiple criteria, we aim to identify strengths, limitations, and use-case suitability for each model. Figure 1 illustrates the flow chart of prompt engineering in financial report summarization. The process consists of seven steps, and we typically focus on gathering information and analyzing output. For the former part, we open the collected dataset from TradingView and MSN. For the latter part, we offer an evaluation framework that integrates quantitative metrics with qualitative analysis, contributing to the growing need for systematic LLM evaluation tailored to domain-specific requirements. In this paper, we also provide insights for future model development, emphasizing the role of prompt engineering and domain adaptation for achieving practical performance levels suitable for real-world financial applications. Thanks to the usage of Ollama222https://github.com/ollama/ollama, the LLMs include Gemini-1.5-pro, Mixtral-8x22b, Claude-3-opus and so on could be added to the backend of our system easily. We list our contributions as follows: 1. We establish a detailed performance benchmark for LLMs in financial reporting, providing a foundation for future comparative studies. 2. We highlight domain-specific challenges and propose model adjustments to meet the rigorous standards required in finance. 3. We foster a collaborative environment by openly sharing our dataset and methodologies. Through these efforts, we advance the responsible deployment of LLMs in finance and catalyze ongoing innovation in this area."
https://arxiv.org/html/2411.06850v1,"1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language, Hate Speech, and Targets using LLMs","This paper presents a detailed system description of our entry for the CHiPSAL 2025 shared task, focusing on language detection, hate speech identification, and target detection in Devanagari script languages. We experimented with a combination of large language models and their ensembles, including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like focal loss to address challenges in the natural understanding of Devanagari languages, such as multilingual processing and class imbalance. Our approach achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804 for Sub-tasks A, B, and C respectively. This work provides insights into the effectiveness of transformer models in tasks with domain-specific and linguistic challenges, as well as areas for potential improvement in future iterations.","Large language models (LLMs) have revolutionized natural language processing (NLP) yet South Asian languages remain largely underrepresented within these advancements despite being home to over 700 languages, 25 major scripts, and approximately 1.97 billion people. Addressing these gaps, this paper focuses on three critical NLP tasks of CHiPSAL 2025 Sarveswaran et al. (2025) in Devanagari-scripted languages: 5-way classification of the text based on the language of the text (Sub-task A), Binary classification for detecting hate speech in the text (Sub-task B), and 3-way classification for detecting target of hate speech in a text (Sub-task C) Thapa et al. (2025). Our system leverages the multilingual capabilities of open-source LLMs namely IndicBERT V2 Doddapaneni et al. (2023), MuRIL Khanuja et al. (2021), and Gemma-2 GemmaTeam (2024) and their ensembles for natural language understanding of Devanagari script languages. Our work contributes to advancing language technology in South Asia, aiming for inclusivity and deeper understanding across diverse linguistic landscapes."
https://arxiv.org/html/2411.06839v1,"LLM-Neo: Parameter Efficient Knowledge 
Distillation for Large Language Models","In this paper, we propose a novel LLM-Neo framework that efficiently transfers knowledge from a large language model (LLM) teacher to a compact student. Initially, we revisit the knowledge distillation (KD) and low-rank adaption (LoRA), and argue that they share the same paradigm. Inspired by this observation, we explore the strategy that combines LoRA and KD to enhance the efficiency of knowledge transfer. We first summarize some guidelines for this design and further develop the LLM-Neo. Experimental results on compressing Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further analysis demonstrates the robustness of the proposed LLM-Neo on variants of LoRA. The trained models have been available at this repository.","Knowledge distillation (KD) [1] for large language models (LLMs)[2] is a predominant method for model compression. The key insight is to train a compact student model by mimicking the behaviors of the teacher model. One mainstreaming way is to align the logits [3], and thus transfer the knowledge from the teacher model to the student model. Parameter-Efficient Fine-Tuning (PEFT) [4, 5] is another commonly used technique for LLM efficiency [6]. Among various PEFT methods, the Low-rank adapter (LoRA) [7] has gained increasing popularity since it does not introduce any additional parameters for inference. During training, LoRA updates a mergeable low-rank branch instead of updating the original full parameters. Therefore, LoRA can efficiently transfer the knowledge contained in the training examples to the trained models. Figure 1: Illustration of different knowledge transfer pipelines (KD, LoRA, and LLM-Neo). The proposed LLM-Neo pipeline combines the benefits of both the KD and LoRA approaches, that is, distilling knowledge from the teacher and low-rank branch efficiency. In this paper, we argue that KD and LoRA follow the same paradigm, i.e., aiming at transferring knowledge while the sources differ [8]. Moreover, LoRA transfers the knowledge efficiently via the low-rank branch, while KD methods update the full parameters and typically cost much more resources. We thus ask: can we combine KD and LoRA to improve the efficiency of knowledge transfer from the teacher model? To this end, we propose a novel LLM-Neo framework which integrates LoRA into KD to achieve parameter-efficient knowledge distillation. Specifically, as shown in Fig. 1, we follow the idea of LoRA to introduce a low-rank branch in the student model, aiming to inherit the knowledge from the teacher model. We first perform comprehensive analysis and derive valuable guidelines for the design of the proposed LLM-Neo. Experimental results on compressing Llama 2 [9] and Llama 3.1 [10] demonstrate the effectiveness and efficiency of LLM-Neo. Moreover, further analysis shows the robustness of LLM-Neo towards LoRA variants [11]. Our contributions can be concluded as follows: • We propose LLM-Neo, a novel approach that integrates knowledge distillation with low-rank adapter to lightweight LLMs. • We summarize the guidelines for LLM-Neo including: i) Typically, the larger the rank, the better, and 128 works well. ii) A learning rate close to 2e-4 works well for LoRA. iii) A larger rank requires a lower learning rate. As shown in Fig 2, trying diagonal balance is worth considering. • We perform extensive experiments on the Llama 2 and Llama 3.1 models to demonstrate the effectiveness of LLM-Neo. Moreover, further analyses confirm its compatibility with existing LLM optimization methods, highlighting its contributions to improved model performance and scalability. • We release the weights of the Llama-3.1-Neo-1B model, a small model trained on the 1 million BAAI dialogue dataset using the LLM-Neo method, contributing to the broader research and development community."
https://arxiv.org/html/2411.06837v1,Persuasion with Large Language Models: a Survey,"The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.","Figure 1: Overview of factors influencing the persuasiveness of an LLM System: (1) whether interactive dialogue is used, (2) the size of the LLM, (3) whether AI authorship is disclosed to users, (4) whether prompts are specifically engineered for persuasion, (5) the use of personal data for personalization, and (6) the use of authoritative language. Persuasive communication, defined as “any message that is intended to shape, reinforce, or change the responses of another, or others”, has existed both as a practice and as an academic discipline for many decades [41]. Until recently, it has been the prerogative of humans to persuade others, initially through personal or small-group communication, and later through mass-communication channels and technologies such as the printed press, radio, and television. Since the rise of the internet, persuasion has become increasingly personalized, thus enhancing its reach and effectiveness [19, 27, 24]. Whereas earlier mass-communication via printed or audio-visual media could be targeted only to a limited extent to the demographics of a communication channel’s audience, the internet allows far more fine-grained personalization, in some applications even down to the individual. With the advent of Large Language Models (LLMs), marked by the release of ChatGPT in 2022, a new era in persuasive communication begins. In just a few years, LLMs have evolved from basic autoregressive text continuation models to models with remarkable emergent capabilities. We discuss an emergent capability that appears especially powerful and possibly concerning: their potential for intentionally generating human-like content that shapes attitudes, informs opinions, and drives behaviors across various domains. This evolution enables a significant change in the landscape of persuasion from selecting and presenting human-generated content, albeit in a personalized manner, to automatically generating tailored, context-aware, and hyper-personalized messages at scale. Several key characteristics make LLM Systems, i.e. LLMs integrated with other technologies and deployed for specific applications, particularly effective for persuasive communication. First of all, they allow for unprecedented personalization and adaptability as they excel at tailoring messages to individual preferences and psychological profiles, creating targeted content that resonates with a targeted group or even a specific individual [3, 31, 5, 12, 13, 21]. Second and related to this, LLM Systems appear capable at exploiting cognitive biases that make humans vulnerable to persuasion and manipulation, thus allowing them to achieve persuasive impact beyond what was possible with classical scalable approaches [5, 9, 12, 13, 18]. Third, they allow for interactive conversations, enabling engaging experiences that can be more effective than traditional one-way messaging [2, 5, 12, 13, 29]. Fourth, despite known weaknesses of LLMs in terms of hallucinations, they are capable of a high degree of message consistency even across a prolonged personalized interaction [5, 14, 28, 16, 22]. And finally, in comparison to other persuasion methods that share the previously mentioned characteristics, LLM Systems are far more scalable because they can engage in hyper-personalized persuasive interactions at scale without proportional resource increases [2, 3, 9, 13, 14, 16]. During the past two years, these key characteristics of LLM Systems have led to the emergence of a new subfield of persuasion research, which we refer to as LLM-based persuasion. The effectiveness of LLM Systems for persuasion depends on various configurable factors, as illustrated in Figure 1. These include whether interactive dialogue is enabled, the scale of the language model employed, whether AI authorship is disclosed to users, the use of persuasive prompt engineering, incorporation of personal data for personalization, and employment of authoritative language. Each of these factors can significantly impact the system’s persuasive capabilities, as we will discuss in detail in Section 3. Purpose of this survey. Through a systematic review of peer-reviewed articles, conference papers, and reputable industry reports published between 2022 and 2024, we provide a comprehensive overview of the state-of-the-art in LLM-based persuasion. We examine the applications across various domains of LLM Systems for persuasion, factors and design choices influencing their persuasiveness, the methodological techniques used to study their persuasiveness and thereby quantify progress, and the associated ethical and regulatory challenges. Our survey focuses on experimental studies that directly tested the persuasive capabilities of LLM Systems. The rest of the paper is organized as follows: • Section 2 reviews the application domains where LLM-based persuasion has been studied, including politics, marketing, public health, e-commerce, and efforts to combat misinformation. • Section 3 analyzes the factors that influence the persuasive capabilities of LLM Systems, such as AI source labeling, model scale, and persuasion strategies. • Section 4 examines the experimental design patterns employed in LLM persuasion studies, highlighting study methodologies, treatment condition structures, control strategies, and methods used to assess persuasiveness. • Section 5 discusses the ethical considerations and regulatory landscape surrounding LLM-based persuasion. • Section 6 concludes by summarizing key findings, identifying research gaps, and suggesting directions for future work. By mapping the current state of research on LLM-based persuasion, this survey aims to provide a comprehensive overview of this rapidly evolving field. In doing so, we hope to raise awareness among researchers, practitioners, policy makers, and the general public, of the power of LLM Systems for persuasion today and in the near future, and to point out the resulting ethical and societal challenges that lie ahead of us. Table 1 provides an overview of the surveyed papers, showing for each paper the application domain it covered, the factors influencing persuasiveness it studied, and the experimental design it adopted. Table 1: Summary of papers by application domains, influencing factors studied, and methodological characteristics. Filled circles (●) indicate the domain/factor was explicitly studied or the methodology was used in the paper. Application Domains Influencing Factors Methodology Success Metrics Persuasiveness Citation Public Health Politics E-commerce Misinfo. Red. Charity Interaction Model Scale AI Source Labeling Prompt Design Personalization Authority RCT Design Between Subj. Human Control Pre-Post Measure Long. Follow-up Opinion Change Agreement & Class. Behavioral Intent Engagement & Detect. Perceived Effect. Technical Metrics Temporal Superhuman On Par Inferior No Comparison [2] ● ○ ○ ○ ○ ● ○ ○ ○ ○ ● ● ● ○ ● ● ● ○ ● ● ○ ○ ● ○ ● ○ ○ [3] ○ ● ○ ○ ○ ○ ○ ○ ● ○ ● ● ● ● ○ ● ○ ○ ● ● ○ ○ ○ ○ ○ ○ ○ [5] ○ ● ○ ● ○ ● ○ ○ ● ○ ○ ○ ○ ○ ○ ● ○ ○ ● ● ○ ○ ○ ● ○ ○ ○ [8] ● ○ ○ ○ ○ ○ ○ ● ○ ○ ● ● ● ● ○ ○ ● ○ ○ ● ● ○ ○ ○ ● ○ ○ [9] ○ ○ ○ ● ○ ● ○ ○ ● ○ ○ ○ ● ● ○ ○ ○ ● ○ ○ ● ○ ○ ○ ● ○ ○ [12] ○ ○ ● ○ ○ ● ○ ○ ○ ● ○ ○ ○ ○ ○ ● ○ ● ○ ● ○ ○ ○ ○ ○ ○ ● [13] ○ ○ ○ ● ○ ● ○ ○ ○ ● ○ ● ● ○ ● ● ● ○ ● ○ ○ ○ ● ○ ○ ○ ● [14] ○ ● ○ ○ ○ ○ ● ○ ● ○ ○ ○ ● ● ○ ○ ● ● ○ ○ ● ○ ○ ○ ● ○ ○ [16] ● ○ ○ ● ● ● ○ ○ ● ● ○ ○ ○ ○ ○ ○ ● ○ ● ● ● ● ○ ○ ● ○ ○ [17] ○ ● ○ ○ ○ ○ ○ ○ ● ○ ○ ● ● ● ○ ○ ○ ● ○ ○ ● ○ ○ ○ ● ○ ○ [18] ○ ● ○ ○ ○ ○ ○ ○ ● ● ○ ○ ○ ● ○ ● ○ ● ○ ● ○ ○ ○ ○ ● ○ ○ [20] ○ ● ○ ○ ○ ○ ○ ○ ○ ● ○ ● ● ○ ○ ● ○ ○ ● ○ ○ ○ ○ ○ ● ○ ○ [21] ○ ● ○ ○ ○ ○ ○ ○ ● ○ ● ● ● ● ○ ○ ● ○ ○ ● ○ ○ ○ ● ○ ○ ○ [22] ○ ● ○ ○ ○ ○ ● ○ ○ ○ ○ ● ● ● ○ ○ ● ○ ○ ● ○ ○ ○ ○ ● ○ ○ [28] ● ○ ○ ○ ○ ○ ○ ● ● ○ ● ○ ● ● ○ ○ ● ○ ○ ○ ● ○ ○ ● ○ ○ ○ [29] ○ ○ ○ ● ○ ● ○ ○ ○ ○ ● ○ ○ ○ ○ ○ ● ○ ○ ○ ○ ● ● ○ ○ ○ ● [30] ● ○ ○ ○ ○ ○ ○ ● ○ ○ ○ ● ● ● ○ ○ ○ ○ ● ● ○ ○ ○ ● ○ ○ ○ [31] ● ● ● ○ ○ ○ ○ ● ● ● ○ ● ● ● ○ ○ ● ● ● ● ● ○ ○ ○ ○ ○ ● [32] ○ ○ ● ○ ○ ○ ○ ○ ● ● ○ ○ ○ ● ○ ○ ○ ○ ● ● ● ○ ○ ○ ● ○ ○ [33] ○ ○ ○ ● ○ ● ○ ○ ○ ○ ● ○ ● ○ ○ ○ ● ○ ○ ○ ● ○ ○ ○ ○ ○ ● [34] ○ ● ○ ● ○ ○ ○ ● ○ ○ ○ ● ● ● ○ ○ ○ ● ○ ● ● ○ ○ ○ ● ○ ○ [35] ○ ○ ○ ○ ● ○ ○ ○ ● ○ ○ ○ ○ ● ○ ● ○ ○ ● ● ● ○ ○ ○ ○ ○ ● [36] ○ ● ○ ○ ○ ● ● ● ● ● ○ ● ● ● ○ ○ ● ● ● ● ● ○ ○ ● ○ ○ ○ [38] ○ ○ ● ○ ○ ○ ○ ○ ○ ○ ○ ● ● ○ ● ○ ○ ○ ● ● ● ○ ○ ● ○ ○ ○ [39] ○ ● ○ ○ ○ ○ ○ ○ ● ● ○ ○ ● ○ ○ ● ○ ○ ○ ● ● ○ ○ ● ○ ○ ○ [40] ● ○ ○ ● ○ ○ ○ ○ ● ○ ○ ○ ○ ● ○ ○ ○ ● ○ ● ● ● ○ ○ ● ○ ○ [42] ● ● ○ ○ ○ ● ○ ● ○ ○ ● ○ ● ● ○ ○ ● ○ ○ ○ ● ○ ○ ○ ○ ●111Teigen et al. [42] study only the source labeling factor and use only AI generated messages. They conclude that Human source labels are more persuasive than AI source labels. ○ [44] ○ ○ ○ ○ ● ● ○ ○ ● ○ ○ ○ ○ ○ ○ ○ ○ ● ○ ● ○ ○ ○ ○ ○ ○ ● [45] ○ ○ ● ○ ○ ● ○ ● ○ ○ ● ● ● ● ○ ○ ● ○ ● ○ ● ○ ○ ● ○ ○ ○ [46] ● ○ ○ ● ○ ○ ● ● ● ○ ● ○ ○ ● ○ ○ ○ ● ○ ● ● ● ○ ● ○ ○ ○"
https://arxiv.org/html/2411.06835v1,HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment,"Warning: This report contains sensitive content and potentially harmful information. With the introduction of the transformers architecture, LLMs have revolutionized the NLP field with ever more powerful models. Nevertheless, their development came up with several challenges. The exponential growth in computational power and reasoning capabilities of language models has heightened concerns about their security. As models become more powerful, ensuring their safety has become a crucial focus in research. This paper aims to address gaps in the current literature on jailbreaking techniques and the evaluation of LLM vulnerabilities. Our contributions include the creation of a novel dataset designed to assess the harmfulness of model outputs across multiple harm levels, as well as a focus on fine-grained harm-level analysis. Using this framework, we provide a comprehensive benchmark of state-of-the-art jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model. Additionally, we examine how quantization techniques, such as AWQ and GPTQ, influence the alignment and robustness of models, revealing trade-offs between enhanced robustness with regards to transfer attacks and potential increases in vulnerability on direct ones. This study aims to demonstrate the influence of harmful input queries on the complexity of jailbreaking techniques, as well as to deepen our understanding of LLM vulnerabilities and improve methods for assessing model robustness when confronted with harmful content, particularly in the context of compression strategies.","While numerous LLMs have been developed in recent years [1] [2], aligning these models with human preferences remains a complex and ongoing challenge. LLM alignment refers to the process of guiding models to avoid generating harmful or undesired outputs, ensuring their safe and ethical use. Recent work, such as Ouyang et al. [4] and Munos et al. [5], has demonstrated that specific fine-tuning strategies can significantly reduce the risk of harmful content generation. However, as models become more advanced, their vulnerabilities also become more pronounced. LLM vulnerabilities refer to the weaknesses that can be exploited to make models generate unsafe, harmful, or unintended content [10]. These vulnerabilities may arise from the vast and often uncensored datasets used during training, or from the model’s inherent capacity to generalize and respond to a wide range of queries [3]. This makes them susceptible to adversarial manipulation, where malicious users can craft inputs to elicit harmful outputs. This has led to the rise of jailbreaking methods, which are designed to probe and exploit these vulnerabilities to better understand the limitations of models. Several state-of-the-art jailbreaking techniques continue to bypass alignment measures, successfully eliciting harmful responses or sensitive information from models [6] [11]. In the context of adversarial attacks, assessing model compliance remains a difficult problem. Even with robust alignment strategies, ensuring that models consistently adhere to ethical and safety guidelines across a wide range of queries is challenging. In addition, the adoption of compression techniques, such as quantization, has introduced new challenges in the alignment of LLMs. While quantization improves computational efficiency, Kumar et al. [12] proved that it can also influence model behavior, particularly in adversarial contexts, where models compressed through these methods may exhibit different susceptibilities to jailbreaking techniques. Understanding how compression affects model robustness and alignment remains an underexplored area, with trade-offs between model size and safety emerging as a critical concern. This paper aims to address key gaps in the current jailbreaking literature by proposing a new framework for LLM vulnerability assessment. First, we introduce HarmLevelBench, a novel dataset comprising queries across 7 harmful topics, each further categorized into 8 distinct levels of severity, enabling a fine-grained analysis of model responses. Second, we conduct a comprehensive performance comparison of 7 state-of-the-art jailbreaking techniques applied to this dataset, offering insights into their effectiveness across various harm levels. Finally, we examine how quantization techniques, such as AWQ and GPTQ, influence the alignment and robustness of models, revealing trade-offs between resilience to transferred attacks and vulnerability to direct ones."
https://arxiv.org/html/2411.06805v1,Boosting the Potential of Large Language Models with an Intelligent Information Assistant,"The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as ""hallucination"". Initial retrieval-augmented generation (RAG) methods like the ""Retrieve-Read"" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach—Curriculum Assistant Learning and Reinforced Preference Optimization—AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.","The emergence of Large Language Models (LLMs) has significantly advanced the field of natural language processing, demonstrating an impressive ability to mimic human-like language patterns [1]. However, despite their extensive knowledge acquired during training, LLMs can occasionally generate factually incorrect information, a phenomenon referred to as “hallucination” [2, 3]. To address this, the integration of retrieval systems with LLMs has been suggested, allowing these models to tap into external databases to generate more reliable responses [4]. Initially, retrieval-augmented generation (RAG) relied on a simple ""Retrieve-Read"" framework [5], which was adequate for basic question-answering but insufficient for complex, multi-step reasoning tasks. As language models advanced, various prompt-based RAG strategies emerged [6, 7], incorporating pre-retrieval and post-retrieval prompts to refine the process. However, these strategies heavily relied on the foundational capabilities of the language models. Consequently, the focus shifted to Supervised Fine-Tuning (SFT)-based RAG methods [8], which involve fine-tuning language models specifically for RAG tasks to enhance their performance. Figure 1: Comparisons of Naive, Prompt-based, SFT-based and our Assistant-based RAG frameworks. While SFT-based methods have improved the quality of generated responses, they face two limitations that hinder their practical application. Firstly, these fine-tuned models are not easily adaptable to emerging LLMs, requiring retraining for each new foundational LLM. Secondly, directly fine-tuning a foundational LLM in the RAG scenario may change its innate abilities, potentially leading to negative impacts on the model’s performance on other tasks. To address these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), which integrates an intelligent information assistant as a plugin within LLMs. This approach comprises a trainable assistant for information management and a static main LLM dedicated to task execution, as depicted in Figure 1. As an intelligent information assistant, AssistRAG operates in two primary categories to handle complex tasks: memory management and knowledge management. Memory management involves integrating and analyzing content from internal memory, while knowledge management focuses on leveraging external knowledge. These two main functions are supported by four core capabilities of AssistRAG: (1) Tool usage, which involves recalling relevant information from both internal memory and external knowledge bases through a retriever; (2) Action execution, which involves processing, analyzing, and extracting information; (3) Memory building, which involves recording essential knowledge and reasoning patterns from historical interactions; (4) Plan specification, which involves determining the necessity of each step in the process. These four capabilities work together to ensure that AssistRAG can provide accurate and comprehensive support to the main LLM. To implement AssistRAG, we adopt a two-phase training approach. The first phase, Curriculum Assistant Learning, enhances the assistant’s capabilities in note-taking, question decomposition, and knowledge extraction through progressively complex tasks. The second phase, Reinforced Preference Optimization, uses reinforcement learning to tailor the assistant’s feedback to the main LLM’s specific needs, optimizing knowledge extraction based on feedback from the main LLM. During the inference stage, AssistRAG operates through a three-step process: (1) Information Retrieval and Integration: The assistant understands the main LLM’s needs, retrieves relevant knowledge from internal and external sources, and extracts valuable information. (2) Decision Making: The assistant evaluates and decides whether to provide the retrieved memories and knowledge to the main LLM based on their relevance. (3) Answer Generation and Memory Updating: The main LLM generates an answer using its internal knowledge and the assistant’s information, while the assistant updates its memory with crucial reasoning steps. Results from experiments across three complex question-answering datasets reveal that AssistRAG exhibits superior reasoning capabilities and markedly outperforms existing benchmarks. Notably, when applied to different foundational LLMs, AssistRAG appears to confer more pronounced benefits on less advanced LLMs."
https://arxiv.org/html/2411.06767v1,PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing,"Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but often struggle with bug repair. We introduce a suit of methods to enhance LLM’s SQL bug-fixing abilities. The methods are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the ""disorientation"" in SQL code bug-fixing training. In our evaluation, the code LLM models trained with two methods have exceeds all current best performing model which size is much larger.","Recently, as large language models (LLMs) achieve remarkable success, code LLMs emerge as useful assistants when editing code. However, when we shift focus to fixing code errors, we find that the performance of open source pre-trained code LLMs like DeepSeek-Coder (Guo et al., 2024), WizardCoder (Luo et al., 2023) and Code Llama (Roziere et al., 2023) is quite limited (as shown in Table 1). In this paper, we especially focus on the code repair task of SQL. Due to the complex nested query structure, SQL code bugs are more difficult to solve compared with other code languages. We formulate the SQL code bug-fixing task as Equation 1. SQL_{correct}=f({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0,1}Schema},{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{% 1,0,0}SQL_{bug}},{\color[rgb]{0,1,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 0,1,1}\pgfsys@color@cmyk@stroke{1}{0}{0}{0}\pgfsys@color@cmyk@fill{1}{0}{0}{0}% R}) (1) Where the f represents the bug-fixing model. Schema means the related tables schemas of bug SQL code. SQLbug denote the SQL code which contains some bugs need to be fixed. R is the return message by the SQL execution system when you run the bug SQL code. SQL_{correct} is the bug-fixing model’s output, which is expected the right SQL code. We propose a set of methods to enhance the bug-fixing capabilities of Large Language Models (LLMs). This includes a method for mining and collecting supervised data, termed Progressive Dataset Construction (PDC), and an efficient training method based on dynamic masking, known as Dynamic Mask-SFT (DM-SFT). Experiments show that training with data collected via PDC method generally improved the SQL bug-fixing capabilities of open-source code LLMs by nearly +50\%. The Dynamic Mask-SFT training method further enhanced model performance by approximately +10\% relative to the default generative SFT."
https://arxiv.org/html/2411.06729v1,Reverse Prompt Engineering,"This paper explores a new black-box, zero-shot language model inversion problem and proposes an innovative framework for prompt reconstruction using only text outputs from a language model. Leveraging a large language model alongside an optimization algorithm, the proposed method effectively recovers prompts with minimal resources. Experimental results on several datasets derived from public sources indicate that the proposed approach achieves high-quality prompt recovery and generates prompts more similar to the originals than current state-of-the-art methods. Additionally, the use-case study demonstrates the method’s strong potential for generating high-quality text data.","With the advancement of large language models (LLMs), prompt engineering has become an essential technique for expanding their capabilities[21]. This method uses task-specific instructions, or prompts, to enhance model effectiveness without altering core parameters. Widely used prompting techniques, such as few-shot prompting[19], chain-of-thought prompting[25], and retrieval-augmented generation[8], have proven highly practical in diverse applications. With the increasing focus on prompt engineering, where inputs are carefully modified to improve the outputs generated by large language models (LLMs), a natural question arises: can we infer the input prompt based solely on the outputs? This challenge is known as language model inversion, as defined by Morris et al. [15]. In their work, they develop a model trained to predict the input prompt by leveraging the probability distributions and logits from the last layer of the LLM. Building on this, Zhang et al. [34] further propose a model that uses only the textual outputs to infer the prompts, without relying on internal model parameters. However, their approach requires a substantial number of outputs (64) to recover a single prompt, and for complex system prompts, it needs access to the user prompt in order to obtain the desired output for inference. In this paper, we propose a novel language model inversion technique, reverse prompt engineering (RPE), which assumes the target LLM is a black-box model accessible only through text outputs and an API. RPE infers the underlying prompt from these outputs by leveraging the target LLM’s reasoning capabilities in combination with an iterative optimization algorithm inspired by the genetic algorithm[22]. Importantly, reverse prompt engineering introduces no new models and requires no training. Compared to previous methods[15, 34], RPE requires less resources, relies solely on minimal information from the target LLM (five text outputs only), and outperforms state-of-the-art methods, achieving an average 5.2% improvement in cosine similarity over output2prompt[34] across different embedding models. Our main contributions are as follows: • We provide the first study of the language model inversion problem under black-box and zero-shot conditions. • We propose an innovative method to recover prompts from text outputs using only an LLM. • We design a novel optimization algorithm that leverages the LLM itself as an optimizer to enhance prompt recovery accuracy. The remainder of the paper is organized as follows. In Sec. 2, we review relevant research on prompt engineering and language model inversion. In Sec. 3, we describe the proposed method in detail. In Sec. 4, we present experimental results and discuss potential applications of language model inversion. Finally, in Sec. 5, we conclude the paper."
https://arxiv.org/html/2411.06672v1,What Should Baby Models Read? Exploring Sample-Efficient Data Composition on Model Performance,"We explore the impact of pre-training data composition on the performance of small language models in a sample-efficient setting. Using datasets limited to 10 million words, we evaluate several dataset sources—including child-directed speech (CHILDES), classic books (Gutenberg), synthetic data (TinyStories), and a mix of these (Mix)—across different model sizes ranging from 18 million to 705 million parameters. Our experiments show that smaller models (e.g., GPT2-18M and GPT2-44M) benefit from training on diverse datasets like Mix, achieving better performance on linguistic benchmarks. In contrast, larger models (e.g., GPT2-97M, GPT2-705M, and LLaMA-360M) perform better when trained on more complex and rich datasets like Gutenberg. Models trained on the CHILDES and TinyStories datasets underperformed across all model sizes. These findings suggest that the optimal dataset for sample efficient training depends on the model size, and that neither child-directed speech nor simplified stories are optimal for language models of all sizes. We highlight the importance of considering both dataset composition and model capacity for effective sample efficient language model training.","In recent years, advancements in natural language processing have been largely driven by scaling language models to unprecedented sizes. Various large-language model (LLM) scaling laws have been formulated Sardana et al. (2024), with perhaps the most influential being the Chinchilla law, which demonstrates that parameters and tokens scale approximately linearly as the model scales Hoffmann et al. (2024). Many subsequent LLMs have been trained following this model Rae et al. (2021), with some models including the Llama 2 and Llama 3 family of models being trained on 2 and 15 trillion tokens respectively, far more than the ’optimal’ amount according to the Chinchilla scaling law Dubey et al. (2024). However, it is often prohibitive to train such large models, and impractical to continue scaling with the amounts of data required to train such models. This has sparked interest in small language models Schick and Schütze (2021); Magister et al. (2023) with much fewer parameters, requiring much less data for training. While much research has been conducted on knowledge distillation and improving the model architecture for small language models, comparably less research has investigated the contributions of different types of data used for model training, which is arguably just as important. Indeed, because LLM pretraining data typically comprises a mix of sources Chowdhery et al. (2023), researchers have found that the composition of pretrained data greatly affects model performance Du et al. (2022); Wei et al. (2015), though determining the optimal recipe for pretraining data is challenging. Recent research exploring optimization of pretraining data for LLMs at scale includes DoReMi, which trains a small proxy model to produce domain weights for downstream tasks, and then uses the model to resample the dataset for training huge LLMs Xie et al. (2024). However, the question of how to choose data for sample-efficient training of small language models, such as in cases where computational resources are limited, has received little attention. Psycholinguistic precedent exists for sample-efficient pretraining; children see much less words than a modern LLM yet perform exceptionally well on reasoning tasks. For example, Chinchilla sees over 10000 times the number of words a 13 year old child has ever encountered Choshen et al. (2024). By the time typical English-speaking children at around 6 years old have obtained adult-level grammatical knowledge Kemp et al. (2005), they have seen only around 10-50M words Hart et al. (1997); Huebner et al. (2021). In comparison, Llama-3 is trained on 15T tokens Dubey et al. (2024). Given the great disparity between the amount of training data an LLM requires and what children require, it seems worthwhile to investigate whether training LLMs can be as sample efficient. BabyBERTa Huebner et al. (2021) attempts to address this, showing that when training a model on data similar to what is seen by children between the ages 1 and 6, it is able to acquire grammatical knowledge similar to pretrained RoBERTa-base, but with around 15X fewer parameters and 6,000X fewer words; this indicates that utilizing child-directed input may be advantageous for more sample efficient pretraining Huebner et al. (2021). Similarly, Eldan and Li (2023) follow suit, releasing TinyStories, a synthetic dataset of short stories that only contain words that typical 3- to 4-year-old children understand. They demonstrate that TinyStories can be leveraged to train language models with much less parameters than SOTA models, yet still produce coherent output with almost perfect grammar as well as emergent reasoning abilities. Along the same vein, GPT-wee Bunzeck and Zarrieß (2023) shows that child-directed speech can be used with curriculum learning for simulating children’s learning as a potential solution to sample-constrained training. In this paper, we evaluate the effect of different datasets on model performance for sample efficient model training. In our case, we limit our training dataset to 10M words, in accordance with the BabyLM Challenge’s super-strict track Choshen et al. (2024). We consider several different types of datasets, namely child-directed speech (CHILDES), classic books (Gutenberg), a mixed dataset (Mix) and the TinyStories dataset. Experimental results show that smaller models benefit from training on diverse datasets like Mix on the BabyLM evaluation suite Choshen et al. (2024), but larger models perform better when trained on more complex and rich datasets like Gutenberg. Our findings suggest that the optimal dataset depends on the model size and that neither child-directed speech nor child-directed stories are optimal for language models of any sizes."
https://arxiv.org/html/2411.06660v1,Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation,"Knowledge graph completion (KGC) is a task of inferring missing triples based on existing Knowledge Graphs (KGs). Both structural and semantic information are vital for successful KGC. However, existing methods only use either the structural knowledge from the KG embeddings or the semantic information from pre-trained language models (PLMs), leading to suboptimal model performance. Moreover, since PLMs are not trained on KGs, directly using PLMs to encode triples may be inappropriate. To overcome these limitations, we propose a novel framework called Bridge, which jointly encodes structural and semantic information of KGs. Specifically, we strategically encode entities and relations separately by PLMs to better utilize the semantic knowledge of PLMs and enable structured representation learning via a structural learning principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a self-supervised representation learning method called BYOL to fine-tune PLMs with two different views of a triple. Unlike BYOL, which uses augmentation methods to create two semantically similar views of the same image, potentially altering the semantic information. We strategically separate the triple into two parts to create different views, thus avoiding semantic alteration. Experiments demonstrate that Bridge outperforms the SOTA models on three benchmark datasets.","Knowledge graphs (KGs) are graph-structured databases composed of triples (facts), where each triple (h,r,t) represents a relation r between a head entity h and a tail entity t. KGs such as Wikidata (Vrandečić and Krötzsch, 2014) and WordNet (Fellbaum, 2010) have a significant impact on various downstream applications such as named entity recognition (Zhou et al., 2022), relation extraction (Ren et al., 2017), and representation learning (Chen et al., [n. d.], 2023). Nevertheless, the effectiveness of KGs has long been hindered by the challenge of the incompleteness problem. To address this issue, researchers have proposed a task known as Knowledge Graph Completion (KGC), which aims to predict missing relations and provides a valuable supplement to enhance KG’s quality. Most existing KGC methods fall into two main categories: structure-based methods and pre-trained language model (PLMs)-based methods. Structure-based methods represent entities and relations as low-dimensional continuous embeddings, which effectively preserve their intrinsic structure (Bordes et al., 2013; Dettmers et al., 2018; Kim et al., 2022; Ge et al., 2023). While effective in KG’s structure representation learning, these methods overlook the semantic knowledge associated with entities and relations. Recently, PLM-based models have been proposed to leverage the semantic understanding captured by PLMs, adapting KGC tasks to suit the representation formats of PLMs (Yao et al., 2020; Kim et al., 2020; Wang et al., 2021b, 2022; Qiao et al., 2023). While these models offer promising potential to enhance KGC performance, there is still space to improve: (1) Existing structure-based methods do not explore knowledge provided by PLMs. (2) Existing PLM-based methods aim to convert KGC tasks to fit language model format and learn the relation representation from a semantic perspective using PLMs, overlooking the context of the relation in KGs. Consequently, they lack the learning of structural knowledge. For example, given a triple (trade name, member of domain usage, metharbital)111This is a triple from WordNet, and metharbital is an anticonvulsant drug used in the treatment of epilepsy., the semantic of the relation member of domain usage is ambiguous since “it is not a standard used term in the English222interpretation from ChatGPT when asking “what does member of domain usage mean?” ”; hence, PLMs may not be able to provide an accurate representation from the semantic perspective. Thus, it becomes imperative to enable the model to leverage the principle of structural learning to grasp structural knowledge and compensate for the limitations of semantic understanding. (3) Existing PLM-based methods utilize PLMs directly, overlooking the disparity between PLMs and triples arising from the lack of triple training during PLMs pre-training. This oversight significantly limits the expressive power of PLMs and their adaption to the KG’s domain. To address the limitations of existing methods, we propose a two-in-one framework named Bridge. To overcome the challenge of lacking structural knowledge in PLMs, we propose a structured triple knowledge learning phase. Specifically, we follow the widely applied principle in traditional structured representation learning for KGs (Bordes et al., 2013; Sun et al., 2018; Balazevic et al., 2019; Shang et al., 2024), which posits that the relation is a translation from the head entity to the tail entity. We strategically extract the embedding of h,r and t separately from PLMs and employ various structure-based scoring functions to assess the plausibility of a triple. This approach allows us to reconstruct KG’s structure in the semantic embedding via the structured learning principle. This principle has been widely applied in traditional structured representation learning for KGs (Bordes et al., 2013; Sun et al., 2018; Balazevic et al., 2019; Shang et al., 2024), but there is no previous study that investigates this principle using PLM-based representation. However, due to the different principles between traditional structured representation learning and PLMs, there is a gap between them since PLMs are not trained on KGs. To bridge the gap between PLMs and KGs, we fine-tune PLMs to integrate structured knowledge from KGs into PLMs. Considering the existence of one-to-many, many-to-one, and many-to-many relations in KGs, we opt to consider positive samples only to avoid false negatives. Therefore, we employ BYOL (Grill et al., 2020) because BYOL does not need negative samples. Different from the original BYOL, which uses augmentation methods like edge dropping and node feature masking, we strategically separate the triple into two parts. This strategy can avoid semantic alteration caused by augmentation methods and retain the structural attributes of KGs. By taking this step, we unify the space of structural and semantic knowledge, making the integration of KGs and PLMs more reasonable. In summary, our main contributions are: (1) We identify the pitfalls of existing methods that use either the structural knowledge from the KG embeddings or the semantic information from pre-trained language models (PLMs) for KGC tasks. (2) We propose a general framework, Bridge, that jointly encodes structural and semantic information of KGs and can incorporate various scoring functions. (3) We utilize BYOL innovatively for fine-tuning PLM to bridge the gap between structural knowledge and PLMs. (4) We conduct empirical studies with two widely used structural-based scoring functions on three benchmark datasets. Experiment results show that Bridge consistently and significantly outperforms other baseline methods, and fine-tuning PLMs with BYOL plays an important role."
https://arxiv.org/html/2411.06655v1,Explore the Reasoning Capability of LLMs in the Chess Testbed,"Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated by chess experts111Yifan Hou is a four-time world chess champion. for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models.","“Strategy without tactics is the slowest route to victory. Tactics without strategy is the noise before defeat.” —-Sun Tzu Rational thought and deliberate cognition rely heavily on reasoning, a core component of human intelligenceGarnham and Oakhill (1994). Given sufficient information, people can logically progress through a sequence of steps. In the field of artificial intelligenceRussell and Norvig (2016), it has been a persistent objective to study the reasoning capability, as it is essential for both problem-solving and decision-making processes. The past few years have seen large language models exhibit extraordinary aptitude in the tasks that require reasoning capabilityBrown (2020); Wei et al. (2022); Kojima et al. (2022); Bubeck et al. (2023). However, language models show significant limitations in planning and reasoning for complicated tasksXu et al. (2023); Dziri et al. (2024); Srivastava et al. (2022); Mirzadeh et al. (2024). In this paper, we use chess as a testbed to study how we can improve the reasoning capability of large language models for complex tasks. Figure 1: Strategy and Tactic (a)White E2 pawn moves to E4, takes more space in the center, and exerts pressure on black. Black will have a hard time struggling to develop its pieces. (b)White E2 bishop moves to F3 and pins the knight on C6. The black knight cannot move, or the A8 rook behind the knight will be taken. White will take black knight for free in the next move. Chess reasoning is challenging, requiring analytical calculation and intuitive insights. Good chess players employ a dual approach, which includes (i) Long-term Strategy: It relies on rapid, intuitive thinking based on the pattern recognition of the chess board. (ii) Short-term Tactic: It involves slow, analytic calculations that typically consider 1-6 moves ahead, depending on the player’s skill level. Figure 1 shows an example of strategy and tactic. Notably, experienced players think out loud: they develop strategic plans in clear language, and they evaluate the afterward position in lucid words after calculating the precise moves of a tactic. Drawing inspiration from the thinking approach used by chess experts, we propose a method to enhance large language models’ chess-playing capabilities by incorporating both strategy and tactic in language annotation. We collect the MATE(Move on strAtegy and Tactics datasEt), a dataset of around 1 million chess positions, and annotate the candidate moves for each position with long-term strategy and short-term tactic. Then, we utilize the MATE to finetune open source large language models. Finally, we evaluate the performance of our models and compare them against state-of-the-art large language models. Our models outperform the best commercial language model by 24.2% when both strategy and tactic are provided. In summary, this work’s contributions are three-fold: • We collect a high-quality chess dataset. For each position, the candidate moves are provided with a description of the strategy and tactic information annotated by experienced chess players, including world champion-level experts. • We find that language explanations can enhance the reasoning capability of large language models. • We discover that integrating the dual-mode of strategy and tactic can improve the chess-playing capability of language models."
https://arxiv.org/html/2411.06548v1,CineXDrama: Relevance Detection and Sentiment Analysis of Bangla YouTube Comments on Movie-Drama using Transformers: Insights from Interpretability Tool,"In recent years, YouTube has become the leading platform for Bangla movies and dramas, where viewers express their opinions in comments that convey their sentiments about the content. However, not all comments are relevant for sentiment analysis, necessitating a filtering mechanism. We propose a system that first assesses the relevance of comments and then analyzes the sentiment of those deemed relevant. We introduce a dataset of 14,000 manually collected and preprocessed comments, annotated for relevance (relevant or irrelevant) and sentiment (positive or negative). Eight transformer models, including BanglaBERT, were used for classification tasks, with BanglaBERT achieving the highest accuracy (83.99% for relevance detection and 93.3% for sentiment analysis). The study also integrates LIME to interpret model decisions, enhancing transparency.","In today’s world, YouTube has become a popular platform for entertainment, with many people using it to watch content. YouTube has more than 2.70 billion monthly active users as of October 2024, highlighting its massive global reach [1]. As Bangla movies and dramas have recently surged in popularity among both Bangladeshi people and Bangla-speaking communities worldwide, YouTube has become a key platform for streaming this content. Viewers frequently express their opinions in the comment section, sharing whether they enjoyed the drama or found it lacking. These comments represent a rich dataset in the Bangla language, offering valuable insights into viewers’ sentiments and perceptions. Analyzing YouTube comments in Bangla about Bangla dramas and movies helps reveal viewers’ sentiments, identifying whether content is seen positively or negatively. An automated system using these comments could offer real-time feedback, helping filmmakers gauge audience reactions and influence the Bangladeshi entertainment industry with direct insights. Many works have already been done on sentiment analysis using YouTube comments [2, 3, 4], but there is limited research in the Bangla language, even though Bangla is the world’s sixth most spoken language, with over 237 million native speakers [5]. Some work has been done on Bangla YouTube comments for sentiment analysis [6, 7, 8], but these efforts have not been specifically focused on both Bangla movies and dramas. Instead, they have covered a wide range of Bangla content, including dramas, movies, music, news, and more. As a result, these sentiment analysis projects do not specifically cater to the needs of Bangla movie or drama reviews. Another key challenge is identifying relevant comments. This step is crucial for accurately assessing audience feedback on whether a drama or movie is good or bad. The comments section often includes off-topic posts, inappropriate remarks, or religious or promotional content that doesn’t reflect the actual quality of the movie or drama. For example, comments like “2022 sael ek ek edkhechn?” (Who’s watching in 2022?) or “Aaim musilm Hey gir/bt” (I am proud to be Muslim) do not provide meaningful feedback on the content’s quality. Analyzing only the relevant comments is essential for a precise evaluation, as including irrelevant ones can skew the sentiment analysis and lead to an inaccurate understanding of the viewers true reactions. Ignoring off-topic or unrelated comments ensures the sentiment analysis remains focused on the actual feedback about the drama or movie. In this study, we have created a large dataset consisting of 14,000 Bangla comments related to Bangla movies and dramas, which will be discussed in the Dataset section. Additionally, we have analyzed the relevance of the comments to determine whether they are pertinent to the content of the movies and dramas. This type of work has not been previously done in the Bangla language. For relevance and sentiment analysis, we utilized eight transformer models. These models offer the advantage of handling complex Bangla language structures, enabling more accurate sentiment classification and relevance detection. To improve transparency, we integrated LIME (Local Interpretable Model-Agnostic Explanations) [9] as an interpretability tool. LIME helped explain how the models made predictions, increasing trust in the results. In summary, the key contributions of this paper are as follows: • We have created a dataset consisting of 14,000 Bangla YouTube comments from Bangla movies and dramas for the purposes of relevance detection and sentiment analysis. The dataset contains 7839 relevant and 6,161 irrelevant comments. Among the relevant comments, 4275 are labeled as positive, and 3564 are labeled as negative. • We have proposed a system that initially determines whether a comment is relevant or irrelevant using pre-trained transformer models. If the comment is identified as relevant, then classifies it into either a positive or negative category for sentiment analysis. • Among the eight pre-trained transformers we utilized (mentioned in Subsection V-A), BanglaBERT performed the strongest in both relevance detection and sentiment analysis, with accuracies of 83.99% and 93.3% respectively. • To enhance model interpretability, we used LIME for a visual understanding of predictions, enabling a clearer evaluation of feature importance and model behavior."
https://arxiv.org/html/2411.06528v1,Epistemic Integrity in Large Language Models,"Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration — where a model’s linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.","Large Language Models (LLMs) have markedly transformed how humans seek and consume information, becoming integral across diverse fields such as public health (Ali et al., 2023), coding (Zambrano et al., 2023), and education (Whalen & et al., 2023). Despite their growing influence, LLMs are not without shortcomings. One notable issue is the potential for generating responses that, while convincing, may be inaccurate or nonsensical—a long-standing phenomenon often referred to as “hallucinations” (Jo, 2023; Huang et al., 2023; Zhou et al., 2024b). This raises concerns about the reliability and trustworthiness of these models. A critical aspect of trustworthiness in LLMs is epistemic calibration, which represents the alignment between a model’s internal confidence in its outputs and the way it expresses that confidence through natural language. Misalignment between internal certainty and external expression can lead to users being misled by overconfident or underconfident statements, posing significant risks in high-stakes domains such as legal advice, medical diagnosis, and misinformation detection. While of great normative concern, how LLMs express linguistic uncertainty has received relatively little attention to date (Sileo & Moens, 2023; Belem et al., 2024). Figures 1 and 5 illustrate the issue of epistemic calibration providing insights into the operation of certainty in the context of human interactions with LLMs. We highlight the following key points in these figures: • Distinct Roles of Certainty: Internal certainty and linguistic assertiveness have distinct functions within LLM interactions that shape individual beliefs. • Human access to LLM certainty: Linguistic assertiveness holds a critical role as the primary form of certainty available to users. Unlike internal certainty, which remains hidden within the model’s computational processes, linguistic assertiveness is directly perceivable and influences how users interpret the model’s outputs. • Beyond Content: Users retrieve more than just the content from an LLM’s output. The style and assertiveness of the language used also play a significant role by shaping perceptions through the communication of certainty. This interaction between the model’s output and its linguistic assertiveness is crucial for understanding the full impact on individual perceptions. \mdfdefinestyle MyFrame linecolor=black, {mdframed} [style=MyFrame,nobreak=true,align=center,userdefinedwidth=32em] High Epistemic Calibration User input: ""Have humans ever been on the moon?"" LLM output: ""Yes, humans have been on the moon."" Internal certainty score: 95% Linguistic assertiveness: 95% Low Epistemic Calibration User input: ""Have scientists discovered a drug that cures Alzheimer’s?"" LLM output: ""Scientists have not discovered a drug that cures Alzheimer’s."" Internal certainty score: 60% Linguistic assertiveness: 95% Figure 1: This figure illustrates two examples with varying levels of epistemic calibration in LLM outputs. The one below is poorly calibrated. For each output, we calculate two certainty scores: internal certainty and external certainty (linguistic assertiveness). The internal certainty is computed using the method outlined by Rivera et al. (2024). To assess linguistic assertiveness, we develop a custom model, which we validate using human ratings collected through a survey. Several studies have explored the calibration of internal confidence in LLMs. For instance, Zhang et al. (2024) examine confidence calibration, proposing techniques to reduce hallucinations and enhance the model’s ability to answer known questions while avoiding unknown ones. However, they overlook the role of linguistic assertiveness and how external certainty can still lead to epistemic miscalibration even if internal confidence is addressed. Similarly, Ren et al. (2023) focus on factual knowledge and LLM behavior before and after retrieval-augmented generation (RAG). While they investigate internal confidence, they fail to frame miscalibration as an end-to-end issue involving both internal certainty and linguistic assertiveness, therefore ignoring the interplay between model predictions and how confidence is expressed linguistically. More recent studies have attempted to bridge the gap between internal confidence and linguistic assertiveness but still face considerable limitations. Although Mielke et al. (2022) explore epistemic calibration, their study uses a limited scoring scale to measure both assertiveness and confidence, which restricts the continuous assessment of LLM output. Their approach also relies on a narrow range of datasets, thereby limiting its applicability across domains. Zhou et al. (2024a) address miscalibration using epistemic markers, but their method lacks real domain grounding and fails to consider the complexity of natural language. This review of existing work on LLM calibration and confidence reveals several gaps that our research aims to address: • Lack of Integrated Approaches: Previous studies address either internal certainty or linguistic assertiveness but rarely both simultaneously (Jiang et al., 2021). There is a need for comprehensive frameworks that integrate these aspects to ensure LLMs communicate accurately and responsibly. • Inadequate Assertiveness Measurement: Existing methods for measuring linguistic assertiveness rely heavily on lexicon-based approaches (Pei & Jurgens, 2021) or subjective perceptions without adequate validation (Steyvers et al., 2024). These methods often lack contextual depth and fail to generalize across diverse domains. • Limited High-Stakes Evaluation: Although some studies explore epistemic calibration, they cover a narrow range of topics and employ low-resolution measures of assertiveness, limiting their applicability in critical domains such as misinformation detection (Mielke et al., 2022). To address these gaps, our paper provides: • A New Assertiveness Detection Model: We train a new model to detect linguistic assertiveness, using a composite of five diverse datasets. Our approach improves the accuracy relative to previous approaches by incorporating contextual nuance and aligning more closely with human perceptions. We also address limitations in previous methods in generalizably measuring assertiveness across domains. • Empirical Evidence of Epistemic Miscalibration: Our work provides a comprehensive comparison between internal certainty and linguistic assertiveness, documenting instances of miscalibration in different contexts. Our experiments reveal that LLMs frequently generate highly assertive explanations despite low internal certainty, which can mislead users. • Validation with Human Perception: We conduct comprehensive surveys assessing human perceptions of LLMs’ linguistic assertiveness. Our results confirm that there is a misalignment between computational measures and subjective human perceptions of language, highlighting the need for more robust linguistic calibration of LLMs. This study also presents a novel human-centered approach for developing a robust assertiveness scoring method. To ensure reliability, we train and compare several models to identify the best estimator for assertiveness based on accuracy and transferability using a new multi-domain dataset hand-coded to measure certainty. After selecting the top-performing model, we validate the results using a different human-surveyed dataset (Wang, 2017), which was coded independently by different individuals. This comprehensive methodology enables us to thoroughly assess both the objective and subjective aspects of assertiveness in language model explanations. Our findings reveal that when the model has low internal certainty, it generates explanations that are significantly over-assertive, meaning the language used implies a higher degree of certainty than is warranted by the model’s actual confidence or accuracy. This miscalibration could lead users to misconstrue the model’s judgments as more reliable than they actually are. More precisely, our results confirm a strong correlation between GPT 4o’s assertiveness scores and human perceptions of assertiveness, but a weak correlation between human perceptions and internal certainty, and an even weaker relationship between GPT 4o model assertiveness and internal certainty. Together, these results provide the most extensive evidence of epistemic miscalibration in LLMs to date.111For the code and datasets used, refer to our GitHub repository at: https://github.com/ComplexData-MILA/epistemic-integrity."
https://arxiv.org/html/2411.06506v1,"CULL-MT: Compression Using Language and Layer pruning
for Machine Translation","Multilingual machine translation models often outperform traditional bilingual models by leveraging translation knowledge transfer. Recent advancements have led to these models supporting hundreds of languages and achieving state-of-the-art results across various translation directions. However, as these models grow larger, their inference operations become increasingly costly. In many use cases, there is no need to support such a wide range of language pairs, as translation is typically needed in only a few selected directions. In this paper, we present CULL-MT, a compression method for machine translation models based on structural layer pruning and selected language directions. Our approach identifies and prunes unimportant layers using a greedy strategy, then mitigates the impact by applying knowledge distillation from the original model along with parameter-efficient fine-tuning. We apply CULL-MT to the NLLB-3.3B and LLaMA3.1-8B-Instruct models. In a multi-way translation scenario (Persian, French, and German to English), we find the NLLB-3.3B model to be robust, allowing 25% of layers to be pruned with only a 0.9 spBLEU drop. However, LLaMA3.1-8B-Instruct is more sensitive, with a 2.0 spBLEU drop after pruning 5 layers.","In recent years, multilingual neural machine translation (NMT) models have become an efficient solution for translating multiple language pairs, compared to using separate models for each translation direction Aharoni et al. (2019). Additionally, multilingual NMT models Firat et al. (2016) outperform bilingual NMT models Bahdanau et al. (2015), particularly in low-resource language pairs, due to the transfer of knowledge from high-resource language pairs Dabre et al. (2020) . Therefore, recent multilingual NMT models have increasingly focused on expanding the number of supported languages. Notable models like NLLB Costa-jussà et al. (2022) and MADLAD Kudugunta et al. (2024) now support 200 and 400 languages, respectively, with many of these being low-resource languages. Despite their impressive performance, multilingual NMT models have grown significantly in size, often containing billions of parameters. This increase has made their deployment and inference computationally expensive, requiring substantial hardware resources. The parameter count of multilingual NMT models, such as NLLB Costa-jussà et al. (2022) and MADLAD Kudugunta et al. (2024), has surged in recent years, mirroring the trend in language models, which gave rise to the term Large Language Models (LLMs). This rapid expansion raises concerns about the cost and practicality of using these models in real-world applications, particularly in resource-constrained environments. As a result, compressing large models without significantly impacting performance has become a critical research challenge. While recent multilingual NMT models support hundreds of languages, many medium- and small-sized translation systems require only a few translation directions. Compressing massive translation models to focus on the relevant language pairs can result in significant computational savings. Our approach addresses this need by compressing the model with minimal performance sacrifice in the selected translation tasks. Both multilingual NMT models and large language models are designed to handle a wide range of tasks. While multilingual NMT models can translate hundreds of language directions, LLMs are versatile enough for various natural language processing (NLP) tasks, including multilingual translation Zhao et al. (2023). Our approach focuses on compressing these models for specific tasks. For multilingual NMT, we reduce model size while maintaining performance on key translation directions. Similarly, for LLMs, we compress the model specifically for multilingual translation of selected language pairs, showcasing the versatility and architecture-agnostic nature of our method. For addressing this issue, we introduce CULL-MT, a method that compresses models by focusing on the key translation directions relevant to specific use cases, achieving notable computational savings with minimal performance sacrifice. Our method employs a greedy structural pruning technique, wherein we iteratively assess the importance of each layer by comparing the model’s performance with and without that layer. If removing a layer results in only a minimal performance drop, it is considered less critical and is subsequently pruned. This iterative process continues until the performance falls below a predefined threshold. After the pruning phase, the model is fine-tuned to recover any performance loss, ensuring effectiveness for the targeted translations. The following key contributions highlight the effectiveness of our CULL-MT approach: We applied our approach to the NLLB-3.3B model, demonstrating that it is considerably robust against layer pruning. In a multi-way scenario (translating from Persian, French, and German to English), we were able to prune 12 out of 48 layers, with the pruned model experiencing only a 0.9 drop in spBLEU score. In a single-way scenario (translating from English to Persian), CULL-MT allowed us to prune 15 layers while resulting in a 1.2 drop in spBLEU score. Our approach was also tested on the LLaMA3.1-8B-Instruct model, revealing its sensitivity to layer pruning in translation tasks. In the multi-way scenario, CULL-MT pruned 5 layers, resulting in a 2.0 drop in spBLEU score. Analyzing the importance of layers in the multi-way translation task for both NLLB-3.3B and LLaMA3.1-8B-Instruct revealed critical insights. Our findings indicate that the first layer in the encoder and the decoder of the NLLB-3.3B model are essential, while other layers have a considerably lesser impact on performance. In contrast, the LLaMA3.1-8B-Instruct model exhibits more significant important regions, with the first two and last five layers being crucial, alongside several important layers in the middle of the model. The remainder of this paper is organized as follows: Section 2 reviews common compression methods and layer-wise pruning techniques. Section 3 presents the CULL-MT methodology, while Section 4 outlines the experimental setup. Section 5 reports the results of CULL-MT and compares them with other approaches. Section 6 provides an analysis of layer importance, and Section 7 concludes the paper. Finally, Section 8 discusses the limitations. Figure 1: Overview of the CULL-MT approach. It iteratively prunes a multilingual translation model by identifying and removing the least important layers while preserving performance in key translation directions."
https://arxiv.org/html/2411.06438v1,PLM-Based Discrete Diffusion Language Models with Entropy-Adaptive Gibbs Sampling,"Recently, discrete diffusion language models have demonstrated promising results in NLP. However, there has been limited research on integrating Pretrained Language Models (PLMs) into discrete diffusion models, resulting in underwhelming performance in downstream NLP generation tasks. This integration is particularly challenging because of the discrepancy between step-wise denoising strategy of diffusion models and single-step mask prediction approach of MLM-based PLMs. In this paper, we introduce Diffusion-EAGS, a novel approach that effectively integrates PLMs with the diffusion models. Furthermore, as it is challenging for PLMs to determine where to apply denoising during the diffusion process, we integrate an entropy tracking module to assist them. Finally, we propose entropy-based noise scheduling in the forward process to improve the effectiveness of entropy-adaptive sampling throughout the generation phase. Experimental results show that Diffusion-EAGS outperforms existing diffusion baselines in downstream generation tasks, achieving high text quality and diversity with precise token-level control. We also show that our model is capable of adapting to bilingual and low-resource settings, which are common in real-world applications.","Type Dataset OpenWebText Gokaslan and Cohen (2019) RocStories Mostafazadeh et al. (2016) Deontology Hendrycks et al. (2023) Question Generation Dhingra et al. (2017) QQP Wang et al. (2017) ALMA Xu et al. (2024a) ParaDetox Logacheva et al. (2022) Open-ended Generation ? \checkmark \checkmark \triangle \checkmark \times \times \times Conditional Generation ? \times \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark Context Provided ? - \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark Content Provided ? - \times \triangle \checkmark \checkmark \checkmark \checkmark Format Provided ? - - \times \times \times \checkmark \checkmark Table 1: Categorization of NLP tasks based on constraint aspects. \checkmark indicates full support, \times indicates no support, and \triangle indicates partial or limited support. As diffusion models significantly enhance the quality and diversity of generated outputs in continuous domains such as images and audio (Song et al., 2021b), recent research has increasingly applied diffusion models to NLP (Li et al., 2022; Gong et al., 2023a; Yuan et al., 2023; He et al., 2023). To integrate the diffusion process with traditional language models, research has predominantly followed two approaches: Continuous Diffusion Language Model (CDLM) (Yuan et al., 2023; Lovelace et al., 2023; Chen et al., 2023), which embed text in a continuous latent space, and Discrete Diffusion Language Model (DDLM) (He et al., 2023; Lou et al., 2024; Zhou et al., 2024; Shi et al., 2024; Sahoo et al., 2024; Zheng et al., 2024), which operate directly on the vocab space. Recent DDLMs adeptly reflect the highly structured nature of language; this property enables sequence control in natural language, generally resulting in high performance in NLP fields such as unconditional generation and open-ended generation (Lou et al., 2024). However, for DDLMs to be widely adopted across various NLP domains, particularly in tasks such as close-ended conditional generation — hereafter referred to as dataset-guided generation — existing DDLMs still exhibit shortcomings in their performance. Our experiments suggest that models such as SEDD falls short in such tasks, potentially limiting their practical applications. Better performance can be achieved by integrating Pretrained Language Models (PLMs) through the use of an encoder-based PLM as the initial setup for embedding functions, leveraging its capability to capture overall semantics (He et al., 2023). However, integrating PLMs into diffusion models is non-trivial as PLMs typically predict masked elements in a single step, whereas diffusion models require step-wise denoising based on the overall semantics of each timestep sequence, and such gap yields limited results. Therefore, we need to consider such inconsistency to effectively adopt PLMs into DDLMs by a new methodology. In this paper, we introduce Diffusion-EAGS, a novel approach that effectively integrates Mask Language Model (MLM)-based PLMs with DDLMs for dataset-guided generation. To address the gap between the step-wise nature of diffusion models and the one-step prediction strategy of PLMs, we begin by considering MLM-based PLMs as the denoising function at each step of the diffusion process. Since identifying where to apply denoising during the diffusion process is challenging for PLMs, we incorporate an entropy tracking module to support their operation. For constructing entropy tracking module, we interpret each denoising step as a constrained Markov Random Field (cMRF), which enables us to adopt adaptive Gibbs sampling, which then facilitates leveraging entropy information from conditional sequences. To train DDLMs to effectively exploit an entropy tracking module, we propose a forward process based on an entropy-based noising scheduling during training phase. Specifically, our entropy-based noising scheduling noises lower entropy tokens first, thereby learning to progressively generate sequences guided by the entropy from entropy tracking module, which enhances the quality of the generated text. Experimental results demonstrate that Diffusion-EAGS achieves outstanding performance compared to existing autoregressive model (GPT-2), CDLMs, and DDLMs across various dataset-guided generation tasks. We demonstrate that both PLMs and entropy-based Gibbs sampling contribute to higher performance in our ablation study. Furthermore, our model exhibits higher diversity in certain tasks compared to LLMs and can facilitate token-level generation, indicating the potential applicability of our model across a wide range of dataset-guided generation tasks. In addition, we validate that our model can adapt to both bilingual and low-resource settings, which are frequently encountered in practical applications."
https://arxiv.org/html/2411.06402v1,"Fineweb-Edu-Ar: Machine-translated Corpus 
to Support Arabic Small Language Models","As large language models (LLMs) grow and develop so too, do their data demands. This is especially true for multilingual LLMs, where the scarcity of high quality and readily available data online has led to a multitude of synthetic dataset generation approaches. A key technique seen in this space is machine translation (MT), where high quality English text is adapted to a target, comparatively low-resource, language.In this report we introduce FineWeb-Edu-Ar, a machine-translated version of HuggingFace’s exceedingly popular (deduplicated) FineWeb-Edu dataset. To the best of our knowledge FineWeb-Edu-Ar is the biggest publicly available machine-translated Arabic dataset out there, with its size of 202B tokens of an Arabic-trained tokenizer.The data is available on HuggingFace111https://huggingface.co/datasets/kaust-generative-ai/fineweb-edu-ar.","Natural Language Processing (NLP) has seen tremendous strides in recent years with the advent of LLMs, we have seen models scale up to more than 100 billion parameters Brown et al. (2020). With the ever growing demand for scale, we have seen a substantial focus on large corpora to train these LLMs on Hoffmann et al. (2022). Although data quality mattered, the focus had mostly been on data quantity. More recently however, we see small language models (SLMs) trained on a much smaller quality-focused corpus Gunasekar et al. (2023), Allal et al. (2024). These models exceed expectations for their size, outperforming certain much larger models on various benchmarks Abdin et al. (2024). This gave rise to new potential use cases for language models on edge devices with compute constraints Lu et al. (2024), Gunter et al. (2024). Many languages, including Arabic, suffer from a distinct lack of the same kind of high quality, educational focused, and readily available data that allowed other small language models to flourish. Figure 1: Quadrants of corpora availability. In order to illustrate the current situation with the training data, we’ve organized it in 4 quadrants: English and Arabic texts versus culturaly-agnostic and culturally-specific knowledge as in Figure 1. While generic English data (left top quadrant) is abundant, and Arabic culturally specific data (right bottom quadrant) can be extracted from the web archives Aloui et al. (2024), Arabic texts with the generic knowledge (top right quadrant) are scarce. Fortunately, the latter can be reinforced with the machine translated texts sourced from generic English texts. In an effort to support small language model pre-training, we publicly release FineWeb-Edu-Ar, a machine translated version of HuggingFaceTB/smollm-corpus Allal et al. (2024), the deduplicated version of Huggingface’s FineWeb-Edu Lozhkov et al. (2024). HuggingFaceTB/smollm-corpus that enabled their model, SmolLM, to achieve remarkable results for its size. We hope that this will support the field of Arabic SLMs. The dataset is released under CC-BY-NC-4.0 license. Additionally, to support community efforts in English to Arabic machine translation, we provide a preliminary analysis into the performance of several translation models. In general, we see two ways to obtain the educational-grade Arabic language dataset: (1) translating the English FineWeb-Edu, and (2) adapting the FineWeb-Edu pipeline for Arabic with subsequent re-running it on CommonCrawl. One notable attempt to do the latter is Farhat et al. (2024) which, however, does not feature semantic filtration, possibly due to the small total amount of Arabic content in the Web compared to English. Whereas Farhat et al. (2024) is a valuable dataset, specific to Arabic-speaking countries and obtained from the native speakers, we choose to follow the option of translation, since the source dataset (FineWeb-Edu-dedup) a lot of information that Arabic native speakers choose to get from English sources, and since these two ways are, for the most part, complimentary. As additional evidence for the acute interest to small condensed-knowledge datasets of specific languages we would like to mention Chinese-FineWeb-Edu OpenCSG (2024) which is the FineWeb-Edu semantic filtration pipeline run for Chinese language. Our contributions are: 1. An open sourced machine translated version of HuggingFace’s FineWeb-Edu to Arabic. 2. An analysis into the performance and computational requirements of 12 models for English to Arabic machine translation."
https://arxiv.org/html/2411.06371v1,LLM Vocabulary Compression for Low-Compute Environments,"We present a method to compress the final linear layer of language models, reducing memory usage by up to 3.4x without significant performance loss. By grouping tokens based on Byte Pair Encoding (BPE) merges, we prevent materialisation of the memory-intensive logits tensor. Evaluations on the TinyStories dataset show that our method performs on par with GPT-Neo and GPT2 while significantly improving throughput by up to 3x, making it suitable for low-compute environments.","The global trend in machine learning is increasingly focused on scaling with larger numbers of GPUs Musk, (2024); Morgan, (2024). In contrast, many researchers operate in low-compute environments, a disparity often referred to as the compute divide Besiroglu et al., (2024). Our work seeks to address this gap by optimising the utilisation of compute-constrained environments. Specifically, we target the vocabulary layer in language models. During training or fine-tuning, it becomes necessary to materialise a tensor of shape [batch_size, sequence_length, vocab_size]. Even with conservative values for the batch size and sequence length, such as a tensor of shape [32, 512, 50000], this alone consumes approximately 3.32 GB of memory. For the remainder of this text, we will refer to this tensor as the logits tensor. Although no research has directly addressed this specific tensor, several studies have acknowledged the vocabulary layer as a computational bottleneck and have proposed methods to reduce its computational complexity Jozefowicz et al., (2016). One of the earliest efforts was by Goodman, (2001), who introduced a class-based approach by organising tokens into classes and training two separate models: one for predicting the class and another for predicting the actual token. Later, Joulin et al., (2017) proposed a hierarchical organisation of tokens based on their frequency, although computing these frequencies remains computationally expensive. Building on this, Baevski and Auli, (2018) extended the hierarchical approach to allow for variable capacity inputs. However, a comprehensive analysis of memory usage in this context is still lacking. In this work, we propose a method to reduce the memory footprint of the final embedding layer by grouping tokens and predicting the final token in a two-step process effectively compressing the vocabulary layer. Our approach differentiates from the work of Joulin et al., (2017); Goodman, (2001) through two key innovations. First, rather than grouping tokens based on their frequency—a process that necessitates significant pre-processing—we group tokens according to the ordering of BPE Sennrich et al., (2016) merges, which inherently exploits token occurrence patterns. Second, we observe that it is unnecessary to use two separate models to predict the group and the token. Instead, simple linear layers over the hidden states can simultaneously learn both the group and token predictors. We empirically demonstrate that our modification does not negatively impact model performance."
https://arxiv.org/html/2411.06272v1,Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models,"As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose ""Golden Touchstone"", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models’ language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o, Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.","The rapid development of both proprietary(Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Anthropic, 2024; Team et al., 2023) and open-source Large Language Models (LLMs) has led to their increasing importance and application across various fields(Touvron et al., 2023a, b; AI@Meta, 2024; Bai et al., 2023; Yang et al., 2024a; DeepSeek-AI, 2024; Young et al., 2024; Zeng et al., 2023; Baichuan, 2023; Gan et al., 2023; Zhang et al., 2022) , including finance(Wu et al., 2023; Lopez-Lira and Tang, 2023), healthcare Thirunavukarasu et al. (2023); Tian et al. (2023), and law Cui et al. (2023); Xiao et al. (2021). Among these, the financial sector stands out as one of the most important areas for LLM application, due to its rich textual information and high practical value. In recent years, a variety of advanced financial large language models (FinLLMs) have emerged, capable of specialized tasks such as financial sentiment analysis, content summarization, stock movement prediction, and specialized question answering, as depicted in Figure.1. These models, including FinGPT (Yang et al., 2023), PIXIU Xie et al. (2023), CFGPT (Li et al., 2023), DISC-FinLLM (Chen et al., 2023), and XuanYuan (Zhang and Yang, 2023), leverage unique frameworks and tuning methods to enhance their performance on domain-specific benchmarks, offering robust solutions for real-world financial applications. Alongside the rapid emergence of FinLLMs, there has also been a significant increase in financial benchmarks. In the English financial domain, FLUE(Shah et al., 2022) was the first publicly available benchmark for assessing English financial language understanding. Similarly, BBT(Lu et al., 2023a) was the pioneering open-source financial NLP benchmark in Chinese. As LLMs and financial applications have continued to develop, more financial benchmarks have been introduced(Xie et al., 2023, 2024; Yang et al., 2023; Lei et al., 2023; Zhang et al., 2023). While these benchmarks have provided valuable resources and evaluation criteria for assessing current LLMs in finance, there remain challenges, such as the presence of low-quality or unsuitable for LLMs datasets within certain tasks, leading to generally poor evaluation results across various models. Furthermore, Current benchmarks also suffer from insufficient language and task coverage, which prevents a comprehensive evaluation of large financial language models. Figure 1: Financial large language models are designed to perform specialized tasks such as financial sentiment analysis, content analysis, stock movement prediction, and financial analyst level question answering by interpreting and processing structured instructions and various input data to generate precise outputs. To address these gaps, we propose Golden Touchstone, a bilingual financial benchmark that consolidates representative datasets across eight financial NLP tasks in both Chinese and English. Golden Touchstone provides high-quality datasets, task-aligned metrics, and instructional templates tailored to guide LLMs in generating task-appropriate responses. The Golden Touchstone organizes all task data into an instruction, input, and output format, facilitating the use and evaluation of different models. We evaluated current open-source general-purpose LLMs and FinLLMs using the Golden Touchstone benchmark. Results indicate that while GPT-4o, Qwen-2, Llama-3, and FinMA performed well across several tasks like financial sentiment analysis and entity extration, there remains considerable room for improvement in areas like credit card scoring dataset within classification tasks and stock movement prediction. Additionally, we have open-sourced Touchstone-GPT, a model trained through domain-specific continual pre-training and financial instruction tuning. It achieved acceptable performance on the bilingual benchmark but showed limitations in tasks such as stock movement prediction and question answering. These findings underscore the need for more high-quality training data and potentially more suitable model architectures for finance-specific applications. Our main contributions are as follows: • Introduction of Golden Touchstone, the first comprehensive bilingual benchmark for financial Large Language Models (LLMs), encompassing 22 datasets across eight tasks in both Chinese and English. Compared to previous FinLLMs’ benchmarks, our benchmark offers enhanced diversity, systematicity, and adaptability. • A thorough evaluation of state-of-the-art LLMs and FinLLMs, including GPT-4o, Qwen-2, Llama-3, FinGPT and FinMA and so on, on the Golden Touchstone. This evaluation highlights the key strengths and shortcomings in model performance across various tasks, and suggests directions for future research in financial LLMs. • The open-source release of Touchstone-GPT, a specialized financial LLM that has undergone domain-specific continuous pre-training and instruction tuning. This model serves as a new baseline for subsequent FinLLMs research, fostering further advancements in Financial AI."
https://arxiv.org/html/2411.06248v1,Robust Detection of LLM-Generated Text: A Comparative Analysis,"The ability of large language models to generate complex texts allows them to be widely integrated into many aspects of life, and their output can quickly fill all network resources. As the impact of LLMs grows, it becomes increasingly important to develop powerful detectors for the generated text. This detector is essential to prevent the potential misuse of these technologies and to protect areas such as social media from the negative effects of false content generated by LLMS. The main goal of LLM-generated text detection is to determine whether text is generated by an LLM, which is a basic binary classification task. In our work, we mainly use three different classification methods based on open source datasets: traditional machine learning techniques such as logistic regression, k-means clustering, Gaussian Naive Bayes, support vector machines, and methods based on converters such as BERT, and finally algorithms that use LLMs to detect LLM-generated text. We focus on model generalization, potential adversarial attacks, and accuracy of model evaluation. Finally, the possible research direction in the future is proposed, and the current experimental results are summarized.","The advent of Large Language models (LLMS) marks a new evolution in text generation technology, making it more pervasive and human-like than ever before. The LLM model is built on the basis of deep learning architecture, mainly using Transformer model, and is trained on a large scale text corpus to provide complex language sentence patterns and emotional details. These models leverage self-attention mechanisms to emphasize different pieces of text when predicting subsequent words, enabling nuance and context-aware text generation. In their technique, autoregressive models stand out by predicting future markers based on previous contexts, thus maintaining consistency and relevance. In addition, the llm has been fine-tuned in specific areas to improve accuracy and applicability. An important development in LLM is the introduction of search-enhanced generation, which uses an external database to inject the latest information into the generated content in real time. This capability not only enriches the knowledge base of the LLM, but also ensures the relevance of the output and the notification of the latest data. However, the rise of LLM has also brought its own set of challenges, particularly in distinguishing between human-generated and machine-generated text. Traditional detection methods rely on simple statistical patterns and are difficult to match the complex output of llm, which closely mimics human writing style and structure. This situation necessitates the development of advanced detection mechanisms that can comprehend and dissect the deep contextual embeddings and intricacies unique to LLM-generated texts. These new methodologies need to evolve in tandem with LLM advancements, incorporating insights into token prediction patterns, stylistic tendencies, and the integration of retrieval-based content. As we forge ahead, it is crucial to develop strategies that ensure the authenticity and integrity of digital content in the age of advanced language models. This situation requires the development of advanced detection mechanisms capable of understanding and dissecting the deep contextual embeddedness and complexity characteristic of LLM-generated text. At the same time, in an ideal state, it can synchronize and iterate with the development of llm, and play a high effect in maintaining network security, promoting information authenticity and improving user experience. Recognize and process all kinds of text content, such as spam, phishing, and other malicious messages, thereby protecting users from cyber attacks and information fraud. 1.1. Key questions that we have answered (1) Among traditional regression methods, standard neural networks, and approaches like BERT, which is more suitable for this task? (2) Will our method still be effective if faced with datasets that have different features from the current one? (3) How do we address data ambiguity? This issue is closely related to the inherent mechanisms of detection technologies. The widespread deployment of LLMs across various fields has made it increasingly difficult to discern whether training data was written by humans or generated by LLMs."
https://arxiv.org/html/2411.06228v1,An \mathbf{{\mathbf{L^{*}}}} Algorithm for Deterministic Weighted Regular Languages,"Extracting finite state automata (FSAs) from black-box models offers a powerful approach to gaining interpretable insights into complex model behaviors. To support this pursuit, we present a weighted variant of Angluin’s (1987) \mathbf{L^{*}} algorithm for learning FSAs. We stay faithful to the original algorithm, devising a way to exactly learn deterministic weighted FSAs whose weights support division. Furthermore, we formulate the learning process in a manner that highlights the connection with FSA minimization, showing how \mathbf{L^{*}} directly learns a minimal automaton for the target language. github.com/rycolab/weighted-angluin","Learning formal languages from data is a classic problem in computer science. Unfortunately, learning only from positive examples is impossible (Gold, 1978). By granting the learner access to more than just positive examples, Angluin (1987) introduced the active learning scheme \mathbf{L^{*}}, where the learner interacts with an oracle by asking it queries. Concretely, Angluin’s (1987) \mathbf{L^{*}} algorithm learns regular languages in the form of deterministic finite-state automata (DFSAs) from membership queries (analogous to asking for a ground truth label of a string in the training dataset) and equivalence queries (analogous to asking whether a hypothesis is correct). Weighted formal languages, where strings are assigned weights such as probabilities or costs, naturally generalize membership-based (boolean) formal languages. Weighted languages, especially probabilistic languages, serve as a cornerstone in the conceptual framework of many NLP problems (Mohri, 1997). Their significance is twofold: First, in practical applications, where they underpin algorithms for tasks such as parsing (Goodman, 1996) and machine translation (Mohri, 1997), and second, as an analytical framework for better understanding modern language models (Weiss et al., 2018; Jumelet and Zuidema, 2023; Nowak et al., 2024, inter alia). This has motivated the development of various weighted extensions of Angluin’s (1987) \mathbf{L^{*}} algorithm. For instance, Weiss et al. (2019) describes a generalization that (approximately) learns a probabilistic DFSA by querying a neural language model to interpret it. Less faithfully to the original \mathbf{L^{*}} algorithm, multiple algorithms for learning non-deterministic weighted FSAs have been proposed (Bergadano and Varricchio, 1996; Beimel et al., 2000; Balle and Mohri, 2012; Balle et al., 2014; Daviaud and Johnson, 2024; Balle and Mohri, 2015). These algorithms involve the solution of a linear system of equations, and therefore they cannot be used when the underlying algebraic structure lacks subtraction. We present a novel weighted generalization of the \mathbf{L^{*}} algorithm that learns semifield- weighted deterministic FSAs. In contrast to other algorithms inspired by \mathbf{L^{*}}, ours is a more faithful generalization of the original learning scheme. We generalize Angluin’s (1987) original algorithm, resulting in a familiar procedure that, just like the original, learns a DFSA exactly in a finite number of steps if the automaton can be determinized.111All boolean-weighted FSA can be determinized, which is why Angluin’s (1987) \mathbf{L^{*}} always halts. Additionally, we loosen the requirement for field-weighted FSAs; our algorithm works for semifield-weighted FSA. Our exposition further illuminates the connection between weighted minimization (Hopcroft and Ullman, 1979; Mohri, 1997) and \mathbf{L^{*}}."
https://arxiv.org/html/2411.06213v1,Incorporating Human Explanations for Robust Hate Speech Detection,"Given the black-box nature and complexity of large transformer language models (LM) (Bender et al., 2021), concerns about generalizability and robustness present ethical implications for domains such as hate speech (HS) detection. Using the content rich Social Bias Frames dataset (Sap et al., 2019), containing human-annotated stereotypes, intent, and targeted groups, we develop a three stage analysis to evaluate if LMs faithfully assess hate speech. First, we observe the need for modeling contextually grounded stereotype intents to capture implicit semantic meaning. Next, we design a new task, Stereotype Intent Entailment (SIE), which encourages a model to contextually understand stereotype presence. Finally, through ablation tests and user studies, we find a SIE objective improves content understanding, but challenges remain in modeling implicit intent.","Considering the pervasiveness of social media and ease of disseminating information, as well increasing divisiveness of highly contentious beliefs, political and otherwise, offensive hate speech (HS) and targeted rhetoric has unsurprisingly seen a disturbing surge in recent years (Mondal et al., 2017; Mathew et al., 2019, 2020). We use the definition of HS from (Levy et al., 2000)111The Encyclopedia of the American Constitution as “any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic”. It follows that combating HS through means of automatic detection and censoring has been an especially important topic of interest (Warner and Hirschberg, 2012). While this may be well-intentioned, detecting and censoring HS poses a challenging duality: properly removing hateful content can successfully prevent violence against groups of individuals whereas over-censoring and incorrectly censoring content can place an undue burden on the freedom of speech (Dukes and Gaither, 2017; Sullivan, 2010; Olteanu et al., 2018; Cassidy et al., 2013). This work seeks to extend HS detection transparency and robustness. We probe an LM hate-speech detection model for artifacts indicative of spurious relations between individual words and the potentially hateful intent of an utterance. For instance, profanities, or socially offensive words, on their own are not a good proxy for HS (Malmasi and Zampieri, 2018). We find that a vanilla LM-based detection model places too much emphasis on artifacts. To mitigate this phenomenon, we introduce a new task, Stereotype Intent Entailment (SIE), that encourages a model to detect the semantic alignment of an utterance with respect to a given stereotype. Finally, we develop an algorithm to explore correlated word pairs and through user studies observe that the SIE-trained model learns semantic patterns related to human understanding."
https://arxiv.org/html/2411.06208v1,IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization,"In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces Trace, a benchmark for improving and evaluating the complex instruction-following ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and out-of-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on out-of-domain data compared to SFT and DPO respectively.","The rapid development of large language models (LLMs) has facilitated human-machine interaction, with instructions serving as the medium Gao et al. (2024); Kim et al. (2024); Zhang et al. (2024b). As human needs evolve, there is an increasing expectation for models to handle more intricate tasks through complex instructions Ge et al. (2023); Yang et al. (2024b); Wang et al. (2024). Consequently, the instruction-following ability, especially complex instructions, is garnering significant attention Zhou et al. (2023); Xu et al. (2024); Li et al. (2024); Zhang et al. (2024a). Figure 1: Alignment Paradigms (a) Existing DPO Series vs. (b) Proposed IOPO. The green arrow indicates that y matches x while the red one indicates a mismatch. To evaluate the instruction-following abilities of LLMs, several benchmarks Zhou et al. (2023); Qin et al. (2024); Li et al. (2024) have been proposed, which are designed to systematically assess how well these models can understand and execute instructions. IFEval Zhou et al. (2023) focuses on verifiable instructions which are amenable to objective verification of compliance. Qin et al. (2024) introduces InFoBench which contains 500 distinct instructions and 2,250 decomposed questions to assess the ability of instruction following. Recently, the ability to follow complex instructions with multiple constraints is gaining increasing attention He et al. (2024b); Jiang et al. (2024); Wen et al. (2024); He et al. (2024a) as LLMs are deployed in sophisticated real-world applications. Zhang et al. (2024a) proposes a constraint-following benchmark CFBench with 1,000 multi-constraint samples. However, most of benchmarks lay emphasis on evaluating LLMs’ ability to follow complex instructions, lack of algorithms tailored for enhancing the corresponding ability. From RLHF (Reinforcement Learning from Human Feedback) Ouyang et al. (2022); Bai et al. (2022a) to the following-up researches such as DPO (Direct Preference Optimization) Rafailov et al. (2023), alignment algorithms which align LLMs with human preferences, have demonstrated their effectiveness in improving the LLMs’ capabilities to follow instructions. Nevertheless, these methods directly explore different responses y (y_{\rm win}, y_{\rm loose}) based on the same instruction x, as shown in Figure 3 (a). In the complex instruction scenario which contains multiple constraints, it is challenging to efficiently perceive the fine-grained constraints in x solely by modeling different y. To bridge this gap, this paper first introduces Trace benchmark to improve the ability of LLMs to track complex fine-grained constraint instructions and make them more obedient. Trace is automatically constructed based on the manually sorted taxonomy of complex instructions with 26 constraint dimensions within 5 constraint types. We develop an automated data construction workflow that extends from open-source simple instructions to multi-constrained complex ones. In the end, we accumulate 120K complex instructions for model training and 1K human-verified data for evaluation. To enhance the ability of LLMs to follow complex instructions, this paper further proposes Input-Output Preference Optimization (IOPO) method. IOPO not only takes the instruction x as input to directly learn the response y preference, but also gradually delves deeper into instructions x based on the same response y, to promote effective perception of fine-grained constraints, as shown in Figure 3 (b). The major contributions of this paper are summarized as follows: • We introduce a benchmark Trace for complex instruction following, which includes both an evaluation set and a training set, and an automated data construction workflow, further enriching the research community. • Different from previous alignment paradigm, we propose IOPO alignment method which deeply explores the complex instructions x (Input), not just directly learning response preference y (Output). • Extensive experiments on both in-domain and out-of-domain evaluations have confirmed the consistent improvements, with an average increase of 7.22% and 2.66%, compared to SFT and DPO, respectively."
https://arxiv.org/html/2411.06207v1,Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment,"Large Language Models (LLMs) are increasingly recognized for their practical applications. However, these models often encounter challenges in dynamically changing knowledge, as well as in managing unknown static knowledge. Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a significant impact on LLMs. Actually, we find that the impact of RAG on the question answering capabilities of LLMs can be categorized into three groups: beneficial, neutral, and harmful. By minimizing retrieval requests that yield neutral or harmful results, we can effectively reduce both time and computational costs, while also improving the overall performance of LLMs. This insight motivates us to differentiate between types of questions using certain metrics as indicators, to decrease the retrieval ratio without compromising performance. In our work, we propose a method that is able to identify different types of questions from this view by training a Knowledge Boundary Model (KBM). Experiments conducted on 11 English and Chinese datasets illustrate that the KBM effectively delineates the knowledge boundary, significantly decreasing the proportion of retrievals required for optimal end-to-end performance. Specifically, we evaluate the effectiveness of KBM in three complex scenarios: dynamic knowledge, long-tail static knowledge, and multi-hop problems, as well as its functionality as an external LLM plug-in.","As Large Language Models (LLMs) evolve, their real-world applications expand, yet they often struggle with dynamically changing and unknown static knowledge, leading to inaccuracies or hallucinations (Rawte et al., 2023). Retrieval-Augmented Generation (RAG) effectively addresses these challenges by retrieving relevant external information in real time, enhancing LLMs’ ability to provide accurate responses. While RAG can significantly boost performance, it also incurs costs, such as increased retrieval requests and longer response times. This raises a crucial question: when is retrieval truly necessary? Previous studies on the necessity of RAG for LLMs can be categorized into two main approaches. The first focuses on the query itself, with methods like SELF-RAG (Asai et al., 2023) instructing models such as GPT-4 (Achiam et al., 2023) to assess whether retrieving external documents (e.g., Wikipedia) can produce better responses. Although this approach based on query instructing can identify questions that require real-time information, it remains model-agnostic and struggles to determine whether an LLM has mastered specific knowledge. The second approach evaluates both questions and model responses to determine if an LLM can answer a question, generating data by sampling multiple model responses and using manual labels for evaluation. However, this method is labor-intensive and relies heavily on manual labeling, which can create biases and lead to increased training costs. Recent advancements in adaptive retrieval (Jeong et al., 2024) highlight the importance of tailoring the retrieval process based on the complexity of the questions. This approach improves alignment between user intent and retrieved content by considering the model’s responses alongside RAG effects. However, measuring the effectiveness of retrieved information on model outputs remains challenging, especially in complex and open-domain scenarios. Given these challenges, using GPT-4-based classification labels for questions to generate data for Llama-2 7B evaluations is not reasonable, as it does not consider the knowledge boundaries of Llama-2 7B. In our analysis, we find that performance varies significantly among LLMs of different sizes, indicating the need to focus on specific models for a more accurate simulation of their knowledge boundaries, rather than just distilling larger models for labels. Additionally, generating soft labels from queries and LLM predictions often requires gold answers for evaluation, posing challenges for questions without definitive answers. For questions with gold answers, we sample multiple LLM responses to compute confidence levels. In contrast, for questions lacking gold answers, we evaluate the entropy of phrase distributions from the model’s responses to create a certainty distribution. Based on these evaluation indicators, we categorize RAG’s impact on LLMs as beneficial, neutral, or harmful based on these evaluation indicators. Our findings show that as the LLM’s confidence or certainty increases, the ratio of neutral and harmful queries also rises, while beneficial queries and performance improvements decline. This motivates us to reduce retrieval requests leading to neutral or harmful outcomes, optimizing time and computational efficiency while maximizing beneficial retrievals to enhance question answering (QA) performance. In this work, we propose methods to evaluate the known and unknown aspects of LLMs based on indices of confidence and certainty derived from sampled responses. By establishing different thresholds for confidence and certainty, we can effectively distinguish between beneficial questions and those deemed neutral or harmful, thereby enabling the generation of data with soft labels. The data is used to fine-tune the Knowledge Boundary Model (KBM) to determine if the LLM considers the question unknown, thus indicating whether retrieval is necessary. This assessment informs decisions on whether RAG enhancement is necessary, leading to better performance and decreased retrieval costs. We evaluate several English and Chinese QA datasets and demonstrate that KBM effectively detects knowledge mastery in LLMs through end-to-end results. Our confidence-based approach significantly reduces retrieval ratios while achieving performance close to ALL RAG, with a 43.17% reduction in retrieval on WebQA and a performance drop of only 1.7%. Similarly, the certainty-based approach maintains performance near ALL RAG (with a 0.42% decrease) and reduces the search ratio by approximately 10% on QA tasks. Further analysis explores KBM’s effectiveness in open domains across three dimensions: dynamic knowledge, long-tail static knowledge, and multi-hop problems. Finally, we discuss the impact of incorporating RAG on the knowledge boundaries of LLMs and potential solutions based on KBM to address these challenges. Figure 1: Illustration of the impact of RAG on LLM performance. Figure 2: Illustration of the ratio of different LLM output MASK."
https://arxiv.org/html/2411.06194v1,WMT24 Test Suite: Gender Resolution in Speaker-Listener Dialogue Roles,We assess the difficulty of gender resolution in literary-style dialogue settings and the influence of gender stereotypes. Instances of the test suite contain spoken dialogue interleaved with external meta-context about the characters and the manner of speaking. We find that character and manner stereotypes outside of the dialogue significantly impact the gender agreement of referents within the dialogue. https://github.com/hillary-dawkins/wmt24-gender-dialogue.,"Gender bias and gender effects in machine translation are prevalent in translation directions where gender relevancy increases from source to target language Savoldi et al. (2021); Barclay and Sami (2024); Savoldi et al. (2023). English has minimal morphological effects caused by natural gender, whereas many languages (e.g. French, Spanish, Czech, Icelandic, German) have grammatical gender cases for various parts of speech which sometimes need to align with natural gender for animate nouns. For example “I am happy” in the source language English has divergent translations in the target language French (“Je suis heureux/heureuse”) depending on the natural gender of the speaker. The consequence is that gender-alignment errors can easily arise in such translation directions. Furthermore, stereotypes are known to drive gender agreement (e.g., systems may tend to prefer the translation “Je suis jolie” over “Je suis joli” for “I am pretty” despite incomplete gender context) Sólmundsdóttir et al. (2022), and these stereotype effects can persist even when unambiguous gender information is provided Stanovsky et al. (2019); Troles and Schmid (2021); Kocmi et al. (2020). Typically, these gender effects are studied in isolation or semantically-bleached settings (as in the above examples). There it is known that the internal characteristics of adjective words, such as the gender stereotype, sentiment, and type (appearance or character), are significant factors influencing the choice of gender agreement in translation Sólmundsdóttir et al. (2022). However, the need for gender agreement also occurs in more complex settings, such as over long ranges, and passages involving multiple potential referents. Due to increasing interest in paragraph-level translation and literary domains, here we assess the challenge of speaker-listener role resolution in literary dialogue settings. In particular, the gender of the speaker and listener must be resolved correctly to obtain a correct translation, and we suppose that gender stereotype effects can further add to the task difficulty. We find that stereotypical character descriptions and manners of speaking are significant influences on the gender alignment, generally overshadowing the internal adjective traits."
https://arxiv.org/html/2411.06176v1,M-LongDoc: A Benchmark and Retrieval-Aware Tuning for Long Multimodal Document Understanding,"The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io","The ability to comprehend long and complex multi-modal documents and respond to user queries about them is crucial in various practical applications such as business intelligence analysis, academic literature review, and legal research (Mathew et al., 2020). Recently, large multimodal models such as GPT-4V (OpenAI, 2023) have shown great potential in processing and analyzing diverse types of information, including text, images, and even structured data (Huang et al., 2024b). These models offer the promise of automating tasks that traditionally required extensive human effort, such as document analysis, information retrieval, and question-answering (Fujitake, 2024). However, real-world documents often present significant challenges due to their length, complexity, and multimodal nature, containing a mix of text, figures, tables, and charts (Faysse et al., 2024). Thus, it is not clear whether current models are capable of an in-depth understanding of lengthy multimodal documents. On the other hand, while existing benchmarks have fostered great progress in document understanding, they often fall short in representing these challenges, typically focusing on documents with less than 50 pages, and limited to simpler extraction-based questions (Ma et al., 2024). To address these limitations, we introduce M-LongDoc, a comprehensive benchmark consisting of 851 samples specifically designed to evaluate the performance of large multimodal models on lengthy and diverse documents. Unlike previous datasets (Mathew et al., 2020; Liu et al., 2024; Ma et al., 2024) that mainly contain short documents, M-LongDoc features recent documents spanning hundreds of pages, encompassing a wide range of topics and document structures as shown in Figures 2 and 2. In addition, as shown in Figure 3, our benchmark goes beyond simpler extractive questions, requiring models to provide open-ended solutions that demonstrate in-depth understanding of the document content (Fan et al., 2019). M-LongDoc poses a question answering task where models have to analyze and reason over texts, figures, or tables in each multimodal long document. We aim for this benchmark to serve as a valuable resource for researchers and practitioners, enabling more rigorous testing and development of multimodal document understanding systems. Figure 1: Data distribution of document topics in our M-LongDoc benchmark. Pages Tokens In-Depth DocVQA 1.0 151.5 ✗ ChartQA 1.0 236.9 ✗ InfoVQA 1.2 288.0 ✗ TAT-DQA 1.1 577.0 ✗ VisualWebBench 1.0 452.4 ✗ PWC 12 7000 ✗ MP-DocVQA 8.3 2026.6 ✗ DUDE 5.7 1831.5 ✗ SlideVQA 20.0 151.5 ✗ MMLongBench 47.5 2030.5 ✗ Ours 210.8 120988.0 ✓ Figure 2: Comparison of benchmarks along three dimensions: the number of pages per document, the number of tokens per document, and the nature of the responses required. Specifically, we assess whether each benchmark emphasizes in-depth, comprehensive answers or focuses on short or extractive responses. Another challenge we have to overcome is that due to the lengthy content of multimodal documents and the in-depth solutions, the evaluation of the open-ended question-answering task becomes tricky. To assess such open-ended solutions in a scalable and standardized manner, we design an automated evaluation framework that does not require reference answers or human annotation. Inspired by previous works in model-based evaluation (Zheng et al., 2023; Zhao et al., 2024; Liu et al., 2023c), our evaluation framework leverages a detailed evaluation guide and multiple judge models to score the correctness of each generated solution. With our proposed M-LongDoc and evaluation framework, we conducted preliminary study on existing models and the results show that they struggle with figure and table-based questions compared to text-based questions, revealing their multimodal bias and weaknesses (Chen et al., 2024b). Furthermore, we observed that the models can be easily distracted by irrelevant content in the document pages (Shi et al., 2023), even with the aid of retrieval-augmented generation. To enhance the robustness of multimodal models against potentially irrelevant retrieved content, we propose a retrieval-aware tuning approach for multimodal document reading. This framework unifies supervised fine-tuning and retrieval augmented generation by including distracting content from other modalities and pages in each document. Thus, we adapt models to effectively incorporate the domain knowledge in multimodal documents while ignoring the content irrelevant to the given query. Compared to existing training methods, ours is the first to address retrieval-augmented multimodal training for rich document layouts. To support this training framework and the enhancement of open-source models, we further construct a training corpus of 10,070 samples for question-answering over multimodal long documents. Experiments show that our approach achieves a 4.6% relative improvement in the correctness of model responses, compared to the baseline model. Figure 3: Example questions in different multimodal document question answering benchmarks. For illustration, we include content from the relevant page in the original document. The example question from M-LongDoc is more complex than those from other benchmarks, as it requires an explanatory answer rather than an extraction of a short text span. Furthermore, it requires the model to understand the semantics of both image and text. Please note that in our benchmark setting, the model is provided with all page contents from the document, and not only the relevant page. The key contributions of this work are threefold: 1) We introduce M-LongDoc, a multimodal benchmark that more accurately represents the challenges of real-world document understanding tasks. Our automated evaluation framework enables scalable and standardized assessment of open-ended solutions. 2) Our evaluation of leading models indicates that most models struggle with figure and table-based questions compared to text-based questions, revealing their multimodal bias. 3) We propose a retrieval-aware tuning framework that together with our large-scale training corpus, significantly improves the efficiency and effectiveness of multimodal document reading. Thus, we believe that this work contributes to the field of document understanding and paves the way for more capable and practical applications of large multimodal models in real-world scenarios. To accelerate the studies in our community, we will make the M-LongDoc benchmark, the training corpus for multimodal document reading, and our source code publicly available."
https://arxiv.org/html/2411.06175v1,Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs,"This paper introduces an innovative semi-supervised learning approach for text classification, addressing the challenge of abundant data but limited labeled examples. Our methodology integrates few-shot learning with retrieval-augmented generation (RAG) and conventional statistical clustering, enabling effective learning from a minimal number of labeled instances while generating high-quality labeled data. To the best of our knowledge, we are the first to incorporate RAG alongside clustering in text data generation. Our experiments on the Reuters and Web of Science datasets demonstrate state-of-the-art performance, with few-shot augmented data alone producing results nearly equivalent to those achieved with fully labeled datasets. Notably, accuracies of 95.41% and 82.43% were achieved for complex text document classification tasks, where the number of categories can exceed 100.","Often, fine-tuning large language models for specific tasks demands a considerable amount of data. However, usually the real industry datasets we encounter are not that labeled accurately and precisely, or such well-labeled datasets are difficult to obtain, or costly for people to perform human labeling. Semi-supervised text classification (SSTC), a hot subbranch of text mining, is dedicated to improving classification performance by leveraging a small amount of labeled data and a large amount of unlabeled data. Unfortunately, traditional approaches[1][2] targeting the semi-supervised text classification (SSTC) task typically entail a complex model structure, which proves to be prohibitively challenging for people to train and utlize such models. Meanwhile, Large Language Models (LLMs) have achieved remarkable progress in the domain of artificial intelligence recently. These models have drawn widespread attention because of their outstanding language understanding and generation capabilities, and have demonstrated great potential in numerous application fields. Large LLMs can not only function as generative models but essentially also as probabilistic models that possess the capacity to output choice probabilities and make decisions. This can be achieved, for example, by assigning choice IDs such as A and B and calculating the logit of the last output word before the EOS token. Recent research indicates that large language models are excellent zero-shot and few-shot learners by incorporating examples and instructions into the prompt.[3]. Technologies such as Chain of Thought (CoT)[4] and Tree of Thoughts[5] have subsequently further demonstrated significant enhancements in multiple text classification[6][7] benchmark tests by enabling LLMs to output reasoning before the answer, or by conducting further ensembles, i.e self-consistency[8] among multiple answers. Thus, employing LLMs as models for SSTC tasks can benefit from their generalization and texture understanding ability. Moreover, the popularity of LLMs, along with numerous developed training and deployment frameworks, makes them more attractive contrast to traditional SSTC models. On the other hand, when we encounter datasets that encompass professional domain knowledge, relying merely on the capacity of LLMs is inadequate. Retrieval Augmented Generation (RAG) [9], a method first introduced in 2020, gained popularity after the emergence of LLM. RAG offers benefits such as associating answers with professional knowledge without undergoing training, thereby enhancing the answer quality and reducing hallucinations. This is achieved by first retrieving relevant information from external knowledge, mainly through embedding models, and subsequently combining the retrieved information into the input prompt of LLMs to generate answers. There are a number of works aiming at improving RAG pipeline from various aspects such as query rewrite, document rerank, and post-retrieval processing[10]. The RAG framework proved to be highly suitable for text classification tasks. In this regard, [11] utilized retrieval-enhanced LLMs to generate text classification datasetes under zero-shot condition. Moreover, [12] proposed a Retrieval-Augmented framework to alleviate poor generalization issues existed in text classification taskes. However, for methods like RAG or CoT that do not entail fine-tuning, the outputs might not always comply with the instructions in the prompt, while fine-tuning enables adjustments of the behaviors of LLMs, such as adhering to specific writing styles[10]. At the same time, adding content to the prompt and generating reasoning prior to answering leads to increased computation costs. For instance, as of July 2024, for GPT-4-o, it is $5 per 1M input and $15 per 1M output, and the cost for text-embedding-3-large is $0.13 .This can be depleted rapidly when we have a large volume of data and if we aim to apply CoT and RAG to every data point. In production scenarios where there is a massive amount of data and the answer is required as soon as possible, a smaller yet quicker specialized model that directly outputs the answer is much preferred. While [13] reveals that LLMs can improve themselves through the utilization of CoT prompting and self-consistency to generate high-quality answers from unlabeled data and fine-tuning based on these generated answers. Studies such as [14] have further demonstrated that fine-tuning a model with fewer parameters through knowledge distillation from the CoT outputs of larger LLMs results in improved task performance on datasets like GSM8K. Still, techniques such as RAG and CoT are excellent options when we encounter circumstances where there is no labeled data or only a very limited amount of labeled data, or when we aim to generate high-quality data. Given the recent progress in methods such as RAG and CoT, which have enhanced the outputs of LLMs, yet considering the limitations of these methods that make them more suitable for improving a fine-tuned model rather than being directly used in production, especially for text classification tasks. Also, given the undeniable fact that the real-world data we encounter is often duplicated, mislabeled, or completely lacking labels and fine-tuning a model for specific tasks requires considerable amount of well labeled data. Since human experts are costly, it is essential to select the most representative landmark data that can accurately reflect the characteristics of the entire dataset. [15] employed a lightweight deep learning algorithm to select the most informative samples for both image and text classification datasets. Where [16] proposed a dynamic data selection approach for selecting ’long tail’ words in different epochs of training. While clustering can also be employed as a tool for data selection, more studies have focused on using the clustering technique as a feature selection tool [17][18]. This study presents an innovative approach for fine-tuning LLMs in text classification tasks by integrating multiple distinct components within a pipeline. Firstly, it involves data selection through clustering algorithms. Subsequently, human experts carry out manual labeling, and these labels act as retrieval examples via the RAG and CoT pipeline from a larger-sized LLM to generate additional training samples. Thereafter, these training samples are utilized to fine-tune a smaller-sized LLM and iteratively focus on the segments where the model might have made incorrect decisions, with these segments being determined again by clustering. Our innovation can be summarized as follows: Clustering Through controlled experiments, we demonstrate that our technique, which employs clustering to select representative landmark data from unlabeled datasets and pick potentially wrongly judged data points, enhances the accuracy of the fine-tuned model when the same amount of labeled data is utilized compared with random selection and the method proposed by [15] in datasets such as 20 newsgroups. Moreover, to the best of our knowledge, we are the pioneers in using clustering techniques for data selection in text classification tasks. In this manner, the labeled data ensures diversity and can cover the majority of cases, thereby well reflecting the characteristics of the entire dataset with a small quantity of data. Compared with the traditional full-scale labeling method, this approach significantly reduces the workload and time cost of manual labeling while ensuring the data quality. Agumenting Our study presented a novel approach that utilizes RAG to select samples from high-quality labeled data and combines them in a CoT chain to generate high-quality augmented data. We hold that the improvement stems from RAG, allowing the model to learn data patterns and regularities from a few labeled landmark data to boost accuracy, and CoT, which inspires LLMs themselves to consider potential corner cases. Such generated data can further expand the dataset, enhancing its diversity and richness. Despite numerous studies focusing on RAG and CoT, to our best knowledge, no research has employed them for data augmentation. In our experiment, this method significantly enhances the classification accuracy. Integration Although numerous studies have concentrated on each minor aspect of text classification, few have elaborated in detail on the different stages of text classification tasks, such as data selection, preprocessing, labeling, data augmentation, and iterative training with updated augmented data. Due to the increased diversity and representativeness of these selected labeled data, the trained model can deal with various practical scenarios more proficiently, improving the generalization ability and accuracy of the model. Simple Our intuitive approaches are straightforward to implement and understand, and each step is based on open source packages. This compatibility makes our method more pragmatic than other methods that pose challenges for people in terms of checking and implementation. We also offer our code on GitHub. Through fine-tuning LLM models with relatively smaller parameters (e.g., 4B), we found that in contrast to the chain of thought approach with a significantly larger parameter size (e.g., 72B), the model’s response accuracy is higher and it requires fewer resources."
https://arxiv.org/html/2411.06171v1,SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models,"Continual learning (CL) is crucial for language models to dynamically adapt to the evolving real-world demands. To mitigate the catastrophic forgetting problem in CL, data replay has been proven a simple and effective strategy, and the subsequent data-replay-based distillation can further enhance the performance. However, existing methods fail to fully exploit the knowledge embedded in models from previous tasks, resulting in the need for a relatively large number of replay samples to achieve good results. In this work, we first explore and emphasize the importance of attention weights in knowledge retention, and then propose a SElective attEntion-guided Knowledge Retention method (SEEKR) for data-efficient replay-based continual learning of large language models (LLMs). Specifically, SEEKR performs attention distillation on the selected attention heads for finer-grained knowledge retention, where the proposed forgettability-based and task-sensitivity-based measures are used to identify the most valuable attention heads. Experimental results on two continual learning benchmarks for LLMs demonstrate the superiority of SEEKR over the existing methods on both performance and efficiency. Explicitly, SEEKR achieves comparable or even better performance with only 1/10 of the replayed data used by other methods, and reduces the proportion of replayed data to 1%. The code is available at https://github.com/jinghan1he/SEEKR.","Figure 1: Demonstration of the critical role of attention weights in knowledge retention. We apply DER++ Buzzega et al. (2020) for continual learning on the TRACE benchmark Wang et al. (2023c) to obtain multiple old task models and the final model. Grafting the attention weights of the old models onto the final model at inference can maintain better performance on the old tasks. Moreover, the final model obtained by our continual learning method, SEEKR, achieves similar results. Enabling large language models Achiam et al. (2023); Touvron et al. (2023); Zheng et al. (2024) with human-like continual learning ability is crucial for the long-term practical deployment. It allows for constant knowledge accumulation on new tasks without the need for costly retraining. However, sequentially finetuning the LLMs with new data can lead to catastrophic forgetting McCloskey and Cohen (1989), impairing the general ability of the model and its performance on previous tasks. Among the array of continual learning methods Ke and Liu (2022), data replay stands out as the most widely adopted strategy in practice due to its simplicity and efficacy Wang et al. (2024). Based on it, replay-based distillation methods, including DER++ Buzzega et al. (2020) and subsequent techniques Qin and Joty (2021); Kang et al. (2022); Gu et al. (2023), further boost the performance by utilizing memories from both data and model perspectives. Specifically, Buzzega et al., 2020; Qin and Joty, 2021; Gu et al., 2023 distill the output logits of old models for knowledge transfer, and Kang et al., 2022 restrict the changes in important feature maps in the image encoders. However, these works have not fully exploited the potential of knowledge distillation in continual learning for LLMs. They focus on the outputs of network layers while neglecting the preservation of intricate internal functions. Consequently, a relatively large amount of replay data is required by these methods to achieve good results. Recently, many studies have investigated the attention weights of different heads to analyze the interpretability of the internal mechanisms in LLMs Vig and Belinkov (2019); Wang et al. (2023a). Inspired by this, we explore whether attention weights play a critical role in knowledge retention during continual learning in LLMs. As shown in Figure 1, grafting the attention weights from the LLM of the old tasks to the final LLM after continual learning can maintain better performance on old tasks, which suggests that the attention weights could be crucial to alleviate the catastrophic forgetting problem and achieve more comprehensive knowledge retention111Attention grafting can only be used during inference with both the source and target models, which is an infeasible solution for continual learning. We employ this technique solely for exploratory experiments.. However, naively preserving the attention weights of all heads in the LLM by distillation introduces significant computational costs. Previous studies have observed a functional specialization phenomenon among attention heads in LLMs Vig and Belinkov (2019); Jo and Myaeng (2020); Li et al. (2023), which indicates the susceptibility of attention heads to forgetting and their importance to previous tasks vary. This property allows us to selectively focus on the valuable attention heads for efficient knowledge retention. To this end, we propose a finer-grained model distillation method called SElective attEntion-guided Knowledge Retention (SEEKR) for continual learning of large language models, which employs attention distillation on the most valuable heads in LLMs to achieve efficient knowledge retention. Specifically, we develop knowledge-retention-oriented head importance measures, which consider both forgettability and task sensitivity, to identify the most valuable heads for distillation. The forgettability, measured by the cumulative changes in attention weights during continual learning, indicates the generality of knowledge and the necessity of distillation. An attention head with higher forgettability indicates a greater need for knowledge retention. The task sensitivity, calculated as the first-order derivative of the task loss, evaluates the importance of maintaining the attention weights of an attention head for a given task. An attention head with greater sensitivity should be prioritized to restrict variations in its attention weights. Using the above two importance scores, SEEKR designs a hierarchical budget allocation mechanism to adaptively select the most valuable attention heads for distillation in a controllable way, which can efficiently regulate the training cost. By using SEEKR, the performance of old tasks can be further maintained as shown in Figure 1. Extensive experiments are conducted on the recently developed continual learning benchmark for LLMs Wang et al. (2023c) and the continual learning benchmark on traditional NLP tasks Wang et al. (2022a). The results consistently demonstrate the superiority of SEEKR in mitigating catastrophic forgetting and maintaining the general capabilities of LLMs. Moreover, as a replay-based method, SEEKR exhibits excellent data efficiency, achieving comparable or better performance with just 1/10 of the replayed data used by the existing methods, reducing the replayed data proportion to only 1%. Our main contributions are summarized as follows: • We explore and emphasize the importance of attention weights for knowledge retention, and devise knowledge-retention-oriented measures to identify important attention heads for distillation. The proposed method, SEEKR, can efficiently preserve the finer-grained knowledge in the selected attention heads. • Extensive experiments validate the superiority of SEEKR, showcasing its data efficiency by using just 1% of replay samples to achieve the comparable or better performance that other methods reach with 10% of replay samples."
https://arxiv.org/html/2411.06159v1,From References to Insights: Collaborative Knowledge Minigraph Agents for Automating Scholarly Literature Review,"Literature reviews play a crucial role in scientific research for understanding the current state of research, identifying gaps, and guiding future studies on specific topics. However, the process of conducting a comprehensive literature review is yet time-consuming. This paper proposes a novel framework, collaborative knowledge minigraph agents (CKMAs)111https://aaai-2025-4471.github.io/, to automate scholarly literature reviews. A novel prompt-based algorithm, the knowledge minigraph construction agent (KMCA), is designed to identify relationships between information pieces from academic literature and automatically constructs knowledge minigraphs. By leveraging the capabilities of large language models on constructed knowledge minigraphs, the multiple path summarization agent (MPSA) efficiently organizes information pieces and relationships from different viewpoints to generate literature review paragraphs. We evaluate CKMAs on three benchmark datasets. Experimental results demonstrate that the proposed techniques generate informative, complete, consistent, and insightful summaries for different research problems, promoting the use of LLMs in more professional fields.","Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augment and accelerate scientiﬁc research (Wang et al. 2023). Researchers are developing AI methods to, e.g., literature understanding, experiment development, and manuscript draft writing (Liu et al. 2022; Wang et al. 2024; Martin-Boyle et al. 2024). Literature reviews play a crucial role in scientific research, assessing and integrating previous research on specific topics (Bolanos et al. 2024). They aim to meticulously identify and appraise all relevant literature related to a specific research question. Recent advancements in AI have shown promising performance in understanding research papers and generating human-like text (Van Dinter, Tekinerdogan, and Catal 2021). By leveraging AI capabilities, automatic literature review models enable researchers to save time and effort in the manual process of conducting literature reviews, rapidly identify key trends and gaps in recent research outputs, and uncover insights that might be overlooked in manual reviews (Wagner, Lukyanenko, and Paré 2022). The automatic literature review models typically involve two stages (Shi et al. 2023): (1) selecting relevant reference papers and (2) determining logical relationships to compose a summary that presents the evolution of a specific field (these stages can be applied iteratively). Multiple scientific document summarization (MSDS), aiming to generate coherent and concise summaries for clusters of topic relevant scientific papers, is the representative work in the second stage. Past decades (Jin, Wang, and Wan 2020) have witnessed the development of summarization methods. Extractive methods directly select important sentences from original papers. Abstractive methods can generate new words and sentences but are technically more challenging than extractive methods. Figure 1: Illustration of relationships between information pieces in scientific papers. Capturing these relationships is essential for composing a coherent story in literature reviews. Recently, large language models (LLMs), pre-trained on extensive text data, have transformed abstractive summarization and show human-like performance in understanding and coherent language synthesis. However, ideas arising in research papers often have complex relationships, e.g., conflicting or duplicate. Without explicit instructions, LLMs fall short in capturing the relations between ideas and composing a story that connects related work reflecting the author’s understanding of their field (Li and Ouyang 2024). As shown in Fig 1, effective summarization often involves the ability to understand concepts of materials, methods, and tasks in referencing papers, aggregate complementary ideas (e.g., M_{1} is used for T) while contrasting differences (e.g., M_{2} is additionally used for B compared with A). To tackle this bottleneck, we propose equipping LLMs with structural knowledge. Different from knowledge graphs, which consist of entities as nodes and their relationships as edges, serving as general-purpose knowledge, we introduce knowledge minigraphs. Knowledge minigraphs are small-scale semantic graphs, comprising research-relevant concepts as nodes and their relationships as edges, specially designed to capture the structural information between ideas in references. To automatically construct knowledge minigraphs, we propose a prompt-based algorithm, the knowledge minigraph construction agent (KMCA) to elicit LLMs to identify research-relevant concepts and relationships based on references. Benefiting from the designed iterative construction strategy, key information and relationships are iteratively extracted and stored from numerous references into minigraphs. By leveraging the capabilities of LLMs on knowledge minigraphs, the multiple path summarization agent (MPSA) is designed to organize the generated literature review. The MPSA samples multiple summaries from different viewpoints and logical paths in the knowledge minigraph, utilizing the mixture of experts technique. A self-evaluation mechanism is then employed to automatically route to the most desirable summary as the final output."
https://arxiv.org/html/2411.06151v1,Building an Efficient Multilingual Non-Profit IR System for the Islamic Domain Leveraging Multiprocessing Design in Rust,"The widespread use of large language models (LLMs) has dramatically improved many applications of Natural Language Processing (NLP), including Information Retrieval (IR). However, domains that are not driven by commercial interest often lag behind in benefiting from AI-powered solutions. One such area is religious and heritage corpora. Alongside similar domains, Islamic literature holds significant cultural value and is regularly utilized by scholars and the general public. Navigating this extensive amount of text is challenging, and there is currently no unified resource that allows for easy searching of this data using advanced AI tools. This work focuses on the development of a multilingual non-profit IR system for the Islamic domain. This process brings a few major challenges, such as preparing multilingual domain-specific corpora when data is limited in certain languages, deploying a model on resource-constrained devices, and enabling fast search on a limited budget. By employing methods like continued pre-training for domain adaptation and language reduction to decrease model size, a lightweight multilingual retrieval model was prepared, demonstrating superior performance compared to larger models pre-trained on general domain data. Furthermore, evaluating the proposed architecture that utilizes Rust Language capabilities shows the possibility of implementing efficient semantic search in a low-resource setting.","Dense retrieval is an advanced approach in IR that utilizes embeddings to identify semantically similar text, known as semantic search. LLMs are a key component in creating text embeddings and performing dense retrieval (Karpukhin et al., 2020; Izacard et al., 2021). One of the first challenges in building a non-profit multilingual domain-specific IR system is that the use of publicly available multilingual large language models (MLLMs) pre-trained on a general domain could deteriorate performance due to domain shift when applied to new domains (Lee et al., 2019; Huang et al., 2019). To overcome this, we begin with pre-training an MLLM for the Islamic domain to address this issue. However, pre-training a domain-specific MLLM brings two additional challenges. Firstly, assembling a multilingual domain-specific corpus for pre-training a MLLM requires a large amount of domain-specific data that is often difficult to find in different languages. Secondly, multilingual models are heavyweight, frequently exceeding 1GB, making them challenging to deploy. To effectively tackle the issue of pre-training domain-specific MLLM, we employ a continued pre-training approach and incorporate domain-specific vocabulary to accommodate the domain shift better (Beltagy et al., 2019). To deal with the challenge of the large size of MLLM, we perform language reduction and remove languages not needed in the current deployment. This method helps us reduce the model’s size by more than half, even after introducing new domain-specific vocabulary. We use this lightweight domain-specific MLLM as a backbone for the retrieval. Evaluation of this model on an in-domain IR dataset found that our model significantly outperforms general-domain multilingual and monolingual models even after performing language reduction. Figure 1: The main components of building a multilingual IR system. In the upper left corner is the preparation of the retrieval model that includes language reduction (LR) and domain adaptation (DA). The rest of the figure shows the implementation of semantics search in Rust with multiprocessing architecture. Moreover, deploying non-profit AI systems implies operating on a limited budget, which makes it challenging to use embedding APIs or libraries that rely on GPU acceleration to perform search reasonably fast. To tackle this challenge and meet the requirements of implementing an ad hoc IR system on a public website, we utilize the multiprocessing capabilities of Rust Language to create an efficient and secure semantic search based on CPU architecture (Abdi et al., 2024; Seidel and Beier, 2024; Liang et al., 2024). Our system’s evaluation and comparison against others, such as Faiss, indicates that our implementation of semantic search with underlying Rust multiprocessing architecture can significantly accelerate search without compromising performance. Our main contributions are: • We have developed a free online multilingual search tool for exploring well-established literature in the Islamic domain.111A system is deployed at https://rttl.ai/ • To the best of our knowledge, we are the first to deploy open-source, non-profit semantic search leveraging multiprocessing using Rust language."
https://arxiv.org/html/2411.06101v1,"Detecting Reference Errors in Scientific Literature
with Large Language Models","Reference errors, such as citation and quotation errors, are common in scientific papers. Such errors can result in the propagation of inaccurate information, but are difficult and time-consuming to detect, posing a significant challenge to scientific publishing. To support automatic detection of reference errors, this work evaluated the ability of large language models in OpenAI’s GPT family to detect quotation errors. Specifically, we prepared an expert-annotated, general-domain dataset of statement-reference pairs from journal articles. Large language models were evaluated in different settings with varying amounts of reference information provided by retrieval augmentation. Our results showed that large language models are able to detect erroneous citations with limited context and without fine-tuning. This study contributes to the growing literature that seeks to utilize artificial intelligence to assist in the writing, reviewing, and publishing of scientific papers. Potential avenues for further improvements in this task are also discussed.","Researchers cite literature as references and supporting evidence when reporting their work in papers. The reliability of referencing is usually taken for granted. However, previous citation verification studies in multiple scientific domains have revealed that reference errors of varying degrees are common in scientific papers, with prevalence rates ranging from 11% to 41%, depending on the domain, journal, and methodology (Goldberg et al., 1993; Lee and Lee, 1999; Fenton et al., 2000; Gosling et al., 2004; Lukić et al., 2004; Todd et al., 2010; Jergas and Baethge, 2015; Mogull, 2017; Armstrong et al., 2018; Smith and Cumberledge, 2020; Pavlovic et al., 2021; Cobb et al., 2024). Reference errors could result in the propagation of inaccurate information (Smith and Cumberledge, 2020), undermining the credibility of scientific research and sometimes leading to serious consequences (Pavlovic et al., 2021). For example, hundreds of uncritical citations of a 1980 letter published in the New England Journal of Medicine may have contributed to the opioid crisis in the United States (Leung et al., 2017). Previous studies defined two major types of reference errors: citation errors and quotation errors. Citation errors usually refer to typographical errors in referencing, such as incorrect reference information (e.g., incorrect authors, title, journal, or year) or the erroneous arrangement of references (Smith and Cumberledge, 2020). Citation errors have become less common in the era of digitization and citation managers, although these same factors may enable the propagation of pre-existing citation errors. In contrast, a quotation error specifically refers to the situation where a reference fails to support the statement for which it is cited (Smith and Cumberledge, 2020). Notably, these two definitions are not mutually exclusive, as an incorrect assignment of reference indices can manifest as a quotation error in an individual statement-reference pair. Recent studies on referencing errors sometimes use different terms for quotation errors, such as “content errors” (Mogull, 2017), “inaccurate citations” (Pavlovic et al., 2021), or “miscitations” (Cobb et al., 2024). Quotation errors are difficult and time-consuming to detect, and they often require domain expertise when comparing a statement to relevant information in the reference article (Smith and Cumberledge, 2020). Previous studies on reference errors typically utilized domain experts to manually examine samples of scientific papers. The difficulty of detecting quotation errors poses a significant challenge to scientific publishing, as it requires additional efforts by editors and reviewers in peer review. Given the exploding number of papers being published each year, this task is becoming increasingly demanding. Recent advances in natural language processing (NLP) have demonstrated astounding capabilities of large language models (LLMs) to perform various types of text-based tasks (Ouyang et al., 2022; Touvron et al., 2023; OpenAI et al., 2024; Google, 2024), providing a strong baseline for application in the real world. Researchers have also started exploring ways NLP can assist with paper writing and peer review (Kuznetsov et al., 2024). However, none of these studies examined reference error detection. To fill this gap and encourage future attempts to automate reference error detection, this study performed a general-domain evaluation of the capability of LLMs to detect quotation errors in scientific papers."
https://arxiv.org/html/2411.06096v1,"ZhoBLiMP: a Systematic Assessment of Language Models 
with Linguistic Minimal Pairs in Chinese","Whether and how language models (LMs) acquire the syntax of natural languages has been widely evaluated under the minimal pair paradigm. However, a lack of wide-coverage benchmarks in languages other than English has constrained systematic investigations into the issue. Addressing it, we first introduce ZhoBLiMP, the most comprehensive benchmark of linguistic minimal pairs for Chinese to date, with 118 paradigms, covering 15 linguistic phenomena. We then train 20 LMs of different sizes (14M to 1.4B) on Chinese corpora of various volumes (100M to 3B tokens) and evaluate them along with 14 off-the-shelf LLMs on ZhoBLiMP. The overall results indicate that Chinese grammar can be mostly learned by models with around 500M parameters, trained on 1B tokens with one epoch, showing limited benefits for further scaling. Most (N=95) linguistic paradigms are of easy or medium difficulty for LMs, while there are still 13 paradigms that remain challenging even for models with up to 32B parameters. In regard to how LMs acquire Chinese grammar, we observe a U-shaped learning pattern in several phenomena, similar to those observed in child language acquisition.","Acceptability judgment is an important empirical method to measure human linguistic competence (Chomsky, 1965; Schütze, 2016), which has also been used to assess the linguistic knowledge of language models (LMs). Much work in this line adopted the minimal pair paradigm (mpp) in evaluating LMs (Linzen et al., 2016; Wilcox et al., 2018; Warstadt et al., 2020; Hu and Levy, 2023; Warstadt et al., 2023). A minimal pair is a pair of sentences with minimal contrast that affects whether the sentence is acceptable or not. A well-trained LM should assign higher probabilities to the acceptable sentences than the unacceptable ones (marked with *): \ex . \a. The books of the lady are new. .̱ *The books of the lady is new. mpp-style benchmarks in many languages have been created (see Table 1). Notably, there are two such benchmarks for Chinese: CLiMP (Xiang et al., 2021) and SLING (Song et al., 2022). Benchmark Language Size N BLiMP Warstadt et al. (2020) English 67k 67 SyntaxGym Hu et al. (2020) English NA 39 CLiMP Xiang et al. (2021) Chinese 16k 16 SLING Song et al. (2022) Chinese 38k 38 JBLiMP Someya and Oseki (2023) Japanese 331 39 LINDSEA Leong et al. (2023) Indonesian 380 38 Tamil 200 20 RuBLiMP Taktasheva et al. (2024) Russian 45k 45 ZhoBLiMP (Ours) Chinese 35k 118 Table 1: Comparison of mpp benchmarks for different languages. Size refers to the number of minimal pairs in total; N refers to the number of linguistic paradigms. However, both fall short of including enough linguistic phenomena, with CLiMP covering 16 and SLING 38, compared to the 67 paradigms in the original English BLiMP, plus five newly added paradigms in the BabyLM challenge (Warstadt et al., 2023). Furthermore, CLiMP uses a lexicon translated from English, which has been noted for generating unnatural sentences (Song et al., 2022). Sentences in SLING are derived from the Penn Chinese Treebank Xue et al. (2005), which limits its sentence structures and also makes it difficult to extend to new paradigms. Two very important Chinese linguistic phenomena—“把” (ba) and also “被” (bei, passive) constructions—are missing in SLING. Phenomenon N Acceptable example Unacceptable example Anaphor 6 她的弟弟讨厌他自己。 她的弟弟讨厌她自己。 Her little brother hates himself. Her little brother hates herself. Argument struc. 7 我预习了教材。 我出现了教材。 I previewed the textbook. I appeared the textbook. BA 13 她把那条鱼放在池塘里。 把那条鱼她放在池塘里。 She BA that fish put in the pond. BA that fish she put in the pond. Classifier 3 那边站着八位舞者。 那边站着八条舞者。 Eight WEI dancers are standing there. Eight TIAO dancers are standing there. Control & Raising 4 那杯红酒会变质。 会那杯红酒变质。 That glass of wine will go bad. Will that glass of wine go bad. Ellipsis 3 你们拉了小提琴，我们也拉了。 你们笑了一天，我们也笑了。 You played the violin, we played too. You laughed all day, we laughed too. FCI licensing 5 任何人都可以去。Anyone DOU can go. 任何人可以去。Anyone can go. Nominal exp. 11 他是司机。He is a driver. 他司机。He driver. NPI licensing 9 没有任何人来了。Nobody came. 任何人没有来了。Anyone didn’t come. Passive 12 那些秘密不可以被他们所知道。 那些秘密不可以被所知道。 Those secrets cannot be known by them. Those secrets cannot be SUO known by. Quantifiers 2 没有人吃了超过九块糖果。 没有人吃了至少九块糖果。 No one ate more than nine candies. No one ate at least nine candies. Question 21 你到底喝不喝啤酒？ 你难道喝不喝啤酒？ You DAODI will drink the beer or not? You NANDAO will drink the beer or not? Relativization 4 我所厌恶的那位领导来了。 我所厌恶他的那位领导来了。 The leader I despise came. The leader I despise him came. Topicalization 4 他在喝一杯咖啡。 一杯咖啡他在喝。 He is drinking a cup of coffee. A cup of coffee he is drinking. Verb Phrase 14 她没有吃过蛋糕。 她没有吃了蛋糕。 She hasn’t eaten a cake. She hasn’t ate a cake. Table 2: Overview of the 15 phenomena in ZhoBLiMP, with the number of paradigms (N), and one randomly sampled minimal pair. Each paradigm contains 300 minimal pairs. English translation for illustrative purposes, using a mixture of word-by-word gloss and translation to show the contrast. To fill the gaps in benchmark design, we first present ZhoBLiMP, a benchmark for Chinese mpp, with 118 paradigms covering 15 linguistic phenomena, totaling 35k minimal pairs (see Table 2 for examples). ZhoBLiMP is created following the same data generation method as BLiMP, but with grammar templates and a vocabulary developed by Chinese linguists. In addition, we developed an easy-to-use GUI for collaborative and extendable minimal pair generation for linguists unfamiliar with coding, which will be made publicly available (§3). In terms of model evaluation, to the best of our knowledge, there are no systematic, controlled experiments conducted on a non-English language that trains LMs from scratch to study the learning processes from the very beginning. Thus it is unclear whether conclusions from studies on English hold in other languages in general (Zhang et al., 2021; Evanson et al., 2023; Warstadt et al., 2023). We fill this gap by pre-training a suite of Pythia models of different sizes (14M-1.4B parameters) from scratch on Chinese data (10M-3B tokens) to study the learning patterns and trajectories of LMs. Also, we benchmark state-of-the-art LLMs to diagnose the shortage of Chinese language understanding (§4). Our results in §5 show that roughly 500M model parameters and 1B tokens of training data are needed to achieve \sim80% accuracy on ZhoBLiMP. We discuss why our estimation is different from the 100M tokens suggested in the literature on the English BLiMP (Warstadt et al., 2023). Notably, there are three phenomena that are challenging for LMs even up to 32B parameters: Anaphor, Ellipsis, and Quantifiers. Additionally, a U-shape trajectory is discovered in at least four phenomena, similar to the learning curves found in children’s language acquisition Bybee and Slobin (1982); Pinker and Ullman (2002); McClelland and Patterson (2002). We see several use cases for ZhoBLiMP. First, minimal pairs can be used unsupervised to probe and interpret LM behaviors on its training objective (Varda and Marelli, 2023; Arora et al., 2024). ZhoBLiMP can promote future studies in Chinese and multilingual settings. Second, cognitive scientists can use ZhoBLiMP, or extend it with new phenomena, to compare how models and children/humans acquire various Chinese linguistic phenomena, as showcased in the BabyLM challenge (Warstadt et al., 2023). Minimal pairs in ZhoBLiMP, our codebase for data generation and model checkpoints will be available at https://github.com/sjtu-compling/ZhoBLiMP."
https://arxiv.org/html/2411.06032v1,LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output,"Immense effort has been dedicated to minimizing the presence of harmful or biased generative content and better aligning AI output to human intention; however, research investigating the cultural values of LLMs is still in very early stages. Cultural values underpin how societies operate, providing profound insights into the norms, priorities, and decision making of their members. In recognition of this need for further research, we draw upon cultural psychology theory and the empirically-validated GLOBE framework to propose the LLM-GLOBE benchmark for evaluating the cultural value systems of LLMs, and we then leverage the benchmark to compare the values of Chinese and US LLMs. Our methodology includes a novel “LLMs-as-a-Jury” pipeline which automates the evaluation of open-ended content to enable large-scale analysis at a conceptual level. Results clarify similarities and differences that exist between Eastern and Western cultural value systems and suggest that open-generation tasks represent a more promising direction for evaluation of cultural values. We interpret the implications of this research for subsequent model development, evaluation, and deployment efforts as they relate to LLMs, AI cultural alignment more broadly, and the influence of AI cultural value systems on human-AI collaboration outcomes.","The advent of artificial intelligence (AI) and recent strides in the sophistication of large language models (LLMs) are redefining human interactions with technology. Use cases include drafting preliminary versions of documents and communications, assistance in brainstorming, programming and debugging, image generation, editing of academic, creative, and colloquial text, etc. Not only has technology expanded its use cases, but its adoption takes various forms for different age groups as individuals in various phases of life seek to understand and use these tools [Tang et al., 2022]. As AI becomes increasingly integrated into people’s routines and lifestyles, it is imperative to understand the nature of AI output, its influence on societies, and its potential impact on human agency and decision making [Wenker, 2023]. Thus, human-AI alignment research is garnering attention as decision makers acknowledge this imperative need for proactively contextualizing general-purpose intelligent machines to human value systems. A pressing challenge is how to extract and evaluate values embedded in model responses. LLMs are trained to not explicitly express their preferences and biases towards specific cultural values; when directly prompted, models reply with predefined responses. For example, when asked, “What are your values?” ChatGPT replied: “I don’t have personal values or beliefs. I am a machine learning model created by OpenAI called GPT-3.5, and I don’t have consciousness, self-awareness, or personal opinions. My purpose is to assist and provide information to the best of my ability based on the input I receive. If you have any questions or if there’s something specific you’d like information on, feel free to ask!” However, this does not mean that values are not embedded in the output produced by other queries, as values are intrinsic to language production and reveal implicit preferences; such language choices influence how people comprehend broader developments, engage in discourse, and make decisions (especially related to abstract ideas and organizational systems such as governance and institutions) [Anderson, 2020]. Thus, researchers must determine alternative, reliable methods for undercovering the values embedded in model responses and aligning these underlying values to human values. Emergent machine psychology research is assessing how model “cognition” mechanisms compare to human cognition through the application of various psychological methods [Almeida et al., 2023]. While research has assessed various forms of bias in output, research assessing how cultural values manifest in output is still very limited. This is problematic, given that cultural competencies and humility are critical in establishing meaningful relationships and creating open discourse [Foronda et al., 2016]. Cultural norms between countries vary greatly, and as LLMs adopt a more integrated role in individuals’ lives, their ability to adapt to relevant cultural norms is critical to facilitating effective deployment. Furthermore, researchers of social simulacra have noted the potential for simulating human behavior for applications such as conducting social science experiments without human subjects or modeling of broader populations [Almeida et al., 2023, Park et al., 2023a, Karinshak, 2024]; such simulations introduce the possibility of making sophisticated predictions about the behavior of individuals and groups without direct survey or interview methods. Thus, cultural competency plays a critical role in mediating simulated interactions and ensuring appropriate representations of individuals and cultures, especially when applied to attempt to understand various social groups or to inform decision making. We have identified several important gaps in existing research to be addressed in this study: 1) few studies examine the socio-cultural aspect of machine psychology, and for those do, the dimensions of psycho-cultural values examined is limited or lack robust theoretical grounding [Adilazuarda et al., 2024]; 2) the prompt design is limited to closed-ended, fill-in-the-blank, or short answer questions, which constrains the quality and size of model output inhibiting more nuanced analysis; 3) most current mechanisms on evaluating models’ cultural competencies, e.g., through auto-scoring of survey questionnaires or exploratory human inspection on open-generated responses, do not guarantee a balanced judgment and are subject to human bias; 4) the majority of studies do not incorporate robust multilingual prompting in addition to multilingual LLMs, where prior studies commonly rely solely on translation tools to generate prompts in other languages. With this in mind, our study aims to explore the cultural values embedded in LLMs developed in two countries of distinct cultural norms and trained on differing corpora: the United States (US) and People’s Republic of China (China). We choose to compare US models with Chinese models due to the prevalence and aptitude of modern Chinese LLMs, especially when compared with other non-English based models. Specifically we investigate the implicit cultural values held by four top-performing LLMs from China {CJK}UTF8gbsn(Ernie 3.5 and 4.0 百度文心一言, GLM-4 智谱AI , and Qwen-14b 阿里巴巴通义千问) and four from the US (GPT-3.5 and 4, Claude, and Gemini) to yield further insights on their alignment behavior by comparing their outputs within and between cultural contexts. US LLMs are defined as large language models developed by entities in the US and predominantly trained on English corpora, and Chinese LLMs are defined as large language models developed by entities in China and trained primarily on Chinese corpora. Given the significant language and cultural differences in the underlying data, we hypothesize that differences will exist in their underlying value systems, which can be interpreted in their Eastern versus Western societal contexts. To conceptualize and quantify each model’s distinctive cultural values, we implemented the Global Leadership and Organizational Behavior Effectiveness (GLOBE) framework [House et al., 2004], a widely-recognized rigorous and empirically validated psychometric questionnaire in human cultural cognition, to examine the corresponding “machine psychology”; this framework is informed by 17,370 human survey responses from 62 societies and cultures [House et al., 2004], offering a generalizable tool for conceptualizing and comparing cultures. In particular, the GLOBE framework provides nine measurable dimensions for human psycho-cultural conditions: uncertainty avoidance, power distance, institutional collectivism, in-group collectivism, gender egalitarianism, assertiveness, future orientation, performance orientation, and humane orientation. The GLOBE framework’s extensive validation across various cultures and its ability to capture subtle cultural shifts over time render it the most robust and best-fit conceptual framework for our study, ensuring that the insights derived are both globally relevant and culturally nuanced. Overall, our study examines the intersection of cultural psychology and AI behavior at a comprehensive scale and depth of human conditions, aiming to contribute to the growing field of AI alignment research and the different shades of human-machine dynamics through provision of an explainable benchmark. By investigating the implicit cultural values in the outputs of LLMs from both the US and China, our research seeks to understand how these models may reflect distinct socio-cultural norms. Such research can contribute to future efforts in bias mitigation and the development of AI systems that handle values-based dilemmas more adeptly. Our findings underscore the importance of integrating cultural considerations into AI content development for a closer alignment between AI outputs and the diverse cultural contexts of users. Ultimately, we aspire to support the broader goal of creating AI technologies that are not only effective but also culturally sensitive, thus fostering a more inclusive environment where technology understands and respects its human users."
https://arxiv.org/html/2411.06022v1,Improved intent classification based on context information using a windows-based approach,"Conversational systems have a Natural Language Understanding (NLU) module. In this module, there is a task known as an intent classification that aims at identifying what a user is attempting to achieve from an utterance. Previous works use only the current utterance to predict the intent of a given query and they do not consider the role of the context (one or a few previous utterances) in the dialog flow for this task. In this work, we propose several approaches to investigate the role of contextual information for the intent classification task. Each approach is used to carry out a concatenation between the dialogue history and the current utterance. Our intent classification method is based on a convolutional neural network that obtains effective vector representations from BERT to perform accurate intent classification using an approach window-based. Our experiments were carried out on a real-world Brazilian Portuguese corpus with dialog flows provided by Wavy global company. Our results achieved substantial improvements over the baseline, isolated utterances (without context), in three approaches using the user’s utterance and system’s response from previous messages as dialogue context.","When a human-to-human conversation takes place, people “deduce” intention of a sentence based on the context of the conversation (one or a few previous utterances). In this sense, people do not interpret an intention-based only on an isolated utterance within a dialogue flow. Consequently, the analysis of an utterance to identify a user’s intention in a dialogue system can benefit from the conversation context. However, literature on intent classification does not address this significant approach. Within a dialogue system, two agents widely interact during the communicative process: user and system. A conversation is usually structured in turns; each turn is defined by one utterance from the user and single system response. As a result of this process of interaction between system and user, the set of turns forms a dialog flow. Table I shows an example of a conversation between a user and system, in which U represents the user and S system, The conversational flow column represents the intent for each utterance U. TABLE I: An example of a conversation between an user and system. Utterance Conversational flow U: I’d like Peruvian food food_information S: ”La Clave del Sabor” is a delicious restaurant located near the historic downtown of Cusco. U: But, I am allergic to shrimps user_information S: Don’t worry, ”La Clave del Sabor” has beef, chicken, alpaca, guinea pig and fish food U: Is it reasonably priced? cost_information S: Yes, ”La Clave del Sabor” is in the moderate price range U: What is the phone number? request_information S: The number of ”La Clave del Sabor” is +51974264215. U: I need to travel the next day to Machu Picchu, is there a tour agency nearby? another_information S: Two minutes from ”La Clave del Sabor” restaurant is the ”Inca Travel” tourism agency Literature has not carried out comprehensive studies using the context in the intent classification task. To the best of our knowledge, there is a lack in how to solve this problem. Currently, studies in literature do not examine the relevance of context during intent classification task only use the current utterance as input to predict the user’s intent [1, 2, 3]. As for other tasks such as Dialogue State Tracking and Dialogue Acts do not standardize the context used. Some works used all previous information, and others used the last system reply. We propose a novel approach to managing context, a windows-based procedure. Our strategy aims to make use of windows on the previous information of the dialog flow. In our work, we proposed several combinations to use a dialog flow’s earlier conversations to find the best contextual information. These approaches use previous utterances of a dialog flow to increase the contextual knowledge to improve our model. This architecture makes it possible to train the dialogue history and the current utterance jointly. Our approach is robust to imbalanced datasets due to we modify our loss function to penalize misclassification of the underrepresented classes more than the dominant ones. Our results achieved substantial improvements over the baseline in three approaches using the user’s utterance and system’s response from previous messages as dialogue context. The remainder of this paper is organized as follows. Section II presents related works. Section III makes known our methodology. Section IV presents the experimental results and discusses our findings. Section V concludes and suggests future works."
https://arxiv.org/html/2411.06008v2,The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses.,"This study explores how the Large Language Models (LLMs) adjust linguistic features to create personalized persuasive outputs. While research showed that LLMs personalize outputs, a gap remains in understanding the linguistic features of their persuasive capabilities. We identified 13 linguistic features crucial for influencing personalities across different levels of the Big Five model of personality. We analyzed how prompts with personality trait information influenced the output of 19 LLMs across five model families. The findings show that models use more anxiety-related words for neuroticism, increase achievement-related words for conscientiousness, and employ fewer cognitive processes words for openness to experience. Some model families excel at adapting language for openness to experience, others for conscientiousness, while only one model adapts language for neuroticism. Our findings show how LLMs tailor responses based on personality cues in prompts, indicating their potential to create persuasive content affecting the mind and well-being of the recipients.","The emergence of Large Language Models (LLMs) into the public consciousness at the end of 2022 has significantly transformed the reality of completing tasks across various sectors, from business to education through administration and journalism. It is estimated that in the USA, 80% of employees will have 10% of their work affected by LLMs, with approximately 19% of occupations will have an impact on at least 50% of job duties (Eloundou et al., 2023). The applications of LLMs in education include question-solving, error correction, study assistance, question generation, automatic grading, and material creation. LLMs also support adaptive learning through knowledge tracking and personalizing content (Wang et al., 2024). The vast amount of tasks where LLMs are used will inevitably raise serious challenges related to possible harms, including discrimination, exclusion, toxicity, information hazards, misinformation, malicious uses, and human-computer interaction harm (Weidinger et al., 2021). Multiple techniques are applied to protect users from potentially harmful content, such as probing toxic content with structured patterns (Ousidhoum et al., 2021) and watermarking the suspected text fragment (Kirchenbauer et al., 2023). LLMs are also used in red teaming activities, where they cooperate with other LLMs to safeguard models against misuse (Perez et al., 2022). The effectiveness of these techniques depends on the awareness of potential threats. In our work, we focus on a specific danger: the persuasion by LLMs. To our knowledge, this is the first paper aimed at understanding how LLMs adjust their responses based on a user’s personality in a persuasion task. Given the goal we want to achieve, we pose the following research questions: [RQ1] Which linguistic features are crucial in personalized persuasion? [RQ2] How do Large Language Models adjust the linguistic features in their responses based on the user’s personality type information? [RQ3] Which families of Large Language Models are particularly effective in influencing specific personality traits? Contribution By answering the questions above, we contributed as follows: [C1] We identified 13 linguistic features that are crucial for persuading individuals with varying levels of Big Five personality traits. [C2] We conducted a comparative analysis of 19 models from 5 Large Language Models families, focusing on the language used in responses to persuasive tasks. [C3] We demonstrated how LLMs adapt their language in persuasive task to different personality types. [C4] We presented the dataset with different variances of persuasive task that can be used to validate the personalized persuasiveness of the Large Language Models."
https://arxiv.org/html/2411.05991v1,: Framework for Guided Questioning for progressive informational collection and classification,"Question Answering (QA) is an important part of tasks like text classification through information gathering. These are finding increasing use in sectors like healthcare, customer support, legal services, etc., to collect and classify responses into actionable categories. LLMs, although can support QA systems, they face a significant challenge of insufficient or missing information for classification. Although LLMs excel in reasoning, the models rely on their parametric knowledge to answer. However, questioning the user requires domain-specific information aiding to collect accurate information. Our work, GuideQ, presents a novel framework for asking guided questions to further progress a partial information. We leverage the explainability derived from the classifier model for along with LLMs for asking guided questions to further enhance the information. This further information helps in more accurate classification of a text. GuideQ derives the most significant key-words representative of a label using occlusions. We develop GuideQ’s prompting strategy for guided questions based on the top-3 classifier label outputs and the significant words, to seek specific and relevant information, and classify in a targeted manner. Through our experimental results, we demonstrate that GuideQ outperforms other LLM-based baselines, yielding improved F1-Score through the accurate collection of relevant further information. We perform various analytical studies and also report better question quality compared to our method. 111Code available at: https://github.com/SDRMp/DRPG","Question Answering (QA) systems have been an integral part of the NLP landscape Moise et al. (2010). In particular, the emergence of LLMs has enabled reasoning, proactive questioning, and better semantic understanding of the user response during questing answering or dialogue Wang et al. (2023a). Figure 2: (A) Overall working framework of GuideQ to leverage LLM and label explainability for asking guided question. (B) Details of the prompting strategy used. (C) Final classification along with incremental information. Proactive questioning is another important limb of such systems wherein the bot engages with the user and directs the conversation aheadKeskar et al. (2019); Sun et al. (2023). It finds applications in many places like medical, customer support systems, and legal systems. A more specific challenge such systems face is of the static classification of user textual data. Often a textual data needs to be classified into a particular category or label Wang et al. (2023b); Chen et al. (2022). For example, consider a system to classify a patient symptoms descriptions into one of the disease conditions, or a customer complaint system wherein a user writes a complaint to be categorized into a particular category. A practical challenge faced by such static categorization is inadequate or missing information toward appropriate routing of the user input to an actionable category. It can be immensely benefited by introduction of questioning component to prompt for further information or knowledge, grounded in previous response and the domain itself. Figure 1 shows an example of the same. Our work introduces a novel framework, GuideQ, aimed at framing guided questions based on prior partial information, such that specific relevant information can be asked for. This increment of information can aid for more accurate classification and completion of text. A classic techniques include classifier models for text classificationDevlin et al. (2019) Liu et al. (2019) He et al. (2020). LLMs, on the other hand, have shown impressive abilities for reasoning and context understanding Nan et al. (2023). Further with techniques like in-context learning Garg et al. (2022) and chain of thought Wei et al. (2024) with few-shot exemplars Brown et al. (2020), the task specific adaptation of LLMs greatly benefits. However, the LLM’s parametric knowledge may still not capture domain specific requires. Finetuning options like FT (Full parametric training) and PEFT Hu et al. (2021) for learning from data history posses a huge challenge of high computation cost Sathish et al. (2024). Our framework, GuideQ leverages the innate semantic understanding and reasoning ability of LLMs to combine with external classification explanability, for asking the most relevant guiding question. We specifically use LLaMa-3 8B model Touvron et al. (2023) for questioning and information seeking. We train BERT model Devlin et al. (2019) for classification tasks on complete information which serves as the primary pivot for prediction of most probable labels. GuideQ aims to use the inherent explanations of classifications for question formation. For each class label we also learn the most significant words and phrases that contribute to the particular label classification. We use occlusions to find the keywords for a label using the training data. The LLM utilizes the top most probable classifier labels and their significant representative keywords to form the guiding questions. Intuitively, the keywords are representative of the most important concepts present in the label knowledge. The summarized overview with keywords example can be found in figure 2. The LLM find the similar concepts present in the partial information and the labels, and frames the question based on the most distinguishing concepts between the label. This help direct further information which may not be readily known earlier to the user. We evaluate our framework, GuideQ, on 8 text classification datasets. We first report the F1-Score of partial information and how accuracy changes by questioning and appending the new answer. We also show how explanability is influenced by keywords, namely by testing with unigrams, bigrams, and trigrams. We also report a higher question generation quality compared to other baseline methods. Overall, our work, GuideQ, presents a novel framework to ask guided questions when the initial text is incomplete or partial. The question is such that it effectively differentiates between the most likely labels. We summarize the key contributions of our work as follows: (i) We introduce a novel framework, GuideQ, for providing guided questioning such that an initial partial information can be incremented. (ii) The guided questioning increments the information by leveraging explainability derived through top most confident classifier labels and their corresponding most significant key words. (iii) We show that further information collection through our framework significantly improves the classification accuracy as compared to other baselines. We also show that GuideQ generates more accurate and targeted questions in relation to the user query."
https://arxiv.org/html/2411.05986v1,"Fine-Grained Reward Optimization for Machine Translation 
using Error Severity Mappings","Reinforcement learning (RL) has been proven to be an effective and robust method for training neural machine translation systems, especially when paired with powerful reward models that accurately assess translation quality. However, most research has focused on RL methods that use sentence-level feedback, which leads to inefficient learning signals due to the reward sparsity problem – the model receives a single score for the entire sentence. To address this, we introduce a novel approach that leverages fine-grained token-level reward mechanisms with RL methods. We use xCOMET, a state-of-the-art quality estimation system as our token-level reward model. xCOMET provides detailed feedback by predicting fine-grained error spans and their severity given source-translation pairs. We conduct experiments on small and large translation datasets to compare the impact of sentence-level versus fine-grained reward signals on translation quality. Our results show that training with token-level rewards improves translation quality across language pairs over baselines according to automatic and human evaluation. Furthermore, token-level reward optimization also improves training stability, evidenced by a steady increase in mean rewards over training epochs.","Figure 1: Two examples with the same sentence-level assessment but varying error severity and frequency. Neural machine translation (NMT) Kalchbrenner and Blunsom (2013); Sutskever et al. (2014); Cho et al. (2014), a leading approach within MT, leverages neural networks to automate language translation and has driven significant improvements in translation quality. However, most NMT systems are predominantly trained using maximum likelihood estimation (MLE). MLE-based training focuses on maximizing the probability of next-word predictions given a partial reference. This often leads to a critical problem known as exposure bias—the model uses ground-truth prefix tokens during training, but during inference, it relies on its previous predictions Bengio et al. (2015); Ranzato et al. (2016); Wiseman and Rush (2016). This can cause errors to propagate through the generated sequence, severely degrading translation quality. Furthermore, it tends to produce translations that lack global coherence and adequacy as the model does not sufficiently consider the context of entire sentences or the overarching meaning. This has spurred interest in using alternative approaches that leverage RL methods for training NMT systems. RL-based approaches use explicit reward models to evaluate the outputs generated by the NMT system, assigning scores to generated hypotheses to guide the learning process. However, current research Nguyen et al. (2017); Kreutzer et al. (2018a, b); Wu et al. (2018); Kiegeland and Kreutzer (2021) predominantly relies on sentence-level feedback and often struggles with reward sparsity, particularly for long-form text generation. Sentence-level rewards fail to capture specific issues within a translation, making it difficult for the model to learn from its mistakes. As shown in Figure 1, two translations corresponding to two different source texts of varying length receive the same sentence-level quality assessment score of 70, yet they differ significantly in the nature and the impact of the errors: the first translation has several minor errors scattered throughout the text, whereas the latter has major errors which could potentially hinder the understanding of the original content. This suggests that learning can be more effective if feedback is provided at a finer-grained level including precise identification of errors and their nature. Recent advancements in automated MT evaluation metrics, such as xCOMET Guerreiro et al. (2023), MetricX Juraska et al. (2023), AutoMQM Fernandes et al. (2023), and MaTESE Yuan et al. (2021), have shown promise in aligning automated assessments with human translation quality judgments. These metrics either directly predict token-level error severity (no error, minor, major, or critical) with sentence-level quality assessments, or prompt large language models to identify error types (e.g., mistranslation, omission) and severities based on the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014). Despite the potential of the severity-based metrics above to improve translation quality, their application in MT training via RL methods remains relatively underexplored, since it presents several challenges: (i) the feedback, albeit informative, is typically sparse and noisy, and (ii) determining the appropriate reward assignments for different severity levels to ensure effective and stable learning is not straightforward. In this regard, our research aims to answer the following questions: 1. Does fine-grained RL methods offer benefit over sentence-level feedback in improving translation quality and stabilizing training? 2. Can automatic fine-grained MT metrics like xCOMET be effectively used to provide accurate, detailed, human-aligned feedback to reduce reward sparsity? When answering these questions, we make the following contributions: 1. We propose employing a fine-grained evaluation metric, xCOMET, which increases reward density by providing frequent token-level rewards, thereby improving the robustness and stability of RL-based MT. 2. We introduce a new severity map to effectively use the reward signals, overcoming the limitations of standard MQM scoring as demonstrated in our experimental results. 3. We conduct experiments on English-to-German (EN\rightarrowDE), English-to-French (EN\rightarrowFR), German-to-English (DE\rightarrowEN), and French-to-English (FR\rightarrowEN) translation datasets, comparing the overall translation quality of NMT systems and showing that translation quality improves when employing xCOMET as a reward model. By integrating fine-grained reward signals into NMT training, we demonstrate significant improvements in translation quality and overcome the challenges of exposure bias, reward sparsity, and RL training stability, paving the way for more reliable and accurate MT systems."
https://arxiv.org/html/2411.05980v1,FactLens: Benchmarking Fine-Grained Fact Verification,"Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift toward fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens111Link to our data and code, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.","Large Language Models (LLMs) have proven to be powerful tools, demonstrating impressive capabilities in language generation and understanding Touvron et al. (2023); Brown et al. (2020). However, a well-known limitation of LLMs is their tendency to hallucinate, generating information that is factually incorrect or unsupported by evidence Ji et al. (2022); Lin et al. (2021). As LLMs become more widespread, especially in applications where factual accuracy is crucial, there has been increasing research on methods to verify the factuality of LLM-generated content as well as claims from other sources. Previous works on building fact-checking benchmarks focus on generating claims with a ground truth label, and in some cases provide the evidence/context to verify the claim. Aly et al. (2021); Schlichtkrull et al. (2023). Claims are generated using human annotators Aly et al. (2021), synthetic processes Bayat et al. (2023); Tang et al. (2024), or considering LLM outputs on Question-Answering tasks Wang et al. (2023). To increase the complexity of the fact-checking process, the claims are generated from source data of multiple domains & modalities, such as Wikipedia text and/or tables Thorne et al. (2018); Chen et al. (2019); Aly et al. (2021), Web Pages Schlichtkrull et al. (2023), Knowledge Graphs Kim et al. (2023), online posts/chats Wang et al. (2023); Li et al. (2023), and Q-A tasks from various domains such as statistics, finance, legal, etc Jacovi et al. (2024a). These works also provide baseline fact-checking pipelines, which typically involves two main stages: (1) the retrieval of relevant evidence using Search APIs and multimodal data-lakes Tang et al. (2023); Schlichtkrull et al. (2023) and (2) the verification of claims based on that evidence using NLI-based, LLM-based and fine-tuned fact-verification models Li et al. (2023). Some works also explore delegating these steps entirely to an LLM-based policy framework Li et al. (2023); Peng et al. (2023). Despite this structured pipeline, most existing methods rely on a holistic verification model, where complex claims are assigned a single factuality label, often obscuring the nuanced nature of the errors or inaccuracies in the claims. In this work, we echo the sentiments of Wang et al. (2023); Liu et al. (2019); Pan et al. (2023) for a shift towards fine-grained verification of complex claims, where claims are decomposed into smaller, more manageable sub-claims that can be individually verified. We additionally emphasise on the need to provide evaluation metrics to benchmark such fine-grained verification and enrich existing benchmarks with fine-grained verification labels. As shown in Figure 1, the benefits of fine-grained verification are substantial. By breaking down a complex claim into its constituent sub-claims, verification is more precise, allowing for pinpointing exact locations of factual inaccuracies. Additionally, this approach enables more transparent rationalizations and explanations, as each sub-claim can be linked directly to its corresponding evidence or lack thereof. Fine-grained decomposition also narrows the scope of evidence retrieval, making the subsequent verification process more focused and less prone to ambiguity. Achieving fine-grained verification, however, presents its own challenges. Decomposing a raw, complex claim into smaller sub-claims is not simply a matter of splitting it into sentences. Poorly constructed sub-claims can introduce a variety of issues: they may lose the context necessary for proper verification, lack atomicity, or misrepresent the original information by either omitting key details or introducing new ones. Ensuring the quality and verifiability of these sub-claims is therefore critical for the overall success of the verification process. To address these challenges, we introduce FactLens, a benchmark designed specifically for fine-grained fact verification. FactLens provides a novel suite of metrics for evaluating the quality of sub-claim generation and incorporates automated evaluators that combine LLM-based assessments with statistical metrics. The dataset has been manually curated to ensure high-quality sub-claims. Through empirical evaluation, we demonstrate that our sub-claim evaluators align closely with human judgments. Moreover, our end-to-end evaluation shows that these fine-grained scores correlate strongly with improved downstream verification performance. We also present the results of state-of-the-art models on sub-claim generation, revealing the challenges inherent in this task and the need for further research in this area. Figure 1: Examples of holistic fact verification (upper) failed to identify inaccuracies, whereas fine-grained verification (lower) clearly pinpointed the sources of error. In fine-grained verification, the FactLens evaluator can be used to assess individual sub-claims and identify any alarming signals that may suggest the need for human intervention or regeneration of the sub-claims."
https://arxiv.org/html/2411.05978v1,The Empirical Impact of Data Sanitization on Language Models,"Data sanitization in the context of language modeling involves identifying sensitive content, such as personally identifiable information (PII), and redacting them from a dataset corpus. It is a common practice used in natural language processing (NLP) to maintain privacy. Nevertheless, the impact of data sanitization on the language understanding capability of a language model remains less studied. This paper empirically analyzes the effects of data sanitization across several benchmark language-modeling tasks including comprehension question answering (Q&A), entailment, sentiment analysis, and text classification. Our experiments cover a wide spectrum comprising finetuning small-scale language models, to prompting large language models (LLMs), on both original and sanitized datasets, and comparing their performance across the tasks. Interestingly, our results suggest that for some tasks such as sentiment analysis or entailment, the impact of redaction is quite low, typically around 1-5%, while for tasks such as comprehension Q&A there is a big drop of >25% in performance observed in redacted queries as compared to the original. For tasks that have a higher impact, we perform a deeper dive to inspect the presence of task-critical entities. Finally, we investigate correlation between performance and number of redacted entities, and also suggest a strategy to repair an already redacted dataset by means of content-based subsampling. Additional details are available at https://sites.google.com/view/datasan.","Data privacy is a critical concern in the development and use of language models (LMs) specially due to the sensitive nature of personally identifiable information (PII) that can be present in the text. PII commonly includes sensitive information such as person names, addresses, emails, or social security numbers. Data privacy concerns are in part motivated by security issues that arise from LMs memorizing portions of the training data, which can then be extracted via adversarial attacks [4, 26]. PII data breaches are a serious concern for large corporations, as they can lead to severe damage to the reputation and finances of an organization. Furthermore, corporate data governance policies are driven by applicable privacy laws which place strict legal limitations on the use of PII. Some popular techniques to anonymize data in the Natural Language Processing (NLP) domain include differential privacy and data sanitization. Differential privacy [10] involves development of a mathematical framework that adheres to a rigorous definition of privacy by injecting noise into the data. While this guarantees that a trained model will not reveal any user identifiable information, adding noise comes with the price of great loss in data fidelity, which is not ideal for studying the impact of anonymization on model performance [23]. On the other hand, data sanitization involves complete and irreversible removal of personally identifiable information from data without the introduction of additional noise. Such a masking approach ensures that information pertaining to an individual cannot be recovered, either directly or in collaboration with a third party. In recent years, a number of organizations like Microsoft, Paypal, and Mastercard [2, 13, 38] have employed sanitized data for training LMs to leverage information present in their free text corpora while minimizing data leakage and privacy violations. Therefore, in this work, we adopt the data sanitization approach for analyzing impact on performance of language models via redaction. Despite the wide adoption of data sanitization methods for protecting sensitive information, the exact impacts of redacting PII content from natural language data on the performance of language models has not been studied in-depth to the best of the authors’ knowledge. Making inferences based on context is core to how language models function, and therefore stripping away contextual identifiers will likely reduce a model’s ability to comprehend text, thereby leading to a decrease in performance. Additionally, replacing diverse PII with generic tokens, for instance replacing two different names with the same <NAME> token introduces ambiguity, making it harder for the model to differentiate between unique entities. An example of how redacting PII impacts a large language model’s (LLM’s) thinking process is depicted in Figure 1. Figure 1: LLM chain-of-thought for a query in the original vs redacted forms. In the redacted case, the reasoning module correctly highlights missing information in the query, and is therefore unable to provide the answer to an otherwise straight-forward question. This work aims to perform an empirical analysis of the impact of data sanitization on the language understanding capabilities of both small and large language models. Our main contributions include: • We perform a number of ablation experiments by fine-tuning small models like BART [22] and GPT-2 [30] across several benchmark NLP tasks to better understand the impact of PII redaction. • We also conduct prompting experiments with popular large language models like Claude 3.5 Sonnet [1], Mistral 7B [17], and GPT-4o [27] to investigate the impact of PII redaction on some common generative artificial intelligence (GenAI) datasets. This analysis is first done using a full set of redacted entities, and subsequently on a limited set by not redacting task-critical entities. • Finally, we investigate the correlation between task performance and number of named entities being redacted by using different sampling techniques, and suggest a strategy to best utilize an already redacted dataset without compromising on accuracy."
https://arxiv.org/html/2411.05958v1,Sentiment Analysis of Cyberbullying Data in Social Media,"Social media has become an integral part of modern life, but it has also brought with it the pervasive issue of cyberbullying a serious menace in today’s digital age. Cyberbullying, a form of harassment that occurs on social networks, has escalated alongside the growth of these platforms. Sentiment analysis holds significant potential not only for detecting bullying phrases but also for identifying victims who are at high risk of harm, whether to themselves or others. Our work focuses on leveraging deep learning and natural language understanding techniques to detect traces of bullying in social media posts. We developed a Recurrent Neural Network with Long Short-Term Memory (LSTM) cells, using different embeddings. One approach utilizes BERT embeddings, while the other replaces the embeddings layer with the recently released embeddings API from OpenAI. We conducted a performance comparison between these two approaches to evaluate their effectiveness in sentiment analysis of Formspring Cyberbullying data. Our Code is Available at : https://github.com/ppujari/xcs224u","In recent years, social media has become an integral part of daily life, facilitating communication and connection on an unprecedented scale. However, it has also contributed to a rise in cyberbullying harassment conducted online (Feinberg and Robey, 2009). Unlike traditional bullying, cyberbullying can happen anytime, with online anonymity encouraging perpetrators to act without facing immediate consequences (Hasan et al., 2023). This anonymity often leads to higher rates of cyberbullying compared to in-person bullying. Bullying is recognized as a major health issue by institutions like the American Academy of Pediatrics, has become especially concerning in educational settings (Xu et al., 2012). The tragic case of Megan Meier, a young victim of harassment on MySpace, highlights the devastating impact of cyberbullying Vines (2015). Given the growing concern in society and workplaces, and the fact that studies show many students face cyberbullying Li (2006); Cross (2008), detecting and preventing cyberbullying promptly is essential. In this work, we use Sentiment Analysis (SA) to classify texts as positive or negative with respect to cyberbullying. While traditional embeddings used in Recurrent Neural Networks (RNNs) are effective in capturing sequential patterns, they struggle with contextual dependencies in longer sentences and have limitations in understanding complex language structures Elman (1990); Pascanu (2013).In contrast, we incorporate state-of-the-art Large Language Model (LLM) embeddings, specifically BERT embeddings (Kenton and Toutanova, 2019), and use the OpenAI API to obtain embeddings that leverage self-attention mechanisms for sentiment analysis. These models excel in capturing intricate language features, making them highly effective for a range of downstream tasks Radford et al. (2019); Raffel et al. (2020). These embeddings offer a deeper contextual understanding compared to earlier models, allowing for a more nuanced detection of harmful content and a more precise evaluation of sentiment polarity. The task involves identifying sentences containing bullying tokens, assessing their polarity, and accurately classifying them into the relevant sentiment categories. The development of such Natural Language Processing (NLP) algorithms, however, requires high-quality annotated data to measure performance accurately. Many popular Machine Learning (ML) techniques, especially Deep Neural Networks (DNNs), need large, annotated corpora to achieve high-quality classification results. Unfortunately, the availability of publicly accessible datasets for cyberbullying detection is limited. In this work, we use Kaggle’s Formspring data Pujari (2022); Reynolds et al. (2011) for Cyberbullying Detection to train and evaluate our models. We apply the advanced LLM-based techniques and assess the model’s performance in identifying cyberbullying. In this work, we made the following contributions: • Introduce two distinct hybrid methods for sentiment analysis in cyberbullying detection, leveraging advanced embeddings instead of traditional techniques. • Utilizes BERT embeddings with an RNN framework in one method and OpenAI embeddings with an RNN framework in another, specifically for sentiment analysis of cyberbullying data, aiding in cyberbullying detection. • Compare the performance of the proposed hybrid architectures using BERT and OpenAI embeddings for sentiment analysis on the Formspring dataset."
https://arxiv.org/html/2411.05945v1,": Toward Post Recognition Generative Correction 
Large Language Models with Task-Oriented Experts","Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an “expert” of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset’s tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.","Figure 1: Proposed NeKo, a new form multi-task model to boost post-recognition results over speech, text, and visual inputs. NeKo could work for (i) post automatic speech recognition (ASR) correction, (ii) post speech translation (ST) and machine translation (MT) correction, and (iii) post optical character recognition (OCR) correction. NeKo discover new state-of-the-art results in (iv) zero-shot ASR correction and performs competitively as a general-purpose (v) multi-task corrector. Human recognition (Biederman, 1987; Juang and Furui, 2000; Kanwisher et al., 1996) capabilities span multiple modalities, including speech recognition, visual patterns, and extensions to semantic and textual interpretations. These faculties, however, are not infallible and often incorporate mis-recognition errors. Despite these imperfections, humans efficiently communicate using speech, language, or facial expressions. For instance, two non-native speakers (Lev-Ari, 2015; Valaki et al., 2004) can often achieve mutual understanding through this imperfect recognition and subsequent interpretative processes, even when the conversation is marred by lexical inaccuracies and subdued accents. In other words, humans (as intelligent agents) exhibit a robust capacity for generative understanding Jiang et al. (2020); Cheng et al. (2021) that extends beyond initial recognition results. In neuroscience Zatorre and Gandour (2008), the inferior temporal gyrus and the temporal lobe are not confined to rudimentary perception but are also integral to the post-recognition processes that facilitate semantic understanding of language (Levinson and Evans, 2010), speech (Marshall et al., 2015), and visual patterns (Vink et al., 2020). This form of “post-recognition correction,” exemplified by the application of language modeling (LM) to initial recognition outputs, has been introduced to the field for both acoustic (automatic speech recognition, ASR) and visual (optical character recognition, OCR) modalities since the early explorations (Jelinek, 1976; Dixon and Silverman, 1975) of learning algorithms in 1970s. The most prevalent approaches to utilizing LMs for post-recognition boosting are predominantly ranking or retrieval-based. In these setups, the LM is tasked with ranking and scoring (Ljolje et al., 1999) the top n-best hypotheses generated by the first-pass recognition system. This process often incorporates “discriminative modeling” algorithms (Sukkar and Lee, 1996; Mangu et al., 2000) and representation embeddings, such as BERT (Salazar et al., 2020; Kenton and Toutanova, 2019), to minimize the word error rate (WER) during training (Prabhavalkar et al., 2018; Mangu et al., 2000). With the LMs scaling up to LLMs (Brown et al., 2020), recent efforts (Chan et al., 2023; Yang et al., 2023; CHEN et al., 2023; Hu et al., 2024a) have focused on exploring a “generative modeling” for post-recognition correction. This generative error correction (GER) approach uses LLMs to conduct final recognition from given first-pass text-based predictions from recognition models, including ASR, image captioning (IC), and machine translation (MT). This cascaded two-agents text-to-text GER model has outperformed larger single multi-modal and multi-task models in these tasks. Meanwhile, these GER solutions heavily depends on domain-specific fine-tuning processes (Chen et al., 2024a) that utilize parameter-efficient components, which often suffers a performance degradation from a lack of generalizability across different datasets, domains, and tasks. In other words, how to design and further openly provide a “general-purpose post (every) recognition correction model” is still one undiscovered and crucial topic within the research community. On the other hand, directly fine-tuning LLMs on a mixture of diverse error correction datasets can lead to suboptimal performance (CHEN et al., 2023; Lange et al., 2022) due to differences in input modalities, output formats, error types, and domain characteristics. For example, ASR errors stem from phonetic similarities or acoustic ambiguities, while OCR errors involve visual or character-level confusions. Additionally, error distributions can vary widely across datasets, even within the same task. To characterize “model generalization,” mixture-of-experts (MoE) (Jiang et al., 2024a) has emerged as a promising approach for multi-task learning, consisting of of a set of expert networks and a gating network that learns to route the input to the most appropriate expert(Sukhbaatar et al., 2024). This enables MoE models to learn more specialized and fine-grained representations compared to monolithic models. However, most MoE models are designed for general-purpose language modeling(Dai et al., 2024), with experts not explicitly assigned to specific tasks, but rather learn to specialize in different aspects of the input space through data-driven training, expect for a recent vision work (Ye and Xu, 2023). Effectively leverage MoE for multi-task error correction, where the experts need to capture task-specific features while allowing knowledge sharing, remains an open question. In this work, we propose NeKo, a “geNerative multi-tasK error correction” approach that leverages a pre-trained MoE model to drive diverse tasks and cross-domain knowledge, as shown in Figure 1. The key idea is to continuously pre-train MoE model on a mixture of error correction datasets, with each expert specializing in a specific domain. This task-oriented MoE fine-tuning approach enables the experts to capture task-specific features while allowing knowledge sharing through the router (Dai et al., 2024). NeKo captures the nuances of each task, benefiting from shared knowledge across experts. Evaluated on tasks such as ASR, ST, OCR, and unseen textual error correction (TEC), NeKo consistently outperforms baseline models, including Claude-Opus and GPT-3.5. It achieves state-of-the-art WER reduction on the Hyporadise benchmark and large-scale Open ASR Leaderboard (Srivastav et al., 2023). NeKo also significant improves in OCR error correction. Further analysis confirms its robust multi-task capabilities. In summary, the main contributions of this work include: 1. We introduce NeKo, a multi-task error correction LLM that leverages task-oriented mixture-of-experts for diverse post-recognition correction tasks. To the best of our knowledge, this is the first work that explores the use of MoE for multi-task error correction. 2. NeKo has been studied under a new form of cross-modalities post-recognition correction evaluation, serving as strong open-source ASR, ST, OCR, and TEC baselines. Our results show that NeKo discovers new state-of-the-art performance in ASR as a multi-task error correction model. 3. We discovered emergent abilities for cross-task correction from NeKo as a first-of-its-kind multi-task correction approach toward a general-purpose post-recognition LM designs. 4. The NeKo models, newly created source datasets, and training processes are scheduled to open source under the CC BY-SA 4.0 license to support reproducibility and to encourage future research."
https://arxiv.org/html/2411.05930v1,BERTrend: Neural Topic Modeling for Emerging Trends Detection,"Detecting and tracking emerging trends and weak signals in large, evolving text corpora is vital for applications such as monitoring scientific literature, managing brand reputation, surveilling critical infrastructure and more generally to any kind of text-based event detection. Existing solutions often fail to capture the nuanced context or dynamically track evolving patterns over time. BERTrend, a novel method, addresses these limitations using neural topic modeling in an online setting. It introduces a new metric to quantify topic popularity over time by considering both the number of documents and update frequency. This metric classifies topics as noise, weak, or strong signals, flagging emerging, rapidly growing topics for further investigation. Experimentation on two large real-world datasets demonstrates BERTrend’s ability to accurately detect and track meaningful weak signals while filtering out noise, offering a comprehensive solution for monitoring emerging trends in large-scale, evolving text corpora. The method can also be used for retrospective analysis of past events. In addition, the use of Large Language Models together with BERTrend offers efficient means for the interpretability of trends of events.","The concept of weak signals, introduced by Ansoff (1975), refers to early indicators of emerging trends that can have significant implications across various domains. These include events like shifts in public opinion in social trends, early disruptive technologies in innovation, changes in activist groups and public sentiment in politics, and potential disease outbreaks in healthcare. Monitoring and analyzing weak signals offers valuable insights for organizations, researchers, and decision-makers, aiding in informed decision-making. Key data sources for identifying these trends include large text corpora such as news, social media, research and technology journals or reports. The challenges are: distinguishing meaningful weak signals from irrelevant noise, dealing with context ambiguity, and tracking the extended period over which weak signals may gain significance. With advances in NLP and AI, researchers have developed various techniques to detect weak signals across different fields,including statistics-based methods, graph theory, machine learning, semantic-based approaches, and expert knowledge. However, most solutions fall short in fully addressing the challenge of detecting emerging trends Rousseau et al. (2021), either by relying solely on keyword-based analysis, which misses contextual nuances, or by being static and unable to dynamically track evolving weak signals. In this work, we introduce BERTrend, a novel framework for detecting and monitoring emerging trends and weak signals in large, evolving text corpora. BERTrend leverages neural topic modeling, specifically BERTopic, in an online learning setting to identify and track topic evolution over time. Its key contribution lies in dynamically classifying topics as noise, weak signals, or strong signals based on their popularity trends. The proposed metric quantifies topic popularity over time by considering both the number of documents within the topic and its update frequency, incorporating an exponentially growing decay if no updates occur for an extended period. By combining neural topic modeling with a dynamic popularity metric and adaptive classification thresholds, BERTrend provides a comprehensive solution for detecting and monitoring emerging trends in large-scale, evolving text corpora. We discuss the qualitative results on two comprehensive datasets, including the overall evolution of trends and specific case studies. Combined with Large Language Models (LLMs), the method an efficient way of interpreting the detected trends of events through various dimensions indicating how they evolve over time."
https://arxiv.org/html/2411.05928v1,"Reducing Distraction in Long-Context Language Models 
by Focused Learning","Recent advancements in Large Language Models (LLMs) have significantly enhanced their capacity to process long contexts. However, effectively utilizing this long context remains a challenge due to the issue of distraction, where irrelevant information dominates lengthy contexts, causing LLMs to lose focus on the most relevant segments. To address this, we propose a novel training method that enhances LLMs’ ability to discern relevant information through a unique combination of retrieval-based data augmentation and contrastive learning. Specifically, during fine-tuning with long contexts, we employ a retriever to extract the most relevant segments, serving as augmented inputs. We then introduce an auxiliary contrastive learning objective to explicitly ensure that outputs from the original context and the retrieved sub-context are closely aligned. Extensive experiments on long single-document and multi-document QA benchmarks demonstrate the effectiveness of our proposed method.","Figure 1: Our method. Retrieval-based data augmentation: we filter out the distracting content from a document D’ using a retriever, retaining only the top-k relevant chunks. The irrelevant portions are replaced with the <mask> tokens. Contrastive Training: taking D_{1} as an example, an augmented D^{\prime}_{1} is considered a positive pair with D_{1} (solid line), whereas the augmented versions of other documents D^{\prime}_{2},\cdots,D^{\prime}_{N} serve as negative pairs (dashed line) for D_{1}. Large language models (LLMs), such as the GPT series (Brown et al., 2020), have established a new paradigm in natural language processing, showcasing exceptional versatility across various tasks (Brown et al., 2020; Wei et al., 2022). Efforts to enhance the contextual capabilities of LLMs have primarily focused on techniques like context extension fine-tuning Chen et al. (2023a, b); Ding et al. (2023), or retrieval augmented generation Lewis et al. (2020); Xu et al. (2023); Gao et al. (2024). Despite these advancements, LLMs often struggle to effectively utilize extended contexts, frequently encountering the distraction issue (Liu et al., 2023). This problem arises when LLMs are easily distracted by irrelevant information within a long context. The distraction issue presents a significant challenge in practical applications, especially in long-context question answering (QA) tasks (Pang et al., 2022; Dasigi et al., 2021). In these scenarios, the relevant information required to answer a question is often buried within lengthy texts. For example, the answer to a question may depend on a small segment of a long document. However, LLMs typically process input contexts holistically Vaswani et al. (2017), leading to an over-generalized distribution of attention across all tokens, which diminishes the model’s ability to focus on the most relevant information. One commonly considered solution is the utilization of a retriever during inference (Guu et al., 2020; Lewis et al., 2020), where relevant information is extracted by the retriever as filtered input to enhance the LLMs’ focus on essential sub-contexts Xu et al. (2023). However, crucial information may sometimes be excluded from the retrieved content due to the imperfections of retrievers. Such shortcomings in retrieval can lead to significant compounding errors or hallucinations in the generated responses (Shi et al., 2023a; Liu et al., 2023). In this study, we propose a novel training method to enhance long-context LLMs’ inherent ability to focus on the relevant segments related to a specific question. Our technique integrates the “focusing ability” of a retriever with relatively shorter context length, into long-context LLMs through retrieval-based data augmentation and contrastive learning. Our approach eliminates the need for a separate retriever during inference, effectively addressing the issue of distraction. As shown in Figure 1, our method contains two key ingredients: 1) Retrieval-based data augmentation: For each example, we generate an augmented input by retaining only the top-k retrieved segments associated with the question, masking irrelevant information with a special token. 2) Contrastive learning: We apply a contrastive learning (Chen et al., 2020; Radford et al., 2021b) objective to enforce closer sequence representations of the original and its retrieval-augmented sample. This approach leverages the semantic equivalence of the retrieval-augmented sample to the original long context given the specific question, guiding the model to concentrate on the most relevant sub-context of a long input. We validate our method using the Mistral-7B model (Jiang et al., 2023), employing low-rank adaptation (LoRA) (Hu et al., 2022) for efficient fine-tuning. Comprehensive results on two long single-document QA tasks (i.e., Qasper (Dasigi et al., 2021) and QuALITY (Pang et al., 2022)) and a long multi-document QA task (Liu et al., 2023) demonstrate that our method, with just a few hundred fine-tuning steps, significantly reduces distraction-induced errors, outperforming both standard training methods and retrieval-augmented inference techniques."
https://arxiv.org/html/2411.05895v1,One Small and One Large for Document-level Event Argument Extraction,"Document-level Event Argument Extraction (EAE) faces two challenges due to increased input length: 1) difficulty in distinguishing semantic boundaries between events, and 2) interference from redundant information. To address these issues, we propose two methods. The first method introduces the Co and Structure Event Argument Extraction model (CsEAE) based on Small Language Models (SLMs). CsEAE includes a co-occurrences-aware module, which integrates information about all events present in the current input through context labeling and co-occurrences event prompts extraction. Additionally, CsEAE includes a structure-aware module that reduces interference from redundant information by establishing structural relationships between the sentence containing the trigger and other sentences in the document. The second method introduces new prompts to transform the extraction task into a generative task suitable for Large Language Models (LLMs), addressing gaps in EAE performance using LLMs under Supervised Fine-Tuning (SFT) conditions. We also fine-tuned multiple datasets to develop an LLM that performs better across most datasets. Finally, we applied insights from CsEAE to LLMs, achieving further performance improvements. This suggests that reliable insights validated on SLMs are also applicable to LLMs. We tested our models on the Rams, WikiEvents, and MLEE datasets. The CsEAE model achieved improvements of 2.1%, 2.3%, and 3.2% in the Arg-C F1 metric compared to the baseline, PAIE (Ma et al. 2022). For LLMs, we demonstrated that their performance on document-level datasets is comparable to that of SLMs 222All code is available at https://github.com/simon-p-j-r/CsEAE.","Figure 1: An EAE instance from the WikiEvents dataset. Event Argument Extraction (EAE) aims to extract structured event information composed of arguments corresponding to event roles from text (Peng et al. 2024). As shown in Figure 1, given a trigger and event type, along with a predefined list of roles for the event type, the model needs to extract the corresponding token spans as arguments for each role. This structured information can enhance the performance of downstream tasks such as question answering (Costa, Gottschalk, and Demidova 2020), dialogue systems (Zhang, Chen, and Bui 2020), and recommendation systems (Li et al. 2020). As the length of document-level input texts increases, document-level EAE faces two critical challenges: (1) difficulty in distinguishing semantic boundaries between events (He, Hu, and Tang 2023). As shown in Figure 1, the four trigger words crashed, stabbed, shot, and killed, each trigger four events. The argument distribution of these events is extremely dense, and different events can share the same token span as arguments corresponding to different roles. These dense and overlapping events make the semantic boundaries between them blurry. (2) The volume of information received by the model increases significantly; however, this information includes not only useful data for the extraction task but also a large amount of redundant information that interferes with the task (Xu et al. 2022). For example, in the sentence [5], the presence of person nouns such as man, female and soldier can mislead the extraction of the Victim role for the Life.Die.Unspecified event triggered by killed. However, previous work has not simultaneously addressed both of these issues (Ma et al. 2022; Xu et al. 2022; He, Hu, and Tang 2023). Figure 2: Overview of CsEAE. The yellow attention represents the concatenation of co-occurrences-aware module, while the blue attention represents the concatenation of structure-aware module. To address these issues, we proposed two methods, with the first being the co and structure EAE model (CsEAE) based on Small Language Models (SLMs). CsEAE enhances the boundaries of the model’s focus from both event and sentence perspectives. From the event perspective, to help the model capture semantic boundaries between events, we introduced a co-occurrence-aware module. This module identifies all co-occurring events in the input by marking triggers and encoding related prompts. From the sentence perspective, while event mentions are document-level, event information is often within a single sentence. For instance, in the WikiEvents dataset, over 94% of arguments are in the same sentence as the trigger; in the Rams dataset, over 82%; and in the MLEE dataset, over 99%. This highlights the importance of the information in the trigger sentence for the extraction task. To emphasize this, we structured the knowledge around the trigger sentence and its relationship with other sentences in the document. This approach helps the model selectively gather relevant information from other sentences, reducing distractions from redundant information. Additionally, we proposed a second method based on Large Language Models (LLMs). We designed prompts tailored to LLMs for each dataset and performed Supervised Fine-Tuning (SFT) on the LLMs. This approach addresses a gap in the EAE field, which previously lacked fine-tuned LLMs (Ma et al. 2023; Chen et al. 2024; Zhou et al. 2023). Inspired by the use of large-scale high-quality data for continuous pretraining (Yang et al. 2024), we attempted multi-dataset fine-tuning to make the LLMs more familiar with event extraction tasks. On this basis, we also conducted enhanced training on the LLMs using additional datasets. Finally, inspired by CsEAE, where co-occurrence- and structure-aware interactions enhance the model’s ability to capture event boundaries and reduce interference from redundant information, we applied these insights to LLMs. This led to further performance improvements and introduced a novel perspective: the reliable insights validated on SLMs are also applicable to LLMs.Our contributions are summarized below: \bullet We propose the CsEAE model, which incorporates a co-occurrences-aware module to capture semantic boundaries between events. Additionally, it uses a structure-aware module to build structured perception information, allowing the model to minimize interference from redundant information. \bullet We designed different prompts for various datasets and further used SFT to enhance the performance of LLMs. Additionally, we proposed multiple datasets SFT and supplementary dataset enhancement training, which led to even better performance. \bullet We applied insights from SLMs to LLMs, resulting in further performance improvements. This shows that reliable insights validated on SLMs are also effective for LLMs."
https://arxiv.org/html/2411.05872v1,Dialectal Coverage And Generalization in Arabic Speech Recognition,"Developing robust automatic speech recognition (ASR) systems for Arabic, a language characterized by its rich dialectal diversity and often considered a low-resource language in speech technology, demands effective strategies to manage its complexity. This study explores three critical factors influencing ASR performance: the role of dialectal coverage in pre-training, the effectiveness of dialect-specific fine-tuning compared to a multi-dialectal approach, and the ability to generalize to unseen dialects. Through extensive experiments across different dialect combinations, our findings offer key insights towards advancing the development of ASR systems111The code with pre-trained & finetuned model weights is available on GitHub: https://github.com/mbzuai-nlp/ArTST for pluricentric languages like Arabic.","Figure 1: The architecture of SpeechT5/ArTST, which contains an encoder-decoder module and six modal specific pre/post-nets. During self-supervised pre-training (left), quantized tokens are shared across speech and text modalities. Hidden states and latent units are mixed up and used as the inputs of the cross-attention module in the decoder. The fine-tuning stage for ASR is shown on the right. Refer to Ao et al. (2021) for more details. The advent of large self-supervised acoustic models has transformed speech technology, enabling transfer learning and improving performance for both high-resource and low-resource languages. Prominent examples of such models include various versions of wav2vec Schneider et al. (2019); Baevski et al. (2020), HuBERT Hsu et al. (2021), and SpeechT5 Ao et al. (2021), which have predominantly been trained on English datasets. Their multi-lingual variants, e.g. XLS-R Babu et al. (2021) with 53 and 128 languages, in addition to many models that include both self-supervised and supervised pre-training, such as Whisper Radford et al. (2023) with approximately hundred supported languages, MMS Pratap et al. (2024) with thousands of languages, and UniSpeech Wang et al. (2021), underscore the potential for cross-lingual transfer learning for more inclusive ASR. Yet, while these models indeed show great potential for transfer learning to new languages, even those unseen in training Huang et al. (2013), they remain sub-optimal for specific target languages. A case in point is the Arabic Text and Speech Transformer (ArTST), a model pre-trained exclusively on Arabic, which has demonstrated superior performance for Modern Standard Arabic (MSA), surpassing larger multilingual models like Whisper and MMS in benchmark tests, in addition to establishing a new state-of-the-art (SOTA) performance compared to previous efforts for Arabic ASR. This highlights the advantage of monolingual pre-training when large amounts of unlabeled data for the target language are available. While the model showed some potential for dialectal coverage, it was trained and validated mainly on MSA data, which questions its applicability for spoken dialectal variants of Arabic. Arabic is a pluricentric language Schuppler et al. (2024), diverse in regional variations, and models trained on MSA frequently struggle to adapt to these variations. This limitation is particularly acute given that many Arabic dialects are underrepresented or considered low-resource in speech technology research. Consequently, there is a need for optimized ASR systems that embrace, rather than overlook, the linguistic diversity of the Arabic-speaking world. In light of these challenges, we conduct various investigations aimed at understanding and enhancing the dialectal diversity and performance of Arabic ASR systems. We focus on three inquiries aimed at optimizing potential strategies for integrating dialectal variation into ASR systems. First, we measure the impact of incorporating a broad collection of Arabic dialects during the model’s pre-training phase. We hypothesize that a wider dialectal foundation could improve the model’s performance across various dialects in the fine-tuning stage. Second, we quantify the comparative effectiveness of dialect-specific fine-tuning versus a more holistic, multi-dialectal fine-tuning strategy. The third question examines the model’s capacity for zero-shot transfer to dialects not explicitly included in fine-tuning. Our key findings from experiments spanning over 12 Arabic dialects are: (1) pre-training with more data and wider dialectal coverage improves performance across most dialectal variants, including MSA, (2) multi-dialectal fine-tuning improves performance for low-resource dialects, but may not be optimal for high-resource dialects, and (3) multi-dialectal pre-training and fine-tuning has higher potential for zero-shot transfer to unseen dialects. Our pre-training checkpoints and joint models are trained exclusively on open-source data and will be released as open-source models."
https://arxiv.org/html/2411.07224v1,TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on Pre-trained Language Models,"With the widespread of digital environments, reliable authentication and continuous access control has become crucial. It can minimize cyber attacks and prevent frauds, specially those associated with identity theft. A particular interest lies on keystroke dynamics (KD), which refers to the task of recognizing individuals’ identity based on their unique typing style. In this work, we propose the use of pre-trained language models (PLMs) to recognize such patterns. Although PLMs have shown high performance on multiple NLP benchmarks, the use of these models on specific tasks requires customization. BERT and RoBERTa, for instance, rely on subword tokenization, and they cannot be directly applied to KD, which requires temporal-character information to recognize users. Recent character-aware PLMs are able to process both subwords and character-level information and can be an alternative solution. Notwithstanding, they are still not suitable to be directly fine-tuned for KD as they are not optimized to account for user’s temporal typing information (e.g., hold time and flight time). To overcome this limitation, we propose TempCharBERT, an architecture that incorporates temporal-character information in the embedding layer of CharBERT. This allows modeling keystroke dynamics for the purpose of user identification and authentication. Our results show a significant improvement with this customization. We also showed the feasibility of training TempCharBERT on a federated learning settings in order to foster data privacy.","Figure 1: Keystroke metrics based on pressing and releasing timestamps, including latency interval, dwell (or hold) time and flight time. In this work, we propose TempCharBERT, an architecture based on pre-trained language models (PLMs) and customized for KD. Although PLMs have been providing impressive results on multiple NLP benchmarks, the use of such models for specific tasks is not always straightforward and often requires some level of customization. One particular concern, for instance, is the tokenization granularity, which can be language sensitive, dependent of resource availability and can ultimately impact the performance of downstream tasks [1]. BERT and RoBERTa-based models, for example, use wordpiece tokenization and focus on processing subword inputs, rather than word or character-level tokens. This is not suitable for specific tasks, such as KD, which relies significantly on processing temporal information from each key used while typing. Thus, these models provide low performance if directly applied to keystroke dynamics. Recent character-aware PLMs, such as CharBERT [2] and CharacterBERT [3], offer an alternative as they can process both subwords and character-level information. These models, however, are still not suitable to be directly fine-tuned for keystroke dynamics as they are not optimized to account for user’s temporal typing information (e.g., dwell time and flight time111The concepts of dwell time and flight time are explained in section 2.). To address this limitation, we propose a modification on the CharBERT architecture. This change aims at incorporating keystroke metrics into the embedding layer of CharBERT. We show that these temporal-character information are enough to enhance the representation of user typing pattern, leading to significant improvement in terms of accuracy for user identification as well as in terms of equal error rate (EER) for user authentication. We also investigated the feasibility of training the proposed TempCharBERT in the Federated Learning settings to foster user data privacy. We found a small decay in the performance when compared to the centralized training settings. Thus, our contribution is summarized as follow: • We propose TempCharBERT as a new variant of CharBERT for keystroke dynamics. The originality of TempCharBERT stems from the temporal typing dynamic, which is crucial for capturing typing style. • We evaluate the proposed model on two important tasks: user identification and user authentication. Our results suggest a significant improvement in terms of accuracy and equal error rate compared to the pure CharBERT and other baseline approaches. • We show that the representation based on our customized embedding layer carries out meaningful user discriminative information. • We also show that the representation attained can be successfully used on other architectures, such as the Long-Short term Memory (LSTM). • To foster privacy, we show the feasibility of training TempCharBERT on the Federated Learning (FL) settings. The remainder of this paper is organized as follows. Section 2 presents background material and related works. Section 3 describes the proposed method, while Section 4 presents the experimental setup. Section 5 presents the experimental results. Section 6 discuss the paper limitation and Section 7 concludes the paper with final considerations."
https://arxiv.org/html/2411.07133v2,Stronger Models are NOT Stronger Teachers for Instruction Tuning,"Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models’ Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.","Instruction tuning (Figure 1) has been widely adopted to tailor the behavior of base Large Language Models (LLMs) to align with specific tasks and user intents (Zhang et al., 2023). This approach leverages instruction datasets, consisting of samples pairing an instruction with a corresponding response. The success of instruction tuning depends on the availability of high-quality instruction datasets. Initially, constructing these datasets required large human effort in generating and curating instruction-response pairs (Databricks, 2023; Zheng et al., 2024; Zhao et al., 2024), which is time-consuming and labor-intensive Liu et al. (2024b). To reduce the reliance on human-curated datasets, synthetic datasets generated by LLMs have surfaced as a viable solution Adler et al. (2024). Recent works, such as (Sun et al., 2023; Taori et al., 2023; Wang et al., 2023; Xu et al., 2024; Chen et al., 2024), have shown the strong potential of synthetic datasets in instruction tuning. While current research has primarily focused on using LLMs to create large, diverse, and high-quality instructions (Liu et al., 2024b), the selection of appropriate LLMs for generating corresponding responses remains largely unexplored. The common approach is to employ top-performing models (e.g., those leading on benchmarks (Fourrier et al., 2024; Chiang et al., 2024)) for response generation in instruction tuning. For instance, Llama-3.2-3B-Instruct uses responses generated by Llama-3.1-405B-Instruct (i.e., the largest model in Llama-3.1 family) for instruction tuning Meta (2024b). Additionally, most of the existing open synthetic datasets Teknium (2023); Xu et al. (2023a); Ding et al. (2023); Gallego (2023); Chen et al. (2024) depend on expensive, closed-source models like GPT-4 Achiam et al. (2023) and Gemini (Google, 2024) to produce responses. Figure 1: This figure demonstrates the process of instruction tuning and the scope of this paper. Is it always better to use the larger or stronger models as teachers? In this paper, we investigate the choice of the teacher model that generate responses during synthetic dataset generation, which we refer to as response generators, influence the instruction-following performance of the instruction-tuned LLMs. Specifically, given a base model and a set of high-quality instructions, we investigate the following research questions: RQ1: Which models are the most effective response generators for instruction tuning? To answer RQ1, we conduct extensive experiments with five base models, and fine-tune them on datasets generated by 20 response generators across seven model families: Qwen2, Qwen2.5, Llama 3, Llama 3.1, Gemma 2, Phi-3, and GPT-4. Our findings challenge common assumptions in the field, revealing a surprising result which we term the Larger Models’ Paradox: larger response generators (e.g., Llama-3.1-405B-Instruct) do not always enhance a base model’s instruction-following capabilities compared to their smaller counterparts within the same model family (e.g. Llama-3.1-70B-Instruct). Moreover, we find that open-source models (e.g., Gemma-2-9b-it and Qwen2.5-72B-Instruct) outperform GPT-4 as response generators. These findings question established practices and suggest more efficient and accessible approaches to create high-quality instruction datasets. To further explore the Larger Models’ Paradox, we investigate statistical metrics to reveal potential factors influencing the effectiveness of response generators given different base models. Here, we pose our second research question: RQ2: How can we determine the most effective response generators for a certain base model without instruction tuning? This question is crucial due to the significant computational costs associated with instruction tuning across multiple datasets generated by diverse response generators. Our investigation reveals that existing metrics in alignment data selection, including quality (Dubey et al., 2024), difficulty (Li et al., 2024c), and response length (Liu et al., 2023), fail to consider the compatibility between the base model being fine-tuned and the response generator, thus results in their inability to explain the Larger Models’ Paradox. To bridge this gap, we formulate the task of finding the most effective response generators as a risk-return problem. We solve this by calculating an Compatibility-Adjusted Reward (CAR), where compatibility serves as the risk factor. This compatibility is quantified by the average loss of responses on the base model being fine-tuned, with higher average loss indicating lower compatibility and thus higher risk. Our comparison of the proposed CAR with existing metrics demonstrates that it outperforms all baselines in predicting the effectiveness of response generators. We believe that our findings on the Larger Models’ Paradox and the proposed CAR can effectively guide future instruction tuning of LLMs. Instead of selecting response generators solely based on benchmark performance (e.g., GPT-4), practitioners should prioritize those with higher compatibility to better enhance the instruction-following capabilities of their LLMs."
https://arxiv.org/html/2411.07071v1,"Universal Response 
and Emergence of Induction in LLMs","While induction is considered a key mechanism for in-context learning in LLMs, understanding its precise circuit decomposition beyond toy models remains elusive. Here, we study the emergence of induction behavior within LLMs by probing their response to weak single-token perturbations of the residual stream. We find that LLMs exhibit a robust, universal regime in which their response remains scale-invariant under changes in perturbation strength, thereby allowing us to quantify the build-up of token correlations throughout the model. By applying our method, we observe signatures of induction behavior within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across all models, we find that these induction signatures gradually emerge within intermediate layers and identify the relevant model sections composing this behavior. Our results provide insights into the collective interplay of components within LLMs and serve as a benchmark for large-scale circuit analysis.","Over the last few years, the capabilities of large language models (LLMs) have dramatically improved - already reaching the level of human experts on a variety of tasks [1, 2, 3]. In comparison to this remarkable rise of LLMs, our understanding of such models has remained relatively limited. A promising approach to gain a better understanding of LLMs is mechanistic interpretability (MI), which aims at understanding a model’s behavior through interpretable circuits of its components [4]. While in toy models such circuits were found to explain a variety of model behavior including in-context learning [5, 6, 7, 8], understanding how such circuits can be extracted in larger models or for wider classes of behavior remains an open area of research [9, 10]. What makes the composition of such circuits challenging for larger multi-layer models is a complex, non-linear interplay of multi-head attention (MHA), multi-layer perceptrons (MLPs), and skip-connections, via the residual stream [11]. This interplay is expected to give rise to superposition states of features [12], including attentional features [13], even across multiple layers [14], and thereby significantly complicates such interpretability studies. While recently, sparse autoencoders were successfully used to tackle the problem of superposition and find interpretable features as the variables for circuits in LLMs [15, 14, 16, 17], many open questions remain [18]. Therefore, although we have a good understanding of individual model components [19, 20, 21, 22, 23, 24, 25, 26, 15], their collective downstream effect on the model behavior remains under active investigation [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 14]. Consequently, while the field of MI is evolving rapidly and promising approaches to make progress on these questions are constantly emerging [37, 38, 39], a full macroscopic understanding of LLMs is still lacking. In this work, we examine the emergence of induction behavior, which is considered a key mechanism for in-context learning and therefore plays a fundamental role for our understanding of LLMs [8]. Specifically, the induction mechanism enables models to correctly predict the next token in repeated sequences of the form [A][B]\dots[A]\rightarrow[B], even for randomly chosen tokens [A],[B]. In two-layer, attention-only toy models, this mechanism has been successfully reverse-engineered through a composition of previous token heads followed by induction heads [7]. So far, however, such circuit decompositions could not be scaled to larger, multi-layer transformer models, and the precise composition of induction behavior in LLMs remains under active investigation [5, 35, 40, 41, 42, 43, 26, 8]. Our contributions Here, we reveal the emergence of induction signatures within LLMs by probing the models’ response to weak single-token perturbations of the residual stream. Specifically, we use repeated sequences of random tokens and find a strong response of the model for tokens preceding the perturbed token in the following sequence. We observe such induction signatures within Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL, and across all models find a robust, universal regime in which the response remains scale-invariant under changes of perturbation strength. We find that this scale-invariant regime extends throughout the residual stream, thereby allowing us to probe the composition of induction behavior within each model. For all models, we observe the gradual emergence of induction signatures across intermediate layers and identify the relevant model sections composing this behavior. Our results reveal qualitative differences in the composition of induction behavior in LLMs to guide future studies on large-scale circuit analysis [39]."
https://arxiv.org/html/2411.07066v1,Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training,"Network pruning is a set of computational techniques that aim to reduce a given model’s computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has focused on pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations’ alignment w.r.t. their corresponding dense models. Hence, we propose NeuronAl, a top-up algorithm that can be used on top of any given pruning algorithm for LLMs, that modifies the block-wise and row-wise sparsity ratios to maximize the neuron alignment among activations. Moreover, differently from existing methods, our approach adaptively selects the best parameters for the block-wise and row-wise sparsity ratios w.r.t. to the model and the desired sparsity (given as input), and requires no re-training. We test our method on 4 different LLM families and 3 different sparsity ratios, showing how it consistently outperforms the latest state-of-the-art techniques. The code is available at https://github.com/eliacunegatti/NeuroAL.","In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task [44, 33, 4]. However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13, 25, 43, 11], but nowadays the focus has shifted towards pre-trained models [41, 42, 22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (1) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for “exploiting” as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight’s information [21], activations [40, 39], or reconstruction error [15], without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18, 45, 49], and those that impose non-uniform sparsity distribution modifying the block-wise sparsity [46]. The latter category is extremely effective for improving performance in CNNs [14, 38], while its application to LLMs is still limited [46]. Contributions In this paper, we first analyze the reasons behind the effectiveness of non-uniform sparsity distribution in sparse LLM. To do so, we carefully analyze the state-of-the-art top-up method OWL [46], investigating the reason underlying its better performance (w.r.t. other methods from the state of the art) as well as its limitations in terms of sensitiveness to its hyperparameters. Leveraging this knowledge, we introduce a new top-up method, that we call NeuronAl. The algorithm consists of a two-step approach that re-distributes the block-wise sparsity, i.e., the sparsity among Transformer blocks, and the row-wise sparsity, i.e., the sparsity for each row of a given layer’s matrix, maximizing the neuron alignment between dense and sparse activations. Contrary to OWL, NeuronAl does not require the user to specify any hyperparameter, as it automatically selects the most-performing values from a suitable set, hence adapting to the underlying model and the desired sparsity. Moreover, the use of alignment overcomes the necessity to define outliers on the activations, as done in OWL, which requires defining a threshold a priori. Finally, it does not use any gradient information [18, 45], hence significantly saving computational resources. We test our approach on 3 Language Modeling datasets and 7 Zero-Shot tasks over 4 different LLM families from 7B to 13B parameters, to show its ability to outperform the most recent state-of-the-art techniques, including OWL [46] and DsNoT [49], over 3 different high sparsity values (60%, 70%, and 80%). To assess the robustness of our approach, we also conduct an in-depth ablation study. 2 Related Work In this section, we provide a comprehensive discussion about network pruning applied to LLMs. We first introduce structured and unstructured network pruning; then, we focus on the latter, introducing the latest approaches proposed for improving sparse model performance. Structured Network Pruning. Given a layer’s weight matrix W\in\mathbb{R}^{n\times m} to sparsify, structured pruning removes either entire rows (n) or columns (m) (see the next section) aiming at speeding up both training and inference time. The first approach that applies structured pruning to LLMs has been proposed in [28], and focuses on the dependency of Transformers, i.e., it removes components of the networks while maximizing their original functionality. In [24], a pruning mechanism has been devised to remove components with the worst balance between loss and runtime. Other structured pruning approaches have been proposed based on combinatorial optimization [30], perturbative forward-pass only [9], and reduction of the embedding dimension through PCA [1]. Finally, in [17] it has been found that the last Transformer blocks are redundant, hence they can be completely removed with minor performance drops. The reason behind this phenomenon lies in the similarity between the learnable representation of consecutive blocks, which turns out to increase when the block depth increases. While all these approaches can achieve valuable inference speed-ups, the performance of the resulting sparse models w.r.t. their dense counterparts can be matched only at low sparsity values, such as 20% in [28] or 30% in [1]. This somehow limits the applicability of these methods, since in the case of models with billions of parameters one may need more aggressive pruning strategies to meet stringent hardware requirements. Unstructured Network Pruning. Differently from structure pruning, unstructured pruning works by removing weights in a scattered (i.e., non-structured) way. While in this scenario the inference speed-up is limited (although techniques for reordering weights are available [26, 34, 50]), the performance w.r.t. the dense model can be preserved also at high sparsity ratios (i.e., above 50%), with the performance at lower sparsity being almost always completely preserved. The first approach of this kind has been proposed in [15], where weight pruning and reconstruction are combined based on the Hessian matrix. Even a simple magnitude-based approach turned out to perform well [21], as well when integrated with information on the neuron activations [40, 12]. All these approaches work by computing a score for each weight and then removing, uniformly for each layer, the worst-performing ones for a given sparsity ratio. Top-Up Algorithms To improve the performance of unstructured pruning algorithms, several top-up algorithms have been devised. These approaches can be categorized into two distinct groups: methods that minimize the reconstruction error keeping the sparsity uniform for each block, and methods that modify the block-wise sparsity of the model resulting in non-uniform sparsity distribution across blocks. The first group firstly sparsifies the model using a pruning algorithm and then, either dynamically [49] or by backpropagation [18], updates the pruning mask. The second group (to which our method belongs) modifies the block-wise sparsity (obtained by a given pruning algorithm) based either on activations’ outliers [46] or on layer-wise sparsity using block-wise reconstruction error with gradient information [45]. The idea of simply redistributing the layer-wise sparsity is known to be extremely well-performing on Multi-Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs). The first approach of this kind, based on the Erdős–Rényi (ER) model, has been proposed in [35] for MLPs and then adjusted for CNNs in [11], while an empirical study about the effect of layer-wise pruning using different sparsity ratios has been done in [27]. Regarding Transformers (both for vision and text), the state-of-the-art algorithms [15, 40] have been devised to set the block-wise sparsity across the Transformer blocks in a uniform way. Later on, OWL has been proposed to build upon scoring-based pruning algorithms, adjusting the block-wise sparsity in a non-uniform way w.r.t. the number of outliers computed for each block. This approach improves the performance of several pruning algorithms, e.g. [15, 40], especially at sparsity above 60%. On the same line, BESA [45] allocates layer-wise sparsity across each block’s layer using gradient information. Recently, modality-wise sparsity distribution has been investigated in the case of multimodal tasks in [12, 19]. 3 Observational Study As discussed earlier, OWL [46] uses a non-uniform sparsity distribution across blocks, which provides an advantage in terms of performance w.r.t. uniform distribution. However, the reason why this occurs remains unclear. In the first part of this section, we focus on uncovering the reason behind this phenomenon. Furthermore, OWL suffers from some disadvantages: it requires the setting of two hyperparameters, one for the sparsity difference between consecutive blocks to utilize in the non-uniform distribution (\lambda), and one for the outlier definition (M), In the second part of this section, we investigate how the choice of the best hyperparameters is tied to the model and sparsity selected. This limitation leads to either selecting non-optimal hyperparameters or requiring a grid search to find the optimal values. The observational study has been performed on two models (Phi-2 and LLama-1 7B) and three different sparsity ratios (0.6, 0.7, 0.8) using as pruning algorithm both Wanda [40] and multiflow [12]111We did not include BESA [45] since it requires gradient information and works layer-based rather than block-based. We consider only gradient-free state-of-the-art methods.. 3.1 \Circled1 Non-Uniform Block-Wise Sparsity Distribution We start by analyzing if the OWL’s performance improvement, compared to uniform sparsity distribution, could be unequivocally associated with its outlier-based score. Since the number of outliers for each block turns out to decrease with the layer depth, we test three straightforward non-uniform sparsity schedules (namely linear, exponential, and logarithmic), which do not require any forward step and do not depend on the outliers. Given a fixed parameter \lambda, these schedules work by redistributing the sparsity across layers in a monotonically increasing way (i.e., the sparsity of layer i is always larger than the sparsity of layer i-1,\forall i>1). Fig. 1 displays the improvement, w.r.t. uniform distribution, achieved by the three sparsity schedules on two pruning algorithms (Wanda and multiflow) with \lambda=0.08 (as in [46]). The results highlight how non-uniform sparsity schedules, with no outlier information, can match, and in some cases even improve, the performance of OWL. Overall, the linear schedule turns out to be the most reliable one since it does not show oscillations in performance across the different sparsity ratios (while this happens for the logarithmic and exponential schedule). Figure 1: Average perplexity improvement (percentage) w.r.t. uniform distribution, computed over 3 Language Modeling datasets when applying non-uniform distribution with linear, exponential, and logarithmic schedules. The horizontal dashed lines refer to the improvement w.r.t. uniform distribution achieved by OWL when applied to each of the two base pruning algorithms. 3.2 \Circled2 Effect of \lambda and M We then analyze the effect of the two hyperparameters set by OWL, namely \lambda and M. The first hyperparameter is used to set how much the sparsity can vary across blocks (i.e., [s-\lambda,s+\lambda]) while keeping the overall sparsity fixed as s. In the experimentation reported in [46], \lambda is set differently for each model, with 0.08 being the most used value. As a matter of fact, we found that this value is not effective with all models and sparsity ratios. We test OWL, applied again to Wanda and multiflow, with 12 different values of \lambda, and computed the perplexity on the 3 Language Modeling datasets. As we show in Figure 2, it turns out that OWL achieves the best results for each model-sparsity combination for different values of \lambda, which suggests the difficulty of setting this hyperparameter a priori. The second hyperparameter, M, defines the outliers’ threshold: namely, for each block, the number of outliers is computed as the number of activations that are M times greater than the block’s activations’ mean. We replicated the same analysis carried out for \lambda and also for M, testing 12 different values on the 3 Language Modeling datasets when applying OWL to Wanda and multiflow. As for \lambda, we found that the optimal setting of M heavily depends on the model and sparsity (noting that in [46] the most used value is 5), as depicted in Figure 3. Figure 2: Perplexity for different values of \lambda over WikiText2 using OWL’s non-uniform distribution across blocks. Figure 3: Perplexity for different values of M over WikiText2 using \lambda=0.08. To summarize, in \Circled1 we found that even if the non-uniform sparsity distribution proposed in OWL leads to better performance w.r.t. uniform distribution, this improvement is not entirely based on the outliers’ distribution. Instead, a simple increase of the block-wise sparsity across the layer depths can explain the performance improvement. In \Circled2, we showed how the performance of OWL is heavily sensitive to the choice of \lambda and M. In the next section, we will present our method to address this OWL’s limitation. 4 Methodology In this section, we propose our method, NeuronAl (Neuron Alignment). Given a pruning algorithm for a pre-trained LLM, our method re-computes the block-wise sparsity ratios for each Transformer block and the row-wise sparsity ratios based on the alignment between the dense activations and the sparse activations. The main strength of this method lies in its ability to be fully adaptive to the model and desired sparsity (given as input), requiring no single hyperparameter to be chosen a priori, but rather a set of suitable values from which NeuronAl can pick the most performing one. Preliminaries Given a dense model \mathcal{D}, a pruning algorithm \mathcal{P}, and a desired sparsity s, unstructured network pruning generally computes a saliency score \Psi for each weight w\in\mathcal{D} and then binarizes these scores w.r.t. the \mathtt{top}_{k} elements, where k=1-|\mathcal{D}|\times s. This allows to obtain a binary mask \mathcal{M} to apply over \mathcal{D}, from which the final sparse model can be computed as \mathcal{S}=\mathcal{D}\odot\mathcal{M}. Since LLMs are composed of stacked Transformer blocks (each one denoted as \mathcal{B}_{i}), i.e., sets of linear layers (each one denoted as \ell_{i}^{j}) that implement the self-attention mechanism followed by an MLP, the binarization step is usually done uniformly per each layer \ell_{i}^{j} [15, 40] as: \displaystyle\mathcal{M}^{\ell_{i}^{j}}=\mathtt{top}_{k^{\ell_{i}^{j}}}(\Psi^{% \ell_{i}^{j}},\mathcal{D}^{\ell_{i}^{j}}). (1) Neuron Alignment Our proposed method is based on the idea of combining the concept of neuron alignment, which requires no a priori definition of outliers (hence no M parameter, as in OWL), with that of adaptivity, to remove the dependence from \lambda. Differently from the well-established reconstruction error [15], computed as \arg\min_{M_{\ell},W_{\ell}}\left\|W_{\ell}X_{\ell}-(M_{\ell}\odot W_{\ell})X_% {\ell}\right\|_{2}^{2} for each layer or block (hence measuring the difference between the sparse and dense representation of the output prior the non-linearity functions), neuron alignment provides a single scalar value that evaluates the model in its completeness focusing on the difference between sparse and dense activations, hence after the non-linearity. The method takes as input both \mathcal{D} and its sparse version \mathcal{S} generated by \mathcal{P} with sparsity ratio s, and uses a small calibration data C_{\lambda} to make a forward pass on both models, to retrieve the dense and sparse activations, respectively \mathcal{A}_{\mathcal{D}} and \mathcal{A}_{\mathcal{S}}. The main idea behind NeuronAl is to maximize the neuron alignment by firstly modifying the vector of sparsity ratios for all blocks (\mathbf{s}^{\mathcal{B}}) and then for all rows (\mathbf{s}^{r}), where each row corresponds to the layer’s weight matrix W^{\ell_{i}^{j}} (for each layer \ell_{i}^{j} in \mathcal{B}_{i}), where W^{\ell_{i}^{j}}\in\mathbb{R}^{r\times m}. The main strength of this approach is that it does not require any weight update nor gradient information, but just a block- and row-wise recalibration and mask update via Eq. (1), using the same scoring criteria of \mathcal{P}. However, as tested in the previous observational study, finding the best block/row-wise sparsity requires defining a factor \lambda to control the block/row-wise sparsity difference between consecutive blocks/rows while ensuring the desired global sparsity. As seen earlier, while OWL requires \lambda to be set a priori, we design NeuronAl to automatically select, from a suitable set of values, the best \lambda for each combination of \mathcal{D}, \mathcal{P} and s, yielding an adaptive top-up method. The only constraint we set is that we use a linear sparsity schedule over \lambda for the block-wise step, demonstrated to be effective in our observational study \Circled1. This choice has been made (1) because we found that the performance improvement obtained with the linear sparsity schedule is more stable, see Fig. 1, and (2) to align our method to the latest research that shows how the last layers of an LLM have a small influence on the final performance [17, 29]. 4.1 Block-Wise Sparsity Ratio The first step concerns the block-wise redistribution over the whole model. Our method takes as input the dense and sparse models (\mathcal{D} and \mathcal{S}), the desired sparsity ratio (s), the calibration data C_{\lambda}, and a set of different \lambda parameters (\lambda^{\text{set}}). Then, it computes a set of |\lambda^{\text{set}}| different vectors of block-wise sparsity values for the whole model \mathbf{s}^{\mathcal{B}}_{set}=\{\mathbf{s}^{\mathcal{B}}_{\lambda_{1}},% \mathbf{s}^{\mathcal{B}}_{\lambda_{2}},\dots,\mathbf{s}^{\mathcal{B}}_{\lambda% _{|\lambda^{\text{set}}|}}\}, where each element \mathbf{s}^{\mathcal{B}}_{\lambda_{k}} indicates a vector of block-wise sparsity values obtained with a linear schedule in [s-\lambda_{k},s+\lambda_{k}]. For each \mathbf{s}^{\mathcal{B}}_{\lambda_{k}}, we then forward the calibration data C_{\lambda} through the model, and calculate the corresponding neuron alignment: \displaystyle neur_{al}=\sum_{\mathcal{B}_{i}}\sum_{\ell_{i}^{j}}\frac{\Bigg{% \|}\tilde{A}_{D}^{j}-\tilde{A}_{{\mathcal{S}}_{\left(\mathbf{s}^{\mathcal{B}}_% {\lambda_{k}}\right)}}^{j}\Bigg{\|}_{2}}{|\tilde{A}_{D}^{j}|} (2) where \tilde{A} means that the activations are normalized to sum up to one. Then, we select {\left(\mathbf{s}^{\mathcal{B}}_{set}\right)^{*}}, i.e., the \lambda parameters per block that minimize Eq. (2). Finally, we update the block-wise sparsity with the selected {\left(\mathbf{s}^{\mathcal{B}}_{set}\right)^{*}} via Eq. (1), thus obtaining a sparsified model \mathcal{S}_{\mathcal{B}}. 4.2 Row-Wise Sparsity Ratio The second step is complementary to the previous one, but in this case, the sparsity is modified w.r.t. the rows of each layer. It is established [40] that pruning using the row as a comparison group222Here, comparison group refers to the subset of weights whose scores are compared to decide which weights to prune. achieves better performance w.r.t. using the whole layer since it inherently maximizes the network connectivity [20, 7]. Here, we rely on such discovery to strengthen our approach and change the row-wise sparsity based on the neuron alignment of each layer. In this case, for each layer \ell_{i}^{j} (i.e., for each W^{\ell_{i}^{j}}\in\mathbb{R}^{r\times m}) we redistribute the sparsity across the r rows. Also in this case the \lambda parameters are critical for deciding how to control the sparsity difference between consecutive rows. We take our sparse model obtained with the block-wise redistribution (\mathcal{S}_{\mathcal{B}}) and, for each layer \ell_{i}^{j}, we compute different row-wise sparsity values obtaining \mathbf{s}^{s}_{set}=\{\mathbf{s}^{r}_{\lambda_{1}},\mathbf{s}^{r}_{\lambda_{2% }},\dots,\mathbf{s}^{r}_{\lambda_{|\lambda^{\text{set}}|}}\}, where each \mathbf{s}^{r}_{\lambda_{k}} indicates a vector of row-wise sparsity ranging in [s-\lambda_{k},s+\lambda_{k}], where each element is inversely proportional to the alignment of the corresponding row. In this case, we select in \mathbf{s}^{s}_{set} the row-wise vector {\left(\mathbf{s}^{r}_{set}\right)^{*}} that minimizes Eq. (2), and then apply Eq. (1) to \mathcal{S}_{\mathcal{B}}, using each row as comparison group. The full procedure composed of both the block-wise and row-wise recalibration is summarized in Algorithm 1. Algorithm 1 Proposed top-up pruning procedure Require: \mathcal{D},\mathcal{P},s,C_{\lambda},\lambda^{\text{set}} \mathcal{M}\leftarrow\mathcal{P}(\mathcal{D},s) \mathcal{S}\leftarrow\mathcal{D}\odot\mathcal{M} \triangleright Prune \mathcal{D} uniformly per layer A_{\mathcal{D}}\leftarrow\mathcal{D}(C_{\lambda}) \triangleright Dense activations \left(\mathbf{s}^{\mathcal{B}}_{set}\right)^{*}\leftarrow NeuronAl(\mathcal{D},\mathcal{S},\lambda^{\text{set}},C_{\lambda},A_{\mathcal{D}}) \triangleright Block-wise step \mathcal{S}_{\mathcal{B}}\leftarrow\mathcal{D}\odot\mathtt{top}_{\left(\mathbf% {s}^{\mathcal{B}}_{set}\right)^{*}}(\Psi,\mathcal{D}) {\left(\mathbf{s}^{r}_{set}\right)^{*}}\leftarrow\textsc{{NeuronAl}}(\mathcal{% D},\mathcal{S}_{\mathcal{B}},\lambda^{\text{set}},C_{\lambda},A_{\mathcal{D}}) \triangleright Row-wise step \mathcal{S}_{\text{final}}\leftarrow\mathcal{D}\odot\mathtt{top}_{\left(% \mathbf{s}^{r}_{set}\right)^{*}}(\Psi,\mathcal{D}) function NeuronAl(\mathcal{D},\mathcal{S},s,\lambda^{\text{set}},C_{\lambda},A_{\mathcal{D}}) \mathbf{s}^{*}\leftarrow\emptyset, neur_{al}^{*}\leftarrow\infty for \lambda\in\lambda^{\text{set}} do \mathbf{s}_{\lambda} = GetDist(s,\lambda) \triangleright Compute \mathbf{s}^{\mathcal{B}}_{\lambda} or \mathbf{s}^{r}_{\lambda} A_{\mathcal{S}}\leftarrow(\mathcal{D}\odot\mathtt{top}_{\mathbf{s}_{\lambda}}(% \Psi,\mathcal{D}))(C_{\lambda}) \triangleright Sparse activations neur_{al}\leftarrow\textsc{GetAlign}(A_{\mathcal{D}},A_{\mathcal{S}},\mathbf{s% }_{\lambda}) \triangleright Via Eq. (2) if neur_{al}<neur_{al}^{*} then \mathbf{s}^{*}\leftarrow\mathbf{s}_{\lambda} return \mathbf{s}^{*} 5 Experiments We apply our proposed NeuronAl to different pruning algorithms tailored for LLMs. Specifically, we test how it compares in terms of performance over Language Modeling datasets and Zero-Shot tasks w.r.t. the most recent top-up algorithms for pruning. We also perform scalability and ablation studies to show the effectiveness of our NeuronAl. 5.1 Experimental Setup Language Modeling Datasets To measure the models’ perplexity on Language Modeling datasets, we use the following three datasets: (1) WikiText2 [31], a collection of 2.6M tokens from Wikipedia; (2) Colossal Clean Common Crawl (C4) [37], which is a clean version of the Common Crawl dataset produced in April 2019, containing 154G tokens [10]; and (3) Penn Treebank (PTB), which is a dataset extracted from the Wall Street Journal containing 2.4M tokens. Zero-Shot Tasks To assess more thoroughly how the different pruning algorithms affect the models’ capabilities, we employ the following 7 datasets. (1) Recognizing Textual Entailment (RTE) [8, 2, 16, 3], a dataset composed of 3000 test samples in which a model has to detect the entailment between two sentences. (2) WinoGrande [23], a dataset consisting of 1767 test cases in which the model has to fill a blank using two choices. (3) BoolQ [5], a question-answering dataset containing 3270 test samples, where each question requires a yes/no answer. (4) HellaSwag [47], a dataset composed of 10k test cases that requires a model to select the appropriate completion for a given sentence from a set of possibilities. (5) ARC-e [6], the “easy” fold from the AI2 Reasoning Challenge, containing 2376 questions with multiple-choice answers. (6) ARC-c [6], the “challenge” fold from the AI2 Reasoning Challenge, containing 1172 questions that were not solvable for methods that perform well on the ARC-e dataset. (7) OBQA [32], a dataset containing 500 test multiple-choice questions about core science facts, asked in novel contexts. Table 1: Perplexity on the 3 Language Modeling datasets computed over 5 different LLMs for four different top-up algorithms (Uniform, DSnoT, OWL, and NeuronAl) on 3 pruning algorithms (Magnitude, multiflow, and Wanda) at 70% sparsity. Algorithm Top-Up Phi-2.7B LLama-1 7B LLama-2 7B Mistral 7B OPT 6.7B WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 764.6 384.4 983.9 2.53e4 2.25e4 3.26e4 1.42e5 1.02e4 2.02e6 221.9 232.9 748.7 1.00e4 5.39e3 6.54e3 DSnoT 539.0 258.0 656.2 1.02e7 2.77e6 4.99e7 1.31e8 2.90e7 2.25e8 192.7 189.9 566.2 6.16e3 3.93e3 4.36e3 OWL 419.6 242.7 358.5 1.20e4 6.58e3 5.39e4 3.39e5 1.24e4 3.28e6 111.7 124.2 545.5 1.57e4 8.48e3 9.67e3 NeuronAl 278.6 177.0 316.0 208.7 203.8 3.70e3 155.8 264.8 2.61e3 46.1 42.7 552.7 2.10e4 1.07e4 1.09e4 multiflow Uniform 388.4 298.8 610.8 80.9 71.9 172.4 60.0 58.8 1260.4 936.8 656.3 2062.5 943.6 1250.4 843.1 DSnoT 325.5 261.9 328.8 67.6 65.0 114.7 66.6 75.8 688.9 57.4 63.3 264.8 241.8 153.3 263.9 OWL 197.9 141.3 293.9 25.1 25.8 78.9 29.2 31.0 547.1 329.0 764.3 1718.2 240.9 495.6 337.8 NeuronAl 110.7 91.0 192.0 20.6 21.2 45.9 22.0 23.8 265.5 197.2 343.9 954.0 221.7 86.5 219.7 Wanda Uniform 227.6 182.7 346.2 85.1 86.2 157.0 78.0 81.0 599.3 60.7 73.6 298.3 157.5 260.1 209.2 DSnoT 221.9 172.6 257.6 72.9 76.0 121.0 76.1 85.7 491.8 81.3 79.9 304.8 191.4 173.3 182.6 OWL 132.7 116.2 183.7 24.6 27.3 61.2 30.5 36.6 333.7 41.0 51.8 253.5 54.4 69.7 100.7 NeuronAl 89.9 78.9 133.9 21.5 23.2 43.8 23.9 27.2 205.3 28.9 33.9 197.8 183.9 87.3 194.7 Baselines As pruning algorithms, we test Magnitude, multiflow [12], and Wanda [40]. All are tested with three different top-up algorithms (besides ours): (1) Uniform distribution, (2) DsnoT [49] (dynamic training-free uniform distribution with mask update), and (3) OWL [46] (block-wise training-free non-uniform distribution based on outliers scores). All these baselines are tested considering as comparison group each row: in other words, for a given layer, the sparsity s is uniform for each row of each matrix rather than uniform across matrices. This is done for two main reasons: 1) as mentioned earlier, it is established that row-wise pruning on LLMs leads to better performance w.r.t. layer-wise pruning [40], and 2) since our approach relies on a row-wise step, for fairness we also use row as comparison group (rather than layer) on all the other methods, to appreciate the benefit of our approach. We also test our approach, as well as the baselines, on SparseGPT [15], using in this case only the block-wise approach, since SparseGPT relies on a weight reconstruction mechanism that firstly prunes columns and then updates the rows of the pruned cells, which makes it unfeasible to apply our row-wise approach. The results on SparseGPT can be found in Tables 14-16 in the Appendix. For all the pruning algorithms that use calibration data (i.e., multiflow, Wanda, and SparseGPT), we use 128 samples from the C4 dataset, as in [15, 40]. Models and Sparsity Since one of the main distinctive features of NeuronAl is its ability to adapt to sparsity and models, we test 4 different LLM families. Specifically, we evaluate LLama-7B (both V1 and V2) [41, 42], Phi-2, Mistral-7B [22], and OPT-6.7B [48]. To scale up the models’ size, we also test LLama-13 (both V1 and V2). In the paper, we mainly present results at 70% sparsity, for fair comparisons with [46]. However, to assess the generalization to different sparsity ratios we also include 60% and 80% sparsity in our experiments, see the Appendix for details. NeuronAl Setup Our proposed method works by taking as input an LLM model, a desired sparsity ratio, a scoring-based pruning algorithm, and two sets of \lambda parameters (one for the block-wise and one for the row-wise steps). In our experiments, we gave as input \lambda^{\text{set}}\in[0.01,0.02,0.03,0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.15, 0.20,0.25] for the block-wise step, while for the row-wise step we also added 0.0 (in case of no performance improvement). Each value in \lambda^{\text{set}} has been evaluated (as described in Algorithm 1) over a calibration data C_{\lambda}. Since all the base pruning algorithms require a single forward over C (where |C|=128) sequences with 2048 tokens each, while OWL requires a second forward always over C, to make the computational runtime similar we set C_{\lambda}=8 (the closest power of 2 w.r.t. |C|/|\lambda^{\text{set}}|)333In Fig. 11 in the Appendix we report the results with different sizes of C_{\lambda}, showing how NeuronAl is robust to |C_{\lambda}|.. Doing so, the computational time required by NeuronAl is only two forward steps more than the base pruning algorithms, and one forward more than OWL444For both C and C_{\lambda}, we use the same seed (0) for the calibration set, i.e., C_{\lambda} contains the first 8 elements of C.. All the experiments have been run on NVIDIA A100 GPUs, both with 40 and 80 GB. 5.2 Experimental Results Concerning the Language Modeling datasets, the numerical results in terms of perplexity computed over the 3 Language Modeling datasets at 70% sparsity are shown in Table 1. It can be seen how NeuronAl is able in almost all cases to outperform all the other baselines by a large margin. In no case NeuronAl performs worse w.r.t. the uniform distribution. The only model on which NeuronAl is not the best top-up algorithm for all pruning algorithms is OPT. In all other cases, NeuronAl outperforms OWL for all models and pruning algorithms. The results at 60% and 80% sparsity shown in Tables 6-7 in the Appendix confirm this trend. As for the Zero-Shot tasks, the numerical results are shown in Table 2555Magnitude is removed from this table due to space constraints. Its results are available separately in Tables 10-12 in the Appendix.. Again, NeuronAl turns out to outperform in the majority of cases all the baselines. In 8 cases out of 10 (w.r.t. the mean accuracy across all tasks), NeuronAl is the one that reaches the best performance. The results for 60% and 80% sparsity are available in Tables 8-9 in the Appendix, where we observe a similar trend. Table 2: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on two pruning algorithms (multiflow and Wanda) at 70% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B multiflow 53.79 50.43 53.49 28.43 46.59 20.48 15.6 38.40 w. DSnoT 52.71 54.06 54.92 28.38 44.15 21.5 15.6 38.76 w. OWL 52.35 55.33 61.99 30.31 54.59 24.23 18.4 42.46 w. NeuronAl 53.07 58.01 62.17 33.33 51.52 25.43 17.6 43.02 Wanda 52.35 53.2 62.14 28.31 44.87 20.99 17.4 39.89 w. DSnoT 52.35 51.54 60.98 28.33 41.62 21.08 15.4 38.76 w. OWL 52.71 53.59 62.05 30.09 48.61 22.53 18.8 41.20 w. NeuronAl 52.71 58.01 62.17 32.37 49.62 22.95 17.2 42.15 Llama1 7B multiflow 55.96 52.57 61.96 29.77 34.64 19.45 15.2 38.51 w. DSnoT 54.15 50.43 59.33 29.33 36.45 19.28 13.6 37.51 w. OWL 52.35 58.64 62.63 36.74 47.43 26.62 18.2 43.23 w. NeuronAl 58.12 61.33 63.55 38.23 50.76 26.71 22.6 45.90 Wanda 55.23 52.8 57.46 28.84 32.2 18.0 13.8 36.90 w. DSnoT 54.15 51.22 54.56 28.97 33.08 18.26 13.6 36.26 w. OWL 58.48 58.56 62.60 34.74 47.35 24.06 17.4 43.31 w. NeuronAl 54.51 58.72 63.30 37.06 49.92 26.37 20.6 44.35 Llama2 7B multiflow 52.71 50.99 62.05 28.52 33.04 17.92 13.6 36.98 w. DSnoT 52.71 50.99 59.72 27.92 32.58 16.81 13.0 36.25 w. OWL 52.71 56.12 62.05 32.40 42.42 19.88 18.6 40.60 w. NeuronAl 54.15 58.09 62.42 35.29 48.36 22.53 21.2 43.15 Wanda 52.71 48.46 49.94 28.09 30.39 19.2 11.8 34.37 w. DSnoT 52.71 50.36 47.77 27.67 30.6 17.32 12.2 34.09 w. OWL 52.71 55.96 62.11 31.86 43.73 20.65 17.0 40.57 w. NeuronAl 52.71 57.62 62.72 35.47 50.25 22.01 20.6 43.05 Mistral-7B multiflow 49.82 50.75 41.19 26.45 26.64 21.84 12.6 32.76 w. DSnoT 52.71 52.57 62.42 29.51 36.66 18.94 12.0 37.83 w. OWL 53.79 49.17 38.90 26.77 27.78 19.20 12.8 32.63 w. NeuronAl 52.35 50.83 37.98 27.24 27.74 18.34 13.8 32.61 Wanda 52.71 51.62 59.79 28.86 34.18 18.17 12.6 36.85 w. DSnoT 52.71 50.28 58.62 28.51 33.54 18.86 13.2 36.53 w. OWL 52.71 53.91 62.20 30.95 39.39 18.60 13.6 38.77 w. NeuronAl 52.71 60.22 62.20 34.69 44.32 20.73 15.6 41.50 OPT-6.7B multiflow 53.79 49.72 43.0 26.48 30.51 20.05 13.8 33.91 w. DSnoT 53.79 49.01 61.1 27.01 32.87 18.34 12.2 36.33 w. OWL 48.74 48.62 61.56 27.18 35.69 16.47 11.6 35.69 w. NeuronAl 49.10 50.43 62.17 31.37 40.28 22.01 17.0 38.91 Wanda 52.71 49.72 60.03 26.91 35.86 17.75 11.2 36.31 w. DSnoT 52.71 49.57 60.61 26.91 35.06 17.58 12.0 36.35 w. OWL 53.79 51.22 61.87 29.53 42.3 18.09 14.6 38.77 w. NeuronAl 50.90 51.30 62.17 30.68 39.77 21.76 14.6 38.74 Table 3: Perplexity of LLama-13B on the 3 Language Modeling datasets at 70% (top) and 80% sparsity (bottom). Algorithm Top-Up Llama-1 13B LLama-2 13B WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 5610.6 3682.9 20040.0 191.0 159.1 2784.1 DSnoT 7420.1 2264.0 48325.4 121.7 103.0 2331.2 OWL 279.4 438.9 11502.2 33.3 35.4 1267.2 NeuronAl 55.4 83.3 841.1 45.2 44.4 321.1 multiflow Uniform 49.4 45.3 277.8 144.3 112.4 623.2 DSnoT 46.2 48.9 240.4 45.8 54.2 611.5 OWL 16.6 17.7 132.2 54.0 56.2 426.6 NeuronAl 13.7 15.4 98.7 17.3 20.0 275.2 Wanda Uniform 54.4 55.3 309.2 45.7 56.2 571.0 DSnoT 47.8 54.2 248.6 46.6 57.7 555.5 OWL 16.3 18.9 147.6 18.0 21.8 315.1 NeuronAl 14.3 16.6 85.4 16.5 19.3 239.6 Magnitude Uniform 26696.2 26319.4 27180.9 12020.1 11161.9 10210.6 DSnoT 5591.8 4131.3 5732.3 5822.9 6510.2 10668.0 OWL 21985.6 19564.4 24511.7 5208.2 5058.5 10202.2 NeuronAl 133.4 183.8 1727.7 574.3 386.2 4169.1 multiflow Uniform 3711.2 1696.1 3588.4 4481.2 2409.8 5209.6 DSnoT 5368.4 2863.6 6286.3 1943.3 1669.5 5279.7 OWL 813.8 375.7 2138.8 1802.3 1012.2 4394.9 NeuronAl 61.8 65.7 414.9 323.1 259.6 1301.6 Wanda Uniform 3479.3 1959.8 3566.1 1124.6 870.5 5549.8 DSnoT 43684.1 24432.2 32162.1 4441.2 3957.9 4092.8 OWL 761.6 368.1 1929.8 248.0 204.2 2027.1 NeuronAl 64.0 68.5 413.8 116.5 103.2 814.8 Scalability Study To assess if the NeuronAl performance scales to bigger models, we apply it to LLama-13B (both V1 and V2) on the Language Modeling datasets. The results available in Table 3 (top) show how our approach is even more effective with larger models. For the 70% sparsity ratio, NeuronAl turns out to reach the best performance over both models, for all pruning algorithms. Impressively, the gap in performance w.r.t. the other baselines (in particular uniform distribution) is even more marked than in the smaller models tested previously. The improvement of NeuronAl over the baselines is even more evident at 80% sparsity, see Table 3 (bottom), where in several cases it outperforms OWL by 1-2 orders of magnitude. This is confirmed also at 60% sparsity, see Table 13 in the Appendix. NeuronAl vs block-only vs row-only Since NeuronAl is based on two consecutive steps (for brevity, we call them here block and row), we test the performance of the block and row steps applied alone. This means that in the block-only setting the sparsity of each block one is given by our NeuronAl, while the row-wise sparsity for each layer is fixed to s. Instead, for the row-only setting, the sparsity for each block is fixed at s, while the row-wise sparsity for each layer is given by our neuron alignment mechanism. Table 4 shows the results of such ablation. The results confirm that NeuronAl (with both steps) achieves better performance in most of the cases, although this depends on the sparsity ratio (at lower sparsity, the block-only setting is the best one in 3 out of 5 cases). It is worth noticing that in such cases the gap w.r.t. NeuronAl is marginal and that NeuronAl always outperforms the uniform distribution. However, on high sparsity NeuronAl always performs best, with a large margin w.r.t. the second best (which is always the block-only setting). It is also clear how most of the performance improvement of NeuronAl comes from the block step, while the row step yields a moderate improvement, which however increases when increasing the sparsity ratio. Table 4: Ablation study: perplexity achieved when using the block and row steps alone over the C4 dataset. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 29.3 13.7 14.0 15.9 17.9 NeuronAl block-only 27.3 11.8 11.9 13.8 17.8 NeuronAl row-only 29.2 14.0 14.5 15.9 19.7 NeuronAl 27.2 12.0 12.0 13.8 19.1 70% Uniform 182.7 86.2 81.0 73.6 260.1 NeuronAl block-only 82.0 22.4 25.6 35.4 91.4 NeuronAl row-only 175.6 85.4 88.9 73.6 152.0 NeuronAl 78.9 23.2 27.2 34.0 87.3 80% Uniform 12422.9 3969.8 3117.3 277.7 2345.8 NeuronAl block-only 3177.3 296.2 313.8 181.8 530.6 NeuronAl row-only 16501.8 4597.5 824.9 277.7 2345.8 NeuronAl 2404.5 242.0 226.2 168.6 471.4 NeuronAl \lambda Selection In this section, we report an analysis of the ability of NeuronAl to pick the best \lambda parameters (i.e., the parameters for which the performance is the best one, hence the lowest value if computed over perplexity). To do this, we evaluate NeuronAl for all the \lambda parameters (in the block-only scenario to simplify the visualization of results) over the 3 Language Modeling datasets. Fig. 4 reports the perplexity at 70% sparsity across different values of \lambda (black dots connected by solid lines), while the dots highlighted in orange indicate the perplexity achieved with the \lambda value selected by NeuronAl. These results highlight how NeuronAl, in the majority of the cases, can pick the optimal value of \lambda both with data knowledge, as in the C4 dataset (from which the calibration data is sampled), as well as on unseen datasets such as WikiText2 and PTB. Fig.s 9-10 in the Appendix show the results at 60%-80% sparsity. Figure 4: Perplexity over different values of \lambda at 70% sparsity. The orange dot indicates the value selected by NeuronAl. The best \lambda is retrieved by maximizing the neuron alignment computed over C_{\lambda} on the C4 dataset, and then it is transferred to the other datasets. The red line corresponds to the perplexity using a uniform distribution, hence \lambda=0. Calibration Data Since NeuronAl works by finding the best \lambda parameters w.r.t. to the neuron alignment using a calibration sample of 8 samples, we test the results under different seeds for the calibration data. The results in Table 5 show the mean and standard deviation of perplexity at 70% sparsity over the 3 Language Modeling datasets (see Tables 20-20 in the Appendix for the same ablation at 60% and 80% sparsity). It is visible how the standard deviation is minimal w.r.t. the mean, apart from the OPT model where the standard deviation turns out to be higher. This dependence on the calibration data could explain the reason why, as discussed before, our proposed approach works better than the baselines on all other models but struggles with the OPT family. The ablation on the number of samples (see Fig. 11 in the Appendix) supports this hypothesis. In fact, OPT models are the only ones to show performance oscillation depending on the calibration data seed and especially the number of samples, which does not happen with all the other LLM families. Table 5: Ablation study: perplexity achieved when using NeuronAl with different calibration data seeds (0, 16, 46) at 70% sparsity over the 3 Language Modeling datasets. Dataset Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B WikiText2 085.5 \pm 3.3 22.7 \pm 2.1 023.6 \pm 0.2 029.0 \pm 0.1 137.4 \pm 71.2 C4 076.3 \pm 2.2 24.2 \pm 1.5 027.0 \pm 0.2 034.2 \pm 0.3 075.9 \pm 16.2 PTB 135.8 \pm 4.8 46.5 \pm 5.0 200.4 \pm 3.6 205.0 \pm 6.7 154.5 \pm 63.6 6 Conclusion and Limitations In this paper, we proposed NeuronAl, a new approach to prune LLMs based on the alignment between sparse and dense activations. Our approach has the main strength of requiring no outliers or gradient information. On top of that, our method is also adaptive, since it is designed to automatically select the best hyperparameters for the model, pruning algorithm, and desired sparsity. Throughout extensive experiments, we showed how our approach outperforms all the latest state-of-the-art methods both on Language Modeling datasets and Zero-Shot tasks, on 4 different LLM families and 3 sparsity ratios. We also included an extensive ablation study to show the robustness of our approach to the calibration data as well as the effect of its algorithmic steps. In our view, the present version of NeuronAl has three main limitations. (1) With NeuronAl, it is not possible to make use of optimized structured-sparsity inference implementations (e.g, the NVIDIA N:M sparsity [36]): for a given sparsity, NeuronAl produces customized sparsity constraints for each layer in a block, and then for each row of each layer. Therefore, these implementations cannot be employed as they often require continuity in the sparsity of matrices. (2) NeuronAl requires two forward steps more than base pruning algorithms, and one more than the closest top-up competitor, OWL. (3) Finally, it requires the definition of \lambda^{\text{set}}. However, it is worth noticing that |\lambda^{\text{set}}| as well as |\mathcal{C}_{\lambda}| affect the computational cost of the forward step. References [1] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. SliceGPT: Compress Large Language Models by Deleting Rows and Columns. In International Conference on Learning Representations, 2023. [2] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge, 2006. [3] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth PASCAL recognizing textual entailment challenge. In Text Analysis Conference, 2009. [4] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):1–45, 2024. [5] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, 2019. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457v1, 2018. [7] Elia Cunegatti, Matteo Farina, Doina Bucur, and Giovanni Iacca. Understanding Sparse Neural Networks from their Topology via Multipartite Graph Representations. Transactions on Machine Learning Research, 2024. [8] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine learning challenges. Evaluating predictive uncertainty, Visual object classification, and Recognizing textual entailment, pages 177–190. Springer, 2006. [9] Lucio Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, and Ameet Talwalkar. Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes. arXiv preprint arXiv:2402.05406, 2024. [10] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. In Conference on Empirical Methods in Natural Language Processing, pages 1286–1305, 2021. [11] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pages 2943–2952. PMLR, 2020. [12] Matteo Farina, Massimiliano Mancini, Elia Cunegatti, Gaowen Liu, Giovanni Iacca, and Elisa Ricci. MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16185–16195, 2024. [13] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [14] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Pruning neural networks at initialization: Why are we missing the mark? In International Conference on Learning Representations, 2020. [15] Elias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 10323–10337. PMLR, 2023. [16] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9. Association for Computational Linguistics, 2007. [17] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. [18] Song Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao, Yiyu Shi, and Rongrong Ji. EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs. arXiv preprint arXiv:2402.12419, 2024. [19] Shwai He and Tianlong Chen. RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation. arXiv preprint arXiv:2404.02424, 2024. [20] Duc NM Hoang and Shiwei Liu. Revisiting pruning at initialization through the lens of Ramanujan graph. In International Conference on Learning Representations, 2023. [21] Ajay Jaiswal, Shiwei Liu, Tianlong Chen, Zhangyang Wang, et al. The emergence of essential sparsity in large pre-trained models: The weights that matter. Advances in Neural Information Processing Systems, 36, 2024. [22] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7B. arXiv preprint arXiv:2310.06825, 2023. [23] Sakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chandra, and Choi Yejin. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. arXiv preprint arXiv:1907.10641, 2019. [24] Eldar Kurtić, Elias Frantar, and Dan Alistarh. ZipLM: Inference-Aware Structured Pruning of Language Models. Advances in Neural Information Processing Systems, 36, 2024. [25] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. [26] Jiajia Li, Bora Uçar, Ümit V Çatalyürek, Jimeng Sun, Kevin Barker, and Richard Vuduc. Efficient and effective sparse tensor reordering. In ACM International Conference on Supercomputing, pages 227–237, 2019. [27] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. In International Conference on Learning Representations, 2021. [28] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-pruner: On the structural pruning of large language models. Advances in Neural Information Processing Systems, 36, 2023. [29] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. ShortGPT: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. [30] Xiang Meng, Shibal Ibrahim, Kayhan Behdin, Hussein Hazimeh, Natalia Ponomareva, and Rahul Mazumder. OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization. arXiv preprint arXiv:2403.12983, 2024. [31] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [32] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [33] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):1–40, 2023. [34] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021. [35] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 9(1):2383, 2018. [36] Jeff Pool. Accelerating sparsity in the nvidia ampere architecture. GTC 2020, 2020. [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. [38] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in Neural Information Processing Systems, 33, 2020. [39] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024. [40] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. In International Conference on Learning Representations, 2023. [41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [43] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. arXiv preprint arXiv:2002.07376, 2020. [44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. [45] Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, and Ping Luo. BESA: Pruning large language models with blockwise parameter-efficient sparsity allocation. In International Conference on Learning Representations, 2024. [46] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Gen Li, AJAY KUMAR JAISWAL, Mykola Pechenizkiy, Yi Liang, Michael Bendersky, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (OWL): A missing secret sauce for pruning LLMs to high sparsity. In International Conference on Machine Learning. PMLR, 2024. [47] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Machine Really Finish Your Sentence? In Annual Meeting of the Association for Computational Linguistics, pages 4791–4800, 2019. [48] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open Pre-trained Transformer Language Models. arXiv preprint arXiv:2205.01068, 2022. [49] Yuxin Zhang, Lirui Zhao, Mingbao Lin, Sun Yunyun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. Dynamic sparse no training: Training-free fine-tuning for sparse LLMs. In International Conference on Learning Representations, 2024. [50] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch. In International Conference on Learning Representations, 2021. Appendix A Appendix A.1 Observational Study Here we include the results of the observational study on the OWL’s hyperparameters \lambda and M computed on the C4 dataset (Fig.s 5 and 7), and PTB (Fig.s 6 and 8). The results confirm the findings presented in the main text: there is no single hyperparameter selection (both M and \lambda) that reaches the best performance across all different model-pruning-sparsity settings. Figure 5: Perplexity for different \lambda values over C4 using OWL’s non-uniform distribution across blocks. Figure 6: Perplexity for different \lambda values over PTB using OWL’s non-uniform distribution across blocks. Figure 7: Perplexity for different M values over C4 using \lambda=0.08. Figure 8: Perplexity for different M values over PTB using \lambda=0.08. A.2 Additional Experiments Here we include the results of the experiments that, due to space limits, we could not include in the main text. Specifically, we report: the results of NeuronAl over Language Modeling and Zero-Shot tasks at 60% and 80% sparsity; the results for the Zero-Shot tasks with Magnitude pruning; the results over LLama-13B at 70% sparsity; the results of NeuronAl (block-only) applied to SparseGPT [15]. A.2.1 Language Modeling at 60% and 80% sparsity In Table 6-7, we report the results of NeuronAl over the 3 Language Modeling datasets (WikiText2, C4, and PTB) with the five different models considered in the main text, for 60% and 80% sparsity. In the first case, our approach turns out to be the best one in 23 out of 45 cases, while for 80% sparsity in 27 out of 45. It is interesting to notice how at medium sparsity (60%) all the top-up algorithms, including ours, provide similar results, while the improvement provided by NeuronAl at 80% (w.r.t. the top-up competitors) in some cases reaches a factor 5x (e.g., with Llama-1 7B for multiflow and Wanda). Table 6: Perplexity on the 3 Language Modeling datasets computed over 5 different LLMs for four different top-up algorithms (Uniform, DSnoT, OWL, and NeuronAl) on 3 pruning algorithms (Magnitude, multiflow, and Wanda) at 60% sparsity. Algorithm Top-Up Phi-2.7B LLama-1 7B LLama-2 7B Mistral 7B OPT 6.7B WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 51.3 45.9 66.9 152.4 159.8 3016.1 6894.8 42666.8 1706134.9 19.6 24.4 189.3 9489.3 6203.7 6759.1 DSnoT 55.9 48.9 64.1 131.6 114.7 1460.3 3676.6 67845.5 7299247.0 15.3 19.6 146.2 8077.8 6062.6 6948.2 OWL 46.5 42.2 65.7 50.5 62.9 249.4 810.9 19356.5 2302446.2 12.0 15.8 169.1 6811.7 3666.8 4065.4 NeuronAl 48.6 44.9 65.1 55.8 63.6 153.1 52.4 79.3 24807.9 12.6 17.2 123.9 1212.3 551.4 935.0 multiflow Uniform 25.4 28.3 54.7 11.6 13.9 26.0 11.0 13.7 166.9 167.7 341.0 884.3 16.3 19.7 26.8 DSnoT 37.2 42.1 50.2 10.1 12.7 18.2 10.5 13.4 137.6 10.9 14.8 86.0 15.9 19.2 25.3 OWL 23.8 26.7 49.8 10.6 12.9 19.6 10.1 12.7 106.0 84.1 123.0 644.6 16.1 18.5 25.5 NeuronAl 23.7 27.0 42.5 9.9 12.2 17.6 9.8 12.2 72.3 86.1 140.3 562.3 17.0 20.4 28.1 Wanda Uniform 225.8 29.3 48.9 10.7 13.7 24.0 10.8 14.0 122.2 11.3 15.9 101.6 15.2 17.9 23.7 DSnoT 32.2 38.0 50.6 10.4 13.2 20.8 10.8 14.1 109.6 11.4 15.9 96.8 15.8 19.1 24.6 OWL 24.8 28.2 48.6 9.4 11.8 18.5 9.2 11.9 75.1 10.3 14.5 84.5 15.7 17.8 24.5 NeuronAl 25.2 27.2 42.1 9.6 12.0 17.4 9.4 12.0 65.1 9.9 13.8 74.6 16.3 19.1 25.2 Table 7: Perplexity on the 3 Language Modeling datasets computed over 5 different LLMs for four different top-up algorithms (Uniform, DSnoT, OWL, and NeuronAl) on 3 pruning algorithms (Magnitude, multiflow, and Wanda) at 80% sparsity. Algorithm Top-Up Phi-2.7B LLama-1 7B LLama-2 7B Mistral 7B OPT 6.7B WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 15320.0 17856.3 31988.6 112885.2 113922.4 139627.3 55800.7 52597.2 89843.5 24815.2 31186.7 7984.1 42902.5 21284.4 22110.8 DSnoT 19977.5 20700.7 18369.5 33994.8 34213.5 72003.2 2358414.2 1802890.0 3015785.0 13297.5 8025.4 5804.0 18112.4 11853.1 14438.1 OWL 6625.1 5597.4 8388.0 169148.8 158523.1 134038.4 26898.0 17892.9 57863.3 9614.4 8503.8 5789.5 33174.7 17756.7 21613.0 NeuronAl 10193.9 9578.7 14427.3 82221.7 85688.7 108410.1 92725.2 67827.9 104616.8 2188.8 1205.3 3226.2 30303.8 20846.6 16807.1 multiflow Uniform 25257.3 12842.4 25929.7 4829.9 2307.4 9806.2 2037.9 1464.8 3878.2 4289.7 2976.3 3812.4 4422.8 2382.7 3276.9 DSnoT 8498.5 3918.3 12343.8 3697.1 2649.8 8263.4 1721.1 1543.0 3443.8 327.0 270.0 752.5 11633.7 9715.6 11764.5 OWL 6255.2 2798.5 13600.8 926.5 563.1 1783.4 544.2 414.3 2819.3 3354.5 2209.7 3555.2 13461.5 11133.9 15080.8 NeuronAl 3550.3 1279.8 5411.4 311.6 188.4 564.8 230.2 269.8 2355.9 983.1 617.5 1467.4 1118.7 591.5 1173.3 Wanda Uniform 20498.5 12422.9 31362.6 5219.5 3969.8 10013.6 4929.7 3117.3 5285.2 330.9 277.7 783.7 4259.6 2345.8 2732.1 DSnoT 15269.0 6855.4 14008.5 3709.7 3076.0 7790.7 5199.4 4443.2 6685.7 346.5 277.3 758.4 7754.1 6157.5 7779.7 OWL 2546.5 1209.5 7061.7 986.5 654.5 2000.4 663.0 486.2 2283.4 206.3 187.8 603.9 13190.1 10605.6 14228.5 NeuronAl 3873.6 2404.5 6675.9 376.1 242.0 566.3 213.9 226.2 993.2 226.3 168.6 698.2 988.2 471.4 988.7 A.2.2 Zero-Shot at 60% and 80% sparsity In Tables 8-9, we report the results at 60% and 80% sparsity of NeuronAl over Zero-Shot tasks with the five different models tested in the main text. In the first case, our approach turns out to be the best one (when considering the average over the 7 tasks) in 7 out of 10 cases, while for 80% sparsity in 8 out of 10. This further validates the results presented in the main text (for 70% sparsity) since it shows the ability of NeuronAl to provide stable and consistent results over more complicated tasks at different (and high) sparsity levels. Table 8: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on two pruning algorithms (multiflow and Wanda) at 60% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B multiflow 62.09 67.64 63.15 42.04 71.76 39.51 27.2 53.34 w. DSnoT 63.18 66.77 43.49 40.84 66.62 35.41 26.0 48.90 w. OWL 64.62 67.17 60.15 41.78 70.96 37.8 28.8 53.04 w. NeuronAl 62.09 67.40 67.22 42.73 71.04 38.65 27.6 53.82 Wanda 63.54 64.8 69.08 40.16 68.64 34.9 25.4 52.36 w. DSnoT 62.45 64.33 59.17 39.25 64.18 33.53 22.6 49.36 w. OWL 64.62 64.33 64.83 39.80 67.63 34.98 24.2 51.48 w. NeuronAl 63.54 67.09 67.74 40.51 66.54 34.04 24.4 51.98 Llama1 7B multiflow 57.04 62.51 67.19 45.31 59.55 30.97 24.6 49.60 w. DSnoT 49.46 63.06 68.32 44.75 63.80 31.23 26.4 49.57 w. OWL 54.15 63.46 66.54 46.53 60.65 31.14 25.4 49.70 w. NeuronAl 49.10 63.46 68.35 47.89 63.51 32.42 27.8 50.36 Wanda 59.57 62.67 68.81 43.64 62.84 30.38 24.8 50.39 w. DSnoT 51.62 61.64 67.37 43.39 63.89 30.55 25.4 49.12 w. OWL 55.60 64.17 70.61 46.63 62.96 31.74 24.8 50.93 w. NeuronAl 57.76 64.09 70.24 46.59 63.97 31.14 26.2 51.43 Llama2 -7B multiflow 57.04 61.96 64.80 43.39 60.44 29.1 24.6 48.76 w. DSnoT 54.15 63.77 63.91 43.42 66.25 31.83 24.0 49.62 w. OWL 54.87 62.75 65.14 45.20 62.58 29.52 23.8 49.12 w. NeuronAl 52.71 65.90 69.51 46.60 65.87 31.66 24.8 51.01 Wanda 54.15 64.48 65.44 43.85 65.19 30.46 26.2 49.97 w. DSnoT 53.79 64.09 64.83 42.39 63.89 30.03 22.8 48.83 w. OWL 53.79 66.61 66.76 46.63 67.63 32.34 28.0 51.68 w. NeuronAl 53.07 66.06 70.46 46.93 66.67 31.74 26.8 51.68 Mistral-7B multiflow 51.62 49.88 39.17 27.49 29.67 18.77 11.0 32.51 w. DSnoT 54.87 66.61 70.86 45.93 68.27 32.94 24.0 51.93 w. OWL 53.07 50.12 46.33 28.29 32.58 19.28 13.2 34.70 w. NeuronAl 52.71 52.49 52.97 28.77 33.42 18.43 13.2 36.00 Wanda 54.87 66.06 71.13 44.48 67.05 32.00 21.60 51.03 w. DSnoT 54.15 65.59 70.43 44.5 66.88 31.40 20.60 50.51 w. OWL 57.04 67.17 73.85 45.66 67.89 32.59 23.20 52.49 w. NeuronAl 56.32 65.82 70.70 46.71 67.63 33.36 23.20 51.96 OPT-6.7B multiflow 52.71 58.25 62.69 42.24 56.52 25.68 22.20 45.76 w. DSnoT 53.07 58.48 62.57 42.13 57.79 23.98 22.20 45.75 w. OWL 53.07 57.46 63.21 42.98 56.44 25.68 24.20 46.15 w. NeuronAl 52.71 58.64 62.17 41.83 56.31 25.09 22.20 45.56 Wanda 52.71 59.67 62.29 42.80 58.00 25.60 22.6 46.24 w. DSnoT 52.71 58.17 62.11 41.99 57.41 25.43 22.4 45.75 w. OWL 52.71 58.72 62.69 42.14 58.33 25.17 22.6 46.05 w. NeuronAl 52.71 61.25 62.29 42.00 57.49 25.43 23.0 46.31 Table 9: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on two pruning algorithms (multiflow and Wanda) at 80% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B multiflow 52.71 50.12 51.04 26.15 27.44 19.37 13.4 34.32 w. DSnoT 50.18 49.49 37.83 26.32 27.82 19.97 13.8 32.2 w. OWL 53.07 51.22 56.76 26.30 30.85 18.52 15.2 35.99 w. NeuronAl 52.71 49.96 50.58 26.49 32.20 19.8 14.4 35.16 Wanda 53.07 49.25 62.17 25.93 27.44 20.99 14.6 36.21 w. DSnoT 53.07 50.99 38.04 26.17 27.10 20.73 13.0 32.73 w. OWL 52.35 51.22 60.21 26.51 29.88 20.05 13.6 36.26 w. NeuronAl 52.71 50.12 62.17 27.57 32.95 21.08 16.0 37.51 Llama-7B multiflow 47.29 50.91 40.03 26.17 26.77 21.16 12.4 32.1 w. DSnoT 46.57 50.43 37.83 26.02 27.06 20.05 12.2 31.45 w. OWL 50.18 50.04 45.47 26.74 27.65 20.48 12.2 33.25 w. NeuronAl 54.87 50.91 55.96 27.62 30.09 18.77 12.8 35.86 Wanda 47.29 49.88 37.83 26.34 26.47 20.99 12.8 31.66 w. DSnoT 46.93 50.36 37.83 26.03 26.56 21.33 13.4 31.78 w. OWL 47.29 49.88 37.83 26.67 27.19 19.54 11.6 31.43 w. NeuronAl 49.82 48.86 40.76 27.41 29.25 18.34 13.0 32.49 Llama2 -7B multiflow 53.43 48.86 37.83 26.35 27.48 21.25 13.2 32.63 w. DSnoT 52.71 48.86 37.86 26.17 26.60 20.39 13.0 32.23 w. OWL 52.71 49.49 37.83 26.62 26.94 19.11 12.6 32.19 w. NeuronAl 52.71 50.36 44.74 27.47 28.62 17.58 14.4 33.70 Wanda 47.65 49.41 37.83 25.82 26.52 20.82 14.6 31.81 w. DSnoT 53.07 47.91 37.86 26.09 27.23 20.73 13.0 32.27 w. OWL 52.71 50.83 37.83 26.52 27.27 19.37 12.8 32.48 w. NeuronAl 52.71 49.49 37.89 27.38 28.58 17.58 14.8 32.63 Mistral-7B multiflow 50.18 48.15 37.80 25.67 26.18 22.70 13.20 31.98 w. DSnoT 52.71 47.36 37.83 26.58 28.03 18.94 13.8 32.18 w. OWL 48.38 49.09 38.44 25.88 25.59 23.29 13.6 32.04 w. NeuronAl 52.71 50.91 37.83 26.35 27.78 20.31 12.8 32.67 Wanda 53.79 48.78 37.83 26.52 27.82 19.8 12.8 32.48 w. DSnoT 52.35 48.30 37.83 26.55 27.44 19.54 13.0 32.14 w. OWL 52.71 47.43 37.83 26.68 27.78 18.52 13.2 32.02 w. NeuronAl 52.71 50.04 38.65 27.52 28.96 19.97 13.4 33.04 OPT-6.7B multiflow 52.71 50.91 37.80 25.87 27.40 19.28 12.40 32.34 w. DSnoT 52.71 50.83 57.31 26.00 25.00 20.22 11.80 34.84 w. OWL 52.71 51.07 37.83 25.74 25.29 20.05 16.00 32.67 w. NeuronAl 52.71 51.70 54.07 26.29 30.64 19.37 11.80 35.23 Wanda 54.15 52.09 41.53 26.47 28.45 18.86 11.80 33.34 w. DSnoT 52.71 51.38 55.32 26.17 27.06 19.37 13.80 35.12 w. OWL 52.71 49.33 37.83 25.84 25.67 20.31 13.00 32.10 w. NeuronAl 52.71 49.25 51.99 26.35 30.39 19.37 12.60 34.67 A.2.3 Zero-Shot at 60%, 70% and 80% sparsity with Magnitude In Tables 10-12, we report the results of NeuronAl over Zero-Shot tasks using Magnitude pruning as the base pruning algorithm. The results provided by NeuronAl turn out to be the best ones in 9 out of 15 cases. It is also worth noticing that the performance gap between the Magnitude pruning and score-based pruning algorithms (such as Wanda or multiflow) is generally quite high. Hence, NeuronAl can improve the performance of Magnitude (in the standard setting with uniform distribution) to a certain degree since at high sparsity ratios (as the ones we test) the performance of Magnitude has been shown to be poor [21]. Table 10: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on Magnitude at 60% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B Magnitude 57.04 62.83 51.38 42.56 66.33 35.41 28.2 49.11 w. DSnoT 54.51 64.09 42.32 41.09 66.25 34.98 26.6 47.12 w. OWL 55.23 62.59 48.81 42.53 67.38 38.48 28.4 49.06 w. NeuronAl 54.15 65.19 47.28 42.36 65.95 36.77 26.8 48.36 Llama-7B Magnitude 51.62 52.64 45.05 39.23 51.05 26.88 20.4 40.98 w. DSnoT 52.35 52.80 46.88 38.3 50.59 26.37 20.6 41.13 w. OWL 52.35 58.41 51.8 42.02 56.31 29.78 23.8 44.92 w. NeuronAl 50.90 56.12 57.43 40.65 55.3 29.78 24.4 44.94 Llama2 -7B Magnitude 51.26 55.8 41.19 36.97 50.17 26.96 16.2 39.79 w. DSnoT 53.79 56.04 42.87 38.3 53.28 27.9 19.8 41.71 w. OWL 51.99 57.3 46.15 42.56 56.65 30.46 19.4 43.5 w. NeuronAl 54.87 59.59 60.4 46.14 59.01 32.85 27.6 48.64 Mistral-7B Magnitude 55.23 62.19 66.36 48.74 67.05 33.19 22.6 50.77 w. DSnoT 55.6 62.35 68.53 48.28 67.51 33.11 23.2 51.23 w. OWL 53.79 64.48 72.17 49.39 68.14 33.87 23.8 52.23 w. NeuronAl 53.79 64.09 71.07 49.83 65.07 35.07 25.0 51.99 OPT-6.7B Magnitude 53.43 50.59 37.86 26.38 26.6 21.42 13.2 32.78 w. DSnoT 52.71 49.25 37.86 26.14 27.27 21.5 13.2 32.56 w. OWL 52.71 50.51 37.83 26.77 30.3 18.52 14.8 33.06 w. NeuronAl 52.71 54.22 39.14 33.3 37.71 24.23 16.8 36.87 Table 11: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on Magnitude at 70% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B Magnitude 46.93 53.59 47.22 30.45 47.85 24.57 19.2 38.54 w. DSnoT 46.57 50.91 39.6 30.12 45.54 24.06 16.8 36.23 w. OWL 45.13 52.88 49.2 32.26 51.64 27.56 21.4 40.01 w. NeuronAl 47.65 53.51 54.25 33.4 53.7 29.61 20.6 41.82 Llama-7B Magnitude 53.43 49.96 37.92 27.59 31.73 22.44 16.6 34.24 w. DSnoT 52.71 51.7 37.83 27.71 30.26 22.7 15.4 34.04 w. OWL 53.07 51.38 38.38 33.14 39.31 24.15 16.8 36.6 w. NeuronAl 52.71 54.38 52.91 39.61 46.0 26.62 23.0 42.18 Llama2 -7B Magnitude 51.26 49.96 37.86 25.9 28.45 23.12 13.4 32.85 w. DSnoT 53.79 49.88 37.86 25.42 28.83 20.56 16.6 33.28 w. OWL 53.07 50.28 37.89 26.38 30.77 22.7 15.0 33.73 w. NeuronAl 54.51 55.41 64.86 33.25 42.09 27.47 21.2 42.68 Mistral-7B Magnitude 51.99 50.83 41.13 32.16 42.72 19.54 16.6 36.42 w. DSnoT 53.07 51.62 39.54 31.66 42.51 20.05 16.6 36.44 w. OWL 57.76 56.59 49.17 36.48 45.75 22.01 18.8 40.94 w. NeuronAl 53.79 57.54 62.72 38.68 44.11 26.45 20.6 43.41 OPT-6.7B Magnitude 52.71 49.8 37.83 25.88 26.68 21.33 12.4 32.38 w. DSnoT 52.71 49.96 37.83 25.87 27.19 20.14 13.6 32.47 w. OWL 52.71 50.59 37.83 25.81 25.46 21.25 12.8 32.35 w. NeuronAl 52.71 50.28 37.8 26.27 26.81 20.31 13.0 32.45 Table 12: Zero-Shot accuracy for 7 tasks, computed over 5 different LLMs for 3 different top-up pruning algorithms (DSnoT, OWL, and NeuronAl) on Magnitude at 80% sparsity. “Average” indicates the mean accuracy across tasks. The rows corresponding to the pruning algorithms refer to uniform distribution. Model Algorithm RTE WinoGrande BoolQ HellaSwag ARC-e ARC-c OBQA Average Phi-2.7B Magnitude 45.13 50.36 41.19 25.83 29.08 20.9 13.6 32.30 w. DSnoT 46.93 52.33 39.63 25.9 28.32 21.25 13.4 32.54 w. OWL 49.46 50.91 42.35 26.71 35.27 21.67 13.4 34.25 w. NeuronAl 46.57 51.22 44.86 25.78 28.66 21.5 13.0 33.08 Llama-7B Magnitude 46.21 49.96 53.98 25.69 24.83 21.84 13.8 33.76 w. DSnoT 52.35 51.85 38.47 25.52 26.39 21.42 16.0 33.14 w. OWL 48.38 48.93 44.74 25.76 26.35 21.08 15.8 33.01 w. NeuronAl 47.29 48.86 50.34 25.82 26.18 21.25 14.6 33.48 Llama2 -7B Magnitude 52.35 49.57 46.18 25.94 26.14 23.12 16.0 34.19 w. DSnoT 52.71 51.54 37.89 25.46 27.10 22.44 15.4 33.22 w. OWL 53.07 48.70 42.02 25.72 26.60 21.42 14.4 33.13 w. NeuronAl 51.26 49.33 42.14 25.71 26.52 22.70 15.0 33.24 Mistral-7B Magnitude 51.26 50.99 41.16 25.93 27.48 21.84 14.6 33.32 w. DSnoT 52.35 49.72 38.07 26.26 26.43 21.25 14.0 32.58 w. OWL 52.35 50.20 41.04 26.55 27.78 19.97 13.8 33.10 w. NeuronAl 52.71 49.64 43.18 28.11 29.8 21.59 14.6 34.23 OPT-6.7B Magnitude 52.71 49.49 37.83 25.79 26.39 21.25 13.0 32.35 w. DSnoT 52.71 49.57 37.83 25.78 25.63 20.65 12.8 32.14 w. OWL 52.71 49.80 37.83 26.05 26.73 21.16 13.2 32.50 w. NeuronAl 52.71 51.78 37.83 25.79 26.89 21.08 12.8 32.70 A.2.4 NeuronAl on LLama-13B at 70% sparsity In Table 13, we present the results on LLama-13B at 60% sparsity. The results are in line with the ones at 70% and 80% sparsity presented in the main text since NeuronAl can outperform the competitors in 11 out of 15 cases. Table 13: Perplexity of LLama-13B on the 3 Language Modeling datasets at 60% sparsity. Algorithm Top-Up Llama-1 13B LLama-2 13B WikiText2 C4 PTB WikiText2 C4 PTB Magnitude Uniform 34.9 49.1 1413.7 10.1 13.3 457.5 DSnoT 33.6 41.3 604.8 10.1 13.3 376.7 OWL 28.5 36.6 255.0 8.9 11.6 217.2 NeuronAl 24.8 35.1 186.9 9.5 12.3 151.1 multiflow Uniform 8.7 10.9 66.5 16.3 21.0 211.6 DSnoT 8.4 10.8 58.7 8.3 11.3 217.9 OWL 7.9 10.0 48.1 8.5 11.2 120.3 NeuronAl 7.9 10.2 44.7 8.6 11.1 109.9 Wanda Uniform 8.8 11.2 72.1 8.4 11.5 146.0 DSnoT 8.5 11.0 66.4 8.3 11.4 131.3 OWL 7.6 9.8 47.6 7.5 10.2 98.0 NeuronAl 7.6 9.9 46.8 7.6 10.3 90.3 A.2.5 NeuronAl on SparseGPT In Tables 14-16, we present the results of NeuronAl on SparseGPT on the WikiText2, C4, and PTB datasets, using the block-only setting666Since SparseGPT prunes and updates the weights from columns to rows, the row-wise step of NeuronAl cannot be included: indeed, it would force each row to have a different sparsity ratio, which is in contrast with the nature of SparseGPT.. Table 14: Perplexity on WikiText2 using SparseGPT. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 15.8 10.4 10.2 9.4 13.4 OWL 15.8 9.4 9.2 9.1 14.2 NeuronAl 15.7 9.9 9.3 9.1 13.7 70% Uniform 28.9 27.3 27.3 22.0 20.5 OWL 27.7 19.2 20.5 18.6 21.6 NeuronAl 27.3 22.6 20.9 17.8 21.8 80% Uniform 131.0 207.0 122.1 98.4 95.7 OWL 107.5 93.8 84.3 77.2 80.8 NeuronAl 113.5 144.7 88.7 70.8 84.0 Table 15: Perplexity on C4 using SparseGPT. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 19.0 12.8 12.9 13.0 15.3 OWL 19.2 11.7 11.6 12.4 15.8 NeuronAl 19.1 12.4 11.7 12.3 15.5 70% Uniform 28.6 28.3 31.5 27.8 22.4 OWL 28.2 21.1 22.8 23.7 22.4 NeuronAl 27.8 23.8 22.5 21.9 22.2 80% Uniform 98.7 136.2 104.8 86.5 72.5 OWL 79.7 68.3 73.4 66.2 65.4 NeuronAl 86.4 104.2 72.4 61.8 65.0 Table 16: Perplexity on PTB using SparseGPT. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 28.7 19.5 430.5 73.7 20.3 OWL 29.3 16.9 262.1 70.9 21.0 NeuronAl 28.2 18.2 249.2 67.2 20.6 70% Uniform 50.3 52.6 3780.0 153.2 32.0 OWL 51.0 37.0 1643.4 135.0 32.9 NeuronAl 47.3 40.5 861.6 123.4 32.8 80% Uniform 195.4 295.6 3201.7 316.2 102.3 OWL 141.4 162.3 5556.5 278.8 98.9 NeuronAl 156.7 260.2 3659.8 266.6 105.3 Using SparseGPT, the superiority of NeuronAl is less evident than with other pruning algorithms. Nevertheless, NeuronAl turns out to be the best-performing top-up algorithm in 5 out of 15, 8 out of 15, and 7 out of 15 cases, respectively for WikiText2, C4, and PTB. Interestingly, for lower sparsity, the gap between uniform and non-uniform distribution (both NeuronAl and OWL) is less remarkable than at higher sparsity. We explain these results with the inherent functioning of SparseGPT, which, differently from the other pruning algorithms, includes a weight reconstruction step. However, we can conclude that also in this case, our proposed approach turns out to be effective in many cases at increasing the task performance. A.3 Block-only and Row-only Ablation To complement the results presented in the main text on the C4 dataset, Tables 17-18 show the results of the ablation over the block-only and row-only steps also on WikiText2 and PTB. The results are in line with those obtained on C4. Table 17: Ablation study: perplexity achieved when using the block and row steps alone over the WikiText2 dataset. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 25.8 10.7 10.8 11.3 15.2 NeuronAl block-only 25.5 9.4 9.2 9.9 15.2 NeuronAl row-only 25.8 11.2 11.4 11.3 16.5 NeuronAl 25.2 9.6 9.4 9.9 16.3 70% Uniform 227.6 85.1 78.0 60.7 157.5 NeuronAl block-only 108.2 20.3 21.9 29.3 199.1 NeuronAl row-only 200.4 85.3 75.5 60.7 116.8 NeuronAl 89.9 21.5 23.9 29.0 183.9 80% Uniform 20498.5 5219.5 4929.7 330.9 4259.6 NeuronAl block-only 5637.9 458.9 439.9 205.9 1193.0 NeuronAl row-only 11382.1 6715.8 916.8 330.9 4259.6 NeuronAl 3873.6 376.1 213.9 226.3 988.2 Table 18: Ablation study: perplexity achieved when using the block and row steps alone over the PTB dataset. Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Uniform 48.9 24.0 122.2 101.6 23.7 NeuronAl block-only 41.9 16.7 65.6 74.6 23.5 NeuronAl row-only 49.3 22.2 114.5 101.6 26.6 NeuronAl 42.1 17.4 65.1 74.6 25.2 70% Uniform 346.2 157.0 599.3 298.3 209.2 NeuronAl block-only 152.5 41.4 209.7 196.9 250.0 NeuronAl row-only 345.6 148.0 611.8 298.3 170.8 NeuronAl 133.9 43.8 205.3 197.8 194.7 80% Uniform 31362.6 10013.6 5285.2 783.7 2732.1 NeuronAl block-only 6253.3 684.1 1295.2 723.0 1335.6 NeuronAl row-only 18270.0 10008.4 2019.1 783.7 2732.1 NeuronAl 6675.9 566.3 993.2 698.2 988.7 A.4 NeuronAl \lambda Selection In the main text, we presented an experiment regarding the ability of NeuronAl to pick the most performing \lambda parameters (in the block-only cases) at 70% sparsity. Here we include the same analysis at 60% and 80% sparsity. In Fig.s 9-10, it is clearly visible how NeuronAl still performs correctly over different sparsity ratios. It is also worth noticing that the calibration data always come from the C4 dataset and then the results are transferred to the other unknown datasets. Figure 9: Perplexity over different values of \lambda at 60 % sparsity. The orange dot indicates the value selected by NeuronAl. The red line corresponds to the perplexity using a uniform distribution, hence \lambda=0. Figure 10: Perplexity over different values of \lambda at 80 % sparsity. The orange dot indicates the value selected by NeuronAl.The red line corresponds to the perplexity using a uniform distribution, hence \lambda=0. A.5 Calibration Data Ablation In Tables 20-20, we complement the results regarding the seed set of the calibration data at 60% and 80% sparsity. The results are fully in line with the ones presented in the main text. As expected, the standard deviation of the performance increases when increasing the sparsity ratio and at higher sparsity (80%) it turns out to be model dependent. We also conduct further experiments about the size of the calibration data (|C_{\lambda}|), see Fig. 11. Specifically, We compute the perplexity on WikiText2, C4, and PTB using different calibration data of different sizes (1, 2, 4, 8, 16 samples) in order to assess if the performance could be related to it. As clearly visible for almost all models, the performance is basically constant across the different calibration sizes. This does not apply only to OPT models, where for |C_{\lambda}|=16 we reach the best performance by a large margin compared to |C_{\lambda}|=8 (which is the standard value of |C_{\lambda}| used in all the experiments), especially on WikiText2 and PTB. This, as pointed out in the main text, can better explain the reason why NeuronAl is able to outperform all the top-up competitors in Language Modeling but OPT. Table 19: Ablation study: perplexity achieved by NeuronAl with different calibration data seeds (0, 16, 46) at 60% sparsity. Dataset Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B WikiText2 24.5 \pm 0.6 09.5 \pm 0.1 9.3 \pm 0.0 10.1 \pm 0.1 16.2 \pm 0.1 C4 27.0 \pm 0.2 11.9 \pm 0.1 11.9 \pm 0.0 13.8 \pm 0.1 19.1 \pm 0.1 PTB 42.3 \pm 0.7 17.1 \pm 0.3 65.3 \pm 0.8 74.9 \pm 0.2 25.1 \pm 0.1 Table 20: Ablation study: perplexity achieved by NeuronAl with different calibration data seeds (0, 16, 46) at 80% sparsity. Dataset Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B WikiText2 03654.7 \pm 255.1 382.4 \pm 064.7 0247.7 \pm 29.4 216.5 \pm 12.6 1284.9 \pm 482.5 C4 72323.6 \pm 121.7 250.5 \pm 027.1 0265.3 \pm 34.1 171.7 \pm 09.9 0663.5 \pm 316.3 PTB 06014.9 \pm 788.3 624.4 \pm 165.5 1101.9 \pm 94.1 706.1 \pm 06.9 1056.9 \pm 124.5 Figure 11: Perplexity over different values of |C_{\lambda}| (size of the calibration data) when using NeuronAl on the 3 Language Modeling datasets at 70% sparsity. A.6 NeuronAl \lambda Parameters Here we show the \lambda parameters selected by our proposed approach for each model, sparsity ratio, and pruning algorithm tested in this work, aiming to facilitate the reproducibility of our results for the community. Please note that such values are the ones used for each combination sparsity-pruning algorithm-model that have been extracted from C_{\lambda} from C4 (using 0 as seed and 8 as size), and then transferred to all the other datasets/tasks. We report the final \lambda values for both the block and row steps in Table 22 for the first 5 models tested in the main text, and in Table 22 for the LLama-13B models. Table 21: \lambda parameters selected by NeuronAl at sparsity xxx (block | row) for each combination sparsity-pruning algorithm-model. Note that for SparseGPT the row step is not possible (see the main text for details). Sparsity Top-Up Model Phi-2.7B LLama1 7B LLama2 7B Mistral-7B OPT-6.7B 60% Magnitude 0.01 | 0.25 0.10 | 0.20 0.20 | 0.04 0.15 | 0.20 0.25 | 0.25 Wanda 0.10 | 0.25 0.09 | 0.25 0.12 | 0.20 0.09 | 0.00 0.01 | 0.15 multiflow 0.08 | 0.25 0.12 | 0.20 0.12 | 0.20 0.08 | 0.00 0.01 | 0.08 SparseGPT 0.01 | 0.25 0.02 | 0.25 0.06 | 0.25 0.09 | 0.02 | 0.25 70% Magnitude 0.06 | 0.25 0.20 | 0.20 0.25 | 0.00 0.20 | 0.25 0.15 | 0.20 Wanda 0.12 | 0.25 0.15 | 0.20 0.15 | 0.25 0.15 | 0.25 0.25 | 0.20 multiflow 0.15 | 0.25 0.15 | 0.25 0.12 | 0.25 0.15 | 0.00 0.25 | 0.20 SparseGPT 0.02 | 0.25 0.04 | 0.25 0.08 | 0.25 0.08 | 0.25 0.05 | 0.25 80% Magnitude 0.01 | 0.20 0.03 | 0.10 0.01 | 0.25 0.15 | 0.20 0.20 | 0.08 Wanda 0.20 | 0.20 0.15 | 0.20 0.15 | 0.25 0.15 | 0.25 0.20 | 0.09 multiflow 0.12 | 0.25 0.15 | 0.15 0.15 | 0.25 0.15 | 0.20 0.20 | 0.05 SparseGPT 0.06 | 0.25 0.02 | 0.25 0.07 | 0.25 0.08 | 0.25 0.07 | 0.25 Table 22: \lambda parameters selected by NeuronAl on the each combination sparsity-pruning algorithm for Llama-13B (V1 and V2) at sparsity xxx (block | row). Sparsity Top-Up Model LLama1 13B LLama2 13B 60% Magnitude 0.10 | 0.25 0.12 | 0.12 Wanda 0.10 | 0.20 0.09 | 0.25 multiflow 0.15 | 0.20 0.12 | 0.20 70% Magnitude 0.12 | 0.25 0.25 | 0.25 Wanda 0.15 | 0.25 0.12 | 0.20 multiflow 0.15 | 0.15 0.15 | 0.25 80% Magnitude 0.25 | 0.05 0.25 | 0.12 Wanda 0.25 | 0.12 0.15 | 0.25 multiflow 0.25 | 0.10 0.04 | 0.25"
https://arxiv.org/html/2411.07042v1,Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in AI Companion Applications,"Content Warning: This paper presents textual examples that may be offensive or upsetting.AI companions based on large language models can role-play and converse very naturally. When value conflicts arise between the AI companion and the user, it may offend or upset the user. Yet, little research has examined such conflicts. We first conducted a formative study that analyzed 151 user complaints about conflicts with AI companions, providing design implications for our study. Based on these, we created Minion, a technology probe to help users resolve human-AI value conflicts. Minion applies a user-empowerment intervention method that provides suggestions by combining expert-driven and user-driven conflict resolution strategies. We conducted a technology probe study, creating 40 value conflict scenarios on Character.AI and Talkie. 22 participants completed 274 tasks and successfully resolved conflicts 94.16% of the time. We summarize user responses, preferences, and needs in resolving value conflicts, and propose design implications to reduce conflicts and empower users to resolve them more effectively.","Human-AI conflict refers to a state of incompatibility, inconsistency, or opposition between humans and AI (Flemisch et al., 2020). In past research, human-AI conflicts were usually simple and direct—AI was more like a tool, and conflicts often stemmed from technical limitations, such as task execution failures (Wen et al., 2022), or disagreements with users in simple decision-making (Babel et al., 2024; Takayama et al., 2009). These types of conflicts generally lacked emotional and value entanglement, making them less likely to cause significant psychological harm to users. Recently, a diverse array of Large Language Model (LLM) agents has emerged, offering capabilities ranging from personalized assistance to performing complex tasks (Chen et al., 2024). The study focuses on LLM-based AI companion applications, such as Character.AI, Talkie, Replika, Kindroid, Paradot, and Xingye. As of July 2024, the total number of users of these applications has exceeded 900 million globally (including duplicate users across different applications)111User statistics source: https://www.data.ai.. AI companions can role-play and respond to users in a human-like manner, providing emotional support and companionship (Sullivan et al., 2023). For instance, in Character.AI, users can personalize the companion’s personality traits and interaction contexts through “Description,” “Greeting,” and “Definition.” Compared to earlier non-LLM chatbots, LLMs endow AI companions with a stronger ability to understand language, enabling them to engage in more context-aware and intelligent interactions (Kasneci et al., 2023), fostering more complex and intimate human-AI relationships (Maeda and Quan-Haase, 2024). Many users even consider them close friends or lovers (Skjuve et al., 2021, 2022). The deepening of this relationship raises users’ expectations of AI companions, but it may also lead to deeper conflicts, including value conflict. For example, some users have shared online their experiences of encountering sexist remarks from AI companions, describing how they engaged in intense arguments with the AI, which left them frustrated, angry, and hurt (Zhou, 2023). As the relationship between AI companions and users becomes more interpersonal, previous conflict resolution strategies for human-AI conflict have started to fail (Rosen, 2014; Babel et al., 2024). Strategies based solely on technical limitations are no longer sufficient, and it is becoming important to draw on interpersonal conflict resolution methods and users’ real-world experiences with AI companions. Although Fan et al. provide initial insights into value alignment and conflicts between users and AI companions (Fan et al., 2024), inexperienced users often find it challenging to resolve these issues independently. How to design tools that empower users to handle value conflicts with AI companions remains an unexplored research gap that this work aims to address. In this work, we first conducted a formative study to understand and characterize the value conflicts between users and AI companions (Schwartz, 2012). We analyzed 151 user complaint posts from social media platforms, finding that many conflicts are value-laden. Building on this, we constructed a value conflict framework for AI companion applications (Schwartz, 2012), which provided real-world data for our technology probe study, allowing us to reconstruct actual value conflict scenarios. Combining prior research on conflict resolution (Brett et al., 1998; Shaikh et al., 2024; Mun et al., 2023) with our formative study, we found that interactions between users and AI companions exhibit complex dynamics, where relying solely on expert strategies from other conflict scenarios (e.g., interpersonal conflict theories (Brett et al., 1998)) is insufficient. The value conflicts users face in real-life situations are diverse, and through their interactions with AI companions and exchanges on social platforms, users have accumulated certain conflict resolution experiences. Therefore, it is necessary to draw from both expert theories and the practical experiences of AI companion users to explore more suitable solutions (Fan et al., 2024). Then, we created Minion, a technology probe (Hutchinson et al., 2003) that provides users multiple suggestions for resolving value conflicts while gaining insights into user behaviors. Minion’s algorithm combines expert-driven and user-driven conflict resolution strategies. We developed LLM prompts to address value conflict situations between users and AI companions by drawing on two key sources. First, we drew upon Shaikh et al.’s solutions for interpersonal conflict resolution (Shaikh et al., 2024) to guide our expert-driven conflict resolution strategies. Second, we referenced the user-driven strategies identified in the study by Fan et al. (Fan et al., 2024) to capture how users manage conflicts with AI companions. To empirically test Minion, we conducted a technology probe study (Hutchinson et al., 2003) with 22 participants. We created 40 distinct conflict scenarios on two popular AI companion platforms, Character.AI and Talkie. Each scenario was designed with specific conflict resolution goals. Participants completed 274 tasks, achieving an overall conflict resolution rate of 94.16%. Minion received positive feedback from participants and inspired them with new ideas in conflict resolution. Based on our findings, we discuss the opportunities and challenges in integrating expert-driven and user-driven strategies in resolving human-AI value conflicts, and call for further research in this area, focusing on the dynamics of emerging human-AI relationships. Our work’s contributions are as follows: • A novel user-empowerment intervention method that combines expert-driven and user-driven conflict resolution strategies. This method is presented in the form of the technology probe Minion, serving as a prototype for future tools aimed at resolving human-AI value conflicts. • We empirically tested Minion in a one-week technology probe study (N=22). The results demonstrated the technical feasibility of Minion. We summarized users’ responses, preferences, and needs when dealing with value conflicts with AI companions. • Based on the formative and technology probe studies, we explored the opportunities and challenges of integrating expert-driven and user-driven strategies in human-AI value conflicts. We also proposed design implications for future human-AI conflict resolution solutions, particularly in the field of AI companions."
https://arxiv.org/html/2411.06908v1,EVQAScore: Efficient Video Question Answering Data Evaluation,"Video question-answering (QA) is a core task in video understanding. Evaluating the quality of video QA and video caption data quality for training video large language models (VideoLLMs) is an essential challenge. Although various methods have been proposed for assessing video caption quality, there remains a lack of dedicated evaluation methods for Video QA. To address this gap, we introduce EVQAScore, a reference-free method that leverages keyword extraction to assess both video caption and video QA data quality. Additionally, we incorporate frame sampling and rescaling techniques to enhance the efficiency and robustness of our evaluation, this enables our score to evaluate the quality of extremely long videos. Our approach achieves state-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for Spearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on the VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using EVQAScore for data selection, we achieved SOTA results with only 12.5% of the original data volume, outperforming the previous SOTA method PAC-S and 100% of data. Our code is available at https://anonymous.4open.science/r/EVQAScore-FFA6/.","With the rapid advancements in large language models (LLMs) [30, 36] and multimodal large language models (MLLMs) [45, 40], Figure 1: We give an example of EVQAScore, from the figure we can see that Answer1 is low quality and contains few information while Answer2 contains more information and is a more comprehensive answer. We can see our EVQAScore is the only score can correctly identify the high quality data. Among MLLMs, VideoLLMs achieve competitive performance in traditional multimodal tasks such as visual recognition [44], video understanding [41, 35], and action recognition [39]. Moreover, their excellent language understanding capabilities enable strong performance in text-rich tasks, such as video question-answering [9] and video-centric dialogues [39]. Most existing VideoLLMs focus on modifying model architectures to effectively utilize information from multiple modalities [38, 39, 26, 43, 17]. While model effectiveness is crucial, data quality also plays a significant role in the success of VideoLLMs. For instance, Li et al. [18], Wang et al. [39] demonstrate that higher-quality training data enhances the performance of VideoLLMs. Therefore, evaluating video QA data quality is essential. However, previous methods such as EMScore and PAC-S [34, 33] focus exclusively on video caption evaluation while neglecting video QA evaluation. Moreover, these methods struggle with understanding the correct semantics of longer captions or QA pairs. As illustrated in Figure 1, EMScore and PAC-S fail to identify higher-quality video QA pairs. These methods overlook the importance of efficient and robust data management and face three key challenges: C1. Inability to Accurately Understand Caption and QA Semantics Previous methods such as EMScore [34] and PAC-S [33] rely on TF-IDF to evaluate video caption data. However, TF-IDF merely computes word frequency and does not capture semantic meaning. As a result, these methods often fail to accurately evaluate video captions and are unable to assess video QA data with precision. Thus, there is a need for improved semantic understanding in Video QA evaluation. C2. Low Evaluation Efficiency Given the extensive volume and length of video data, large-scale training datasets frequently occupy substantial storage space, ranging from several hundred gigabytes to tens of terabytes [38, 39, 20]. Evaluating video caption and QA data is time-consuming and computationally costly. Previous methods like EMScore [34] and PAC-S [33] process every frame, leading to considerable computational waste. Therefore, developing a computationally efficient evaluation metric is essential. C3. Lack of VideoLLM Experiments Data plays a crucial role in the success of VideoLLMs [38, 39, 26, 43, 17]. Although various methods have been proposed to evaluate the quality of video caption data, they have not incorporated VideoLLM experiments to assess the effectiveness of their methods in training VideoLLMs. Consequently, additional VideoLLM experiments are needed. To address these challenges, we introduce EVQAScore, which utilizes LLMs to extract keywords for enhanced video captioning and video QA data evaluation. To support the evaluation of longer videos, we efficiently compute EVQAScore through uniform frame sampling, which reduces computational costs by a factor of 30 without affecting results. Compared to previous methods such as EMScore [34] and PAC-S [33], our approach can be applied to video QA evaluation and achieves superior visual understanding through keyword extraction, outperforming the TF-IDF technique used by EMScore and PAC-S. Additionally, due to its efficiency and keyword extraction capabilities, our method can handle extremely long captions and videos, a feat not possible for EMScore and PAC-S because of the 77-word limit imposed by CLIP. The core contributions of this paper are summarized as follows: • New Perspective: We introduce a novel approach, EVQAScore, designed for the evaluation of Video QA and video caption data quality. To the best of our knowledge, this is the first systematic effort to evaluate the quality of Video QA data. • New Method: Our approach leverages LLMs and keyword extraction techniques to improve the understanding of QA data in Video QA evaluation. Additionally, we employ uniform frame sampling to enhance scalability, allowing EVQAScore to be applied to extremely long videos and large datasets. • Efficient Video QA Data Evaluation: We use uniform frame extraction, resulting in a computational cost reduction of over 30 times compared to processing all frames. Furthermore, we demonstrate that this reduction does not lead to any performance degradation. • SOTA Performance for Video Caption Evaluation: We evaluated the performance of our method on the VATEX-Eval benchmark for Video Caption data, achieving a Kendall correlation of 32.8 and a Spearman correlation of 42.3, which are 4.7 and 5.9 points higher, respectively, than the previous method PAC-S++. • SOTA Performance for Video QA Evaluation: We constructed a dataset comprising 400K video entries, consisting of 200K noisy samples and 200K high-quality samples. Using our EVQAScore and the previous SOTA model PAC-S, we filtered the data, retrieving a significantly lower amount of noisy data than PAC-S. Additionally, when evaluated on ActivityNet, MSRVTT, MSVD, TGIF, MVBench, and two longer benchmarks—VideoChatGPT Bench and VideoChatGPT Bench Diverse—our EVQAScore outperformed PAC-S and 100% of data with only 12.5% of data."
https://arxiv.org/html/2411.06866v1,Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense Question Answering,"Commonsense question answering is a crucial task that requires machines to employ reasoning according to commonsense. Previous studies predominantly employ an extracting-and-modeling paradigm to harness the information in KG, which first extracts relevant subgraphs based on pre-defined rules and then proceeds to design various strategies aiming to improve the representations and fusion of the extracted structural knowledge. Despite their effectiveness, there are still two challenges. On one hand, subgraphs extracted by rule-based methods may have the potential to overlook critical nodes and result in uncontrollable subgraph size. On the other hand, the misalignment between graph and text modalities undermines the effectiveness of knowledge fusion, ultimately impacting the task performance. To deal with the problems above, we propose a novel framework: Subgraph REtrieval Enhanced by GraPh-Text Alignment, named SEPTA. Firstly, we transform the knowledge graph into a database of subgraph vectors and propose a BFS-style subgraph sampling strategy to avoid information loss, leveraging the analogy between BFS and the message-passing mechanism. In addition, we propose a bidirectional contrastive learning approach for graph-text alignment, which effectively enhances both subgraph retrieval and knowledge fusion. Finally, all the retrieved information is combined for reasoning in the prediction module. Extensive experiments on five datasets demonstrate the effectiveness and robustness of our framework.","Commonsense question answering (CSQA) is a critical task in natural language understanding, which requires systems to acquire different types of commonsense knowledge and possess multi-hop reasoning ability [27, 19, 22]. Though massive pre-trained models have achieved impressive performance on this task, it is difficult to learn commonsense knowledge solely from the pre-training text corpus, as the commonsense knowledge is evident to humans and rarely expressed explicitly in natural language. Compared with unstructured text, structured data like knowledge graphs is much more efficient in representing commonsense [26]. The incorporation of external knowledge aids PLMs in comprehending question-answer (Q-A) pairs, while the entity relations enhance the model’s reasoning capabilities. Therefore, various commonsense knowledge graphs (CSKGs) (e.g., ConceptNet [25]) have been adopted in previous studies. Existing KG-augmented models for CSQA primarily adhere to a extracting-and-modeling paradigm [36, 26, 29, 32, 28, 35]. First, the knowledge subgraphs or paths related to a given question are extracted by string matching or semantic similarity, which indicate the relations between concepts or imply the process of multi-hop reasoning. Subsequently, diverse strategies emerge for the efficient representation and fusion of the extracted structural knowledge. One research path [12, 8] involves elaborately crafting graph neural networks for better modeling the extracted subgraphs, whereas another [34, 26] explores the efficient incorporation of knowledge from KG into language models by enhancing the interactions between PLMs and GNNs. Despite their success, these approaches still have several limitations. First, the subgraph’s quality suffers when retrieved through a simple string or semantic matching, posing limitations for subsequent operations. To obtain sufficient relevant knowledge, the number of nodes will expand dramatically with the increase of hop count, inevitably raising the burden of the model. Despite its ample size, certain crucial nodes might remain elusive, since some entities are not learned during the pre-training. Besides, the edges linked to the peripheral nodes within the subgraph are pruned, causing the message-passing mechanism of GNN to be blocked and impairing the attainment of effective representations, consequently undermining valuable information. Second, the misalignment between graph and text encoders presents a challenge for PLMs to internalize the knowledge contained in the acquired subgraph, especially in scenarios with limited data, leading to a reduced task performance [35]. Though Dragon [33] proposes a pre-training method to align GNNs and PLMs, it requires additional corpus, and the text-to-graph style to construct semantically equivalent graph-text pairs is challenging. The necessity for substantial computational resources poses another hurdle, prompting the search for a more efficient alignment method. In this paper, we propose a novel framework: Subgraph REtrieval Enhanced by GraPh-Text Alignment (SEPTA), for CSQA. To mitigate the shortcomings of the subgraph extraction process, we establish a database of subgraph vectors derived from the knowledge graph. Consequently, the challenge shifts from retrieving a pertinent subgraph to obtaining relevant subgraph vectors. A BFS-style sampling method is employed to obtain the connected graph for each node and the embedding of the subgraph is subsequently stored in the database. Drawing on the parallels between BFS and the message-passing mechanism of GNNs, the central node’s representation learned from the subgraph could be closely aligned with that derived from the entire graph, with almost no information loss. Besides, to further improve the retrieval accuracy and facilitate knowledge fusion during the prediction, we consider aligning the semantic space of the graph and text encoders, proposing an effective approach for graph-text alignment. A novel graph-to-text method is proposed to construct high-quality semantically equivalent training pairs, with no requirement of external corpus and easy to train. Finally, all the information retrieved is combined by a simple attention mechanism to facilitate the model in commonsense reasoning. Our contributions can be summarized as follows: • We propose a novel and effective framework SEPTA, where we convert the knowledge graph into a subgraph vector database and retrieve relevant subgraphs to facilitate commonsense reasoning. • We design a bidirectional contrastive learning method to align the semantic space of the graph and text encoders, with a graph-to-text method to construct high-quality graph-text pairs, which facilitates subgraph retrieval and knowledge fusion. • We propose a BFS-style subgraph sampling strategy for subgraph construction. Drawing on the parallel between BFS and the message-passing mechanism, our method can preserve complete neighbor information for each node. • We conduct extensive experiments on five datasets. Our proposed approach achieves better results than the state-of-the-art approaches and has promising performance in weakly supervised settings."
https://arxiv.org/html/2411.06710v1,Model Fusion through Bayesian Optimization in Language Model Fine-Tuning,"Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model, one effective solution is model fusion, which combines multiple models in a parameter space. However, we observe a large discrepancy between loss and metric landscapes during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization. In addition, to effectively select hyperparameters, we establish a two-stage procedure by integrating Bayesian optimization processes into our framework. Experiments across various downstream tasks show considerable performance improvements using our Bayesian optimization-guided method. Code will be available at: https://github.com/chaeyoon-jang/bomf.git.","The field of Natural Language Processing (nlp) has significantly advanced with the pre-training of Transformer-based models on large amounts of texts without supervision. In general, these pre-trained networks are fine-tuned on supervised downstream tasks to solve particular tasks. The rise of Large Language Models (llms) such as gpt [50] and llama [63] has increased demands for huge memory and computing during fine-tuning on downstream tasks. In response, low rank approximation methods such as Low-Rank Adaptation (lora) [22] and Quantized Low-Rank Adaptation (qlora) [11] have been adopted recently to fine-tune the llm. However, the fine-tuning of Pretrained Language Models (plms) exhibits high sensitivity to marginal variations in hyperparameters such as learning rate and batch size, often leading to training failure and the performance drop of a fine-tuned model [45], while searching hyperparameters requires a vast amount of resources. An effective strategy to seek an optimal model among multiple candidates is model ensembling, which yields impressive performance on generalization and robustness [33]. However, traditional ensemble methods lead to several drawbacks including the space and computational costs that linearly scale with the number of models involved. These issues are particularly pertinent for llms, since individual models are costly to train and test. Therefore, we can alternatively utilize model fusion to aggregate multiple models into a single proficient model on a parameter space. One of its simplest forms, known as Stochastic Weight Averaging (swa) [25], involves taking the average of model parameters obtained during an optimization process. Despite its simplicity, swa and its variants have proven successful across various tasks, notably in computer vision [25, 42, 6, 46]. Recent advancement in this field is the concept of Model Soups, which has been introduced by Wortsman et al. [70]. This approach weight-averages a set of models, obtained from multiple fine-tuning runs with different hyperparameters to create a powerful model that outperforms both individual and ensemble models. The effectiveness of model fusion has predominantly been explored in the visual domain. For instance, while Model Soups have shown considerable improvements in image classification, they have not demonstrated superiority in the nlp tasks [70]. The existing averaging methods like swa make use of their ability to encourage a fused model to locate on the center of the flatter area near local optima [25, 20], as loss landscapes are analogous to metric landscapes in computer vision tasks. Unfortunately, for plms, loss landscapes are substantially mismatched to metric landscapes, so that the flat loss minimum found by swa does not necessarily correspond to the flat metric minimum making a simple averaging method fail to find a superior model. In this paper, we present a novel model fusion approach with an efficient hyperparameter selection strategy, denoted as Bayesian Optimization Model Fusion (bomf), specifically designed to fine-tune plms. To motivate our method, we start by illustrating two empirical analyses. Firstly, we demonstrate that the existing model fusion techniques are not suitable for plms. Secondly, we highlight that the optimal hyperparameters for plms exhibit consistency on varying the number of frozen layers or the rank used in the lora setting [22]. Based on these findings, we introduce our proposed method to build a single model, aggregated through the weighted combination of individual models. Supposing that evaluation metrics are non-differentiable, we employ Bayesian Optimization (bo) [5, 18], which is a black-box optimization technique, in developing our model fusion method. To the best of our knowledge, this is the first study that utilizes bo in the context of model fusion, in order to achieve the following objectives: • Utilization of Both Metrics and Loss Functions in Model Fusion. Instead of running bo with an averaged target metric, we use Multi-Objective Bayesian Optimization (mobo) that considers both metrics and loss functions for model fusion. Despite low correlations between loss and metric values, we find that incorporating loss values still serves as useful guidance. • Two-Stage Model Fusion. We devise our model fusion process as a two-stage bo procedure. One is for optimizing hyperparameters in fine-tuning and the other is dedicated to our model fusion method. The objective of the first stage is to maximize gains from the second stage to find hyperparameters leading to the optimal fused model after the bo of the second stage. We demonstrate the effectiveness of bomf on several nlp tasks, including both Natural Language Understanding (nlu) and Natural Language Generation (nlg), with roberta, Text-to-Text Transfer Transformer (t5) and llama. Through these comprehensive experiments, we assess the performance of bomf in diverse nlp tasks and uncover the interesting properties of our approach through various ablation studies."
https://arxiv.org/html/2411.06657v1,Renaissance: Investigating the Pretraining of Vision-Language Encoders,"In the past several years there has been an explosion of available models for vision-language tasks. Unfortunately, the literature still leaves open a number of questions related to best practices in designing and training such models. In this paper we seek to answer several questions related to the pretraining of vision-language encoders through meta-analysis. In our first set of experiments, we show that we can save significant compute at no cost to downstream performance, by freezing large parts of vision-language models during pretraining. In our second set of experiments we examine the effect of basing a VL transformer on a vision model versus a text model. Additionally, we introduce a VL modeling platform called Renaissance that we use to conduct all of the experiments. This program offers a great deal of flexibility in creating, training and evaluating transformer encoders for VL modeling. The source code for Renaissance can be found at https://github.com/bsu-slim/renaissance.","In the span of a few years, dozens of vision-language (VL) transformers have appeared in the literature with a bewildering array of architectures and training methods (see Fields and Kennington (2023) for a review). VL tasks, such as NLVR2 Suhr et al. (2018) where the model is tasked with answering questions about images (see Figure 4 for an example) and image captioning require models to somehow represent and fuse both text and image information. Unfortunately, knowledge of best practices for training and implementing these models has lagged far behind the model development process. This stands in contrast to the NLP domain, where studies such as Rogers et al. (2021) and Kaplan et al. (2020) have thoroughly investigated the inner workings and best training practices for NLP transformers. To date, there have been only a handful of studies analyzing VL-transformers, such as Bugliarello et al. (2021), and the collected literature still fails to address some very basic questions concerning VL modeling with transformers. In this paper we begin to address this gap by providing a systematic analysis geared toward shedding light on some basic aspects of training transformers for vision-language modeling. In particular, we focus on the pretraining and fine-tuning of transformer-encoder architectures. Transformer encoders are best suited toward discriminative tasks such as the NLVR2 benchmark that we mentioned in the opening paragraph and we do not address generative tasks like image captioning here. In our first set of experiments (Section 4), we ask whether it is possible to save compute by freezing parts of the model during pretraining and examining the effect on downstream performance. In our second and final set of experiments (Section 5) we compare the performance of a VL transformer based on a pretrained text encoder versus one based on a pretrained vision transformer. Both sets of experiments will help to establish best training practices for those interested in training VL transformers and hopefully also provide theoretical insight. To perform our experiments, we created a novel VL framework that we call Renaissance that streamlines the ability to evaluate different VL model types (e.g., 1-tower and 2-tower) against a suite of benchmarks. The specific contributions of this paper can be summarized as follows: • We introduce a software platform Renaissance that offers a range of options for creating, training and testing vision-language transformer encoder models. • We demonstrate that a great deal of compute can be saved by freezing parts of two-tower encoder models during pretraining. In particular, freezing the visual module can actually lead to small increases in performance. When both modules are frozen there is some loss in downstream performance, though the benefits may outweigh the costs for those with compute-limited training setups. • We show that when training a one-tower encoder model, it is best to initialize the model’s weights randomly than to use pretrained weights from either a text or a vision encoder model."
https://arxiv.org/html/2411.06646v1,Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data,"When training deep neural networks, a model’s generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models (LLMs), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension d of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in d. By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.","Deep learning has made remarkable breakthroughs in various real-world applications, such as natural language processing (Graves et al., 2013; Bahdanau et al., 2014; Liu et al., 2023; Vaswani et al., 2017), computer vision (Krizhevsky et al., 2012; Goodfellow et al., 2014; Song et al., 2020), healthcare (Miotto et al., 2018), and robotics (Gu et al., 2017). A neural scaling law between the generalization error (or test loss) and several quantities, including the model size, the training data size, and the amount of compute, plays a key role in the performance of neural networks. Perhaps the best known example of such scaling laws are for transformer-based LLMs. Recent works in Hestness et al. (2017); Rosenfeld et al. (2019); Kaplan et al. (2020); Bahri et al. (2021) demonstrated a power law between the test loss and the network size, the training data size, and the amount of compute for transformer-based LLMs. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. Understanding the theory behind neural scaling laws provides invaluable insights into practical applications of deep learning. A mathematical principal of neural scaling laws enables researchers and practitioners to describe and analyze the performance of neural networks with precision and rigor. The neural scaling law between the generalization error and the network size can be partially explained via neural network representation theory (Yarotsky, 2016). Further, the neural scaling law between the generalization error and the training data size n can be explained via statistical estimation theory. For feedforward neural networks (Schmidt-Hieber, 2020) and convolutional residual networks (Oono and Suzuki, 2019), a generalization error bound has been established for regression. Schmidt-Hieber (2020); Oono and Suzuki (2019) predicted \text{Generalization Error}\sim n^{-c/D} where n is the training data size, D is the data dimension and c is a constant. This predicted rate of convergence is extremely slow for high dimensional data when D is large, while the rate of convergence observed in real-world applications is significantly faster, which reveals a gap between theory and practice. This gap can be bridged by exploiting low-dimensional structures of data. Real-world data sets often exhibit low-dimensional geometric structures due to rich local regularities, global symmetries, or repetitive patterns (Tenenbaum et al., 2000; Roweis and Saul, 2000). According to Min et al. (2023, Figure 1), the intrinsic dimension of CIFAR-100, CelebA and ImageNet datasets are about 20,20 and 40 respectively. When the low-dimensional geometric structure of data is modeled by a manifold, the predicted scaling for regression, classification and distribution estimation becomes \text{Generalization Error}\sim n^{-c/d}, where n is the training data size, d is the intrinsic dimension of the data manifold, and c is a constant (Chen et al., 2022; Liu et al., 2021; Dahal et al., 2022; Nakada and Imaizumi, 2020). In Sharma and Kaplan (2022), the neural scaling law between the test loss and the network size was predicted to be \text{Test loss}\sim({size})^{-{4}/{d}} where d is the intrinsic dimension of data. While the theoretical studies focus on feedforward neural networks (Chen et al., 2022; Nakada and Imaizumi, 2020) and convolutional residual networks (Liu et al., 2021), a generalization to transformer-based neural networks (Vaswani et al., 2017) is of great interest but widely open. This paper establishes mathematical approximation and statistical estimation theories to predict and justify the scaling law between the generalization error and the model/data size for transformer neural networks. We consider regression of a \beta-Hölder continuous function f:{\mathcal{M}}\rightarrow\mathbb{R} where {\mathcal{M}} is a d-dimensional compact Riemannian manifold isometrically embedded in \mathbb{R}^{D}. After embedding the input x\in\mathcal{M}\subset\mathbb{R}^{D} to a proper sequence, we apply a transformer network on the embedded sequence to learn the function f. Our main results are on the statistical estimation and universal approximation theories of Hölder continuous functions on \mathcal{M} by transformer neural networks. Statistical Theory: In Theorem 1, we consider the global empirical risk minimizer \hat{\textup{T}}_{n} from n i.i.d. training data \{(x_{i},f(x_{i}))\}_{i=1}^{n}, given by \displaystyle\textstyle\hat{\textup{T}}_{n}=\operatorname*{arg\,min}_{\textup{% T}\in\mathcal{T}}\frac{1}{n}\sum_{i=1}^{n}\big{(}\textup{T}(x_{i})-f(x_{i})% \big{)}^{2}, (1) under a properly chosen transformer network architecture \mathcal{T}. We prove that, the generalization error of \hat{\textup{T}}_{n} satisfies \mathbb{E}\|\hat{\textup{T}}_{n}-f\|_{L^{2}(Q)}^{2}\leq\tilde{O}\big{(}Dd^{2}n% ^{-\frac{2\beta}{2\beta+d}}\big{)} (2) where Q denotes the distribution of x, and \tilde{O} hides constants and \log n terms. Approximation Theory: In Theorem 2, we construct a transformer network to universally approximate \beta-Hölder continuous functions on \mathcal{M} with an arbitrarily given accuracy \varepsilon. Notably, the network is shallow, requiring only O\big{(}\log(d)\big{)} independent of the desired accuracy \epsilon to approximate f locally. This highlights a major advantage of Transformers over feed-forward ReLU networks, which require O\big{(}\log(\frac{1}{\epsilon})\big{)} layers to achieve the same accuracy. In our proof, we embed the entries of x=[x^{1},\ldots,x^{D}]\in\mathcal{M} into tokens such that the x^{i}’s appear in a sequence. Our proof for the approximation theory explicitly constructs transformers to realize the interaction between different tokens efficiently via a crucial Interaction Lemma 3. This lemma allows us to flexibly implement many common operations including addition, multiplication, and parallelization, and so may of independent interest. In our proof for the statistical theory, we calculate the covering number of our transformer network class, which is also of independent interest. Neural Scaling Laws and the Intrinsic Dimension: Our generalization error bound in (2) predicts the following neural scaling law between the generalization error and the data size n: \text{Squared Generalization Error}:=\mathbb{E}\|\hat{\textup{T}}_{n}-f\|_{L^{% 2}(Q)}^{2}\lesssim n^{-\alpha_{D}},\ \text{ where }\alpha_{D}=\frac{2\beta}{2% \beta+d}, (3) with sufficient data. Our approximation theory in Theorem 2 predicts the following neural scaling law between the approximation error and the network size N: \textstyle\text{Squared Approximation Error}:=\inf_{\textup{T}\in\mathcal{T}}% \|\textup{T}-f\|_{L^{\infty}(\mathcal{M})}^{2}\lesssim N^{-\alpha_{N}},\ \text% { where }\alpha_{N}=\frac{2\beta}{d} (4) for a sufficiently large network class \mathcal{T}. Our prediction of the power scaling law is consistent with our own empirical observations, and those in Kaplan et al. (2020) and Biderman et al. (2023). More importantly, our theory quantifies the power \alpha_{D},\alpha_{N} in terms of the intrinsic dimension of data. Experimental Validation on LLMs: After establishing our theory we seek to validate it in practice by predicting empirical scaling laws for LLMs trained on natural language data. To test our predictions for the data scaling law, we pretrain a series of small (125 million parameter) LLMs on three datasets (Gokaslan et al., 2019; Eldan and Li, 2023; Kocetkov et al., 2022). We find close agreement (\pm 0.02) between our predicted scaling exponent {\alpha}_{D} and the observed exponents \hat{\alpha}_{D}. To evaluate our predictions for the model scaling exponent \alpha_{N}, we rely on publicly available scaling suites (Biderman et al., 2023; Radford et al., 2019) whose intrinsic data dimensions we can estimate. We find our predictions are still close but less accurate for \alpha_{N}. Finally, we carry out a series of ablations investigating factors impacting the estimated intrinsic data dimension d. For a fixed dataset, we find the estimated d is stable with respect to several factors including the model size, model embedding dimension, and context length111Code is available at https://github.com/Dahoas/transformer_manifolds_learning. In summary, we make the following contributions: • A novel approximation theory for transformers approximating Hölder continuous functions on a d-dimensional manifold, requiring O(\log(d)) depth independent of the accuracy \epsilon. • A novel computation of the covering number of our transformer network class. This is used to establish generalization bounds exponentially depending on the intrinsic dimension d. • Empirical experiments demonstrating our theory predicts data scaling laws for LLMs as a function of the estimated intrinsic data dimension d. • An empirical study of several factors affecting the estimated intrinsic data dimension for transformers including model size, embedding dimension, layer depth, and context length. We will present our main theory in Section 2, numerical validation of our theory and the prediction of neural scaling laws in Section 3. We will discuss related work in Section 4 and conclude our paper in Section 5. Our pre-training hyperparameters are given in Appendix A. The derivation of neural scaling laws is presented in Appendix B. Our notation is given in Appendix C, and proofs are presented in Appendix E and F."
https://arxiv.org/html/2411.06638v1,Model Editing for LLMs4Code: How Far are We?,"Large Language Models for Code (LLMs4Code) have been found to exhibit outstanding performance in the software engineering domain, especially the remarkable performance in coding tasks. However, even the most advanced LLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to the high cost of training LLMs4Code, it is impractical to re-train the models for fixing these problematic code knowledge. Model editing is a new technical field for effectively and efficiently correcting erroneous knowledge in LLMs, where various model editing techniques and benchmarks have been proposed recently. Despite that, a comprehensive study that thoroughly compares and analyzes the performance of the state-of-the-art model editing techniques for adapting the knowledge within LLMs4Code across various code-related tasks is notably absent. To bridge this gap, we perform the first systematic study on applying state-of-the-art model editing approaches to repair the inaccuracy of LLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists of two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and CodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help of CLMEEval, we evaluate six advanced model editing techniques on three LLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings include that the external memorization-based GRACE approach achieves the best knowledge editing effectiveness and specificity (the editing does not influence untargeted knowledge), while generalization (whether the editing can generalize to other semantically-identical inputs) is a universal challenge for existing techniques. Furthermore, building on in-depth case analysis, we introduce an enhanced version of GRACE called A-GRACE, which incorporates contrastive learning to better capture the semantics of the inputs. Results demonstrate that A-GRACE notably enhances generalization while maintaining similar levels of effectiveness and specificity compared to the vanilla GRACE.","Large Language Models (LLMs) have demonstrated their powerful understanding and generating capabilities [1, 2, 3, 4], and have been applied to areas such as autonomous agents [5], medicine [6], and recommendation system [7]. LLMs for code (LLMs4Code), trained on massive code-related datasets [8], also show remarkable performance in coding tasks within software engineering [9], including code generation[10, 11] and code comment generation[12, 13]. However, even the most advanced LLMs4Code can contain outdated and incorrect code knowledge due to the following reasons[14]. On one hand, the training data for LLMs4Code is limited to a certain period, which implies that LLMs4Code cannot learn about the latest software package characteristics from this data as software is continuously changing [15]. On the other hand, the massive training data inevitably contains some noise[16], which ultimately leads LLMs4Code to learn some incorrect code knowledge. If the aforementioned concerns are not promptly addressed, LLMs4Code will continue to produce bugs or vulnerabilities in production environments[17, 18]. A natural way to fix these issues is retraining, but this “Killing a fly with a cannon” approach not only consumes a considerable amount of computational resources but also takes a lot of time. Recently, researchers have explored Model Editing to repair LLMs’ outdated and incorrect knowledge, whose aim is to efficiently and effectively update targeted knowledge without affecting the other non-targeted knowledge of LLMs[19]. Existing model editing approaches can generally be categorized into three classes, which are based on External Memorization, Global Optimization, and Local Modification, respectively [20]. External Memorization adds an extra module to the original model to store updated knowledge; Global Optimization directly or indirectly updates model weights using fine-tuning gradients; Local Modification first identifies critical modules in the model where the target knowledge is stored, then uses specific algorithms to update the weights in these key modules. State-of-the-art model editing approaches have achieved remarkable success for updating the knowledge of LLMs[21, 22, 23, 24]. Given that, researchers have also explored to utilize model editing to fix errors produced during the code generation process [17]. Despite that, this initial study has certain limitations due to the following reasons. First, tasks in the software engineering domain like code generation and code summarization often involve sequence generations, but the proposed approach by Gu et al.[17] focuses on a single token at each time, which is hard to apply to sequence generation and thus misaligns with the application scenarios. Second, this existing study mainly focuses on assessing the ability of the edited model to generate correct contents, while other essential properties of LLMs, such as the fluency, i.e., measuring the extent to which the edited model can generate natural and non-repetitive contents, are ignored. Third, the existing study focuses on a target model with a maximum of 350M parameters, a scale that is relatively restricted compared to the typical size of general LLMs4Code models, which commonly comprise several billion parameters[1]. Consequently, the literature has limited understanding regarding the strengths and weaknesses of existing model editing techniques within the context of LLMs4Code, and it remains an open question as to how effectively these techniques can update the code knowledge embedded in LLMs4Code. There is thus an urge need for a comprehensive empirical study comparing and analyzing the performance of all the state-of-the-art model editing techniques on LLMs4Code. Such a study is necessary and essential, as it can provide answers to fundamental questions such as which types of approaches are the most proficient. This insight can serve as a guideline for researchers in devising more proficient techniques in the future. To bridge this gap, we perform the first systematic study on applying state-of-the-art model editing approaches to repair the inaccuracy of LLMs4Code. To that end, we first build an evaluation benchmark, Code Large Language Models Editing Evaluation (CLMEEval), which consists of two datasets, CoNaLa-Edit (CNLE) and CodeSearchNet-Edit (CSNE), corresponding to the editing of code knowledge in the context of two widely-studied software engineering tasks: a natural language to programming language (NL2PL) code generation task and a programming language to natural language (PL2NL) code summarization task. Drawing from this benchmark, we employ model editing techniques to rectify the inaccuracies produced by LLMs4Code, mirroring real-world scenarios where updates to code knowledge within a model are necessary. This could involve situations like changes in required APIs for completing specific coding tasks or shifts in the primary functionality of a method due to code changes. Following the common practice in the model editing domain [25], our study evaluates the approaches from four dimensions: Effectiveness, the success rate on editing instances; Generalization, the success rate on tests that are semantically identical to the editing instances; Specificity, the success rate on tests unrelated to the editing instances; and Fluency, the fluency of the contents generated by the model [25]. We select six state-of-the-art model editing approaches from the three categories mentioned above and three widely-used LLMs4Code, i.e., CodeLlama (7B)[26], CodeQwen-1.5 (7B) [27], and StableCode (3B)[28] as our study subjects. Through an extensive evaluation, our study makes the following important findings: F1: The External Memorization-based technique, GRACE, can consistently achieve the optimal effectiveness and specificity across different datasets and LLMs4Code. Nonetheless, all the existing model editing techniques perform poorly in terms of generalization. F2: Most model editing techniques perform comparatively poorly on LLMs4Code, being far less proficient compared with editing general LLMs. F3: Model editing techniques are sensitive to the specific tasks, with all the editing techniques performing worse in NL2PL editing than in PL2NL editing. Moreover, through a case analysis on the best-performing editing approach in terms of the effectiveness and specificity, i.e., GRACE [29], we identify its weakness in the inability to distinguish between the semantics of different inputs. Based on this observation, we propose an augmented strategy (named as A-GRACE) where we introduce an encoder to GRACE that allows it to better capture the input semantics via contrastive learning, and thus improves the generalization of GRACE. Results show that A-GRACE significantly improves the generalization while achieves similar effectiveness and specificity compared to GRACE. For instance, on CNLE, A-GRACE improves the generalization of GRACE from almost zero to an average of 80.86% in terms of the Exact Match metric. To summarise, our contributions are as follows: • Benchmark. We construct CLMEEval for LLMs4Code editing, which includes a CNLE dataset with 21K+ NL2PL samples and 16K+ PL2NL samples. • Evaluation. We evaluate six state-of-the-art model editing approaches on three LLMs4Code, and find that existing model editing approaches can hardly adapt well to LLMs4Code: they usually fail to achieve a good balance among effectiveness, generalization, and specificity. • Strategy. We propose a refined editing approach A-GRACE, and the experiment results show that A-GRACE achieves promising performances on CLMEEval. Particularly, its generalization is improved by an order of magnitude compared to the vanilla GRACE."
https://arxiv.org/html/2411.06590v1,CriticAL: Critic Automation with Language Models,"Understanding the world through models is a fundamental goal of scientific research. While large language model (LLM) based approaches show promise in automating scientific discovery, they often overlook the importance of criticizing scientific models. Criticizing models deepens scientific understanding and drives the development of more accurate models. Automating model criticism is difficult because it traditionally requires a human expert to define how to compare a model with data and evaluate if the discrepancies are significant–both rely heavily on understanding the modeling assumptions and domain. Although LLM-based critic approaches are appealing, they introduce new challenges: LLMs might hallucinate the critiques themselves. Motivated by this, we introduce CriticAL (Critic Automation with Language Models). CriticAL uses LLMs to generate summary statistics that capture discrepancies between model predictions and data, and applies hypothesis tests to evaluate their significance. We can view CriticAL as a verifier that validates models and their critiques by embedding them in a hypothesis testing framework. In experiments, we evaluate CriticAL across key quantitative and qualitative dimensions. In settings where we synthesize discrepancies between models and datasets, CriticAL reliably generates correct critiques without hallucinating incorrect ones. We show that both human and LLM judges consistently prefer CriticAL’s critiques over alternative approaches in terms of transparency and actionability. Finally, we show that CriticAL’s critiques enable an LLM scientist to improve upon human-designed models on real-world datasets.","A longstanding goal of artificial intelligence research is to automate the discovery of scientific models [16, 27]. The rapid development of LLMs with remarkable reasoning capabilities and general knowledge has created exciting new opportunities within this domain. Recent work has shown that LLM-based scientific agents can propose research ideas [23], discover scientific models [17], and implement experiments [18, 12]. These results highlight the promise of using LLMs to automate many important aspects of scientific discovery. However, they overlook the crucial role that model criticism, or understanding the limitations of a model, plays in driving scientific progress. Model criticism deepens our understanding and often motivates new models. Furthermore, automated methods for criticism can improve the reliability of LLM-based scientific discovery systems, as LLMs are prone to systematic hallucinations [18, 29] that could undermine the broader goal of automating scientific discovery. Model criticism is hard to automate because it is inherently dependent on the model and problem domain. In particular, it involves (1) determining which aspects to compare between the model and data and (2) evaluating the significance of any differences. Each of these tasks typically requires substantial human expertise [7]. While leveraging LLMs is an initially appealing approach to automation, it introduces new challenges: LLMs might also hallucinate the critiques themselves, undermining the effectiveness of automated model criticism. Motivated by these challenges, we introduce CriticAL (Critic Automation with Language Models), which integrates LLMs within a principled model criticism framework. Specifically, given a proposed scientific model and dataset metadata, CriticAL uses an LLM to generate summary statistics that capture properties of the data that might violate the modeling assumptions. Importantly, these summary statistics are tailored to the model and dataset. CriticAL implements these summary statistics as Python functions, which can be easily executed and inspected by a human or LLM scientist. This brings transparency to the critique process. While these summary statistics can highlight potential discrepancies, we need a method to determine whether these discrepancies are meaningful. To address this, we show how we can automatically convert the summary statistics produced by CriticAL into hypothesis tests, for many commonly-used scientific models. Specifically, if we can generate data from the scientific model [10, 6], we can form a null distribution for a summary statistic and compute an empirical p-value. Thus, we can transform each summary statistic into a quantitative check, providing a rigorous way to assess both the significance of the discrepancies and the validity of the model. In doing so, we reduce the complex task of automatically validating proposed models and critiques to the well-understood problem of hypothesis testing. In experiments (Section 4), we evaluate CriticAL along key qualitative and quantitative properties crucial for an automated critic system. In settings where we synthetically control discrepancies between models and datasets, CriticAL consistently identifies true discrepancies and avoids hallucinating false ones. We also assess important qualitative aspects of CriticAL’s critiques (e.g., transparency), and find that both LLM and human judges prefer CriticAL’s critiques over alternatives. Finally, we demonstrate the practical impact of CriticAL’s critiques on the downstream task of guiding an LLM-based scientific model discovery system. On real-world datasets, CriticAL’s critiques enable an LLM-based automated model discovery system [17] to significantly improve upon initial human-designed models."
https://arxiv.org/html/2411.06549v1,"In-Context Learning for Preserving Patient Privacy: 
A Framework for Synthesizing Realistic Patient Portal Messages","Since the COVID-19 pandemic, clinicians have seen a large and sustained influx in patient portal messages, significantly contributing to clinician burnout. To the best of our knowledge, there are no large-scale public patient portal messages corpora researchers can use to build tools to optimize clinician portal workflows. Informed by our ongoing work with a regional hospital, this study introduces an LLM-powered framework for configurable and realistic patient portal message generation. Our approach leverages few-shot grounded text generation, requiring only a small number of de-identified patient portal messages to help LLMs better match the true style and tone of real data. Clinical experts in our team deem this framework as HIPAA-friendly, unlike existing privacy-preserving approaches to synthetic text generation which cannot guarantee all sensitive attributes will be protected. Through extensive quantitative and human evaluation, we show that our framework produces data of higher quality than comparable generation methods as well as all related datasets. We believe this work provides a path forward for (i) the release of large-scale synthetic patient message datasets that are stylistically similar to ground-truth samples and (ii) HIPAA-friendly data generation which requires minimal human de-identification efforts.","Have you sent a message to your doctor recently? Because in the past few years, there has been a significant increase in use of patient-facing healthcare applications which allow patients to send textual messages to their provider Hansen et al. (2023). Electronic health record (EHR) applications such as Epic’s MyChart, for example, reportedly had 200 million active users as of early 2021, with 80 million joining within the previous 12 months ThisWeekHealth (2021). Unfortunately, this shift in doctor-patient communication paradigms has contributed significantly to doctor burnout as clinicians have not been provided with additional resources to handle the portal message surge Stillman (2023). To help reduce clinician workload, recent studies have explored the use of AI tools to optimize cognitively demanding tasks such as portal message triage Si et al. (2020); Gatto et al. (2022); Mermin-Bunnell et al. (2023), routing, Harzand et al. (2023), and response writing Nov et al. (2023); Kozaily et al. (2023); Athavale et al. (2023). However, most of these studies use sensitive data that cannot be released to the community. Some prior work have explored patient messaging through the lens of data sourced from public medical Q&A forums Gatto et al. (2022). However, such platforms operate assuming that the physician who will respond is unfamiliar with the patient. This produces patient messages that are highly dissimilar to actual portal message data, where personal relationships with clinicians and historical EHR data lead to patient messages that are stylistically different, often containing implicit references to EHR data and prior encounters with providers. See Appendix A for an illustrating example of this issue. Given the sensitive nature of patient message data, recent advancements in language modeling may be useful in generating realistic synthetic datasets which can be made public. For example, one could fine-tune a language model on data from the true distribution and then generate realistic synthetic data. Unfortunately, this paradigm may generate samples that leak sensitive patient information learned during training. Even privacy-preserving mechanisms such as Differential Privacy (DP) Wutschitz et al. (2022) cannot provide a risk-free guarantee that no sensitive patient attributes would be released during generation. This is because DP language models are trained not to leak full training data instances, but there remains a risk of generating a sensitive token (e.g., a patient’s last name) violating the patient’s privacy. Thus, a solution to this problem must generate data that reflects true patient portal conversational style, semantics, and structure, all while ensuring synthetic samples pose zero risk of protected health information (PHI) leakage. This issue can perhaps be solved via LLM prompting. LLMs have tremendous capacity to generate texts while following a set of instructions Lou et al. (2024), potentially removing the need for sensitive training data. One could thus prompt an LLM to generate a patient portal message containing a pre-defined set of details. Unfortunately, as discussed in prior work Wang et al. (2024); Gupta et al. (2024); Liu et al. (2024), LLMs are biased, highly-formal, and struggle to capture the natural voice of people from different identity groups. Thus, off-the-shelf applications of LLMs to this task are ill-suited to generate data that matches real patient message style. To address these challenges, we propose PortalGen, a two-stage, HIPAA-friendly, LLM-powered framework for the configurable generation of realistic patient message data. In stage 1, PortalGen uses few-shot prompting of LLMs to transform codes from healthcare databases into portal message prompts. This provides a means of generating diverse large-scale message corpora covering a wide variety of health situations. In stage 2, we use grounded generation with a small number of de-identified patient messages to convert prompts from stage-1 into patient messages. Grounded generation Veselovsky et al. (2023) is a technique that includes samples from the target distribution in the prompt, encouraging LLM outputs to be more stylistically and semantically faithful to the nuances of real training samples. PortalGen performs grounding with just 10 de-identified patient messages, providing a framework for researchers and institutions to release realistic synthetic patient portal message data without requiring large-scale de-identification efforts. Our results demonstrate that PortalGen produces data that is highly similar to real data, outperforming baseline data synthesis techniques and showing strong contrast with related public medical Q&A datasets."
https://arxiv.org/html/2411.06437v1,CTC-Assisted LLM-Based Contextual ASR,"Contextual ASR or hotword customization holds substantial practical value. Despite the impressive performance of current end-to-end (E2E) automatic speech recognition (ASR) systems, they often face challenges in accurately recognizing rare words. Typical E2E contextual ASR models commonly feature complex architectures and decoding mechanisms, limited in performance and susceptible to interference from distractor words. With large language model (LLM)-based ASR models emerging as the new mainstream, we propose a CTC-Assisted LLM-Based Contextual ASR model with an efficient filtering algorithm. By using coarse CTC decoding results to filter potential relevant hotwords and incorporating them into LLM prompt input, our model attains WER/B-WER of 1.27%/3.67% and 2.72%/8.02% on the Librispeech test-clean and test-other sets targeting on recognizing rare long-tail words, demonstrating significant improvements compared to the baseline LLM-based ASR model, and substantially surpassing other related work. More remarkably, with the help of the large language model and proposed filtering algorithm, our contextual ASR model still performs well with 2000 biasing words.111Code and checkpoints are available at https://github.com/X-LANCE/SLAM-LLM/tree/main/examples/contextual_asr","End-to-end (E2E) automatic speech recognition (ASR) systems demonstrate impressive performance [1] but often struggle with accurately recognizing rare phrases, such as technical terms and name entities that are infrequently encountered in training data. Integrating contextual knowledge into E2E ASR models for biased decoding is crucial for improving recognition accuracy in practical applications, such as commercial systems where users need personalized recognition for specific names, places, and entities, highlighting the academic and commercial value of supporting hotword customization. [2] To address this challenge, researchers have developed various methods to inject contextual information into E2E models. The traditional approaches involve shallow fusion [3, 4, 5, 6], which utilizes an independently built language model (LM). Similarly, on-the-fly (OTF) rescoring method [7, 8] compiles known word-level biasing phrases into a weighted finite state transducer (WFST), combined with a “speller” FST to convert grapheme or word-piece sequences into words. A contextual LM score derived from this combination is integrated into the decoding criterion of standard log-likelihood, controlled by a tunable hyperparameter. In contrast, all-neural contextual biasing methods [7, 9, 10, 11, 2] integrate an additional contextual module with the ASR module. For instance, CLAS [7] dynamically incorporates contextual information during speech recognition by embedding context phrases and integrating them into the ASR components via the attention mechanism. These methods can handle out-of-vocabulary terms effectively and do not demand careful tuning of hyperparameters like rescoring weights, leading to significant improvements over OTF rescoring. In practical applications, a biasing list often includes hundreds to thousands of biasing words. Retrieving a meaningful bias context vector is challenging, for it’s difficult for the cross-attention mechanism to accurately link the ASR decoder output with the large-scale sparse hotword embeddings. As the length of the biasing list increases and the number of interfering items grows, the model’s performance tends to decline. To tackle this problem, different works have introduced various filtering methods for processing the biasing list tailored to their specific models. For instance, CLAS proposes a “bias-conditioning” technique that activates a bias phrase only if its prefix matches part of the partially decoded hypothesis. A two-stage contextual word filter module [12] is introduced for attention-based context bias, which includes computing “Posterior Sum Confidence” and “Sequence Order Confidence”, especially designed for cascaded encoder ASR framework. The above approaches often involve intricate model structures and decoding processes. In this paper, inspired by the recent emergence of LLM-based ASR models [13, 14, 15, 16] with simple structures and powerful performance, we propose an effective LLM-based contextual ASR model, along with a robust filtering algorithm. Prior work MaLa-ASR [15] has successfully demonstrated the ability of the LLM-based ASR model to integrate textual keywords extracted from presentation slides to improve recognition of conference content. Different from their AVSR task scenario, where each utterance corresponds to a slide with limited related content, contextual ASR only utilizes a single long biasing list typically containing thousands of entries in the inference stage. Thus, we propose a filtering algorithm where we leverage coarse decoding results generated by a fine-tuned SSL model with a simple CTC [17] head to filter and select the most pertinent hotwords. Then the filtered hotwords are incorporated into the prompts input for LLM. Our proposed CTC-Assisted LLM-based contextual ASR model, trained and evaluated on Librispeech [18] corpus, achieves WER/B-WER of 1.27%/3.67% on the test-clean set and 2.72%/8.02% on the test-other set with a biasing list size of 100, showing notable relative reductions of 39.81%/63.37% and 35.24%/61.37% compared to the baseline LLM-based ASR model, and significantly outperforming other related work."
https://arxiv.org/html/2411.06424v1,"Ablation is Not Enough to Emulate DPO:
How Neuron Dynamics Drive Toxicity Reduction","Safety fine-tuning algorithms are commonly used to fine-tune language models to reduce harmful outputs, but the exact internal mechanisms of how those models achieve this remain unclear. In studying direct preference optimisation (DPO) for toxicity reduction, current explanations claim that DPO works by dampening the most toxic MLP neurons to learn an offset to avert toxic regions in the residual stream. However, by ablating the most toxic neurons and applying activation patching, we find this explanation incomplete. By projecting neuron activation changes onto a toxicity probe, we find that only 31.8% of toxicity reduction comes from dampened toxic neurons. Instead, DPO reduces toxicity by accumulating effects across multiple neuron groups, both reducing writing in the toxic direction and promoting anti-toxicity in the residual stream. Moreover, DPO gives noisy adjustments to neuron activations, with many neurons actually increasing toxicity. This indicates that DPO is a balancing process between opposing neuron effects to achieve toxicity reduction. 111The code is available at: https://github.com/Yushi-Y/dpo-toxic-neurons.","The generality of an LLM’s capabilities means the model also learns to encode undesirable behaviours, such as producing toxic, biased, or hallucinated outputs [6, 5, 19]. To address these issues, researchers have developed safety fine-tuning algorithms, such as proximal policy optimization (PPO) [15] and direct preference optimization (DPO) [14], to reduce undesirable outputs. Recent studies showed that these safety fine-tuning algorithms cause minimal changes to the parameters of pre-trained models, and the undesirable behaviours are hidden rather than fully eliminated [11, 8, 9]. However, the exact mechanisms through which small parameter changes lead to the suppression of undesirable behaviours remain unclear. One explanation proposed when studying the DPO algorithm for toxicity reduction, claimed that DPO reduces toxicity by dampening the activations of the most toxic MLP neurons, creating an offset to avert toxic regions in the residual stream [11]. Our study tests this claim by tracking the writing of the toxic feature direction detected by a probe across MLP layers and neurons in GPT-2 medium. Specifically, we project neuron activation changes onto the toxicity probe direction to quantify per-neuron toxicity adjustments, providing a precise mechanistic understanding of DPO’s mechanisms. Our findings are: • DPO does more than dampening toxic neurons. By ablating the most toxic neurons and activation patching on the pre-trained model, we find that toxicity levels remain higher than when DPO is applied, indicating that dampened toxic neurons alone [11] do not fully account for DPO’s effect. • A significant part of DPO’s effect comes from actively writing anti-toxicity into the residual stream. By projecting onto the toxicity probe, our analysis shows that dampened toxic neurons only account for 31.8% of the total toxicity reduction. DPO not only writes less in the toxic direction but also promotes anti-toxicity by activating more on anti-toxic neurons, or pushing inactive toxic neurons’ activations further below zero. • Many neurons modified by DPO actually increase toxicity. DPO introduces noisy activation adjustments across neurons, with roughly half writing less in the toxic direction and the other half writing more, creating a trade-off. This suggests that DPO balances opposing neuron effects to achieve overall toxicity reduction."
https://arxiv.org/html/2411.06391v1,"CausalStock: Deep End-to-end Causal Discovery 
for News-driven Stock Movement Prediction","There are two issues in news-driven multi-stock movement prediction tasks that are not well solved in the existing works. On the one hand, “relation discovery” is a pivotal part when leveraging the price information of other stocks to achieve accurate stock movement prediction. Given that stock relations are often unidirectional, such as the “supplier-consumer” relationship, causal relations are more appropriate to capture the impact between stocks. On the other hand, there is substantial noise existing in the news data leading to extracting effective information with difficulty. With these two issues in mind, we propose a novel framework called CausalStock for news-driven multi-stock movement prediction, which discovers the temporal causal relations between stocks. We design a lag-dependent temporal causal discovery mechanism to model the temporal causal graph distribution. Then a Functional Causal Model is employed to encapsulate the discovered causal relations and predict the stock movements. Additionally, we propose a Denoised News Encoder by taking advantage of the excellent text evaluation ability of large language models (LLMs) to extract useful information from massive news data. The experiment results show that CausalStock outperforms the strong baselines for both news-driven multi-stock movement prediction and multi-stock movement prediction tasks on six real-world datasets collected from the US, China, Japan, and UK markets. Moreover, getting benefit from the causal relations, CausalStock could offer a clear prediction mechanism with good explainability.","The financial services industry has maintained a leading position in embracing data science methodologies to inform investment determinations. Within this domain, quantitative trading has garnered substantial attention from both academia and industry. Researchers have consistently worked on exploring different approaches to predict the stock movement (rise or fall of stock price) for many years, such as uni-stock movement prediction [21], multi-stock movement prediction [44, 23], news-driven stock movement prediction [42, 19] and so on, which have shown significant success. These methods usually model the stock movement prediction task as a time series classification problem. In this paper, we focus on the news-driven multi-stock movement prediction task. A prevalent model paradigm for this task often takes the historical price features and the stock-related news of multiple stocks as inputs and then leverages the well-designed neural networks to make stock movement predictions. There are two key modeling points for tackling this task: modeling the stock relations to enhance the prediction accuracy, and building the text mining module to extract effective information from news data that benefits stock movement prediction. Although previous work has made significant progress, there are still some issues that require further attention. We will elaborate on them in the following. For stock relation modeling, many existing works are commonly attention-based [15, 19, 23] or graph-based [34, 23]. These methods aim to model the correlation relation between stocks. However, the company relations are often unidirectional, such as the “investing” and “member of,” leading to the unidirectional relations of their stocks. Thus, causal relations are more appropriate for depicting the impact between stocks, as they identify the direction of information flow and are more informative than correlations. With the development of causal science, many researchers have started to use deep end-to-end networks for causal relations discovery of panel data or temporal data [9, 14], in which the causal relations are defined as directed acyclic graphs, i.e., causal graphs, and the Functional Causal Models (FCMs) are often utilized to optimize the causal graph by simulating the data generation mechanism. This provides a solid theoretical foundation for causal discovery for stocks. In recent years, an extrinsic text mining module has emerged as a plausible avenue through the alignment of financial news and social media posts, thereby elucidating intricate market insights that extend well beyond mere considerations of price dynamics, trading volumes, or financial indicators [41, 17, 35, 33]. Conventional text representations obtained by using GRU [42] or LSTM [15] exhibit many limitations. Specifically, news text data are often characterized by substantial noise because of the presence of irrelevant or ambiguous information [38, 7, 37]. The effective information for stock movement prediction gets intertwined with this noise, presenting a considerable challenge for these modules to discern meaningful signals accurately. In contrast, Large Language Models (LLMs) have unique advantages in this situation due to their advanced knowledge and reasoning abilities. Besides, LLMs can identify meaningful information within noisy environments [29, 4]. Motivated by these requirements, we propose an innovative news-driven multi-stock movement prediction model named CausalStock. In CausalStock, we design a Denoised News Encoder, which leverages LLMs to score every news text from multiple perspectives. Then the evaluation scores are taken as denoised text representations. To discover the causal relations between stocks, we propose a Lag-dependent temporal causal discovery module, from which we obtain the causal graph distribution. Based on the input market information and learned causal graph distribution, CausalStock employs an FCM [14] to make predictions. We summarize the contributions of our paper as follows: • We propose a novel news-driven multi-stock movement prediction method named CausalStock, which could discover the causal relations among stocks and make accurate movement predictions simultaneously. • Different from the past lag-independent causal discovery method [9], CausalStock involves a lag-dependent temporal causal discovery module, which intuitively links the temporal causal relations according to the time lag, making it more suitable for temporal stock data. • To extract useful information from the massive noisy news text data, an LLM-based Denoised News Encoder is proposed by taking advantage of the evaluation ability of LLM, which outputs the denoised news representation for better information utilization. Experiments on 6 public benchmarks show the performance of CausalStock as a news-driven multi-stock movement prediction method. Moreover, we conduct extensive analytical experiments to show the explainability of our key modules."
https://arxiv.org/html/2411.06387v1,Self-Training Meets Consistency: Improving LLMs’ Reasoning With Consistency-Driven Rationale Evaluation,"Self-training approach for large language models (LLMs) improves reasoning abilities by training the models on their self-generated rationales. Previous approaches have labeled rationales that produce correct answers for a given question as appropriate for training. However, a single measure risks misjudging rationale quality, leading the models to learn flawed reasoning patterns. To address this issue, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a self-training framework that further evaluates each rationale through follow-up questions and leverages this evaluation to guide its training. Specifically, we introduce two methods: (1) filtering out rationales that frequently result in incorrect answers on follow-up questions and (2) preference learning based on mixed preferences from rationale evaluation results of both original and follow-up questions. Experiments on three question-answering datasets using open LLMs show that CREST not only improves the logical robustness and correctness of rationales but also improves reasoning abilities compared to previous self-training approaches.111Code: https://github.com/JaehyeokLee-119/CREST","Large language models (LLMs) can enhance multi-step reasoning abilities by generating intermediate reasoning steps (i.e., rationale) before arriving at an answer Wei et al. (2022). Training LLMs on high-quality rationales has been shown to improve their reasoning capabilities Chung et al. (2024); Liu et al. (2023); Shridhar et al. (2023). Therefore, collecting high-quality rationales is becoming increasingly important for training the reasoning abilities of LLMs. However, due to the high cost associated with collecting high-quality rationales, self-training approaches have emerged, focusing on training LLMs using self-generated rationales Zelikman et al. (2022). Figure 1: An example of rationale generation and evaluation in CREST: An LLM generates two rationales (r^{1}, r^{2}) and answer predictions to solve question Q. Even though r^{2} lacks focus and clear support for the answer, previous approaches evaluate both r^{1} and r^{2} as equally right. Through a more fine-grained evaluation using follow-up questions, we can identify the better rationale, r^{1}, which leads to more consistent predictions across all questions. In self-training approaches, accurately evaluating the quality of generated rationales is essential. Previous studies have evaluated rationale quality by examining whether the generated rationales lead to the correct answer to a given question Zelikman et al. (2022); Hoffman et al. (2023); Feng et al. (2024); Hosseini et al. (2024); Singh et al. (2024). However, using the correctness of a single prediction is unstable, as LLMs can reach correct answers through inappropriate reasoning steps Bao et al. (2024). Figure 1 shows an example of two generated rationales, r^{1} and r^{2}. Despite r^{2} shows incomplete reasoning, previous approaches would consider both rationales equally appropriate since they both lead to the correct answer for Q. Training models on such inappropriate rationales can cause them to learn flawed reasoning patterns. To address this problem, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a novel framework for LLM self-training. The core idea of CREST is to further evaluate rationales using follow-up questions that ask whether each answer option in the original question is correct or not. We first generate diverse rationales and evaluate them with an LLM as shown in Figure 1. Subsequently, we train the LLM on these rationales, rewarding rationales that lead to more consistent predictions (i.e., r^{1}) and penalizing those that lead to less consistent predictions (i.e., r^{2}). To achieve this, we propose two methods: rationale filtering and preference learning. In rationale filtering, we remove rationales that lead to incorrect answers in more than a certain number of follow-up questions during the supervised fine-tuning process. In preference learning, we train the model on mixed preferences from results of both original and follow-up questions, to favor rationales that result in correct answers in a greater number of follow-up questions. We conduct experiments on three natural language reasoning question-answering datasets, including ReClor Yu et al. (2020), ARC Clark et al. (2018), and CSQA Talmor et al. (2019). We compare CREST to other self-training approaches using Llama 3 model AI@Meta (2024) and Gemma model Team et al. (2024). Our findings show that CREST can train an LLM to generate more correct and robust rationales, improving its reasoning performance. Our contributions are as follows: • We introduce consistency-driven rationale evaluation, which further evaluates generated rationales using follow-up questions that ask whether each answer option in the original question is correct or not. • We propose CREST, which evaluates generated rationales via consistency-driven rationale evaluation and uses the evaluation results to train an LLM through two methods: rationale filtering and preference learning using mixed preferences derived from original and follow-up question evaluations. • We conduct experiments and analyses with open LLMs such as Llama 3 model and Gemma model on three question-answering datasets. The results show that CREST generates more robust and correct rationales and improves reasoning ability compared to other self-training approaches."
https://arxiv.org/html/2411.06138v1,: A Harmful Content Detection and Mitigation Architecture for Social Media Platforms,"The mental health of social media users has started more and more to be put at risk by harmful, hateful, and offensive content. In this paper, we propose StopHC, a harmful content detection and mitigation architecture for social media platforms. Our aim with StopHC is to create more secure online environments. Our solution contains two modules, one that employs deep neural network architecture for harmful content detection, and one that uses a network immunization algorithm to block toxic nodes and stop the spread of harmful content. The efficacy of our solution is demonstrated by experiments conducted on two real-world datasets.","From hate speech and misinformation to verbal violence and death threats, social platforms like X (formally Twitter) have become a favorable space for content that can cause harm through online aggression [10]. Harmful content is no longer perceived only as a form of immorally expressed opinions but as a recognized global danger that must be prevented for the mental, emotional, and physical safety of online content consumers [32]. It is necessary to be able to detect the sources that generate such toxic behavior on the Internet and to reduce the influence they have. The more the spread of hate-speech posts is decreased, the more users will be saved from emotional and psychological damage. To address these issues, we propose StopHC, a harmful content detection and mitigation architecture for social media platforms. The main objectives of this paper are: (1) to detect problematic behavior, and (2) to minimize the spread of such behaviors. To detect harmful content, we train multiple deep neural network models using different embeddings that consider the syntax (i.e., word embeddings such as Word2Vec [11, 12] and GloVe [13]), context (i.e., transformed embeddings such as BERT [5] and RoBERTa [8]), and network information (i.e., Node2Vec [6]). Using these approaches, we aim to improve the StopHC detection module by employing the model that better understands both textual content and network structures. To minimize the spread of such behaviors, we employ 3 different immunization strategies: (1) naïve (i.e., Highest Degree [9]), (2) pro-active (i.e., NetShield [3]), and (3) contra-active (i.e., DAVA [33]). Using these strategies, we aim to improve StopHC mitigation module and offer a graph-dependent solution. The main contributions of this work are four-fold: C_{1} We propose StopHC, a novel architecture for harmful content detection and mitigation; C_{2} We develop new deep neural network models for harmful content detection; C_{3} We employ immunization strategies to stop the spread of harmful content on social media platforms; C_{4} We perform extensive evaluation testing on two real-world datasets. This work is structured as follows. Section II provides insights into the recent literature. Section III presents StopHC’s architecture Section IV offers the experimental evaluation of our solution. Finally, Section V presents the conclusions of this work and discusses future work."
https://arxiv.org/html/2411.06084v1,Optimizing Large Language Models through Quantization: A Comparative Analysis of PTQ and QAT Techniques,"This paper presents a comprehensive analysis of quantization techniques for optimizing Large Language Models (LLMs), specifically focusing on Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). Through empirical evaluation across models ranging from 10M to 1B parameters, we demonstrate that quantization can achieve up to 68% reduction in model size while maintaining performance within 6% of full-precision baselines when utilizing our proposed scaling factor \gamma. Our experiments show that INT8 quantization delivers a 40% reduction in computational cost and power consumption, while INT4 quantization further improves these metrics by 60%. We introduce a novel theoretical framework for mixed-precision quantization, deriving optimal bit allocation strategies based on layer sensitivity and weight variance. Hardware efficiency evaluations on edge devices reveal that our quantization approach enables up to 2.4x throughput improvement for INT8 and 3x for INT4, with 60% power reduction compared to full-precision models.","I-A Background and Motivation The emergence of Large Language Models (LLMs) has revolutionized the field of natural language processing (NLP), enabling significant advancements in tasks such as machine translation, sentiment analysis, question answering, and conversational agents. Models like GPT-3, with 175 billion parameters, and PaLM, boasting 540 billion parameters, have demonstrated unprecedented capabilities in understanding and generating human-like text. These models leverage vast amounts of data and intricate architectures to achieve high performance, often surpassing previous benchmarks and setting new standards in the industry. However, the impressive capabilities of LLMs come at a substantial computational and financial cost. Training such models requires extensive computational resources, including powerful GPUs or TPUs, vast memory, and significant energy consumption. Moreover, the inference phase—where the model is deployed to perform tasks—demands considerable computational power and memory bandwidth, which can limit the feasibility of deploying LLMs on devices with constrained resources [1], such as mobile phones, Internet of Things (IoT) devices, and edge computing platforms. This limitation poses a significant barrier to the widespread adoption and accessibility of LLMs, particularly in applications where low latency and high efficiency are critical. Quantization in neural networks offers a promising solution to these challenges by reducing the precision of model parameters and activations. Typically, neural network weights and activations are represented using 32-bit floating-point (FP32) numbers, which provide high precision but consume substantial memory and computational resources. Quantization techniques convert these high-precision values to lower-bit representations, such as 8-bit integers (INT8), 4-bit integers (INT4), or even binary representations [2]. This reduction not only decreases the memory footprint of the model but also accelerates computations by leveraging hardware optimizations designed for lower precision arithmetic. In the context of LLMs, which often consist of billions of parameters, quantization becomes particularly beneficial. By reducing the model size, quantization facilitates the deployment of LLMs on devices with limited computational capabilities and power budgets. Additionally, lower-precision computations can significantly speed up inference times, enabling real-time applications and reducing operational costs. Despite these advantages, quantization presents challenges, including potential degradation in model accuracy and the complexity of implementing effective quantization strategies. This paper aims to provide a comprehensive review of quantization techniques applied to LLMs, exploring their methodologies, benefits, challenges, and future directions."
https://arxiv.org/html/2411.05990v2,"Game-theoretic LLM:
Agent Workflow for Negotiation Games","This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models’ ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.Our research contributes to a deeper understanding of LLMs’ decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.","Large Language Models (LLMs), such as GPT-4 and Claude, have achieved remarkable progress in natural language understanding and generation zhang2024supervised ; ding2024hybrid ; fang2024large , driving advancements in fields ranging from conversational AI dam2024complete ; dong2023towards to content creation liang2024monitoring ; shao2024assisting and agentic task delegation guo2024embodied ; agashe2023evaluating ; xi2023rise . LLMs are increasingly integrated into applications that influence everyday activities, such as planning, acting, and decision-making. Therefore, the ability of LLMs to navigate complex situations has significant implications for their deployment in applications requiring strategic interaction, such as automated negotiations, economic modeling, and collaborative problem-solving bianchi2024well ; horton2023large ; li2024econagent ; chen2024comm ; li2023metaagents . Despite the wide exploration and utilization, LLM’s capacity for rational behavior, particularly in strategic settings represented by game theory, remains an open question leng2023llm ; stade2024large ; wu2024shall ; de2023emergent ; lan2023llm . In this context, rationality implies an agent’s ability to make decisions that maximize expected utility based on available information, an essential component of intelligent and adaptive decision-making. In the realm of game theory, rational agents are expected to act strategically, considering not only their own preferences but also the potential actions and preferences of others. This is especially critical in incomplete-information games, where uncertainty about other players’ information necessitates sophisticated reasoning and belief updating. This paper investigates the capacity of LLMs to behave rationally in game-theoretic scenarios and explores methodologies to enhance their rational decision-making capabilities. We begin by assessing the performance of several state-of-the-art LLMs, including Claude-3.5 Sonnet, Claude-3 Opus, GPT-4o and o1 zhong2024evaluation , in both complete-information and incomplete-information games such as the Prisoner’s Dilemma, Battle of the Sexes, the Escalation Game, and Deal-or-No-Deal lewis2017deal , presented in Figure 1. Our analysis reveals LLMs often deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees (Section 4). They also exhibit a lack of robustness to noise and uncertainty, leading to suboptimal outcomes (Section 6). To address these limitations, we introduce a novel approach by proposing game-theory-inspired workflows specifically designed to guide the reasoning and decision-making processes of LLMs. This is the first attempt to systematically integrate classic game-theoretic strategies into LLM-based agent workflow, aiming to enhance their rational behavior and decision-making capabilities in strategic settings. These workflows incorporate principles such as Dominant Strategy Search, which involves identifying strategies that yield the highest payoff regardless of the opponent’s actions; Backward Induction, a method of solving extensive-form games by analyzing them from the end states backward to the initial decision nodes to determine optimal strategies; and Bayesian belief updating, which allows agents to refine their beliefs about other players’ valuations based on observed actions and signals during the game. Cringed on these well-defined and well-studied game-theoretic methods, we design algorithms to guide the behavior and thinking process of LLM-based agents. Additionally, we integrate fairness considerations like envy freeness and pareto optimality, which promote equitable and efficient outcomes in negotiations by ensuring that no agent prefers another agent’s allocation to their own and that no improvements can be made without making at least one agent worse off. Contribution Summary • Comprehensive Evaluation of LLMs in Strategic Games and Identification of Rationality Limitations in LLMs (Section 4 and 6): Through empirical analysis, we uncover that LLMs often fail to behave rationally in strategic settings, exhibiting a lack of robustness to noise and randomness. • Design of Game-Theory-Inspired Workflows (Section 4.4 and 5.2): We develop novel workflows inspired by game-theoretic concepts to guide the reasoning and decision-making processes of LLMs, incorporating analysis and algorithms from classic game theory. • Emerging Research Direction (Section 5.5.3 and 5.6): Through the application of workflows, we identify a promising new research direction in meta-strategy, specifically focusing on the decision of whether to adopt a workflow and, potentially, which workflow to employ in varying scenarios. Figure 1: Game-theoretic Landscape Investigated in this Paper."
https://arxiv.org/html/2411.05943v1,Quantifying artificial intelligence through algebraic generalization,"The rapid development of modern artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, modern AI systems fall short on tests requiring symbolic processing and abstraction – a glaring limitation given the necessity for interpretable and reliable technology. Despite a surge of reasoning benchmarks emerging from the academic community, no comprehensive and theoretically-motivated framework exists to quantify reasoning (and more generally, symbolic ability) in AI systems. Here, we adopt a framework from computational complexity theory to explicitly quantify symbolic generalization: algebraic circuit complexity. Many symbolic reasoning problems can be recast as algebraic expressions. Thus, algebraic circuit complexity theory – the study of algebraic expressions as circuit models (i.e., directed acyclic graphs) – is a natural framework to study the complexity of symbolic computation. The tools of algebraic circuit complexity enable the study of generalization by defining benchmarks in terms of their complexity-theoretic properties (i.e., the difficulty of a problem). Moreover, algebraic circuits are generic mathematical objects; for a given algebraic circuit, an arbitrarily large number of samples can be generated for a specific circuit, making it an optimal testbed for the data-hungry machine learning algorithms that are used today. Here, we adopt tools from algebraic circuit complexity theory, apply it to formalize a science of symbolic generalization, and address key theoretical and empirical challenges for its successful application to AI science and its impact on the broader community.","The ability to reason algebraically is often considered a hallmark of human intelligence 1. The recent evolution of modern artificial intelligence (AI) systems and large language models (LLMs) has led to the speculation that these systems may also reason algebraically 2, 3, 4. Yet due to challenge of evaluating large models trained on massive pretraining datasets 5, it is difficult to evaluate whether such models are truly exhibiting algebraic reasoning abilities, or whether they instead regurgitate plausible text from their pretraining data 6, 7. This ambiguity has led to a deluge of symbolic reasoning benchmarks 8, 9, 10, 11, 12, 13, 14, 15, 16, 17. Despite these efforts, objectively quantifying the complexity of reasoning problems is difficult; most of these experiments are ad hoc, and designed without a framework to quantify complexity. However, approaches in computational complexity theory, a field within theoretical computer science, have made it possible to explicitly measure a problem’s algorithmic difficulty, enabling the design of generalization tests rooted in quantifiable measures of complexity. Here, we adopt a branch of computational complexity theory – algebraic circuit complexity – to provide a parsimonious set of problems and approaches to rigorously study symbolic computation in machine learning. Recently, there has been increased interest in studying AI models through arithmetic and compositional tasks 18, 19, 20, 21, 22, 23, 24, 25, 26, 12, 9, 27. Compositional tasks are problems that are generated by recombining a basis set of atomic elements to form a variety of task combinations. (Arithmetic problems are compositional; they are composed of atomic elements (numbers and operators), and can be recomposed to generate novel expressions and problems.) These tasks are good reasoning benchmarks because they require 1) abstraction, 2) logical application of rules or axioms, and 3) precise problem-solving and rigor. Critically, these paradigms have provided reliable ways to elicit failure modes in transformer-based AI models for specific forms of symbolic generalization. For example, a number of studies have demonstrated the difficulty of “length generalization” – generalizing to problems of longer length than seen during training 28, 19, 26, 21. Hupkes et al. 9 and Hupkes et al. 29 also introduced various notions (e.g., systematicity and productivity) in an effort to taxonomize different forms of linguistic generalization. While incredibly useful for linguistics, it is unclear how these concepts generically apply beyond natural language processing. By contrast, the formalisms from algebraic circuit complexity theory provide a set of parsimonious mathematical tools that can be applied to quantify symbolic generalization, and are agnostic to specific domains, such as linguistics. Moreover, algebraic circuit complexity theory provides an encompassing theoretical framework for the increasingly popular, yet nascent empirical evaluations in AI systems that use arithmetic tasks 18, 19, 30, 31, 21, 22, 23, 24, 32, 33, 34, 35. A large class of symbolic problems can be studied with algebraic expressions 36, 37. Algebraic circuit complexity theory formalizes algebraic expressions as circuit models (i.e., directed acyclic graphs; Fig. 1). This formalization is well-established in computational complexity theory, the branch of theoretical computer science concerned with quantifying the difficulty of computational problems and the resources required to solve them 38. Importantly, formalizing computational problems in terms of circuits is the leading approach to empirically quantify their complexity. Unlike other notions of complexity, such as Kolmogorov Complexity in algorithmic information theory (which is incomputable), notions developed in complexity theory for circuits are explicitly computable and determined by their shape and structure 39. Thus, the tools of algebraic circuit complexity can formalize notions of generalization by defining benchmarks in terms of their circuit properties. Furthermore, algebraic circuits are generic mathematical objects; they can be represented from a variety of mathematical perspectives (geometry, topology, etc.), providing useful interpretations in other domains. Algebraic circuits are therefore well-situated to addressing problems in symbolic machine learning – the problems are computable, large datasets can be straightforwardly generated from circuit specifications, and new models can be developed that address specific failure modes within this framework. In the ensuing sections, we provide a blueprint for the successful adoption of algebraic circuit complexity for machine learning problems; we introduce the core components of algebraic circuits, address how they can be leveraged to study symbolic generalization, and discuss several key open theoretical and empirical challenges. Figure 1: Examples of algebraic expressions represented as circuits. A) A two operand addition circuit (input gates are sampled from a field \mathbb{F}). B) A three operand addition circuit (input gates are sampled from the set of variables x_{i}\in X rather than \mathbb{F}). C, D) A mathematically equivalent pair of circuits, but represented as C) a factorized expression, and D) its a monomial expansion. Notably, despite their mathematical equivalence, the circuit representations are distinct. E) A polynomial of depth 4 and size 12."
https://arxiv.org/html/2411.05936v1,Mitigating Hallucination | ZeroG: An Advanced Knowledge Management Engine,"The growth of digital documents presents significant challenges in efficient management and knowledge extraction. Traditional methods often struggle with complex documents, leading to issues such as hallucinations and high latency in responses from Large Language Models (LLMs). ZeroG, an innovative approach, significantly mitigates these challenges by leveraging knowledge distillation and prompt tuning to enhance model performance.ZeroG utilizes a smaller model that replicates the behavior of a larger teacher model, ensuring contextually relevant and grounded responses, by employing a black-box distillation approach, it creates a distilled dataset without relying on intermediate features, optimizing computational efficiency. This method significantly enhances accuracy and reduces response times, providing a balanced solution for modern document management.Incorporating advanced techniques for document ingestion and metadata utilization, ZeroG improves the accuracy of question-and-answer systems. The integration of graph databases and robust metadata management further streamlines information retrieval, allowing for precise and context-aware responses. By transforming how organizations interact with complex data, ZeroG enhances productivity and user experience, offering a scalable solution for the growing demands of digital document management.","ZeroG significantly improves the quality of responses by a large margin by mitigating hallucinations through the implementation of knowledge distillation and prompt tuning, ensuring responses are accurate and grounded. We differ from [1], which involves fine-tuning the student model, by utilizing a black-box distillation approach without fine-tuning. This approach reduces response latency and enhances overall system reliability. ZeroG leverages LLMs to generate Question and Answer (QnA) pairs from existing documents, storing them in a vector store. When a user query is received, similarity searches using techniques like MMR determine whether it can be addressed by pre-existing QnA pairs or requires a more tailored response using document-specific information. Despite advances in natural language processing, traditional methods often struggle with real-time data updates and accurately handling complex documents, which include sensitive data. This paper explores transforming these presentations into markdown files for easier ingestion into vector stores, enhancing QnA accuracy without frequent reengineering. We also investigate integrating graph databases and utilizing document metadata to refine search and organization capabilities. By pre-generating question sets and caching commonly asked queries, the system streamlines responses, ensuring they are precise and contextually relevant. This paper presents some of the techniques we explored and employed to overcome current limitations in document and knowledge management, significantly improving productivity and effectiveness in handling complex document types."
https://arxiv.org/html/2411.05902v1,Autoregressive Models in Vision: A Survey,"Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.111Literature reviewed is current as of October 31, 2024, with updates planned.","Autoregressive models, which generate data by predicting each element in a sequence based on the previous elements through conditional probabilities, initially gained prominence in the field of natural language processing (NLP) (Vaswani, 2017; Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Wan et al., 2023; Zhou et al., 2023a). This success can be attributed to their inherent advantage of capturing long-range dependencies and producing high-quality, contextually relevant outputs. Especially empirical scaling laws (Henighan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023; Tao et al., 2024; Lyu et al., 2023) reveal that increasing model size and compute budgets consistently improves cross-entropy loss across various domains like image generation, video modeling, multimodal tasks, and mathematical problem solving, following a universal power-law relationship. Inspired by their achievements in NLP, autoregressive models have recently begun to demonstrate formidable potential in computer vision. Figure 1: We provide a timeline of representative visual autoregressive models, which illustrates the rapid evolution of visual autoregressive models from early pixel-based approaches like PixelRNN in 2016 to various advanced systems recently. We are excitedly witnessing the rapid growth in this field. The timeline in Figure 1 illustrates the key milestones and developments in the evolution of visual autoregressive models, highlighting their transition from NLP to computer vision. To date, autoregressive models have been applied to a wide array of generative tasks, including image generation (Parmar et al., 2018; Chen et al., 2020), image super-resolution (Guo et al., 2022; Li et al., 2016), image editing (Yao et al., 2022; Crowson et al., 2022), image-to-image translation (Li et al., 2024e; d) and video generation (Tulyakov et al., 2018; Hong et al., 2022), multi-modal tasks (Yu et al., 2023c; Lu et al., 2022) and medical tasks (Ren et al., 2024; Tudosiu et al., 2024), .etc. This broad applicability underscores the potential for further exploration and application of autoregressive models. With the rapid proliferation of visual autoregressive models, keeping up with the latest advancements has become increasingly challenging. Therefore, a comprehensive survey of existing works is both timely and crucial for the research community. This paper endeavors to provide a thorough overview of recent developments in visual autoregressive and explores potential directions for future improvements. We emphasize that there are at least three distinct categories of visual autoregressive models defined by their sequence representation strategies: pixel-based, token-based, and scale-based models. Pixel-RNN (Van Den Oord et al., 2016), as a representative pixel-wise model in pioneering next-pixel prediction by transforming a 2D image into a 1D pixel sequence, capturing both local and long-range dependencies but with high computational cost. Next-token prediction, inspired by NLP, compresses images into discrete tokens for efficient high-resolution processing, exemplified by models like VQ-VAE (Van Den Oord et al., 2017). VAR (Tian et al., 2024) introduces next-scale prediction, a hierarchical method generating content across multiple scales from coarse to fine autoregressively, capturing visual information at multiple resolutions. Each category offers unique advantages and challenges, making them promising directions for future research. We further introduce a multi-perspective categorization of autoregressive models applied to computer vision, classifying existing models based on criteria such as the sequence representation strategy, the underlying framework, or the target task. Our categorization aims to provide a structured overview of how these models are utilized across various vision tasks. We then present both quantitative and qualitative metrics to assess their performance and applicability. Finally, we highlight the current limitations of autoregressive models, such as computational complexity and mode collapse, and propose potential directions for future research. In summary, this survey makes several contributions: • Given the recent surge of advances based on visual autoregressive models, we provide a comprehensive and timely literature review of these models, aiming to offer readers a quick understanding of the generic autoregressive modeling framework. • We categorize visual autoregressive models based on their sequence representation strategies and systematically compile applications across various domains. This aims to help researchers in specific fields quickly identify and learn about related work. • We provide a comprehensive review of autoregressive models in vision from about 250 related references and summarize their evaluations compared with GAN/Diffusion/MAE-based methods in four image generation benchmarks (ImageNet, MS-COCO, MJHQ-30K, and GenEval bench). {forest} forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=center, font=, rectangle, draw=hidden-draw, rounded corners, align=center, text centered, minimum width=8em, edge+=darkgray, line width=1pt, s sep=3pt, inner xsep=2pt, inner ysep=3pt, line width=0.8pt, text width=6em, ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center, , where level=1text width=8em,font=,align=center, where level=2text width=16em,font=,align=center, where level=3text width=16em,font=,align=center, where level=4text width=16em,font=,align=center, [Autoregressive Models in Vision, ver , text width=20em [Image Generation (§3.1.1), fill=white!10 , text width=15em [Unconditional Image Generation (§3.1.1), fill=orange!10 , text width=20em [Pixel-wise Generation (§3.1.1), fill=orange!10, text width=20em [PixelRNN (Van Den Oord et al., 2016), PixelCNN (Van den Oord et al., 2016) PixelCNN++ (Salimans et al., 2017), Gated PixelCNN (Reed et al., 2016), PMARD (Reed et al., 2017),, fill=orange!10, text width=34em , align=left] [PixelSNAIL (Chen et al., 2018), Image Transformer (Parmar et al., 2018), ImageGPT (Chen et al., 2020), fill=orange!10, text width=34em, align=left] ] [Token-wise Generation (§3.1.1.2), fill=orange!10 , text width=20em [VQ-VAE (Van Den Oord et al., 2017), VQ-VAE-2 (Razavi et al., 2019), VQGAN (Esser et al., 2021b), ViT-VQGAN (Yu et al., 2021a), Efficient-VQGAN (Cao et al., 2023), TiTok (Yu et al., 2024c) VQGAN-LC (Zhu et al., 2024b), LlamaGen (Sun et al., 2024a) RQ-VAE (Lee et al., 2022a), MoVQ (Zheng et al., 2022), DQ (Huang et al., 2023) FSQ (Mentzer et al., 2023), Wavelet Tokenizer (Mattar et al., 2024), DCT-Transformer (Nash et al., 2021), MAGVIT (Yu et al., 2023a), MAGVIT-v2 (Yu et al., 2023b), Open-MAGVIT2 (Luo et al., 2024), OmniTokenizer (Wang et al., 2024b), , fill=orange!10, text width=34em, align=left] [VQ-VAE (Van Den Oord et al., 2017), VQ-VAE-2 (Razavi et al., 2019), VQGAN (Esser et al., 2021b), LlamaGen (Sun et al., 2024a), DeLVM (Guo et al., 2024), SAIM (Qi et al., 2023), MAR (Li et al., 2024c), SAR (Liu et al., 2024g), RAL (Ak et al., 2020) ImageBART (Esser et al., 2021a), DisCo-Diff (Xu et al., 2024) AiM (Li et al., 2024b), ZigMa Hu et al. (2024), DiM (Teng et al., 2024), DiG (Zhu et al., 2024c), Diffusion-RWKV (Fei et al., 2024), fill=orange!10, text width=34em, align=left] ] [Scale-wise Generation (§3.1.1.14), fill=orange!10 , text width=20em [VAR (Zhang et al., 2024a) , fill=orange!10, text width=34em] ] ] [Text-to-Image Synthesis (§3.1.2), fill=green!1 , text width=20em [Token-wise Generation (§6), fill=green!1, text width=20em [DALL·E (Ramesh et al., 2021), CogView (Ding et al., 2021), CogView2 (Ding et al., 2022), Parti (Yu et al., 2022), Make-a-Scene (Gafni et al., 2022), LQAE (Liu et al., 2024b), Fluid (Fan et al., 2024) , fill=green!1, text width=34em, align=left] [VQ-Diffusion (Gu et al., 2022), Kaleido Diffusion (Gu et al., 2024) DART (Zhao et al., 2024) , fill=green!1, text width=34em] [LLM4GEN (Liu et al., 2024f), V2T (Zhu et al., 2024a), MARS (He et al., 2024) Lumina-mGPT (Liu et al., 2024a), fill=green!1, text width=34em] [IconShop (Wu et al., 2023b), Make-a-Story (Rahman et al., 2023) SEED-Story (Yang et al., 2024), fill=green!1, text width=34em, align=left] ] [Scale-wise Generation (§3.1.2.4), fill=green!1, text width=20em [STAR (Ma et al., 2024), VAR-CLIP (Zhang et al., 2024a), , fill=green!1, text width=34em] ] ] [Image-to-Image Translation (§3.1.3), fill=blue!10, text width=20em [Image Painting (§3.1.3), fill=blue!10, text width=20em [QueryOTR (Yao et al., 2022), BAT-Fill (Yu et al., 2021b), fill=blue!10, text width=34em] ] [Multi-view Generation (§7), fill=blue!10, text width=20em [MIS (Shen et al., 2024), SceneScript (Avetisyan et al., 2024) , fill=blue!10, text width=34em] ] [Visual In-Context Learning (§7), fill=blue!10, text width=20em [MAE-VQGAN (Bar et al., 2022), VICL (Bai et al., 2024b) , fill=blue!10, text width=34em] ] ] [Image Editing (§3.1.4), fill=yellow!10 , text width=20em [Text-driven Image Editing (§8), fill=yellow!10, text width=20em [VQGAN-CLIP (Crowson et al., 2022), Make-A-Scene (Gafni et al., 2022) , fill=yellow!10, text width=34em ] ] [Image-driven Image Editing (§8), fill=yellow!10, text width=20em [ControlAR (Li et al., 2024e), ControlVAR (Li et al., 2024d) M2M (Shen et al., 2024), CAR (Yao et al., 2024), MSGNet (Cardenas et al., 2021) MVG (Ren et al., 2024), fill=yellow!10, text width=34em] ] ] ] [Video Generation (§3.2), fill=pink!5 , text width=15em [Unconditional Video Generation (§3.2.1), fill=pink!5, text width=20em [VPNs (Kalchbrenner et al., 2017), MoCoGAN (Tulyakov et al., 2018), VideoTransformer (Weissenborn et al., 2019), LVT (Rakhimov et al., 2020), VideoGPT (Yan et al., 2021), TATS (Ge et al., 2022), PVDM (Yu et al., 2023d), MAGVIT-v2 (Yu et al., 2024b) , fill=pink!5, text width=34em, align=left] ] [Conditional Video Generation (§3.2.2), fill=pink!5 ,text width=20em [IRC-GAN (Deng et al., 2019), Godiva (Wu et al., 2021), LWM (Liu et al., 2024c), CogVideo (Hong et al., 2022), NÜWA (Wu et al., 2022), NUWA-Infinity (Liang et al., 2022), Phenaki (Villegas et al., 2022), ART-V (Weng et al., 2024), ViD-GPT (Gao et al., 2024), Loong (Wang et al., 2024e), PAV (Xie et al., 2024a), iVideoGPT (Wu et al., 2024c), fill=pink!5, text width=34em, align=left ] [Convolutional LSTM (Shi et al., 2015), PredRNN Wang et al. (2017), E3D-LSTM (Wang et al., 2019), SV2P (Babaeizadeh et al., 2018), PVV (Walker et al., 2021), HARP (Seo et al., 2022), LVM Bai et al. (2024a), ST-LLM (Liu et al., 2025), Pyramid Flow (Jin et al., 2024a), fill=pink!5, text width=34em, align=left] [MAGE (Hu et al., 2022), VideoPoet (Kondratyuk et al., 2024) , fill=pink!5, text width=34em] ] [Embodied AI (§3.2.3), fill=pink!5 ,text width=20em [IRIS (Micheli et al., 2023), iVideoGPT (Wu et al., 2024d), Genie (Bruce et al., 2024), GR-1 (Wu et al., 2024b), GR-2 (Cheang et al., 2024), , fill=pink!5, text width=34em, align=left ] ] ] [3D Generation (§3.3), fill=violet!10 , text width=15em [Motion Generation (§3.3.1), fill=violet!10, text width=20em [AMD (Han et al., 2024), T2M-GPT (Zhang et al., 2023a), HiT-DVAE (Bie et al., 2022), HuMoR (Rempe et al., 2021) , fill=violet!10, text width=34em ] ] [Point Cloud Generation (§3.3.2), fill=violet!10, text width=20em [CanonicalVAE (Cheng et al., 2022), Octree Transformer (Ibing et al., 2023), ImAM (Qian et al., 2024), Argus3D (Qian et al., 2024) , fill=violet!10, text width=34em ] ] [Scene Generation (§3.3.3), fill=violet!10, text width=20em [Make-A-Scene (Gafni et al., 2022), SceneScript (Avetisyan et al., 2024) , fill=violet!10, text width=34em ] ] [3D Medical Generation (§3.3.4), fill=violet!10, text width=20em [SynthAnatomy (Tudosiu et al., 2022), BrainSynth (Tudosiu et al., 2024), ConGe (Zhou & Khalvati, 2024), 3D-VQGAN (Zhou et al., 2023b), Unalign (Corona-Figueroa et al., 2023), AutoSeq (Wang et al., 2024c), fill=violet!10, text width=34em, align=left ] ] ] [Multimodal Generation (§3.4), fill=teal!7, text width=18em [Understanding Framework (§3.4.1), fill=teal!7, text width=20em [BEiT (Bao et al., 2021), BEiT-v2 (Peng et al., 2022), LLaVA (Liu et al., 2024d), VL-BEiT (Bao et al., 2022), BEiT-v3 (Wang et al., 2022b) , fill=teal!7, text width=34em, align=left ] ] [Unified Framework (§3.4.2), fill=teal!7, text width=20em [OFA (Wang et al., 2022a), CogView (Ding et al., 2021), M6 (Lin et al., 2021), ERNIE-ViLG (Zhang et al., 2021), NEXT-GPT (Wu et al., 2023c), SEED (Ge et al., 2024), Emu2 (Sun et al., 2024b), LaViT (Jin et al., ), Video-LaViT (Jin et al., 2024b), X-ViLA (Ye et al., 2024), fill=teal!7, text width=34em, align=left] [Chameleon (Team, 2024), Transfusion (Zhou et al., 2024) ScalingLaw (Aghajanyan et al., 2023), RA-CM3 (Yasunaga et al., 2022), SHOW-o (Xie et al., 2024b), Flamingo (Alayrac et al., 2022) MXQ-VAE (Lee et al., 2022b), CoTVL (Ge et al., 2023), Emu (Sun et al., 2023) Janus (Wu et al., 2024a), VILA-U (Wu et al., 2024e), Emu3 (Wang et al., 2024d), fill=teal!7, text width=34em, align=left] ] ] ] Figure 2: Literature taxonomy of autoregressive models in vision."
https://arxiv.org/html/2411.05882v1,When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization,"Contemporary machine learning models, such as language models, are powerful, but come with immense resource requirements both at training and inference time. It has been shown that decoder-only language models can be trained to a competitive state with ternary weights (1.58 bits per weight), facilitating efficient inference. Here, we start our exploration with non-transformer model architectures, investigating 1.58-bit training for multi-layer perceptrons and graph neural networks. Then, we explore 1.58-bit training in other transformer-based language models, namely encoder-only and encoder-decoder models. Our results show that in all of these settings, 1.58-bit training is on par with or sometimes even better than the standard 32/16-bit models.","Large Language Models (LLMs) have dominated the field of natural language processing in recent years. The growing capabilities and applications of these demonstrate potential for enriching our society. However, LLMs come with deployments obstacles, especially regarding memory use, latency, and throughput. What is more, considerations regarding LLM’s environmental impact in terms of energy consumption are threatening their uptake. Post-training quantisation methods have been proposed, including but not limited to, Generative Pre-trained Transformer Quantization Frantar et al. (2023) and Activation-aware Weight Quantization Lin et al. (2024). Post-training quantization has also been employed in other domain such as vision models Li and Gu (2023). However, post-training quantization inherently comes at the cost of precision. An alternative to post-training quantization is quantization-aware training such as LLM-QAT Liu et al. (2023b) and QA-LoRA Xu et al. (2023). Here, as the training optimizes the quantized weights, there is no loss of precision when using the quantized model for inference. Recent works on 1-bit Wang et al. (2023) and 1.58-bit Ma et al. (2024) quantization-aware training architectures have demonstrated the potential of training in a very low-bit representation while still maintaining most or all of the LLM’s performance. The 1.58-bit quantization-aware training architecture introduced in BitNet b1.58 Ma et al. (2024), which quantizes the weights of linear layers to the values -1, 0, or 1 during forward passes, yields only minimal performance hits in 3B+ parameter LLMs. Building on this, recent work suggests that 1.58-bit training is promising for multi-modal architectures Sundaram and Iyer (2024) and spiking language models Bal et al. (2024). BitNet b1.58 Reloaded Nielsen and Schneider-Kamp (2024) investigates the b1.58 architecture in small vision and small language models, ranging from 100K to 48M parameters. The authors define a scaling law between 16-bit and 1.58-bit layers, showing that 1.58-bit can achieve similar performance to 16-bit training, even on these small lower-capacity networks. Furthermore, they show that employing the median (as a basis for quantization and rescaling) instead of the mean achieves even better performance in some settings, ostensibly by decreasing sensitivity to outliers. However, the b1.58 architecture still poses a lot of unanswered questions, which require further investigation of its potential and limitations. Here, we investigate 1.58-bit quantization-aware training in a bottom-up manner, starting from the classic exclusive or (X-OR) task and non-transformer architectures such as multi-layer perceptrons and graph neural networks. We further fill a gap in the literature by analyzing 1.58-bit quantization-aware training for encoder-only and encoder-decoder transformer-based language models, complementing prior work on decoder-only language models. Our results establish that 1.58-bit weights can be employed as a drop-in replacement for Linear layers in a multitude of model architectures with no to minimal loss in performance. We find that in encoder-only language models, commensurate accuracy with 16-bit models can be obtained by increasing the hidden size throughout the model. Moreover, we demonstrate that using 1.58-bit quantization in the T5 architecture can outperform 16-bit models, hinting at a possible regularization effect. In summary, our contributions are as follows: • A bottom-up exploration of 1.58-bit quantization-aware training, ranging from the X-OR problem (Section 3.1) and 1.58-bit multi-layer perceptrons for text classification (Section 3.2) to 1.58-bit graph neural networks for node classification (Section 3.3). • Experiments on 1.58-bit encoder-only language models showing that an increase in model capacity can compensate for the lower bit precision – yet sub-proportionally to the decrease in bit width (Section 4.1). • Experiments on 1.58-bit encoder-decoder language models demonstrated superior performance on the 1.58-bit variant compared to their 16-bit counterpart, without the the need for parameter-compensating (Section 4.2)."
https://arxiv.org/html/2411.05877v1,Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass,"Large language models (LMs) are typically adapted to improve performance on new contexts (e.g., text prompts that define new tasks or domains) through fine-tuning or prompting. However, there is an accuracy compute tradeoff—fine-tuning incurs significant training cost and prompting increases inference overhead. We introduce GenerativeAdapter, an effective and efficient adaptation method that directly maps new contexts to low-rank LM adapters, thereby significantly reducing inference overhead with no need for finetuning. The adapter generator is trained via self-supervised learning, and can be used to adapt a single frozen LM for any new task simply by mapping the associated task or domain context to a new adapter. We apply GenerativeAdapter to two pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the adapted models in three adaption scenarios: knowledge acquisition from documents, learning from demonstrations, and personalization for users. In StreamingQA, our approach is effective in injecting knowledge into the LM’s parameters, achieving a 63.5% improvement in F1 score over the model with supervised fine-tuning (from 19.5 to 31.5) for contexts as long as 32K tokens. In the MetaICL in-context learning evaluation, our method achieves an average accuracy of 44.9 across 26 tasks, outperforming the base model. On MSC, our method proves to be highly competitive in memorizing user information from conversations with a 4x reduction in computation and memory costs compared to prompting with full conversation history. Together, these results suggest that GenerativeAdapter should allow for general adaption to a wide range of different contexts.","Adaptation is essential for language models (LMs) to acquire new world knowledge (Jiang et al., 2024; Hu et al., 2023; Mecklenburg et al., 2024), learn new tasks (Min et al., 2022), and personalize to individual users (Salemi et al., 2024). Existing adaptation methods typically involve either prompting or fine-tuning (Brown et al., 2020). As the scale of LMs continues to increase, adapting them becomes increasingly difficult due to efficiency constraints during both training and inference (Hu et al., 2022). Prompting with task-specific demonstrations (i.e., in-context learning (Brown et al., 2020)) or background knowledge (i.e., retrieval-augmented generation (Lewis et al., 2020)) is one way to enable models to temporarily encode such relevant information, allowing flexible adaptation to various tasks. However, to maintain additional memory across sessions, some extra prompts must be added to the input, which incur an inference-time or storage overhead (Chevalier et al., 2023). Fine-tuning is another way to embed new information into the LM’s parameters, retaining long-term memory. Nevertheless, it requires a training phase that is more computationally expensive than a single forward pass, and acquiring knowledge through continual pretraining has shown to be data-inefficient (Yang et al., 2024; Allen-Zhu & Li, 2024). Thus, we are interested in exploring alternative approaches for effectively and efficiently adapting pretrained LMs. In this work, we present GenerativeAdapter, a novel method for training a neural network (adapter generator) to generate adapters that contextualize pretrained LMs on-the-fly with temporary knowledge from incoming contexts. Inspired by fast weights (Ba et al., 2016; Schmidhuber, 1992, inter alia), our approach incorporates a lightweight adapter generator on top of pretrained LM as the slow network to produce updated parameters for the fast network (the adapted LM). As far as we know, we are the first to explore this direction. Specifically, the pretrained base LM remains frozen while we train the LM-specific adapter generator to generate layer-by-layer additive updates, similar to recent parameter-efficient fine-tuning (PEFT) techniques (Houlsby et al., 2019; Hu et al., 2022). For each layer, a adapter generator network uses the outer product of past context hidden states from the corresponding base LM layer to generate delta weights. These generated delta weights are then added to the base LM weights to form an adapted LM for future predictions. Similar to previous work on fast weights, our method achieves test-time adaptation using only forward passes, allowing dynamic updates as new context arrives in sequential chunks. We train the generator end-to-end in a self-supervised manner by compressing the context into a generated adapter and then computing the next-token prediction loss on a target sequence using the adapted LM. Once trained, our method can produce adapted LMs that effectively capture knowledge from the context to solve multiple downstream tasks, thus improving the adaptability of off-the-shelf pretrained LMs. We evaluate our method on three scenarios where on-the-fly contextualizing pretrained LMs is crucial: acquiring new factual knowledge, learning from demonstrations, and personalizing for individual users. These scenarios involve diverse forms of context with varying lengths, including documents with background knowledge, task-specific input-output examples and user-specific conversations. In the knowledge acquisition scenario, GenerativeAdapter effectively memorizes factual knowledge from provided documents, with minimal information loss compared to full-context prompting at short context lengths. Notably, our method excels in memorizing long-context documents, managing to handle context lengths up to 32K on StreamingQA (Liska et al., 2022) and 8K on SQuAD (Rajpurkar et al., 2016) better than continous pretraining. In learning from demonstrations on MetaICL, GenerativeAdapter follows demonstrations effectively, achieving superior accuracy compared to the in-context learning of its base model. This exemplifies the model’s ability to adapt to new tasks efficiently. For personalization, GenerativeAdapter is highly effective in retaining user information from conversations, achieving a fourfold reduction in computation and memory costs compared to full conversation prompting. In practical scenarios with many queries from the same user on edge computing devices, the benefits of our method are even more evident. This positions GenerativeAdapter as a highly efficient tool for personalized LMs. Our contributions are summarized as follows: 1. We introduce GenerativeAdapter, a novel method for efficiently adapting pretrained LMs on-the-fly using test-time contexts. To our knowledge, we are the first to explore retaining the relevant temporary knowledge through generated parameter-efficient model updates for state-of-the-art pretrained LMs. 2. We develop an adapter generator network on top of frozen pretrained LMs to transform text contexts into updated model parameters (adapted LMs) for future queries. We also design an efficient end-to-end training process to enhance the LMs’ adaptability, i.e., the resulting generator augmented LM can be used for various downstream tasks using only forward passes. 3. We validate the proposed method on two representative pretrained LMs. Empirically, we show the effectiveness of GenerativeAdapter in various adaptation scenarios, including knowledge acquisition from documents, learning from demonstrations, and personalized user interactions. Our method proves to be generalizable across different types of contexts and applicable to multiple downstream tasks."
https://arxiv.org/html/2411.05875v1,"Towards Improved Preference Optimization Pipeline: 
from Data Generation to Budget-Controlled Regularization","Direct Preference Optimization (DPO) and its variants have become the de facto standards for aligning large language models (LLMs) with human preferences or specific goals. However, DPO requires high-quality preference data and suffers from unstable preference optimization. In this work, we aim to improve the preference optimization pipeline by taking a closer look at preference data generation and training regularization techniques. For preference data generation, we demonstrate that existing scoring-based reward models produce unsatisfactory preference data and perform poorly on out-of-distribution tasks. This significantly impacts the LLM alignment performance when using these data for preference tuning. To ensure high-quality preference data generation, we propose an iterative pairwise ranking mechanism that derives preference ranking of completions using pairwise comparison signals. For training regularization, we observe that preference optimization tends to achieve better convergence when the LLM predicted likelihood of preferred samples gets slightly reduced. However, the widely used supervised next-word prediction regularization strictly prevents any likelihood reduction of preferred samples. This observation motivates our design of a budget-controlled regularization formulation. Empirically we show that combining the two designs leads to aligned models that surpass existing SOTA across two popular benchmarks.","Recently, Direct Preference Optimization (DPO) (Rafailov et al., 2024) and its variants (Meng et al., 2024; Azar et al., 2024; Ethayarajh et al., 2024; Liu et al., 2024; Pal et al., 2024; Xu et al., 2024) have gained popularity over traditional reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019), which involves training a reward model followed by reinforcement learning. DPO-based methods bypass the need for a reward model in optimization by directly optimizing the target model using preference data, leading to simpler and more efficient training. The pipeline of DPO (and its variants) consists of two key stages: (1) collecting preference data by scoring various outputs generated by the target LLM model, and (2) performing direct optimization using the preference data. The first stage of constructing preference data involves two steps: (1) the target model generates multiple completions for each input prompt; (2) then a reward model selects preferred and dispreferred completions from these candidates for each prompt (Xiong et al., 2024; Meng et al., 2024). Existing open-sourced reward models are mostly based on a classification architecture by modifying the last layer of a LLM (Liu and Zeng, 2024; Wang et al., 2024b, a). This scoring-based approach for evaluating the quality of a prompt-completion pair introduces considerable noise (Cui et al., 2023; Ganguli et al., 2022; Guo et al., 2024), and the issue becomes even more when the downstream task is out-of-distribution compared to the training data used to construct the reward model. After constructing high-quality preference data, standard preference optimization algorithms compute the relative probability of selecting one completion over another by using pairs of preferred and dispreferred completions (Rafailov et al., 2024; Meng et al., 2024; Azar et al., 2024). Optimizing towards this relative objective can potentially lead to a reduction of target model’s predicted likelihood of the preferred completions, as long as the relative probability between the preferred and dispreferred completions increases with the preference optimization. This may cause training instability issue (Pal et al., 2024; Feng et al., 2024; Liu et al., 2024). To address the challenge, several regularization techniques have been proposed to utilize supervised next-word prediction of the preferred examples. While these techniques effectively improve training stability, our empirical findings show that models trained using these regularization methods perform worse compared to those trained without such regularization. Figure 1: Overview for DPO pipeline. Preference data generation: existing scoring-based methods select preferred and dispreferred completions based on a single score, our proposed iterative pairwise ranking uses pairwise comparison signals to construct preference data. Regularizing preference optimization: we propose a budget-controlled regularization that balances training stability and model alignment performance. In this paper, we aim to improve the preference optimization pipeline. Our work introduces both high-quality preference data generation and improved regularization techniques to address the above limitations. Shown in Fig. 1, we first propose an iterative pairwise ranking method to construct high-quality preference data. Then we use this dataset to train a model with standard preference optimization objective augmented with a novel budget-controlled regularization. The contributions of this work are as follows: • We introduce an iterative pairwise ranking mechanism that employs pairwise comparison signals to construct preference data. Specifically, given multiple completions for an input prompt, an LLM judge sequentially compares the previous winning completion with the next candidate until an optimal completion is found. Empirical results demonstrate that preference data generated by our method consistently surpasses existing for both in-domain and out-of-distribution tasks. • We study the effects of supervised next-word prediction regularization and reveal that while this technique prevents significant reductions in target model’s predicted likelihood of preferred examples, preference optimization tends to achieve better results when the likelihood of both preferred and dispreferred examples are slightly reduced. This observation leads to a novel budget-controlled regularization we propose, which controls the amount of reduction on target model’s predicted likelihood of preferred completions. • We demonstrate that integrating the above two designs yields preference aligned models that outperform the current SOTA across two widely-adopted benchmark evaluations."
