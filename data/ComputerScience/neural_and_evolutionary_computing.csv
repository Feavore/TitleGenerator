URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10047v1,"Nonlinear Neural Dynamics and Classification Accuracy
in Reservoir Computing","Reservoir computing - information processing based on untrained recurrent neural networks with random connections - is expected to depend on the nonlinear properties of the neurons and the resulting oscillatory, chaotic, or fixpoint dynamics of the network. However, the required degree of nonlinearity and the range of suitable dynamical regimes for a given task are not fully understood. To clarify these questions, we study the accuracy of a reservoir computer in artificial classification tasks of varying complexity, while tuning the neuron’s degree of nonlinearity and the reservoir’s dynamical regime. We find that, even for activation functions with extremely reduced nonlinearity, weak recurrent interactions and small input signals, the reservoir is able to compute useful representations, detectable only in higher order principal components, that render complex classificiation tasks linearly separable for the readout layer. When increasing the recurrent coupling, the reservoir develops spontaneous dynamical behavior. Nevertheless, the input-related computations can ’ride on top’ of oscillatory or fixpoint attractors without much loss of accuracy, whereas chaotic dynamics reduces task performance more drastically. By tuning the system through the full range of dynamical phases, we find that the accuracy peaks both at the oscillatory/chaotic and at the chaotic/fixpoint phase boundaries, thus supporting the ’edge of chaos’ hypothesis. Our results, in particular the robust weakly nonlinear operating regime, may offer new perspectives both for technical and biological neural networks with random connectivity.","Deep learning models, which over the past decades have seen enormous progress [1, 2], including present large language models [3], are dominated by artificial neural networks with a feedforward architecture, so that information passes sequentially from input to output layers. In contrast, Recurrent Neural Networks (RNNs) rely on feedback connections and thus function as autonomous dynamical systems [4], capable of maintaining neural activity even without ongoing external inputs. Certain ’universal’ properties of RNNs, such as their ability to approximate arbitrary functions [5] or dynamical systems [6], and other unique strengths of these systems have driven a substantial research interest into the detailed features of RNNs, for example their ability to retain the information of sequential input time series over extended periods [7, 8, 9, 10, 11, 12], or their learning of effective representations by dynamically balancing the compression and expansion of information [13]. The control of RNN dynamics is another key research area, including studies that investigate how external and internal noise can influence network behavior [14, 15, 16, 17, 18, 19, 20, 21]. Additionally, RNNs have been proposed as powerful tools for modeling neural processes in neuroscience [22]. In particular sparse RNNs, which resemble the human brain in their relatively low average node degree [23], have been shown to possess remarkable properties, including enhanced information storage capabilities [24, 25, 26, 27]. In our own previous research, we have systematically analyzed the relation between network structure and dynamical properties, starting with small recurrent three-neuron motifs [28]. We then demonstrated how statistical parameters of the weight matrix can be used to control the dynamics in large, freely running RNNs [29, 21]. Another major focus of our past work have been various noise-induced resonance phenomena in these complex dynamical systems [20, 30, 31, 19, 32, 33, 34]. As a next step, we now move from free-running RNNs to the field of neural reservoir computing: Untrained, randomly connected RNNs, so-called reservoirs, can be used to perform actual tasks, in combination with a simple readout layer that is optimized ’in one shot’ by using highly effective techniques from linear algebra. In this still unconventional type of neural data processing, some of the reservoir neurons receive external input signals, often in a sequential manner, which are then incorporated into the ongoing recurrent dynamics of the system, where they are gradually spreading over the entire network and becoming nonlinearly transformed. At the end of each input episode, the momentary state of the reservoir represents a high-dimensional and (randomly) processed representation of the sequential input data. The role of the readout layer is then to ’pick out’ from the reservoir state all information that is useful in the context of solving a predefined task. Reservoir computing includes diverse models such as liquid state machines [35], echo state networks [7], extreme learning machines [36], and the concept of winnerless competition [37]. In this work, we are using deterministic RNNs, consisting of neurons with \tanh-activation functions that are connected randomly. The magnitudes of the connections are drawn from Gaussian distributions, and the overall structure of the weight matrix is controlled by certain statistical parameters, such as the general strength w of the connections (the standard deviation of the normal weight distribution), the density d of non-zero connections, and the balance b between excitatory and inhibitory connections. Using insights from our former work [29, 21], we can thus control the dynamical state of the RNN by these statistical parameters, in particular by w and b. Here, we are interested in the relationship between a reservoir’s dynamical state and its performance in a specific classification task. For this purpose, the actual reservoir dynamics is measured by the fluctuations and correlations of neural activations over time, as well as the degree of nonlinearity in the neural transformations. Together, these quantities allow us to asses the system’s dynamical attractor state. Simultaneously, the classification performance is quantified by the accuracy, that is, the fraction of correctly predicted labels with respect to ground truth. The optimal dynamical regime of RNNs for computational purposes has been a question of considerable interest over the past decades. For a long time, the dominating opinion was that the ’edge of chaos’ [38, 7, 35], a border regime between over-sensitive chaotic dynamics and rigid oscillatory or fixpoint behavior, would probably be the ’sweet spot’ for reservoir computing and for similar techniques that exploit an untrained complex dynamical system for data processing. However, more recent work has challenged this hypothesis. For example, Carroll [39] tested different reservoir computer designs and discovered that while some configurations did indeed perform optimally near the edge of chaos, others performed better away from this point. Using our \tanh-reservoir, followed by a linear readout layer that is ’trained’ for each classification task with the pseudoinverse technique, as well as a subsequent argmax-function to produce definite predicted class labels, we come to a slightly more nuanced conclusion: The edge of chaos is, after all, a preferred dynamical regime for computational RNNs, at least for our investigated tasks and in a certain range of recurrent coupling strengths w. For relatively large couplings w, as we continuously tune the excitatory/inhibitory balance b from the extreme value -1 (where only inhibitory connections are present, generating an oscillatory reservoir dynamics) to +1 (where only excitatory connections are present, thus trapping the reservoir in a global fixpoint), a plot of the accuracy A(b) as a function of balance b shows indeed two ’edge-of-chaos’ peaks, one between the oscillatory and chaotic regimes, and another between the chaotic and fixpoint regime. On the other hand, we find that the ’data processing power’ of the reservoir remains intact also for rather small coupling strengths w, while the spontaneous dynamics of the reservoir is greatly suppressed in this weak coupling regime. Surprisingly, this makes the accuracy A(b) almost independent from the balance b - a very robust feature of RNNs that might also be exploited in biological neural networks."
https://arxiv.org/html/2411.10017v1,Difficulties of the NSGA-II with the Many-Objective LeadingOnes Problem,"The NSGA-II is the most prominent multi-objective evolutionary algorithm (cited more than 50,000 times). Very recently, a mathematical runtime analysis has proven that this algorithm can have enormous difficulties when the number of objectives is larger than two (Zheng, Doerr. IEEE Transactions on Evolutionary Computation (2024)). However, this result was shown only for the OneMinMax benchmark problem, which has the particularity that all solutions are on the Pareto front, a fact heavily exploited in the proof of this result.In this work, we show a comparable result for the LeadingOnesTrailingZeroes benchmark. This popular benchmark problem appears more natural in that most of its solutions are not on the Pareto front. With a careful analysis of the population dynamics of the NGSA-II optimizing this benchmark, we manage to show that when the population grows on the Pareto front, then it does so much faster by creating known Pareto optima than by spreading out on the Pareto front. Consequently, already when still a constant fraction of the Pareto front is unexplored, the crowding distance becomes the crucial selection mechanism, and thus the same problems arise as in the optimization of OneMinMax. With these and some further arguments, we show that the NSGA-II, with a population size by at most a constant factor larger than the Pareto front, cannot compute the Pareto front in less than exponential time.","Many real-world optimization problems feature several conflicting objectives. This results in incomparable optimal solutions—the Pareto optima—, where each optimum that is better than another one in at least one objective is necessarily worse in another objective. Solvers are therefore expected to return a rich and diverse set of Pareto optima, for discrete problems also all of them, collectively known as the Pareto front. Due to the hardness of many multi-objective problems, heuristic algorithms are oftentimes employed and constitute an essential class of solvers in this field [ZQL+11]. The most prominent multi-objective heuristic is the non-dominated sorting genetic algorithm II [DPAM02] (NSGA-II), counting over 50 000 citations on Google Scholar. The NSGA-II belongs to the successful line of multi-objective evolutionary algorithms [ZQL+11] (MOEAs). As such, it maintains a multi-set (the population) of N solutions and refines it iteratively, creating new solutions based on random mutations of existing ones and then selecting the best solutions found so far. This results in complex dynamics, for which the first mathematical runtime guarantees were proven only recently [ZLD22]: The NSGA-II optimizes the bi-objective OneMinMax [GL10] (OMM) and LeadingOnesTrailingZeros [LTZ04] (LOTZ) benchmarks of size n in, respectively, O(Nn\log n) and O(Nn^{2}) function evaluations (in expectation), when the population size N is at least a constant factor larger than the size of the Pareto front. These runtimes agree with the asymptotic runtimes of other MOEAs (with suitable parameter ranges), such as the (global) simple evolutionary multi-objective optimizer [LTZ+02, Thi03, Gie03] ((G)SEMO), the (\mu+1) SIBEA [BFN08, NSN15], the decomposition-based multi-objective evolutionary algorithm [LZZZ16] (MOEA/D; only for a OMM variant), the NSGA-III [WD23, ODNS24], the SMS-EMOA [ZD24c], and the Strength Pareto Evolutionary Algorithm 2 [RBLQ24] (SPEA2). Runtime analyses confirming the strength of the NSGA-II have also been conducted in other directions, such as for combinatorial optimization [CDH+23], for noisy optimization [DOSS23a], with crossover [DOSS23b, DQ23], and in approximative settings [ZD24a]. All these positive result, however, regard bi-objective problems, and this is no coincidence. In a very recent result [ZD24b], it was shown that the NSGA-II fails to optimize the OMM benchmark efficiently once the number of objectives is 3 or more. More precisely, it was proven that the NSGA-II with any population size that is linear in the size of the Pareto front for an exponential number of iterations cannot reach a population that witnesses the Pareto front. The reason for this drastic decline in performance was attributed to the secondary criterion in the selection of the next population, the crowding distance. We note that a declining performance of NSGA-II for a higher number of objectives has also been observed empirically [KYD03], but this work does not exhibit such a drastic different between two and three objective, and it also does not blame the crowding distance for the difficulties. That indeed the crowding distance is the likely culprit can now also be seen from the fact that the NSGA-III [WD23, ODNS24] and the SMS-EMOA [BZLQ23, ZD24c] provably easily optimize many classic benchmarks in three or more objectives; here we note that these two algorithms, as the NSGA-II, use non-dominated sorting as first selection criterion, but use a different secondary selection criterion, namely closeness to reference points for the NSGA-III and the hypervolume for the SMS-EMOA. In this light, the negative result of [ZD24b] appears to be a crucial step towards understanding the strengths and weaknesses of the different MOEAs massively used in practice. A clear limitation of this result, however, is the fact that it applies only to the OMM benchmark. This benchmark is extremely simple and in particular has the uncommon property that all solutions are Pareto-optimal, a fact heavily exploited in the proof of the result. While one could argue that proven difficulties on a simple benchmark should indicate even greater problems on more complex benchmarks, in view of the importance of this questions, it appears desirable to back up this intuition with more rigorous research. This is what we aim at in this work. Our contribution: We show that the poor performance of the NSGA-II for more than 2 objectives is not restricted to OMM but extends to the LOTZ benchmark. This benchmark does not have the particularity that all solutions are Pareto-optimal, rather the vast majority of the solutions are dominated by others. In that sense, this benchmark is closer to a real-world multi-objective optimization problem (while, of course, still being a synthetic benchmark simple enough to admit mathematical runtime analyses). More precisely, we show the following result. Let the number of objectives m be an even integer (the LOTZ benchmark is only defined for even numbers of objectives). Assume that m\geq 4. Denote by M=(\frac{2n}{m}+1)^{m/2} the size of the Pareto front of the m-objective LOTZ benchmark of problem size n. Consider optimizing this benchmark via the NSGA-II with population size at most aM, where a>1 is an arbitrary constant. Then for all T\in\mathbb{N}, with probability at least 1-T\exp(-\Omega(n)) the NSGA-II misses a constant fraction of the Pareto front for the first T iterations (see Theorems 1 and 6 and note that the precise failure probability of the result can be even smaller than the simplified T\exp(-\Omega(n)) expression stated here). This result implies that the expected runtime (time to have the full Pareto front witnessed by the population) of the NSGA-II on LOTZ is at least exponential in expectation and with high probability when the number of objective is at least 4. Although our main result is comparable to the one for the m-objective OMM benchmark by Zheng and Doerr [ZD24b], due to the more complex fitness landscape of the LOTZ problem, our analysis requires significant additional effort. In the OMM problem, all solutions are Pareto-optimal, and consequently, they are all contained in the first (and only) front of the non-dominated sorting of the combined parent and offspring population. Hence in this first front, a non-trivial selection decision is made via the crowding distance, and the deficiencies of the crowding distance detected in [ZD24b] lead to the undesired loss of Pareto-optimal solution values. The LOTZ benchmark, in contrast, has (strictly) dominating solution values. Consequently, it could happen that the first front of the non-dominated sorting contains at most N solutions, and in this case, these (which include all Pareto optima in the population) would all survive into the next generation. Hence our main technical contribution is a careful analysis of the population dynamics, which shows that the size of the first front of the non-dominated sorting grows to above N faster than that all Pareto optima are found; then, as in the case of OMM, a non-trivial selection decision is made in the first front and the short-comings of the crowding distance come into play. We complement our theoretical results with an empirical analysis. Here we observe that the NSGA-II is indeed capable of quickly finding a constant fraction of the Pareto front but struggles to get past a constant fraction. Larger populations manage to cover larger fractions of the Pareto front. In addition, we study random and binary-tournament parent selection mechanisms as well as uniform crossover and see that the performance is essentially the same. Last, and perhaps most interestingly, we consider the NSGA-II with one-bit mutation and see that while it still does not cover the entire Pareto front, it covers a much larger fraction than the algorithms with bit-wise mutation. We have no explanation for this. Outline: In Section 2, we introduce our notation and terminology and define the NSGA-II as well as the m-objective LOTZ benchmark (mLOTZ). We prove in Section 3 that the NSGA-II is inefficient on mLOTZ for m\geq 4, with Theorems 1 and 6 as our main result. We complement our theoretical analyses empirically in Section 4, and we conclude our paper in Section 5."
https://arxiv.org/html/2411.09933v1,JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging,"With the rapid advancement of large language models (LLMs), foundational models (FMs) have seen significant advancements. Healthcare is one of the most crucial application areas for these FMs, given the significant time and effort required for physicians to analyze large volumes of patient data. Recent efforts have focused on adapting multimodal FMs to the medical domain through techniques like instruction-tuning, leading to the development of medical foundation models (MFMs). However, these approaches typically require large amounts of training data to effectively adapt models to the medical field. Moreover, most existing models are trained on English datasets, limiting their practicality in non-English-speaking regions where healthcare professionals and patients are not always fluent in English. The need for translation introduces additional costs and inefficiencies. To address these challenges, we propose a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging (JRadiEvo). This is the first attempt to extend a non-medical vision-language foundation model to the medical domain through evolutionary optimization of model merging. We successfully created a model that generates accurate Japanese reports from X-ray images using only 50 translated samples from publicly available data. This model, developed with highly efficient use of limited data, outperformed leading models from recent research trained on much larger datasets. Additionally, with only 8 billion parameters, this relatively compact foundation model can be deployed locally within hospitals, making it a practical solution for environments where APIs and other external services cannot be used due to strict privacy and security requirements.","In recent years, foundational models (FMs) have seen remarkable advancements, transforming various fields by offering more sophisticated and powerful solutions [1]. A key driver of this progress has been the rise of large language models (LLMs), which have greatly expanded the capabilities of FMs, particularly in processing and generating text with high accuracy and contextual understanding. This has sparked exponential growth in research [2], leading to the development of vision-language models that integrate visual and textual data [3, 4, 5], as well as fine-tuning approaches that enhance model performance for specific tasks [6, 7]. Healthcare is one of the most critical application areas for foundational models. The need to develop models tailored to healthcare is essential, particularly because physicians often face the challenge of reviewing large volumes of medical data, such as X-rays, which can be time-consuming and demanding. Advanced FMs can help alleviate this burden by enabling quicker and more efficient diagnoses, improving the overall effectiveness of healthcare delivery and patient outcomes. In response to this need, various FMs have been fine-tuned specifically for the healthcare domain, further enhancing their accuracy and effectiveness in clinical settings [7, 8, 9]. However, despite these advancements, several challenges remain. One significant issue is that most of the models developed so far, such as LLaVA-Med [7] and MedPaLM 2 [8], are predominantly in English, whereas many healthcare professionals and patients are not always proficient in English. For these models to be truly practical, there is a pressing need to expand their capabilities to non-English languages. Relying on a two-step process, where the model first generates output in English and then translates it, can introduce additional costs and complexity, making it less efficient and accessible. Additionally, publicly available datasets that can be used to train these models, such as MIMIC-CXR [10] and IU X-Ray [11], are overwhelmingly in English, with very few datasets available in other languages. Translating the large amounts of data needed for training into other languages with high quality is a costly and resource-intensive process. This scarcity of non-English datasets makes it difficult to develop models that can handle non-English languages. Furthermore, due to privacy concerns, it is challenging to collect and use patient data for model training, further complicating the creation of such datasets. Also, the use of large models through APIs, such as GPT-4 [12], is often impractical in healthcare settings because of the stringent privacy regulations that protect patient data, which limits the deployment of these models in real-world clinical environments. To address these challenges, this paper presents a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging [13] (JRadiEvo), a first attempt to extend a multimodal vision-language model for non-English medical text generation by utilizing evolutionary optimization of model merging [13]. JRadiEvo was developed by merging a non-medical vision-language model, medical text-to-text models, and a Japanese-language text-to-text model using an evolutionary algorithm. This innovative approach enabled the efficient creation of a Japanese radiology report generation model using only a minimal amount of Japanese-language data, addressing the critical need for non-English medical text generation in a resource-constrained environment. Below we outline our key contributions, which aim to advance the field of multimodal foundational models in healthcare: 1. Efficient use of limited non-English medical data: In the context of the difficulty in collecting non-English datasets, JRadiEvo demonstrates the ability to create a non-English medical report generation model by translating and utilizing only 50 cases from publicly available English datasets. This approach highlights the efficiency of the development process, demonstrating how a non-English medical report generation model can be created using extremely limited data and annotations. Additionally, it is noteworthy that not only was the dataset used after translation limited to 50 cases, but the entire dataset used to create JRadiEvo consisted of just 50 cases. This underscores the fact that JRadiEvo efficiently utilizes a very limited amount of data, demonstrating an effective approach to handling medical data under strict privacy and security constraints. 2. Novel application of model merging in the medical vision-language model: Traditionally, adapting models to the medical domain has relied on fine-tuning or training from scrach. To the best of our knowledge, there are no existing study of applying model merging alone to adapt a vision-language model to the medical domain. While recent research [13] has proposed using evolutionary optimization of model merging for vision-language models, this approach has been limited to natural images. To our knowledge, no prior studies have extended this technique to medical images or other domain-specific imagery beyond natural images. 3. Lightweight model for local deployment: JRadiEvo is an 8B parameter model, making it lightweight enough to be deployed on local hospital computing systems without the need for external APIs. This local deployment capability addresses critical privacy and security concerns, allowing hospitals to maintain control over patient data. Additionally, given the challenges of equipping facilities with expensive GPUs proportional to patient numbers, JRadiEvo’s compact size and low GPU memory requirements make it practical for widespread use. 4. Cost-efficient training process: JRadiEvo eliminates the need for computationally expensive backpropagation during training, enabling a far more efficient learning process compared to training a new model or fine-tuning. Additionally, by leveraging model merging instead of fine-tuning, JRadiEvo avoids the common issue of catastrophic forgetting [14, 15, 16] that often occurs during fine-tuning, allowing for a more stable and efficient development process."
https://arxiv.org/html/2411.09779v1,Variational methods for Learning Multilevel Genetic Algorithms using the Kantorovich Monad,"Levels of selection and multilevel evolutionary processes are essential concepts in evolutionary theory, and yet there is a lack of common mathematical models for these core ideas. Here, we propose a unified mathematical framework for formulating and optimizing multilevel evolutionary processes and genetic algorithms over arbitrarily many levels based on concepts from category theory and population genetics. We formulate a multilevel version of the Wright-Fisher process using this approach, and we show that this model can be analyzed to clarify key features of multilevel selection. Particularly, we derive an extended multilevel probabilistic version of Price’s Equation via the Kantorovich Monad, and we use this to characterize regimes of parameter space within which selection acts antagonistically or cooperatively across levels. Finally, we show how our framework can provide a unified setting for learning genetic algorithms (GAs), and we show how we can use a Variational Optimization and a multi-level analogue of coalescent analysis to fit multilevel GAs to simulated data.","Figure 1: Multilevel evolutionary process for ant colonies. Selection may act at the cellular, individual or colony level, and fitness any of these levels may conflict with fitness at any other. An evolutionary process exhibits multilevel selection when it operates on units at multiple levels, each with its own definition of fitness. For example, an evolutionary process involving ant colonies will exhibit units at least three levels: cells (basic units), ants (individuals), and colonies (groups of individuals). Such processes are important in diverse contexts. Evolutionary theory hypothesizes that major transitions in evolution, such as the emergence of multicellularity, can be modeled in terms of multilevel selection [calcott2011major, okasha2006evolution]. At a different scale, cancer can also be viewed as a multilevel evolutionary process [okasha2021cancer]. Further, in the context of genetic algorithms, the use of hierarchical fitness functions has been shown to offer advantages when solving particular classes of problem [watson2006compositional, watson1999hierarchically]. Despite the apparent ubiquity of multilevel evolutionary processes, few canonical models exist for studying this phenomenon in its essential aspects. Prior models include evolutionary game theoretic models based on Moran dynamics [traulsen2006evolution], models based on hierarchical genetic algorithms [watson2006compositional], and group or kin selection models based on the recursive Price equation [frank2020generalized, okasha2006evolution]. While the Moran dynamics approach of [traulsen2006evolution] is informative as a minimal model (involving only cooperator and defector strategies), it is unclear how to extend this model to capture additional levels of important structure, such as large genotypes, complex maps between genotype and phenotype, and levels beyond the group level. Further, while genetic algorithm based approaches such as [watson2006compositional] allow a high level of generality, the connection to biological evolutionary models is indirect, limiting their use as a test-bed for studying such phenomena. In addition, approaches based on the recursive Price equation [frank2020generalized, okasha2006evolution] offer a general theoretical mechanism for probing phenomena related to multilevel selection; however, in general, they are not dynamically sufficient, and hence cannot be used as a complete model of an evolutionary process (for instance, for simulation). Finally, while all of the above offer models of multilevel processes, they do not consider how the parameters of such models may be learned from data (real or synthetic) generated by such processes; indeed, this problem has received little attention, and it may be argued that developing methods of inference for multilevel processes is necessary for the objective identification and characterization of such processes in nature (for instance, determining the number of levels present, see [warrell2020cyclic]). In light of the above, we introduce here a minimal model of multilevel selection based on the Wright-Fisher model, which we refer to as a ‘multilevel Wright-Fisher process’. Our model is minimal in the sense of including only those components necessary to embed a basic level of biological realism into the model; hence, we allow arbitrarily large genotypes and complex maps between genotype and phenotype to be embedded, while providing a consistent mechanism for extending the model to an arbitrary number of levels. We are motivated here by a desire to both simulate and characterize such processes theoretically, as well as to use this model as a test-bed for developing algorithms to infer multilevel evolutionary processes in a sufficiently complex setting, which will have the required scalability to be applied to genomics scale data. A particularly promising domain for such algorithms is in the area of cancer genomics, where the inference of predictive evolutionary models can help inform inference of tumor clonal structure and patient stratification for treatment selection, for example. Our approach is inspired by multiple recent advances in machine learning and computer science. To allow maximal generality in our framework (in terms of number of levels and underlying spaces), we formulate our model in terms of stochastic processes over the Kantorovich Monad, which may be defined in the category of bounded metric spaces [van2005metric]. This allows the model to be extended over an indefinite number of levels by using the Wasserstein distance recursively to define similarity between pairs of members in a population, pairs of populations themselves, and pairs of meta-populations. Conveniently, this approach also allows us to define recursive objective functions using the Wasserstein distance, which may be optimized efficiently. Training objectives using the Wasserstein distance have been employed extensively in recent machine learning, as an alternative to likelihood-based objectives (see [tolstikhin2017wasserstein]), but we are not aware of prior methods that have used the Kantorovich Monad as a means of defining recursive objectives for similarly hierarchically structured models. We provide a theoretical analysis of the properties of our framework, deriving a multilevel Price equation for the model, and using this to characterize regimes of parameter space within which selection acts antagonistically or cooperatively across levels. In addition, we introduce a general optimization framework, combining Variational and Bayesian optimization methods with classical Coalescent analysis methods [felsenstein2004inferring]. There have been several recent attemps to use variational techniques to provide improved methods for phylogenetic inference [moretti2021variational, zhang2018variational, zhang2020improved]. Our methods offer distinct advantages over such approaches, by placing minimal constraints on the evolutionary processes to be learned, including being applicable to single and multilevel processes. Additionally, our approach is orthogonal to those above by applying a variational optimization (VO) as opposed to a variational inference (VI) approximation [leordeanu2008smoothing]. Sec. 2 outlines our multilevel Wright-Fisher model, and Sec. 3 then provides an analysis of theoretical properties of our framework. Sec. 4 introduces our optimization framework, including a multi-level analogue of coalescent analysis. Sec. 5 illustrates our optimization approach on a small-scale example problem, involving learning a multilevel genetic algorithm over solutions to traveling salesman problems, and Sec. 6 concludes with a discussion."
https://arxiv.org/html/2411.09004v1,The geometry of the Deep Linear Network,"This article provides an expository account of training dynamics in the Deep Linear Network (DLN) from the perspective of the geometric theory of dynamical systems. Rigorous results by several authors are unified into a thermodynamic framework for deep learning.The analysis begins with a characterization of the invariant manifolds and Riemannian geometry in the DLN. This is followed by exact formulas for a Boltzmann entropy, as well as stochastic gradient descent of free energy using a Riemannian Langevin Equation. Several links between the DLN and other areas of mathematics are discussed, along with some open questions.","In its simplest form, deep learning is a version of function approximation by neural networks, where the parameters of the network are determined by given data. The best fit is determined, or the network is trained, by minimizing an empirical cost function using gradient descent. Many of the mysteries of deep learning concern training dynamics: Do we have convergence? If so, how fast and to what? How do we make training more efficient? How do the training dynamics depend on the network architecture or on the size of data? This article focuses on mathematical foundations. Our goal is to illustrate the utility of the geometric theory of dynamical systems for the study of these questions. While gradient flows have been studied in mathematics since the 1930s, gradient flows arising in deep learning have two subtle aspects –overparametrization and degenerate loss functions – that prevent naive applications of the standard theory of gradient flows (see Section 3.2) . We present a geometric framework for a simplified model, the Deep Linear Network (DLN), where these aspects can be studied with complete rigor. The DLN is deep learning for linear functions. This reduces the training dynamics to gradient flows on spaces of matrices. Despite its apparent simplicity, the model has a rich mathematical structure. Overparametrization provides a foliation of phase space by invariant manifolds. Of these, there is a fundamental class of invariant manifolds, the balanced manifolds, which are themselves foliated by group orbits. This geometric structure allows us to define a natural Boltzmann entropy (the logarithm of the volume of a group orbit) that may be computed explicitly. Microscopic fluctuations that underlie the entropy may be described by Riemannian Langevin equations. This approach unifies the work of several authors into a thermodynamic framework. In particular, it suggests an entropic origin for implicit regularization. My view is that the DLN is a gift to mathematics from computer science. On one hand, it is subtle, but tractable, providing a rich set of practical questions and insights. On the other hand, the study of the DLN is filled with sharp theorems, exact formulas and unexpected mathematical structure. While the gradient dynamics of the DLN are different from the standard theory, we also see familiar aspects in surprising combinations. This gives the analysis a classical feel, even though all the results here were obtained in the very recent past. There is plenty more to be discovered and the real purpose of this article is to explain why. In order to apply the methods of dynamical systems theory, what is of most value is to understand the dynamicists ‘way of seeing’. This is not so much a collection of theorems, as a systematic use of geometry, and particular examples, to figure out what questions one should ask. Geometric methods provide a powerful intuition that is often a source of new discoveries. This is the approach we use in this article. All the theorems we prove are guided by the work of computer scientists in the area. While this article does include some advanced mathematics, especially Riemannian geometry and random matrix theory, we stress explicit calculations, heuristic insights and representative examples. We hope this approach reveals the conceptual power of dynamical systems theory while remaining of interest to practitioners. The references are representative, not exhaustive, since our aim in this article is to provide a pedagogical treatment. We include a brief discussion of the literature on the DLN in Section 13.2. For broader surveys of deep learning that include related mathematical ideas, the reader is referred to the recent books [1, 37]. The article concludes with open questions that emerge from this perspective. These include specific mathematical questions on the DLN, as well as the extension of our entropy formula to gauge groups arising in deep learning."
https://arxiv.org/html/2411.09702v1,On the Surprising Effectiveness of Attention Transfer for Vision Transformers,"Conventional wisdom suggests that pre-training Vision Transformers (ViT) improves downstream performance by learning useful representations. Is this actually true? We investigate this question and find that the features and representations learned during pre-training are not essential. Surprisingly, using only the attention patterns from pre-training (i.e., guiding how information flows between tokens) is sufficient for models to learn high quality features from scratch and achieve comparable downstream performance. We show this by introducing a simple method called attention transfer, where only the attention patterns from a pre-trained teacher ViT are transferred to a student, either by copying or distilling the attention maps. Since attention transfer lets the student learn its own features, ensembling it with a fine-tuned teacher also further improves accuracy on ImageNet. We systematically study various aspects of our findings on the sufficiency of attention maps, including distribution shift settings where they underperform fine-tuning. We hope our exploration provides a better understanding of what pre-training accomplishes and leads to a useful alternative to the standard practice of fine-tuning. Code to reproduce our results is at https://github.com/alexlioralexli/attention-transfer.","Pre-training has emerged as a dominant paradigm in machine learning and has significantly improved performance on a variety of tasks [27, 11, 2, 22]. In computer vision in particular, self-supervised representation learning methods [21, 6, 4, 22] and weakly supervised methods [40, 45] have enabled learning from large amounts of images. It is widely accepted that these methods work because they teach models useful features that are relevant for downstream tasks. But is this story actually true? Perhaps there is another capability learned during pre-training that is sufficient to explain its benefits. In this paper, we present an alternative explanation: pre-training teaches the model how information should be routed between tokens. We specifically focus on Vision Transformers (ViT) [12], not only because they are the most popular architecture for scaling, but also because Transformers explicitly decouple this information flow. Inter-token communication is solely fulfilled by attention, while the remaining bulk of computation are intra-token operations that are applied to each token independently. In contrast, other architectures such as ConvNets [33, 20] simultaneously expand the receptive fields and extract the features, making it difficult to isolate the effect of information flow. We hypothesize that the features computed by the intra-token operations are not essential to explain the benefits of pre-training, and that the pre-trained attention maps are typically sufficient for downstream tasks. We test our hypothesis by introducing a new set of methods called attention transfer. Concretely, we treat a pre-trained ViT as the teacher and train a student model for downstream tasks while transferring only the attention patterns from the teacher. In contrast to the common fine-tuning paradigm of transferring all the weights (which mixes the effect of features and attention maps), only the inter-token flow is transferred. In this way, the student must learn features from scratch, while isolating the benefits of the attention maps learned during pre-training. Figure 1: Using only attention is sufficient for full performance. By copying the attention maps (top) from a MAE [22] pre-trained ViT-L [12], a ViT-L can reach a top-1 accuracy of 85.1 on ImageNet-1K [10] – recovering 77.8% of the gap between no transfer (training from scratch, 83.0) and full transfer (fine-tuning all the weights, 85.7). Distilling attention maps (bottom) can even fully match MAE weight tuning while only transferring the inter-token flow. We study two types of attention transfer. The first is Attention Copy, which directly “copy-and-pastes” the attention maps. The learning is fully decoupled, as inter-token computation is entirely from the teacher, and the student only learns intra-token patterns routed by the teacher’s attention maps. This is well-suited as a scientific probe, but is less practical since both networks need to be forwarded during the inference. The second is Attention Distillation, where the student simply distills attention patterns from the teacher, whose attention maps are no longer used after training. This is practical, but also helps identify the importance of the teacher’s inter-token information flow. While both attention transfer variants are straightforward, we find them highly effective. Figure 1 illustrates this with a ViT-L [12] pre-trained using Masked Autoencoding (MAE) [22]. Compared to no transfer (training from scratch) and full transfer (fine-tuning all the MAE weights), Attention Copy can close most of the gap in performance, whereas Attention Distillation can match the fine-tuning accuracy on ImageNet-1K classification [10]. This is achieved by only transferring the inter-token flow from the same model. Furthermore, since attention transfer requires the student to learn features from scratch, those features are significantly different from the teachers’ (Figure 5) and improve ImageNet-1K accuracy score to 86.3 (+0.6) when ensembled with the teacher (Figure 6). To summarize, we make the following contributions: • Detailed analysis on the sufficiency of attention maps. We find that solely using the pre-trained attention patterns is typically sufficient to achieve the same downstream accuracy as fine-tuning on ImageNet-1K. Furthermore, we observe practical benefits, as ensembling with attention transfer significantly improves ImageNet performance. This calls into question the commonly-believed story that pre-training is only about feature learning. While our main observation is robust w.r.t. different models and pre-training methods, we do find settings where pre-trained features are indeed necessary to realize the full gains from pre-training. Our bare-minimum solution for attention transfer is more affected by data distribution shifts compared to weight tuning. Section 4 presents extensive analyses to better understand the behaviors of attention transfer. They are i) partial transfer with a subset of layers or heads; ii) variants of our method that transfer other attention-related activations; and importantly, iii) various ways to verify that the student is not just re-learning the teacher model. Section 5 systematically tests how well our findings apply across a variety of pre-training and fine-tuning datasets, pre-training methods, model sizes, and tasks. • Attention transfer methods. We introduce Attention Copy and Attention Distillation, which are methods to train a ViT on a downstream task while utilizing only the attention maps of a pre-trained teacher ViT. These methods help us understand the role of the features versus the attention patterns learned during pre-training. With further research, attention transfer could offer a potential alternative to the decade-long practice of fine-tuning pre-trained vision models [16, 12, 22]. Nearly all aspects of the fine-tuning pipeline have been thoroughly examined, suggesting a probable saturation of recipes. Weight sharing can also face security risks (e.g., white-box attacks [17]). We hope our systematic examination of attention transfer sheds new light on how to leverage pre-trained ViTs, and will help establish this approach as an effective alternative when weight transfer is less applicable. Figure 2: Two types of Attention transfer for Vision Transformers. Attention Copy (left): We simply “copy-and-paste” the attention maps from a pre-trained teacher model to a randomly initialized student one. Other weights of the student are then trained via supervised learning. This fully decouples inter-token learning (from the teacher) and intra-token learning (in the student); but is less practical. Attention Distillation (right): The student computes its own attention maps, with an additional cross-entropy loss to distill patterns from the teacher during training. The teacher is no longer used during inference. H: number of heads; L: number of Transformer layers."
https://arxiv.org/html/2411.09648v1,Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable Medical Information,"This paper introduces Med-Bot, an AI-powered chatbot designed to provide users with accurate and reliable medical information. Utilizing advanced libraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq, Med-Bot is built to handle the complexities of natural language understanding in a healthcare context. The integration of llama-assisted data processing and AutoGPT-Q provides enhanced performance in processing and responding to queries based on PDFs of medical literature, ensuring that users receive precise and trustworthy information. This research details the methodologies employed in developing Med-Bot and evaluates its effectiveness in disseminating healthcare information.","The integration of artificial intelligence (AI) into healthcare has catalyzed a transformative shift in how medical services are delivered, with medical chatbots emerging as a prominent innovation. These chatbots leverage AI and natural language processing (NLP) technologies to offer users a range of healthcare-related services, from providing medical information to assisting with diagnostics and treatment suggestions. This evolution addresses the growing demand for accessible healthcare solutions amid the shortage of medical professionals and the increasing complexity of patient needs. Medical chatbots are designed to enhance patient engagement by offering timely, accurate, and personalized responses. Their ability to interact with users in natural language, coupled with advanced algorithms for understanding and processing medical queries, allows them to simulate the experience of consulting with a healthcare provider. This capability not only improves patient accessibility to medical information but also supports healthcare professionals by streamlining routine inquiries and administrative tasks. As the field continues to advance, various methodologies and technologies are being explored to enhance the effectiveness of these chatbots. Researchers are focusing on improving the accuracy of diagnostics, the relevance of information provided, and the overall user experience. Innovations such as context-aware processing and advanced machine learning algorithms are playing crucial roles in refining these systems. In this evolving landscape, our research seeks to push the boundaries further by employing cutting-edge techniques to enhance the capabilities of medical chatbots. By integrating state-of-the-art technologies and methodologies, we aim to address existing limitations and provide a more robust, adaptive, and reliable solution for healthcare assistance."
https://arxiv.org/html/2411.08735v1,New advances in universal approximation with neural networks of minimal width,"Deep neural networks have achieved remarkable success in diverse applications, prompting the need for a solid theoretical foundation. Recent research has identified the minimal width \max\{2,d_{x},d_{y}\} required for neural networks with input dimensions d_{x} and output dimension d_{y} that use leaky ReLU activations to universally approximate L^{p}(\mathbb{R}^{d_{x}},\mathbb{R}^{d_{y}}) on compacta. Here, we present an alternative proof for the minimal width of such neural networks, by directly constructing approximating networks using a coding scheme that leverages the properties of leaky ReLUs and standard L^{p} results. The obtained construction has a minimal interior dimension of 1, independent of input and output dimensions, which allows us to show that autoencoders with leaky ReLU activations are universal approximators of L^{p} functions. Furthermore, we demonstrate that the normalizing flow LU-Net serves as a distributional universal approximator. We broaden our results to show that smooth invertible neural networks can approximate L^{p}(\mathbb{R}^{d},\mathbb{R}^{d}) on compacta when the dimension d\geq 2, which provides a constructive proof of a classical theorem of Brenier and Gangbo. In addition, we use a topological argument to establish that for FNNs with monotone Lipschitz continuous activations, d_{x}+1 is a lower bound on the minimal width required for the uniform universal approximation of continuous functions C^{0}(\mathbb{R}^{d_{x}},\mathbb{R}^{d_{y}}) on compacta when d_{x}\geq d_{y}.","{justify} Feed-forward neural networks (FNNs) are compositions of affine transformations and activation functions, which are applied dimension-wise to their respective vector inputs. Their structure makes them adaptable to transform inputs of any given dimension into outputs of any desired dimension. As FNNs have achieved promising results in applications across many different areas related to the approximation of multidimensional functions, the question arises whether these results are based on the strong theoretical approximation capabilities of the model structure. {justify} Indeed, this turned out to be true: a rich set of mathematical theorems, called universal approximation (UAP) results, have been proven, demonstrating that FNNs with suitable activation functions can approximate key function classes used in practice, specifically continuous and L^{p} functions. Mathematically, universal approximation results characterize the closure of a function class, corresponding to the neural network architecture, with respect to a chosen norm. This is usually either an L^{p} norm with p\in[1,\infty) for classes of L^{p} integrable functions, or the supremum norm for continuous functions on bounded sets. Usually, the corresponding function classes are restricted to arbitrary compact subsets of the input space. For example, if a neural network architecture is a L^{p} universal approximator, then the closure of the function class with respect to the L^{p} norm describing all possible architecture configurations, restricted to a compact set \mathcal{K}, is a superset of all L^{p} functions restricted to \mathcal{K}. Similarly, we call a function class a uniform universal approximator if its closure, with respect to the supremum norm, contains the target function class of continuous functions. If the approximation holds not only on compact sets but on the entire input space, it is referred to as a global universal approximation result. {justify} The classical results are the universal approximation theorems of Cybenko [8], Hornik et al. [17], and Pinkus [30]. They analyze the universal approximation properties of the so-called shallow FNNs, which typically have either a single hidden layer or a uniformly bounded number of hidden layers, and can have arbitrary width, i.e. they can contain any number of neurons in the hidden layers. In 1988, Hornik et al. [17] proved that shallow FNNs with non-decreasing sigmoidal functions universally approximate the set of continuous functions. Independently, Cybenko [8] showed in 1889 that shallow FNNs with continuous sigmoidal activation functions universally approximate continuous functions and L^{1} integrable functions on compact sets. Moreover, in 1999, Pinkus [30] proved that shallow FNNs with a continuous activation function, arbitrary width, and uniformly bounded depth are universal approximators of continuous functions if and only if the activation function is non-polynomial. {justify} With the rise of deep neural networks, which have achieved even better results than classical shallow neural networks in various applications, universal approximation results for narrow FNNs have become a popular research topic. Narrow FNNs are neural networks of fixed width and arbitrary depth, i.e. they can have an arbitrary number of layers, but the number of neurons in each layer is upper bounded by the (maximal) width. In this context, for given activation functions, target function class, and chosen norm, the minimal (-maximal) width w_{\min} refers to the lower bound on the (maximal) width such that the corresponding FNNs, utilizing these activations, can have the universal approximation property for the desired function class. To align with literature conventions, we will refer to w_{\min} as minimal width. In this sense, w_{\min} determines the thinnest architecture of a FNN with a given activation function to universally approximate the targeted function class. Additionally, for a fixed FNN, we introduce the minimal interior dimension d_{\min}, which is the minimal number of neurons in a layer over all layers of the neural network. In this regard, an FNN with d_{\min} smaller than the input and output dimension can be understood as an autoencoder as the network encodes all inputs into features of lower dimension until the layer where d_{\min} is reached and then decodes these features back to higher dimensional output. {justify} Modern universal approximation results for narrow FNNs not only show that bounds for the width exist for the universal approximation property, but also specify concrete bounds on w_{\min}. It is important to note that to obtain an upper bound, approximations of arbitrary precision must be constructed for every function within the target function class. In contrast, proving a lower bound only requires finding a single function in the class that the FNN architecture cannot approximate if its width is below the bound. Consequently, proving upper bounds for the minimal width, is generally considered more challenging than proving lower bounds. {justify} An interesting property of universal approximation theorems is achieved when the minimal width allows for the construction of narrow networks with an invertible architecture. Invertible neural networks form bijective mappings, which require squared weight matrices, resulting in fixed-width layers across the entire network architecture including the input and output layer. This invertible structure enables generative learning via maximum likelihood training, as e.g. implemented in normalizing flows [10, 20, 21]. Consequently, universal approximation results for narrow networks, if these bounds are sufficiently tight, can also implicitly demonstrate the universal approximation properties of normalizing flows through invertible neural network architectures. A suitable network architecture for this purpose is LU-Net [6], an invertible neural network specifically designed to resemble a “vanilla” design as closely as possible. In this way, LU-Net directly inherits the universal approximation properties established in theorems for narrow networks, while also retaining the generative capabilities of normalizing flows due to its invertible architecture. Reference Function class Activation Minimal width Minimal int. dim. Park [28] L^{p}(\mathbb{R}^{d_{x}},\mathbb{R}^{d_{y}}) ReLU w_{\min}=\max\{d_{x}+1,d_{y}\} d_{\min}=1 L^{p}(\mathcal{K},\mathbb{R}^{d_{y}}) cont. non-poly. w_{\min}\leq\max\{d_{x}+2,d_{y}+1\} d_{\min}=1 Cai [5] L^{p}(\mathcal{K},\mathbb{R}^{d_{y}}) Arbitrary w_{\min}^{*}:=\max\{d_{x},d_{y}\}\leq w_{\min} - L^{p}(\mathcal{K},\mathbb{R}^{d_{y}}) LReLU+ABS w_{\min}=w_{\min}^{*}=\max\{d_{x},d_{y}\} - L^{p}(\mathcal{K},\mathbb{R}^{d_{y}}) LReLU w_{\min}=\max\{2,d_{x},d_{y}\} - Ours L^{p}(\mathcal{K},\mathbb{R}^{d_{y}}) LReLU w_{\min}\leq\max\{2,d_{x},d_{y}\} d_{\min}=1 Table 1: Summary of known results for the minimal width and minimal interior dimension of FNNs with different activation functions for universal approximation of Lebesgue-integrable functions w.r.t to the L^{p} norms, where p\in[1,\infty). We define w_{\min}^{*} as the lowest width required for UAP with arbitrary activation functions, which is the same for uniform and L^{p} UAP. We denote the input and output dimensions by d_{x} and d_{y}, respectively. \mathcal{K} is a placeholder for a non-empty compact subset of \mathbb{R}^{d_{x}}. The upper bounds hold for all such compact sets and the lower bounds hold for at least one specific \mathcal{K}. LReLU denotes the leaky ReLU activations (see Definition 2.8), ABS the absolute value and ""cont. non-poly."" any continuous non-polynomial activation function. {justify} In recent years, several important UAP results have been presented for narrow FNNs of arbitrary depth. We present the state of the art L^{p} universal approximation results for narrow FNNs in Table 1, omitting results that have already been surpassed. Additionally, in this paragraph, we discuss most L^{p} UAP results, including relevant older ones that have been outclassed. In 2017, Lu et al. [24] showed upper and lower bounds for the minimum width of ReLU FNNs for global universal approximation of functions in L^{1}(\mathbb{R}^{d_{x}},\mathbb{R}^{d_{y}}), while , in 2020, Kidger and Lyons [19] proved an upper bound on the minimum width of ReLU FNNs for the global universal approximation of functions in L^{p}(\mathbb{R}^{d_{x}},\mathbb{R}^{d_{y}}). Later in 2020, Park et al. [28] fully characterized the minimum width for FNNs with ReLU activations for global universal approximation of functions in L^{p}(\mathbb{R}^{d_{x}},\mathbb{R}^{d_{y}}) and concluded w_{\min}=\max\{2,d_{x},d_{y}\}, achieving tighter bounds than those shown by Lu et al. [24] and Kidger and Lyons [19]. In addition, Park et al. [28] derived an upper bound on the minimum width w_{\min}\leq\max\{d_{x}+2,d_{y}+1\} for FNNs with continuous non-polynomial activation functions with respect to the universal approximation of L^{p}(\mathcal{K},\mathbb{R}^{d_{y}}), where \mathcal{K}\subset\mathbb{R}^{d_{x}} is a compact set. Recently, in 2023, Cai [5] showed that the minimal width for universal approximation of L^{p}(\mathcal{K},\mathbb{R}^{d_{y}}) for compact \mathcal{K}\subset\mathbb{R}^{d_{x}} is lower bounded by \max\{d_{x},d_{y}\} . Moreover, they showed that FNNs with leaky ReLU (LReLU) activations achieve w_{\min}=\max\{2,d_{x},d_{y}\}, and hence attain the optimal minimal width over all classes of activations if \max\{d_{x},d_{y}\}\geq 2. Additionally, Cai [5] considered FNNs with LReLU+ABS to obtain a minimum width of \max\{d_{x},d_{y}\}, therefore achieving the optimal minimal width even in the one-dimensional case. Reference Function class Activation Minimal width & Minimal int. dim. Kidger [19] C(\mathcal{K},\mathbb{R}^{d_{y}}) cont. non-poly.1 w_{\min}\leq d_{x}+d_{y}+1 C(\mathcal{K},\mathbb{R}^{d_{y}}) non-aff. poly. w_{\min}\leq d_{x}+d_{y}+2 Hanin [15] C(\mathcal{K},\mathbb{R}^{d_{y}}) ReLU d_{x}+1\leq w_{\min}\leq d_{x}+d_{y} Park [28] C(\mathcal{K},\mathbb{R}^{d_{y}}) ReLU+STEP w_{\min}=\max\{d_{x}+1,d_{y}\}, d_{\min}=1 C([0,1],\mathbb{R}^{2}) ReLU \max\{d_{x}+1,d_{y}\}<w_{\min}=3 Cai [5] C(\mathcal{K},\mathbb{R}^{d_{y}}) Arbitrary w_{\min}^{*}:=\max\{d_{x},d_{y}\}\leq w_{\min} C(\mathcal{K},\mathbb{R}^{d_{y}}) ReLU+\operatorname{FLOOR} w_{\min}=\max\{2,d_{x},d_{y}\} C(\mathcal{K},\mathbb{R}^{d_{y}}) \operatorname{UOE}+\operatorname{FLOOR} w_{\min}=w_{\min}^{*}=\max\{d_{x},d_{y}\} C([0,1],\mathbb{R}^{d_{y}}) \operatorname{UOE} w_{\min}=w_{\min}^{*}=d_{y} Johnson [18] C(\mathcal{K},\mathbb{R}) unif. cont.2 d_{x}+1\leq w_{\min} Ours {}^{\ddagger}C(\mathcal{K},\mathbb{R}^{d_{y}}),d_{x}\geq d_{y} C_{\operatorname{Lip}}^{\operatorname{mon}}(\mathbb{R},\mathbb{R})^{{\dagger}} d_{x}+1\leq w_{\min} • 1 Continuous non-polynomial functions f:\mathbb{R}\rightarrow\mathbb{R} that are differentiable for at least one point z\in\mathbb{R} with f^{\prime}(z)\neq 0. • 2 Requires the existence of a sequence of bijective functions that uniformly approximate the activation. • ‡ The inequality only holds for compact sets \mathcal{K}\subset\mathbb{R}^{d_{x}} with non-empty interior, such that Proposition 10.5 is applicable. • † The set of activation functions needs to be a subset of C_{\operatorname{Lip}}^{\operatorname{mon}}(\mathbb{R},\mathbb{R}), i.e. all activations are Lipschitz continuous and monotone (for details see Definition 2.35 and 5). Table 2: Summary of known results for the minimal width of FNNs with different activation functions for universal approximation of continuous functions w.r.t. the supremum norm. The minimal interior dimension is only included in the table if it is non-trivial and smaller than the minimal width, i.e. if 1\leq d_{\min}<w_{\min}. \mathcal{K} is a placeholder for a non-empty compact subset of \mathbb{R}^{d_{x}}.The upper bounds hold for all such compact sets and the lower bounds hold for at least one specific \mathcal{K}. FLOOR refers to the function that rounds a real number down to the nearest integer. UOE refers to the set of functions that exhibit universal ordering or extrema (see Definition 7 in Cai [5]). {justify} In Table 2, we provide an overview of state of the art for uniform universal approximation of continuous functions. In this regard, in 2018, Hanin and Sellke [15] derived lower and upper bounds for the minimal width required for universal approximation of continuous functions with ReLU FNNs. Moreover, in 2020, Kidger and Lyons [19] obtained uniform universal approximators for the continuous functions, using FNNs that utilize either continuous non-polynomial or non-affine polynomial activations. Later in 2020, Park et al. [28] showed that FNNs with ReLU+STEP activations achieve a minimal width of w_{\min}=\max\{d_{x}+1,d_{y}\}. Furthermore, in 2023, Cai [5] established that the minimal width required for uniform universal approximation with FNNs, regardless of the activation function, is bounded from below by w_{\min}^{*}:=\max\{d_{x},d_{y}\}. Additionally, Cai [5] showed that FNNs with ReLU+FLOOR activations achieve w_{\min}=\max\{2,d_{x},d_{y}\} for uniform universal approximation of continuous functions, making them optimal when \max\{d_{x},d_{y}\}\geq 2. Furthermore, they concluded that FNNs using UOE+FLOOR activations attain the minimal width w_{\min}=\max\{d_{x},d_{y}\} for universal approximation. It is worth noting that while this result indicates that UOE+FLOOR FNNs are optimal across all input and output dimensions in theory, UOE activations are primarily a theoretical construct and not readily practical. Therefore, ReLU+FLOOR FNNs offer a more feasible choice for practical applications, while being theoretically optimal for most choices of dimensions. {justify} Furthermore, in 2018, Johnson [18] derived the lower bound of d_{x}+1 for FNNs with uniformly continuous activations that can be uniformly approximated by bijective functions when the output dimension is d_{y}=1. Our 5 shows that d_{x}+1 is a lower bound for uniform universal approximation of continuous functions for FNNs with monotone Lipschitz continuous functions, provided the output dimension satisfies d_{x}\geq d_{y}. This shows that a width of at least d_{x}+1 is required to achieve uniform universal approximation of continuous functions for almost all of the activation functions commonly used in practice [12, 9], when d_{x}\geq d_{y}. Therefore 5 can be seen as a generalization of the universal approximation results from Johnson [18] to higher output dimensions d_{y} if they’re smaller or equal than d_{x}. It is important to note that, in this case, the lower bound is tighter by one compared to the lower bound for arbitrary activations stated by Cai [5]. {justify} Our main contributions in this work are divided into four parts. In the first part, we prove 1 in Section 6, which shows the upper bound \max\{2,d_{x},d_{y}\} for the universal approximation of L^{p}(\mathcal{K},\mathbb{R}^{d_{y}}) on arbitrary compact sets \mathcal{K}\subset\mathbb{R}^{d_{x}} with FNNs equipped with leaky ReLU activations in 1. To formally prove this, we present a coding scheme in Section 3 to construct explicit approximations with leaky ReLU FNNs in Section 4 and Section 5. We note that the same result was originally established in [5], but by a different approach that exploits the strong approximation properties of flow maps for ordinary differential equations [22], and approximating them with leaky ReLU FNNs [11]. Therefore, the proof of Cai [5] essentially relies on the strong approximation capabilities of neuralODE [33] corresponding to Theorem 2.3 of Li et al. [22] that is is is similar to the well-known Theorem 2. (i) of Brenier and Gangbo [3], which shows that diffeomorphisms are L^{p} universal approximators. In contrast, our proof of 1 relies only on basic results of analysis and also explicitly constructs the approximation sequences of neural networks via the coding scheme introduced by Park et al. [28]. {justify} In the second part, we show that LU decomposable neural networks with invertible leaky ReLU activations can universally approximate L^{p}(\mathcal{K},\mathbb{R}^{d}) on arbitrary compact sets \mathcal{K}\subset\mathbb{R}^{d}, which we state formally in 2 and prove in Section 7. With this result, we then analyze the properties of LU-Net [6] as a normalizing flow. More specifically, we prove that LU-Net can transform an absolutely continuous source distribution into a sequence of distributions that converges in law to any predefined target distribution. We provide the latter statement in 3 and give the formal proof in Section 8. Similar results on distributional universal approximation have been obtained for other normalizing flows, in particular affine coupling flows [10] and NeuralODE [7], see also [35, 22, 33, 25]. {justify} In the third part we show that the set of smooth diffeomorphisms in the form of LU-decomposable almost leaky ReLU neural networks are universal approximators of L^{p}(\mathcal{K},\mathbb{R}^{d}) on any compact set \mathcal{K}\subset\mathbb{R}^{d}, which is the statement of 4 that we prove in Section 9. This means that for any compact \mathcal{K}\subset\mathbb{R}^{d} and f\in L^{p}(\mathcal{K},\mathbb{R}^{d}) there exists an approximating sequence of f in L^{p} norm of smooth invertible LU-decomposable neural networks that are limits of sequences of LU-decomposable neural networks and whose inverses are also smooth LU-decomposable neural networks. In that sense, 4 identifies a dense subset of L^{p}(\mathcal{K},\mathbb{R}^{d}) consisting of C^{\infty} diffeomorphisms that can be approximated arbitrarily by leaky ReLU FNNs. Moreover, our Corollary 2.33 resulting from 4, immediately implies the well-known Theorem 2.5 (i) of Brenier and Gangbo [3], which shows that the set of diffeomorphisms is dense in L^{p}(\mathcal{K},\mathbb{R}^{d}) for all compact sets \mathcal{K}. {justify} In the fourth part, we present 5, with the proof detailed in Section 10. Here, we establish that d_{x}+1 is a lower bound for universal approximation of continuous functions with FNNs that employ monotone Lipschitz continuous activation functions, whenever the input dimension d_{x} is greater or equal than the output dimension d_{y}. The key idea in this proof is to show that homeomorphisms are not uniform universal approximators for continuous functions — a well-known result that we revisit in Section 10, though it is not among our primary contributions. 5 has important implications for neural network design. It implies that FNNs with common activation functions [13, 9] — such as ReLU, LReLU, or sigmoid-like functions including hyperbolic tangent and the logistic sigmoid — cannot achieve uniform universal approximation of continuous functions if their width is restricted to the input dimension d_{x}, provided d_{x}\geq d_{y}. {justify} Structure of the paper. Section 2 is split into five subchapters corresponding to the five main theorems, which each introduce the relevant notation for one of the theorems, explain the relevancy of them and list some strong corollaries that follow from them. Section 3 defines and motivates the coding scheme from Park et al. [28], which is essential for the proof of our main 1, and shows it’s approximation capabilities. Section 4 analyzes the capabilities of leaky ReLUs for the approxiamtion of piecewise linear functions and Section 5 gives several preliminary results that show how leaky ReLU FNNs can be used to approximate the parts of the coding scheme, which both is combined in Section 6 to prove 1. In Section 7 we prove 2 that generalizes 1 to LU decomposable neural networks, which are INNs with LU decomposable linear transformations. We apply this in Section 8 to obtain that the normalizing flow LU-Net has the distributional universal approximation property. Section 9 further generalizes 1 to obtain a set smooth diffeomorphisms in the form of INNs that universally approximate L^{p}(\mathbb{R}^{d},\mathbb{R}^{d}). In Section 10 we give a formal proof of 5, which shows that the minimal width for uniform universal approximation of the continuous functions C^{0}(\mathbb{R}^{d_{x}},\mathbb{R}^{d_{y}}) with FNNs using Lipschitz continuous and monotone activations must be at least d_{x}+1 when the input dimension d_{x} is greater than or equal to the output dimension d_{y}. Furthermore, a comprehensive conclusion and outlook of the research on universal approximation for narrow neural networks is presented in Section 11. In the beginning of the appendix A a list of most abbreviations and notations, used in this paper, is given and moreover auxiliary definitions, some well-known results and proofs, necessary for the main proofs, are presented in the rest of the appendix."
https://arxiv.org/html/2411.08437v1,Evolutionary Algorithm with Detection Region Method for Constrained Multi-Objective Problems with Binary Constraints,"Solving constrained multi-objective optimization problems (CMOPs) is a challenging task. While many practical algorithms have been developed to tackle CMOPs, real-world scenarios often present cases where the constraint functions are unknown or unquantifiable, resulting in only binary outcomes (feasible or infeasible). This limitation reduces the effectiveness of constraint violation guidance, which can negatively impact the performance of existing algorithms that rely on this approach. Such challenges are particularly detrimental for algorithms employing the \varepsilon-based method, as they hinder effective relaxation of the feasible region. To address these challenges, this paper proposes a novel algorithm called DRMCMO based on the detection region method. In DRMCMO, detection regions dynamic monitor feasible solutions to enhance convergence, helping the population escape local optima. Additionally, these regions collaborate with the neighbor pairing strategy to improve population diversity within narrow feasible areas. We have modified three existing test suites to serve as benchmark test problems for CMOPs with binary constraints(CMOP/BC) and conducted comprehensive comparative experiments with state-of-the-art algorithms on these test suites and real-world problems. The results demonstrate the strong competitiveness of DRMCMO against state-of-the-art algorithms. Given the limited research on CMOP/BC, our study offers a new perspective for advancing this field.","\IEEEPARstart Many engineering applications involve multiple conflicting optimization objectives and one or more constraints. For example, testing resource allocation problems[1], web service location-allocation problems[2], and hybrid renewable energy system optimization [3] all fall into this category. Solving these problems requires balancing multiple objectives while ensuring all constraints are satisfied, making the problem-solving process more complex and challenging. These problems can be classified as constrained multiobjective optimization problems (CMOPs). In real-world optimization, there exists a class of constraints that may be unknown or unquantifiable, introducing additional complexity. For example, collision avoidance is a critical constraint in autonomous driving that often provides only binary feedback [4]. Similarly, in the Job Shop Problem, strict precedence relationships exist between certain operations [5], yielding binary outcomes: if these constraints are not satisfied, the solution is deemed infeasible. In multi-robot path planning, constraints that prevent collisions between robots also provide only binary feedback, without quantifiable results [6]. These non-quantifiable constraint conditions are common in CMOPs and require careful consideration. Recently, Li et al. [7] defined a specific class of CMOPs with unknown constraints, which they call CMOP/UC. In fact, the CMOP/UC they consider is a CMOP with binary constraint conditions—constraints that can only return 0 (satisfied) or 1 (violated). This paper will refer to this problem as CMOP/BC, i.e., ”Constrained Multi-Objective Problems with Binary Constraints.” Over the past two decades, evolutionary algorithms(EAs) have significantly advanced in solving multiobjective optimization problems [8]. These multiobjective optimization algorithms(MOEAs) can be broadly categorized into three types: 1) Dominance-based algorithms: Examples include NSGA-II [9] and SPEA2 [10], which excel at problems with conflicting objectives and produce well-distributed Pareto-optimal solutions. 2) Decomposition-based algorithms: Algorithms such as MOEA/D [11] and MSOPS-II [12] are advantageous in many objectives problems, providing satisfactory Pareto sets with limited resources. 3) Indicator-based algorithms: Algorithms such as IBEA [13] and HypE [14] use performance indicators to guide the selection process towards well-distributed and high-quality solutions. Each algorithm type has its strengths and weaknesses, making it essential for researchers to select or hybridize them based on problem characteristics. However, these MOEAs cannot directly solve CMOPs as they lack essential components for handling constraints [15]. Therefore, various effective constraint-handling techniques (CHTs) have been developed over the past few decades to enable MOEAs to tackle CMOPs. These CHTs can be broadly categorized into the following approaches: 1) Feasibility-oriented methods [9] [16]: These methods prioritize feasible solutions during the selection process, which may result in excessive selection pressure on feasible solutions. 2) \varepsilon-based methods [17] [18]: These methods solve CMOPs by adjusting constraint boundaries, though selecting an appropriate \varepsilon variant can be challenging. 3) Multi-objective optimization-based methods [19] [20]: Some methods reformulate the constrained problem as a multiobjective optimization problem, treating constraint violations as additional objectives, which can complicate objective optimization. 4) Stochastic ranking [21]: This approach probabilistically favors individuals with superior objective convergence while occasionally selecting those with better constraint violations. However, not all solutions exhibit dominance relationships, and determining these probabilities requires careful consideration. 5) Penalty functions [22] [23]: These algorithms incorporate constraint information into the fitness function, penalizing infeasible solutions, but tuning the penalty parameters can be challenging. These fundamental CHTs can be integrated into different MOEA frameworks. For example, multi-population or multi-stage constrained MOEAs may employ a combination of these techniques to effectively solve complex CMOPs. By leveraging these advanced CHTs, MOEAs have become increasingly capable of addressing various real-world optimization problems with diverse constraints. In CMOP/BC, however, the binary nature of constraint outcomes complicates the accurate quantification of constraint violations (CV) [7]. This limitation may lead to a decline in the performance of many otherwise effective CHTs and CMOEAs. For instance, in CMOP/BC problems, \varepsilon-based methods struggle to scale the feasible region, resulting in significant performance drops. It is also worth noting that research on CMOP/BC is currently limited, with few practical algorithms available. To promote further research in this area, this paper proposes a constrained multiobjective evolutionary algorithm framework based on the detection region method (DRMCMO). The main contributions of DRMCMO are as follows: [] 1. Proposing the Detection Region Method (DRM): We introduce a detection region method to address the challenges of solving constrained multiobjective optimization problems with binary constraints (CMOP/BC), particularly the limitations of \varepsilon-based methods. This method dynamically relaxes the selection pressure on feasible solutions and monitors promising feasible solutions to assist the population in escaping local optima. 2. Adapting Existing CMOP Test Problems: In light of the limited research on CMOP/BC, we have adapted three existing CMOP benchmark test suites for conducting comparative experiments. 3. Outstanding Experimental Performance: Our experimental results highlight the competitiveness of the proposed algorithm, showcasing its effectiveness in addressing CMOP/BC challenges. The remainder of this paper is organized as follows. Section 2 presents the foundational concepts relevant to this study and reviews current constrained multiobjective evolutionary algorithms (CMOEAs). Section 3 details the proposed algorithm and its components. Sections 4 and 5 outline the experimental settings and analyze the results of a series of experiments, respectively. Finally, Section 6 concludes the paper and discusses potential directions for future work."
https://arxiv.org/html/2411.08674v2,Reducing ADC Front-end Costs During Training of On-sensor Printed Multilayer Perceptrons,"Printed electronics technology offers a cost-effective and fully-customizable solution to computational needs beyond the capabilities of traditional silicon technologies, offering advantages such as on-demand manufacturing and conformal, low-cost hardware. However, the low-resolution fabrication of printed electronics, which results in large feature sizes, poses a challenge for integrating complex designs like those of machine learning (ML) classification systems. Current literature optimizes only the Multilayer Perceptron (MLP) circuit within the classification system, while the cost of analog-to-digital converters (ADCs) is overlooked. Printed applications frequently require on-sensor processing, yet while the digital classifier has been extensively optimized, the analog-to-digital interfacing, specifically the ADCs, dominates the total area and energy consumption. In this work, we target digital printed MLP classifiers and we propose the design of customized ADCs per MLP’s input which involves minimizing the distinct represented numbers for each input, simplifying thus the ADC’s circuitry. Incorporating this ADC optimization in the MLP training, enables eliminating ADC levels and the respective comparators, while still maintaining high classification accuracy. Our approach achieves 11.2x lower ADC area for less than 5% accuracy drop across varying MLPs.","Recently, there has been a growing trend fueled by the fourth industrial revolution and the Internet of Things to integrate intelligence into everyday items. Applications like wearables, fast-moving consumer goods, basic healthcare devices, and disposable sensors for pharmaceuticals have not yet, fully incorporate computing capabilities [1]. These products need computing technology that is ultra-low cost, thin, and conformal. Traditional lithography-based CMOS technologies can’t meet these requirements, limiting computing’s reach [2]. Printed electronics (PE) offer a promising solution with on-demand, ultra-low cost fabrication, ideal for short-lifetime, disposable products. PE uses various printing methods like jet, screen, or gravure printing [2]. These techniques are mask-less, portable, and additive, reducing manufacturing costs and production times [2]. The simplicity of additive manufacturing allow ultra low-cost, at sub-cent levels, electronic circuits. However, this low precision fabrication, results in higher device latency and lower integration density compared to silicon VLSI systems [2], making the design of more complex circuits a challenge in PE. Nevertheless, the target applications are viable in printed electronics due to their relaxed frequency and precision requirements [2]. We focus on Electrolyte-Gated FET (EGFET) technology, which has low supply voltage (\leq 1$\mathrm{V}$), making it suitable for battery-powered applications [1]. Figure 1: Area and Power Evaluation of the printed classification system in [3]. PE’s target applications require smart sensor processing, which starts with analog frontend to digitize analog sensor data using ADCs, followed by machine learning classifiers, such as Multilayer Perceptrons (MLPs) [2]. However the complexity of these classification system oppose a challenge to their realization in PE due to their large gate count. To mitigate this limitation, exploiting the high customization capabilities of the PE technology by bespoke implementations in which the hardware is tailored to a specific dataset and model were proposed [4]. In [3, 5, 6, 7], the authors combined the bespoke architecture alongside the well established Approximation Computing paradigm where a small, accuracy loss resulted in significant area and power gains of the MLP classifier. Nevertheless, all the previous works focused only on the reduction of the MLP classifier inside the overall classification system, neglecting the area and power consumption of the Analog-to-Digital Converts (ADCs). In Figure 1, an area and power analysis within the classification system is presented, by using the MLP in [3]. As shown in Figure1, since the MLP classifier is optimized using approximate bespoke mapping, the ADC ratio in the classification system consuming on average 58% area and 74% power of the entire classification system, making the ADCs the dominant source of area and power overhead in the classification system. Inspired by the fact that different sensor data have different distributions in a given range (e.g., 4bits), not all the representations are required and thus high accuracy can be achieved albeit discard them. Leveraging the above, in this work, we propose the design of bespoke pruned ADCs for each input with the minimum possible representations required saving thus hardware by removing the circuitry of the unused input representation. In more details, we use a Genetic Algorithm (GA) to explore which representations of the ADC can be pruned alongside a quantization-aware training (QAT) of the MLP. Further, our ADC-optimization is orthogonal to any other training approach. In the literature some ADC optimization have been proposed in other technologies rather than in printed electronics. In [8], a spike-based scheme avoids ADCs by a comparator-register architecture but both of these components are hardware expensive in printed electronics[4]. In [9], prune crossbars to eliminate ADCs in ReRAM architectures, which in our architectures is equivalent to feature reduction. Our approach goes further by optimizing the remaining ADCs. To the best of our knowledge, this is the first time that such a framework111https://github.com/floAfentaki/Approximation-Techniques-Targeting-Printed-MLPs is proposed for ADC-efficient printed MLP-based classification systems. Our experiments across various datasets showcase that our framework reduces both area and power of the required ADCs on average 11.2\times and 13.2\times respectively."
https://arxiv.org/html/2411.07860v1,Integrating Chaotic Evolutionary and Local Search Techniques in Decision Space for Enhanced Evolutionary Multi-Objective Optimization,"This paper presents innovative approaches to optimization problems, focusing on both Single-Objective Multi-Modal Optimization (SOMMOP) and Multi-Objective Optimization (MOO). In SOMMOP, we integrate chaotic evolution with niching techniques, as well as Persistence-Based Clustering combined with Gaussian mutation. The proposed algorithms, Chaotic Evolution with Deterministic Crowding (CEDC) and Chaotic Evolution with Clustering Algorithm (CECA), utilize chaotic dynamics to enhance population diversity and improve search efficiency. For MOO, we extend these methods into a comprehensive framework that incorporates Uncertainty-Based Selection, Adaptive Parameter Tuning, and introduces a radius R concept in deterministic crowding, which enables clearer and more precise separation of populations at peak points. Experimental results demonstrate that the proposed algorithms outperform traditional methods, achieving superior optimization accuracy and robustness across a variety of benchmark functions.","Optimization problems, both single-objective and multi-objective, are pivotal in various scientific and engineering fields, often demanding robust and adaptable solutions to navigate complex, high-dimensional landscapes. However, traditional optimization algorithms frequently face significant challenges, especially in multi-modal and multi-objective scenarios. In the context of single-objective multi-modal optimization (SOMMOP), existing algorithms often fall prey to premature convergence, where the search process becomes trapped in local optima, thus failing to uncover superior global solutions [1]. This issue is particularly acute in multi-modal landscapes, where multiple peaks exist, and traditional methods(such as Genetic Algorithm(GA)) struggle to maintain sufficient diversity, resulting in inadequate exploration of the search space [2, 3]. Achieving a balance between global exploration and local exploitation remains elusive, often leading to slower convergence rates and suboptimal solutions [4]. To address these challenges in SOMMOP, we propose two novel algorithms: Chaotic Evolution with Deterministic Crowding (CEDC) [5] and Chaotic Evolution with Clustering Algorithm (CECA) [6]. Both algorithms capitalize on the inherent advantages of chaotic evolution, which introduces local search capability into the chaotic evolution [7], thereby enhancing the algorithm’s ability to thoroughly explore the search space and avoid local optima [8]. Chaotic evolution uses the ergodic nature of chaos to ensure a diverse and extensive sampling of potential solutions, which is crucial for preventing premature convergence and maintaining population diversity [9]. In CEDC, we integrate chaotic evolution with deterministic crowding - a method that preserves multiple optimal solutions by ensuring that similar individuals compete within subpopulations [10]. This approach not only maintains diversity but also enhances the robustness of the optimization process, effectively addressing the issue of premature convergence. Meanwhile, CECA combines chaotic evolution with persistence-based clustering and Gaussian mutation. Persistence-based clustering identifies and preserves stable and significant structures within the population, aiding in the discovery and maintenance of multiple optima [11]. Gaussian mutation further refines solutions within promising regions, ensures precise local search, and enhances the extensive exploration capability of the chaotic evolutionary algorithm [12]. Our experimental results, validated on the CEC2013 benchmark functions, demonstrate the superior performance of these algorithms, particularly in terms of achieving higher peak ratios and maintaining a well-distributed set of solutions [13]. The success of CEDC and CECA in single-objective optimization motivated us to extend these methods to the domain of multi-objective optimization (MOO). Figure 1: Comparison between Decision Space and Objective Space for ZDT3 (Multimodal). There is a conflicting relationship between the optimization of different objectives in multi-objective optimization problems (MOO) [14]. Unlike single objective optimization problems, which have only one optimal solution, multi-objective optimization problems have a set of equilibrium solutions as their optimal solution. In multi-objective optimization problems, the challenges of balancing exploration and exploitation, maintaining diversity, and ensuring scalability become even more pronounced due to the complexity of optimizing multiple conflicting objectives simultaneously [1]. Various advancements have been made to tackle these challenges in MOO. For instance, the Strength Pareto Evolutionary Algorithm (SPEA2) [15] introduced a fine-grained fitness assignment and a clustering technique to maintain diversity. The Hypervolume-based algorithms [16] have also gained popularity, focusing on maximizing the volume dominated by the Pareto front, which directly influences the selection process. Innovations like the Multi-objective Evolutionary Algorithm based on Decomposition (MOEA/D) [17], which decomposes a multi-objective problem into a number of scalar optimization subproblems, have significantly impacted the field. Additionally, Deb et al.’s work on NSGA-III [18] has introduced reference point-based non-dominated sorting, enhancing the algorithm’s performance in handling problems with many objectives. The \epsilon-MOEA [19] focuses on using an adaptive grid-based selection to ensure a well-distributed Pareto front, which has also contributed to the ongoing improvement of MOO algorithms. Moreover, recent developments in hybrid approaches, such as the integration of evolutionary strategies with machine learning techniques [20], and adaptive multi-objective optimization [21], demonstrate the potential for further enhancing optimization efficiency and robustness. These advancements underscore the importance of continuously improving MOO algorithms to handle increasingly complex and high-dimensional problems effectively. Although the Non-dominated Sorting Genetic Algorithm II (NSGA-II) [22] is one of the most widely adopted methods for MOO, it still grapples with issues such as maintaining diversity and avoiding premature convergence, especially in high-dimensional objective spaces [17]. To address these issues, we have enhanced NSGA-II by integrating the methods developed for SOMMOP. Specifically, we have refined the deterministic crowding method [23] by introducing a radius R parameter, which allows for clearer and more precise separation of populations at peak points, thereby enhancing the algorithm’s search performance. Additionally, our approach introduces innovations not only in the objective space but also in the decision space, where fewer methods typically focus. The decision space represents the set of possible solutions, while the objective space reflects the evaluated outcomes of these solutions. Figure 1 illustrates this distinction using the ZDT3 test function. By employing clustering techniques within the decision space, we group solutions based on their decision variables rather than their objective values. This approach facilitates a more organized and effective exploration of the solution landscape, which is especially beneficial in multimodal scenarios like ZDT3, where diverse decision regions can lead to similar optimal solutions in the objective space. This paper introduces these advanced algorithms for both SOMMOP and MOO. Our methods harness the strengths of each strategy, creating a synergistic effect that significantly improves overall performance. We validate the effectiveness of our approaches using major multi-objective benchmarks: ZDT, DTLZ and WFG [24, 25, 26]. Experimental results demonstrate the superior performance of these algorithms compared to traditional methods, highlighting their potential for addressing complex optimization problems across various domains. The main contributions of this paper can be summarized as follows: 1. Development of CEDC and CECA for Single-Objective Multi-Modal Optimization (SOMMOP): Two novel evolutionary computation methods, Chaotic Evolution with Deterministic Crowding (CEDC) and Chaotic Evolution with Clustering Algorithm (CECA), are introduced for tackling SOMMOP. These methods are designed to enhance exploration and maintain diversity by integrating chaotic dynamics and clustering mechanisms. We applied these methods to the CEC benchmark functions, using peak ratio as the evaluation metric. The results demonstrated superior performance compared to traditional single-objective optimization techniques. 2. Integration into NSGA-II for Multi-Objective Optimization (MOO): The techniques developed for SOMMOP are adapted and integrated into the Non-dominated Sorting Genetic Algorithm II (NSGA-II). In addition, we introduce clustering strategies within the decision space to improve search efficiency and algorithm performance. This approach focuses on organizing the search process by grouping similar solutions, thereby enhancing convergence and diversity. The modified algorithm is tested on ZDT and other benchmark functions, and its performance is evaluated using Hypervolume (HV) and Inverted Generational Distance (IGD) metrics. The results show that our approach significantly outperforms the original NSGA-II in maintaining a well-distributed Pareto front. These innovations contribute to a more efficient optimization process, particularly in complex, high-dimensional landscapes. By incorporating chaotic evolution and clustering, the proposed methods address common challenges such as premature convergence and the balance between exploration and exploitation. Structure of the Paper: Chapter I introduces the overarching goals of this study, which focuses on the design and validation of innovative optimization algorithms for both single-objective and multi-objective problems. This chapter not only highlights our key contributions but also provides an outline of the paper’s structure. Chapter II offers a detailed review of the current methodologies in both single-objective and multi-objective optimization, emphasizing their existing limitations and the constraints they impose. Building on this foundation, Chapter III presents our proposed methods, including the Chaotic Evolution with Deterministic Crowding (CEDC) and Chaotic Evolution with Clustering Algorithm (CECA) for single-objective optimization, alongside the Clustering-Enhanced NSGA-II with Chaotic Evolution (CEC-NSGAII) for multi-objective optimization. This chapter meticulously details the algorithms and techniques employed. In Chapter IV, we demonstrate the efficacy of our approaches through rigorous experimentation, where we discuss the evaluation metrics, test functions, and present our results under conditions that ensure a fair comparison with existing algorithms. Subsequently, Chapter V provides an in-depth analysis of the experimental results outlined in Chapter IV, shedding light on the performance and robustness of our methods. Finally, Chapter VI synthesizes the key takeaways of the paper, summarizing our contributions and reflecting on the significance of our advancements in the field of optimization."
https://arxiv.org/html/2411.07397v1,Spiking Transformer Hardware Accelerators in 3D Integration,"Spiking neural networks (SNNs) are powerful models of spatiotemporal computation and are well suited for deployment on resource-constrained edge devices and neuromorphic hardware due to their low power consumption. Leveraging attention mechanisms similar to those found in their artificial neural network counterparts, recently emerged spiking transformers have showcased promising performance and efficiency by capitalizing on the binary nature of spiking operations. Recognizing the current lack of dedicated hardware support for spiking transformers, this paper presents the first work on 3D spiking transformer hardware architecture and design methodology. We present an architecture and physical design co-optimization approach tailored specifically for spiking transformers. Through memory-on-logic and logic-on-logic stacking enabled by 3D integration, we demonstrate significant energy and delay improvements compared to conventional 2D CMOS integration.","Transformer models have significantly advanced model capabilities in language modeling and computer vision, and have found widespread adoption across various application domains (Dosovitskiy et al., 2021; Ramesh et al., 2021). At the heart of these models lies a self-attention mechanism, which captures rich contextual information by considering all elements in a long input sequence, blending global and local sequence details into a unified representation. Spiking neural networks (SNNs) are more biologically plausible than their non-spiking artificial neural network (ANN) counterparts (Gerstner and Kistler, 2002). Notably, SNNs can harness powerful temporal coding, facilitate spatiotemporal computation based on binary activations, and achieve ultra-low energy dissipation on dedicated neuromorphic hardware (Akopyan et al., 2015; Davies et al., 2018; Lee et al., 2022). Recent spiking transformers showcased promising performance and efficiency by capitalizing on the binary nature of spiking activation (Zhou et al., 2023; Zhang et al., 2022; Zhu et al., 2023; Yao et al., 2023). However, there is a current lack of dedicated hardware architectures for spiking transformers (Zhou et al., 2023; Zhang et al., 2022; Zhu et al., 2023; Yao et al., 2023). Our goal is to fill this gap by developing optimized architectures capable of accelerating spatiotemporal spiking workloads for spiking transformers using 3D integration as a technology enabler. We see many opportunities that 3D integration can offer to enable biologically-inspired spiking transformers. Firstly, memory-on-logic stacking capability in 3D configurations allows for the storage of a significant portion of model parameters within local memory, ensuring swift and parallel memory access. Secondly, logic-on-logic stacking in 3D opens avenues for significant enhancements in energy efficiency, particularly in spike delivery management within SNN architecture. Ultimately, in a longer run, the ultra-dense neuron-to-neuron connectivity enabled by 3D integration promises improvements in SNN learning accuracy and efficiency, thereby propelling semiconductor chip emulation closer to the capabilities of the human brain. Challenges and Contributions In this work, we adopt face-to-face(F2F)-bonded 3D integration technology to enable dedicated spiking transformer accelerators with memory-on-logic and logic-on-logic configurations. Contribution 1: We propose the first dedicated 3D accelerator architecture for spiking transformers, which explore spatial and temporal weight reuse to support spike-based computation in transformer models . Contribution 2: We enable the first 3D memory-on-logic and logic-on-logic interconnection schemes to significantly minimize energy consumption and latency, whereby delivering highly-efficient spiking neural computing systems with low area overhead. Compared to 2D CMOS integration, the 3D accelerator offers substantial improvements. For the spiking MLP workload, it provides a 7.0% increase in effective frequency, 50% area reduction, and reductions of 7.8% in power consumption, 68.3% in memory access latency, and 69.5% in memory access power. For the spiking self-attention workload, the enhancements include a 6.3% increase in effective frequency, 50% area reduction, and reductions of 1.5% in power consumption, 74.2% in memory access latency, and 49.3% in memory access power. $*$$*$footnotetext: Corresponding author."
https://arxiv.org/html/2411.07873v1,Diverse capability and scaling of diffusion and auto-regressive models when learning abstract rules,"Humans excel at discovering regular structures from limited samples and applying inferred rules to novel settings. We investigate whether modern generative AI systems can similarly learn underlying rules from finite samples and perform reasoning through conditional sampling. Inspired by Raven’s Progressive Matrices task, we designed GenRAVEN dataset, where each sample consists of three rows, and one of the 40 relational rules governing the object position, number, or attributes applies to all three rows. We trained generative models to learn the data distribution, where samples are encoded as 3×9×9 integer arrays to focus on rule learning. We compared two major families of generative models: diffusion models (EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their ability to generate structurally consistent samples and perform panel completion via unconditional and conditional sampling. We found that diffusion models excel at unconditional generation, producing novel and more consistent samples from scratch and memorize less, but perform less well in panel completion, even with advanced conditional sampling methods like Twisted Diffusion Sampler. Conversely, autoregressive models excel at completing missing panels in a rule-consistent manner but generate less consistent samples unconditionally. We observe diverse data scaling behaviors: for both model families, rule learning emerges at a certain dataset size – around thousands examples per rule. With more training data, diffusion models improve both their unconditional and conditional generation capabilities. However, for autoregressive models, while panel completion improves with more training data, unconditional generation consistency declines. Our findings highlight complementary capabilities and limitations of diffusion and autoregressive models in rule learning and reasoning tasks, suggesting avenues for further research into their mechanisms and potential for human-like reasoning.","1 Background Figure 1: Design of the study A. Example Raven’s progression matrix, and its encoding as a 3\times9\times9 integer array. The underlying rule is constant shape. B.C. Two families of generative models: Diffusion and autoregressive model, and their training method: denoising and predicting the next token. D. The 40 relational rules, with 5 rules held out during training. Human excels at discovering regular structure from a small number of samples, and they can further apply such rule to novel settings to generate new samples or complete missing parts based on the same rule. The Raven’s progressive matrix (RPM) [27] is a famous task in human reasoning literature. In the generative version of this task (GenRAVEN), the subject observes two complete rows of panels and is tasked to complete the third row in a manner that is consistent with the first two rows (Fig.1A). Ideally, the subjects need to infer the underlying rule consistent with the first two rows and apply it correctly to the third row. How can we train a general learning system to solve such reasoning task? If we conceptualize all rule-conforming samples as a joint distribution, then rule learning can be framed as a generative modeling problem or learning the correct joint. Further, reasoning about the missing panel can be framed as sampling from the conditional probability [25]. One conceptual problem is, that given finite training samples, the rule governing them, or the ‘true‘ joint distribution is under-specified. The rules or the distribution learned by the system should be affected by its inductive bias, be it human or AI. Given this ambiguity, we asked whether modern generative AI systems could learn the correct ""joint distribution"" given finite samples. If so, can they reason and fill in the missing parts in a sample through conditional sampling? In the current age of Generative AI, there are two prominent families of generative models: autoregressive models and diffusion models. The autoregressive model generally dominates discrete sequence data, such as language, music, genomics [4, 14, 13], while the diffusion model excels at continuous data, such as image, video, audio, molecule structure, robot trajectories [28, 34, 29, 5, 3]. Both of them are capable of unconditional generation and conditional sampling based on partial observation: e.g. prompting for autoregressive model or inpainting for diffusion models. Given their similar capability, it’s interesting to compare them back-to-back on the same reasoning task and see how their different modeling method might lead to different learning and scaling behaviors. In this work, we trained a diverse set of generative models from both the diffusion family (EDM, DiT, SiT) and autoregressive family (GPT, Mamba) to learn the data distribution and then use conditional sampling techniques to perform inference, i.e., reasoning about the missing panel. We studied their performance as a function of data scale and model scale. We found that generally, diffusion models excel at unconditional sampling from the joint, creating structurally consistent samples from scratch, but perform less well for sampling conditioned on given panels. Conversely, autoregressive models excel at completing missing panels in a rule-consistent manner, but they perform less well in generating consistent samples from scratch with unconditional sampling. They also exhibit diverse scaling behavior when the size of datasets is varied. Our results call for further investigation into the seemingly complementary capabilities and limitations of diffusion and autoregressive models."
https://arxiv.org/html/2411.07654v1,Spike Talk in Power Electronic Grids,"Emerging distributed generation demands highly reliable and resilient coordinating control in microgrids. To improve on these aspects, spiking neural network is leveraged, as a grid-edge intelligence tool to establish a talkative infrastructure, Spike Talk, expediting coordination in next-generation microgrids without the need of communication at all. This paper unravels the physics behind Spike Talk from the perspective of its distributed infrastructure, which aims to address the Von Neumann Bottleneck. Relying on inferring information via power flows in tie lines, Spike Talk allows adaptive and flexible control and coordination itself, and features in synaptic plasticity facilitating online and local training functionality. Preliminary case studies are demonstrated with results, while more extensive validations are to be included as future scopes of work.","The energy consumption of data centers has become a major concern in modern society. As the distributed energy resources (DERs) are increasingly promoted, the carbon footprint is also becoming more critical in power grids where large amount of data are involved [1]. Meanwhile, distributed generation is escalating the demand for coordinating control in cyber-physical microgrids to ensure operational reliability. In turn, challenges persist thereupon, involving delays [2] and susceptibility to cyberattacks [3]. It is therefore of much value to study on a decentralized transition of the operation paradigm to address both challenges. Under this scenario, Talkative Power has been accordingly developed, aiming to co-transfer power and information along transmission lines [4]. System resilience is effectively improved, while additional energy consumption on the transmission line is inevitable. besides, as Talkative Power relies on request-respond information exchange protocol, its scalability is limited when multiple agents are involved in information exchange simultaneously. In contrast, we delve into the realm of a publish-subscribe protocol, where information is fetched locally as needed. Navigated by the biologically plausible neuron model [5, 6], spiking neural network (SNN) has emerged with great advantage in energy efficient computation due to its event-driven feature. Beyond the von-Neumann computing architecture activated by real numbers and perceptrons, SNN leverages a leaky-charge framework instead that are triggered by asynchronous spikes. Empowered by SNN, we formalize the Spike Talk tailored for microgrids, harnessing power flow dynamics to infer remote information locally. As spiking neurons and spiking neural networks have the features of synaptic plasticity and spike-timing-dependent plasticity (STDP), the neuromorphic infrastructure also shows potential in online learning and effectively reduces the data and energy requirements for training. Furthermore, the necessity for communication channels is eliminated, thus effectively addressing the resilience challenges in microgrids. This article thus delineates the infrastructure of Spike Talk and explains it from the perspective of its decentralized and online learning features. The remaining parts of this article is organized as follows: Section II introduces the inspiration of neuromorphic infrastructure for power grids from the von Neumann bottleneck. Section III elaborates on the online learning potential of Spike Talk by investigating the training principles. Section IV presents a case study, and Section V concludes the entire article. By discussing its inherent advantages, more promising real-world applications are implied, which should be the future scope of our work."
https://arxiv.org/html/2411.07634v1,Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel Machine Scheduling,"Scheduling problems pose significant challenges in resource, industry, and operational management. This paper addresses the Unrelated Parallel Machine Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent Reinforcement Learning (MARL) approach. The study introduces the Reinforcement Learning environment and conducts empirical analyses, comparing MARL with Single-Agent algorithms. The experiments employ various deep neural network policies for single- and Multi-Agent approaches. Results demonstrate the efficacy of the Maskable extension of the Proximal Policy Optimization (PPO) algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced scenarios, Multi-Agent approaches reveal challenges in cooperative learning but a scalable capacity. This research contributes insights into applying MARL techniques to scheduling optimization, emphasizing the need for algorithmic sophistication balanced with scalability for intelligent scheduling solutions.","Scheduling problems constitute a subset of optimization challenges that find widespread applications across various sectors, encompassing resource management [1], industry [2, 3], and operational management [4]. In particular, production scheduling, an essential facet of manufacturing, revolves around the efficient and cost-effective allocation of limited resources to support production processes. In this context, implementing flexible and intelligent strategies for industrial scheduling emerges as an imperative. The primary objective of scheduling problem optimization is to identify an advantageous combination of decision variables within a defined search space. These decision variables dictate the order in which processes or tasks are assigned to a set of machines, typically entailing complex combinatorial problems. These problems often involve optimizing multiple objectives, depending on the system demands [5]. In industrial settings, the overall aim predominantly implies minimizing job completion times while accommodating other objectives, such as resource utilization or environmental considerations. It is important to note that real-world industrial scheduling problems imply exploring extensive search spaces, usually culminating in NP-hard problem instances [6]. This inherent intricacy poses complex challenges to the research and development community. This characteristic has attracted considerable attention from the research community, as exemplified in the comprehensive survey by Allahverdi et al. [7]. Notwithstanding the advancements in this field, current approaches have notable limitations [8], encompassing computational complexity [9] and the capacity to generalize and adapt to diverse problem instances [10, 11]. In response to these constraints, contemporary research has increasingly employed advanced decision-making techniques. Deep Learning techniques have obtained significant attention in intelligent decision-making systems within the research community [12]. Specifically, Reinforcement Learning (RL) approaches have emerged as valuable solutions to addressing scheduling in complex environments [13]. This machine learning paradigm relies on utilizing an intelligent agent that learns from the actions it takes and the rewards it receives in response to those actions. Reinforcement Learning models exhibit adaptability to non-deterministic environments, making it a flexible approach for optimization within dynamic and uncertain environments. This paper presents a study of a Multi-Agent Reinforcement Learning (MARL) approach to addressing an optimal job scheduling problem. The research is performed in the Unrelated Parallel Machine scheduling problem (UPMS) with setup times and resources proposed by Fanjul et al. [14], a single-stage job scheduling problem variant. This research presents the novelty of employing a MARL approach that extends beyond traditional methods. In this sense, the work reviews the RL environment deployed in the study and conducts empirical analyses, comparing the MARL approach with various Single-Agent algorithms. The contribution aims to provide a practical evaluation of the efficacy of addressing complex decision-making problems like UPMS. The remainder of the article is structured as follows: Section 2 provides an overview of reinforcement learning on scheduling problems. Section 3 formally describes the problem addressed in the paper. Section 4 describes the implementation details. Section 5 introduces the proposed approach and implementation insights. Section 6 presents the evaluation and validation process conducted in the experiments. Finally, section 7 contains the concluding remarks and outlines some areas for further research."
https://arxiv.org/html/2411.07243v1,Neuropsychology and Explainability of AI: A Distributional Approach to the Relationship Between Activation & Similarity of Neural Categories in Synthetic Cognition,"We propose a neuropsychological approach to the explainability of artificial neural networks, which involves using concepts from human cognitive psychology as relevant heuristic references for developing synthetic explanatory frameworks that align with human modes of thought. The analogical concepts mobilized here, which are intended to create such an epistemological bridge, are those of categorization and similarity, as these notions are particularly suited to the categorical ”nature” of the reconstructive information processing performed by artificial neural networks. Our study aims to reveal a unique process of synthetic cognition, that of the categorical convergence of highly activated tokens. We attempt to explain this process with the idea that the categorical segment created by a neuron is actually the result of a superposition of categorical sub-dimensions within its input vector space.","Within an explainability framework, the neuropsychology of artificial intelligence focuses on studying synthetic neural cognitive mechanisms, considering them as new subjects of cognitive psychology research. The goal is to make artificial neural networks used in language models understandable by adapting concepts from human cognitive psychology to the interpretation of artificial neural cognition. In this context, the notion of categorization is particularly relevant because it plays a key role as a process of segmentation and reconstruction of informational data by the neural vectors of synthetic cognition. Thus, in this study, the aim is to use the concept of categorization, as understood in human cognitive psychology (particularly in its relation to the notion of similarity), to apply it to the analysis of neural behavior and to infer certain synthetic cognitive processes underlying the observed behaviors."
https://arxiv.org/html/2411.07057v1,"Randomized Forward Mode Gradient
for Spiking Neural Networks in Scientific Machine Learning","Spiking neural networks (SNNs) represent a promising approach in machine learning, combining the hierarchical learning capabilities of deep neural networks with the energy efficiency of spike-based computations. Traditional end-to-end training of SNNs is often based on back-propagation, where weight updates are derived from gradients computed through the chain rule. However, this method encounters challenges due to its limited biological plausibility and inefficiencies on neuromorphic hardware. In this study, we introduce an alternative training approach for SNNs. Instead of using back-propagation, we leverage weight perturbation methods within a forward-mode gradient framework. Specifically, we perturb the weight matrix with a small noise term and estimate gradients by observing the changes in the network output. Experimental results on regression tasks, including solving various PDEs, show that our approach achieves competitive accuracy, suggesting its suitability for neuromorphic systems and potential hardware compatibility.","Recent advances in machine learning have greatly expanded the capabilities of artificial intelligence (AI) for applications in solving differential equations, function approximation, and various other fields [1, 2, 3, 4, 5]. As demand grows for energy-efficient and computationally feasible solutions, spiking neural networks (SNNs) have attracted significant attention within the machine learning community. Unlike traditional artificial neural networks (ANNs), which rely on continuous activation functions, SNNs operate with sparse, binary spiking events, making them more efficient in terms of energy consumption [6, 7, 8, 9, 10]. Furthermore, SNNs benefit from recent advancements in neuromorphic hardware (e.g., Intel’s Loihi 2 chip [11, 12]), which enables SNNs to approximate brain-like computations, hence facilitating lightweight faster models. Despite these benefits, most deep neural networks still rely on back-propagation for training. This method, however, is considered “biologically implausible” as it lacks symmetry with the way biological neural systems learn, does not engage massive parallelism, and is often incompatible with neuromorphic hardware. This incongruity underscores the need for novel learning algorithms better suited to SNNs. Weight perturbation offers a promising alternative, where small perturbations are applied to the synaptic connections during the forward pass, and weight updates are adjusted in response to changes in the loss function. Instead of perturbing weights directly, forward-mode automatic differentiation (AD) can be employed to compute a directional gradient along the perturbation direction [13]. Forward-mode AD has seen a resurgence in deep learning applications [14]. In this work, we leverage weight perturbation to train SNNs, aiming for greater biological plausibility. We explore two different methods for determining surrogate gradients and implementing perturbations. Evaluating these methods on regression tasks that are more changeling [15, 16, 17], particularly relevant to scientific machine learning (SciML), our results indicate the viability of this approach, positioning it as a step towards realizing SNNs on neuromorphic hardware [18] without reliance on back-propagation. The paper is organized as follows. In section 2, we review related works, and in section 3 we describe the methodology. In section 4 we present our results, and we conclude in section 5 with a summary."
https://arxiv.org/html/2411.06792v1,Evolving Efficient Genetic Encoding for Deep Spiking Neural Networks,"By exploiting discrete signal processing and simulating brain neuron communication, Spiking Neural Networks (SNNs) offer a low-energy alternative to Artificial Neural Networks (ANNs). However, existing SNN models, still face high computational costs due to the numerous time steps as well as network depth and scale. The tens of billions of neurons and trillions of synapses in the human brain are developed from only 20,000 genes, which inspires us to design an efficient genetic encoding strategy that dynamic evolves to regulate large-scale deep SNNs at low cost. Therefore, we first propose a genetically scaled SNN encoding scheme that incorporates globally shared genetic interactions to indirectly optimize neuronal encoding instead of weight, which obviously brings about reductions in parameters and energy consumption. Then, a spatio-temporal evolutionary framework is designed to optimize the inherently initial wiring rules. Two dynamic regularization operators in the fitness function evolve the neuronal encoding to a suitable distribution and enhance information quality of the genetic interaction respectively, substantially accelerating evolutionary speed and improving efficiency. Experiments show that our approach compresses parameters by approximately 50% to 80%, while outperforming models on the same architectures by 0.21% to 4.38% on CIFAR-10, CIFAR-100 and ImageNet. In summary, the consistent trends of the proposed genetically encoded spatio-temporal evolution across different datasets and architectures highlight its significant enhancements in terms of efficiency, broad scalability and robustness, demonstrating the advantages of the brain-inspired evolutionary genetic coding for SNN optimization.","Artificial neural networks have provided important insights into numerous application areas [1, 2], but the large number of matrix operations increases significantly with the size of the network. Spiking Neural Networks [3], as the third generation of neural networks, achieve a low-energy computing paradigm by simulating the communication characteristics of brain neurons and leveraging the inherent energy efficiency advantages of discrete signal processing and are more biologically plausible. Most of the research on SNN optimization focuses on the training mechanism, which can be roughly divided into plasticity-based training [4], conversion-based training [5], and gradient-based training [6], which has become the most mainstream method due to its high efficiency. However, a large number of time steps, proxy gradient calculations, and increasingly deeper network architecture designs also make SNNs increasingly computationally expensive. Common techniques for optimizing computational overhead include network pruning and quantization [7], which greatly reduce storage and computational costs by pruning redundant connections [8] and reducing operations [9]. Despite offering numerous insights and successful methodologies for training on complex tasks, current SNN models still lack effective integration with brain-inspired mechanisms to achieve a balance between cost and performance. Generation after generation, environmental pressures drive biological neural systems to evolve specific responses to complex tasks, with neural connections continually adapting to encode crucial information into the connectome [10]. Evolution has endowed approximately 21,000 genes with the ability to support the complex computing capabilities of the brain’s 10^{10} neurons and 10^{15} synapses. This compact and efficient encoding method not only saves biological energy, but also facilitates genetic optimization during evolution, thereby supporting complex cognitive functions and highly flexible behavioral adaptability. This inspired us to design a gene-scaled neuronal coding paradigm for SNNs, controlling the entire network with very few parameters, and simulate the evolutionary process of the brain to optimize the genetic encoding. Based on this, this paper implements genetically encoded neural networks on a variety of common network architectures, re-encoding weights with neuronal encoding each layer and global shared gene interaction matrices, greatly compressing parameters and energy consumption. Furthermore, to accelerate the evolution and training, we propose a spatio-temporal evolutionary SNN framework. Spatially, the initial wiring of neuronal encodings and gene interaction matrix are optimized through the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Temporally, the dynamic regularization scale helps solutions transition from freely learning complex features to gradually establishing stable functional patterns, ultimately achieving a significant reduction in energy consumption without compromising performance. By integrating principles of neuroevolution, the proposed method develops robust, efficient architectures capable of performing complex tasks at very low computational cost, reflecting the evolutionary encoding of behaviors in the brain’s connectome. In general, the contributions of this work can be summarized as follows: 1) We develop a Genetically Encoded Evolutionary (GEE) spiking neural network that improves the performance by learning neuronal encoding rather than directly updating weights. The controllable genetic scale greatly reducing the number of parameters and computational cost without losing accuracy. 2) To improve the quality of solutions, we propose a Spatio-Temporal dynamical Evolution (STE) framework for initial wiring of neuronal encoding and gene interactions. Two dynamic regularization operators, spatial entropy and temporal difference regularization, help improve the evolution efficiency and greatly reduce the cost. 3) On CIFAR10, CIFAR100 and ImageNet datasets, the proposed GEE achieves superior performance with significantly lower energy consumption, achieving efficient SNN evolution with a brain-inspired efficient computational paradigm."
https://arxiv.org/html/2411.05820v1,Guiding Genetic Programming with Graph Neural Networks,"In evolutionary computation, it is commonly assumed that a search algorithm acquires knowledge about a problem instance by sampling solutions from the search space and evaluating them with a fitness function. This is necessarily inefficient because fitness reveals very little about solutions – yet they contain more information that can be potentially exploited. To address this observation in genetic programming, we propose EvoNUDGE, which uses a graph neural network to elicit additional knowledge from symbolic regression problems. The network is queried on the problem before an evolutionary run to produce a library of subprograms, which is subsequently used to seed the initial population and bias the actions of search operators. In an extensive experiment on a large number of problem instances, EvoNUDGE is shown to significantly outperform multiple baselines, including the conventional tree-based genetic programming and the purely neural variant of the method.","The blueprint of evolutionary algorithms assumes that the fitness function is the only means by which the search method is informed about the characteristics of a given problem instance. This design choice is inspired by natural evolution, where a species cannot improve its adaptations otherwise than by spawning randomly diversified offspring, some of which have the chance of being fitter than others. However, there is no reason to keep imposing this information bottleneck if other sources of informative guidance are available, which is relatively common in the practice of metaheuristic search algorithms. For instance, if a problem instance features constraints, one may seed the initial population with candidate solutions that comply with them; if the distributions of some variables happen to be known in advance, one may design search operators that take those distributions into account. In this study, we aim at eliciting problem-specific knowledge also from the candidate solutions themselves and from how they are being evaluated. As per the No Free Lunch Theorem (Wolpert and Macready, 1997), an optimization algorithm informed about the characteristics of a problem instance cannot perform worse on average than an uninformed algorithm. However, gathering useful knowledge about a problem and turning it into information that is ‘actionable’ for the search policy is difficult in domains where the fitness function depends on solutions in a complex way. One domain with this characteristic is genetic programming (GP), where solutions are programs or other symbolic expressions that reveal their characteristics only once executed. Turning the effects of program execution into search guidance is difficult, but can be realized as a learnable mapping. To this aim, we hybridize the GP heuristics with a bespoke graph neural network (GNN) designed to generate graphs of programs. Given an instance of a GP problem represented as a set of input-output examples, the GNN is queried on it to produce a sample of GP subprograms, which is then used to seed the GP population and bias the search operators. We apply this approach to symbolic regression (SR), but it can be easily generalized to other domains. The main contributions of this study are (i) EvoNUDGE, a neuro-evolutionary method for solving SR problems (Sec. 3) and its experimental assessment on a range of SR benchmarks (Sec. 5). The remaining sections comprise problem formulation (Sec. 2) and the review of related works (Sec. 4)."
https://arxiv.org/html/2411.05816v1,Learning Characteristics of Reverse Quaternion Neural Network,"The purpose of this paper is to propose a new multi-layer feedforward quaternion neural network model architecture, Reverse Quaternion Neural Network which utilizes the non-commutative nature of quaternion products, and to clarify its learning characteristics. While quaternion neural networks have been used in various fields, there has been no research report on the characteristics of multi-layer feedforward quaternion neural networks where weights are applied in the reverse direction. This paper investigates the learning characteristics of the Reverse Quaternion Neural Network from two perspectives: the learning speed and the generalization on rotation. As a result, it is found that the Reverse Quaternion Neural Network has a learning speed comparable to existing models and can obtain a different rotation representation from the existing models.","In recent years, machine learning technology has made tremendous progress with the rise of deep learning. In particular, it has achieved remarkable success in fields such as image recognition, natural language processing, and speech recognition, and is still being actively researched. A high-dimensional neural network [4, 3, 1, 5, 2] is a type of neural network that uses numbers of two or more dimensions, such as complex numbers and quaternions, to represent the parameters of the neural network . High-dimensional neural networks can deal with hypercomplex-valued signals naturally. It is well-known that complex-valued neural networks and quaternion neural networks require fewer parameters (weights, biases) and have several times faster learning speeds than usual real-valued neural networks [8, 9, 6, 7]. Recent studies have actively explored the application of high-dimensional neural networks in areas such as speech recognition and image processing. Zhu et al.[10] have extended the convolutional neural networks (CNN) [11] to quaternions. This approach represents the relationship between RGB colors in an image through the rotation of quaternions in the imaginary parts (i, j, and k axes), and has shown higher accuracy in color image processing compared to real-valued convolutional neural networks. The quaternion convolutional neural network, proposed by Parcollet et al. [12], introduces a model that divides the feature map into individual components of quaternions and convolves them. This approach has demonstrated superior recognition capabilities in speech recognition tasks compared to real-valued CNNs. A quaternion is a mathematical concept introduced by Hamilton [13] in 1843. A quaternion consists of a real part and three imaginary parts (i, j, k), each representing an independent dimension. This characteristic makes quaternions particularly suitable for representing rotations in a three-dimensional space. Furthermore, quaternions possess the property that the commutative law of multiplication does not apply, meaning that the order of multiplication significantly affects the outcome. As described below, neural networks utilizing quaternions exhibit different characteristics compared to real-valued and complex-valued neural networks in this meaning. A quaternion neural network is a neural network model where all parameters are represented as quaternions, enabling the network using quaternions to represent data in higher dimensions, and it particularly excels in representing rotations and orientations in three-dimensional space. By utilizing the rich expressiveness of quaternions, quaternion neural networks can capture spatial data features that traditional neural networks may miss, especially in fields where spatial information is crucial, such as 3D graphics and robotics. In the context of 3D spatial transformations, Matsui et al. [14] conducted an experiment comparing three tasks - scaling, parallel translation, and rotation - using geometric object data with the Quaternion Multi-layer Perceptron (QMLP) and the real-valued MLP (Multi-layer Perceptron). The results showed that while the real-valued MLP failed to learn the transformations, the QMLP successfully learned them and performed perfectly in all three tasks. The existence of two types of quaternion neurons, based on non-commutativity, was pointed out in [7]. Yoshida et al. [15] investigated the existence conditions of the energy functions for the Hopfield-type recurrent quaternion neural network and the one with weights applied in reverse order . As a result, they clarified that there is no difference between the usual Hopfield-type recurrent quaternion neural network model and the quaternion neural network with weights applied in reverse order. In this paper, we propose a new feedforward quaternion neural network architecture, Reverse Quaternion Neural Networks (called RQNN for short here), which utilizes the non-commutative property of quaternions. In the model, by altering the order of quaternion multiplication, it is possible to construct neural networks with different characteristics. This approach is expected to capture information and offer a different expressive power that traditional quaternion neural networks cannot achieve. As a matter of facts , we conducted experiments on the learning speed and the generalization ability on rotation of the RQNN and the usual multi-layer feedforward quaternion neural network, and showed the differences between the two quaternion neural networks."
https://arxiv.org/html/2411.05806v1,SkipSNN: Efficiently Classifying Spike Trains with Event-attention,"Spike train classification has recently become an important topic in the machine learning community, where each spike train is a binary event sequence with temporal-sparsity of signals of interest and temporal-noise properties. A promising model for it should follow the design principle of performing intensive computation only when signals of interest appear. So such tasks use mainly Spiking Neural Networks (SNNs) due to their consideration of temporal-sparsity of spike trains. However, the basic mechanism of SNNs ignore the temporal-noise issue, which makes them computationally expensive and thus high power consumption for analyzing spike trains on resource-constrained platforms. As an event-driven model, an SNN neuron makes a reaction given any input signals, making it difficult to quickly find signals of interest. In this paper, we introduce an event-attention mechanism that enables SNNs to dynamically highlight useful signals of the original spike trains. To this end, we propose SkipSNN, which extends existing SNN models by learning to mask out noise by skipping membrane potential updates and shortening the effective size of the computational graph. This process is analogous to how people choose to open and close their eyes to filter the information they see. We evaluate SkipSNN on various neuromorphic tasks and demonstrate that it achieves significantly better computational efficiency and classification accuracy than other state-of-the-art SNNs.","Figure 1: The problem definition of efficient classification of spike trains. The spike trains are generated by an event camera, which is an imaging sensor that responds to local changes in brightness. Each pixel inside an event camera operates independently and asynchronously, reporting changes in brightness as they occur, and staying silent otherwise. Therefore, each image can be considered as binary event image. Motivation. Spike trains are sequences of binary signals where 1s are spikes and 0s are not spikes. Such data are common to a variety of domains and are classically analogous to electrochemical signals in the human brain. Spike train datasets are generated from event cameras, which resemble the human eye. Event cameras, also called neuromorphic cameras, require little energy and are designed to capture objects at high speed. Thus, spike train datasets naturally arise during the development of dynamic vision devices [1, 2, 3, 4, 5]. Recently, spike train classification has attracted much attention in the machine learning community [6, 7, 8, 9, 10, 11, 12]. Compared with the traditional sequence classification tasks, spike train classification is unique in the following two aspects [13, 14, 15, 16]: 1) Temporal-sparsity of signals of interest. The label of a spike train is only related with certain objects that may only appear in a very small portion of the whole time window. 2) Temporal-noise problem. The signals at the majority of time steps are generated from background activities that are not related to objects of interest. These two properties of spike train classification impede the application of the widely used deep learning models, e.g., recurrent neural networks (RNNs), because of their high computational costs, unnecessarily spent on the whole time window. Tasks on spike train data are instead usually processed on energy-sensitive platforms such as wireless monitors and drones, so more computationally efficient models are required. To this end, we propose a new design principle to guide developement of machine learning models for spike train classification: perform intensive computation only when signals of interest appear. Knowledge Gap. Spiking Neural Networks (SNNs) are potential candidates for spike train classification [8, 6, 9], since they are attempt to meet the aforementioned design principle by considering the temporal-sparsity of spike trains. They take spike trains as inputs and outputs, using biologically inspired, event-driven computation and communication in their design. An SNN neuron’s core function is to react only when its cummulative membrane potential exceeds a fixed value. As a result, the neuron has a chance to be activated only when it currently has an event signal, which is passed in as a binary spike. Thus, compared to traditional deep learning models, SNNs can build large-scale neural networks with far less energy and memory for spike train classification. However, SNNs primarily consider the temporal-sparsity of spike trains, overlooking the critical aspect of temporal-noise. As illustrated in Figure 1, consider a scenario where a drone equipped with an event camera detects obstacles to adjust its route. For most of the operational timeline, the camera records signals irrelevant to obstacle detection. Nevertheless, SNNs respond to any detected signals, even if they are merely noise. Consequently, SNNs often fail to adhere to the principle that models should activate only in response to signals of interest. We find that this misalignment is a fundamental factor contributing to the poor generalization and decreased energy efficiency observed in SNNs in real-world applications. To achieve high classification accuracy with low computational cost for real-world spike trains, we need to follow the aforementioned design principle by considering both temporal-sparsity of useful signals and temporal-noise issue. An intuitive approach, which we pioneer in this paper, is for the model to stop processing data when the relevant object is out of its field of view. This behavior is analogous to how we open and close our eyes to filter out the information we see. Challenges. We propose a novel method for allowing SNNs to efficiently classify spike trains. Solving this problem is challenging for two main reasons: • Neuron Consistency: The promise of SNNs comes from their likeness to real neural circuits in the human brain. Maintaining this similarity is essential to successful SNNs. However, in the standard SNN when a neuron enters a hibernation state, it is hard to wake it up again if there is no new signal input to the network. This means that if the model ignores the input at a timestep, the neurons in the network will lack new input signals. This keeps the neurons silent, making the model likely to ignore potentially useful signals in the future. Thus, when extending SNNs to our problem setting, it is challenging to train successful spiking neurons to skip updates. • Non-differentiability: SNNs are notoriously difficult to train due to non-differentiable nature of spike activity. Most related works use the rectangular function or sigmoid to approximate the corresponding derivative. However, in practice we find that when our optimization objective considers both accuracy and efficiency, this approximation leads to decayed performance. Additionally, optimization largely depends on the initial values of the parameters of the model. Even though some parameter initialization methods such as Glorot [17] work for traditional artificial neural networks, they lack theoretical basis in SNNs. Designing an efficient optimization algorithm is the second challenge we face. Figure 2: Differences among Recurrent Neural Network (RNN) , SkipRNN [18], Spiking Neural Network (SNN) [8, 9] and SkipSNN (ours). SkipSNN outperforms others in computational efficiency and classification accuracy with an event-attention mechanism for noise filtering. Proposed Method. To solve our problem, we introduce an event attention mechanism that enables SNNs to dynamically highlight useful signals in the input spike train. We extend existing SNNs to have two different states: awake and hibernating, inspired by how people’s eyes open and close, turning on and off data intake. If our SNN enters its awake state at time step t, it will consider the input at t. Otherwise, if it hibernates at time step t, it will ignore the input at t. To this end, we design a controller that switches the model between these two states. Since this is not differentiable, we also introduce a new loss function with a penalty that trades off accuracy and computational cost. In this way, our extended SNN learns to mask out noise by skipping updates and shorten the effective size of the computational graph without requiring any additional supervision signal. We refer to our model as SkipSNN and illustrate the difference between it and traditional SNN in Figure 2. Contributions. Our key contributions are as follows: • We define the problem and modeling principle of general spike train classification, which is important for smart dynamic sensor systems with limited energy. • We propose SkipSNN, which solves this problem and can be used on energy-limited dynamic sensor devices. • We develop an efficient optimization technique to train our SkipSNN model. • We demonstrate that our model outperforms recent state-of-the-art alternatives by achieving higher accuracy and lower computational cost when tested on both the neuromorphic MNIST and DVS-Gesture datasets. The rest of our paper is organized as follows. First, we review related work, then introduce details of the background methods. Next, in Section 4, we present our proposed method. We then describe our experimental setup and discuss our results in Section 5. Finally, we conclude the paper with key take-aways and give some directions for future work."
https://arxiv.org/html/2411.05802v1,Similarity-based Context Aware Continual Learning for Spiking Neural Networks,"Biological brains have the capability to adaptively coordinate relevant neuronal populations based on the task context to learn continuously changing tasks in real-world environments. However, existing spiking neural network-based continual learning algorithms treat each task equally, ignoring the guiding role of different task similarity associations for network learning, which limits knowledge utilization efficiency. Inspired by the context-dependent plasticity mechanism of the brain, we propose a Similarity-based Context Aware Spiking Neural Network (SCA-SNN) continual learning algorithm to efficiently accomplish task incremental learning and class incremental learning. Based on contextual similarity across tasks, the SCA-SNN model can adaptively reuse neurons from previous tasks that are beneficial for new tasks (the more similar, the more neurons are reused) and flexibly expand new neurons for the new task (the more similar, the fewer neurons are expanded). Selective reuse and discriminative expansion significantly improve the utilization of previous knowledge and reduce energy consumption. Extensive experimental results on CIFAR100, ImageNet generalized datasets, and FMNIST-MNIST, SVHN-CIFAR100 mixed datasets show that our SCA-SNN model achieves superior performance compared to both SNN-based and DNN-based continual learning algorithms. Additionally, our algorithm has the capability to adaptively select similar groups of neurons for related tasks, offering a promising approach to enhancing the biological interpretability of efficient continual learning.","Lifelong learning is the prominent capability of biological intelligence and the significant challenge of artificial intelligence. In the process of continuously encountering new environments, the brain effectively identifies the connections between new and old knowledge through task contexts. It reshapes the neural network by associating new tasks with similar prior knowledge to adapt to new information, while strengthening the old knowledge Bar (2007, 2004). However, there is still much room for existing continual learning research to improve context-based efficient and flexible learning. Spiking Neural Networks (SNNs) Maass (1997) have been extensively researched due to their high efficiency and bio-interpretability, incorporating SNNs with continual learning mechanisms of the brain provides natural advances. Nevertheless, to the best of our knowledge, there is little continual learning for SNNs. Only ASP Panda et al. (2017) and HMN Zhao et al. (2022) use STDP-based regularization and neuronal activity-based subnetwork selection to overcome catastrophic forgetting, but they are only suitable for shallow networks. DSD-SNN Han et al. (2023b) and SOR-SNN Han et al. (2023a) apply brain-inspired continual learning algorithms to deep SNNs, ignoring the effect of task-to-task associations. The brain adaptively modulates neuronal generation, allocation, extinction, and reuse for continual learning. Therefore, except for a few continual learning algorithms based on SNNs, we also focus on structure extension algorithms based on DNNs Van de Ven and Tolias (2019). Structure expansion methods assign separate sub-network structures for different tasks, ensuring that learning new tasks does not interfere with previous ones. They can be categorized into progressive neural networks and subnetwork selection algorithms. Among them, progressive networks require assigning a new network to each new task, with full connectivity between the old and new networks Rusu et al. (2016); Siddiqui and Park (2021). Subnetwork selection algorithms select task-specific sparse masks among a finite network Fernando et al. (2017); Gao et al. (2022) Chandra et al. (2023); Hu et al. (2024). However, these two structure expansion algorithms face the following challenges: 1) Catastrophic increase in energy consumption due to the growth of the network scale. The number of progressive neural network parameters increases linearly with the number of tasks Yan et al. (2021); Huang et al. (2023). Despite the subnetwork selection algorithms managing network size, the global network state transitions from sparse to fully connected configuration, resulting in augmented energy consumption Rajasegaran et al. (2019); Xu and Zhu (2018). 2) Cross-task knowledge transfer: the progressive network reuses all past knowledge indiscriminately Rusu et al. (2016), and the subnetwork algorithms do not take into account the association between tasks when learning task-specific masks Dekhovich et al. (2023); Sokar et al. (2021). 3) Structure expansion algorithms rely on prior knowledge of the task affiliation of the current sample to determine the relevant elements to utilize Yoon et al. (2018); Chandra et al. (2023). Thus, most algorithms are primarily suited for task-incremental learning (completing a given task in testing) and are insufficient for class-incremental learning (completing all learned tasks in testing). To address these issues, DNN-based continual learning has been drawn to some extent from the biological continual learning mechanisms Ma et al. (2023), such as contextual similarity recognition. They consider similarity usually focus on the synapses Ke et al. (2022) and data Wang et al. (2023) and involve designing additional evaluation networks Ke et al. (2020). Upon identifying similar tasks, all neurons of the similar tasks are directly reused without any distinction Wang et al. (2022). These factors can lead to different parts in similar tasks interfering, neither are they able to activate similar neural circuits in similar tasks as the brain does. Inspired by the brain contextual task knowledge recognition and flexible neural circuit allocate and reuse mechanisms, we propose the Similarity-based Context Aware continual learning for Spiking Neural Networks (SCA-SNN). Firstly, we designed a task similarity evaluation method that integrates the current data and network state to determine task similarity. Leveraging this assessment, we adaptively reuse neurons from previous tasks and generate a certain number. The underlying principle is that the more similar the new task is to the previous one, the less new network expansion and the more reuse of existing network. More importantly, to avoid redundancy caused by the extensive reuse of neurons, we draw inspiration from the developmental plasticity of human brain neurons, which adhere to the principle of ’use it or lose it’ Bruer (1999). This means that neurons habituated to certain tasks require stronger repetitive stimulation to be reactivated in a new task Grissom and Bhatnagar (2009), while those unrelated to the new tasks will be disconnected. We design a gradient-based method for selecting reused neurons, ensuring that only neurons contributing effectively to the new task are reused. We validate the proposed model on CIFAR100, mini-Imagenet general continual learning dataset, and FMNIST-MNIST, SVHN-CIFAR100 mixed dataset. Our model effectively identifies task similarity relationships to guide more flexible and efficient neuron allocation, thereby reducing energy consumption and achieving superior performance. Meanwhile, the proposed model can adaptively assign similar neuron populations to similar tasks like the human brain. Our SCA-SNN contribution points are as follows: \bullet We propose the Similarity-based Context Aware model in SNN, which leverages task similarity to selectively reuse similar neurons from previous tasks and flexibly expand new neurons for new tasks. The proposed model promotes cross-task knowledge transfer and belongs to advanced exploration for brain-inspired SNN continual learning. \bullet We design an effective neuron reuse strategy inspired by the habituation mechanism observed in the biological brain neurons. This approach carefully selects neurons that are truly beneficial for new tasks. By filtering out superfluous neurons, our approach enhances the efficiency of energy and utilization of known knowledge, preventing irrelevant neurons from interfering with the execution of new tasks. \bullet Extensive experiments across various class-incremental and task-incremental learning demonstrate that the proposed model achieves the state-of-the-art performance of the spiking neural networks. Meanwhile, due to the sparsity of the network and the discrete characterization of SNN, the proposed method significantly reduces the energy consumption."
https://arxiv.org/html/2411.05798v1,A Genetic Algorithm for Multi-Capacity Fixed-Charge Flow Network Design,"The Multi-Capacity Fixed-Charge Network Flow (MC-FCNF) problem, a generalization of the Fixed-Charge Network Flow problem, aims to assign capacities to edges in a flow network such that a target amount of flow can be hosted at minimum cost. The cost model for both problems dictates that the fixed cost of an edge is incurred for any non-zero amount of flow hosted by that edge. This problem naturally arises in many areas including infrastructure design, transportation, telecommunications, and supply chain management. The MC-FCNF problem is NP-Hard, so solving large instances using exact techniques is impractical. This paper presents a genetic algorithm designed to quickly find high-quality flow solutions to the MC-FCNF problem. The genetic algorithm uses a novel solution representation scheme that eliminates the need to repair invalid flow solutions, which is an issue common to many other genetic algorithms for the MC-FCNF problem. The genetic algorithm’s efficiency is displayed with an evaluation using real-world \chCO2 capture and storage infrastructure design data. The evaluation results highlight the genetic algorithm’s potential for solving large-scale network design problems.","The Multi-Capacity Fixed-Charge Network Flow (MC-FCNF) problem is a well-studied optimization problem encountered in many domains including infrastructure design, transportation, telecommunications, and supply chain management [1, 2, 3]. In the MC-FCNF problem, each edge in the network has multiple capacities available to it, with each capacity having its own fixed construction and variable utilization costs. The objective of the MC-FCNF problem is to assign capacities to edges in the network such that a target flow amount can be hosted at minimal cost. The MC-FCNF problem is a generalization of the Fixed-Charge Network Flow (FCNF) problem, which has a single capacity (and fixed and variable costs) available per edge. The MC-FCNF problem is NP-Hard to approximate within the natural logarithm of the number of vertices in the graph [3]. As such, finding optimal solutions to large instances is often computationally infeasible. Significant work has already been done on solving the MC-FCNF and FCNF problems using many techniques including mathematical programming, branch and bound, and optimal approaches [4, 5, 6, 7, 8]. Multi-capacity edge networks are often referred to as buy-at-bulk network design problems, and are often framed as facility location problems, which is similar to the MC-FCNF problem but with added demand constraints on sinks [9, 10, 11]. Genetic algorithms have also been introduced for variants of the MCNF problem [12, 13, 14, 15, 16, 17, 18, 19]. In this paper, we introduce a novel genetic algorithm to solve the MC-FCNF problem. The novel contribution of our genetic algorithm is the representation of a flow solution by an array of parameters that scale the fixed-costs for each edge in the network. This representation ensures that each array corresponds to a valid flow, thereby eliminating the need for computationally expensive repair functions that are required by other genetic algorithms for the MC-FCNF problem [12, 13, 15, 20, 17, 18, 19]. By avoiding costly repair functions, the proposed algorithm is able to efficiently find high-quality solutions to very large MC-FCNF problem instances. The proposed genetic algorithm is inspired by slope scaling techniques previously employed for the FCNF problem [4, 21]. It is a matheuristic, as it employs mathematical programming to calculate a flow solution from a linear program parameterized with the fixed-cost scaling arrays [22]. Our genetic algorithm is similar to an algorithm proposed by [23], though ours takes a different two-stage approach to handle multi-capacity edges. Additionally, we provide more insight into the existence of the optimal solution in the search space. An evaluation is presented that designs \chCO2 capture and storage (CCS) infrastructure deployments using real-world data composed of thousands of vertices and tens of thousands of edges. In the evaluation, the genetic algorithm is compared to the the solution of an optimal integer linear program formulation of the MC-FCNF problem. Results from the evaluation demonstrate the utility of the genetic algorithm for very large networks, even if the solution is very small compared to the full network. The rest of this paper is organized as follows: Section 2 formally introduces the MC-FCNF program and formulates it as an integer linear program. Section 3 presents a linear programming modification to the integer linear program that serves as the core to the genetic algorithm. Section 4 introduces the genetic algorithm and discusses the existence of the optimal solution in the search space. Section 5 presents an evaluation of the genetic algorithm on real-world CCS data and the paper is concluded in Section 6."
https://arxiv.org/html/2411.07120v1,"Efficient Adaptive Optimization via Subset-Norm and Subspace-Momentum: Fast, Memory-Reduced Training with Convergence Guarantees","We introduce two complementary techniques for efficient adaptive optimization that reduce memory requirements while accelerating training of large-scale neural networks. The first technique, Subset-Norm adaptive step size, generalizes AdaGrad-Norm and AdaGrad(-Coordinate) by reducing the second moment term’s memory footprint from O(d) to O(\sqrt{d}) through step-size sharing, where d is the model size. For non-convex smooth objectives under coordinate-wise sub-gaussian gradient noise, we prove a noise-adapted high-probability convergence guarantee showing improved dimensional dependence over existing methods. Our second technique, Subspace Momentum, reduces the momentum state’s memory footprint by operating in a low-dimensional subspace while applying standard SGD in the orthogonal complement. We establish high-probability convergence rates under similar relaxed assumptions. Empirical evaluation on LLaMA models from 60M to 1B parameters demonstrates the effectiveness of our methods, where combining subset-norm with subspace-momentum achieves Adam’s validation perplexity in approximately half the training tokens (6.8B vs 13.1B) while using only 20% of the Adam’s optimizer-states memory footprint and requiring minimal additional hyperparameter tuning.","Adaptive optimizers like Adam [KB14], AdaGrad [DHS11], and RMSProp [TH+12] are de facto methods for training large-scale deep neural networks. However, the optimizer states for the momentum and second moment (or adaptive step size) terms are memory intensive, consuming as much as twice the size of the model. As deep neural networks continue to grow in the era of large-language models (LLMs), concerns that were previously overlooked, such as the memory consumption of optimizer states, have become an active area of research. Indeed, numerous methods have recently emerged to reduce the memory footprint of optimizer states (e.g. Adam’s momentum and second moment terms) with approaches ranging from quantization [LCZ24, DLSZ21, DPHZ24], low-rank decomposition [HSW+21, LMSR23, ZZC+24, SS18], sketching-based dimensionality reduction [MLW+24, HCM24], etc. Existing methods either lacks theoretical guarantees (or requires strong assumptions), trades too much performance, or requires expensive additional tuning for the memory saving, especially in pretraining tasks. In this paper, we present memory efficient adaptive optimization schemes with high-probability convergence guarantees under standard assumptions. First, we provide a generic template in Algorithm 1, which captures a broad range of first-order optimizers that leverage either momentum or adaptive step sizes. Many standard optimizers can be represented within this framework, as shown in Table 1, by varying choices of momentum and adaptive step-size terms. Generally, optimizers with higher memory requirements, such as Adam, tend to outperform more memory-efficient alternatives like SGD and RMSProp. However, we aim to achieve the best of both worlds: high performance with reduced memory consumption. To this end, we propose distinct compression schemes for each term: Subset-Norm (Section 3) for the adaptive step size term and Subspace-Momentum (Section 4) for the momentum term. These memory-efficient components are modular within the framework of Algorithm 1. Algorithm 1 Generic Template for Stochastic Adaptive Optimizers with Momentum 1:Initial point x_{1}\in\mathbb{R}^{d}, base step size \eta>0, and constant \epsilon>0. 2:for t=1 to T do 3: Obtain stochastic gradient \widehat{\nabla}f(x_{t}) 4: m_{t}=\text{update\_momentum}\left(\widehat{\nabla}f(x_{t});m_{t-1}\right) \triangleright Update momentum 5: v_{t}^{2}=\text{update\_adaptive\_stepsize}\left(\widehat{\nabla}f(x_{t});v_{t% -1}^{2}\right) \triangleright Update adaptive step size 6: x_{t+1}=x_{t}-\eta\cdot\frac{m_{t}}{v_{t}+\epsilon} \triangleright Apply update step 7:end for Table 1: Update rules for common optimizers in the framework of Algorithm 1. We omit bias correction terms and numerical stabilizer \epsilon for simplicity. Memory for optimizer state is shown for model of size d. Optimizer Memory update_adaptive_stepsize update_momentum Adam {\color[rgb]{1,0,0}2d} \beta_{2}v_{t-1}^{2}+(1-\beta_{2})\cdot\widehat{\nabla}f(x_{t})^{2} \beta_{1}m_{t-1}+(1-\beta_{1})\widehat{\nabla}f(x_{t}) SGDm d N/A \beta m_{t-1}+(1-\beta)\widehat{\nabla}f(x_{t}) AdaGrad d v_{t-1}^{2}+\widehat{\nabla}f(x_{t})^{2} \widehat{\nabla}f(x_{t}) AdaGrad-Norm {\color[rgb]{0,1,0}1} v_{t-1}^{2}+\left\lVert\widehat{\nabla}f(x_{t})\right\rVert^{2} \widehat{\nabla}f(x_{t}) RMSProp d \sqrt{\beta_{1}v_{t-1}^{2}+(1-\beta_{1})\cdot\widehat{\nabla}f(x_{t})^{2}} \widehat{\nabla}f(x_{t}) SGD {\color[rgb]{0,1,0}1} N/A \widehat{\nabla}f(x_{t}) Our contributions. We introduce two memory-efficient optimization algorithms for large-scale language model training: Subset-Norm (SN) for adaptive step-size memory reduction and Subspace-Momentum (SM) for momentum compression. While existing approaches trade performance for memory savings, our theoretically-grounded methods achieve both a reduced memory footprint and improved convergence. Our primary contributions are: • Subset-Norm (SN): A memory-efficient adaptive step-size algorithm with high-probability convergence guarantees for non-convex objectives under coordinate-wise sub-gaussian noise. By unifying AdaGrad-coordinate and AdaGrad-Norm’s analysis, we show that the SN adaptive step size (Algorithm 2) achieves improved dimensional dependence, reducing the memory footprint from O(d) to O(\sqrt{d}). On LLaMA pretraining tasks, AdamSN achieves better perplexity than Adam across a range of model sizes, while using significantly less memory and introducing no additional hyperparameters.111Although the subset size can be tuned (Section 5.1.1), we provide heuristics in Section 3.3 that work effectively across model sizes, eliminating the need for additional tuning. • Subspace-Momentum (SM): A momentum compression method that applies momentum in a chosen subspace and SGD in its orthogonal complement with high-probability convergence guarantees under \sigma-sub-gaussian noise for non-convex smooth objectives. When combined with SN, our method (AdamSNSM) reduces the memory footprint of Adam from d+d to d/r+\sqrt{d} (see Table 4) while delivers improved training speed and performance.222Typically, r is chosen to be around 4 for projecting to a subspace of dimension d/r. Empirical evaluation on LLaMA models ranging from 60M to 1B parameters demonstrates that our techniques scale effectively and show better stability than existing optimizers. Our proposed methods are simple to implement, require minimal additional hyperparameter tuning, and are compatible with modern distributed training frameworks like FSDP [ZGV+23, RRRH20]. Figure 1: Validation perplexity over training steps for Adam, GaLore, AdamSN, and AdamSNSM during LLaMA 1B model training for 13.1 billion tokens. Optimizer memory footprint is shown in parentheses. (Left) Adam achieves a perplexity of 17.27 at 50,000 steps, while AdamSN and AdamSNSM exhibit lower perplexity earlier in training at 42,000 and 32,000 steps, respectively. (Right) Adam achieves a perplexity of 16.00 at 100,000 steps, while AdamSN and AdamSNSM exhibit lower perplexity earlier in training at 58,000 and 48,000 steps, respectively. GaLore is slower than Adam overall. 1.1 Related works As model sizes grow, memory-efficient training techniques have become crucial. Following up on AdaFactor [SS18], low-rank methods like Galore [ZZC+24], LoRA [HCM24], and ReLORA [LMSR23] reduce memory usage by approximating large weight matrices with low-rank representations. Projection-based approaches, such as GRASS [MLW+24] and FLORA [HCM24], compress gradients or combine low-rank ideas with projections to reduce memory requirements. Recently, BAdam [LYL24], a block coordinate descent method that utilizes Adam as an inner solver, has been proposed for fine-tuning large language models. However, these methods are largely heuristic-driven and often lack convergence guarantees. In contrast, methods like SM3 [AGKS19], which uses subset (cover) statistics to show convergence in online learning, and MicroAdam [MSM+24], which provides convergence guarantees for a gradient compression scheme with error correction, offer theoretical guarantees, but evaluations are mainly limited to fine-tuning tasks. Additional approaches to reducing memory during training include optimizer quantization [LCZ24, DLSZ21, DPHZ24], attention computation compression/optimization [WRHS22, DFE+22, Dao23, SBZ+24], activation checkpointing [CXZG16], and distributed training [RRRH20]. For inference, compression techniques are also actively being explored [SK24, DLBZ22, XLS+24, LTT+24, FAHA23]. Convergence analysis of non-convex optimization methods has seen significant progress, with recent works providing convergence proofs for adaptive algorithms like Adam [LRJ24, DBBU22]. Numerous studies have explored convergence properties of various adaptive and stochastic gradient methods [CLSH18, DBBU22, EN21, LNN+23, LNEN23, WWB19, ZSJ+19, RKK18, Nes83], while lower bound analyses [ACD+23] have highlighted fundamental limits in non-convex optimization. Finally, [NN24] is a preliminary workshop version of this work that contains a version of the SN step-size."
https://arxiv.org/html/2411.06802v1,Identifying the impact of local connectivity patterns on dynamics in excitatory-inhibitory networks,"Networks of excitatory and inhibitory (EI) neurons form a canonical circuit in the brain. Seminal theoretical results on dynamics of such networks are based on the assumption that synaptic strengths depend on the type of neurons they connect, but are otherwise statistically independent. Recent synaptic physiology datasets however highlight the prominence of specific connectivity patterns that go well beyond what is expected from independent connections. While decades of influential research have demonstrated the strong role of the basic EI cell type structure, to which extent additional connectivity features influence dynamics remains to be fully determined. Here we examine the effects of pair-wise connectivity motifs on the linear dynamics in excitatory-inhibitory networks using an analytical framework that approximates the connectivity in terms of low-rank structures. This low-rank approximation is based on a mathematical derivation of the dominant eigenvalues of the connectivity matrix, and predicts the impact on responses to external inputs of connectivity motifs and their interactions with cell-type structure. Our results reveal that a particular pattern of connectivity, chain motifs, have a much stronger impact on dominant eigenmodes than other pair-wise motifs. In particular, an over-representation of chain motifs induces a strong positive eigenvalue in inhibition-dominated networks and generates a potential instability that requires revisiting the classical excitation-inhibition balance criteria. Examining effects of external inputs, we show that chain motifs can on their own induce paradoxical responses, where an increased input to inhibitory neurons leads to a decrease in their activity due to the recurrent feedback. These findings have direct implications for the interpretation of experiments in which responses to optogenetic perturbations are measured and used to infer the dynamical regime of cortical circuits.","Circuits of excitatory and inhibitory (EI) neurons are believed to form the fundamental components of information-processing in the brain [1, 2, 3, 4]. Network models of recurrently-connected excitatory and inhibitory units have therefore become an essential tool for understanding neural dynamics and computation. Such models have helped uncover fundamental principles such as the role of excitation-inhibition balance for sustaining irregular activity [5, 6, 7, 8, 9], and the importance of inhibition for stabilizing neural activity [8, 10, 11, 12] and normalizing responses [13]. A phenomenon that has attracted particular attention are paradoxical responses, which refer to situations where an increase in the external input to the inhibitory neurons results in a decrease of their activity because of recurrent interactions [10, 11, 14, 13, 15, 16, 13, 12]. Recent theoretical analyses have argued that such paradoxical responses to external inputs can reveal the dynamical regime of the underlying excitatory-inhibitory network [14, 17, 18], and these insights have been used to interpret experimental measurements of responses to optogenetic perturbations [19, 20, 21]. These seminal theoretical results on excitatory-inhibitory networks however are derived via a key simplifying assumption. In standard models, the strength of the synaptic coupling between any two neurons depends on their types, but is otherwise assumed to be an independent random variable uncorrelated across synapses. This assumption is typically used to reduce a full network to a simpler circuit model that describes how the mean activities of different populations interact through averaged synaptic weights (Fig 1(a,b)). Recent synaptic-resolution experimental datasets from various species and brain areas have however revealed the prevalence of non-trivial connectivity patterns [22, 23, 24, 25, 26, 27, 28, 29, 30]. In particular, a recently released dataset from mice and humans [23, 30] reported the prominence of second-order motifs - specific correlations between pairs of synapses, such as reciprocal, chain, convergent, and divergent motifs (Fig 1(c)) - that go well beyond what is expected from independent connections, and highlights the need for a theoretical understanding of the effects of such patterns. While the presence of reciprocal connectivity motifs has been long recognized and examined within network models [29], several studies have found their influence on recurrent dynamics to be relatively modest [31, 30, 32, 33]. Other studies have argued for the importance of several types of synaptic motifs working together to determine statistical properties of network activity, such as average synchrony or correlation among neurons and network-wide dimensionality [34, 35, 32, 30, 36, 37, 38, 39, 40]. In a general theoretical analysis [35], chain motifs were found to have a dominant role in determining population-averaged responses of networks to their inputs. Nevertheless, the impact of chain motifs on excitation-inhibition balance – including central issues of stability and paradoxical responses – as well as the general interplay among these motifs and structures established by cell-type specific connectivity, remain open and intriguing questions. Figure 1: Schematic of the multi-population network model. (a) The network consists of P populations, each represented by a different color. This population structure defines the statistics of synaptic connectivity. (b) The corresponding connectivity matrix consists of P^{2} blocks. Synaptic weights within each block share identical statistical properties. (c) Four different types of second-order motifs, corresponding to different pair-wise correlations between synaptic weights. (d) A low-rank approximation of the connectivity matrix can integrate both the population structure and the pair-wise motif statistics. In this study, we examine how the interaction between population structure and synaptic motifs influences recurrent dynamics and the presence of paradoxical responses. To this end, we expand a previously introduced theoretical framework that allows us to reduce large networks of multiple populations to a low-dimensional description that incorporates connectivity statistics beyond the mean via a low-rank approximation [33]. Applying this theory to excitatory-inhibitory networks with chain motifs, we demonstrate that these connectivity patterns significantly impact the eigen-spectrum of the connectivity matrix, and thereby the overall recurrent dynamics. Specifically, we found that chain motifs can create strong positive feedback even in inhibition-dominated networks, and are therefore a source of a potential instability that requires revisiting the classical conditions of excitation-inhibition balance. Moreover we show that chain motifs strongly influence the responses of different populations to external inputs, and can control whether the responses are paradoxical or not. These findings highlight the intricacy of the relationship between the responses to inputs and the underlying connectivity, and in particular sound a note of caution for interpreting results of experiments in terms of only average connectivity strengths among E and I cells. This manuscript is organized as follows. Sec. I defines the connectivity and the network model. Sec. II introduces the general theoretical framework based on a low-rank approximation of the connectivity, which allows us to analytically investigate the effects of local motifs, in particular chain and reciprocal motifs, on recurrent dynamics and responses to external inputs. Secs. III and IV apply this theory to fully-connected and sparse excitatory-inhibitory networks with chain motifs. In Sec. III we analyze the influence of chain motifs on the eigenspectra of these models. In Sec. IV, we use these results to study the responses to external inputs and under what condition they are paradoxical. Table 1: List of notations. Notation Description i,j Single neuron indices p,q Population indices N_{p} Number of neurons in population p \alpha_{p} Fraction of neurons in population p J^{0}_{pq} Mean synaptic weight from population p to population q \sigma_{pq} standard deviation of the synaptic weights from population p to population q \sigma Re-scaled homogeneous standard deviation of the synaptic weights \tau^{c/r}_{pq} Correlation coefficient of the chain/reciprocal connectivity motifs c Connection probability in sparse networks J Excitatory synaptic weights in the sparse E-I network g Relative (mean) strength of inhibitory to excitatory synapses in (Gaussian) sparse E-I network \gamma Ratio of inhibitory to excitatory population size"
https://arxiv.org/html/2411.06613v1,"Are Neuromorphic Architectures Inherently Privacy-preserving?
An Exploratory Study","While machine learning (ML) models are becoming mainstream, including in critical application domains, concerns have been raised about the increasing risk of sensitive data leakage. Various privacy attacks, such as membership inference attacks (MIAs), have been developed to extract data from trained ML models, posing significant risks to data confidentiality. While the mainstream work in the ML community considers traditional Artificial Neural Networks (ANNs) as the default neural model, neuromorphic architectures, such as Spiking Neural Networks (SNNs) have recently emerged as an attractive alternative mainly due to their significantly low power consumption. These architectures process information through discrete events, i.e., spikes, to mimic the functioning of biological neurons in the brain. While the privacy issues have been extensively investigated in the context of traditional ANNs, they remain largely unexplored in neuromorphic architectures, and little work has been dedicated to investigate their privacy-preserving properties. In this paper, we investigate the question whether SNNs have inherent privacy-preserving advantage. Specifically, we investigate SNNs’ privacy properties through the lens of MIAs across diverse datasets, comparatively with ANNs . We explore the impact of different learning algorithms (surrogate gradient and evolutionary learning), programming frameworks (snnTorch, TENNLab, and LAVA), and various parameters on the resilience of SNNs against MIA. Our experiments reveal that SNNs demonstrate consistently superior privacy preservation compared to ANNs, with evolutionary algorithms further enhancing their resilience. For example, on the CIFAR-10 dataset, SNNs achieve an AUC as low as 0.59 compared to 0.82 for ANNs, and on CIFAR-100, SNNs maintain a low AUC of 0.58, whereas ANNs reach 0.88. Furthermore, we investigate the privacy-utility trade off through Differentially Private Stochastic Gradient Descent (DPSGD) observing that SNNs incur a notably lower accuracy drop than ANNs under equivalent privacy constraints.","As ML systems become more sophisticated and widespread, individuals are increasingly relying on these systems, entrusting them with personal and professional data. Consequently, the risk of sensitive information exposure is growing significantly in multiple sectors (Bertino, 2016) including healthcare (Abouelmehdi et al., 2017), finance (Tripathi and Mukhopadhyay, 2020), national security (Thuraisingham, 2002), education (Florea and Florea, 2020) and consumer services (Lee et al., 2015). It is particularly alarming in fields such as healthcare, where the confidentiality of patient data is extremely sensitive as a breach could result in severe personal and financial implications, affecting patient care and institutional credibility (Luo et al., 2018; JM et al., 2018; U.S. Department of Health & Human Services, 2024). In finance, the integrity of financial transactions and records is fundamental for maintaining market stability and preventing fraud (Yu and He, 2021), while in national security , safeguarding classified information is essential to protect national interests and prevent threats to public safety (Kim et al., 2010). This has led to the development of various privacy attacks targeting ML models to extract sensitive information, including Model Inversion Attacks (Fredrikson et al., 2015), Attribute Inference Attacks (Gong and Liu, 2018), Model Stealing Attacks (Juuti et al., 2019), and Membership Inference Attacks (MIAs) (Shejwalkar et al., 2021). Among these, MIAs are particularly prominent as a significant threat to data privacy (Salomon, 2012), wherein an adversary seeks to ascertain if a specific data point was part of the dataset used to train the model. This intrusion risks exposing sensitive information about individuals in the training dataset, potentially compromising personal data confidentiality (Shokri et al., 2017). While most research on privacy attacks has concentrated on ANNs, there is limited exploration within neuromorphic computing, particularly with Spiking Neural Networks (SNNs). Designed to emulate the dynamic behavior of biological neurons (Ghosh-Dastidar and Adeli, 2009), SNNs process information through discrete, temporally encoded spikes (Roy et al., 2019), enabling them to handle time-sensitive data efficiently (Hong et al., 2017). Their suitability for edge computing (Shi et al., 2016) and resource-constrained environments further enhances their value, as SNNs effectively process real-world spatiotemporal patterns (Wang et al., 2018). This capability positions SNNs as a promising alternative to traditional neural networks for applications requiring dynamic, real-time data processing. This work addresses the privacy concerns associated with SNNs through a structured investigation of three core areas: (i) the resilience of ANN and SNN models to Membership Inference Attacks (MIAs), (ii) the factors influencing the privacy-preserving properties of SNNs, and (iii) the privacy-utility trade-off in ANN and SNN models using the DPSGD algorithm. We consider that the potential resilience of SNNs against MIAs is based on two key aspects. Firstly, the non-differentiable and discontinuous nature of SNNs may weaken the correlation between the model and individual data points, making it more challenging for an attacker to identify the membership of a particular data point in the training set (Meng et al., 2022). Secondly, the unique encoding mechanisms employed by SNNs introduce an additional layer of stochasticity (Olin-Ammentorp et al., 2021) and variability to the data representation. This added complexity can make it more difficult for an attacker to deduce the unique characteristics of individual data points, thereby making them more indistinguishable. Investigating the resilience of SNNs against MIAs, our experimental results consistently demonstrated that SNNs exhibit higher resilience to MIAs across the datasets including MNIST, F-MNIST, Iris, Breast Cancer, CIFAR-10, CIFAR-100, and ImageNet. This is evidenced by the lower Area Under the Curve (AUC) values for the Receiver Operating Characteristic (ROC) curves in SNNs compared to their ANN counterparts. Furthermore, our exploration domain encompassed various learning algorithms (surrogate gradient-based and evolutionary learning), programming frameworks (snnTorch, TENNLab, and LAVA), and a wide range of parameters within them, providing a comprehensive analysis of the factors influencing the inherent privacy-preserving properties of SNNs. This in-depth exploration revealed that evolutionary learning algorithms have shown to boost this resilience more effectively compared to the gradient-based methods. In order to enhance data privacy and explore the compromises between privacy and utility, we study the implementation of the DPSGD algorithm as a privacy defense mechanism (Xu et al., 2021). This introduces controlled noise into the training process, making it harder for attackers to infer the presence of specific data points. However, improved privacy often comes at the cost of reduced model performance, known as the privacy-utility trade-off (Song et al., 2013). Through the experiments, we observe that SNNs exhibit a notably lower performance drop compared to ANNs for the same level of privacy guarantee. This finding further reinforces our hypothesis regarding the inherent privacy-preserving properties of SNNs. This paper offers the following notable findings in the field of data privacy, particularly in the context of SNNs: • SNNs exhibit higher resilience against MIAs compared to ANNs, with lower AUC scores on CIFAR-10 (SNN: 0.59 vs. ANN: 0.82) and CIFAR-100 (SNN: 0.58 vs. ANN: 0.88), highlighting their potential as a more secure alternative in privacy-sensitive applications. • Evolutionary learning algorithms outperform gradient-based methods in MIA resilience, maintaining a consistent AUC of 0.50 across all parameters for Iris and Breast Cancer datasets, compared to 0.57 and 0.55 AUC scores for gradient-based algorithms, respectively. • Privacy-utility tradeoff analysis revealing that SNNs incur a lower accuracy drop compared to ANNs when applying DPSGD: For F-MNIST, with privacy guarantees ranging from 0.22 to 2.00, the average accuracy drop is 12.87% for SNNs, significantly lower than the 19.55% drop observed in ANNs. It should be emphasized that while this investigation highlights SNNs’ enhanced privacy characteristics, these findings are specifically contextualized within privacy-preservation applications. The architectural properties of SNNs that facilitate efficient hardware implementation and reduced computational overhead make them particularly appealing for resource-constrained environments. However, these advantages should not be interpreted as a general superiority of SNNs over ANNs across all applications. Rather, this work is motivated by the intuition that SNNs’ unique information processing mechanisms may offer specific advantages in privacy preservation, warranting systematic investigation in this particular domain."
https://arxiv.org/html/2411.06367v1,BayesNAM: Leveraging Inconsistency for Reliable Explanations,"Neural additive model (NAM) is a recently proposed explainable artificial intelligence (XAI) method that utilizes neural network-based architectures. Given the advantages of neural networks, NAMs provide intuitive explanations for their predictions with high model performance. In this paper, we analyze a critical yet overlooked phenomenon: NAMs often produce inconsistent explanations, even when using the same architecture and dataset. Traditionally, such inconsistencies have been viewed as issues to be resolved. However, we argue instead that these inconsistencies can provide valuable explanations within the given data model. Through a simple theoretical framework, we demonstrate that these inconsistencies are not mere artifacts but emerge naturally in datasets with multiple important features. To effectively leverage this information, we introduce a novel framework, Bayesian Neural Additive Model (BayesNAM), which integrates Bayesian neural networks and feature dropout, with theoretical proof demonstrating that feature dropout effectively captures model inconsistencies. Our experiments demonstrate that BayesNAM effectively reveals potential problems such as insufficient data or structural limitations of the model, providing more reliable explanations and potential remedies.","Explainable artificial intelligence (XAI) has become a significant field of research as machine learning models are increasingly applied in real-world systems including finance and healthcare. To provide insight into the underlying decision-making process behind the predictions made by these models, numerous researchers have developed various techniques to assist human decision-makers. Recently, Agarwal et al.[1] proposed a neural additive model (NAM) that utilizes neural networks to achieve both high performance and explainability. NAM is a type of generalized additive model (GAM) that involves the linear or non-linear transformation of each input and yields the final prediction through an additive operation. Previous studies have demonstrated that NAM not only learns complex relationships between inputs and outputs but also provides a high level of explainability based on neural network architectures and training techniques. Figure 1: Inconsistency of NAM, where two independent NAMs trained with the same dataset and architecture output different explanations solely due to different random seeds. In this paper, we analyze a critical yet overlooked phenomenon: the inconsistency phenomenon of NAM. Fig. 1 illustrates this issue, where two independent NAMs, trained on the same dataset and architecture, produce different explanations due solely to variations in random seeds. Such inconsistency has traditionally been viewed as a problem to be solved [2]. However, we argue that these inconsistencies are not merely obstacles but can offer valuable insights to uncover external explanations within the data model. Through a simple theoretical model, we show that NAMs naturally exhibit the inconsistency phenomenon even when trained on usual datasets that contain multiple important features. Building on this insight, we propose the Bayesian Neural Additive Model (BayesNAM), a novel framework that combines Bayesian neural networks with feature dropout to harness these inconsistencies for more reliable explainability. We also provide theoretical proof that feature dropout effectively leverages inconsistency. Our real-world experiments demonstrate that BayesNAM not only provides more reliable and interpretable explanations but also highlights potential issues in the data model, such as insufficient data and structural limitations within the model. The main contributions can be summarized as follows: • We investigate the inconsistency phenomenon of NAMs and analyze this phenomenon through a simple theoretical model. • We propose a new framework BasyesNAM, which utilizes Bayesian neural network and feature dropout. We also establish a theoretical analysis of the efficacy of feature dropout in leveraging inconsistency information. • We empirically demonstrate that BayesNAM is particularly effective in identifying data insufficiencies or structural limitations, offering more reliable explanations and insights for decision-making."
https://arxiv.org/html/2411.06236v2,Zero-Shot NAS via the Suppression of Local Entropy Decrease,"Architecture performance evaluation is the most time-consuming part of neural architecture search (NAS). Zero-Shot NAS accelerates the evaluation by utilizing zero-cost proxies instead of training. Though effective, existing zero-cost proxies require invoking backpropagations or running networks on input data, making it difficult to further accelerate the computation of proxies. To alleviate this issue, architecture topologies are used to evaluate the performance of networks in this study. We prove that particular architectural topologies decrease the local entropy of feature maps, which degrades specific features to a bias, thereby reducing network performance. Based on this proof, architectural topologies are utilized to quantify the suppression of local entropy decrease (SED) as a data-free and running-free proxy. Experimental results show that SED outperforms most state-of-the-art proxies in terms of architecture selection on five benchmarks, with computation time reduced by three orders of magnitude. We further compare the SED-based NAS with state-of-the-art proxies. SED-based NAS selects the architecture with higher accuracy and fewer parameters in only one second. The theoretical analyses of local entropy and experimental results demonstrate that the suppression of local entropy decrease facilitates selecting optimal architectures in Zero-Shot NAS.","Neural architecture search (NAS) algorithms automate the process of designing architectures, overcoming the limitations in terms of manual design efficiency. Nevertheless, the evaluation of networks relies on computationally expensive network training, which makes NAS dependent on high-performance GPUs (Real et al. 2019; Liu et al. 2018; Liu, Simonyan, and Yang 2019). Numerousds studies have been conducted to improve the search speed of NAS metho (Ren et al. 2021; Liu et al. 2021b; Jaafra et al. 2019; Baymurzina, Golikov, and Burtsev 2022; Cai et al. 2018). Zero-Shot NAS (Li et al. 2024) is a promising paradigm in accelerating network performance evaluation. It leverages geometric features derived from the network parameters or gradient landscape as evaluation criteria without complete training of networks. Challenges persist despite the effectiveness of existing zero-cost proxies. (i) Current zero-cost proxies run networks or invoke backpropagations, which is time-consuming for large search spaces (e.g., as shown in Example 1). (ii) Most proxies rely on the input data. The reliance on input data inadvertently underestimates the significance of architecture properties. To overcome these challenges, we propose a topology-based proxy to extract the architecture properties as evaluation criteria. Objectives. The overriding objective of this study is to develop a zero-cost proxy distinguished by its speed and reduced consumption of floating-point operations. This proxy is supposed to be data-free and network-running-free. Method and Results. From the perspective of entropy decrease, this study explores the impact of network architecture on the local entropy of feature maps. We prove that decreasing the local entropy of feature maps degrades specific features to biases, reducing network performance. Furthermore, irrational operation settings (including convolution, pooling, and skip connection) are proven to trigger local entropy decrease. Based on the above analyses, the suppression of local entropy decrease (SED) proxy is proposed to quantify the suppression of local entropy decrease as a proxy for network performance. The computation of SED, which is data-free and network-running-free, relies solely on architecture topologies to accelerate the evaluation. A schematic of SED calculation is provided in Figure 1. Experimental results demonstrate that SED outperforms most state-of-the-art (SOTA) zero-cost proxies on multiple benchmarks. In terms of efficiency, SED significantly accelerates architecture evaluation, taking only 3.4e-5 seconds to evaluate an architecture in NATS-Bench-TSS. In contrast, existing SOTA methods (e.g., grad_norm (Abdelfattah et al. 2021)) take at least 0.54 seconds. In terms of architecture selection, SED selects architectures ranked 3rd, 1st, and 24th for NATS-Bench-TSS using CIFAR-10, CIFAR-100, and ImageNet16-120, respectively. The Spearman’s \rho of SED achieves 0.09 higher than the SOTA proxy on NAS-Bench-301 out of 2,000 randomized architectures. Moreover, the SED-based NAS selects the architectures with the highest accuracy among the compared proxies in complete NAS tasks. For example, SED-based NAS selects architecture with 81.09% test accuracy, which is 0.42% higher than the SOTA proxies. Contributions. Compared with previous work, this study utilizes network architecture topologies to construct the proxy rather than network parameters and input data. The main contributions of this study are as follows: • Theoretically, we prove that specific network architectures reduce local entropy. Moreover, for classification tasks, entries in feature maps degrade to biases as the local entropy decreases, thereby reducing network performance. • Based on the theoretical analyses, the suppression of local entropy decrease caused by architectures is quantified as a proxy for network performance. This proxy outperforms SOTA proxies on multiple benchmarks, reducing the time consumed in architecture evaluation by three orders of magnitude compared to SOTA proxies. Example 1 Evaluation of a single architecture in the NATS-Bench-TSS benchmark using grad_norm (Abdelfattah et al. 2021) on a 3090 GPU takes only 0.54 seconds. NATS-Bench-TSS consists of 4 nodes and 5 related operations, totaling 15,625 architectures, and it takes 2.3 GPU hours to evaluate all the architectures in NATS-Bench-TSS. The architecture space with 5 nodes and 5 related operations, using the exact construction mechanism as NAS-Bench-TSS, contains 9,765K architectures. However, evaluating this architecture space takes at least 1,480 GPU hours based on grad_norm."
https://arxiv.org/html/2411.06124v1,Exploring Structural Nonlinearity in Binary Polariton-Based Neuromorphic Architectures,"This study investigates the performance of a binarized neuromorphic network leveraging polariton dyads, optically excited pairs of interfering polariton condensates within a microcavity to function as binary logic gate neurons. Employing numerical simulations, we explore various neuron configurations, both linear (NAND, NOR) and nonlinear (XNOR), to assess their effectiveness in image classification tasks. We demonstrate that structural nonlinearity, derived from the network’s layout, plays a crucial role in facilitating complex computational tasks, effectively reducing the reliance on the inherent nonlinearity of individual neurons. Our findings suggest that the network’s configuration and the interaction among its elements can emulate the benefits of nonlinearity, thus potentially simplifying the design and manufacturing of neuromorphic systems and enhancing their scalability. This shift in focus from individual neuron properties to network architecture could lead to significant advancements in the efficiency and applicability of neuromorphic computing.","Artificial neural networks (ANNs) have revolutionized data processing by emulating the intricate network of neurons in the human brain, enabling significant advances in fields ranging from robotics to healthcare [1, 2]. These systems process information through interconnected nodes or neurons that can learn to perform complex tasks, leading to improvements in decision-making and pattern recognition technologies. As the demand for these technologies grows, so does the interest in developing various hardware implementations to support them [3]. These hardware platforms include electronic-based systems, which leverage silicon-based technologies, and photonic systems, which exploit the interaction of light and matter to enhance speed and reduce energy consumption [4, 5, 6, 7, 8]. Another promising option is the use of exciton-polaritons, quasiparticles that combine the properties of light and matter. These are being investigated for their potential in neuromorphic computing, particularly because of their rapid operation times and potentially low power consumption [9, 10]. Each of these platforms aims to offer unique advantages, whether in scalability, speed, or energy efficiency, to meet the growing computational demands of modern ANNs. In light of the 2024 Nobel Prize in Physics awarded to John Hopfield and Geoffrey Hinton for foundational advances that have shaped the modern era of neural networks and machine learning, our exploration into polariton-based neuromorphic architectures gains added relevance. Hopfield’s seminal contributions to the physics of exciton-polaritons [11, 12] and neural network theory [13, 14] have inspired new approaches that blend these fields. This fusion of knowledge is at the heart of our study, underscoring the potential of exciton-polaritons in neuromorphic computing to push the boundaries of processing speeds and the inherent capability for parallel data handling. Binarized neural networks (BNNs) represent a specific approach to enhancing the computational efficiency of artificial neural networks [15, 16, 17]. By simplifying the weights and activations within the network to just two levels, typically 0 and 1, BNNs drastically reduce the computational complexity and memory usage required for neural processing. Although this simplification often results in lower accuracy compared to networks with full-precision weights, BNNs excel in scenarios where speed, power efficiency, and low resource consumption are more critical than achieving the highest possible accuracy, making them well-suited for applications in internet of things, edge computing, and other environments where autonomy and limited resources are key considerations [17, 18, 19]. Binarized neural networks have been effectively realized using exciton-polaritons. In the notable implementation described in Ref. [9], artificial neurons function as XOR gates. The used technique utilizes nonresonant laser pulses, acting as the input signals, to selectively excite spatially localized exciton-polariton condensates, that interact with each other. The resulting output signals vary in energy, reflecting the different combinations of the inputs. This approach has proven successful in pattern recognition tasks, achieving approximately 96% accuracy on the MNIST (Mixed National Institute of Standards and Technology) dataset, a standard benchmark in machine learning for handwritten digit recognition, under noisy conditions using a single-hidden-layer network. The impressive potential of this solution is further underscored by subsequent assessments of its remarkable energy efficiency, as reported in [10]. Nonlinearity is a cornerstone in the operational efficiency of neural networks, essential for executing tasks beyond the scope of linear computational models. This includes distinguishing overlapping data sets or solving inherently complex problems. The nonlinear activation function within each neuron exemplifies this intrinsic nonlinearity, defining how inputs are transformed into outputs in a way that linear operations cannot [20]. Exciton polaritons, known for their pronounced nonlinear properties due to polariton-polariton interactions, are especially valuable in this context. The distinctive nonlinearity of polaritons is the key element that drives the functionality of both continuous-weight networks [5] and binarized neuromorphic systems [9]. Recent research [21, 22, 23, 24] challenge the emphasis traditionally placed on the inherent nonlinearity in individual neurons within neural networks, see also [25, 26]. Studies have demonstrated that nonlinear computations can be realized using purely linear optical systems by adjusting the parameters of these systems. This development underscores that achieving nonlinearity does not necessarily rely on the physical nonlinearity of the system’s components. By encoding inputs as parameters rather than direct signals, a linear system can emulate nonlinear behavior. This approach shifts the focus from the inherent properties of the materials to the configuration of the system itself, which facilitates structural nonlinearity arising from the arrangement and interactions among its components. In our recent paper [27], we have theoretically proposed a binarized neuromorphic network architecture based on a lattice of pairwise coupled exciton polariton condensates. In this geometry, each pair of condensates, referred to as a polariton dyad [28], serves as artificial binary neurons functioning similarly to OR gates. Unlike XOR gate neurons utilized in work [9], the OR operation is linear. Nevertheless, in [27], we demonstrated that our proposed architecture effectively addresses the inherently nonlinear challenge of image classification, exemplified by the recognition tasks in the MNIST dataset. Our current study elucidates the role of structural nonlinearity in solving recognition tasks. We explore the potential for modifying the operation of polariton neurons proposed in [27] to function as both linear (NAND and NOR) and nonlinear (XNOR) gates. Through numerical experiments, we compare the image classification accuracies, allowing us to question whether the significance of inherent nonlinearity, typical of individual computational elements such as neurons, might be overstated."
https://arxiv.org/html/2411.06011v1,Exploring the impact of reflexivity theory and cognitive social structures on the dynamics of doctor-patient social system,"Conventional economic and socio-behavioural models assume perfect symmetric access to information and rational behaviour among interacting agents in a social system. However, real-world events and observations appear to contradict such assumptions, leading to the possibility of other, more complex interaction rules existing between such agents. We investigate this possibility by creating two different models for a doctor-patient system. One retains the established assumptions, while the other incorporates principles of reflexivity theory and cognitive social structures. In addition, we utilize a microbial genetic algorithm to optimize the behaviour of the physician and patient agents in both models. The differences in results for the two models suggest that social systems may not always exhibit the behaviour or even accomplish the purpose for which they were designed and that modelling the social and cognitive influences in a social system may capture various ways a social agent balances complementary and competing information signals in making choices.","Conventional economic and behavioural studies have long assumed that an agent has complete access to information and adheres to rational behaviour to model interactions between social agents [KorobkinUlen2000, Soros2013]. However, evidence challenging these assumptions has accumulated, pointing to other, more plausible theories [BromileyPapenhausen2003, Crotty2017]. This paper presents a novel model of social system design and analysis. Our model incorporates ideas from the theory of reflexivity [Soros2013] and cognitive social structures [Krackhardt1987a] offering a socio-cognitive approach to understanding the behaviours of individual social agents and emergent social phenomena. Reflexivity theory, considered formally proposed for economic analysis by George Soros [Soros2013],[Umpleby2018], is similar to the concepts found in the studies and literature of second-order cybernetics [Scott2004]. The theory, in a nutshell, suggests that social agents and the environment, which constitute a single system, are involved in a feedback loop with not just negative feedback, as usually considered to be the case in classical economic analysis of markets and social phenomena, but also positive feedback to and from the agents and the environment driving the overall system towards a specific, attractor state [Davis2020]. The critical insight from the theory that we have utilized in our model is that feedback loops between the agents and their environments change both the agents and the environments; once the agents act, the environments that they are in change as well, which influences the following actions the agents take and so it continues. Furthermore, the social agents can only access their own subjective realities of these changes, which are different from an objective reality (See Figure 1). These subjective realities can often drive many social phenomena, including business cycles and stock market bubbles [Beinhocker2013],[Soros2013], despite their possible deviations from the objective reality. Figure 1: Outline of Reflexivity Theory [Soros2013] Cognitive social structures, an extension of the theory of social structures, was instrumental in understanding that social agents are situated in a social network with relationship dynamics and that the position of social agents in a network influences the perceptions, decisions and actions of each agent in that network [Brands2013],[Krackhardt1987a]. Once again, similar to what we observe in the theory of reflexivity, the social agents do not have direct access to objective reality according to this framework. Instead, social agents perceive their interactions with others through a subjective lens, forming the basis for their actions and decisions. However, the critical difference between the two theories is that while the theory of reflexivity explores the dynamics of the interactions of social agents in a system, cognitive social structures uncover the matrix of relationships that influence the subjective perceptions of each agent [Frank2015]. Our primary motivation behind pursuing this research is the intuition that socially embedded, reflexive agents have different sets of behaviours that lead to the emergence of social phenomena which are closer to what we observe in our social systems compared to what we observe from classical economic and sociological models where rationality and perfect information assumptions are made. It is essential to realize and recognize that perceptions of individual agents are real in their consequences, even if there is no direct, one-to-one relationship between observed behaviours and objective reality. To flesh out our intuition, we compare and contrast observations from the simulation of two types of models, which we call the ”classical” and ”cognitive social system”, respectively. In particular, we explore the behaviours of doctors and patients in a primary healthcare network. We have chosen this social system partly because of the abundance of literature regarding the choices and behaviours of patients and doctors [Djulbegovic.etal2014],[Harris2003],[Kozikowski.etal2022] and also because of its utilitarian nature; better comprehension of this system would lead to more robust and precise healthcare services that will help in saving lives and improving standards of living [Cabrera.etal2011],[Comis.etal2021]. The classical model employs perfect information and rational behaviour assumptions, while the cognitive social system integrates the theory of reflexivity and cognitive social structures. We want to find out whether the best doctors, as determined by certain traits in the models, such as their credentials and abilities to conduct research, among others, get the most patients. In classical models, the best doctors would ideally have the best reputation because patients would know the best doctors and choose them accordingly to receive treatments. However, in the cognitive social system, the best doctors may not have the most favourable reputations because of social ties, which modulate the perception of their abilities and, thus, alter the judgements of patients choosing them. We implement a microbial genetic algorithm to optimize the agents in both models [Harvey2011]. We use this variation of the genetic algorithm because both of our models are relatively small in scale, and using a complete, conventional genetic algorithm may detract from the focus of our analysis. While a microbial genetic algorithm is more skeletal than a conventional one, it is more than adequate for our purposes in this paper. Ultimately, we are convinced that cognitive social systems can be generalized to understand other social systems, such as those found in areas such as education and defense, among others, and even design new ones. The potential of cognitive social system modelling to capture the dynamics of agents’ interactions embedded in social systems more accurately than conventional models is immense, as long as the modeller can rely on sound literature and verify the models with well-founded data."
https://arxiv.org/html/2411.05873v1,Poor Man’s Training on MCUs: A Memory-Efficient Quantized Back-Propagation-Free Approach,"Back propagation (BP) is the default solution for gradient computation in neural network training. However, implementing BP-based training on various edge devices such as FPGA, microcontrollers (MCUs), and analog computing platforms face multiple major challenges, such as the lack of hardware resources, long time-to-market, and dramatic errors in a low-precision setting. This paper presents a simple BP-free training scheme on an MCU, which makes edge training hardware design as easy as inference hardware design. We adopt a quantized zeroth-order method to estimate the gradients of quantized model parameters, which can overcome the error of a straight-through estimator in a low-precision BP scheme. We further employ a few dimension reduction methods (e.g., node perturbation, sparse training) to improve the convergence of zeroth-order training. Experiment results show that our BP-free training achieves comparable performance as BP-based training on adapting a pre-trained image classifier to various corrupted data on resource-constrained edge devices (e.g., an MCU with 1024-KB SRAM for dense full-model training, or an MCU with 256-KB SRAM for sparse training). This method is most suitable for application scenarios where memory cost and time-to-market are the major concerns, but longer latency can be tolerated.","On-device training, that is training deep neural networks (DNN) on edge devices, enables a DNN model pre-trained on cloud to improve itself on newly observed data and adapt to cross-domain or out-of-domain distribution shifts after edge deployment. It also allows the model to adapt to user personalization locally, which protects user privacy over sensitive data (e.g., healthcare and financial data). As physic-informed machine learning has been increasingly used for safety-critical decision-making in autonomous systems, there has been also growing interest in on-device fine-tuning or end-to-end training. In federated learning, a machine learning model also needs to be trained periodically on each local edge node, then updated on a global centralized server. Backward propagation (BP) [1] is used in almost all neural network training frameworks for gradient computation. BP is actually a reverse-mode automatic differentiation (AD) [2, 3] approach implemented based on the information of a computational graph. While a forward-mode AD is suitable for computing the gradient of a single-input multiple-out function, BP is more suitable for a multiple-input (i.e., many network parameters) and single-output (i.e., training loss) function. With sophisticated AD packages, operating systems, and compilers, BP can be called with just one command (e.g., loss.backward() in PyTorch) on a CPU- or GPU-based desktop or cloud computing platform. This has greatly simplified the development and deployment of modern neural network models. However, training a neural network on resource-constrained edge hardware [e.g., a microcontroller unit (MCU), FPGA or photonic platform] is completely different from the training task on a desktop or cloud platform, due to the limited hardware resources and software support. Specifically, implementing a standard BP-based training framework on edge devices are often prevented by three major challenges: • Memory Challenge. Edge devices like MCU have a very limited run-time memory (e.g., STM32F746 with only 256-KB user SRAM, or STM32H7B3 with 1024-KB user SRAM). This budget is often below the memory requirement of storing all network parameters, making full-model BP-based training impossible for most realistic cases. By choosing tailored network models (e.g., MCUNet [4]), using real-quantized graphs and a co-designed lightweight back-end (e.g., the TinyEngine [4, 5]), one may perform edge inference with a low memory cost (e.g., 96 KB for the MCUNet-in1 model [4]). However, the memory cost of a full-model BP-based training (e.g., 7.4 MB for MCUNet-in1) is far beyond the memory capacity. Existing training methods on MCU update only a small subset of model parameters (e.g., only the last layer [6, 7], bias vectors [8] to reduce the memory cost, yet leads to significant (e.g., ¿10%) accuracy drop. Sparse update [4, 9]) could narrow this gap, yet requires computation-intensive searches and compilation-level optimization on cloud. • Precision Challenge. Low-precision quantized computation is often utilized on digital edge hardware (e.g., MCU and FPGA) to reduce latency, memory cost, and energy consumption. However, low-precision operations pose great challenges for BP-based training. BP was originally designed for the gradient computation of smooth functions. Thus, error-prone approximation techniques such as straight-through estimators [10] are required to handle non-differentiable functions in quantized neural network training. The errors introduced by these approximation techniques increase as hardware precision reduces. They also propagate and accumulate through different layers, leading to dramatic accuracy drop, unstable training behaviors, or even divergence [11, 12]. • Time-to-Market Challenge. While BP can be done on CPU or GPU with just one line of code (e.g., loss.backward() in PyTorch), implementing it on edge devices can be very challenging. Due to the lack of automatic differentiation packages [2] and sophisticated operating systems on edge platforms, designers often have to implement the math and hardware of gradient computation manually. On some platforms (e.g., integrated photonics), novel devices must be invented and fabricated to perform BP [13]. This error-prone process needs numerous debugs and design trade-offs. As a result, designing edge training hardware is more time-consuming than designing inference hardware. For instance, our own experience shows that an experienced FPGA designer can design a high-quality inference accelerator within one week, yet it takes over one year to implement an error-free training accelerator on FPGA. This long time to market is often unacceptable in the industry due to the fast evolution of AI models. Paper Contributions. The above challenges motivate us to ask the following question: {mdframed} [userdefinedwidth=5.8inch, align=center] Can we make the edge training hardware design as easy and memory-efficient as inference hardware design? In this paper, we show that the answer is affirmative, with the assumption that memory budget and time to market are given higher priority over runtime latency. Our key idea is to completely bypass the complicated BP implementation by proposing a quantized zeroth-order (ZO) method to train a real-quantized neural network model on MCU. This training method only uses quantized forward evaluations to estimate gradients. As a result, we can use a similar memory cost of inference to achieve full-model training under the tiny memory budget of an MCU. This quantized ZO training framework can be used as a plug-and-play tool added to quantized inference hardware, therefore the design complexity and time to market can be dramatically reduced. Our specific contributions are briefly summarized below: 1. ZO Quantized Training for Edge Devices. We propose a BP-free training framework via quantized zeroth-order optimization to enable full-model and real-quantized training on MCUs under extremely low memory budget (e.g., 256-KB SRAM for sparse training or 1024-KB SRAM for dense training). This framework enjoys low memory cost and easy implementation. Furthermore, it shows better accuracy than quantized BP-based training in low-precision (e.g., INT8) settings since no error-prone straight-through estimator is needed. 2. Convergence Improvement. ZO training suffers from slow convergence rates as the number of training variables increases. Previous assumption of low intrinsic dimensionality [14] or coordinate-wise gradient estimation [15] does not work in on-device training. To improve the training convergence, we propose a learning-rate scaling method to stabilize each training step. We also employ a few dimension-reduction methods to improve the training convergence: (i) a generic layer-wise gradient estimation strategy that combines weight perturbation and node perturbation for ZO gradient estimation, (ii) a sparse training method with task-adaptive block selection to reduce the number of the trainable parameters. 3. MCU Implementation. We implement the proposed BP-free training framework on an MCU (full-model training on an STM32H7B3 with 1024-KB user SRAM, and sparse training on an STM32F746 with 256-KB user SRAM). A quantized inference engine is easily converted to a training engine, with only an additional control unit, a temporary gradient buffer, and a pseudo-random number generator. To our best knowledge, this is the first framework to enable full-model training under such a tiny memory budget (STM32H7B3 with 1024-KB user SRAM). 4. Experimental Validation. We conduct extensive experiments on adapting a pre-trained image classification model to unseen image corruptions and fine-grained vision classification datasets. On adapting image corruptions, our BP-free training outperforms current quantized BP-based training with an average 6.4% test accuracy improvement. Our method can also match the performance of back-propagation training on fine-grained vision classification datasets. Figure 1: (a): Overview of BP-free training framework. A quantized inference engine is easily converted to a training engine by adding control unit and repeatedly calling the inference accelerator. (b): Training memory comparison of different training methods. The numbers are measured with MCUNet-in1 [4], batch size 1, and resolution 128\times 128. Our Key idea is summarized in Fig. 1 (a). As demonstrated in Fig. 1 (b), our method is the only solution to enable full-model training on commodity-level MCU (e.g., STM32H7B3 with 1024-KB user SRAM) without auxiliary memory. This memory cost is the minimum to enable full-model training (478 KB model parameters plus 190 KB peak inference memory), 5.46\times more memory-efficient than the memory cost of quantized BP, and >400\times more memory efficient than BP-based training in PyTorch which includes back-end memory overhead. BP-free sparse training further reduces the memory cost to fit a smaller budget (e.g., 256-KB SRAM)."
