URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10037v1,Exact Computation of Error in Approximate Circuits using SAT and Message-Passing Algorithms,"Effective usage of approximate circuits for various performance trade-offs requires accurate computation of error. Several average and worst case error metrics have been proposed in the literature. We propose a framework for exact computation of these error metrics, including the error rate (ER), mean absolute error (MAE), mean squared error (MSE) and the worst-case error (WCE). We use a combination of SAT and message-passing algorithms. Our algorithm takes as input the CNF formula for the exact and approximate circuits followed by a subtractor that finds the difference of the two outputs. This is converted into a tree, with each vertex of the tree associated with a sub-formulas and all satisfying solutions to it. Once this is done, any probability can be computed by setting appropriate error bits and using a message passing algorithm on the tree. Since message-passing is fast, besides ER and MAE, computation of metrics like MSE is also very efficient. In fact, it is possible to get the entire probability distribution of the error. Besides standard benchmarks, we could compute the error metrics exactly for approximate Gaussian and Sobel filters, which has not been done previously.","Over the past decade, approximate circuits have gained traction as an effective method to trade off error for performance metrics like energy savings and frequency of operation in error tolerant applications. Computing the error in these circuits is an essential step towards determining the acceptability of the approximation. The system (S) used for error analysis consists of the exact and approximate circuits along with an error miter that models the desired error metric. In this paper, our focus is on exact computation of average and worst case error metrics that have been proposed in the literature. This includes the error rate (ER), mean absolute error (MAE), mean squared error (MSE) and the worst case error (WCE). Evaluation of these error metrics exactly is challenging since the outputs of both the exact and approximate circuits have to be known for all possible values of the inputs. In the past, methods used for exact error analysis include exhaustive enumeration, analysis based on binary and algebraic decision diagrams (BDD/ADD) and model counting (#SAT) based analysis. Exhaustive enumeration is infeasible for larger circuits. BDD based error analysis techniques have been proposed in [1, 2, 3, 4, 5]. In these methods, a BDD is constructed for the entire system S and traversal of the BDD is used to compute the error metrics. The miters used for various error metrics are included in [3]. An alternate method based on symbolic computer algebra and construction and traversal of ADDs has been proposed in [6, 7]. The advantage of their technique is that a single method can be used to get all error metrics, including relative errors. Methods proposed in [8, 9] use model counting for computation of certain error metrics. WCE analysis in particular can be performed effectively using SAT solvers [8, 3]. In [9], the authors propose circuit aware model counter VACSEM that integrates logic simulation into a #SAT solver GANAK [10] and use it to compute ER and MAE. I-A Motivation BDD based methods have shown limited scalability and we have not seen results for metrics like MAE and MSE for beyond 32-bit approximate adders. The #SAT solver VACSEM has proved to be efficient for computation of ER and MAE of up to 128-bit adders and 16-bit multipliers. Their method depends on partitioning the system into single output sub-miters. It does not allow for a straightforward extension to computing metrics that require a sat-count for a joint assignment of the error bits, as for example MSE. Moreover, each new error metric in VACSEM requires construction of the corresponding error miter, partitioning of the system based on the output of the error miter and re-synthesizing the partitions before model counting. In this respect, the one-method-fits-all proposed in [7] is attractive, but has shown limited scalability. I-B Contribution Our algorithm takes as input the CNF formula for the exact and approximate circuits followed by a subtractor that finds the difference of the two outputs. This is converted into a tree, with each vertex of the tree associated with a sub-formulas and all satisfying solutions to it. Once this is done, any probability can be computed by setting appropriate error bits and using a message passing algorithm on the tree. Since message-passing is fast, besides ER and MAE, computation of metrics like MSE is also very efficient. In fact, it is possible to get the entire probability distribution of the error. Besides standard benchmarks, we are able to compute metrics for 128 bit adders as well as approximate Gaussian and Sobel filters, with competitive runtimes."
https://arxiv.org/html/2411.10393v1,Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic Programs with Loops,"We study the problem of bounding the posterior distribution of discrete probabilistic programs with unbounded support, loops, and conditioning. Loops pose the main difficulty in this setting: even if exact Bayesian inference is possible, the state of the art requires user-provided loop invariant templates. By contrast, we aim to find guaranteed bounds, which sandwich the true distribution. They are fully automated, applicable to more programs and provide more provable guarantees than approximate sampling-based inference. Since lower bounds can be obtained by unrolling loops, the main challenge is upper bounds, and we attack it in two ways. The first is called residual mass semantics, which is a flat bound based on the residual probability mass of a loop. The approach is simple, efficient, and has provable guarantees.The main novelty of our work is the second approach, called geometric bound semantics. It operates on a novel family of distributions, called eventually geometric distributions (EGDs), and can bound the distribution of loops with a new form of loop invariants called contraction invariants. The invariant synthesis problem reduces to a system of polynomial inequality constraints, which is a decidable problem with automated solvers. If a solution exists, it yields an exponentially decreasing bound on the whole distribution, and can therefore bound moments and tail asymptotics as well, not just probabilities as in the first approach.Both semantics enjoy desirable theoretical properties. In particular, we prove soundness and convergence, i.e. the bounds converge to the exact posterior as loops are unrolled further. We also investigate sufficient and necessary conditions for the existence of geometric bounds. On the practical side, we describe Diabolo, a fully-automated implementation of both semantics, and evaluate them on a variety of benchmarks from the literature, demonstrating their general applicability and the utility of the resulting bounds.","Probabilistic programming Probabilistic programming is a discipline that studies programming languages with probabilistic constructs (Barthe et al., 2020). The term is overloaded however. At the intersection with randomized algorithms and program analysis, it usually means a programming language with a construct for probabilistic branching or sampling from probability distributions. As such, it is simply a language to express programs with random numbers and researchers study program analysis techniques for termination probabilities, safety properties, cost analysis, and others. At the intersection with statistics and machine learning, probabilistic programming is used to express (Bayesian) statistical models (van de Meent et al., 2018). Bayesian inference is a very successful framework for reasoning and learning under uncertainty: it updates prior beliefs about the world with observed data to obtain posterior beliefs using Bayes’ rule. As such, the programming languages for Bayesian models provide a construct for conditioning on data in addition to sampling from distributions. Since Bayesian inference is a difficult problem, a lot of research focuses on inference algorithms, in particular their correctness and efficiency. This paper contributes to both areas by developing methods to bound the distributions arising from probabilistic programs, especially those with loops. \displaystyle Throws:=0;Die:=0; \displaystyle{\color[rgb]{.5,0,.5}\mathsf{while}}\,{Die\neq 6}\,\allowbreak\{ \displaystyle\quad Die\sim{\color[rgb]{0,.5,.5}\mathsf{Uniform}}\{1\mathchar 4% 4\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]6\}; \displaystyle\quad{\color[rgb]{.5,0,.5}\mathsf{observe}}\,Die\in\{2\mathchar 4% 4\relax\nolinebreak[3]4\mathchar 44\relax\nolinebreak[3]6\}; \displaystyle\quad Throws\mathbin{{+}{=}}1\} Figure 1. A probabilistic program with a loop and conditioning. Example 1.1. To illustrate the concept, consider the following puzzle due to Elchanan Mossel. You throw a fair six-sided die repeatedly until you get a 6. You observe only even numbers during the throws. What is the expected number of throws (including the 6) conditioned on this event? This is a surprisingly tricky problem and most people get it wrong on the first try111In a survey on Gil Kalai’s blog, only 27% of participants chose the correct answer (https://gilkalai.wordpress.com/2017/09/07/tyi-30-expected-number-of-dice-throws/)., based on the incorrect assumption that it is equivalent to throwing a die with only the three faces 2, 4, and 6. Probability theory and statistics abound with such counterintuitive results (e.g. the Monty-Hall problem), and probabilistic programming offers a precise way to disambiguate their description and make them amenable to automatic analysis and inference tools. Mossel’s problem can be expressed as the probabilistic program in Fig. 1. The program has a loop that samples a die until it shows 6, and conditions on the number being even. In each iteration, the counter Throws is incremented. 1.1. Challenges Bayesian inference In Bayesian inference, Bayes’ rule is used to update prior distributions p(\theta) of model variables \theta with observed data x to obtain posterior distributions: p(\theta\mid x)=\frac{p(x\mid\theta)p(\theta)}{p(x)}. In practice, such Bayesian statistical models are too complex for manual calculations and inferring their posterior distribution is a key challenge in Bayesian statistics. There are two approaches: exact and approximate inference. Exact inference aims to find an exact representation of the posterior distribution. Such methods impose heavy restrictions on the supported probabilistic programs and do not usually scale well. Practitioners therefore mostly use approximate methods that do not aim to compute this distribution exactly, but rather to produce unbiased or consistent samples from it. If the probabilistic program does not contain conditioning, samples can simply be obtained by running the program. But with observations, program runs that violate the observations must be rejected. Since the likelihood of the observations is typically low, simple rejection sampling is inefficient, and thus practical samplers use more sophisticated techniques, such as Markov chain Monte Carlo. While more scalable, these approaches typically do not provide strong guarantees on the approximation error after a finite amount of time (Gelman et al., 2013, Section 11.5). Loops Loops are essential to the expressiveness of programming languages but notoriously hard to analyze. This applies even more strongly to the probabilistic setting, where deciding properties like termination is harder than in the deterministic setting (Kaminski and Katoen, 2015). Even if a program does not use conditioning, loops can still make sampling difficult. For example, a program may terminate almost surely, but its expected running time may be infinite. This prevents sampling-based approaches since they need to run the program. Furthermore, many inference algorithms are not designed to handle unbounded loops and may return erroneous results for such programs (Beutner et al., 2022). On the formal methods side, various approaches for probabilistic loop analysis have been proposed, employing techniques such as martingales, moments, and generating functions (see Section 7). If all program variables have finite support, the program can be translated to a probabilistic transition system and techniques from probabilistic model checking can be used. None of these analysis techniques can be applied to Example 1.1 however: methods from cost analysis do not support conditioning and probabilistic model checking requires finite support (but Throws is supported on \mathbb{N}). The approach by Klinkenberg et al. (2024) via generating functions is theoretically applicable, but requires the user to provide a loop invariant template, i.e. a loop invariant where certain parameters may be missing. Unfortunately, such an invariant cannot always be specified in their language (Klinkenberg et al., 2024, Example 25). Even in cases where this is possible, we argue that figuring out its shape is actually the hard part: it already requires a good understanding of the probabilistic program and its distribution, so it is not a satisfactory solution. 1.2. Guaranteed bounds To deal with the above challenges, we investigate guaranteed bounds on the program distribution. “Guaranteed” here refers to a method that computes deterministic (non-stochastic) results about the mathematical denotation of a program (Beutner et al., 2022). Such bounds are applicable more often than exact inference, e.g. in the presence of loops/recursion, and provide more assurance than approximate methods, which have at best stochastic guarantees. Why are such bounds useful? Partial correctness properties In quantitative program analysis, one can verify safety properties by bounding the probability of reaching an unsafe state. Bounding reachability probabilities is also a common problem in probabilistic model checking and quantitative program verification, yet it has not seen much attention in the context of probabilistic programming with conditioning, aside from the work by Beutner et al. (2022) and Wang et al. (2024). Neither of those can bound moments of infinite-support distributions, whereas our work finds tight bounds on the expected value of Throws in Fig. 1 (see Section 6.4). Figure 2. Histogram of samples from two inference algorithms (importance sampling and Pyro’s HMC), and the guaranteed bounds from Beutner et al. (2022). The bounds show that Pyro’s HMC produces wrong results. (Source: Beutner et al. (2022)) Checking approximate inference In the context of Bayesian inference, the bounds can be useful to check and debug approximate inference algorithms and their implementations. If the approximate results clearly contradict the bounds, the inference algorithm is likely incorrect, or some of its assumptions are violated, or it has not converged. Beutner et al. (2022) provide an example of this: the inference tool Pyro yields wrong results for a probabilistic program with loops, but their bounds can detect this issue (Fig. 2).222 The cause turned out to be an undocumented assumption in the inference algorithm. Pyro’s implementation seems to assume that the dimension (number of samples in a program run) of the problem is constant, which is violated when sampling inside probabilistic loops. Another problem with approximate inference is the tail behavior of the posterior distribution, which is often crucial for the quality of the approximation (Liang et al., 2023). Previous work on guaranteed bounds (Beutner et al., 2022; Wang et al., 2024) does not address this aspect, but our work can bound the tail behavior as well. Table 1. Comparison of our two approaches with the most relevant related work on probabilistic programs with loops. (Cond.: supports (Bayesian) conditioning; Inf.: branching on variables with infinite support is allowed; Cont.: continuous distributions allowed; Auto.: fully automated; Prob.: computes/bounds probabilities; Mom.: computes/bounds moments; Tails: computes/bounds tail asymptotics of this shape. Partial support is denoted by “\sim”.) Type Cond.? Inf.? Cont.? Auto.? Prob.? Mom.? Tails? Moosbrugger et al. (2022) exact ✗ ✗ ✓ ✓ \sim ✓ O(n^{-k}) Beutner et al. (2022) bounds ✓ ✓ ✓ ✓ ✓ ✗ ✗ Wang et al. (2024) bounds ✓ ✓ ✓ ✓ ✓ ✗ ✗ Klinkenberg et al. (2024) exact ✓ ✓ ✗ ✗ ✓ ✓ ✗ Resid. mass sem. (Section 3) bounds ✓ ✓ ✗ ✓ ✓ ✗ ✗ Geom. bounds (Section 4) bounds ✓ ✓ ✗ ✓ ✓ ✓ O(c^{n}) Problem Statement Given a probabilistic program with posterior distribution \mu on \mathbb{N}, our goal is to bound: (1) probability masses: given n\in\mathbb{N}, find l\mathchar 44\relax\nolinebreak[3]u\in[0\mathchar 44\relax\nolinebreak[3]1] such that l\leq\mathbb{P}_{X\sim\mu}[X=n]\leq u; (2) moments: given k\in\mathbb{N}, find l\mathchar 44\relax\nolinebreak[3]u\in\mathbb{\mathbb{R}}_{\geq 0} such that l\leq\mathbb{E}_{X\sim\mu}[X^{k}]\leq u; (3) tail asymptotics: find c\in[0\mathchar 44\relax\nolinebreak[3]1) such that \mathbb{P}_{X\sim\mu}[X=n]=O(c^{n}). 1.3. Contributions In this paper, we develop two new methods to compute guaranteed bounds on the distribution of discrete probabilistic programs with loops and conditioning. Lower bounds can simply be found by unrolling each loop a finite number of times. The main challenge is upper bounds and we attack it in two ways: the first is simple, always applicable, and efficient, but coarse; the second is more sophisticated and expensive, but yields much more informative bounds if applicable. A summary of the most relevant related work is presented in Table 1 and a detailed account in Section 7. The first semantics, called residual mass semantics (Section 3), is based on the simple idea of bounding the remaining probability mass after the loop unrollings, which has not previously been described, to our knowledge. We make the following contributions: • We introduce the residual mass as a simple but effective idea to bound posterior probabilities. • We prove soundness and convergence of the bounds to the true distribution (as loops are unrolled further and further). • We implement the semantics in a tool called Diabolo and demonstrate empirically that the implementation is more efficient than previous systems (Section 6.3). The second semantics, called geometric bound semantics (Section 4), is the main novelty of this paper. The idea is to bound the distribution of loops in a more fine-grained manner with geometric tails, rather than a flat bound as in the first semantics. • We present the concept of a contraction invariant for a loop, which yields upper bounds on the distribution (Section 4.1). • We introduce a family of distributions called eventually geometric distributions (EGDs) that are used as an abstract domain to overapproximate the distribution of a loop (Section 4.2). • We present the geometric bound semantics (Section 4.3) which reduces the synthesis problem of such an invariant to solving a system of polynomial inequalities. If successful, it immediately yields bounds on probability masses and, contrary to the first semantics, also on moments and tail probabilities of the program distribution. • We prove soundness of the semantics and convergence of the bounds, as loops are unrolled further and further (Section 4.5). • We identify necessary conditions and sufficient conditions for its applicability (Section 4.5). • We fully automate it in our tool Diabolo (Section 5): contrary to previous work (Klinkenberg et al., 2024), it does not rely on the user to provide a loop invariant (template). • We demonstrate its applicability on a large proportion of benchmarks from the literature and compare it to previous approaches and the residual mass semantics (Section 6). Full proofs and additional details can be found in Appendices A, B, C and D and Zaiser (2024b). 1.4. Limitations Our work deals with discrete probabilistic programs with hard conditioning. This means that programs cannot sample or observe from continuous distributions. Variables in our programming language take values in \mathbb{N}; negative numbers are not supported (see Section 8.1 for possible extensions). While our language is Turing-complete, some arithmetic operations like multiplication as well as some common infinite-support distributions (e.g. Poisson) are not directly supported (see Section 2.2 for details on our language’s expressivity). The initial values of the program variables are fixed: our methods cannot reason parametrically about these inputs. The residual mass semantics can yield bounds on the distribution of any such probabilistic program, but convergence with increasing unrolling is only guaranteed if the program terminates almost surely. If the program distribution has infinite support, we cannot bound the moments or tails: the bound does not contain enough information for this. The geometric bound semantics yields EGD bounds, which allow bounding moments and tails. On the other hand, such bounds do not exist for all programs. Our experiments show that this is not a big concern for many probabilistic programs with loops in practice: EGD bounds exist for a majority of examples we found in the literature. Another limitation of EGD bounds is that they cannot represent correlation of the tails of two variables, which may lead to imprecise tail bounds or failing to find bounds at all. Finally, solving the system of polynomial inequalities arising from the semantics, while decidable, can be hard in practice and does not scale to very large programs. It should be noted that scalability is a general issue in probabilistic program analysis owing to the hardness of the problem (Dagum and Luby, 1993) and not specific to our work. 1.5. Notation and conventions We use the Iverson brackets [\varphi] to mean 1 if \varphi is satisfied and 0 otherwise. We write variables representing vectors in bold ({\bm{\alpha}}), tensors (multidimensional arrays) in uppercase and bold (\mathbf{T}), and random or program variables in uppercase (X). We write \mathbf{0} and \mathbf{1} for the constant zero and one functions. We write \mathbf{0}_{n} and \mathbf{1}_{n} for the zero and one vectors in \mathbb{R}^{n}. To update the k-th component of a vector {\bm{\alpha}}, we write {\bm{\alpha}}[k\mapsto v]. Vectors {\bm{\alpha}}\in\mathbb{R}^{n} are indexed as \alpha_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak% [3]\alpha_{n}. We abbreviate [d]:=\{0\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3% ]d-1\}. Tensors \mathbf{T}\in\mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]} are indexed as \mathbf{T}_{i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax% \nolinebreak[3]i_{n}} where i_{k} ranges from 0 to d_{k}-1. We write \mathbf{0}_{[d_{1}]\times\dots\times[d_{n}]} or simply \mathbf{0} for the zero tensor in \mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]}. We write |\mathbf{T}|=(d_{1}\dots\mathchar 44\relax\nolinebreak[3]d_{n}) for the dimensions of \mathbf{T}\in\mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]}, and in particular |\mathbf{T}|_{i}=d_{i}. To index \mathbf{T} along dimension k, we write \mathbf{T}_{k:j}\in\mathbb{R}^{[d_{1}]\times\cdots\times[d_{k-1}]\times[d_{k+1% }]\times\cdots\times[d_{n}]}, which is defined by (\mathbf{T}_{k:j})_{i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44% \relax\nolinebreak[3]i_{k-1}\mathchar 44\relax\nolinebreak[3]i_{k+1}\mathchar 4% 4\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i_{n}}=\mathbf{T}_% {i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i% _{k-1}\mathchar 44\relax\nolinebreak[3]j\mathchar 44\relax\nolinebreak[3]i_{k+% 1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i_{n}}. We often write tensor indices as {\bm{i}}:=(i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax% \nolinebreak[3]i_{n}) for brevity. We also abbreviate {\bm{\alpha}}^{{\bm{i}}}:=\prod_{k=1}^{n}\alpha_{i}^{i_{k}}. Other binary operations (+, -, \min, \max, etc.) work elementwise on vectors and tensors, e.g. ({\bm{\alpha}}+{\bm{\beta}})_{j}:=\alpha_{j}+\beta_{j} and {\bm{\alpha}}\leq{\bm{\beta}} if and only if \alpha_{j}\leq\beta_{j} for all j."
https://arxiv.org/html/2411.09366v1,LTLf+ and PPLTL+: Extending LTLf and PPLTL to Infinite Traces,"We introduce LTLf+ and PPLTL+, two logics to express properties of infinite traces, that are based on the linear-time temporal logics LTLf and PPLTL on finite traces. LTLf+/PPLTL+ use levels of Manna and Pnueli’s LTL safety-progress hierarchy, and thus have the same expressive power as LTL. However, they also retain a crucial characteristic of the reactive synthesis problem for the base logics: the game arena for strategy extraction can be derived from deterministic finite automata (DFA). Consequently, these logics circumvent the notorious difficulties associated with determinizing infinite trace automata, typical of LTL reactive synthesis. We present DFA-based synthesis techniques for LTLf+/PPLTL+, and show that synthesis is 2EXPTIME-complete for LTLf+ (matching LTLf) and EXPTIME-complete for PPLTL+ (matching PPLTL). Notably, while PPLTL+ retains the full expressive power of LTL, reactive synthesis is EXPTIME-complete instead of 2EXPTIME-complete. The techniques are also adapted to optimally solve satisfiability, validity, and model-checking, to get EXPSPACE-complete for LTLf+ (extending a recent result for the guarantee level using LTLf), and PSPACE-complete for PPLTL+.","Reactive synthesis is concerned with synthesizing programs (aka, strategies) for reactive computations (e.g., processes, protocols, controllers, robots) in active environments (Pnueli and Rosner 1989; Finkbeiner 2016; Ehlers et al. 2017). The basic techniques for reactive synthesis share several similarities with Model Checking, and are based on the connections between Logics, Automata, and Games (Fijalkow et al. 2023). The most common specification language is possibly Linear Temporal Logic (LTL) (Pnueli 1977). Reactive Synthesis for LTL involves the following Steps: (1) having a specification \varphi of the desired system behavior in LTL, in which one distinguishes controllable and uncontrollable variables; (2) extracting from the specification an equivalent automaton on infinite words, corresponding to the infinite traces satisfying \varphi; (3) (differently from Model Checking) determinizing the automaton to obtain an arena for a game between the system and the environment; (4) solving the game, by fixpoint computation, for an objective determined by the automaton’s accepting condition (e.g., a parity objective for LTL), yielding a strategy for the system that fulfills the original specification \varphi. Model Checking is mature, and many of its techniques may be exploited in Reactive Synthesis as well, e.g., symbolic techniques based on Boolean encodings may be used to compactly represent the game arena and to compute fixpoints over it. However, despite this, Step (3) remains a major performance obstacle. For LTL, this involves determinizing nondeterministic Büchi automata, which is notoriously difficult (Vardi 2007). This has held back the use of reactive synthesis in applications. Reactive synthesis is deeply related to Planning (De Giacomo and Rubin 2018; Alberto, Bienvenu, and McIlraith 2019), and in particular to (strong) planning for temporally extended goals in fully observable nondeterministic domains (Cimatti et al. 2003; Bacchus and Kabanza 1998, 2000; Calvanese, De Giacomo, and Vardi 2002; Baier and McIlraith 2006; Baier, Fritz, and McIlraith 2007; Gerevini et al. 2009). A key characteristic of Planning is that the system continuously receives a goal, “thinks” about how to achieve it, synthesizes a plan, executes the plan, and repeats (Geffner and Bonet 2013). This suggests to focus on goal specifications that can be satisfied on finite traces. Recently, this led to a shift in Reactive Synthesis to focus on logics on finite traces (instead of infinite traces), e.g., LTLf (De Giacomo and Vardi 2013, 2015). The advantage of focusing on finite traces is that in Step (3) one can rely on (classic) automata operating on finite traces, including deterministic finite automata (DFA), and use known determinization algorithms with good practical performance. The development of LTLf synthesis (De Giacomo and Vardi 2015) has brought about scalable tools that are unprecedented in reactive synthesis (Zhu et al. 2017; Bansal et al. 2020; De Giacomo and Favorito 2021; De Giacomo et al. 2022). Beside LTLf, another finite-trace logic that is gaining popularity in AI is Pure Past LTL (PPLTL) (De Giacomo et al. 2020; Cimatti et al. 2020; Bonassi et al. 2023b, a, 2024). This is a variant of LTLf that sees the trace backwards and has the notable property that one can obtain a symbolic (i.e., factorized) DFA directly from the formula in linear time; moreover, while the size of the (non-symbolic) DFA corresponding to an LTLf formula can be double-exponential in the size of the formula itself, the size of the DFA corresponding to a PPLTL formula is at most a single-exponential in the size of the formula. Nevertheless, not all specifications of interest can be expressed on finite traces. For example, the planning domain is an infinite-trace specification: the planning domain will continue to respond to actions (with preconditions satisfied) by producing its possibly nondeterministic effects, forever. Not to mention recurrence, persistence, reactivity, and other properties typically used in Model Checking. When dealing with infinite traces, using (same variants of ) LTL as the specification language is an obvious choice. Can we lift the DFA techniques at the base of the success story of LTLf and PPLTL synthesis to full LTL? In this paper we answer this question positively! To do so, we leverage the classic hierarchy of LTL properties — the safety-progress hierarchy (Manna and Pnueli 1990).111 The hierarchy was introduced by Lichtenstein, Pnueli, and Zuck in 1985, later described in detail by Manna and Pnueli in 1990 and in their books (Manna and Pnueli 1992, 1995, 2010); also, see the survey (Piterman and Pnueli 2018). It consists of six classes of semantic properties, organized by inclusion. The bottom, first, level has, on the one hand, the safety properties (that intuitively express that nothing bad ever happens), and on the other the guarantee properties, sometimes also called co-safety properties, (that express that something good eventually happens); the second level consists of the class of obligation properties, obtained as positive Boolean combination of safety and guarantee properties; the third level contains, on the one hand, the recurrence properties (that express that something good occurs infinitely often), and on the other the persistence properties (that say that nothing bad occurs infinitely often); and the fourth level contains the reactivity properties, which are obtained as positive Boolean combination of recurrence and persistence properties. Each property is semantically defined in terms of sets of finite traces, e.g., a set F of finite-traces induces a basic safety (resp. progress) property that consists of an infinite trace iff every prefix (resp. all but finitely many prefixes) of the trace are in F. The reactivity properties contain all properties expressible in LTL.222The hierarchy is not limited to LTL, i.e., to properties that are expressible in first-order logic (FO) over infinite sequences (Kamp 1968), but extends to omega-regular properties, i.e., to monadic-second order logic (MSO) over infinite sequences. Indeed all the results we present here can be extended to omega-regular properties by substituting LTLf (resp. PPLTL) by its MSO-complete variant LDLf (resp. PPLDL) (De Giacomo and Vardi 2013). We revisit Manna and Pnueli’s hierarchy, and exploit it to define extensions of LTLf and PPLTL, which we call LTLf+ and PPLTL+, that can express arbitrary LTL properties on infinite traces. These new logics retain a crucial characteristic for reactive synthesis of their base logics: one can exploit the techniques for translating LTLf and PPLTL formulas into DFAs (De Giacomo and Vardi 2015; De Giacomo et al. 2020). These DFAs combine in a simple product to form the game arena for strategy extraction of LTLf+/PPLTL+ specifications. Naturally, the game objectives for LTLf+/PPLTL+ go beyond the simple adversarial reachability for LTLf/PPLTL. In particular, we exploit a variation of the Emerson-Lei condition (Emerson and Lei 1987) for handling Boolean combinations, and the possibility of translating these conditions into parity conditions (typically used for LTL) or to fixpoint computations (Hausmann, Lehaut, and Piterman 2024). We show that the worst-case complexity for synthesis of LTLf+ (resp. PPLTL+) is the same as for the base logics LTLf (resp. PPLTL), i.e., 2EXPTIME-complete (resp. EXPTIME-complete). The EXPTIME-complete result for synthesis in PPLTL+ is particularly interesting because, on the one hand, it shows that the exponential gap between PPLTL and LTLf (De Giacomo et al. 2020; Bonassi et al. 2023b) is maintained when extended to handle the full safety-progress hierarchy; and, on the other hand, it gives one a logic with the same expressive power as LTL but for which synthesis can be solved in EXPTIME instead of 2EXPTIME. Previous efforts to achieve reactive synthesis with exponential complexity focused on proper fragments of LTL (Arteche and Hermo 2024). We also adapt our DFA-based techniques and establish that reasoning — satisfiability, validity, and model-checking — for LTLf+ (resp. PPLTL+) is EXPSPACE-complete (resp. PSPACE-complete). The EXPSPACE-completeness result, which may appear surprising since satisfiability and model checking for LTL are both PSPACE-complete (Clarke et al. 2018), in fact confirms and extends a recent EXPSPACE-hardness result for model checking the fragment of LTLf+ limited to the guarantee class (Bansal et al. 2023). In other words, although LTLf+ defines infinite-trace properties using explicit reference to finite-trace properties defined in LTLf, it provides a computational advantage for synthesis but not for reasoning. Conversely, reasoning in PPLTL has the same cost as reasoning in LTL."
https://arxiv.org/html/2411.09121v1,": From Verification of Quantum Circuits
to Verification of Quantum Programs","We present a verifier of quantum programs called AutoQ 2.0. Quantum programs extend quantum circuits (the domain of AutoQ 1.0) by classical control flow constructs, which enable users to describe advanced quantum algorithms in a formal and precise manner. The extension is highly non-trivial, as we needed to tackle both theoretical challenges (such as the treatment of measurement, the normalization problem, and lifting techniques for verification of classical programs with loops to the quantum world), and engineering issues (such as extending the input format with a support for specifying loop invariants). We have successfully used AutoQ 2.0 to verify two types of advanced quantum programs that cannot be expressed using only quantum circuits: the repeat-until-success (RUS) algorithm and the weak-measurement-based version of Grover’s search algorithm. AutoQ 2.0 can efficiently verify all our benchmarks: all RUS algorithms were verified instantly and, for the weak-measurement-based version of Grover’s search, we were able to handle the case of 100 qubits in \sim20 minutes.","Quantum programs are an extension of quantum circuits that provide users with greater control over quantum computing by allowing them to use more complex programming constructs like branches and loops. Some of the most advanced quantum algorithms cannot be defined by quantum circuits alone. For example, certain class of programs, such as the repeat-until-success (RUS) algorithms [40] (which are commonly used in generating special quantum gates) and the weak-measurement-based version [7] of Grover’s search algorithm [30], use a loop with the condition being a classical value (0 or 1) obtained by measuring a particular qubit. This added expressivity presents new challenges, particularly in terms of verification. The additional complexity comes from the measurement operation, where a particular qubit is measured to obtain a classical value (and the quantum state is partially collapsed, which might require normalization), and reasoning about control flow induced by branches and loops. In classical program verification, a prominent role is played by deductive verification [29, 33, 31], represented, e.g., by the tools Dafny [36], KeY [5], Frama-C [9], VeriFast [35], VCC [22], and many more. These tools only require the users to provide specifications in the form of pre- and post-conditions, along with appropriate loop invariants. The rest of the proving process is entirely (in the ideal case) automated. Unfortunately, in the realm of quantum computing, similar fully automated deductive verification tools are, to the best of our knowledge, missing. Advanced tools for analysis and verification of quantum programs—based on, e.g., quantum Hoare logic and the tool CoqQ [49] or the path-sum formalism [6] and the tool Qbricks [15]—are quite powerful but require a significant amount of human effort. To bridge this gap, we present AutoQ 2.0, a major update over AutoQ 1.0 [19] with an added support for quantum programs (AutoQ 1.0 only supported quantum circuits). In AutoQ 1.0, given a triple \{P\}\,C\,\{Q\}, where P and Q are the pre- and post-conditions recognizing sets of (pure) quantum states (represented by tree automata) and C is a quantum circuit, we can verify if all quantum states in P reach some state in Q after executing C. In AutoQ 2.0, we addressed several key challenges to make the support of quantum programs possible. First, we need to handle branch statements. The key issue here is to handle measurement of quantum states whose value is used in a branch condition. For this we developed automata-based algorithms to compute the quantum states after the measurement (Section 5). The second challenge is the handling of loop statements. Similarly to deductive verification of classical programs, we require the users to provide an invariant for each loop. With the loop invariant provided, we developed a framework handling the rest of the verification process fully automatically. Moreover, we show that a naive implementation of the measurement operation will encounter the probability amplitude normalization problem. This is handled by designing a new algorithm for entailment testing (Section 6). Under this framework, the preconditions, postconditions, and invariants are all described using a new automata model called level-synchronized tree automata (LSTAs) [2]. LSTAs are specifically designed to efficiently encode quantum states and gate operations. As the core data structure of the tool, we provide a formal definition of LSTAs in Section 2.2 to facilitate the presentation of our new entailment testing approach. We used AutoQ 2.0 to verify various quantum programs using the repeat-until-success (RUS) paradigm [40], as well as the weak-measurement-based version [7] of Grover’s search [30] (Section 7). AutoQ 2.0 can efficiently verify all our benchmarks. The verification process for all RUS algorithms was instantaneous and for the weakly measured versions of Grover, we were able to handle the case of 100 qubits in \sim20 min. To the best of our knowledge, AutoQ 2.0 is currently the only tool for verification of quantum programs with such a degree of automation. Related work. Our work aligns with Hoare-style verification of quantum programs, a topic extensively explored in prior studies [50, 42, 47, 26, 38]. This approach, inspired by D’Hondt and Panangaden, utilizes diverse Hermitian operators as quantum predicates, resulting in a robust and comprehensive proof system [24]. However, specifying properties with Hermitian operators is often non-intuitive and difficult for automation due to their vast matrix sizes. Consequently, these methods are typically implemented using proof assistants like Coq [10], Isabelle [44], or standalone tools built on top of Coq, like CoqQ [49]. These tools require substantial manual effort in the proof search. The Qbricks approach [16] addresses the challenge of proof search by combining cutting-edge theorem provers with decision procedures, leveraging the Why3 platform [28]. Nevertheless, this approach still demands considerable human intervention. In the realm of automatic quantum software analysis tools, circuit equivalence checkers [6, 21, 32, 46, 22] prove to be efficient but less flexible in specifying desired properties, primarily focusing on equivalence. These tools are valuable in compiler validation, with notable examples being QCEC [14], Feynman [6], and SliQEC [18, 43]. Quantum model checking, supporting a rich specification language (various temporal logics [27, 39, 45]), is, due to its limited scalability, more suited for verifying high-level protocols [8]. QPMC [27] stands out as a notable tool in this category. Quantum abstract interpretation [48, 41] over-approximates the reachable state space to achieve better scalability, but so far handles only circuits. The work in [51, 25] aims at the verification of parameterized quantum programs like variational quantum eigensolver (VQE) or quantum approximate optimization algorithm (QAOA). However, the correctness properties they focused are very different from what AutoQ 2.0 can handle. While the mentioned tools are fully automated, they serve different purposes or address different phases of the development cycle compared to AutoQ 2.0."
https://arxiv.org/html/2411.08635v1,Synthesis with Privacy Against an Observer,"We study automatic synthesis of systems that interact with their environment and maintain privacy against an observer to the interaction. The system and the environment interact via sets I and O of input and output signals. The input to the synthesis problem contains, in addition to a specification, also a list of secrets, a function \mathsf{cost}:I\cup O\rightarrow\mathbb{N}, which maps each signal to the cost of hiding it, and a bound b\in\mathbb{N} on the budget that the system may use for hiding of signals. The desired output is an (I/O)-transducer T and a set {\mathcal{H}}\subseteq I\cup O of signals that respects the bound on the budget, thus \sum_{s\in{\mathcal{H}}}\mathsf{cost}(s)\leq b, such that for every possible interaction of \mathcal{T}, the generated computation satisfies the specification, yet an observer, from whom the signals in {\mathcal{H}} are hidden, cannot evaluate the secrets.We first show that the problem’s complexity is 2EXPTIME-complete for specifications and secrets in LTL, making it no harder than synthesis without privacy requirements. We then analyze the complexity further, isolating the two aspects that do not exist in traditional synthesis: the need to hide secret values and the need to choose the set {\mathcal{H}}. We do this by studying settings in which traditional synthesis is solvable in polynomial time – when the specification formalism is deterministic automata and when the system is closed – and show that each of these aspects adds an exponential blow-up in complexity. We continue and study bounded synthesis with privacy, where the input includes a bound on the synthesized transducer size, as well as a variant of the problem in which the observer has knowledge, either about the specification or about the system, which can be helpful in evaluating the secrets. Additionally, we study certified privacy, where the synthesis algorithm provides certification that the secrets remain hidden.","Synthesis is the automated construction of correct systems from their specifications [BCJ18]. While synthesized systems are correct, there is no guarantee about their quality. Since designers will be willing to give up manual design only after being convinced that the automatic process replacing it generates systems of comparable quality, it is extremely important to develop and study quality measures for automatically-synthesized systems. An important quality measure is privacy: making sure that the system and its environment do not reveal information they prefer to keep private. Privacy is a vivid research area in Theoretical Computer Science. There, the notion of differential privacy is used for formalizing when an algorithm maintains privacy. Essentially, an algorithm is differentially private if by observing its output, one cannot tell if a particular individual’s information is used in the computation [DN03, DMNS16]. Another related notion is obfuscation in system development, where we aim to develop systems whose internal operation is hidden [BGI+12, GGH+16]. Obfuscation has been mainly studied in the context of software, where it has exciting connections with cryptography [BGI+12, GGH+16]. In the setting of automated synthesis in formal methods, a very basic notion of privacy has been studied by means of synthesis with incomplete information [Rei84, KV00, CDHR06]. There, the system should satisfy its specification eventhough it only has a partial view of the environment. Lifting differential privacy to formal methods, researchers have introduced the temporal logic HyperLTL, which extends LTL with explicit trace quantification [CFK+14]. Such a quantification can relate computations that differ only in non-observable elements, and can be used for specifying that computations with the same observable input have the same observable output. The synthesis problem of HyperLTL is undecidable, yet is decidable for the fragment with a single existential quantifier, which can specify interesting properties [FHL+20b]. In [KL22], the authors suggested a general framework for automated synthesis of privacy-preserving reactive systems. In their framework, the input to the synthesis problem includes, in addition to the specification, also secrets. During its interaction with the environment, the system may keep private some of the assignments to the output signals, and it directs the environment which assignments to the input signals it should keep private. Consequently, the satisfaction value of the specification and secrets may become unknown. The goal is to synthesize a system that satisfies the specification yet keeps the value of the secrets unknown. Finally, lifting obfuscation to formal methods, researchers have studied the synthesis of obfuscation policies for temporal specifications. In [WRR+18], an obfuscation mechanism is based on edit functions that alter the output of the system, aiming to make it impossible for an observer to distinguish between secret and non-secret behaviors. In [DDM10], the goal is to synthesize a control function that directs the user which actions to disable, so that the observed sequence of actions would not disclose a secret behavior. In this paper we continue to study privacy-preserving reactive synthesis. As in [KL22], our setting is based on augmenting the specification with secrets whose satisfaction value should remain unknown. Unlike [KL22], the system and the environment have complete information about the assignments to the input and output signals, and the goal is to hide the secrets from a third party, and to do so by hiding the assignment to some of the signals throughout the interaction. As an example, consider a system that directs a robot patrolling a warehouse storage. Typical specifications for the system require it to direct the robot so that it eventually reaches the shelves of requested items, it never runs out of energy, etc. An observer to the interaction between the system and the robot may infer properties we may want to keep private, like dependencies between customers and shelves visited, locations of battery docking stations, etc. If we want to prevent the observer from inferring these properties (a.k.a,. the secrets), we have to hide the interaction from it. Different effort should be made in order to hide different components of the interaction (alarm sound, content of shelves, etc.). Our framework synthesizes a system that realizes the specification without the secrets being revealed, subject to restrictions on hiding of signals. As another example, consider a scheduler that should grant access to a joint resource. The scheduler should maintain mutual exclusion (grants are not given to different users simultaneously) and non-starvation (all requests are granted), while hiding details like waiting time or priority to specific users. In Examples 2.4 and 2.4, we describe in detail the application of our framework for the synthesis of such a scheduler, as well as its application in the synthesis of a robot that paints parts of manufactured pieces. The robot should satisfy some requirements about the generated pattern of colors while hiding other features of the pattern. Formally, we consider a reactive system that interacts with its environments via sets I and O of input and output signals. At each moment in time, the system reads a truth assignment, generated by the environment, to the signals in I, and it generates a truth assignment to the signals in O. The interaction between the system and its environment generates a computation. The system realizes a specification \varphi if all its computations satisfy \varphi [PR89]. We introduce and study the problem of synthesis with privacy in the presence of an observer. Given a specification \varphi, and secrets \psi_{1},\ldots,\psi_{k} over I\cup O, our goal is to return, in addition to a system that realizes the specification \varphi, also a set {\mathcal{H}}\subseteq I\cup O of hidden signals, such that the satisfaction value of the secrets \psi_{1},\ldots,\psi_{k} is unknown to an observer that does not know the truth values of the signals in {\mathcal{H}}. Thus, secrets are evaluated according to a three-valued semantics. The use of secrets enables us to hide behaviors, rather than just signals. 111Hiding of signals is a special case of our framework. Specifically, hiding of a signal p can be done with the secrets Fp and F\neg p. Obviously, hiding all signals guarantees that the satisfaction value of every secret is unknown. Hiding of signals, however, is not always possible or involves some cost. We formalize this by adding to the setting a function \mathsf{cost}:I\cup O\rightarrow{\mathbb{N}}, which maps each signal to the cost of hiding its value, and a bound b\in{\mathbb{N}} on the budget that the system may use for hiding of signals. The set {\mathcal{H}} of hidden signals has to respect the bound, thus \sum_{s\in{\mathcal{H}}}\mathsf{cost}(s)\leq b. In some cases, it is desirable to hide the truth value of a secret only when some condition holds. For example, we may require to hide the content of selves only in some sections of the warehouse. We extend our framework to conditional secrets: pairs of the form \langle\theta,\psi\rangle, where the satisfaction value of the secret \psi should be hidden from the observer only when the trigger \theta holds. In particular, when \theta=\psi, we require to hide the secret only when it holds. For example, we may require to hide an unfair scheduling policy only when it is applied. Note that a conditional secret \langle\theta,\psi\rangle is not equivalent to a secret \theta\rightarrow\psi or \theta\rightarrow\neg\psi, and that the synthesized system may violate the trigger, circumventing the need to hide the secret. For example, by synthesizing a fair scheduler, the designer circumvents the need to hide an unfair policy. We show that synthesis with privacy is 2EXPTIME-complete for specifications and secrets in LTL. Essentially, once the set {\mathcal{H}} of hidden signals is determined, we can compose an automaton for the specification with automata that verify, for each secret, that the assignments to the signals in (I\cup O)\setminus{\mathcal{H}} can be completed both in a way that satisfies the secret and in a way that does not satisfy it. A similar algorithm works for conditional secrets. While the complexity of our algorithm is not higher than that of LTL synthesis with no privacy, it would be misleading to conclude that handling of privacy involves no increase in the complexity. The 2EXPTIME complexity follows from the need to translate LTL specifications to deterministic automata on infinite words. Such a translation involves a doubly-exponential blow-up [KV05a, KR10], which possibly dominates other computational tasks of the algorithm. In particular, two aspects of synthesis with privacy that do not exist in usual synthesis are a need to go over all possible choices of signals to hide, and a need to go over all assignments to the hidden signals. Our main technical contribution is a finer complexity analysis of the problem, which reveals that each of the two aspects above involves an exponential complexity: the first in the number of signals and the second in the size of the secret. We start with the need to go over all assignments of hidden signals and show that even when the specification is \mathtt{T}, the set {\mathcal{H}} of hidden signals is given, and there is only one secret, given by a deterministic Büchi automaton, synthesis with privacy is EXPTIME-complete. This is exponentially higher than synthesis of deterministic Büchi automata, which can be solved in polynomial time. We continue to the need to go over all possible choices of {\mathcal{H}}. For that, we focus on the closed setting, namely when I=\varnothing, and the case the specification and secrets are given by deterministic automata. We show that while synthesis with privacy can be then solved in polynomial time for a given set {\mathcal{H}}, it is NP-complete when {\mathcal{H}} is not given, even when the function \mathsf{cost} is uniform. We continue and study three variants of the problem: bounded synthesis, certified privacy, and knowledgeable observer, briefly described below. One way to cope with the 2EXPTIME complexity of LTL synthesis, which is carried over to a doubly-exponential lower bound on the size of the generated system [Ros92], is bounded synthesis. There, the input to the problem includes also a bound on the size of the system [SF07, Ehl10, KLVY11]. In a setting with no privacy, the bound reduces the complexity of LTL synthesis to PSPACE, as one can go over all candidate systems. We study bounded synthesis with privacy and show that privacy makes the problem much harder: it is EXPSPACE-complete when the specification and secrets are given by LTL formulas, and is PSPACE-complete when they are given by deterministic parity (or Büchi) automata. The second variant we consider is one in which in addition to hiding the secret, the system has to generate a certificate for privacy. The certificate is an assignment to the hidden signals that differs from the assignment generated during the interaction and with which the secret has a different satisfaction value. This way, the user gets a witness computation that proves privacy is preserved. We examine the difference from the standard setting and specifically show that the need to generate certificates in an online manner makes specification and secrets harder to realize. Then, we provide a solution for synthesis with certified privacy and show that it is 2EXPTIME-complete and can be solved Safralessly (i.e., without determinization of a nondeterministic Büchi automaton) [KV05b]. Finally, recall that a system keeps a secret \psi private if an observer cannot reveal the truth value of \psi: every observable computation can be completed both to a computation that satisfies \psi and to a computation that does not satisfy \psi. We study a setting in which the observer has additional knowledge. First, we consider the setting in which the observer knows the specification \varphi of the system. Consequently, the observer knows that only completions that satisfy \varphi should be taken into account. If, for example, \varphi\rightarrow\psi, then \psi cannot be kept private. We describe an algorithm for this variant of the problem and analyze the way knowledge of the specification affects the complexity. In particular, we show that the problem becomes EXPTIME-complete even when the specification is given by a deterministic Büchi automaton and the secrets are of a fixed size. Then, we consider the even more demanding setting in which the observer knows the structure of the system. Consequently, the observer knows the set of possible computations of the system and thus may restrict to them when trying to evaluate the secret. We describe a procedure for checking whether a given (I/O)-transducer \mathcal{T} hides a given secret from an observer that knows \mathcal{T}. We leave the synthesis problem for this variant open and briefly explain why we suspect it is undecidable."
https://arxiv.org/html/2411.08421v1,Modest Sets are Equivalent to PERs,"The aim of this article is to give an expository account of the equivalence between modest sets and partial equivalence relations. Our proof is entirely self-contained in that we do not assume any knowledge of categorical realizability. At the heart of the equivalence lies the subquotient construction on a partial equivalence relation. The subquotient construction embeds the category of partial equivalence relations into the category of modest sets. We show that this embedding is a split essentially surjective functor, and thereby, an equivalence of categories. Our development is both constructive and predicative, and employs the language of homotopy type theory. All the mathematics presented in this article has been mechanised in Cubical Agda.","Kleene, in his seminal work [kleene-number-realizability] developed the technique of “recursive realizability” to show how to give a computational meaning to the logic of Brouwer’s intuitionism. The realizability relationship is written \mathtt{a}\Vdash x and is read “\mathtt{a} realizes x” or “\mathtt{a} is a realizer of x”. We want to think of the realizability relationship as capturing the intuition that \mathtt{a} is some kind of “machine code representation” of a mathematical object x. The discipline of categorical realizability emerged with Martin Hyland’s discovery of the effective topos [hyland-effective-topos]. Categorical realizability studies realizability toposes (such as the effective topos) and important subcategories thereof through the lens of categorical logic and topos theory. The effective topos is the “world of recursive realizability”. It massively generalises Kleene’s original realizability interpretation from Heyting arithmetic to topos logic. Additionally, it serves as a semantic playground for studying computability theory [bauer-synthetic-computability-theory] and domain theory “synthetically” [hyland-synthetic-domain-theory]. Recall that a theorem due to Freyd [freyd-abelian-categories] tells us that any small complete category in \mathsf{Sets} is necessarily posetal. This result extends to all Grothendieck toposes, so that, in any Grothendieck topos, a small complete category is necessarily posetal [shulman-small-complete-category-grothendieck] [gubkin-freyds-theorem-grothendieck-topos]. The category of partial equivalence relations (when arranged as an internal category in the effective topos), however, is a non-posetal small complete category [hyland-small-complete-category]. This fact is notable since it clearly highlights how realizability toposes differ from Grothendieck toposes. That this category is small and complete is also useful in the semantics of the polymorphic \lambda-calculus [longo-moggi-omega-set]. Both modest sets and partial equivalence relations capture the intuitive notion of a “data type”. It is fitting that they are equivalent. The aim of this article is to give a detailed and self-contained proof of the equivalence. Obviously, this fact is well-known to experts. It already shows up in Hyland’s articles on the effective topos [hyland-effective-topos, Proposition 7.2] and on PERs [hyland-small-complete-category, p. 151]. I claim no originality for the mathematics presented here. Mistakes and opinions are my own. Our development is completely constructive in that we do not assume the Law of Excluded Middle (LEM) [hott-book, Section 3.4] or the Axiom of Choice (AC) [hott-book, Section 3.8]. It is also predicative in that we do not assume the axiom of propositional resizing [hott-book, Axiom 3.5.5]. That said, experience suggests that any interesting applications of this equivalence require us to assume propositional resizing. Acknowledgments I have benefited greatly from many insightful discussions with Jon Sterling. Additionally, Jon explained to me the main idea behind the construction of the backwards direction of the isomorphism in Section 6.3. Tom de Jong and Ian Ray gave me feedback on an earlier draft of this article. Tom also taught me a LaTeX hack to get the \faCog symbol to behave well. I am grateful to them for the support and time they have given me."
https://arxiv.org/html/2411.08040v1,The Universal PDDL Domain,"In AI planning, it is common to distinguish between planning domains and problem instances, where a “domain” is generally understood as a set of related problem instances. This distinction is important, for example, in generalised planning, which aims to find a single, general plan or policy that solves all instances of a given domain. In PDDL, domains and problem instances are clearly separated: the domain defines the types, predicate symbols, and action schemata, while the problem instance specifies the concrete set of (typed) objects, the initial state, and the goal condition. In this paper, we show that it is quite easy to define a PDDL domain such that any propositional planning problem instance, from any domain, becomes an instance of this (lifted) “universal” domain. We construct different formulations of the universal domain, and discuss their implications for the complexity of lifted domain-dependent or generalised planning.","In AI planning, a distinction is often made between planning domains and problem instances, where a “domain” is intuitively understood to be a set, typically infinite, of related or similar problem instances. This concept is important in, for instance, planning with domain-specific control knowledge (?, ?, ?), and in generalised planning, which seeks a single, general plan or policy that solves all instances of a given domain (?). It is materialised in many modelling languages for specifying planning problems, such as PDDL (?), in which the domain and problem instance are syntactically separate. In PDDL, the domain definition contains types, and parameterised predicates and action schemata. The problem instance definition provides the concrete set of (typed) objects, the initial state and the goal condition. ? (?) and ? (?) argue that PDDL’s notion of domain is too weak, in that it does not always allow the modeller to explicitly state the constraints necessary to define precisely the intended set of problem instances, such as constraints on intended “valid” initial states and goals. Here, we will show that PDDL’s notion of domain is also in another sense too general: specifically, that it is possible (indeed, quite trivial) to define a domain such that any planning problem instance, of any domain, is an instance of this “universal” domain. There is, however, is caveat: While the universal domain is a parameterised PDDL domain, consisting of types, predicates and action schemata, instances of this domain are arbitrary propositional planning problems. This means that although any PDDL domain–problem pair can be turned into an instance of the universal domain, doing so requires grounding it, with the consequent potentially exponential increase in size. We will also argue that it is not possible to define a universal domain in PDDL such that any domain–problem pair can be expressed as an instance of this domain of a size that is polynomial in that of the domain–problem pair. (define (domain planning) (:types action proposition) (:predicates (pre ?a - action ?p - proposition) (add ?a - action ?p - proposition) (del ?a - action ?p - proposition) (true ?p - proposition)) (:action apply :parameters (?a - action) :precondition (forall (?p - proposition) (imply (pre ?a ?p) (true ?p))) :effect (and (forall (?p - proposition)(when (add ?a ?p) (true ?p)))(forall (?p - proposition)(when (and (del ?a ?p) (not (add ?a ?p))) (not (true ?p))))) ) ) Figure 1: The universal domain for propositional planning. (define (problem sussman) (:domain planning) (:objects ontable_A ontable_B ontable_C on_A_B on_A_C on_B_A on_B_C on_C_A on_C_B clear_A clear_B clear_C holding_A holding_B holding_C hand_empty - proposition pickup_A pickup_B pickup_C putdown_A putdown_B putdown_C stack_A_B stack_A_C stack_B_A stack_B_C stack_C_A stack_C_B unstack_A_B unstack_A_C unstack_B_A unstack_B_C unstack_C_A unstack_C_B - action) (:init (pre pickup_A ontable_A) (pre pickup_A clear_A) (pre pickup_A hand_emtpy) (add pickup_A holding_A) (del pickup_A ontable_A) (del pickup_A clear_A) (del pickup_A hand_empty) (pre pickup_B ontable_B) (pre pickup_B clear_B) (pre pickup_B hand_emtpy) (add pickup_B holding_B) (del pickup_B ontable_B) (del pickup_B clear_B) (del pickup_B hand_empty) (pre pickup_C ontable_C) (pre pickup_C clear_C) (pre pickup_C hand_emtpy) (add pickup_C holding_C) (del pickup_C ontable_C) (del pickup_C clear_C) (del pickup_C hand_empty) (pre putdown_A holding_A) (add putdown_A ontable_A) (add putown_A clear_A) (add putdown_A hand_empty) (del putdown_A holding_A) (pre putdown_B holding_B) (add putdown_B ontable_B) (add putown_B clear_B) (add putdown_B hand_empty) (del putdown_B holding_B) (pre putdown_C holding_C) (add putdown_C ontable_C) (add putown_C clear_C) (add putdown_C hand_empty) (del putdown_C holding_C) (pre stack_A_B holding_A) (pre stack_A_B clear_B) (add stack_A_B on_A_B) (add stack_A_B clear_A) (add stack_A_B hand_empty) (del stack_A_B holding_A) (del stack_A_B clear_B) (pre stack_A_C holding_A) (pre stack_A_C clear_C) (add stack_A_C on_A_C) (add stack_A_C clear_A) (add stack_A_C hand_empty) (del stack_A_C holding_A) (del stack_A_C clear_C) (pre stack_B_A holding_B) (pre stack_B_A clear_A) (add stack_B_A on_B_A) (add stack_B_A clear_B) (add stack_B_A hand_empty) (del stack_B_A holding_B) (del stack_B_A clear_A) (pre stack_B_C holding_B) (pre stack_B_C clear_C) (add stack_B_C on_B_C) (add stack_B_C clear_B) (add stack_B_C hand_empty) (del stack_B_C holding_B) (del stack_B_C clear_C) (pre stack_C_A holding_C) (pre stack_C_A clear_A) (add stack_C_A on_C_A) (add stack_C_A clear_C) (add stack_C_A hand_empty) (del stack_C_A holding_C) (del stack_C_A clear_A) (pre stack_C_B holding_C) (pre stack_C_B clear_B) (add stack_C_B on_C_B) (add stack_C_B clear_C) (add stack_C_B hand_empty) (del stack_C_B holding_C) (del stack_C_B clear_B) (pre unstack_A_B on_A_B) (pre unstack_A_B clear_A) (pre unstack_A_B hand_empty) (add unstack_A_B holding_A) (add unstack_A_B clear_B) (del unstack_A_B on_A_B) (del unstack_A_B clear_A) (pre unstack_A_C on_A_C) (pre unstack_A_C clear_A) (pre unstack_A_C hand_empty) (add unstack_A_C holding_A) (add unstack_A_C clear_C) (del unstack_A_C on_A_C) (del unstack_A_C clear_A) (pre unstack_B_A on_B_A) (pre unstack_B_A clear_B) (pre unstack_B_A hand_empty) (add unstack_B_A holding_B) (add unstack_B_A clear_A) (del unstack_B_A on_B_A) (del unstack_B_A clear_B) (pre unstack_B_C on_B_C) (pre unstack_B_C clear_B) (pre unstack_B_C hand_empty) (add unstack_B_C holding_B) (add unstack_B_C clear_C) (del unstack_B_C on_B_C) (del unstack_B_C clear_B) (pre unstack_C_A on_C_A) (pre unstack_C_A clear_C) (pre unstack_C_A hand_empty) (add unstack_C_A holding_C) (add unstack_C_A clear_A) (del unstack_C_A on_C_A) (del unstack_C_A clear_C) (pre unstack_C_B on_C_B) (pre unstack_C_B clear_C) (pre unstack_C_B hand_empty) (add unstack_C_B holding_C) (add unstack_C_B clear_B) (del unstack_C_B on_C_B) (del unstack_C_B clear_C) (true ontable_A) (true on_C_A) (true clear_C) (true ontable_B) (true clear_B)) (:goal (and (true on_A_B) (true on_B_C))) ) Figure 2: Example of a problem instance of the universal propositional planning domain."
https://arxiv.org/html/2411.07667v1,Formalization of physics index notation in Lean 4,"The physics community relies on index notation to effectively manipulate types of tensors. This paper introduces the first formally verified implementation of index notation in the interactive theorem prover Lean 4. By integrating index notation into Lean, we bridge the gap between traditional physics notation and formal verification tools, making it more accessible for physicists to write and prove results within Lean. We also open up a new avenue through which AI tools can be used to prove results related to tensors in physics. Behind the scenes our implementation leverages a novel application of category theory.","In previous work [1], the author initiated the digitalization (or formalization) of high energy physics results using the interactive theorem prover Lean 4 [2] in a project called HepLean. Lean is a programming language with syntax resembling traditional pen-and-paper mathematics. Users can write definitions, theorems, and proofs in Lean, which are then automatically checked for correctness using dependent-type theory. The HepLean project is driven by four primary motivations: (1) to facilitate easier look-up of results through a linear storage of information; (2) to support the creation and proof of new results using automated tactics and AI tools; (3) to facilitate checking of result in high energy physics for correctness; and (4) to introduce new pedagogical methods for high-energy physics and computer science. HepLean is part of a broader movement of projects to formalize parts, or all, of mathematics and science. The largest of these projects is Mathlib [3], which aims to formalize mathematics. Indeed, HepLean uses many results from Mathlib, and has Mathlib as an import. Other projects in mathematics include the ongoing effort led by Kevin Buzzard to formalize the proof of Fermat’s Last Theorem into Lean [4]. In the realm of the sciences, Bobbin et al. [5] looks at absorption theory, thermodynamics, and kinematics in Lean, whilst the package SciLean [6], is a push in the direction of scientific computing within Lean. Physicists rely heavily on specialized notation to express mathematical concepts succinctly. Among these index notation is particularly prevalent, as it provides a compact and readable way to represent specific types of tensors and operations between them. Such tensors form a backbone of modern physics. Having a way to use index notation in Lean is crucial for the digitalisation of results from high energy physics. In addition to making results from high energy physics easier to write and prove in Lean, it will make the syntax more familiar to high energy physicists. However, there are challenges in implementing index notation in Lean, namely, the need for a formal and rigorous implementation that is also easy and nice to use. Such an implementation can now be found as part of HepLean: https://heplean.github.io/HepLean/ and is the subject of this paper. We hope that the implementation presented here will not only enhance usability of Lean but also promotes the adoption of formal methods in the physics community. To give an example of the up-shot of our implementation, the result regarding Pauli matrices that \sigma^{\nu\alpha\dot{\beta}}\sigma_{\nu}^{\alpha^{\prime}\dot{\beta}^{\prime}% }=2\epsilon^{\alpha\alpha^{\prime}}\epsilon^{\beta\beta^{\prime}} is written in Lean, using our implementation, as follows: Lean will correctly interpret this result as a tensor expression with the correct contraction of indices and permutation of indices between each side of the expression. Our implementation can handle different species of tensors, for example real Lorentz tensors, complex Lorentz tensors and ordinary tensors (e.g., vectors and matrices). That said, at the time of writing only the most involved, complex Lorentz tensors, have been implemented. Previous implementations of index notation have been made in programming languages like Haskell [7]. However, the programs they appear in do not provide the formal verification capabilities inherent in Lean. The formal verification requirement of Lean, and the need to cover different species of tensors, introduces unique challenges in implementing index notation, necessitating (what we believe is) a novel solution. In Section 2 of this paper, we will discuss the details of our implementation. In Section 3 we will give two examples of definitions, theorems and proofs in Lean using index notation to give the reader an idea of how our implementation works in practice. The first of these examples involves a lemma regarding the contraction of indices of symmetric and antisymmetric tensors. The second involves examples related to the Pauli matrices and bispinors. We finish this paper in Section 4 by discussing potential future work related to this project. Notation In this paper, we will follow Lean’s notation for types and terms. For example if C is a type (which can be thought of as similar to a set), then c : C says that c is an element of the type C. In addition, instead of having two streams of notation for mathematical objects, one from the Lean code and one in LaTeX, we will use Lean code throughout to represent mathematical objects. Throughout Section 2, we will assume a basic knowledge of the theory of symmetric monoidal categories."
https://arxiv.org/html/2411.07431v1,"Continuous Domains for Function Spaces Using Spectral
Compactification","We introduce a continuous domain for function spaces over topological spaces which are not core-compact. Notable examples of such topological spaces include the real line with the upper limit topology, which is used in solution of initial value problems with temporal discretization, and various infinite dimensional Banach spaces which are ubiquitous in functional analysis and solution of partial differential equations. If a topological space \mathbb{X} is not core-compact and \mathbb{D} is a non-singleton bounded-complete domain, the function space [\mathbb{X}\to\mathbb{D}] is not a continuous domain. To construct a continuous domain, we consider a spectral compactification \mathbb{Y} of \mathbb{X} and relate [\mathbb{X}\to\mathbb{D}] with the continuous domain [\mathbb{Y}\to\mathbb{D}] via a Galois connection. This allows us to perform computations in the native structure [\mathbb{X}\to\mathbb{D}] while computable analysis is performed in the continuous domain [\mathbb{Y}\to\mathbb{D}], with the left and right adjoints used for moving between the two function spaces.","The tight link between topology and the theory of computation is well-known and has been investigated extensively in the literature. This link is clearly manifested in the theory of domains [18], which have, in particular, provided a natural computational framework for mathematical analysis. This line of research was initiated by Edalat’s work on dynamical systems [4]. Ever since, domains have been used for the study of several other concepts and operators of mathematical analysis, e. g., exact real number computation [12, 5], differential equation solving [10], stochastic processes [2], reachability analysis of hybrid systems [9, 19], and robustness analysis of neural networks [23]. In such applications, when the topological spaces involved have some desirable properties (e. g., metrizability, local compactness, etc.) the construction of the domain model can be relatively straightforward. In the absence of favourable properties, however, domain models do not arise naturally and one may look for substitute constructions, an example of which can be found in [15] for robustness analysis of systems with state spaces that are not (locally) compact. Another example is encountered in the solution of initial value problems (IVPs). For the Picard method of IVP solving, continuous domains of functions arise naturally [8, 10, 14]. The situation is slightly different for the methods that are based on temporal discretization (e. g., Euler and Runge-Kutta methods). While it is still possible to use classical domain models when an imperative style of computation is adopted [7], a functional implementation via the fixpoint operator requires a substitute domain construction [6]. Let us discuss this in more detail. Assume that the following IVP is given: \left\{\begin{array}[]{r@{\hspace{0.5ex}=\hspace{0.5ex}}l}y^{\prime}(t)\hskip 2% .15277pt=\hskip 2.15277pt&f(y(t)),\\ y(t_{0})\hskip 2.15277pt=\hskip 2.15277pt&y_{0},\\ \end{array}\right. (1) in which t_{0}\in\mathbb{R}, y_{0}\in\mathbb{R}^{n}, and f:\mathbb{R}^{n}\to\mathbb{R}^{n} is a continuous vector field, for some natural number n\geq 1. Assume that a solution exists over a lifetime of [t_{0},T], for some T>t_{0}. In a domain-theoretic framework, one would search for a solution of (1) in the space of functions from [t_{0},T] to the interval domain: {\mathbb{I}}\mathbb{R}^{n}_{\bot}\coloneqq\left\{{\mathbb{R}^{n}}\right\}\cup% \left\{{\prod_{i=1}^{n}[a_{i},b_{i}]}\;\vrule\;{\forall i\in\left\{{1,\ldots,n% }\right\}:a_{i},b_{i}\in\mathbb{R}\text{ and }a_{i}\leq b_{i}}\right\}, (2) ordered by superset relation, i. e., \forall X,Y\in{\mathbb{I}}\mathbb{R}^{n}_{\bot}:X\sqsubseteq Y\stackrel{{% \scriptstyle\vartriangle}}{{\iff}}X\supseteq Y. Hence, the set \mathbb{R}^{n} is the least element of the interval domain {\mathbb{I}}\mathbb{R}^{n}_{\bot}. For applications such as differential equation solving, the interval domain {\mathbb{I}}\mathbb{R}^{n}_{\bot} is considered with the Scott topology. What is equally important is the topology on the interval [t_{0},T]. For the Picard method, the Euclidean topology on [t_{0},T] is the suitable topology. As the Euclidean topology over [t_{0},T] is locally compact, the space of functions from [t_{0},T] to {\mathbb{I}}\mathbb{R}^{n}_{\bot}—which are continuous with respect to the Euclidean topology on [t_{0},T] and the Scott topology on {\mathbb{I}}\mathbb{R}^{n}_{\bot}—ordered by pointwise ordering is a continuous domain. The Euclidean topology, however, is not suitable in a functional framework in the presence of temporal discretization. To see this, note that, by integrating both sides of (1), we obtain y(t+h)=y(t)+\int_{t}^{t+h}f(y(\tau))\thinspace\mathrm{d}\tau, for all t\in[t_{0},T] and h\in[0,T-t]. This can be written as: y(t+h)=y(t)+i(t,h), (3) in which the integral i(t,h) represents the dynamics of the solution from t to t+h. Thus, a general schema for validated solution of the IVP (1) with temporal discretization may be envisaged as follows: (i) For some k\geq 1, consider the partition Q=(q_{0},\ldots,q_{k}) of the interval [t_{0},T]. (ii) Let Y(t_{0})\coloneqq y_{0}. (iii) For each j\in\left\{{0,\ldots,k-1}\right\} and h\in(0,q_{j+1}-q_{j}]: Y(q_{j}+h)\coloneqq Y(q_{j})+I(q_{j},h), (4) where I(q_{j},h) is an interval enclosure of the integral factor i(q_{j},h) from equation (3). The operator I, in general, depends on several parameters, including (enclosures of) the vector field and its derivatives, the enclosure Y(q_{j}), the index j, etc. In (4), the operator ‘+’ denotes interval addition, and for the method to be validated, the term I(q_{j},h) must account for all the inaccuracies, e. g., floating-point error, truncation error, etc. In step (iii) of the schema, the solver moves forward in time, from q_{j} to q_{j+1}. This requires keeping the state, i. e., the solution up to the partition point q_{j}, and referring to this state in iteration j. As such, the schema has an imperative style, and indeed encompasses various validated approaches to IVP solving with temporal discretization in the literature, including [7]. This is in contrast with the functional style adopted in the definition of the Picard operator in [8, 10], and in language design for real number computation. For instance, the languages designed in [12, 13, 3] for computation over real numbers and real functions are functional languages based on lambda calculus, with their denotational semantics provided by domain models. In a functional framework, the solution of the IVP (1) is obtained as the fixpoint of a higher-order operator. Domain models are particularly suitable for fixpoint computations of this type. A straightfoward (but, flawed) way of obtaining a fixpoint formulation for the above general schema is to define a functional \Phi over interval functions as follows: \Phi(Y)(x)\coloneqq\left\{\begin{array}[]{ll}y_{0},&\text{if }x=t_{0},\\ Y(q_{j})+I(q_{j},x-q_{j}),&\text{if }q_{j}<x\leq q_{j+1}.\end{array}\right. The fixpoint of this operator (if it exists) will be the right choice. The problem is that, the enclosures obtained by applying \Phi do not have upper (respectively, lower) semi-continuous upper (respectively, lower) bounds, even if the initial enclosure has continuous bounds. This situation is illustrated in Fig. 1, where the true solution of the IVP is drawn in magenta color. We begin with an initial enclosure of the solution drawn in green color, i. e., the outer solid curves. As can be seen, the upper and lower bounds of this initial enclosure are continuous. After applying \Phi once, we obtain the first approximation of the solution, with piecewise affine upper (in solid black) and lower (in solid blue) bounds. As can be seen at points q_{0}, q_{1}, q_{2}, and q_{3}, the upper bound is not upper semi-continuous, and the lower bound is not lower semi-continuous. If we apply the operator \Phi a second time, we obtain a tighter (i. e., more accurate) enclosure of the solution, which is drawn in dashed lines. But these bounds also have the same problem with semi-continuity, i. e., the upper bound is not upper semi-continuous, and the lower bound is not lower semi-continuous. Hence, by [6, Proposition 2.10], this results in approximations of the solution of the IVP which are not continuous with respect to the Euclidean topology on [t_{0},T], and can only be continuous with respect to the so-called upper limit topology. Recall that the upper limit topology has as its base the collection \{{(a,b]}\mid{a,b\in\mathbb{R}}\} of half-open intervals. This topology is known not to be locally compact (see, e. g., [6, Proposition 4.5]). This shortcoming motivated the substitute construction presented in [6], where a more detailed justification of why the upper limit topology is needed can also be found, together with fixpoint formulations of Euler and Runge-Kutta operators. tyq_{0}q_{1}q_{2}q_{3}q_{4}•\circ••\circ\circ•\circ•\circ•\circ•\circ•\circ• Figure 1: The correct semi-continuity of the bounds of the successive approximations cannot be guaranteed, even when the process starts from an initial enclosure with continuous bounds (in green). The curve in magenta depicts the true solution, and the black (respectively, blue) piecewise affine curves represent the upper (respectively, lower) bounds of the enclosures obtained from applying \Phi once (solid) and twice (dashed). In a more general setting, assume that \mathbb{X}\equiv(X,\tau_{\mathbb{X}}) is a T_{0} topological space, and \mathbb{D} is a non-singleton bounded-complete domain (bc-domain). From [11], we know that the function space [\mathbb{X}\to\mathbb{D}] is a bc-domain if and only if \mathbb{X} is core-compact. Thus, the main challenge is to develop a computational framework for the function space [\mathbb{X}\to\mathbb{D}] when \mathbb{X} is not core-compact, e. g., when \mathbb{X} is the interval [t_{0},T] endowed with the upper limit topology, and \mathbb{D}={\mathbb{I}}\mathbb{R}^{n}_{\bot} with the Scott topology. In [6], a domain is constructed as a substitute for this function space via abstract bases. In contrast with [6], the aim of the current article is to show that we can work directly on the space \mathbb{X} and obtain the same substitute via Stone duality. To be more specific, using the well-known results in Stone duality, we construct a topological space \hat{\mathbb{X}} with the following properties: • \hat{\mathbb{X}} is a core-compact (in fact, spectral) space and \mathbb{X} can be embedded into \hat{\mathbb{X}} as a dense subspace. • The function spaces [\mathbb{X}\to\mathbb{D}] and [\hat{\mathbb{X}}\to\mathbb{D}] are related via a Galois connection. Such a construction is useful for computable analysis within a domain framework. When \mathbb{X} is not core-compact, the non-continuous directed-complete partial order (dcpo) [\mathbb{X}\to\mathbb{D}] is used for implementation of algorithms, whereas analysis of computability is carried out over the continuous domain [\hat{\mathbb{X}}\to\mathbb{D}], subject to the existence of a suitable effective structure over [\hat{\mathbb{X}}\to\mathbb{D}]. 1.1 Related Work Compactification is a fundamental concept in topology. Classical examples such as Stone-Čech and one-point compactification [20] have been introduced primarily for Hausdorff topological spaces. In the non-Hausdorff setting, Smyth’s stable compactification [21] is the closest to ours. In fact, our construction can be obtained as a special case of Smyth’s stable compactification by considering the so-called fine quasi-proximities, resulting in compactifications that are spectral. In [21], this special case is refered to as spectralization, whereas we use the term spectral compactification to emphasize the compactification aspect of the construction. Spectral compactification is indeed an important special case of stable compactification which is suitable for computational purposes. Here, we keep the presentation simple and do not use quasi-proximities which form the foundation of Smyth’s construction. We obtain all the basic properties that we need in this simpler framework. We point out that spectral compactification is fundamentally different from Stone-Čech and one-point compactifications in that, even when a space \mathbb{X} is compact and Hausdorff, its spectral compactification may not be T_{1} (for an example, see [21, page 338]). Another aspect of our work here is the idea of a substitute construction. Such constructions can be useful when the topological spaces do not have favourable properties. In [6], we used the idea of a substitute construction in the context of IVP solving. Another example is presented in [15], in the context of robustness analysis. In [15], we studied robustness analysis of systems with state spaces \mathbb{S} which are not locally compact. In such cases, the lattice \hat{\mathbb{C}}(\mathbb{S}) of closed subsets of \mathbb{S} (under superset relation) may not be continuous, let alone \omega-continuous. The lattice of closed subsets is central to robustness analysis. Hence, we construct an \omega-continuous lattice \mathbb{L} which is related to \hat{\mathbb{C}}(\mathbb{S}) via a suitable adjunction. 1.2 Structure of the Paper The preliminaries, including a brief reminder of basic concepts from domain theory and Stone duality, are presented in Section 2. In Section 3, we establish a Galois connection between the function spaces [\mathbb{X}\to\mathbb{D}] and [\mathbb{Y}\to\mathbb{D}], where \mathbb{D} is a bc-domain, \mathbb{X} and \mathbb{Y} are topological spaces, and \mathbb{X} is densely embedded in \mathbb{Y}. A detailed account of the spectral compactification of topological spaces is presented in Section 4. A continuous domain for the space of functions from an arbitrary T_{0} space \mathbb{X} to a bc-domain \mathbb{D} is constructed in Section 5 based on spectral compactification of \mathbb{X}, and we prove that the result is equivalent to the construction based on abstract bases developed in [6]. We conclude the article with some remarks in Section 6."
https://arxiv.org/html/2411.07966v1,"Feasibly Constructive Proof of Schwartz-Zippel Lemma 
and the Complexity of Finding Hitting Sets","The Schwartz-Zippel Lemma states that if a low-degree multivariate polynomial with coefficients in a field is not zero everywhere in the field, then it has few roots on every finite subcube of the field. This fundamental fact about multivariate polynomials has found many applications in algorithms, complexity theory, coding theory, and combinatorics. We give a new proof of the lemma that offers some advantages over the standard proof.First, the new proof is more constructive than previously known proofs. For every given side-length of the cube, the proof constructs a polynomial-time computable and polynomial-time invertible surjection onto the set of roots in the cube. The domain of the surjection is tight, thus showing that the set of roots on the cube can be compressed. Second, the new proof can be formalised in Buss’ bounded arithmetic theory {\mathsf{S^{1}_{2}}} for polynomial-time reasoning. One consequence of this is that the theory {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}) for approximate counting can prove that the problem of verifying polynomial identities (PIT) can be solved by polynomial-size circuits. The same theory can also prove the existence of small hitting sets for any explicitly described class of polynomials of polynomial degree.To complete the picture we show that the existence of such hitting sets is equivalent to the surjective weak pigeonhole principle \mathsf{dWPHP}({\mathsf{PV}}), over the theory \mathsf{S^{1}_{2}}. This is a contribution to a line of research studying the reverse mathematics of computational complexity (cf. Chen-Li-Oliveira, FOCS’24). One consequence of this is that the problem of constructing small hitting sets for such classes is complete for the class APEPP of explicit construction problems whose totality follows from the probabilistic method (Kleinberg-Korten-Mitropolsky-Papadimitriou, ITCS’21; cf. Korten, FOCS’21). This class is also known and studied as the class of Range Avoidance Problems (Ren-Santhanam-Wang, FOCS’22).","We study constructive proofs of the well-known Schwartz-Zippel Lemma, and their applications in complexity theory. This statement is sometimes referred to as the “Schwartz-Zippel-DeMillo-Lipton Lemma” following [47, 42, 15]. However, this result goes back to Øystein Ore, 1922 [34] (over finite fields) and was subsequently rediscovered by other authors; see [3] who unearthed the history of this lemma. Due to this complicated history, it is sometimes called “The Polynomial Identity Lemma”. Theorem 1.1 (Schwartz-Zippel Lemma). Let \mathbb{F} be a field, let \overline{x} be a set of n indeterminates, let S\subseteq\mathbb{F} be a finite subset of field elements, and let P(\overline{x}) be a multivariate polynomial in the indeterminates \overline{x} with coefficients in the field \mathbb{F} and maximum individual degree at most d. Then, either every point in \mathbb{F}^{n} is a root of P(\overline{x}), or the number of roots in S^{n} is at most d\cdot n\cdot|S|^{n-1}. In particular, the number of roots in S^{n} is either |S|^{n} or at most d\cdot n\cdot|S|^{n-1}.111Our version of the lemma is for maximum individual degree and is closely related to Zippel’s version of the lemma in [47]. Zippel’s bound, also for maximum individual degree, is slightly tighter than the one stated here, namely: |S|^{n}\cdot(1-(1-d/|S|)^{n})\leq d\cdot n\cdot|S|^{n-1}. There is also a better known version of the lemma for total degree D, where the bound is D\cdot|S|^{n-1}. Note that d\leq D\leq d\cdot n, so the bound for total degree is never further than a factor of n from our bound. In the regime where |S| is bigger than n and d, which is the setting of most applications, all bounds are equally useful and yield similar conclusions. See [30] and the discussion there, for a comparison of the bounds, and for a discussion on how tight they are. The Schwartz-Zippel Lemma is a cornerstone in randomised algorithms and the use of randomness in computing, with a wide range of applications in computational complexity, coding theory, graph algorithms, and algebraic computation. The lemma shows that evaluating a non-zero polynomial on inputs chosen randomly from a large enough set is likely to find an input that produces a non-zero result. This offers a fast test with good guarantees for checking if a polynomial is identically zero. While the lemma provides significant efficiency advantages in its applications, it is based on an existential statement—the existence of many non-roots for non-zero polynomials—without offering a deterministic way to find these non-roots or easily witness their absence. Accordingly, the standard textbook proof of the lemma, which goes by induction on the number of variables, although simple, does not reveal a feasible constructive argument. Specifically, it treats multivariate polynomials as potentially exponential sums of monomials. This non-constructive nature of the lemma is one reason why providing feasible constructive proofs has been challenging (cf. [28]). Although different in nature, feasible constructivity in proofs and algorithms often go hand in hand, either informally or, at times, formally through translation theorems between proofs and computation. This work presents a new proof of the Schwartz-Zippel Lemma that fits within the framework of feasibly constructive proofs in a precise manner. We then demonstrate several applications of this proof, both in feasible constructive mathematics and in computational complexity, as we explain in the following. Organisation of the introduction. In Section 1.1 we discuss the motivation behind seeking feasibly constructive proofs in general, and specifically the approach taken here to carry out the new proof within the formal logic framework of bounded arithmetic, which are formal theories accompanying and complementing the development of complexity theory. In Section 1.2 we describe (in the “meta-theory”) the new feasibly constructive proof of Schwartz-Zippel. For those interested to first see the new proof, it is presented at the end of Section 1.2. We precede it with an exposition that aims to explain the intuition behind the new proof. In Section 1.3 we discuss applications of the new proof to bounded arithmetic, where our new argument helps to settle the open problem of formalising the Schwartz-Zippel Lemma in bounded arithmetic. Specifically, in Section 1.3.2 we discuss how to prove the existence of small hitting sets and hence formalise Polynomial Identity Testing (PIT) in the theory. In Section 1.3.3 we describe a “reversal” theorem, in which the dual weak pigeonhole principle—namely, the statement that a function cannot map surjectively a small domain onto a large range—is shown to be equivalent to the existence of small hitting sets. And in Section 1.3.4 we show that this reversal theorem implies that finding hitting sets is complete for the class of Range Avoidance Problems (aka {\mathsf{APEPP}}). 1.1 The Quest for Feasibly Constructive Proofs While existence proofs, particularly those establishing the presence of certain combinatorial objects, are very useful—for instance, in probabilistic combinatorics, where one seeks to identify objects with certain properties that are as large or small as possible, such as expanders or combinatorial designs—it is widely acknowledged that constructive arguments, which provide explicit methods for constructing these objects, are often even more fruitful. This is especially significant in algorithmic contexts, where the utility of the objects (e.g., expanders) depends on their “explicitness”, meaning that they must be feasibly computable, typically in polynomial time. In computational complexity theory, the notions of constructivity and explicitness are equally critical. Fundamental questions about separating complexity classes hinge on explicit languages, such as the satisfiability problem (SAT), and whether SAT can be solved by polynomial-size circuits. For random, non-explicit languages, the analogous problems are almost trivially resolved: a simple counting argument shows that there exist non-explicit Boolean functions that cannot be computed by Boolean circuits of polynomial-size. A related but distinct approach to feasible constructivity in complexity theory is found in the framework of bounded arithmetic, which is the field that studies the computational complexity of concepts needed to prove different statements. In this setting, one aims to formalise constructivity in a more rigorous and systematic manner. Bounded arithmetic is a general name for a family of weak formal theories of arithmetic (that is, natural numbers, and whose intended model is \mathbb{N}). These theories are characterized by their axioms and language (set of symbols), starting from a basic set of axioms providing the elementary properties of numbers. Each bounded arithmetic theory possesses different additional axioms postulating the existence of different sets of numbers, or different kinds of induction principles. Based on its specific axioms, each theory of bounded arithmetic proves the totality of functions from different complexity classes (e.g., polynomial-time functions). We can typically consider such theories as working over a logical language that contains the function symbols of that prescribed complexity class. In this sense, proofs in the theory use concepts from a specific complexity class, and we can say that the theory captures “reasoning in this class” (e.g., “polynomial-time reasoning”). In the current work we shall start with a standard naturally written constructive proof (in the “meta-theory”) of the Schwartz-Zippel Lemma (Section 1.2), following the first, less formal approach, to constructivity. We then show how the new proof fits in the formal approach to feasible constructivity of bounded arithmetic. We moreover exemplify the usual benefits of such proofs by showing applications both in bounded arithmetic and computational complexity. Background on theories of bounded arithmetic, their utility and applications. While the first theory for polynomial-time reasoning was the equational theory \mathsf{PV} considered by Cook [14], bounded arithmetic goes back to the work of Parikh [35] and Paris-Wilkie [36]. In a seminal work, Buss [8] introduced other theories of bounded arithmetic and laid much of the foundation for future work in the field. Using formal theories of bounded arithmetic is important for several reasons. First, it provides a rigorous framework to ask questions about provability, independence, and the limits of what can be proved by which means and arguments. The quest for “barriers” in computational complexity—namely, the idea that some forms of arguments are futile as a way to solve the fundamental hardness problems in complexity—such as Relativisation by Baker, Gill and Solovay [6], Natural Proofs by Razborov and Rudich [40] or Algebrisation by Aaronson and Wigderson [1], has played an important role in complexity theory. Nevertheless, the language of formal logic provides a more systematic framework for such a project (cf. [2, 19] and the recent work [11]). In that sense, bounded arithmetic allows us to identify suitable logical theories capable of formalising most contemporary complexity theory, and determine whether the major fundamental open problems in the field about lower bounds are provable or unprovable within these theories. Furthermore, bounded arithmetic serves as a framework in which the bounded reverse mathematics program is developed (in an analogy to Friedman and Simpson reverse mathematics program [44]). In this program, one seeks to find the weakest theory capable of proving a given theorem. In other words, we seek to discover what are the axioms that are not only sufficient to prove a certain theorem, but rather are also necessary. Special theorems of interest are those of computer science and computational complexity theory. The motivation is to shed light on the role of complexity classes in proofs, in the hope to delineate, for example, those concepts that are needed to progress on major problems in computational complexity from those that are not. For instance, it has been identified that apparently most results in contemporary computational complexity can be proved using polynomial-time concepts (e.g., in \mathsf{PV}) (cf. [38]), and it is important to understand whether stronger theories and concepts are needed to prove certain results. Accordingly, recent results in bounded arithmetic [11] seek to systematically build a bounded reverse mathematics of complexity lower bounds, in particular, as these are perhaps the most fundamental questions in complexity. This serves to establish complexity lower bounds as “fundamental mathematical axioms” which are equivalent, over the base theory, to rudimentary combinatorial principles such as the pigeonhole principle or related principles. Indeed, we will show that the (upper bound) statement about the existence of small hitting sets is equivalent to the (dual weak) pigeonhole principle. (It is interesting to note that the existence of explicit small hitting sets, which implies efficient PIT, is also a disguised lower bound statement as shown by Kabanets and Impagliazzo [23]; though we do not attempt to formalise or pursue this direction in the current work.) Another advantage of bounded arithmetic comes in the form of “witnessing” theorems. These are results that automatically convert formal proofs (of low logical-complexity theorems, namely, few quantifier alternations) in bounded arithmetic to feasible algorithms (usually, deterministic polynomial-time ones). Witnessing theorems come in different flavours and strength, and recent work show the advantage of this method in both lower and upper bounds [10, 18, 29]. Moreover, the somewhat forbidding framework of bounded arithmetic forces one to think algorithmically from the get go, optimising constructions. This resulted in new arguments to existing results, which proved very useful in complexity, beyond the scope of bounded arithmetic. A celebrated example is Razborov’s new coding argument of the Håstad’s switching lemmas [17], which emerged as work in bounded arithmetic [39]. (Intriguingly, our new argument for Schwartz-Zippel will also be based on a coding argument.) Randomness and feasibly constructive proofs. One central part of complexity that was challenging to fit into bounded arithmetic is that of random algorithms. Randomness usually entails thinking of probability spaces of exponential size (e.g., the outcome of n coin flips), and so cannot be directly used in most bounded arithmetic theories which cannot state the existence of exponential size sets. Initial work by Wilkie (unpublished) [26, Theorem 7.3.7], taken as well by Thapen [45], and systematically developed in a series of works of Jeřábek (cf. [21]), concerned how to work with randomness in bounded arithmetic. Nevertheless, one of the most well-known examples for using randomness in computing, the question of formulating Schwartz-Zippel and polynomial identity testing in particular, was left open due to the exponential nature of the standard argument (i.e., the need to treat polynomials as sums of exponential many monomials; see the first paragraph in Section 1.2). For example, Lê and Cook in [28] list this as an open question (where they settled for formalising a special case of the Schwartz-Zippel Lemma). The formalisation of Schwartz-Zippel Lemma and PIT in bounded arithmetic and its consequences we present, hopefully exemplify several of the benefits of bounded arithmetic described above. It serves to fill in the missing link in the formalisation of complexity of randomness; it produces a new (coding) argument of Schwartz-Zippel lemma that may be of independent interest; it establishes the existence of hitting sets as equivalent to the dual weak pigeonhole principle, and thus provides further examples of the “axiomatic” nature of building blocks in complexity theory; lastly, it has consequences to computational complexity, by showing that hitting sets are complete for the class of range avoidance problems. 1.2 New Proof of the Schwartz-Zippel Lemma The standard Schwartz-Zippel proof proceeds by induction on the number of variables n but is not a feasibly constructive proof. It is non-constructive in the sense that each inductive step involves stating properties of objects that are of exponential size. For example, in step i of the induction, the proof states that the current polynomial on n-i+1 variables can be rewritten into a univariate polynomial in the last variable, with coefficients taken from the ring of polynomials in the first n-i variables. The non-constructive character of the proof stems then from the fact that there is no (known) efficient way of iterating this rewriting n times, unless the polynomial is given in explicit sum of monomials form. Note, however, that the sum of monomials form is typically of exponential size. Moshkovitz [32] provided an alternative proof of the Schwartz-Zippel Lemma, but only for finite fields. The proof in [32] does not use explicit induction, but it is anyway unclear how to make it strictly constructive in our sense, namely how to identify polynomial-time algorithms for the concepts used in the proof, and then use these to formalise the whole argument in a relatively weak theory. We refer to Moshkovitz’ proof again later in this section to compare it with our approach. Towards our new proof. For the sake of exposition, let us begin with two natural but failed attempts to a new proof. In what follows, let P(\overline{x}) be a polynomial over the field \mathbb{F}, with n variables and maximum individual degree d, and let {\overline{a}}\in\mathbb{F}^{n} be a given non-root; P({\overline{a}})\neq 0. Let S\subseteq\mathbb{F} be a finite subset of the field. In the first attempt, we try to cover the cube S^{n} with at most n\cdot|S|^{n-1} lines, each emanating from the non-root {\overline{a}}. These lines are the subsets of \mathbb{F}^{n} of the form \{{\overline{a}}+t\cdot{\overline{b}}:t\in\mathbb{F}\} for some {\overline{b}}\in\mathbb{F}^{n}. For each such line L, note that P retricted to L, defined as P_{L}(t):=P({\overline{a}}+t\cdot{\overline{b}}), is a non-zero univariate polynomial of degree at most d. It is non-zero because it evaluates to P({\overline{a}})\not=0 at t=0, and it has degree at most d because it is a linear restriction of P. Therefore, by the fundamental theorem for (univariate) polynomials, each such line has at most d roots, and since the lines cover S^{n}, we count at most d\cdot n\cdot|S|^{n-1} roots in S^{n} in total, and we are done. The problem with this approach is that it is not always possible to cover |S|^{n} with at most n\cdot|S|^{n-1} lines emanating from a single point {\overline{a}}. A simple counterexample can be found already at dimension n=2 with S=\{0,1,\ldots,q-1\} and {\overline{a}}=(0,0): the 2q-1 points of the form (i,q-1) or (q-1,i) with i\in S require each its own line emanating from the origin, and the remaining q^{2}-2q-2 points cannot be covered with one more line. In the second attempt, we want to cover the cube S^{n} again with at most n\cdot|S|^{n-1} lines, but now we try with parallel lines. For example we could consider the set of axis-parallel lines of the form \{(c_{1},\ldots,c_{k-1},t,c_{k+1},\ldots,c_{n}):t\in\mathbb{F}\} with c_{j}\in S for all j\not=k, for some fixed k=1,\ldots,n. The problem with this approach now is that it is not clear that all such lines will go through some non-root of P. It is tempting to consider some sets of parallel lines that are more related to the given non-root {\overline{a}}, such as the set of lines of the form \{{\overline{b}}+t\cdot{\overline{a}}:t\in\mathbb{F}\} whose gradient is {\overline{a}}. This is indeed the approach taken by Moshkovitz in [32], but as far as we can see this does not work for arbitrary non-roots {\overline{a}}, and works only for a specific kind of non-roots that seem hard to find in the first place. Our approach. We are now ready to explain the two new ideas that we use in our proof, and how they overcome the obstacles of the previous two attempts. First, instead of covering the roots in S^{n} with n\cdot|S|^{n-1} lines where the polynomial is non-zero, we are going to encode each root in S^{n} with one in n\cdot|S|^{n-1} such lines, together with an additional number i in 1,\ldots,d. The lines we use to encode the roots are not necessarily pair-wise parallel, though each line will be parallel to one of the axes. Second, to actually find this line, our proof uses a hybrid-type argument. Concretely, to encode the root {\overline{c}}, we start at a line through {\overline{a}} and end at a line through {\overline{c}}. Along the way, the process travels across at most n axis-parallel lines of the cube S^{n}, changing the dimension of travel at each step. The hybrid-argument is used to preserve the property that the restriction of P to the current line is still a non-zero polynomial. The exact details of how this is done are explained below. New proof of Schwartz-Zippel lemma: Let {\overline{a}}=(a_{1},\ldots,a_{n})\in\mathbb{F}^{n} be such that P({\overline{a}})\not=0, and let S be a finite subset of the field \mathbb{F}. Each vector {\overline{c}}=(c_{1},\ldots,c_{n})\in S^{n} of field elements in S can be encoded with n numbers, each from 1 to |S|, by identifying each c_{i} with its position in an arbitrary ordering of the finite set S. Our goal is to encode the roots of P in S^{n} with shorter codewords. To achieve this we will use only the fact that we know a non-root {\overline{a}}\in\mathbb{F}^{n}. Let {\overline{c}}=(c_{1},\ldots,c_{n})\in S^{n} be such that P({\overline{c}})=0. Find the minimal index k between 1 and n such that P(c_{1},\dots,c_{k-1},a_{k},a_{k+1},\dots,a_{n})\neq 0 while P(c_{1},\dots,c_{k-1},c_{k},a_{k+1},\dots,a_{n})=0. Such a k must exist since by assumption P({\overline{a}})\not=0 and P({\overline{c}})=0. Observe that, if we are given both {\overline{a}} and {\overline{c}}, then finding k is easy by looping through the coordinates from left to right. The argument hinges on the following observation: Knowing this k allows us to encode, given c_{1},\dots,c_{k-1},c_{k+1},\dots,c_{n}, the field element c_{k}\in S by a single number i between 1 and d. Therefore, we can use i, together with k and the positions of c_{1},\ldots,c_{k-1},c_{k+1},\ldots,c_{n} in the fixed ordering of S, as a code for {\overline{c}}. This shows that the set of roots in S^{n} can be encoded using only d\cdot n\cdot|S|^{n-1} numbers instead of the trivial |S|^{n} bound, concluding the argument. In detail, consider the root {\overline{c}}=(c_{1},\dots,c_{k-1},c_{k},c_{k+1}\dots,c_{n}), where k is the minimal index as above. We encode {\overline{c}} by the n-1 elements c_{1},\dots,c_{k-1},c_{k+1},\dots,c_{n} from S, together with k and a second index i such that c_{k}\in S is the i-th root in S of the univariate polynomial Q(t)=P(c_{1},\dots,c_{k-1},t,a_{k+1},\dots,a_{n}). This encoding works because given the numbers k and i, and the elements (c_{1},\dots,c_{k-1},c_{k+1}\dots,c_{n}) we can recover {\overline{c}}. Indeed, by the choice of k and i, the univariate polynomial Q(t) is non-zero and has degree at most d, which means that it has no more than d roots in \mathbb{F}. By looping through S we can find its i-th root in S which, by construction, is c_{k}. The fact that each root of P in S^{n} is coded by n-1 of its components and the two positive integers k and i finishes the proof as it means that the total number of such roots is bounded above by the number of all possible such codes: d\cdot n\cdot|S|^{n-1}. (0,0,0) Illustration of the encoding process. Refer to the cube on the right. The x,y,z axes are represented by the vertical dimension, and the two horizontal dimensions, respectively. The given non-root is the red dot \overline{a}=(1,0,0). The root to encode is the blue dot \overline{c}=(3,2,1). The process starts at the red line (t,0,0) through \overline{a}, parallel to the x axis. Replacing the first component a_{1} by c_{1}, we check if P(c_{1},a_{2},a_{3})=0. If not, we travel along the red line for c_{1}-a_{1}=2 units to reach the green line (3,t,0), parallel to the y axis. Next we check if P(c_{1},c_{2},a_{3})=0. If not, we travel along the green line for c_{2}-a_{2}=2 units to reach the blue line (3,2,t), parallel to the z axis. We test now if P(c_{1},c_{2},c_{3})=0, which checks, because {\overline{c}} is a root. The journey ends here. In this example it took us k=3 steps to reach the root. Note that this was the first k in 1,\ldots,n that caused P to vanish on the hybrid (c_{1},\ldots,c_{k-1},c_{k},a_{k+1},\ldots,a_{n}). The blue line, call it L, is the one we use to encode the root {\overline{c}}. To encode it, we use the index of c_{k} as a root of the univariate polynomial P_{L}(t) obtained by restricting P to this line. If this index is i in 1,\ldots,d, then we encode the root \overline{c} by (i,k,(c_{1},\ldots,c_{k-1},c_{k+1},\ldots,c_{n})). In the example, if the index i happens to be 1, then we use (1,3,(3,2)). Note that this encoding depends on the given non-root \overline{a}, but any non-root serves the purpose of encoding all roots in S^{n}. 1.3 Applications 1.3.1 Schwartz-Zippel Lemma in the Theory We begin with a short informal description of the theories, language and axioms we shall use. \mathsf{PV}: This is the language with a function symbol for every polynomial-time function, with its meaning specified by the equations that define it via Cobham’s bounded recursion on notation. \mathsf{S^{1}_{2}}: The first level of Buss’ family of theories [8] for basic number theory whose definable functions are precisely the polynomial-time functions. It contains basic axioms for properties of numbers (e.g., associativity of product), together with a polynomial induction axiom for \mathsf{NP}-predicates. The extension of {\mathsf{S^{1}_{2}}} with all {\mathsf{PV}}-symbols and the Cobham equations as axioms is denoted by {\mathsf{S^{1}_{2}}}({\mathsf{PV}}). The theory has the same theorems as {\mathsf{S^{1}_{2}}} in the base language (see [8]), and it is customary to abuse notation and still call it {\mathsf{S^{1}_{2}}} instead of the heavier {\mathsf{S^{1}_{2}}}({\mathsf{PV}}). \mathsf{dWPHP}(\mathsf{PV}): The set of dual weak pigeonhole principle axioms \mathsf{dWPHP}(f), for every polynomial-time function symbol f\in{\mathsf{PV}}. This axiom states the simple counting principle that a function f cannot map surjectively a domain of size N to a range of size 2N or more; namely, there is a point in the set of size 2N that is not covered by f. Wilkie (unpublished; see [26, Theorem 7.3.7]) observed the connection between this principle and randomness in computation (within bounded arithmetic): roughly speaking, when f has a small domain but much larger co-domain, with high probability a point in the co-domain will not be covered by f. Hence, the ability to pick such a point is akin to witnessing this probabilistic argument. \mathsf{S^{1}_{2}}+\mathsf{dWPHP}(\mathsf{PV}): \mathsf{S^{1}_{2}} (indeed {\mathsf{S^{1}_{2}}}({\mathsf{PV}}); see above), augmented with the axioms \mathsf{dWPHP}(f) for every polynomial-time function symbol f\in{\mathsf{PV}}. This is a theory that can serve as a basis for probabilistic reasoning (close to Jeřábek’s theory for approximate counting [21]; cf. [20]). With this notation we can now state the form of Schwartz-Zippel Lemma that we prove. Here, and in the rest of this introduction, let [q] denote the set \{1,\ldots,q\}. Theorem 1.2 (Schwartz-Zippel Lemma in \mathsf{S^{1}_{2}}; informal, see Theorem 3.9). Let P be a polynomial of degree d, given as an algebraic circuit, with integer coefficients and n variables. Then, either P is zero everywhere on \mathbb{Z}, or for every positive integer q there is a (polynomial-time) function f that given any non-root {\overline{a}}=(a_{1},\ldots,a_{n})\in\mathbb{Z}^{n} with P({\overline{a}})\not=0, returns a function f({\overline{a}}):{\sf codes}\to{\sf roots} that maps the set of codes surjectively onto the set of roots in the cube [q]^{n}, and |{\sf codes}|\leq d\cdot n\cdot q^{n-1}. Here the codes and the function f are defined according to the encoding scheme in Section 1.2. Since given a non-root {\overline{a}} the function f({\overline{a}}) is (provably) surjective onto the roots in the cube [q]^{n}, the number of roots of P in the cube is at most the number of codes, or |{\sf codes}|\leq d\cdot n\cdot q^{n-1}. To actually reason in the theory about the size of exponential-size sets like {\sf codes} we could have chosen to invoke approximate counting (based on Jeřábek’s theories [21], which would require the inclusion of the dual weak pigeonhole principle \mathsf{dWPHP}). However, we opt not to do this for the Schwartz-Zippel Lemma. Rather, we will show that this formulation of the Schwartz-Zippel Lemma, together with the \mathsf{dWPHP}(\mathsf{PV}) axiom, suffices to apply the lemma in its standard applications, such as PIT and finding small hitting sets (see below). En route to the proof of Theorem 1.2, we prove in the theory {\mathsf{S^{1}_{2}}} one half of the Fundamental Theorem of Algebra (FTA). This is the fundamental theorem of univariate polynomials stating that every non-zero polynomial of degree d with complex coefficients has exactly d complex roots. The theorem naturally splits into two halves: the half that states that there are at least d roots, and the half that states that there are at most d roots. While the at least statement relies on special properties of the complex numbers, the proof of the at most statement relies only on the fact that univariate polynomials over a field admit Euclidean division. In particular, the at most statement holds also for polynomials over any subring of a field; e.g., the integers. Theorem 1.3 (Half of Fundamental Theorem of Algebra in \mathsf{S^{1}_{2}} informal; see Lemma 3.4). Every non-zero univariate polynomial of degree d with integer coefficients has at most d roots on (every finite subset of) \mathbb{Z}. While the underlying idea of this proof is standard, it is somewhat delicate to carry out the argument in {\mathsf{S^{1}_{2}}} because we need to keep track of the bit-complexity of the coefficients that appear along the way in the computations. It is well-known that certain widely-used algorithms working with integers or rational numbers, including Gaussian Elimination, could incur exponential blow-ups in bit-complexity if careless choices were made in their implementation; cf. [16]. We also note that Jeřábek [22] formalised Gaussian Elimination over rationals in {\mathsf{S^{1}_{2}}}, and proved also the same half of the FTA that we prove, but only for finite fields, where exponential blow ups cannot occur. 1.3.2 Existence of Hitting Sets and PIT in the Theory For a field \mathbb{F} and a set of algebraic circuits \mathscr{C} over \mathbb{F} with n variables, we say that a set H\subseteq\mathbb{F}^{n} is a hitting set for \mathscr{C} if for every non-zero polynomial P in \mathscr{C} there exists a point {\overline{a}}\in H such that P({\overline{a}})\neq 0. In other words, if P is non-zero, H ‘hits’ it. Hitting sets are important because when they are explicit and small they allow for derandomization of PIT: running through the full hitting set one can test if a given algebraic circuit is the zero polynomial or not. By Theorem 1.2, the theory {\mathsf{S^{1}_{2}}} proves (by means of a surjective map) that every non-zero n-variable algebraic circuit with small degree d has relatively few roots in [q]^{n}. By a counting argument (or the union bound, cf. [43, Theorem 4.1]), it follows that for any given bounds d and 2^{m} on the degree and the number of circuits in the class \mathscr{C}, there is a set H\subseteq[q]^{n} of {\mathsf{poly}}(n,d,m) points, with q={\mathsf{poly}}(n,d), that intersects the set of non-roots of every non-zero circuit in the class. This set H is thus a hitting set of polynomial size, and we say it is a hitting set for \mathscr{C} over [q]. We show that this counting argument can now be formalised in the theory {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}). Theorem 1.4 (Small Hitting Sets Exist in {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}); informal, see Theorem 4.4). For every class \mathscr{C} of algebraic circuits with integer coefficients that is definable in the theory, with n variables, polynomial degree, and polynomial size, there exists a polynomial-size hitting set for \mathscr{C} over [q] with q={\mathsf{poly}}(n). The argument in the theory makes two uses of the axiom \mathsf{dWPHP} and is roughly as follows. We begin by showing that, if q is sufficiently large but polynomial, then a non-zero polynomial with n variables and polynomial degree always has non-root {\overline{a}} in [q]^{n}. To see this, recall the function f({\overline{a}}):{\sf codes}\to{\sf roots} from Theorem 1.2, that given a polynomial P and a non-root {\overline{a}} of P, surjectively maps all codes of roots to the roots of P. Note that the set {\sf roots} is a subset of [q]^{n}, which has size q^{n}, and recall that the set {\sf codes} has size at most d\cdot n\cdot q^{n-1}, where d is the degree of P. Thus, when q\geq 2dn, the \mathsf{dWPHP} axiom applies to show that there exists a point {\overline{a}}_{0} in [q]^{n} that is not in the range {\sf root} of f({\overline{a}}). This {\overline{a}}_{0} is thus a non-root of P in the set [q]^{n}, like we wanted. Next we show how to use this fact to get a hitting set with a second application of the \mathsf{dWPHP} axiom. Let \mathscr{C} be a class of algebraic circuits with n variables, syntactic degree at most d, and size at most s. Consider the function \begin{array}[]{llllll}g&:&\mathscr{C}\times[q]^{n}\times\textsf{codes}^{r}&% \to&\textsf{roots}^{r}\\ &&(P,{\overline{a}},{\overline{c}}_{1},\ldots,{\overline{c}}_{r})&\mapsto&(f({% \overline{a}})({\overline{c}}_{1}),\ldots,f({\overline{a}})({\overline{c}}_{r}% )),\end{array} (1) where {\overline{c}}_{1},\ldots,{\overline{c}}_{r} are candidate codes, each from the code-set codes of size n\cdot d\cdot q^{n-1}, and {\overline{a}} is a potential non-root of P in [q]^{n}. The parameter r should be sufficiently big, but polynomial. Then, it follows by construction and the fact proved in the previous paragraph that any point outside the range of g is a hitting set, since it will have a non-root for every algebraic circuit in \mathscr{C}. To find the point outside the range of g we invoke the \mathsf{dWPHP} axiom, using again the assumption that q\geq 2dn. One immediate consequence of Theorem 1.4 is that the theory {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}) proves that the problem of verifying polynomial identities {\mathsf{PIT}} can be solved by polynomial-size Boolean circuits, so is in {\mathsf{P\text{/}poly}}. We read this as adding evidence to the claim that the theory is sufficiently powerful to prove most contemporary results in complexity theory. In particular, it adds interest to the question of proving that the major lower bound conjectures of computational complexity are consistent with {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}) and stronger theories; see [27, 10, 5] for more on this line of work. 1.3.3 Contribution to Reverse Mathematics of Complexity Theory The fact that the dual weak pigeonhole principle suffices to prove the existence of small hitting sets raises a natural question: Is it also necessary? A positive answer would provide a combinatorial characterization of the algebraic statement that small hitting sets exist. It would also shed light on the role or the necessity of the probabilistic method in the usual proof of this existential statement. We show how to achieve a version of these two goals. We define a formal scheme of hitting sets axioms called {\mathsf{HS}}({\mathsf{PV}}). We follow two provisos. First, in view of the generality of Theorem 1.4, we define the axiom scheme to contain one axiom for each definable class \mathscr{C} of algebraic circuits; the axiom states that each slice \mathscr{C}_{n}, consisting of the circuits of \mathscr{C} with n variables, has small hitting sets. Second, in the definition of the axiom for \mathscr{C}, we need to decide whether to let it state the existence of hitting sets of unspecified but polynomial size, or to let it state the existence of hitting sets of some specified polynomial size. The bound established in Theorem 1.4 is actually of the form {\mathsf{poly}}(m,n) where m is the logarithm of the number of circuits in the n-th slice, and {\mathsf{poly}}(m,n) refers to a fixed polynomial of m and n. This dependence on m is common in most proofs of existence by the union bound. While the claim that hitting sets of any possibly larger but unspecified size exist would of course be also true, it turns out that asking the axiom to provide a hitting set of some fixed polynomial bound seems crucial in the proof of necessity of \mathsf{dWPHP}({\mathsf{PV}}) that we are after. We chose the latter because this is what is sufficient, and it is still natural. These two provisos motivate the following definition (informal; see Definition 5.1): \mathsf{HS}(\mathsf{PV}): The set of hitting set axioms {\mathsf{HS}}(g), for every g\in{\mathsf{PV}}. This states that if g defines a class \mathscr{C} with its n-th slice having 2^{m} algebraic circuits with n variables, polynomial degree, and polynomial size, then there is a hitting set for \mathscr{C} over [q] of size {\mathsf{poly}}(m,n), with q={\mathsf{poly}}(n). With the right definitions in place we can state the theorem that characterizes the proof-theoretic strength of the existence of small hitting sets: Theorem 1.5 (Reverse Mathematics of Hitting Sets; informal, see Theorem 5.2). The axioms schemes \mathsf{dWPHP}({\mathsf{PV}}) and {\mathsf{HS}}({\mathsf{PV}}) are provably equivalent over the theory {\mathsf{S^{1}_{2}}}. As remarked earlier, the sufficiency claim follows from Theorem 1.4. To prove the necessity we show how to use a hitting set to find a point outside the range of any given polynomial-time function f:[N]\to[2N]. To do this we design a class \mathscr{C}_{f} of N many low-degree algebraic circuits each vanishing on the appropriate representation of a point in the image of f. We do so in such a way that a hitting set for \mathscr{C}_{f} will correspond to an element in [2N]\setminus\mathrm{Img}(f), completing the proof. To make this actually work we need to use a technique known as amplification, which goes back to the work of Paris-Wilkie-Woods [37]. The same kind of technique was discovered even earlier, in cryptography, to build pseudorandom number generators from hardcore bits; see the work of Blum-Micali [7]. The details of this argument can be found in Section 5 and Appendix A. 1.3.4 Application to Computational Complexity and Range Avoidance Problem The proof-sketch we gave for Theorem 1.5 reveals a two-way connection between the computational problem of finding hitting sets and the so-called Range Avoidance Problem, or {\mathsf{Avoid}}. The latter problem asks to find a point outside the range of a given function f:[N]\to[2N]. In recent years, {\mathsf{Avoid}} has been studied with competing names. It was first studied by Kleinberg-Korten-Mitropolsky-Papadimitriou [24], calling it 1-{\mathsf{Empty}}, and later by Korten [25], renaming it {\mathsf{Empty}}. Those works defined it as the canonical complete problem for a complexity class of total search problems they called {\mathsf{APEPP}}. Ren-Santhanam-Wang [41] studied it too, calling it {\mathsf{Avoid}}, in their range avoidance problem approach to circuit lower bounds. Our new coding-based proof of the existence of hitting sets shows that its associated search problem is in {\mathsf{APEPP}}. The proof of Theorem 1.5 yields its completeness in the class: Theorem 1.6 (Completeness of Finding Hitting Sets; informal, see Theorem 1.6). The total search problem that asks to find witnesses for the hitting set axioms of the scheme {\mathsf{HS}}({\mathsf{PV}}) is {\mathsf{APEPP}}-complete under {\mathsf{P}}^{{\mathsf{NP}}}-reductions. Perhaps not too surprisingly, the proof of Theorem 1.6 is almost identical to the proof of Theorem 1.5. Indeed, the necessity for the {\mathsf{NP}}-oracle in the reductions comes (only) from the use of the amplification technique in the proof, as in earlier uses of this method; cf. [25]. A handful of complete problems for {\mathsf{APEPP}} were known before, and some required {\mathsf{P}}^{{\mathsf{NP}}}-reductions too (cf. [24, 25]). But, to our knowledge, none of these complete problems related to the problem of constructing hitting sets for algebraic circuits. Here we used the new constructive proof of the Schwartz-Zippel Lemma to find an example of this type."
https://arxiv.org/html/2411.06944v1,Finite Variable Counting Logics with Restricted Requantification,"Counting logics with a bounded number of variables form one of the central concepts in descriptive complexity theory. Although they restrict the number of variables that a formula can contain, the variables can be nested within scopes of quantified occurrences of themselves. In other words, the variables can be requantified. We study the fragments obtained from counting logics by restricting requantification for some but not necessarily all the variables.Similar to the logics without limitation on requantification, we develop tools to investigate the restricted variants. Specifically, we introduce a bijective pebble game in which certain pebbles can only be placed once and for all, and a corresponding two-parametric family of Weisfeiler-Leman algorithms. We show close correspondences between the three concepts.By using a suitable cops-and-robber game and adaptations of the Cai-Fürer-Immerman construction, we completely clarify the relative expressive power of the new logics.We show that the restriction of requantification has beneficial algorithmic implications in terms of graph identification. Indeed, we argue that with regard to space complexity, non-requantifiable variables only incur an additive polynomial factor when testing for equivalence. In contrast, for all we know, requantifiable variables incur a multiplicative linear factor.Finally, we observe that graphs of bounded tree-depth and 3-connected planar graphs can be identified using no, respectively, only a very limited number of requantifiable variables.","Descriptive complexity is a branch of finite model theory that essentially aims at characterizing how difficult logical expressions need to be in order to capture particular complexity classes. While we are yet to find or rule out a logic capturing the languages in the complexity class P, there is an extensive body of work regarding the descriptive complexity of problems within P. Most notably, there is the work of Cai, Fürer, and Immerman [3] which studies a particular fragment of first-order logic. This is the fragment \mathsf{C}^{k} in which counting quantifiers are introduced into the logic, but the number of variables is restricted to being at most k. The seminal result in [3] shows that this logic fails to define certain graphs up to isomorphism, which in turn proves that inflationary fixed-point logic with counting IFP+C fails to capture P. Although the fragment \mathsf{C}^{k} restricts the number of variables, it is common for variables to be reused within a single logical formula. In particular, variables can be nested within scopes of quantified occurrences of themselves. In other words, they can be requantified. In our work, we are interested in understanding what happens if we limit the ability to reuse variables through requantification. In fact, we may think of reusability as a resource (in the vein of time, space, communication, proof length, advice etc.) that should be employed economically. It turns out that the ability to limit requantification provides us with a more detailed lens into the landscape of descriptive complexities within P, much in the fashion of fine-grained complexity theory. Results and techniques Let us denote by \mathsf{C}^{(k_{1},k_{2})} the fragment of first-order logic with counting quantifiers in which the formulas have at most k_{1} variables that may be requantified and at most k_{2} variables that may not be requantified. First, we show that many of the traditional techniques of treating counting logics can be adapted to the setting of limited requantification. Specifically, it is well known that there is a close ternary correspondence between the logic \mathsf{C}^{k}, the combinatorial bijective k-pebble game, and the famous (k-1)-dimensional Weisfeiler-Leman algorithm [3, 22, 26]. We develop versions of the game and the algorithm that also have a limit on the reusability of resources. For the pebble game, a limit on requantification translates into pebbles that cannot be picked up anymore, once they have been placed. For the Weisfeiler-Leman algorithm, the limit on requantification translates into having some dimensions that “cannot be reused”. In fact the translation to the algorithmic viewpoint is not as straightforward as one might hope at first. Indeed, we do not know how to define a restricted version of the classical Weisfeiler-Leman algorithm that corresponds to the logic \mathsf{C}^{(k_{1},k_{2})}. However, we circumvent this problem by employing the oblivious Weisfeiler-Leman algorithm (OWL). This variant is often used in the context of machine learning. In fact, Grohe [17] recently showed that k+1-dimensional OWL is in fact exactly as powerful as k-dimensional (classical) WL. We develop a resource-reuse restricted version of the oblivious algorithm and prove equivalence to our logic. Indeed, we formally prove precisely matching correspondences between the limited requantification, limited pebble reusability, and the limited reusable dimensions (Theorem 3.6). Next, we conclusively clarify the relation between the logics within the two-parametric family \mathsf{C}^{(k_{1},k_{2})}. We show that in most cases limiting the requantifiability of a variable strictly reduces the power of the logic. We argue that no amount of requantification-restricted variables is sufficient to compensate the loss of an unrestricted variable. However, these statements are only true if at least some requantifiable variable remains. In fact, exceptionally, \mathsf{C}^{(1,k_{2})} is strictly less expressive than \mathsf{C}^{(0,k^{\prime}_{2})} whenever k_{2}^{\prime}>2k_{2} (Theorem 4.7). To show the separation results, we adapt a construction of Fürer [13] and develop a cops-and-robber game similar to those in [12, 19]. In this version, some of the cops may repeatedly change their location, while others can only choose a location once and for all. Using another graph construction, we rule out various a priori tempting ideas concerning normal forms in \mathsf{C}^{(k_{1},k_{2})}. To this end we show that formulas in the logics can essentially become as complicated as possible, having to repeatedly requantify all of the requantifiable variables an unbounded number of times, before using a non-requantifiable variable (Corollary 4.10). In terms of the pebble game, it seems a priori unclear when an optimal strategy would employ the non-reusable pebbles. However, the corollary says that in general one has to conserve the non-reusable pebbles for possibly many moves until a favorable position calls for them. Having gone through the technical challenges that come with the introduction of reusability, puts us into a position to discuss the implications. Indeed, as our main result, we argue that our finer grained view on counting logics through restricted requantification has beneficial algorithmic implications. Specifically, we show that equivalence with respect to the logic \mathsf{C}^{(k_{1},k_{2})} can be decided in polynomial time with a space complexity of O\bigl{(}n^{k_{1}}\log n\bigr{)}, hiding quadratic factors depending only on k_{1} and k_{2} (Theorem 5.8). This shows that while the requantifiable variables each incur a multiplicative linear factor in required space, the restricted variables only incur an additive polynomial factor. In particular, equivalence with respect to the logics \mathsf{C}^{(0,k_{2})} can be decided in logarithmic space. To show these statements, we leverage the fact that, because non-requantifiable variables cannot simultaneously occur free and bound, the \mathsf{C}^{(k_{1},k_{2})}-type of a variable assignment does not depend on the \mathsf{C}^{(k_{1},k_{2})}-type of assignments which disagree regarding non-requantifiable variables. Moreover, we use ideas from an algorithm of Lindell, which computes isomorphism of trees in logarithmic space [31] to implement the iteration steps of our algorithm. Generally, we believe the new viewpoint may be of interest in particular for applications in machine learning, where the WL-hierarchy appears to be too coarse for actual applications with graph neural networks (see for example [1, 2, 32, 41]). In the process of the space complexity proof, we also show that the iteration number of the resource-restricted Weisfeiler-Leman algorithm described above is at most (k_{2}+1)n^{k_{1}}-1 (Corollary 5.3). Justifying the new concepts of restricted reusability, we observe that there are interesting graph classes that are identified by the logics \mathsf{C}^{(k_{1},k_{2})}. We argue that \mathsf{C}^{(0,d+1)} identifies all graphs of tree-depth at most d (Theorem 6.3) and that \mathsf{C}^{(2,2)} identifies all 3-connected planar graphs (Theorem 6.8). Outline of the paper After briefly providing necessary preliminaries (Section 2) we formally introduce the logics \mathsf{C}^{(k_{1},k_{2})}, the pebble game with non-reusable pebbles, the (k_{1},k_{2})-dimensional oblivious Weisfeiler-Leman algorithm, and prove the correspondence theorem between them (Section 3). We then relate the power of the logics to each other and rule out certain normal forms (Section 4). We then analyze the space complexity (Section 5) and finally provide two classes of graphs that are identified by our logics (Section 6). Further related work In addition to the references above, let us mention related investigations. Over time, a large body of work on descriptive complexity has evolved. For insights into fundamental results regarding bounded variable logics, we refer to classic texts [25, 26, 34, 35]. However, highlighting the importance of the counting logics \mathsf{C}^{k}, let us at least mention the Immerman-Vardi theorem [24, 40]. It says that on ordered structures, least fixed-point logic LFP captures P. Since LFP has the same expressive power as IFP+C on ordered structures, also IFP+C, whose expressive power is closely related to the expressive power of the logics \mathsf{C}^{k}, captures P. We should also mention the work of Hella [22] introducing the bijective k-pebble game which forms the basis for our resource restricted versions. (Counting logics on graph classes) Because of the close correspondence between the logic \mathsf{C}^{k} and the (k-1)-dimensional Weisfeiler-Leman algorithm, our investigations are closely related to the notion of the Weisfeiler-Leman dimension of a graph defined in [15]. Given a graph G this is the least number of variables k such that \mathsf{C}^{k+1} identifies G. In particular, on every graph class of bounded Weisfeiler-Leman dimension, the corresponding finite variable counting logic captures isomorphism. Graph classes with bounded Weisfeiler-Leman dimension include graphs with a forbidden minor [14] and graphs of bounded rank-width (or equivalently clique width) [20], which in both cases is also shown to imply that IFP+C captures P on these classes. For a comprehensive survey we refer to [27]. Our observations for planar graphs follow from techniques bounding the number of variables required for the identification of planar graphs [28]. Other recent classes not already captured by the results on excluded minors and rank-width include, for example, some polyhedral graphs [29], some strongly regular graphs [4], and permutation graphs [21]. (Logic and tree decompositions) In [9] and independently [8] it was shown that \mathsf{C}^{k}-equivalence is characterized by homomorphism counts from graphs of tree-width at most k-1. Likewise, homomorphism counts from bounded tree-depth graphs characterize equivalence in counting logic of bounded quantifier-rank [16]. Recently, these results were unified to characterize logical equivalence in finite variable counting logics with bounded quantifier-rank in terms of homomorphism counts [12]. (Space complexity) Ideas underlying Lindell’s logspace algorithm for tree isomorphism have also been used in the context of planar graphs [6] and more generally bounded genus graphs [10]. Similar results exist for classes of bounded tree-width [5, 11]. (Further recent results) Let us mention some quite recent results in the vicinity of our work that cannot be found in the surveys mentioned above. Regarding the quantifier-rank within counting logics, there is a recent superlinear lower bound [19] improving Fürer’s linear lower bound construction [13]. Further, very recent work on logics with counting includes results on rooted unranked trees [23] and inapproximability of questions on unique games [36]. Finally, there has been a surge in research on descriptive complexity within the context of machine learning (see [17, 37, 38])."
https://arxiv.org/html/2411.06409v1,Automated Strategy Invention for Confluence of Term Rewrite Systems,"Term rewriting plays a crucial role in software verification and compiler optimization. With dozens of highly parameterizable techniques developed to prove various system properties, automatic term rewriting tools work in an extensive parameter space. This complexity exceeds human capacity for parameter selection, motivating an investigation into automated strategy invention. In this paper, we focus on confluence, an important property of term rewrite systems, and apply machine learning to develop the first learning-guided automatic confluence prover. Moreover, we randomly generate a large dataset to analyze confluence for term rewrite systems. Our results focus on improving the state-of-the-art automatic confluence prover CSI: When equipped with our invented strategies, it surpasses its human-designed strategies both on the augmented dataset and on the original human-created benchmark dataset Cops, proving/disproving the confluence of several term rewrite systems for which no automated proofs were known before.","Term rewriting studies substituting subterms of a formula with other terms (Baader and Nipkow 1998), playing an important role in automated reasoning (Bachmair and Ganzinger 1994), software verification (Meseguer 2003), and compiler optimization (Willsey et al. 2021). Mathematicians have developed various techniques to analyze the properties of term rewrite systems (TRSs). However, many properties are undecidable (Baader and Nipkow 1998), implying that no technique can consistently prove a particular property. To navigate this undecidability, modern term rewriting provers typically employ complicated strategies, incorporating wide arrays of rewriting analysis techniques, with the hope that one will be effective. Each technique often accompanies several flags to control its behavior. The diversity of techniques and their controlling flags result in a vast parameter space for modern automation term rewriting provers. Manually optimizing strategies for undecidable problems is beyond human capacity given the extensive parameter space, inspiring us to apply machine learning to search for appropriate strategies automatically. In this paper, we focus on confluence, an important property of term rewriting, and discuss automated strategy invention for the state-of-the-art confluence prover CSI (Nagele, Felgenhauer, and Middeldorp 2017). We modify Grackle (Hůla and Jakubův 2022), an automatic tool to generate a strategy portfolio for a solver, encoding strategies that require transformations and complex schedules such as parallelism. We also conduct data augmentation on the human-built confluence problems database (Cops)111https://cops.uibk.ac.at/, a representative benchmark for the annual confluence competition (CoCo)222https://project-coco.uibk.ac.at/. As Cops has been created manually, it includes only 577 TRSs. They are of high quality, but the relatively small number is still inadequate for data-driven machine learning techniques that require large amounts of training data. To handle this problem, we generate a large number of TRSs randomly, but ensure that they are interesting enough to analyze. For this, we develop a procedure to confirm a relative balance in the number of problems most efficiently solved by different confluent analysis techniques within the dataset. We evaluate our strategy invention approach in Cops and the augmented dataset. On both of the datasets, the invented strategies surpass CSI’s default strategy. In particular, we prove (non-)confluence for several TRSs that have not been proved by any automatic confluence provers in the history of the CoCo competition. As an example, our invented strategy is able to prove the non-confluence for the Cops problem 991.trs, never proved by any participant in CoCo. The key to solving the problem is the application of the redundant rule technique (Nagele, Felgenhauer, and Middeldorp 2015) with non-standard arguments. CSI’s default strategy performs redundant -narrowfwd -narrowbwd -size 7 prior to performing non-confluence analysis. The flags narrowfwd and narrowbwd determine the categories of redundant rules to generate. Our tool automatically discovered that by changing the original redundant rule transformation to redundant -development 3 -size 7, we are able to prove this problem. A larger value for the flag development causes a larger number of development redundant rules to be added. We notice that the value three is crucial as values below three are ineffective for 991.trs. This is only one of the several problems which our new strategies can solve as discussed in the later sections. Contributions • To our best knowledge, our work is the first application of machine learning to automatic confluence provers. We automatically generate a lot of strategies for the state-of-the-art confluence prover CSI and combine them as a unified strategy. • We build a large dataset for confluence analysis, comprising randomly generated term rewrite systems and problems in the Cops dataset. • Empirical results show that our strategy invention approach surpasses CSI’s default strategy both in Cops and the augmented datasets. Notably, we discover several proofs for (non-)confluence that have never been discovered by any automatic confluence provers in the annual confluence competition."
https://arxiv.org/html/2411.06675v1,FCA using the Concept Explorer in 2024,"In this note we give a very short introduction to Formal Concept Analysis, accompanied by an example in order to build concept lattices from a context. We build the lattice using the Java-based software Concept Explorer (ConExp) in a recent version of Linux.Installing an appropriate Java version is necessary, because ConExp was developed some time ago using a Sun Java version, which is not open-source. As a result, it has been observed that ConExp will not build a lattice when started with an open-source Java version. Therefore, we also sketch the procedure we followed to install an appropriate Java version which makes ConExp work again, i.e., to “build lattices again”. We also show how to start ConExp with a 32 bit Java version, which requires a few additional libraries.","Declaration The purpose of this note is to make Formal Concept Analysis a bit more visible and accessible, give some references to theory and to show that the Java software called the Concept Explorer (ConExp) can still be used to build lattices. We were asked to write this note, because we managed to make ConExp build lattices again and thus make it usable again in 2024, almost 18 years after its last publication date in 2006. This note is organized as follows. In Section 1 we introduce basic notions of Formal Concept Analysis. Section 2 illustrates how to use of the Concept Explorer on a small example and gives references to its official documentation. Finally, Sections 3 and 4 list compatible Java versions, available today, and describe the procedure we used to install a working Concept Explorer."
https://arxiv.org/html/2411.06043v1,The subTuring degrees,"In this article, we introduce a notion of reducibility for partial functions on the natural numbers, which we call subTuring reducibility. One important aspect is that the subTuring degrees correspond to the structure of the realizability subtoposes of the effective topos. We show that the subTuring degrees (that is, the realizability subtoposes of the effective topos) form a dense non-modular (thus, non-distributive) lattice. We also show that there is a nonzero join-irreducible subTuring degree (which implies that there is a realizability subtopos of the effective topos that cannot be decomposed into two smaller realizability subtoposes).","1.1. Background The analysis of the degrees of non-computability is a central subject in computability theory. The key notion used to compare degrees of non-computability of total functions on the natural numbers is Turing reducibility. The best known reducibility for partial functions is Kleene’s relative partial computability or nondeterministic Turing reducibility [3, Chapter 11], which coincides with enumeration reducibility for the graphs of partial functions. Therefore, it has been widely believed that the study of degrees of partial functions can be absorbed into the theory of enumeration degrees. However, unlike the case of total functions, other candidates for the notion of reducibility for partial functions have been proposed. Sasso [11] used a type of relative computation with sequential (non-parallel) access to an oracle to introduce a reducibility notion between partial functions. This degree structure, which Sasso called the T-degrees of partial functions, has been little studied since then in degree theory, but the importance of Sasso’s degree structure has been highlighted in a context quite different from that of degree theory. In constructive mathematics, the pioneering work was done by Goodman [5], who used a model of relative computation similar to Sasso’s to show the so-called Goodman’s theorem: The system {\sf HA}^{\omega} of finite type Heyting arithmetic plus the axiom of choice {\sf AC} is conservative over Heyting arithmetic {\sf HA}. The proof uses the realizability interpretation relative to a generic partial choice function; see also [2, 12]. The notion of computability relative to a partial function used here is precisely what we call subTuring reducibility (Definition 1.1), which is a slight modification of Sasso’s definition (first formally introduced by Madore [9] and later by Kihara [6]). The exact same reducibility has also been used in van Oosten’s semantical proof of de Jongh’s theorem [13], for example: For a formula A in intuitionistic predicate calculus {\sf IQC}, {\sf IQC} proves A iff {\sf HA} proves every arithmetical substitution instance of A. The proof of backward direction is done by replacing predicate symbols with mutually generic arithmetical formulas. The tool used here is a pca-valued sheaf (pca stands for partial combinatory algebra). Each pca used in the proof is of the form K_{1}[f], where K_{1}[f] represents a model of computability relative to a partial function f. To be more precise, K_{1} stands for Kleene’s first algebra; that is, the standard model of computability on the natural numbers, and the relative computability is precisely subTuring reducibility. Later, van Oosten [14] used the same relative computability, which he called a dialogue, to introduce the type structure of finite type functionals based on partial functions. Given a partial combinatory algebra A, the dialogue for a partial function f yields a new algebra A[f]; see e.g. [15]. As one of the notable results, Faber-van Oosten [4, Corollary 2.11] showed that a realizability subtopos (that is, a realizability topos which is a subtopos) of the effective topos \mathcal{E}\!f\!f is nothing but the realizability topos over K_{1}[f] for some partial function f. This shows that the structure of the subTuring degrees of partial functions on the natural numbers corresponds exactly to the structure of the realizability subtoposes of the effective topos \mathcal{E}\!f\!f. In short, a modification of Sasso’s model of sequential computation, i.e., subTuring reducibility, was essential in various works on constructive mathematics, realizability topos theory, etc. For these reasons, Kihara [6] has isolated this notion of subTuring reducibility, and raised the analysis of its structure as an important problem. The issue of structural analysis of subTuring degrees has also been raised by Madore [9]. This article addresses the problem [9, 6] of analyzing the structure of the subTuring degrees (in other words, the structure of the realizability subtoposes of \mathcal{E}\!f\!f). In this article, we only aim to perform a pure degree-theoretic analysis, but in the subsequent article [8], we will apply the powerful techniques we have developed in this article to various model constructions in constructive mathematics. Let us summarize some of the results we present in this article. We show that the subTuring degrees (that is, the realizability subtoposes of the effective topos) form a dense, non-distributive, non-modular lattice. We also show that there is a nonzero join-irreducible subTuring degree (which implies that there is a realizability subtopos of the effective topos that cannot be decomposed into two smaller realizability subtoposes). 1.2. Preliminaries For the basics of computability theory, see [3, 10]. 1.3. Definition In Sasso’s model of relative computation, access to an oracle is always sequential, so parallel access is not allowed. In other words, if we make a query to an oracle during a computation, we must wait until the oracle replies. However, if the oracle is a partial function, it is possible that the oracle will not return forever, in which case the computation will be stuck. Therefore, a query we make must always be a value in the domain of the oracle if we want the computation to halt. Let us formulate this Sasso’s idea rigorously. For a partial computable function \Phi\colon\!\!\!\subseteq\omega^{<\omega}\to 2\times\omega, the g-relative sequential computation \Phi[g] is defined as follows: (1) Assume that n is given as input, the oracle’s responses (a_{0},\dots,a_{s-1}) are given by the sth round, and the computation has not yet halted. (2) If \Phi(n,a_{0},\dots,a_{s-1}) does not halt, then neither does \Phi[g](n) (written \Phi[g](n)\uparrow). Otherwise, we have \Phi(n,a_{0},\dots,a_{s-1})\downarrow=\langle i_{s},q_{s}\rangle. (a) If i_{s}=1, the computation halts and q_{s} is the output of this computation. (b) If i_{s}=0, proceed to the next step (3). (3) If q_{s}\in{\rm dom}(g), the computation continues with a_{s}=g(q_{s}). If q_{s}\not\in{\rm dom}(g), the computation never halts. If the computation arrives at (2a) above, we write \Phi[g](n)\downarrow=q_{s}. If the computation goes to (2b), we call q_{s} a query. Note that this sequential computation has the following monotonicity: f\subseteq g implies \Phi[f]\subseteq\Phi[g]. Definition 1.1. For partial functions f,g\colon\!\!\!\subseteq\omega\to\omega, we say that f is subTuring reducible to g (written f\leq_{\rm subT}g) if there exists a partial computable function \Phi such that f\subseteq\Phi[g]; that is, for any n\in{\rm dom}(f) we have \Phi[g](n)\downarrow=f(n). Roughly speaking, f\leq_{\rm subT}g if there exists a Turing functional \Phi which computes {\Phi^{g}(n)\downarrow}=f(n) without making a query outside of {\rm dom}(g), whenever an input n is given from {\rm dom}(f). Hereafter, \Phi[g] is sometimes written as \Phi^{g}. Remark. Sasso’s Turing reducibility f\leq_{T}g is defined as f=\Phi[g]; that is, it requires the condition {\rm dom}(f)={\rm dom}(\Phi[g]). Thus, f\leq_{\rm subT}g if and only if f has an extension \hat{f} with \hat{f}\leq_{T}g. Definition 1.1 is first proposed by Madore [9]. In particular, if we consider only total functions, it agrees exactly with the usual Turing reducibility. Observation 1.2. If g is total, then g\leq_{T}f if and only if g\leq_{\rm subT}f. 1.4. Basics Restricting to binary functions does not change the structure. Observation 1.3. Any partial function f\colon\!\!\!\subseteq\omega\to\omega is subTuring equivalent to a partial binary function G_{f}\colon\!\!\!\subseteq\omega\to 2. Proof. Let G_{f} be the graph of f in the following sense: G_{f}(n,m)=\begin{cases}1&\mbox{ if }{f(n)\downarrow}=m\\ 0&\mbox{ if }{f(n)\downarrow}\not=m\\ \uparrow&\mbox{ if }f(n)\uparrow\end{cases} For G_{f}\leq_{\rm subT}f, to compute G_{f}(n,m), just ask for the value of f(n), where (n,m)\in{\rm dom}(G_{f}) implies n\in{\rm dom}(f). For f\leq_{\rm subT}G_{f}, to compute f(n), ask for the values of G_{f}(n,0),G_{f}(n,1),G_{f}(n,2),\dots, where n\in{\rm dom}(f) implies (n,m)\in{\rm dom}(G_{f}) for any m. If n\in{\rm dom}(f) then the answer G(n,k)=1 will be returned at some point, so we can output k at that time. ∎ A subTuring degree is total if it contains a total function. Observation 1.4. If the domain of f\colon\!\!\!\subseteq\omega\to\omega is c.e., then f has a total subTuring degree. Similarly, if the domain of f\colon\!\!\!\subseteq\omega\to\omega is computable relative to f, i.e., \chi_{{\rm dom}(f)}\leq_{\rm subT}f, then f has a total subTuring degree. First, the minimum requirement for a reducibility notion is that it be preordered. Observation 1.5. \leq_{\rm subT} is transitive. Proof. Assume that f\leq_{\rm subT}g\leq_{\rm subT}h. Then we get f\subseteq\Phi[g] and g\subseteq\Psi[h]. By monotonicity, we get f\subseteq\Phi[g]\subseteq\Phi[\Psi[h]]. By composing the two sequential computations, we get f\leq_{\rm subT}h. ∎ Observation 1.6. The least subTuring degree contains of exactly those functions with a partial computable extension. 1.5. Realizability theory To emphasize the importance of subTuring degrees, as mentioned in Section 1.1, this is not only related to pure computability theory, but also to realizability theory [16]. The purpose of Section 1.5 is to explain this connection, and since it will not be used in the remaining sections, readers who are only interested in pure computability theory may skip this section. Definition 1.7. A partial combinatory algebra (pca) is a set equipped with a partial binary operation which is combinatory complete; see [16]. For a partial function f\colon\!\!\!\subseteq\omega\to\omega, we consider the pca K_{1}[f]=(\omega,\ast_{f}) of f-relative computability; that is, the partial binary operation \ast_{f}\colon\!\!\!\subseteq\omega^{2}\to\omega is defined as follows: {e\ast_{f}n\downarrow}=m\iff{\Phi_{e}[f](n)\downarrow}=m, where \Phi_{e} is the eth partial computable function. Let {\sf RT}(A) be the realizability topos induced by a pca A. Fact 1 ([4, Corollary 2.11]). Every realizability subtopos of the effective topos is equivalent to one of the form {\sf RT}(K_{1}[f]) for some partial function f on the natural numbers. Moreover, there is an inclusion of toposes {\sf RT}(K_{1}[f])\hookrightarrow{\sf RT}(K_{1}[g]) if and only if g\leq_{\rm subT}f. Assuming the former assertion, the latter assertion can also be derived from the correspondence between the subTuring degrees and some suborder of the poset of Lawvere-Tierney topologies; see also [7, 6]. The important point is that this is based on subTuring reducibility, and not on nondeterministic Turing reducibility (enumeration reducibility). In this way, the theory of subTuring degrees can be seen as a classification theory of the realizability subtoposes of the effective topos, or a classification theory of K_{1}-based pcas."
https://arxiv.org/html/2411.05943v1,Quantifying artificial intelligence through algebraic generalization,"The rapid development of modern artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, modern AI systems fall short on tests requiring symbolic processing and abstraction – a glaring limitation given the necessity for interpretable and reliable technology. Despite a surge of reasoning benchmarks emerging from the academic community, no comprehensive and theoretically-motivated framework exists to quantify reasoning (and more generally, symbolic ability) in AI systems. Here, we adopt a framework from computational complexity theory to explicitly quantify symbolic generalization: algebraic circuit complexity. Many symbolic reasoning problems can be recast as algebraic expressions. Thus, algebraic circuit complexity theory – the study of algebraic expressions as circuit models (i.e., directed acyclic graphs) – is a natural framework to study the complexity of symbolic computation. The tools of algebraic circuit complexity enable the study of generalization by defining benchmarks in terms of their complexity-theoretic properties (i.e., the difficulty of a problem). Moreover, algebraic circuits are generic mathematical objects; for a given algebraic circuit, an arbitrarily large number of samples can be generated for a specific circuit, making it an optimal testbed for the data-hungry machine learning algorithms that are used today. Here, we adopt tools from algebraic circuit complexity theory, apply it to formalize a science of symbolic generalization, and address key theoretical and empirical challenges for its successful application to AI science and its impact on the broader community.","The ability to reason algebraically is often considered a hallmark of human intelligence 1. The recent evolution of modern artificial intelligence (AI) systems and large language models (LLMs) has led to the speculation that these systems may also reason algebraically 2, 3, 4. Yet due to challenge of evaluating large models trained on massive pretraining datasets 5, it is difficult to evaluate whether such models are truly exhibiting algebraic reasoning abilities, or whether they instead regurgitate plausible text from their pretraining data 6, 7. This ambiguity has led to a deluge of symbolic reasoning benchmarks 8, 9, 10, 11, 12, 13, 14, 15, 16, 17. Despite these efforts, objectively quantifying the complexity of reasoning problems is difficult; most of these experiments are ad hoc, and designed without a framework to quantify complexity. However, approaches in computational complexity theory, a field within theoretical computer science, have made it possible to explicitly measure a problem’s algorithmic difficulty, enabling the design of generalization tests rooted in quantifiable measures of complexity. Here, we adopt a branch of computational complexity theory – algebraic circuit complexity – to provide a parsimonious set of problems and approaches to rigorously study symbolic computation in machine learning. Recently, there has been increased interest in studying AI models through arithmetic and compositional tasks 18, 19, 20, 21, 22, 23, 24, 25, 26, 12, 9, 27. Compositional tasks are problems that are generated by recombining a basis set of atomic elements to form a variety of task combinations. (Arithmetic problems are compositional; they are composed of atomic elements (numbers and operators), and can be recomposed to generate novel expressions and problems.) These tasks are good reasoning benchmarks because they require 1) abstraction, 2) logical application of rules or axioms, and 3) precise problem-solving and rigor. Critically, these paradigms have provided reliable ways to elicit failure modes in transformer-based AI models for specific forms of symbolic generalization. For example, a number of studies have demonstrated the difficulty of “length generalization” – generalizing to problems of longer length than seen during training 28, 19, 26, 21. Hupkes et al. 9 and Hupkes et al. 29 also introduced various notions (e.g., systematicity and productivity) in an effort to taxonomize different forms of linguistic generalization. While incredibly useful for linguistics, it is unclear how these concepts generically apply beyond natural language processing. By contrast, the formalisms from algebraic circuit complexity theory provide a set of parsimonious mathematical tools that can be applied to quantify symbolic generalization, and are agnostic to specific domains, such as linguistics. Moreover, algebraic circuit complexity theory provides an encompassing theoretical framework for the increasingly popular, yet nascent empirical evaluations in AI systems that use arithmetic tasks 18, 19, 30, 31, 21, 22, 23, 24, 32, 33, 34, 35. A large class of symbolic problems can be studied with algebraic expressions 36, 37. Algebraic circuit complexity theory formalizes algebraic expressions as circuit models (i.e., directed acyclic graphs; Fig. 1). This formalization is well-established in computational complexity theory, the branch of theoretical computer science concerned with quantifying the difficulty of computational problems and the resources required to solve them 38. Importantly, formalizing computational problems in terms of circuits is the leading approach to empirically quantify their complexity. Unlike other notions of complexity, such as Kolmogorov Complexity in algorithmic information theory (which is incomputable), notions developed in complexity theory for circuits are explicitly computable and determined by their shape and structure 39. Thus, the tools of algebraic circuit complexity can formalize notions of generalization by defining benchmarks in terms of their circuit properties. Furthermore, algebraic circuits are generic mathematical objects; they can be represented from a variety of mathematical perspectives (geometry, topology, etc.), providing useful interpretations in other domains. Algebraic circuits are therefore well-situated to addressing problems in symbolic machine learning – the problems are computable, large datasets can be straightforwardly generated from circuit specifications, and new models can be developed that address specific failure modes within this framework. In the ensuing sections, we provide a blueprint for the successful adoption of algebraic circuit complexity for machine learning problems; we introduce the core components of algebraic circuits, address how they can be leveraged to study symbolic generalization, and discuss several key open theoretical and empirical challenges. Figure 1: Examples of algebraic expressions represented as circuits. A) A two operand addition circuit (input gates are sampled from a field \mathbb{F}). B) A three operand addition circuit (input gates are sampled from the set of variables x_{i}\in X rather than \mathbb{F}). C, D) A mathematically equivalent pair of circuits, but represented as C) a factorized expression, and D) its a monomial expansion. Notably, despite their mathematical equivalence, the circuit representations are distinct. E) A polynomial of depth 4 and size 12."
