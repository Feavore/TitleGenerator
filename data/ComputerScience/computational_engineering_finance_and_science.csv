URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10307v1,"Identifying companies and financial actors exposed to marine
tipping points","Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Here we used satellite data to track fishing vessels operating in areas prone to marine regime shifts, as identified by their loss of resilience and vulnerability to marine heatwaves, and uncovered their corporate beneficial owners and shareholders. Despite some data gaps, we identified key countries, companies, and shareholders exposed to tipping risk. We also outline the potential challenges and opportunities that these actors may face if marine ecosystems shift to less productive states.","Marine ecosystems are expected to undergo large biological reorganizations that can affect primary productivity and other ecosystem services1. These abrupt and nonlinear changes, known as regime shifts, pose major challenges for managers as they are difficult to predict and reverse2. For example, the collapse of Newfoundland cod in the 1990s is estimated to have caused large economic losses, over 40,000 job losses, and the crash of an iconic industry and cultural practice in the region3. Likewise, kelp forests have been observed to transition to urchin barrens, resulting in the loss of nursery habitats for commercially important fish species4,5. Similarly, over 500 cases of oxygen-depleted zones have been reported around the globe, leading to harmful algae blooms and mass mortality events in fish which impact human health, local livelihoods and food provision6,7. Exposure to regime shifts is mediated by pressures on marine ecosystems, such as climate change, overfishing, or pollution1. Marine heatwaves are expected to increase in frequency and intensity8,9, with stark economic consequences for companies, regions and nations that depend on ocean productivity10. For example, the Gulf of Alaska experienced a heat anomaly known as the Blob from 2014 to 2019, causing an estimated US$24M loss in a fishery valued at US$103M annually. Similarly, harmful algae blooms led to US$40M loss in tourist spending along the coast of Washington in 201510. Other areas highly vulnerable to heatwaves include the tropical Pacific Ocean, the Caribbean Sea, northern Australia, the Eastern China Sea and the Western Pacific Ocean10. Increasing sea surface temperature can destabilize upwellings – currents that bring nutrient-rich water to the ocean’s surface – and affect some of the world’s most productive fisheries11. While insufficient nutrients can reduce primary productivity and cascade up through marine food webs, an excess can cause coastal eutrophication and hypoxia6, with detrimental consequences to fish stocks. Climate change is expected to reduce fish biomass by 3 to 22% in tropical and temperate areas by the end of the century12. Overfishing and nutrient pollution from agriculture and urban waste can exacerbate this impact. A combination of oceanographic and climatic factors heightens the risk of regime shifts, putting both ecosystems and human livelihoods at risk2,13. However, proactive management strategies that account for these risks could help mitigate exposure to tipping points2. Companies and financial institutions are key actors that contribute to the extraction of marine resources, while also being affected economically by changes in marine ecosystems. About 35.4% of fish stocks globally are currently depleted or overexploited14, with only 13 companies controlling around 16% of wild seafood capture, and just five companies responsible for 48% of farmed salmon production15,16. Fishing companies contribute to ecological degradation through harmful practices like overfishing17, destructive fishing methods18, and non-compliance with regulations19,20. By removing biomass from the ocean, fishing has historically prevented the sequestration of 21.8M tons of carbon14,21. In addition, the industry’s return on investment is becoming increasingly risky. Modern fishing fleets travel twice as far but catch only one-third of what they used to per kilometre travelled22, with 54% of high seas fishing would be unprofitable without public subsidies14,23. Figure 1: Areas at risk of tipping points. We computed extreme and severe heatwave events based on historical records of sea surface temperature (A, B). Areas in colour are places where the probability is non-zero, expressed as the mean number of days spent on a heatwave event. Ocean colour data was used to compute early warning signals of critical transitions (C). Note that there are no warnings computed for higher latitudes due to missing data. For the analysis of areas prone to marine regime shifts, we kept areas where the probability of extreme and severe heatwaves is non-zero, or where at least three early warning signals were found (see Methods). However, it should be in the best interest of companies and their shareholders to act as stewards of the seas and maintain a productive and healthy ocean24,25. Financial actors – such as banks, asset managers, and pension funds – could play an important role in conservation by deciding not to finance economic activities that alter the ecological integrity of the biosphere26–29. For instance, a small set of financial actors have an increasing influence over climate stability through their equity holdings in companies operating in the Amazon and boreal forests26. Likewise, large asset managers and other financial institutions invest in companies operating in areas vulnerable to the emergence of infectious diseases27. This leaves them exposed to the impacts of their investments30. Climate-change-driven extreme weather events are expected to cost up to USD 14 trillion annually by 210031, directly threatening the solvency of insurers and reinsurers32. Investors and asset managers face climate exposure being at risk of stranded assets, changes in commodity prices, scarcity of resources, disruption of supply chains, and infrastructure damages33. In response, financial actors are increasingly expected to disclose their risks through initiatives such as the Task Forces on Climate- and Nature-Related Financial Disclosures (TCFD and TNFD, respectively) or the EU non-financial reporting directive. Analyses of financial exposure to biodiversity loss and ecosystem degradation are mostly focused on terrestrial ecosystems, with little research addressing ocean-related financial risks34. Yet, fishing companies, their shareholders, and the countries they operate in are exposed to potentially irreversible changes and consequent declines in ecosystem services, with consequences on the economy, investments and livelihoods. However, being exposed to regime shifts also means being in a unique position to take action, avoid tipping points, and contribute to sustainable transformations. In this paper, we asked who are these corporate and financial actors exposed to marine tipping points? Figure 2: Descriptive statistics Number of vessels per flag reported (A), shareholders’ country of origin (B), shareholder type (C), and the probability density of their ownership shares (D). Most fishing companies analyzed have only a few shareholders (E). Countries ranked by their number of foreign investments (out-degree) and the number of investments received (in-degree), the size of the dot is proportional to the number of domestic investments (F). Countries in the yellow line have the same number of incoming and outgoing links in the network, above the line are countries with disproportionally more investments, and below countries which receive the investments."
https://arxiv.org/html/2411.08905v1,Synthesis Method for Obtaining Characteristic Modes of Multi-Structure Systems,"This paper introduces an efficient method of characteristic mode decomposition for multi-structure systems. Our approach leverages the translation and rotation matrices associated with vector spherical wavefunctions, enabling the synthesis of a total system’s characteristic modes through independent simulation of each constituent structure. We simplify the computationally demanding translation problem by dividing it into three manageable sub-tasks: rotation, z-axis translation, and inverse rotation, which collectively enhance computational efficiency. Furthermore, this method facilitates the exploration of structural orientation effects without incurring additional computational overhead. To demonstrate the effectiveness of our approach, we present a series of compelling numerical examples that not only validate the accuracy of the method but also highlight its significant advantages.","The theory of characteristic modes is pivotal in antenna analysis and design [1, 2, 3]. Its derived form—substructure characteristic modes theory—has gained prominence for revealing the intrinsic electromagnetic properties of structures within complex environments [4, 5, 6]. This approach is increasingly employed in designing antennas for handheld devices and platform-mounted systems [7]. Recent extensions have unified various characteristic mode formulations, which were traditionally based on the method of moments (MoM), into a more robust scattering-based framework, significantly enhancing numerical efficiency [8, 9]. Despite these advancements, the computational burdens for systems with multiple structures, such as antenna arrays, remain substantial. In the unified framework [8, 9], characteristic modes are obtained by eigenvalue decomposition of the transition matrix (T-matrix) or scattering matrix. This matrix serves as an operator that connects the incident and structural scattering responses, solely determined by the properties of the structures within a designated enclosing sphere. This characteristic indicates the possibility to construct the T-matrix for combined structures from independent structural data, thus facilitating the rapid computation of characteristic modes for complex systems. This paper primarily focuses on optimizing and exploring this potential. Although extensive research in fields such as optical scattering has effectively elucidated and summarized the technique of deriving the total system’s T-matrix from the individual structures’ T-matrices [10, 11, 12, 13], particularly through employing the two translation properties of spherical wavefunctions [14, 15], we revisit this established method from a unique perspective. We have developed a fully matrixized representation that is not only concise and efficient but also facilitates easier understanding compared to conventional series representations. This is particularly advantageous in scenarios that require specifying the radiation background for characteristic mode decomposition, where our approach minimizes repetitive computations. Following the method outlined in [16], we decompose the general translation problem into three sub-steps: rotation, z-axis translation, and inverse rotation, eschewing the direct solutions typically employed in conventional studies. This significantly curtails computation time, as z-axis translations are inherently simpler. Moreover, by adjusting the translation direction using the rotation matrix, we can correlate problems that have identical translation distances but different directions—common in uniformly arranged structures—thereby enhancing the reuse of the translation matrix and further boosting computational efficiency. The integration of the rotation matrix within our method introduces additional degrees of freedom; notably, it enables alterations to the structure’s orientation during post-processing, obviating the need to recompute the T-matrix. This enhancement is crucial for investigating the effects of the structure’s posture or polarization. To substantiate our approach, we present a series of illustrative numerical examples that demonstrate the substantial advantages of our synthesis technique in deriving characteristic modes. These examples highlight the potential applications of our method in complex structures and antenna arrays."
https://arxiv.org/html/2411.09329v1,Improving hp-Variational Physics-Informed Neural Networks for Steady-State Convection-Dominated Problems,"This paper proposes and studies two extensions of applying hp-variational physics-informed neural networks, more precisely the FastVPINNs framework, to convection-dominated convection-diffusion-reaction problems. First, a term in the spirit of a SUPG stabilization is included in the loss functional and a network architecture is proposed that predicts spatially varying stabilization parameters. Having observed that the selection of the indicator function in hard-constrained Dirichlet boundary conditions has a big impact on the accuracy of the computed solutions, the second novelty is the proposal of a network architecture that learns good parameters for a class of indicator functions. Numerical studies show that both proposals lead to noticeably more accurate results than approaches that can be found in the literature.","Convection-diffusion-reaction (CDR) problems are fundamental models for simulating transport events. CDR problems capture the interaction between a fluid’s bulk motion, or convection, its progressive spreading of properties by random molecular motion, or diffusion, and the impact from other quantities in coupled systems, which might be modelled as the reaction term. They constitute a framework for modelling the transport of variables like temperature or concentration. Let \Omega\subset\mathbb{R}^{2} be a bounded domain with polygonal Lipschitz-continuous boundary \partial\Omega. The Lebesgue and Sobolev spaces on this domain are denoted by L^{p}(\Omega) and W^{k,p}(\Omega), respectively, where, 1\leq p\leq\infty, k\geq 0. The Hilbert space, equivalent to W^{k,2}(\Omega), is denoted by H^{k}(\Omega). Then, a linear CDR boundary value problem, already in nondimensionalized form, is given by \displaystyle\begin{split}-\varepsilon\Delta u({\boldsymbol{x}})+{\boldsymbol{% b}}({\boldsymbol{x}})\cdot\nabla u({\boldsymbol{x}})+c({\boldsymbol{x}})u({% \boldsymbol{x}})&=f({\boldsymbol{x}}),\quad\text{in}\ \Omega,\\ u({\boldsymbol{x}})&=g({\boldsymbol{x}}),\quad\text{on}\ \partial\Omega.\end{split} (1) Here, {\boldsymbol{x}}=(x,y)\in\overline{\Omega}, u({\boldsymbol{x}}) is the unknown scalar solution and f({\boldsymbol{x}})\in L^{2}(\Omega) is a known source function. In addition, \varepsilon\in\mathbb{R}^{+} is the diffusion coefficient, {\boldsymbol{b}}\in(W^{1,\infty}(\Omega))^{2} is the convection field, and c\in L^{\infty}(\Omega) is the reaction field. The Dirichlet boundary"
https://arxiv.org/html/2411.08822v1,A probabilistic reduced-order modeling framework for patient-specific cardio-mechanical analysis,"Cardio-mechanical models can be used to support clinical decision-making. Unfortunately, the substantial computational effort involved in many cardiac models hinders their application in the clinic, despite the fact that they may provide valuable information. In this work, we present a probabilistic reduced-order modeling (ROM) framework to dramatically reduce the computational effort of such models while providing a credibility interval. In the online stage, a fast-to-evaluate generalized one-fiber model is considered. This generalized one-fiber model incorporates correction factors to emulate patient-specific attributes, such as local geometry variations. In the offline stage, Bayesian inference is used to calibrate these correction factors on training data generated using a full-order isogeometric cardiac model (FOM). A Gaussian process is used in the online stage to predict the correction factors for geometries that are not in the training data. The proposed framework is demonstrated using two examples. The first example considers idealized left-ventricle geometries, for which the behavior of the ROM framework can be studied in detail. In the second example, the ROM framework is applied to scan-based geometries, based on which the application of the ROM framework in the clinical setting is discussed. The results for the two examples convey that the ROM framework can provide accurate online predictions, provided that adequate FOM training data is available. The uncertainty bands provided by the ROM framework give insight into the trustworthiness of its results. Large uncertainty bands can be considered as an indicator for the further population of the training data set.","Over the past decades, the use of patient-specific computational models in biomedical research and clinical practice has become abundant [1, 2, 3, 4, 5]. In cardiology, a large variety of patient-specific computational models has been developed in the context of specific diseases and phenomena [6, 7, 8, 3, 4]. In our previous work [9, 10, 11] we have, for example, developed a computational model to study cardiac mechanics in the context of Ventricular Tachycardias (VTs), taking into consideration echocardiogram data. The objective of patient-specific models varies from gaining a fundamental understanding to supporting clinical decision making. The requirements of a computational model strongly depend on its intended purpose. A computational model used in biomedical research to enhance fundamental understanding typically requires detailed anatomical and physiological representations. The evaluation of such a model often involves substantial computational effort. Although this is generally not problematic in view of the modeling purpose, the computational demand can make calibration procedures impractical, warranting the accurate determination/measurement of the model parameters. In contrast, a computational model used for clinical decision support is typically required to have limited computational effort, as to not delay the decision-making process. Limiting the computational effort is typically achieved by reducing the level of detail of the anatomical and physiological representations to what is needed to support the clinician. In this setting, it is very important that the model can be tailored easily to a patient through model calibration and that it can accommodate the large uncertainties inherent to the clinical setting. The results obtained from a biomedical research model are often highly informative for clinical decision making, but its application in clinical practice can be hindered by the substantial computational effort. This creates a need for reduced-order modeling (ROM) approaches [12], which involve an offline stage in which a computationally demanding full-order model (FOM) is simplified to a computationally affordable reduced-order model. This reduced-order model – which should balance model details and accuracy with computational effort – is then used to support the clinical decision-making process in the online stage. In this work we intend to develop a ROM approach for a patient-specific cardiac mechanics model. We specifically consider the echocardiogram-based isogeometric analysis model developed by Willems et al. [9] (Figure 1) as the FOM. This model represents the anatomy of the ventricles by means of non-uniform rational B-splines (NURBS), which can be fitted to echocardiogram data [10]. The time-dependent mechanical behavior is modeled by a tissue-scale nonlinear continuum mechanics description, which is coupled to a lumped parameter model for the arterial system and a phenomenological activation potential relation. The model parameters are selected based on expertise from clinical research. This model computes the time-dependent mechanical response in the form of displacement fields, from which many other properties, such as stress fields and pressure-volume curves, are derived. When solving the model for multiple cardiac cycles, as required to reach cyclic steady-state conditions, typical simulation times surmount to multiple hours. Although the model settings can be optimized for computational efficiency, usage of this model for clinical decision making is impractical, especially because calibration procedures in that setting would require many evaluations of the model. The model would, however, be very suitable for application in the offline stage of a ROM approach, where it is used to generate training data. Figure 1: Schematic of the considered full-order cardiac model. Patient-specific input (circles) is mapped on patient-specific output (diamonds) by combining a NURBS-based fitting algorithm with an isogeometric cardiac solver. Besides the requirement that the ROM should be computationally affordable, it should also be able to incorporate as much of the information about a patient as possible. We therefore require the reduced-order model to use the same input as the FOM, i.e., the echocardiograms, tissue stiffness parameters and parameters for the circulatory system and activation potential law. We assume that for clinical decision making the pressure-volume curves are most important, and that displacement and stress fields as obtained from the full-order model are not essential in this setting. Optionally, such detailed output should, however, be obtainable in a subsequent offline evaluation of the full-order model. The choice for a specific ROM approach depends on the characteristics of the FOM on which it is based, as well as on functionality, accuracy and computational complexity requirements of the ROM. A myriad of methods to construct a ROM is available. Inspired by Peherstorfer et al.111The authors present their classification in a multi-fidelity modeling context. [13], we classify the ROM models resulting from these methods as: 1. A simplified reduced-order model is constructed from a FOM by making additional modeling assumptions, for example regarding the geometry or physics, to allow for mathematical simplification of the problem formulation. In essence, this follows a traditional modeling approach, in which domain-specific expertise is used to obtain a simplified ROM that adequately balances model functionality and accuracy on the one hand with computational complexity on the other. Consequently, in many research domains there is a wealth of literature on simplified ROMs. Due to the additional modeling assumptions, the simplified reduced-order model parameters and results in general do not coincide with those of the FOM on which it is based. For the model parameters, this means that available data for the FOM needs to be cast into a form suitable for the ROM, a process that typically involves loss of information. The results of simplified ROMs are typically less detailed compared to their FOM counterparts, restricting the objectives for which they can be used. 2. A projection-based reduced-order model retains the formulation of the FOM but reduces its complexity by identifying an adequate solution subspace in which this formulation is solved. Projection-based methods, of which proper orthogonal decomposition [14] and the reduced basis method [15] are prominent examples, exploit the mathematical structure of the FOM in a generic way and hence do not require (extensive) domain-specific expertise. A projection-based ROM typically has the same parameters and outcomes as the full-order model. The accuracy and computational complexity of the ROM can be balanced through the selection of the subspace on which the FOM is projected. For complex problems, for example incorporating time-dependent and nonlinear phenomena, finding an adequate balance can be complicated [16, 17]. 3. A data-fit reduced-order model – also commonly referred to as a surrogate model – provides an abstract mapping between the input and output data of the FOM based on a set of FOM results. Such methods, of which neural networks [18] and Gaussian processes [19] are prominent examples, in principle consider the FOM as a black box. Although standard data-fit ROMs are physics-agnostic, they need to be tailored to the specific input and output under consideration. The amount of training data needed to attain a data-fit ROM with sufficient accuracy depends on the input and output spaces, which can make the generation of a FOM data set computationally demanding in the case of large parameter spaces. We note that, as with any classification, not all ROM approaches fit perfectly, and that methods have been developed that combine ingredients of these classes. A typical example of such hybrid methods are physics-informed neural networks [20]. The ROM framework developed in this work, which is schematically illustrated in Figure 2, leverages functionality from different classes of ROMs. The core of the framework is the consideration of a simplified reduced-order cardiac model in the form of an organ-scale one-fiber cardiac model, similar to that of Arts et al. [21]. This simplified reduced-order modeling approach is motivated by the fact that the one-fiber reduced-order model is established in clinical practice as part of the CircAdapt model [22]. Projection-based methods are expected to be computationally demanding on account of the nonlinear and time-dependent character of the considered FOM, and data-fit ROMs are impractical on account of the number of FOM simulations that would be required for training. While the physiological model parameters of the FOM can, to a large extent, be used directly in the employed ROM, the scan data cannot be incorporated directly. In order to introduce this input into the reduced-order model, we consider a projection-based reduced-order model for the anatomy, in which we use a proper orthogonal decomposition to represent patient-specific geometries by a relatively small number of modal coefficients. A data-fit reduced-order model in the form of a Gaussian process is then used to map this geometric input onto the effective parameters of the cardiac ROM. Figure 2: Schematic of the developed ROM framework. The framework considers the same input (circles) as the FOM (Figure 1) on which it is based. Since the ROM framework incorporates the scan-fitting algorithm in the online stage, it generates the same spline anatomy output as the FOM (light blue filled diamond). In the online stage, it also generates hemodynamical and mechanical output in the same form as that of the FOM, albeit based on a simplified ROM (open diamonds). If required, these output quantities can also be evaluated using the FOM in a subsequent offline stage, after which they can be used as additional training data. We acknowledge that the choices made in our ROM framework are strongly driven by our own research experience and that alternative choices are possible and may even be better depending on the considered setting and goals. In light of this, our goal is to develop a versatile ROM framework, in the sense that it is not specific to our choice of models. Alternative cardiac models should be usable without making fundamental changes to the framework, and ideally different types of problems should also be possible to consider, provided that their structure is similar. This paper is organized as follows. In Section 2 we commence with the introduction of the two cardiac models that serve as the FOM and the ROM in our framework. In Section 3 we then introduce the Gaussian process that maps geometric inputs onto the ROM parameter space. Subsequently, Section 4 introduces the Bayesian calibration framework that is used to generate training data for this Gaussian process. With all elements of the ROM framework in place, in Section 5 we then first extensively test it in the context of an idealized NURBS ventricle parametrized by two geometric quantities. We then extend the application of the framework to a scan-based setting in Section 6, in which the geometric input quantities are replaced by a modal decomposition of the NURBS ventricle. Finally, conclusions and recommendations are presented in Section 7."
https://arxiv.org/html/2411.08149v1,Design optimization of semiconductor manufacturing equipment using a novel multi-fidelity surrogate modeling approach,"Careful design of semiconductor manufacturing equipment is crucial for ensuring the performance, yield, and reliability of semiconductor devices. Despite this, numerical optimization methods are seldom applied to optimize the design of such equipment due to the difficulty of obtaining accurate simulation models. In this paper, we address a practical and industrially relevant electrostatic chuck (ESC) design optimization problem by proposing a novel multi-fidelity surrogate modeling approach. The optimization aims to improve the temperature uniformity of the wafer during the etching process by adjusting seven parameters associated with the coolant path and embossing. Our approach combines low-fidelity (LF) and high-fidelity (HF) simulation data to efficiently predict spatial-field quantities, even with a limited number of data points. We use proper orthogonal decomposition (POD) to project the spatially interpolated HF and LF field data onto a shared latent space, followed by the construction of a multi-fidelity kriging model to predict the latent variables of the HF output field. In the ESC design problem, with hundreds or fewer data, our approach achieves a more than 10% reduction in prediction error compared to using kriging models with only HF or LF data. Additionally, in the ESC optimization problem, our proposed method yields better solutions with improvements in all of the quantities of interest, while requiring 20% less data generation cost compared to the HF surrogate modeling approach.","The semiconductor industry is rapidly advancing, driven by intense global competition and a persistent push to innovate. Semiconductor devices are continuing to shrink in size while growing increasingly complex, presenting significant challenges in design and manufacturing (Chien et al. (2011); Wong et al. (2020)). It has become increasingly critical and challenging to mitigate this growing complexity while simultaneously improving yield, cost-effectiveness, performance, and scalability. Addressing these challenges necessitates the use of design optimization methods to navigate these intricate and unintuitive trade-offs. The design of semiconductor manufacturing equipment involves many coupled parameters, but accurately modeling the impact of these parameters on metrics such as yield and performance is challenging. Experimentally measuring these quantities is not only costly but also time-consuming, while predicting these quantities through high-fidelity simulations requires substantial computational resources and time. As a result, developing fast and reliable predictive models for design optimization is difficult, due to the limited availability of high-quality data. The compounding challenges of optimizing many coupled design parameters with sparse data for modeling the influence of these parameters are common to many design problems in the development of the semiconductor manufacturing equipment. Here, we are specifically interested in a design problem involving the electrostatic chuck, where the goal is to maximize the uniformity of the temperature field on the wafer surface during the etching process. The electrostatic chuck is used to hold the semiconductor wafer during multiple stages of semiconductor manufacturing, including not only etching but also deposition and lithography. It is critical to precisely hold the wafer position through an electrostatic force, but it is also critical to limit the wafer temperature while minimizing variability across the surface through a carefully designed cooling system (O’Hanlon and Parks (1992); Bubenzer and Schmitt (1990)). The wafer temperature field can be predicted with relatively high accuracy using a dynamic heat-transfer simulation of the wafer coupled to a fluid dynamics simulation of the coolant flow (considered here as the high-fidelity model), or with lower accuracy but significantly reduced computational cost using a steady-state heat-transfer simulation with simplified assumptions for the coolant flow (considered here as the low-fidelity model) (Yoon et al. (2023)). This design problem presents several characteristics that make design optimization challenging. The first challenge is that both the high- and low-fidelity models have expensive evaluation times. The low-fidelity model has a roughly order-of-magnitude lower computation time, but its accuracy is not sufficient for effective design. The second challenge is that while the design space is moderate-dimensional, the state space (whose elements represent the discretized temperature field) is high-dimensional. Thus, any surrogate model used in place of the high- and low-fidelity models needs to model the entire temperature field, because the precise quantities of interest can differ across practical settings, in terms of both the spatial quantity (e.g., mean, maximum, standard deviation) and the localization (e.g., inner radial zone, outer radial zone). In practice, training a surrogate model to predict specific scalar quantities would require retraining frequently, whenever the application demands changes to the optimization objective or constraints, at significant manual effort and time. Additionally, the problem formulated here is representative of many challenges faced in the semiconductor manufacturing process, where often hundreds of etching steps involved. In each of these steps, similar design problems arise for the ESC, but under varying plasma conditions, chemical compositions, and radial frequency power. As a result, the ESC design must be optimized for each specific etching condition. This makes the data generation cost of the surrogate modeling approach we use particularly crucial, as it must be applied repeatedly across various etching processes. Motivated by these challenges, this paper presents a novel methodology for electrostatic chuck design optimization based on multi-fidelity surrogate modeling and proper orthogonal decomposition (POD). This methodology, specifically focused on temperature uniformity maximization, has three steps. The first step applies POD to reduce the dimension of the state space by computing a singular value decomposition given snapshots of the state vectors (i.e., the discretized, steady-state temperature field) at different points in the parameter space. The second step constructs a surrogate model from the low-fidelity simulation data that predicts the discretized temperature field in the reduced state space. Since the outputs are a small set of coefficients with respect to the reduced basis, our methodology permits use of one of a wide range of possible surrogate modeling approaches, such as kriging. The third step constructs a second surrogate model from the high-fidelity simulation data that also predicts the discretized temperature field in the reduced state space, where this surrogate model is the discrepancy function in a multi-fidelity model. We describe this novel methodology in the context of a specific, industrially relevant electrostatic chuck (ESC) design problem with seven design parameters, including emboss contact ratios and coolant channel dimensions. We report results found in applying the new methodology to this electrostatic chuck design problem, including the error due to the dimension reduction using POD, the accuracy of the multi-fidelity surrogate model compared to low- and high-fidelity surrogate models of roughly equivalent computational expense, and the validation of this methodology through evaluation of the computed optimized designs using the high-fidelity model, which is treated as the ground-truth model. This paper is organized as follows: Section 2 provides an overview of surrogate-based design optimization, multi-fidelity surrogate modeling, and semiconductor manufacturing equipment optimization. Section 3 details the ESC design optimization problem and outlines the context and motivation for multi-fidelity surrogate modeling. Section 4 presents the proposed multi-fidelity surrogate modeling approach. Section 5 presents the surrogate modeling results and demonstrates the application of the proposed method to solve the ESC optimization problem. Finally, Section 6 summarizes the findings and offers concluding remarks."
https://arxiv.org/html/2411.08647v1,"The Galactica database: an open, generic and versatile tool for the dissemination of simulation data in astrophysics","The Galactica simulation database is a platform designed to assist computational astrophysicists with their open science approach based on FAIR (Findable, Accessible, Interoperable, Reusable) principles. It offers the means to publish their numerical simulation projects, whatever their field of application or research theme and provides access to reduced datasets and object catalogs online. The application implements the Simulation Datamodel IVOA standard.To provide the scientific community indirect access to raw simulation data, Galactica can generate, on an ”on-demand” basis, custom high-level data products to meet specific user requirements. These data products, accessible through online WebServices, are produced remotely from the raw simulation datasets. To that end, the Galactica central web application communicates with a high-scalability ecosystem of data-processing servers called Terminus by means of an industry-proven asynchronous task management system. Each Terminus node, hosted in a research institute, a regional or national supercomputing facility, contributes to the ecosystem by providing both the storage and the computational resources required to store the massive simulation datasets and post-process them to create the data products requested on Galactica, hence guaranteeing fine-grained sovereignty over data and resources.This distributed architecture is very versatile, it can be interfaced with any kind of data-processing software, written in any language, handling raw data produced by every type of simulation code used in the field of computational astrophysics. Its generality and versatility, together with its excellent scalability makes it a powerful tool for the scientific community to disseminate numerical models in astrophysics in the exascale era.","The Amsterdam call for Open Science in 2016 started to promote public access to both the scientific publications and the data obtained with public funds. This call has been implemented at national level all over the world in recent years in the form of Open Science plans and programs to encourage the effective sharing of publications and research data. The first international framework on open science, the UNESCO Recommendation on Open Science (UNESCO, 2021), was adopted by 193 countries attending UNESCO’s General Conference in 2021. In recent years, data publication and reuse has become a key requirement demanded by both funding agencies and resource (computation or observation time) allocation committees. In the field of astrophysics, a number of initiatives have been taken to disseminate scientific data, mostly in astronomy and to a lesser extent, for numerical models in computational astrophysics. Online science platforms with astronomical data have been made available in the past few decades, e.g. the Sloan Digital Sky Survey (York et al., 2000), the Dark Energy Survey Science Portal (The Dark Energy Survey Collaboration, 2005; Fausti Neto et al., 2018) to provide the scientific community with access to theses surveys. In the same manner, similar dedicated science portals are currently being developed for observational data that will be produced by the Euclid space telescope (Laureijs et al., 2011), the Square Kilometer Array (SKA) telescope (Lazio, 2009) or the Vera Rubin Observatory (Ivezić et al., 2019). If the dissemination of astronomical data has been greatly facilitated by the standardization of file format, for example the FITS format (Wells et al., 1981), or the MeasurementSet (Kemball & Wieringa, 2000) standard in radio astronomy, computational astrophysicists lack a standardized data model to enable them to exchange their numerical simulation data in a uniform way. Nevertheless, a few simulation projects have released their data to the community, many of them in the cosmology field: e.g. the MultiDark (Prada et al., 2012) and Bolshoi (Klypin et al., 2011) simulations hosted on the CosmoSim database, the galaxy cluster merger catalog (ZuHone et al., 2018) hosted on the yt Hub, the Illustris project (Vogelsberger et al., 2014; Nelson et al., 2019), the web portal for cosmological hydrodynamical simulations (Ragagnin et al., 2017), CosmoHub (Carretero et al., 2017; Tallada et al., 2020) or the Theoretical Astrophysical Observatory (Bernyk et al., 2016). Some projects even attempted to combine infrastructures in the field of turbulence studies, e.g. the Johns Hopkins Turbulence Database (Li et al., 2008), the Catalogue for Astrophysical Turbulence Simulations (Burkhart et al., 2020) or in the field of star formation (the StarFormat database) or interstellar medium (ISMDB). One of the major collaborative efforts led to the release of the generic Django-Daiquiri framework (Galkin et al., 2020, and references therin), a web application developed with the aim of being deployed for each project to host the data produced by the numerical simulations conducted in the project. It greatly lowered the technical barrier individual research groups have to overcome to publish their data on a web application, but unfortunately the required technical expertise and maintenance expenses are still out of reach for a majority of small individual projects led by computational astrophysicists."
https://arxiv.org/html/2411.08550v1,"Graph Neural Networks in Supply Chain Analytics and Optimization: Concepts, Perspectives, Dataset and Benchmarks","Graph Neural Networks (GNNs) have recently gained traction in transportation, bioinformatics, language and image processing, but research on their application to supply chain management remains limited. Supply chains are inherently graph-like, making them ideal for GNN methodologies, which can optimize and solve complex problems. The barriers include a lack of proper conceptual foundations, familiarity with graph applications in SCM, and real-world benchmark datasets for GNN-based supply chain research. To address this, we discuss and connect supply chains with graph structures for effective GNN application, providing detailed formulations, examples, mathematical definitions, and task guidelines. Additionally, we present a multi-perspective real-world benchmark dataset from a leading FMCG company in Bangladesh, focusing on supply chain planning. We discuss various supply chain tasks using GNNs and benchmark several state-of-the-art models on homogeneous and heterogeneous graphs across six supply chain analytics tasks. Our analysis shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models by around 10-30% in regression, 10-30% in classification and detection tasks, and 15-40% in anomaly detection tasks on designated metrics. With this work, we lay the groundwork for solving supply chain problems using GNNs, supported by conceptual discussions, methodological insights, and a comprehensive dataset.","Supply chain is a dynamic network of organizations that participate in the various processes and activities that produce value in the form of products and services for consumers via upstream and downstream linkages (Mentzer et al. 2001), entailing a continuous flow of information, goods, and money among its various stages (Higuchi and Troutt 2004). As the supply chain consists of interconnected entities in a complex network, it involves intricate interdependencies and complex decision-making processes (Surana et al. 2005). Additionally, the modern supply chain generates enormous data, with relationships and dependencies between entities requiring sophisticated models to capture (Sadeghiamirshahidi et al. 2014). The potential benefits of using computational methods to solve supply chain problems include improved coordination, efficient logistics, and effective supply chain solutions (Chaovalitwongse and Furman 2013). Figure 1: Supply Chain as a Graph of Interconnected Company, Products, Distributors and Customers. Graph Neural Networks (GNNs) are widely used across various sectors due to their ability to model complex relationships in data. In social networks, GNNs help analyze user connections for recommendations, information analysis and community detection Wasi et al. (2024a). In biology, they assist in drug discovery by modelling molecular interactions (Johnson et al. 2024; Wasi et al. 2024b). GNNs are used in finance for fraud detection (Wu, Chao, and Li 2024), in HR for job matching Wasi (2024), and in research for knowledge graph reasoning Kosasih et al. (2022) and anomaly detection Sudrich, Borges, and Beigl (2017); Zhou et al. (2023). Their strength lies in effectively capturing dependencies in non-Euclidean data, making them versatile for tasks involving connected entities. Connecting to supply chain, GNNs can enable the modelling of complex relationships and dependencies within the supply chain, facilitating tasks such as sales predictions, production planning, risk assessment, and uncovering hidden risks (Zhou et al. 2023; Kosasih et al. 2022). By leveraging GNN methodologies, it is possible to optimize supply chain operations, enhance risk management, and improve decision-making processes by extracting relevant information from graph data and inferring multiple types of hidden relationship risks (Kosasih et al. 2022). Production planning plays a pivotal role in supply chain management by forecasting future product or service demand, aiding organizations in optimizing inventory levels, production schedules, and resource allocation (Aamer, Eka Yani, and Alan Priyatna 2020; Zougagh, Charkaoui, and Echchatbi 2020). The accuracy of demand prediction significantly impacts company revenue, prompting the exploration of diverse deep learning and machine learning models (Alves, Silva, and Mateus 2021; Pirhooshyaran and Snyder 2020; Pacella and Papadia 2021). While traditional models have shown promise, GNNs offer a unique advantage in modelling the network-like structures inherent in supply chains, such as global trade flows or social networks (Kosasih and Brintrup 2022), as shown in Figure 1. Despite limited prior studies on GNNs in supply chains, recent research has demonstrated their utility in tasks like hidden link prediction to mitigate risk and uncover hidden dependencies (Aziz et al. 2021). However, several challenges remain: there is a lack of comprehensive conceptual foundations and formulations specific to supply chain applications of GNNs, and researchers often lack awareness of the diverse tasks that GNNs can address in this domain. Moreover, the scarcity of publicly available datasets and proper benchmarks hinders thorough evaluation and development of GNN models for supply chains. These gaps underscore the need for detailed methodologies and robust datasets to advance research and practical applications of GNNs in supply chain optimization. To address these challenges, we take several key steps. First, we thoroughly discuss and connect supply chains with graph structures, formulating them as graphs for effective GNN application. We provide detailed definitions and explanations of how supply chain components map to graph properties, illustrating how GNNs can utilize these properties to tackle supply chain problems efficiently. Second, we introduce a new, multi-perspective benchmark dataset tailored for GNN applications in supply chain planning, accompanied by a comprehensive exploratory study of the dataset. Third, we explore a variety of supply chain tasks from different graph perspectives and benchmark state-of-the-art models on both homogeneous and heterogeneous graphs across six distinct analytics and modelling tasks. Our experiments demonstrate that GNN-based models significantly outperform traditional statistical and machine learning methods in various supply chain applications, underscoring the vital role of GNNs in advancing supply chain analysis and modelling. The contributions of this research are summarized into these key aspects: • We comprehensively discuss and connect supply chains with graphs, and formulate supply chains as graphs for effective application of GNNs. We define and explain how various supply chain components correspond to graph properties, and demonstrate how GNNs can leverage these properties to solve supply chain problems efficiently. • We introduce a new, multi-perspective benchmark dataset for GNN problem-solving in supply chain planning, paving the way for extensive research and analytics using GNNs in supply chain management. Additionally, we present a comprehensive exploratory study of the dataset. The dataset is publicly available at DOI: 10.5281/zenodo.13652826 under the CC BY 4.0 Licence. • We describe and explore various tasks of supply chain from different graph perspectives like homogeneous graph, heterogeneous graph and hypergrphs; bench-marking the performance of state-of-the-art models on both homogeneous and heterogeneous graphs across six different supply chain analytics and modelling tasks on the dataset. • Our dataset analysis and modelling experiments show that graph-based models outperform traditional statistical and machine learning methods in various supply chain applications, highlighting the critical role of GNNs in effective supply chain analysis and modelling. • We also discuss real-life implications of the work, addressing their societal impacts and offering insights into potential future research directions; along with limitations of the current study and supply chain as graph methods. With these contributions, we aid to lay the groundwork for advancing the application of GNNs in supply chain contexts. The following sections will review the related works and provide a detailed analysis of machine learning and GNN approaches in supply chain management."
https://arxiv.org/html/2411.08257v1,GPTree: Towards Explainable Decision-Making via LLM-powered Decision Trees,"Traditional decision tree algorithms are explainable but struggle with non-linear, high-dimensional data, limiting its applicability in complex decision-making. Neural networks excel at capturing complex patterns but sacrifice explainability in the process. In this work, we present GPTree, a novel framework combining explainability of decision trees with the advanced reasoning capabilities of LLMs. GPTree eliminates the need for feature engineering and prompt chaining, requiring only a task-specific prompt and leveraging a tree-based structure to dynamically split samples. We also introduce an expert-in-the-loop feedback mechanism to further enhance performance by enabling human intervention to refine and rebuild decision paths, emphasizing the harmony between human expertise and machine intelligence. Our decision tree achieved a 7.8% precision rate for identifying “unicorn” startups at the inception stage of a startup, surpassing gpt-4o with few-shot learning as well as the best human decision-makers (3.1% to 5.6%).","Thanks to their explainability, decision trees are among the most intuitive and popular methods in machine learning (second only to linear regression). Their tree-like structure enables users to easily follow the decision-making process, with each split representing an interpretable rule based on the input data. This transparency is valuable in domains such as the Venture Capital (VC) industry where the stakes are high for each investment decision. In practice, however, decision trees often underperform when faced with non-linear, high-dimensional datasets and are inherently unsuitable for text-rich and multi-modal datasets. Thus, our work to extend decision trees is motivated by this fundamental question: how to incorporate LLMs? 05101520Best GPTreemodelGPTreew/expertGPTreeTier-1 seedfundsgpt-4oIndexingstrategy17.97.87.25.63.11.9Precision (%) Figure 1: Comparison of Different Models/Methods In recent years, Large Language Models (LLMs) have emerged as powerful tools capable of capturing the intricacies of natural language: models such as gpt-4o and gpt-1o preview have showcased exceptional capabilities in advanced reasoning and multi-modal tasks. However, LLMs are often seen as “black boxes” due to their elusive architectures. In addition, prompt engineering techniques like chain-of-thought and tree-of-thought, combined with extensive prompt chaining, are frequently required to produce accurate and contextually relevant responses. Not only does this require considerable human intervention and expertise but is also prone to trial-and-error procedures in crafting prompts that provide meaningful outputs. Therefore, we address another question: how to design a robust and explainable approach that minimizes human intervention while maintaining high performance? In this paper, we present GPTree, a novel framework that combines the explainability of decision trees with the advanced reasoning capabilities of LLMs, also incorporating an efficient expert-in-the-loop feedback system. Figure 2: GPTree pipeline In summary, our work makes the following contributions: • We introduce an LLM-powered decision tree model to dynamically split samples using a combination of LLM inference, code-based and clustering nodes, giving users the full explainability of traditional decision trees along with the flexibility of working with unstructured text and potentially multimodal datasets. • We eliminate the need for feature engineering and prompt chaining, instead replacing it with our expert-in-the-loop feedback mechanism. This further enhances performance by enabling a human expert to refine and rebuild decision paths post-training, leveraging the cooperation potential between human expertise and machine intelligence. • We conduct a comprehensive empirical evaluation of our approach within the VC landscape, where explainable decision-making is essential. Our experimental analysis demonstrates the effectiveness of our approach at identifying “unicorn” startups in comparison to human decision-makers within the industry, based on data collected from over 115K US-based companies founded more than 8 years ago."
https://arxiv.org/html/2411.07560v1,EUR/USD Exchange Rate Forecasting incorporating Text Mining Based on Pre-trained Language Models and Deep Learning Methods,"This study introduces a novel approach for EUR/USD exchange rate forecasting that integrates deep learning, textual analysis, and particle swarm optimization (PSO). By incorporating online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior performance compared to traditional econometric and machine learning models. The research employs advanced text mining techniques, including sentiment analysis using the RoBERTa-Large model and topic modeling with LDA. Empirical findings underscore the significant advantage of incorporating textual data, with the PSO-LSTM model outperforming benchmark models such as SVM, SVR, ARIMA, and GARCH. Ablation experiments reveal the contribution of each textual data category to the overall forecasting performance. The study highlights the transformative potential of artificial intelligence in finance and paves the way for future research in real-time forecasting and the integration of alternative data sources.","In the intricate tapestry of the global financial market, the exchange rate between the Euro and the US Dollar (EUR/USD) stands as a pivotal thread, weaving together the economic narratives of two powerhouse currencies. As a barometer of the economic relations between the Eurozone and the United States, the EUR/USD exchange rate holds profound implications for international trade, cross-border investments, and the overall health of the global economy. Accurately forecasting this exchange rate has become a holy grail for financial market participants, as it can provide a strategic edge in navigating the complex and ever-shifting currents of the foreign exchange market [1, 2]. Recent advancements in the field of EUR/USD exchange rate forecasting have been propelled by the advent of machine learning and the proliferation of big data. State-of-the-art methodologies have harnessed the power of deep learning architectures, such as Long Short-Term Memory (LSTM) networks, to capture the non-linear and temporal dependencies inherent in financial time series data [3, 4]. These models have shown promising results in predicting exchange rate movements by learning from vast amounts of historical data and uncovering hidden patterns that traditional econometric models might overlook [5]. However, despite the significant strides made by these cutting-edge approaches, there remain notable limitations. One critical drawback is the reliance on structured, quantitative data, such as historical prices and economic indicators [6]. While these data points provide valuable insights, they fail to fully capture the rich tapestry of qualitative information that shapes market sentiment and drives currency fluctuations. News articles, financial reports, and social media discussions often contain nuanced and real-time insights that can significantly influence exchange rate dynamics. Neglecting these qualitative data sources can lead to an incomplete understanding of the market and suboptimal forecasting performance. To address this challenge, we propose a novel approach that seamlessly integrates both quantitative and qualitative data to enhance the accuracy and robustness of EUR/USD exchange rate forecasting [7, 8]. Our methodology leverages the power of LLMs, specifically GPT-4, to process and extract relevant information from vast amounts of textual data. By employing advanced techniques such as prompt engineering and sentiment analysis, we enrich the predictive power of LSTM networks, enabling them to consider both historical price patterns and real-time market sentiments [9, 10]. Furthermore, we incorporate PPSO to fine-tune the hyperparameters of our model, ensuring optimal performance and adaptability to changing market conditions [11]. This research introduces several innovative contributions to the field of financial forecasting, enhancing existing methodologies and introducing novel approaches: • We demonstrate how to effectively integrate state-of-the-art LLMs for preprocessing and annotating financial data, significantly improving the quality and relevance of the dataset for forecasting purposes. This involves utilizing LLMs and prompt engineering techniques to filter noise from high-noise news sources, ensuring cleaner and more accurate data for analysis. • Our study showcases the combined use of twitter-RoBERTa-Large-topic-sentiment-latest and RoBERTa-Large models for extracting sentiment and identifying hidden relationships within textual data, thereby deepening and expanding the analytical scope. • We introduce an innovative approach to EUR/USD exchange rate forecasting by employing a PSO-LSTM model, a concept that is relatively novel in this domain. This method signifies a shift from traditional econometric models to a comprehensive framework that integrates qualitative and quantitative data for improved forecasting accuracy. The remainder of this paper is structured as follows: Section 2 provides a comprehensive literature review, positioning our research within the existing body of knowledge and highlighting the gaps we aim to address. Section 3 delves into the methodology and system design of our approach, including a detailed explanation of prompt engineering, LLMs, transformer architecture, and the PSO-LSTM model. Section 4 presents our empirical results, showcasing the performance of our model in comparison to baseline methods. Section 5 discusses the implications of our findings and conducts ablation studies to validate the contribution of each component of our approach. Finally, Section 6 concludes the paper, summarizing our key contributions, limitations, and outlining future research directions."
https://arxiv.org/html/2411.07310v1,Advancements in Constitutive Model Calibration: Leveraging the Power of Full-Field DIC Measurements and In-Situ Load Path Selection for Reliable Parameter Inference,"Accurate material characterization and model calibration are essential for computationally-supported high-consequence engineering decisions. Current characterization and calibration methods (1) use simplified test specimen geometries and global data, (2) cannot guarantee that sufficient characterization data is collected for a specific model of interest, (3) use deterministic methods that provide best-fit parameter values with no uncertainty quantification, and (4) are sequential, inflexible, and time-consuming.This work brings together several recent advancements into an improved workflow called Interlaced Characterization and Calibration (ICC) that advances the state-of-the-art in constitutive model calibration. The ICC paradigm (1) employs tools to efficiently use full-field data to calibrate high-fidelity material models, (2) aligns the data needed with the data collected by adopting an optimal experimental design protocol, (3) quantifies parameter uncertainty through Bayesian inference, and (4) incorporates these advances into a quasi real-time feedback loop. The ICC framework is demonstrated here on the calibration of a material model using simulated full-field data for an aluminum cruciform specimen being deformed bi-axially. The cruciform is actively driven through the myopically optimal load path using Bayesian optimal experimental design, which selects load steps that yield the maximum expected information gain (EIG). To aid in numerical stability and preserve computational resources, the full-field data is dimensionally reduced via principal component analysis (PCA), and fast surrogate models which approximate the input-output relationships of the expensive finite element model are used. The tools developed and demonstrated here show that high-fidelity constitutive models can be efficiently and reliably calibrated with quantified uncertainty, thus supporting credible decision-making and potentially increasing the agility of solid mechanics modeling by enabling utilization of computational simulations at earlier stages of the design cycle.††SAND2024-15320O","Computational simulation is relied upon to understand complex engineering problems and provides essential information in decision-making. In finite element analysis of solid mechanics, constitutive relationships describe the response of a material to various external stimuli. Typically, these constitutive equations contain material-dependent parameters that are determined through material characterization and model calibration. Calibration and subsequent validation of the constitutive equations and parameters is an essential part of accurate computational simulations and structural analyses. Thus, there is a corresponding level of emphasis placed on the calibration process, both in terms of efficiency and accuracy, in order to establish confidence in predictions of the engineering problem at hand. Traditional characterization and calibration procedures are often suboptimal in terms of efficiency and accuracy, which challenges the effective use of computational simulation for design purposes. The two processes (material characterization and model calibration) are typically performed sequentially and independent of each other, and it is not uncommon for multiple experimental campaigns to be needed in order to obtain the proper data for calibration [1]. In addition to the procedure, the type of data collected is an important consideration for calibration efficiency. Traditionally, tests which target specific stress states (i.e., uniaxial tension, notched tension, top hat shear, etc.) have been used for calibration. These tests provide limited information per specimen; therefore, large test matrices are needed which include various tests, sample geometries and cut directions in order to reveal complex material behaviors. As a result of the current characterization and calibration workflow in tandem with traditional testing procedures, the model calibration process is elongated and expensive, thus delaying the support of computational simulation for design and engineering decisions. Several recent advancements have led to improvements in constitutive model calibration. For one, the use of digital image correlation (DIC) to collect rich full-field data through camera-based measurements has led to a paradigm shift in material characterization. Also known by the community as Material Testing 2.0 (MT2.0) [2], full-field DIC data collected from complex testing configurations (specimen shape plus loading) has greatly improved the information content that can be garnered from a single experiment for model calibration [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]. Moreover, researchers have developed several different methods—including Chebychev and Zernicke polynomials [14, 15, 16, 17, 18, 19] and various types of wavelets [20, 21, 22]—for decomposing the field data into a spectral domain in order to reduce the dimensionality of the data while retaining its rich information content. However, selection of the proper experimental configuration, or design, which greatly influences the suitability of the data for model calibration, remains a challenging task. Thus, the second area of improvement for model calibration is the development of procedures for optimal experimental design within the context of solid mechanics. In general, there are two different approaches which have been adopted: 1) optimization of the geometry of the specimen [23, 24, 25, 10], or 2) optimization of the load path of deformation [26, 27]. Both approaches use various techniques to construct an experiment which will yield the most useful data for model calibration, and, consequently, improve constitutive model calibrations and predictions. While the advancements made in material characterization and experimental design are significant, they have been, until this point, developed independently of each other. The contribution of this work is to drive forward the state-of-the-art in model calibration by bringing together these recent advancements into one improved workflow by 1) developing tools to efficiently use full-field data to calibrate high-fidelity material models and 2) aligning the data needed with the data collected by adopting the optimal experimental design protocol introduced in Ricciardi et al. [27]. The new workflow, which has been coined Interlaced Characterization and Calibration (ICC), actively controls the load path of a complex specimen in situ. The ICC framework was validated in [27] for a simplified exemplar problem of deforming a material point and calibrating with stress-strain data. This work extends the ICC framework from a material point to a complete structural problem of a cruciform specimen tested in a planar biaxial load frame. The ICC framework is demonstrated synthetically with the simulated deformation of the cruciform specimen in order to calibrate a plasticity model for an aluminum alloy. The development of the tools required for this demonstration are a critical stepping stone in moving towards utilization of the ICC framework with real-time data collection and model calibration. The remainder of this paper is organized as follows: First, the components of the ICC framework as applied to load path selection of a cruciform specimen in a biaxial load frame are presented (Sec. 2). Next, a discussion of the experimental setup, constitutive model, statistical methods and dimension reduction of full-field data, as well as all other details of the framework are described in detail (Sec. 3). The ICC framework is demonstrated on an exemplar problem to calibrate an elasto-plastic model using synthetically generated full-field DIC and global load data (Sec. 4). Finally, various ICC algorithmic decisions and considerations for future work are discussed (Sec 5). In summary, this work brings together several recent advancements in model calibration into one enhanced calibration framework to improve both the efficiency and results of constitutive model calibration."
https://arxiv.org/html/2411.07800v1,Kernel-based retrieval models for hyperspectral image data optimized with Kernel Flows,"Kernel-based statistical methods are efficient, but their performance depends heavily on the selection of kernel parameters. In literature, the optimization studies on kernel-based chemometric methods is limited and often reduced to grid searching. Previously, the authors introduced Kernel Flows (KF) to learn kernel parameters for Kernel Partial Least-Squares (K-PLS) regression. KF is easy to implement and helps minimize overfitting. In cases of high collinearity between spectra and biogeophysical quantities in spectroscopy, simpler methods like Principal Component Regression (PCR) may be more suitable. In this study, we propose a new KF-type approach to optimize Kernel Principal Component Regression (K-PCR) and test it alongside KF-PLS. Both methods are benchmarked against non-linear regression techniques using two hyperspectral remote sensing datasets.","Hyperspectral imagery is unlocking possibilities for environmental monitoring. Environmental retrieval models that utilize remotely sensed data are already being used to track water quality [1], agriculture [2], vegetation health [3], and in climate observations [4]. Deep learning has been proven to be successful for retrieval model creation from hyperspectral data [5]. However, it comes with limitations. If for instance a large number of parameters are optimized or the network is too deep, the models are prone to overfitting. Another common drawback is that the interpretability of deep learning models is limited, and a significant amount of data is often needed to train an unbiased model. Such large and complete datasets to train accurate deep learning models are difficult to produce. Airborne or spaceborne hyperspectral datasets with synchronized ground measurements are challenging and costly to put together [6]. Data needs to be collected adequately over space and time, which also requires obtaining field measurements over the full time period. The size of the datasets required by traditional chemometric models, such as the partial least-squares regression (PLS), is, however, smaller. This comes at the cost of reduced ability to model non-linear relationships between spectra and biogeophysical quantities. When the input and output are highly collinear, simpler approaches such as the Principal Component Regression (PCR) may be more appropriate [7]. Kernelized versions of chemometric methods are able to achieve improved performance in the case of non-linear dependency modeling, but learning the kernel function and its parameters is a non-trivial task. In literature, this is achieved through, for example, kernel target alignment, feature space matrix or genetic algorithm [8]. In this paper, we propose using Kernel Flows (KF) [9] to optimize chemometric retrieval models. The benefits of KF include: (i) reduced data overfitting through its cross-validation approach, (ii) ease of usage, as it converges to correct parameters regardless of initial values, and (iii) the ability to achieve global optimal values instead of local minima. We extend our previous work on optimizing kernel parameters for K-PLS [8] to Kernel PCR (K-PCR), illustrated with two hyperspectral data cases: a soil moisture soft sensor and a vegetation trait model. A newly developed cross-validation error loss function [10] was utilized in adapting the KF workflow to K-PCR and was also tested on KF-PLS."
https://arxiv.org/html/2411.06565v1,Foundation Model for Composite Materials and Microstructural Analysis,"The rapid advancement of machine learning has unlocked numerous opportunities for materials science, particularly in accelerating the design and analysis of materials. However, a significant challenge lies in the scarcity and high cost of obtaining high-quality materials datasets. In other fields, such as natural language processing, foundation models pre-trained on large datasets have achieved exceptional success in transfer learning, effectively leveraging latent features to achieve high performance on tasks with limited data. Despite this progress, the concept of foundation models remains underexplored in materials science. Here, we present a foundation model specifically designed for composite materials. Our model is pre-trained on a dataset of short-fiber composites to learn robust latent features. During transfer learning, the MMAE accurately predicts homogenized stiffness, with an R2 score reaching as high as 0.959 and consistently exceeding 0.91, even when trained on limited data. These findings validate the feasibility and effectiveness of foundation models in composite materials. We anticipate extending this approach to more complex three-dimensional composite materials, polycrystalline materials, and beyond. Moreover, this framework enables high-accuracy predictions even when experimental data are scarce, paving the way for more efficient and cost-effective materials design and analysis.","The mechanical properties of composite materials are highly dependent on their microstructure. Understanding and predicting the effective properties of composites based on their microstructural configurations is crucial for designing and optimizing advanced materials. Machine learning (ML) models have emerged as valuable tools for rapidly predicting these properties, opening new avenues in materials science research [1, 2, 3, 4, 5, 6]. For instance, Abueidda et al. [3] developed a convolutional neural network (CNN) to predict the mechanical properties of two-dimensional checkerboard composites. In addition to CNNs, other architectures have been employed to address different aspects of material property prediction. Mianroodi et al. [4] utilized a U-Net architecture to predict the stress fields in polycrystalline materials, demonstrating the ability of ML models to capture complex stress distributions. Graph neural networks (GNNs) have also been integrated with deep material networks to predict the mechanical responses of composites [5]. Furthermore, Zhongbo et al. [6] employed a pre-trained transformer model to predict the mechanical response of elastoplastic composite materials, illustrating the versatility of transformer architectures in materials modeling. While these models have achieved considerable success, they predominantly rely on supervised learning, which requires large, labeled datasets for training. The necessity of extensive datasets poses a significant challenge due to the scarcity and high cost of acquiring training data, whether through experiments or simulations. Moreover, many of these models are problem-specific and may not generalize to different tasks, limiting their broader applicability [7]. In contrast, self-supervised learning approaches have revolutionized fields such as natural language processing (NLP) and computer vision by reducing the dependence on labeled data. Notably, models like BERT (Bidirectional Encoder Representations from Transformers) [8, 9] have utilized masked language modeling to learn rich representations from unlabeled text data. The advent of foundation models has further centralized information from diverse data modalities, enabling a single model to be adapted for a wide range of downstream tasks [10, 11]. The concept of foundation models has also been successfully applied to computer vision. Masked autoencoders (MAEs), introduced by He et al. [12], employ a strategy analogous to BERT by masking portions of the input image and training the model to reconstruct the missing parts. In these MAEs, the encoder is replaced with vision transformers (ViTs) [13], and the decoder is a lightweight transformer designed for reconstruction. MAEs have demonstrated performance that, in some instances, outperforms state-of-the-art contrastive learning methods with ViTs, highlighting the efficacy of self-supervised learning in visual domains [14]. Despite these advancements, exploring foundation models within materials science remains limited. Applying self-supervised learning to materials offers the potential to overcome data scarcity and enhance model generalizability across different tasks and material systems. Therefore, in this work, we propose a foundation model specifically designed for composite materials, termed the material masked autoencoder (MMAE), to validate the feasibility of this approach. We generated a dataset comprising 100,000 grayscale images of short-fiber composites to facilitate self-supervised learning. By pre-training the MMAE on this dataset, the model learns robust and representative embeddings of the composite microstructures without reliance on labeled data. We anticipate that these embeddings capture essential features of the microstructures, making them highly informative for downstream tasks. To evaluate the effectiveness of the pre-trained MMAE, we employed direct numerical simulation (DNS) to generate datasets of homogenized stiffness for two types of composites: short-fiber composites and circular inclusion composites. We then fine-tuned the pre-trained models on significantly smaller labeled datasets to predict the homogenized stiffness of composites. Our results demonstrate that the MMAE achieves high accuracy, with a coefficient of determination (R2) reaching as high as 0.959 and consistently exceeding 0.91 even when trained on limited data. This performance highlights the potential of the MMAE as a foundation model for composite materials and paves the way for more efficient and cost-effective materials design and analysis."
https://arxiv.org/html/2411.06065v1,DFT: A Dual-branch Framework of Fluctuation and Trend for Stock Price Prediction,"Stock price prediction is of significant importance in quantitative investment. Existing approaches encounter two primary issues: First, they often overlook the crucial role of capturing short-term stock fluctuations for predicting high-volatility returns. Second, mainstream methods, relying on graphs or attention mechanisms, inadequately explore the temporal relationships among stocks, often blurring distinctions in their characteristics over time and the causal relationships before and after. However, the high volatility of stocks and the intricate market correlations are crucial to accurately predicting stock prices. To address these challenges, we propose a Dual-branch Framework of Fluctuation and Trend (DFT), which decomposes stocks into trend and fluctuation components. By employing a carefully design decomposition module, DFT effectively extracts short-term fluctuations and trend information from stocks while explicitly modeling temporal variations and causal correlations. Our extensive experiments demonstrate that DFT outperforms existing methods across multiple metrics, including a 300% improvement in ranking metrics and a 400% improvement in portfolio-based indicators. Through detailed experiments, we provide valuable insights into different roles of trends and fluctuations in stock price prediction. Code is available at https://github.com/cq-dong/DFT˙25.","Stock price prediction is a fundamental task in the field of quantitative investment (Fan and Shen 2024). However, predicting stock price trends is extremely challenging due to the high volatility and chaos of the stock market. Active trading behaviors such as buying and selling by investors drive stock price fluctuations. In addition, the stock market is also affected by many factors, including economic indicators, financial reports, political events, investor sentiment, etc. (Qian et al. 2024). Achieving a high prediction accuracy remains an ongoing challenge in this domain. Figure 1: Overall performance comparsion on the CSI800 and S&P500 stock datasets. Many works have achieved remarkable results in improving prediction performance. Traditional machine learning methods, such as decision trees and support vector machines, are used to model stock return changes (Nugroho, Adji, and Fauziati 2014; Chen and Guestrin 2016; Kamble 2017; Xie et al. 2013). However, these methods require the manual construction of large financial indicator features, and it is difficult to model complex dynamic correlations between stocks. With the advancement of deep learning, existing methods use the powerful representation ability of neural networks to mine stock trends and correlations. At present, there are two main research ideas for stock return prediction. (1) Time correlation. Stock price movements are caused by continuous changes in supply and demand, and time trend changes have obvious dependencies. Examples include recurrent neural networks (Nelson, Pereira, and De Oliveira 2017; Cho et al. 2014), convolutional neural networks (Bai, Kolter, and Koltun 2018), which model stock trends through individual stock time series features. (2) Stock correlation. Different stocks in the market form complex and dynamic dependencies due to various factors such as division of labor and industry status, and stock price fluctuations will affect each other. Fusion of stock spatiotemporal features through graphs or self-attention mechanisms (Qian et al. 2024; Xia et al. 2024; Li et al. 2024) can effectively improve prediction capabilities. However, these methods face two limitations. First, existing works ignore the importance of mining individual stock fluctuation information to predict stock returns. Stock trend information is a long-term, relatively stable change, representing the overall direction; stock fluctuation information refers to the short-term change component after removing the long-term trend, representing the profitability of the stock itself and reflecting the change in the power comparison of the long-short game. Fully exploiting volatility information can capture the implied price-volume patterns from the individual stock perspective and enhance forecasting ability when combined with the overall trend. Previous works often treat stock information as a whole without distinguishing between trend and fluctuation components, or they perform a simple decomposition followed by fusion and embedding, which leads to the mutual interference of trends and fluctuations between different stocks in the market. Second, they often oversimplify the time correlation of stocks. Specifically, the correlation of stocks is dynamically updated rather than fixed, and the features of different time steps have time series feature differences. In addition, the interaction of stock representations depends on the time order. The information on earlier steps will affect the information on later steps and vice versa. However, when conducting correlation mining, existing methods either perform time alignment operations (Yoo et al. 2021a), which oversimplifies the unique properties of individual stocks at different time steps, or fail to distinguish the temporal representation of stocks, which ignores the causal relationship before and after the stock information (Li et al. 2024), weakening the model’s time modeling ability. To solve the above problems, we propose a Dual-branch Framework of Fluctuation and Trend (DFT) for Stock Price Prediction. We believe that fluctuation information is a unique attribute of individual stocks, while trends are the concentrated expression of many stocks. Therefore, we design a fluctuation-trend dual-branch model to extract features separately. By separately modeling the order of correlation, we avoid the interference of fluctuation between stocks, and can give a full play to the different roles of the two in capturing stock characteristics, which is more conducive to prediction returns. In addition, we skillfully combine the advantages of RWKV time series representation and the relationship modeling ability of the self-attention mechanism while preserving the attributes of different time steps, maintaining the causal correlation of time series, and fully mining the complex cross-time causal relationship and stock correlation. Our contributions are summarized as follows: • We propose a new dual-branch stock prediction framework to effectively capture the temporal and stock correlations from two aspects: fluctuation and trend. To the best of our knowledge, this is the first stock price prediction model that uses a learning-based method to capture long-term trends and short-term fluctuation information simultaneously. • We consider the role of differentiated representations of different time steps in time series correlation and causal correlation and fully mine the complex time dependency in stock data. • We conducted experiments to validate the designs of our proposed method and demonstrated its superiority compared to baselines. Through a comprehensive analysis, we provide valuable insights into the different roles of trends and fluctuations in stock price forecasting and the impact of temporal causality."
https://arxiv.org/html/2411.07228v1,"Tooling or Not Tooling?
The Impact of Tools on Language Agents for Chemistry Problem Solving","To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents’ ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.","Large language models (LLMs) have demonstrated impressive problem-solving capabilities in many disciplines (wang2024mmlupro; yue2024mmmu; grossmann2023ai). When it comes to chemistry, LLMs still face significant challenges, such as incorrect calculation, lack of domain knowledge, or inability to perform certain tasks like reaction prediction (guo2023eighttask; mirza2024chembench). To address these limitations, LLM-based agents integrated with tools have been proposed to tackle chemistry-specific problems (wang2024agentsurvey; ramos2024chemistryagentsurvey). For example, ChemCrow (bran2023chemcrow) expands LLMs’ capabilities by incorporating 18 tools, ranging from web search to chemical reaction prediction. Similarly, Coscientist (boiko2023coscientist) integrates the control of cloud labs to enable LLMs to automate wet lab experiments. Despite the promise of these tool-augmented agents, existing evaluations have been largely qualitative and limited in scope. For example, ChemCrow is assessed with only 14 individual tasks mainly focusing on compound synthesis, and Coscientist’s evaluation involves merely six specific tasks. These narrow assessments leave a large gap in our understanding of how tool-augmented agents perform across diverse chemistry tasks in real-world applications. In this work, we conduct a comprehensive evaluation of LLM-based agents on different chemistry tasks to grasp a deep understanding of their potential and limitations. To explore and enhance the capabilities of agents in diverse and complex chemistry scenarios, we introduce ChemAgent, a new chemistry agent capable of handling a wide spectrum of tasks. It leverages the ReAct framework (yao2023react) and integrates 29 tools, such as a search tool for PubChem (kim2019pubchem), several molecular property predictors, as well as many practical tools present in ChemCrow. Then, we adapt two categories of real-world chemistry problems for systematic evaluation: specialized tasks and general questions. For specialized tasks, we use SMolInstruct (yu2024llasmol), which contains 14 types of specialized molecule- and reaction-centric tasks. For general questions, we use MMLU-Chemistry and GPQA-Chemistry, which are chemistry-related subsets of the MMLU (hendryckstest2021mmlu) and GPQA (rein2023gpqa) benchmarks, containing exam-like questions ranging from high school, college, to graduate level. Through comprehensive experiments, we show that: While ChemAgent substantially outperforms ChemCrow on all chemistry tasks, it does not consistently outperform the base LLMs without tools. In addition, the impact of tool augmentation is highly dependent on task characteristics. For specialized chemistry tasks involving professional molecular representations (e.g., SMILES (weininger1988smiles)) and specialized chemical operations (e.g., compound synthesis), augmenting LLMs with task-specific tools can yield substantial performance gains. Nonetheless, for general chemistry questions that require fundamental knowledge and extensive reasoning, ChemAgent cannot address these challenges adequately and underperforms the base LLMs. Further analysis along with a chemistry expert shows that ChemAgent’s underperformance on general chemistry questions is primarily due to delicate mistakes at intermediate stages of its problem-solving process, such as wrong reasoning steps and information oversight. Overall, our findings indicate that tool augmentation may introduce additional complexity that hinders LLM reasoning and thus does not always help in chemistry problem-solving. Future research may improve LLM-based agents for chemistry by optimizing cognitive load and enhancing reasoning and information verification abilities."
https://arxiv.org/html/2411.06566v1,A Fully Analog Pipeline for Portfolio Optimization,"Portfolio optimization is a ubiquitous problem in financial mathematics that relies on accurate estimates of covariance matrices for asset returns. However, estimates of pairwise covariance could be better and calculating time-sensitive optimal portfolios is energy-intensive for digital computers. We present an energy-efficient, fast, and fully analog pipeline for solving portfolio optimization problems that overcomes these limitations. The analog paradigm leverages the fundamental principles of physics to recover accurate optimal portfolios in a two-step process. Firstly, we utilize equilibrium propagation, an analog alternative to backpropagation, to train linear autoencoder neural networks to calculate low-rank covariance matrices. Then, analog continuous Hopfield networks output the minimum variance portfolio for a given desired expected return. The entire efficient frontier may then be recovered, and an optimal portfolio selected based on risk appetite.","Portfolio optimization involves creating an investment portfolio that balances risk and return. The objective is to allocate assets optimally to maximize expected returns while minimizing risk. Naturally, this problem is of great interest to financial organizations and is pivotal in risk management. However, the problem, formulated by Markowitz’s mean-variance model [1], must be solved in practice. Namely, it is well known that estimates of pairwise covariance between assets are notoriously poor [2]. A large financial company may have hundreds of thousands of assets n covering equities, bonds, derivatives, and more, but with only a small sample of observations over the desired timescale. The samples tend to include significant amounts of noise, distorting the underlying relationships between the assets. The symmetric covariance matrix has n(n+1)/2 total unique terms: n(n-1)/2 pairwise correlations and n variances. Hence, the number of unique terms behaves as \mathcal{O}(n^{2}), which leads to significant potential for an ill-conditioned covariance matrix [3]. To overcome this issue, factor models were introduced that vastly reduce the dimensionality, and thus the number of numerical estimates required [4]. Factor methods produce low-rank covariance matrices that retain only the largest eigenvalues and discard small eigenvalues associated with noise. Despite this development, the computation of optimal portfolios remains energy-intensive as the efficient frontier is mapped out in n dimensions. In high-frequency trading, this becomes a time-sensitive computation as assets are purchased and sold on microsecond timescales, and portfolios must be regularly rebalanced not to exceed risk appetites. Much attention has been focused on portfolio optimization in the high-frequency domain [5, 6, 7], including the use of evolutionary algorithms to update efficient frontiers [8]. By using such fundamental principles as minimizing entropy, energy, and dissipation [9], or, perhaps, incorporating quantum phenomena like superposition and entanglement [10], we can advance and surpass the classical computations of these problems. At the forefront of this drive to alternate architectures is the integration of analog, physics-based algorithms and hardware, which involve translating complex optimization problems into universal spin Hamiltonians [11, 12, 13]. Indeed, the mean-variance portfolio optimization framework can be encoded into a Hamiltonian’s coupling strengths with the physical system recovering the Hamiltonian’s ground state, which corresponds to the optimal portfolio solution [14, 15]. Efficient mapping from the original problem description to spin Hamiltonian enables the problem to remain manageable despite increasing complexity [16]. In Section II, we introduce the mean-variance optimization framework for calculating optimal portfolios. Then, in Section III, we show that analog continuous Hopfield networks can solve portfolio optimization problems by evolving to the minimum of an energy function that encodes the problem parameters. In Section IV, we address the issues of estimating pairwise covariance by introducing the low-rank approximation that relies on a low-dimensional latent variable representation. In Section V, we show that calculating such a representation can be done using linear autoencoder neural networks, and in Section VI how these networks can be trained on analog hardware using equilibrium propagation. Section VII brings everything together, starting with raw data observations and working through the entire analog pipeline."
https://arxiv.org/html/2411.06391v1,"CausalStock: Deep End-to-end Causal Discovery 
for News-driven Stock Movement Prediction","There are two issues in news-driven multi-stock movement prediction tasks that are not well solved in the existing works. On the one hand, “relation discovery” is a pivotal part when leveraging the price information of other stocks to achieve accurate stock movement prediction. Given that stock relations are often unidirectional, such as the “supplier-consumer” relationship, causal relations are more appropriate to capture the impact between stocks. On the other hand, there is substantial noise existing in the news data leading to extracting effective information with difficulty. With these two issues in mind, we propose a novel framework called CausalStock for news-driven multi-stock movement prediction, which discovers the temporal causal relations between stocks. We design a lag-dependent temporal causal discovery mechanism to model the temporal causal graph distribution. Then a Functional Causal Model is employed to encapsulate the discovered causal relations and predict the stock movements. Additionally, we propose a Denoised News Encoder by taking advantage of the excellent text evaluation ability of large language models (LLMs) to extract useful information from massive news data. The experiment results show that CausalStock outperforms the strong baselines for both news-driven multi-stock movement prediction and multi-stock movement prediction tasks on six real-world datasets collected from the US, China, Japan, and UK markets. Moreover, getting benefit from the causal relations, CausalStock could offer a clear prediction mechanism with good explainability.","The financial services industry has maintained a leading position in embracing data science methodologies to inform investment determinations. Within this domain, quantitative trading has garnered substantial attention from both academia and industry. Researchers have consistently worked on exploring different approaches to predict the stock movement (rise or fall of stock price) for many years, such as uni-stock movement prediction [21], multi-stock movement prediction [44, 23], news-driven stock movement prediction [42, 19] and so on, which have shown significant success. These methods usually model the stock movement prediction task as a time series classification problem. In this paper, we focus on the news-driven multi-stock movement prediction task. A prevalent model paradigm for this task often takes the historical price features and the stock-related news of multiple stocks as inputs and then leverages the well-designed neural networks to make stock movement predictions. There are two key modeling points for tackling this task: modeling the stock relations to enhance the prediction accuracy, and building the text mining module to extract effective information from news data that benefits stock movement prediction. Although previous work has made significant progress, there are still some issues that require further attention. We will elaborate on them in the following. For stock relation modeling, many existing works are commonly attention-based [15, 19, 23] or graph-based [34, 23]. These methods aim to model the correlation relation between stocks. However, the company relations are often unidirectional, such as the “investing” and “member of,” leading to the unidirectional relations of their stocks. Thus, causal relations are more appropriate for depicting the impact between stocks, as they identify the direction of information flow and are more informative than correlations. With the development of causal science, many researchers have started to use deep end-to-end networks for causal relations discovery of panel data or temporal data [9, 14], in which the causal relations are defined as directed acyclic graphs, i.e., causal graphs, and the Functional Causal Models (FCMs) are often utilized to optimize the causal graph by simulating the data generation mechanism. This provides a solid theoretical foundation for causal discovery for stocks. In recent years, an extrinsic text mining module has emerged as a plausible avenue through the alignment of financial news and social media posts, thereby elucidating intricate market insights that extend well beyond mere considerations of price dynamics, trading volumes, or financial indicators [41, 17, 35, 33]. Conventional text representations obtained by using GRU [42] or LSTM [15] exhibit many limitations. Specifically, news text data are often characterized by substantial noise because of the presence of irrelevant or ambiguous information [38, 7, 37]. The effective information for stock movement prediction gets intertwined with this noise, presenting a considerable challenge for these modules to discern meaningful signals accurately. In contrast, Large Language Models (LLMs) have unique advantages in this situation due to their advanced knowledge and reasoning abilities. Besides, LLMs can identify meaningful information within noisy environments [29, 4]. Motivated by these requirements, we propose an innovative news-driven multi-stock movement prediction model named CausalStock. In CausalStock, we design a Denoised News Encoder, which leverages LLMs to score every news text from multiple perspectives. Then the evaluation scores are taken as denoised text representations. To discover the causal relations between stocks, we propose a Lag-dependent temporal causal discovery module, from which we obtain the causal graph distribution. Based on the input market information and learned causal graph distribution, CausalStock employs an FCM [14] to make predictions. We summarize the contributions of our paper as follows: • We propose a novel news-driven multi-stock movement prediction method named CausalStock, which could discover the causal relations among stocks and make accurate movement predictions simultaneously. • Different from the past lag-independent causal discovery method [9], CausalStock involves a lag-dependent temporal causal discovery module, which intuitively links the temporal causal relations according to the time lag, making it more suitable for temporal stock data. • To extract useful information from the massive noisy news text data, an LLM-based Denoised News Encoder is proposed by taking advantage of the evaluation ability of LLM, which outputs the denoised news representation for better information utilization. Experiments on 6 public benchmarks show the performance of CausalStock as a news-driven multi-stock movement prediction method. Moreover, we conduct extensive analytical experiments to show the explainability of our key modules."
https://arxiv.org/html/2411.06272v1,Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models,"As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose ""Golden Touchstone"", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models’ language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o, Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.","The rapid development of both proprietary(Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Anthropic, 2024; Team et al., 2023) and open-source Large Language Models (LLMs) has led to their increasing importance and application across various fields(Touvron et al., 2023a, b; AI@Meta, 2024; Bai et al., 2023; Yang et al., 2024a; DeepSeek-AI, 2024; Young et al., 2024; Zeng et al., 2023; Baichuan, 2023; Gan et al., 2023; Zhang et al., 2022) , including finance(Wu et al., 2023; Lopez-Lira and Tang, 2023), healthcare Thirunavukarasu et al. (2023); Tian et al. (2023), and law Cui et al. (2023); Xiao et al. (2021). Among these, the financial sector stands out as one of the most important areas for LLM application, due to its rich textual information and high practical value. In recent years, a variety of advanced financial large language models (FinLLMs) have emerged, capable of specialized tasks such as financial sentiment analysis, content summarization, stock movement prediction, and specialized question answering, as depicted in Figure.1. These models, including FinGPT (Yang et al., 2023), PIXIU Xie et al. (2023), CFGPT (Li et al., 2023), DISC-FinLLM (Chen et al., 2023), and XuanYuan (Zhang and Yang, 2023), leverage unique frameworks and tuning methods to enhance their performance on domain-specific benchmarks, offering robust solutions for real-world financial applications. Alongside the rapid emergence of FinLLMs, there has also been a significant increase in financial benchmarks. In the English financial domain, FLUE(Shah et al., 2022) was the first publicly available benchmark for assessing English financial language understanding. Similarly, BBT(Lu et al., 2023a) was the pioneering open-source financial NLP benchmark in Chinese. As LLMs and financial applications have continued to develop, more financial benchmarks have been introduced(Xie et al., 2023, 2024; Yang et al., 2023; Lei et al., 2023; Zhang et al., 2023). While these benchmarks have provided valuable resources and evaluation criteria for assessing current LLMs in finance, there remain challenges, such as the presence of low-quality or unsuitable for LLMs datasets within certain tasks, leading to generally poor evaluation results across various models. Furthermore, Current benchmarks also suffer from insufficient language and task coverage, which prevents a comprehensive evaluation of large financial language models. Figure 1: Financial large language models are designed to perform specialized tasks such as financial sentiment analysis, content analysis, stock movement prediction, and financial analyst level question answering by interpreting and processing structured instructions and various input data to generate precise outputs. To address these gaps, we propose Golden Touchstone, a bilingual financial benchmark that consolidates representative datasets across eight financial NLP tasks in both Chinese and English. Golden Touchstone provides high-quality datasets, task-aligned metrics, and instructional templates tailored to guide LLMs in generating task-appropriate responses. The Golden Touchstone organizes all task data into an instruction, input, and output format, facilitating the use and evaluation of different models. We evaluated current open-source general-purpose LLMs and FinLLMs using the Golden Touchstone benchmark. Results indicate that while GPT-4o, Qwen-2, Llama-3, and FinMA performed well across several tasks like financial sentiment analysis and entity extration, there remains considerable room for improvement in areas like credit card scoring dataset within classification tasks and stock movement prediction. Additionally, we have open-sourced Touchstone-GPT, a model trained through domain-specific continual pre-training and financial instruction tuning. It achieved acceptable performance on the bilingual benchmark but showed limitations in tasks such as stock movement prediction and question answering. These findings underscore the need for more high-quality training data and potentially more suitable model architectures for finance-specific applications. Our main contributions are as follows: • Introduction of Golden Touchstone, the first comprehensive bilingual benchmark for financial Large Language Models (LLMs), encompassing 22 datasets across eight tasks in both Chinese and English. Compared to previous FinLLMs’ benchmarks, our benchmark offers enhanced diversity, systematicity, and adaptability. • A thorough evaluation of state-of-the-art LLMs and FinLLMs, including GPT-4o, Qwen-2, Llama-3, FinGPT and FinMA and so on, on the Golden Touchstone. This evaluation highlights the key strengths and shortcomings in model performance across various tasks, and suggests directions for future research in financial LLMs. • The open-source release of Touchstone-GPT, a specialized financial LLM that has undergone domain-specific continuous pre-training and instruction tuning. This model serves as a new baseline for subsequent FinLLMs research, fostering further advancements in Financial AI."
https://arxiv.org/html/2411.06159v1,From References to Insights: Collaborative Knowledge Minigraph Agents for Automating Scholarly Literature Review,"Literature reviews play a crucial role in scientific research for understanding the current state of research, identifying gaps, and guiding future studies on specific topics. However, the process of conducting a comprehensive literature review is yet time-consuming. This paper proposes a novel framework, collaborative knowledge minigraph agents (CKMAs)111https://aaai-2025-4471.github.io/, to automate scholarly literature reviews. A novel prompt-based algorithm, the knowledge minigraph construction agent (KMCA), is designed to identify relationships between information pieces from academic literature and automatically constructs knowledge minigraphs. By leveraging the capabilities of large language models on constructed knowledge minigraphs, the multiple path summarization agent (MPSA) efficiently organizes information pieces and relationships from different viewpoints to generate literature review paragraphs. We evaluate CKMAs on three benchmark datasets. Experimental results demonstrate that the proposed techniques generate informative, complete, consistent, and insightful summaries for different research problems, promoting the use of LLMs in more professional fields.","Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augment and accelerate scientiﬁc research (Wang et al. 2023). Researchers are developing AI methods to, e.g., literature understanding, experiment development, and manuscript draft writing (Liu et al. 2022; Wang et al. 2024; Martin-Boyle et al. 2024). Literature reviews play a crucial role in scientific research, assessing and integrating previous research on specific topics (Bolanos et al. 2024). They aim to meticulously identify and appraise all relevant literature related to a specific research question. Recent advancements in AI have shown promising performance in understanding research papers and generating human-like text (Van Dinter, Tekinerdogan, and Catal 2021). By leveraging AI capabilities, automatic literature review models enable researchers to save time and effort in the manual process of conducting literature reviews, rapidly identify key trends and gaps in recent research outputs, and uncover insights that might be overlooked in manual reviews (Wagner, Lukyanenko, and Paré 2022). The automatic literature review models typically involve two stages (Shi et al. 2023): (1) selecting relevant reference papers and (2) determining logical relationships to compose a summary that presents the evolution of a specific field (these stages can be applied iteratively). Multiple scientific document summarization (MSDS), aiming to generate coherent and concise summaries for clusters of topic relevant scientific papers, is the representative work in the second stage. Past decades (Jin, Wang, and Wan 2020) have witnessed the development of summarization methods. Extractive methods directly select important sentences from original papers. Abstractive methods can generate new words and sentences but are technically more challenging than extractive methods. Figure 1: Illustration of relationships between information pieces in scientific papers. Capturing these relationships is essential for composing a coherent story in literature reviews. Recently, large language models (LLMs), pre-trained on extensive text data, have transformed abstractive summarization and show human-like performance in understanding and coherent language synthesis. However, ideas arising in research papers often have complex relationships, e.g., conflicting or duplicate. Without explicit instructions, LLMs fall short in capturing the relations between ideas and composing a story that connects related work reflecting the author’s understanding of their field (Li and Ouyang 2024). As shown in Fig 1, effective summarization often involves the ability to understand concepts of materials, methods, and tasks in referencing papers, aggregate complementary ideas (e.g., M_{1} is used for T) while contrasting differences (e.g., M_{2} is additionally used for B compared with A). To tackle this bottleneck, we propose equipping LLMs with structural knowledge. Different from knowledge graphs, which consist of entities as nodes and their relationships as edges, serving as general-purpose knowledge, we introduce knowledge minigraphs. Knowledge minigraphs are small-scale semantic graphs, comprising research-relevant concepts as nodes and their relationships as edges, specially designed to capture the structural information between ideas in references. To automatically construct knowledge minigraphs, we propose a prompt-based algorithm, the knowledge minigraph construction agent (KMCA) to elicit LLMs to identify research-relevant concepts and relationships based on references. Benefiting from the designed iterative construction strategy, key information and relationships are iteratively extracted and stored from numerous references into minigraphs. By leveraging the capabilities of LLMs on knowledge minigraphs, the multiple path summarization agent (MPSA) is designed to organize the generated literature review. The MPSA samples multiple summaries from different viewpoints and logical paths in the knowledge minigraph, utilizing the mixture of experts technique. A self-evaluation mechanism is then employed to automatically route to the most desirable summary as the final output."
https://arxiv.org/html/2411.05859v1,Enhancing Financial Fraud Detection with Human-in-the-Loop Feedback and Feedback Propagation,"Human-in-the-loop (HITL) feedback mechanisms can significantly enhance machine learning models, particularly in financial fraud detection, where fraud patterns change rapidly, and fraudulent nodes are sparse. Even small amounts of feedback from Subject Matter Experts (SMEs) can notably boost model performance. This paper examines the impact of HITL feedback on both traditional and advanced techniques using proprietary and publicly available datasets. Our results show that HITL feedback improves model accuracy, with graph-based techniques benefiting the most. We also introduce a novel feedback propagation method that extends feedback across the dataset, further enhancing detection accuracy. By leveraging human expertise, this approach addresses challenges related to evolving fraud patterns, data sparsity, and model interpretability, ultimately improving model robustness and streamlining the annotation process.","Financial fraud detection is essential for maintaining the security and integrity of financial systems. As fraud techniques become more sophisticated, traditional detection methods struggle to effectively identify and prevent fraud. Machine learning (ML) techniques have emerged as powerful tools, leveraging large datasets to detect patterns and anomalies indicative of fraud. However, these systems face challenges such as the need for extensive labeled data, the dynamic nature of fraud, and the complexity of domain-specific knowledge. Human-in-the-loop (HITL) feedback mechanisms offer a promising solution to these challenges by incorporating human expertise into the ML process. HITL involves active human participation in the machine learning pipeline, providing critical insights, annotations, and feedback to enhance model performance. It addresses key issues such as limited labeled data, model interpretability, and adapting to evolving fraud patterns. In financial fraud detection, HITL systems leverage domain knowledge to identify subtle patterns that automated models might overlook. This allows for more accurate model training and validation, reducing false positives and ensuring better fraud detection. Fraud detection presents several challenges ideal for HITL, including imbalanced datasets, adversarial fraudsters, and complex fraud patterns. Fraudsters continually adapt to evade detection, creating an environment where models must be updated frequently. Graph-based approaches, which model transactions as networks, have shown promise in capturing complex fraud patterns but require expert optimization. In this paper, we introduce a HITL framework for financial fraud detection, combining human expertise with advanced ML techniques. Our approach incorporates annotation from proprietary and public datasets, interactive model training, and a novel feedback propagation algorithm. We evaluate the impact of HITL feedback using standard metrics, demonstrating improvements in detection accuracy, robustness, and interpretability. By integrating HITL into fraud detection systems, we improve data annotation, model interpretability, and adaptability to dynamic fraud patterns. Our proposed framework combines advanced ML techniques with a novel feedback propagation method, significantly enhancing fraud detection performance across various algorithms. This research highlights the potential of HITL to improve both traditional and state-of-the-art methods in financial fraud detection while introducing a novel technique for propagating feedback signals throughout the dataset."
https://arxiv.org/html/2411.05856v1,Evaluating the Economic Implications of Using Machine Learning in Clinical Psychiatry,"With the growing interest in using AI and machine learning (ML) in medicine, there is an increasing number of literature covering the application and ethics of using AI and ML in areas of medicine such as clinical psychiatry. The problem is that there is little literature covering the economic aspects associated with using ML in clinical psychiatry. This study addresses this gap by specifically studying the economic implications of using ML in clinical psychiatry. In this paper, we evaluate the economic implications of using ML in clinical psychiatry through using three problem-oriented case studies, literature on economics, socioeconomic and medical AI, and two types of health economic evaluations. In addition, we provide details on fairness, legal, ethics and other considerations for ML in clinical psychiatry.","With the success of artificial intelligence (AI) and machine learning (ML) within areas such as transportation and finance, there is an increasing interest in using those within areas of medicine. One of the areas of interest is psychiatry. There is a growing interest in applying ML in clinical practice within psychiatry, and more recently, research is being conducted to understand its effectiveness in psychiatry. However, no research has investigated the economic implications associated with its use. Our study addresses this gap by evaluating the economic implications of using ML in clinical psychiatry. This paper will first review the current situation of clinical psychiatry and economics, and the economic, socioeconomic, and medical incentives for ML in clinical psychiatry. Then, using three cases studies, we will evaluate the economic implications of using ML in clinical psychiatry. We will conclude by discussing ethical, legal and other considerations for ML in clinical psychiatry."
https://arxiv.org/html/2411.05805v1,"Variational Bayes Decomposition for Inverse Estimation 
with Superimposed Multispectral Intensity","A variational Bayesian inference for measured wave intensity, such as X-ray intensity, is proposed in this paper. The data is popular to obtain information about unobservable features of an object, such as a material sample and the components of it. The proposed method assumes particles represent the wave, and their behaviors are stochastically modeled. The inference is accurate even if the data is noisy because of a smooth prior setting. Moreover, in this paper, two experimental results show feasibility of the proposed method.","Machine learning has been applied to scientific activities, such as extracting information from observational data[1]. One example of those is experimental data analysis of microscope images of new materials. Conventionally, material researchers carefully inspect experimental data to find features in them. The researchers, however, might miss these features as doing otherwise takes a lot of effort. Therefore, machine learning to automatically extract features or hints for discovery from experimental data is highly needed. This paper focuses on unsupervised machine learning applied to inverse estimation with an observed a SMI (Superimposed Multispectral Intensity) of wave. The observation is often performed to measure directly unobservable features. For instance, SAS (Small Angle Scattering) is one of the popular observation methods. In SAS, a wave ray, such as X-rays, is irradiated into a material sample to obtain the micro grain size of it. The ray incident upon the sample interacts with the micro grain therein, and the beam is scattered as multispectral rays due to the interactions with the grains. Therefore, the scattered-ray intensity reflects the grain size. However, since the material sample contains grains of multiple sizes, the rays scattered by the grains are superimposed into an intensity observation. The observed SMI must be decomposed by grain size to explain the sample composition. Because a component corresponding to a grain size is known in the theory about the scattering phenomena, the decomposition makes the microstructure clear. Therefore machine learning is applied to the decomposition[2][3]. Not only limited to SAS, SMI is often obtained for physics observatory such as microscopes, and also there are various multispectral particles, such as from X-ray scattering, ion-beam scattering, etc. Therefore, a general method of SMI decomposition is expected to be applied to versatile measurements. There are existing methods for decomposition, but a method that focuses on robustness is notably lacking. Reasonable methods are parameter fitting and stochastic inference, such as the ML (Maximum Likelihood) and MAP (Maximum A Posteriori) inference, are known. However, the model for these methods can be fit with noise, resulting in quite noisy results: this is called overfitting. To address this, regularization parameters are often induced into the model, but the parameters are manually adjusted. There is a risk that the adjustment may introduce arbitrariness in the experimental results. Therefore, a stochastic method resistant to overfitting is proposed in this work. The proposed method is based-on VB (Variational Bayes) inference, where not only observations are stochastic but also parameters. A prior distribution of the parameters is first assumed in the proposed method. The prior distribution is revised with observations and the expectation values of the revised distribution lead to the decomposed factors. Because the prior distribution restricts the revised distribution, overfitting can be reduced."
