URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10061v1,"EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation","Recent work on human animation usually involves audio, pose, or movement maps conditions, thereby achieves vivid animation quality. However, these methods often face practical challenges due to extra control conditions, cumbersome condition injection modules, or limitation to head region driving. Hence, we ask if it is possible to achieve striking half-body human animation while simplifying unnecessary conditions. To this end, we propose a half-body human animation method, dubbed EchoMimicV2, that leverages a novel Audio-Pose Dynamic Harmonization strategy, including Pose Sampling and Audio Diffusion, to enhance half-body details, facial and gestural expressiveness, and meanwhile reduce conditions redundancy. To compensate for the scarcity of half-body data, we utilize Head Partial Attention to seamlessly accommodate headshot data into our training framework, which can be omitted during inference, providing a free lunch for animation. Furthermore, we design the Phase-specific Denoising Loss to guide motion, detail, and low-level quality for animation in specific phases, respectively. Besides, we also present a novel benchmark for evaluating the effectiveness of half-body human animation. Extensive experiments and analyses demonstrate that EchoMimicV2 surpasses existing methods in both quantitative and qualitative evaluations.","Diffusion-based video generation has seen significant advancements[8, 12, 21, 8, 12, 21, 11, 5, 1, 9, 33, 3], prompting extensive research in human animation. Human animation, as a subset of video generation domain, aims at synthesizing natural and realistic human-centric video sequences from a reference character images. Current research on human animation commonly employs pretrained diffusion models as backbone, and involves corresponding condition injection modules to introduce control conditions[6, 14, 38, 26, 30, 31, 37, 17], so that lifelike animation can be well generated. Unfortunately, there is still gaps between academic study and industrial needs: 1) Head region limitation; 2) Condition-injection complexity. 1) Head Region Limitation. On the one hand, prior human animation works[6, 30, 31, 37] mainly focus on generating head-region videos, neglecting the synchronization of the audio and the shoulders-below body. The most recent work[17] has improved half-body animation with auxiliary conditions and injection modules beyond audio-driven module. 2) Condition-Injection Complexity. On the other hand, the commonly-used control conditions (e.g. text, audio, pose, optical flow, movement maps) can provide a solid foundation for lifelike animation. Particularly, current research efforts concentrate on aggregating supplementary conditions, which result in unstable training due to multi-condition incoordination, and elevated inference latency stemming from intricate condition-injection modules. For the first challenge, a straightforward baseline exists to accumulate conditions related to shoulder-below body, such as half-body key points maps. However, we discover that this approach remains infeasible because of increased complexity of conditions (for the second challenge). In this paper, to remedy the aforementioned issues, we introduce an novel end-to-end method-EchoMimicV2, building upon the portrait animation method EchoMimic [6]. Our proposed EchoMimicV2 strives for striking quality of half-body animation yet with simplified conditions. To this end, EchoMimicV2 exploits the Audio-Pose Dynamic Harmonization (APDH) training strategy to modulate both audio and pose conditions, and meanwhile reduce the redundancy of the pose condition. Additionally, it utilizes a stable training objective function, termed Phase-specific Loss (PhD Loss), to enhance motion, details, and low-level quality, replacing the guidance of redundant conditions. Specifically, APDH is inspired by the waltz dance step, where audio and pose condition perform as synchronized dance partners. As the pose gracefully steps back, the audio seamlessly advances, perfectly filling in the space to create a harmonious melody. As a result, the control scope of the audio condition is extended from the mouth to the entire body via Audio Diffusion, and meanwhile the pose condition is confined from the entire body to the hands via Pose Sampling. Given that the primary regions responsible for audio expression located in the mouth lips, we initiate our audio condition diffusion from the mouth part. Additionally, due to the complementarity of gestural and verbal communication, we retain hand pose condition for the gesture animation so that the Head Region Limitation challenge is overcome, extending to half-body region animation. Throughout this process, we find a free lunch for data augmentation. When audio condition only controls the head region via Head Partial Attention, we can seamlessly incorporate padded headshots data to enhance facial expressions without requiring additional plugins like[17]. We also list the advantages of our proposed EchoMimicV2 in Table 1. Moreover, we propose a stable training objective function, Phase-specific Loss (PhD Loss), with two goals: 1) to enhance the motion representation with incomplete pose; 2) to improve details and low-level visual quality not governed by audio. While it is intuitive to employ a multi-loss mechanism that integrates pose, detail, and low-level visual objective functions concurrently, such an approach typically requires extra models, including Pose Encoders and VAE Decoders. Given that the ReferenceNet-based backbone already demands significant computational resources[14], implementing a multi-losses training becomes impractical. Through experimental analysis, we segments the denoising process into three distinct phases, each with its primary focus: 1) Pose-dominant phase, where motion poses and human contours are initially learned; 2) Detail-dominant phase, where character-specific details are refined; and 3) Quality-dominant phase, where the model enhances the color and other low-level visual qualities. Consequently, the proposed PhD Loss is tailored to optimize the model for each specific denoising phase, that is, Pose-dominant Loss for the early phase, Detail-dominant Loss for the middle phase, and Low-level Loss for the final phase, ensuring a more efficient and stable training process. Additionally, to facilitate the community in quantitative evaluation of half-body human animation, we have curated a test benchmark, named EMTD, comprising half-body human videos sourced from the Internet. We conducted extensive qualitative and quantitative experiments and analyses, demonstrating that our method achieves state-of-the-art results. EchoMimicV2 CyberHost[17] Audio+RefImage + Hand Pose Sequence + Body Movement Sequence - + Face Crop Injection Module - + Hand Crop Injection Module - + Full-body Pose Guidence Table 1: The simplification of our proposed EchoMimicV2. In summary, our contributions are as follows: • We propose EchoMimicV2, an end-to-end audio-driven framework to generate striking half-body human animation yet driven by simplified conditions; • We propose APDH strategy to meticulously modulate audio and pose condition, and reduce pose condition redundancy; • We propose HPA, a seamlessly data augmentation to enhance the facial expressions in half-body animations, no need for additional modules; • We propose PhD Loss, a novel objective function to enhance the motion representation, appearance details and low-level visual quality, alternating the guidance of complete pose condition; • We provide a novel evaluation benchmark for half-body human animation. • Extensive experiments and analyses are conducted to verify the effectiveness of our proposed framework, which surpasses other state-of-the-arts methods. Figure 1: The overall pipeline of our proposed EchoMimicV2."
https://arxiv.org/html/2411.09156v1,DyGASR: Dynamic Generalized Exponential Splatting with Surface Alignment for Accelerated 3D Mesh Reconstruction,"Recent advancements in 3D Gaussian Splatting (3DGS), which lead to high-quality novel view synthesis and accelerated rendering, have remarkably improved the quality of radiance field reconstruction. However, the extraction of mesh from a massive number of minute 3D Gaussian points remains great challenge due to the large volume of Gaussians and difficulty of representation of sharp signals caused by their inherent low-pass characteristics. To address this issue, we propose DyGASR, which utilizes generalized exponential function instead of traditional 3D Gaussian to decrease the number of particles and dynamically optimize the representation of the captured signal. In addition, it is observed that reconstructing mesh with Generalized Exponential Splatting(GES) without modifications frequently leads to failures since the generalized exponential distribution centroids may not precisely align with the scene surface. To overcome this, we adopt Sugar’s approach and introduce Generalized Surface Regularization (GSR), which reduces the smallest scaling vector of each point cloud to zero and ensures normal alignment perpendicular to the surface, facilitating subsequent Poisson surface mesh reconstruction. Additionally, we propose a dynamic resolution adjustment strategy that utilizes a cosine schedule to gradually increase image resolution from low to high during the training stage, thus avoiding constant full resolution, which significantly boosts the reconstruction speed. Our approach surpasses existing 3DGS-based mesh reconstruction methods, as evidenced by extensive evaluations on various scene datasets, demonstrating a 25% increase in speed, and a 30% reduction in memory usage.","In 3D computer vision, reconstructing surface mesh from multiple calibrated views is a foundational task. Initially, point clouds are typically derived from image collections using traditional Multi-View Stereo (MVS) techniques, and triangular meshes are subsequently constructed from them [1]. Recently, Neural Radiance Fields (NeRF) [2] represent neural implicit surface reconstruction techniques and have emerged as formidable competitors. This technique commonly employs Multi-Layer Perceptrons or hash encoding technologies [3] to attribute geometric properties like density [4] or the signed distance functions to the nearest surface (SDF) [5, 6, 7] to spatial points. In the domain of neural implicit mesh reconstruction, SDF-based methods are particularly prominent because they facilitate volumetric rendering and enable the learning of implicit surface representations through density functions derived from SDF. Impressive results are attained in small scenes through rendering supervision methods based on neural implicit techniques, however, these methods struggle in complex or large-scale scenes, particularly in those with extensive untextured areas [8, 9]. To address these challenges, structural priors such as depth [10], normal regularization [8], point clouds [9], and semantic information have been incorporated into the optimization process in previous studies, alongside refined sampling strategies such as voxel keypoint guidance [11] and hierarchical sampling [9]. While these strategies enhance the accuracy of surface mesh reconstruction, they significantly increase computational demands and extend training duration. Although some methods [9] use MVS-predicted point clouds as priors for mesh reconstruction, these clouds are sparse and noisy, failing to capture the scene’s detailed features. Following NeRF, the 3DGS method was recently introduced and has gained popularity [12]. This method excels in generating dense geometric point clouds and explicitly storing the scene’s structure in parameter space, enabling direct edits to 3D scenes. Each Gaussian’s parameters include position in 3D space, covariance matrix, opacity, and spherical harmonics coefficients. However, Gaussians optimized through 3DGS are not directly usable as priors for mesh reconstruction due to their large numbers, slow training speeds, and predominantly internal scene positioning, which may produce noisy outcomes. Figure 1: Illustrates that our method, excels in both training time and reconstruction quality, achieving the highest performance. Our method aims to reduce training times and storage costs while surpassing state-of-the-art reconstruction quality. We noted the inherent assumption of low-pass characteristics in 3DGS signal modeling, as illustrated in Figure 2(a, b). Given the high-frequency discontinuities in most scenes and the memory burden from numerous tiny Gaussians in 3DGS, we chose a more appropriate basis. Inspired by the work presented in the GES[13], we have incorporated the Generalized Exponential Splatting(GES), which represents various signal points with fewer particles and greater precision, over ordinary Gaussians. However, since GES-generated geometric point clouds’ centers do not align with actual scene surfaces, we adopt Sugar’s approach and introduce Generalized Surface Regularization(GSR)[14] to align these point clouds with the surface, optimizing parameters to control the shape of generalized exponential splatting. concurrently. Ideally, when generalized exponential splatting distributions are flat and uniformly distributed over the surface, the SDF calculated by the density function minimizes discrepancies with the SDF from actual generalized exponential distribution, allowing precise representation of surface attributes. A level set, extracted from the density function, undergoes point sampling to facilitate surface mesh creation via Poisson reconstruction [1]. Furthermore, we abandoned the original training methods and introduced a strategy that smoothly transitions resolution from small to large, significantly speeding up training convergence and stability while enhancing reconstruction quality. Fig.1 displays the high-quality results of our reconstruction and rendering. This work makes the following contributions: (1) We propose DyGASR, employing GES over 3DGS to generate fewer, more effective point cloud priors and introducing GSR to align these priors with scene surfaces, thus accelerating the reconstruction of high-quality surface mesh. (2) Additionally, we apply a dynamic resolution training strategy that smoothly transitions from low to high resolution, effectively shortening training durations and reducing memory consumption. (3) The effectiveness of our method for 3D mesh reconstruction is demonstrated through ablation studies. Evaluations across multiple datasets show a 25% reduction in training time, a 30% decrease in memory usage, and a 0.29 dB improvement in PSNR over the SOTA method. Figure 2: We verify the low-pass characteristics of 3D Gaussians. In (a), the GT image is shown on the left, and a rendering after 500 iterations of 3DGS on the right, both analyzed in the frequency domain via Fourier transform along the same horizontal line. The rendering appears in green and the GT image in blue, highlighting that the low-pass characteristics of 3DGS do not perfectly align with the scene’s signal features. In (c), generalized exponential functions are displayed, where \epsilon=1 represents a scaled Laplace distribution, \epsilon=2, a scaled Gaussian distribution, along with other signal shapes such as triangles and squares. Here, \epsilon serves as a parameter for each component in our method."
https://arxiv.org/html/2411.09066v1,A multidimensional measurement of photorealistic avatar quality of experience,"Photorealistic avatars are human avatars that look, move, and talk like real people. The performance of photorealistic avatars has significantly improved recently based on objective metrics such as PSNR, SSIM, LPIPS, FID, and FVD. However, recent photorealistic avatar publications do not provide subjective tests of the avatars to measure human usability factors. We provide an open source test framework to subjectively measure photorealistic avatar performance in ten dimensions: realism, trust, comfortableness using, comfortableness interacting with, appropriateness for work, creepiness, formality, affinity, resemblance to the person, and emotion accuracy. We show that the correlation of nine of these subjective metrics with PSNR, SSIM, LPIPS, FID, and FVD is weak, and moderate for emotion accuracy. The crowdsourced subjective test framework is highly reproducible and accurate when compared to a panel of experts. We analyze a wide range of avatars from photorealistic to cartoon-like and show that some photorealistic avatars are approaching real video performance based on these dimensions. We also find that for avatars above a certain level of realism, eight of these measured dimensions are strongly correlated. In particular, for photorealistic avatars there is a linear relationship between avatar affinity and realism; in other words, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario. We provide several extensions of this test framework for future work and discuss design implications for telecommunication systems. The test framework is available at https://github.com/microsoft/P.910.","Photorealistic avatars are human avatars that look, move, and talk like real people. Photorealistic avatars can be used for various applications, such as: • Telecommunication: Photorealistic avatars can be used instead of two-dimensional webcam videos to create a virtual meeting space. Users can interact while maintaining correct eye gaze, allowing participants to know who is looking at whom, which increases trust (Nguyen and Canny, 2007) while reducing video fatigue by reducing hyper-gaze (Gale et al., 1975; Fauville et al., 2021). • Health care: Photorealistic avatars can be used to provide virtual consultations, training, therapy, and education for patients and medical professionals. For example, a photorealistic avatar of a doctor can explain a diagnosis, prescribe a treatment, or demonstrate a procedure to a patient. • Education: Photorealistic avatars can be used to create immersive, personalized, and interactive learning environments for students and teachers. For example, a photorealistic avatar of a teacher can guide students through a lesson, provide feedback, or answer questions. • Retail and e-commerce: Photorealistic avatars can be used to enhance the (online) shopping experience for customers, sellers, and customer service. For customers, photorealistic avatars can enable virtual try-on experiences for clothing, accessories, or makeup, allowing customers to visualize how products will look on them before making a purchase. • Entertainment: Photorealistic avatars can be used to create realistic and engaging characters for games, movies, shows, and social media. For example, a photorealistic avatar of an actor can perform scenes, interact with fans, or promote brands. While there are many applications of photorealistic avatars, our focus is on evaluating them for the telecommunication scenario to improve trust and reduce video fatigue. Therefore the avatar test sequences we use are targeted for telecommunication, which includes people talking and expressing a wide range of emotions (happy, sad, surprised, fear, anger, and disgust). The performance of photorealistic avatars has been improving significantly recently based on objective metrics such as PSNR (Gonzalez and Woods, 2006), SSIM (Wang et al., 2004), LPIPS (Zhang et al., 2018), FID (Heusel et al., 2017), and FVD (Unterthiner et al., 2019). However, recent avatar publications do not provide subjective tests of the avatars to measure human usability factors (e.g., (Saito et al., 2024; Tian et al., 2024; Xu et al., 2024b; Huang et al., 2024; Shao et al., 2024; Zhou et al., 2024; Deng et al., 2024; Kirschstein et al., 2024; Xu et al., 2024a; Liu et al., 2024)). We believe this is because such subjective tests are challenging, but also because there is no standardized or readily available method to do so. The usability factors that have been previously suggested and studied for avatars include realism (Inkpen and Sedlins, 2011), comfortableness using (Inkpen and Sedlins, 2011), comfortableness interacting with (Inkpen and Sedlins, 2011), appropriateness for work (Inkpen and Sedlins, 2011), creepiness (Inkpen and Sedlins, 2011), formality (Inkpen and Sedlins, 2011), resemblance to the person (Inkpen and Sedlins, 2011), trust (ITU-T Recommendation P.1320, 2022), and emotion accuracy (ITU-T Recommendation P.1320, 2022). We also include affinity to study the uncanny valley effect (Mori et al., 2012). The research questions we want to answer in this work are: • RQ1: Are the objective metrics currently used to develop avatars (PSNR, SSIM, LPIPS, FID, FVD) sufficient to achieve the performance goals of avatars, especially the human usability factors for avatars? • RQ2: Which human usability factors are the most important for photorealistic avatars? • RQ3: Can we develop an accurate and reproducible test framework to measure human usability factors for avatars? • RQ4: Is there an uncanny valley effect for photorealistic avatars in the telecommunication scenario? Our contributions in this work are: • We provide an open source test framework to subjectively measure photorealistic avatar quality of experience in ten dimensions using crowdsourcing. • The crowdsourced subjective test framework is highly reproducible and accurate compared to a panel of experts. • We show that the correlation of nine of these subjective dimensions to PSNR, SSIM, LPIPS, FID, and FVD is weak and one (emotion accuracy) is moderate, motivating the need for an available subjective test framework as well as improved objective metrics to measure avatar performance. • We analyze a wide range of avatars from photorealistic to cartoon-like and show some photorealistic avatars are approaching real video quality based on these subjective metrics. • We show that for avatars with a realism > 2 (out of a 1-5 Likert scale) eight of the ten measured dimensions are strongly correlated, which leads to a dimensionality reduction of ten to three for the survey. • We show that for photorealistic avatars there is a linear relationship between avatar affinity and realism. In other words, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario; the more realistic the avatar is, the more affinity there is to the avatar. In Section 2, we review related work in this area. In Section 3, we describe the test framework design to measure avatar quality of experience, and in Section 4, we show that the test framework is both reproducible and accurate. Using the test framework, we show the results and analysis in Section 5. Finally, we provide conclusions, future extensions, system design implications, and limitations in Section 6."
https://arxiv.org/html/2411.08930v1,Structured Pattern Expansion with Diffusion Models,"Recent advances in diffusion models have significantly improved the synthesis of materials, textures, and 3D shapes. By conditioning these models via text or images, users can guide the generation, reducing the time required to create digital assets. In this paper, we address the synthesis of structured, stationary patterns, where diffusion models are generally less reliable and, more importantly, less controllable.Our approach leverages the generative capabilities of diffusion models specifically adapted for the pattern domain. It enables users to exercise direct control over the synthesis by expanding a partially hand-drawn pattern into a larger design while preserving the structure and details of the input. To enhance pattern quality, we fine-tune an image-pretrained diffusion model on structured patterns using Low-Rank Adaptation (LoRA), apply a noise rolling technique to ensure tileability, and utilize a patch-based approach to facilitate the generation of large-scale assets.We demonstrate the effectiveness of our method through a comprehensive set of experiments, showing that it outperforms existing models in generating diverse, consistent patterns that respond directly to user input.","Hand-drawn structured patterns are central to computer graphics, with applications spanning various domains in design and digital art. Creating these patterns remains a complex and time-consuming task that requires specialized expertise. AI-assisted content creation offers the potential to simplify this process. For instance, learning-based image synthesis methods have shown impressive generation capabilities for natural images [Rombach et al., 2022; Brock et al., 2019; Karras et al., 2018, 2020; Podell et al., 2023]. However, the application of these methods to pattern-like synthesis has primarily focused on unstructured, realistic materials [Zhou et al., 2018, 2023, 2024; He et al., 2023; Vecchio et al., 2024b, a; Vecchio and Deschaintre, 2024], leaving the creation of structured patterns an underexplored task. In contrast, the synthesis of highly structured vector patterns has been assessed by automatically discovering and exploiting their structure, geometry and topology [Tu et al., 2020; Reddy et al., 2020] or by optimizing the procedural parameters of differentiable vector pattern to match a sketch or a viewport edit [Riso and Pellacini, 2023; Riso et al., 2022]. Our work focuses on structured patterns with a hand-drawn appearance, characterized by the repetitions of sketch-like shapes filled with solid colors and defined by sharp, crisp edges. Formally, these structured patterns consist of stationary repetitions of recognizable shapes, each with individual variations, and are drawn with piecewise-constant colors. Examples of these patterns are shown throughout the paper, with Fig. 2 also highlighting examples of textures outside of our scope. We focus on this type of pattern for their importance in design applications and due to the general lack of learning-based methods addressesing their synthesis. In-domain samples Out-of-domain samples Figure 2. Pattern Category. In this work, we focus on structured, stationary, patterns in a hand-drawn style, characterized by repeated recognizable shapes drawn in flat colors (left). Unstructured or aperiodic patterns, as well as photorealistic textures, fall outside the scope of this paper (right). Our approach leverages Latent Diffusion Models [Rombach et al., 2022] as a foundation for the synthesis. Although these models have achieved significant advancements in natural image generation, they are not optimized to generate structured patterns. One primary limitation is that the synthesized patterns often lack quality, as these models are typically trained to generate photorealistic images with unstructured, chaotic textures and high-frequency, stochastic color variations. When applied to structured patterns, these methods often fail to maintain the inherent structure, sharpness, and cohesive visual style of the patterns. Furthermore, design applications often require precise pattern controll by the users, often lacking in generative approaches, generally focusing on text-to-image synthesis. While high-level conditioning may be sufficient for natural image synthesis, specifying the exact structure and appearance of a pattern is far more challenging. Even when using images as conditioning inputs, existing methods perform inconsistently in producing structured patterns in our domain. To address this gap in the literature and offer artists a more accessible yet controllable tool for content creation, we propose a diffusion-based model specifically designed for the synthesis and expansion of structured, stationary patterns. In particular, we leverage the extensive knowledge already available in large-scale models, such as Stable Diffusion [Rombach et al., 2022; Podell et al., 2023], and adapt it to the patterns domain by training a “lightweight” LoRA [Hu et al., 2022]. Doing so we limit the computational and data requirements of training a diffusion model from scratch, but helps to retain the expressivity of models trained on large-scale datasets like LAION [Schuhmann et al., 2021], while adapting it to our specific domain. To that end, we collect a dataset of procedurally designed patterns that we use to train our LoRA. We base our architecture on an inpainting pipeline, which supports the expansion of a partial, hand-drawn input sketch into a larger pattern while preserving its structural integrity and details. During inference, we leverage noise rolling and patch-based synthesis to produce large-scale, tileable patterns, at high quality in a reliable way. These design choices allow us to generate large-scale, tileable patterns that accurately follow the input sketch, while adding a limited degree of variation avoiding visible repetitions. We qualitatively evaluate the effectiveness of our approach across a diverse range of input patterns, demonstrating significant improvements over previous state-of-the-art methods in texture synthesis. To assess user satisfaction with generation quality, we also conduct a user study that captures preferences and perceived fidelity in the synthesized outputs. Additionally, we analyze our architecture through a comprehensive set of experiments and ablation studies to highlight the benefits of our design choices. The results show that our method consistently generates a wide variety of structure patterns, effectively preserving the structure and visual coherence of the input sketches. In summary, the contributions of our work are as follows: • we present a new diffusion-based approach for structured pattern synthesis and expansion; • we introduce a new medium-scale dataset for fine-tuning generative models on the pattern domain; • we demonstrate the generation capabilities of our model for different types of structured patterns and show its ability to control the generation precisely from input sketches; • we validate the improvements over other generative methods, non-specifically trained for patterns, underlying the need for a specifically trained model."
https://arxiv.org/html/2411.08818v1,On integer sequences for rendering limit sets of Kleinian groups,"We present a technique for rendering limit sets for kleinian groups, based upon the base transformation of integers and which aims at saving memory resources and being faster than the traditional dictionary based approach.","1.1 Inversions and isometric circles. We will see that circles are special shapes in the theory of Möbius maps. There exists a subgroup G of Mobius maps T(z)=a_{C}+\frac{r_{C}^{2}}{\overline{z-a_{C}}}, (3) which are defined circle inversions (or reflections) and map circles C to themselves (so they are conformal transformations), whereas interior and exterior are swapped (fig. 1.1/a). Sizes of the objects inside of these regions would not be preserved. & Figure 1: Tessellation by circle inversions. The top diagram illustrates how circle inversion works. At the bottom, the disc images under the action of a subgroup whose generators have no self-intersecting inversion circles and known as of Schottky type. That said would be enough to guess that the repeating application of circle inversions could generate sequences of image objects whose size would progressively decrease for instance (fig. 1.1); hence we could speak of limit sets here in the previous terms. For instance, let C be a circle centered at a_{C}=x_{C}+iy_{C} and with radius r_{C}. Conversely, we can determine a_{C} and r_{C} in (3) from C. Hence T maps every object outside (resp. inside) C inside (resp. outside) it (fig. 1.1); then, T\circ T=I, or T^{2}(z)=I. C is defined inversion circle and it is the inverse of itself, i.e., invariant under T(z). From now on, let this circle be denoted as C_{\textnormal{INV}}. There exists another family of invariant circles for (1), which are called isometric,333This word comes from the union of the two Greek terms iso and metric = same size. where g(C_{\textnormal{ISO}})=C_{\textnormal{ISO}}=g^{-1}(C_{\textnormal{ISO}}) and which are algebraically defined by the equality \left|cz+d\right|=1, with c\neq 0.444They are centered at -d/c and with radius 1/\left|c\right|, hence there are no such circles when the LFT is not a rational map. Isometric circles will be denoted C_{\textnormal{ISO}} here. Their unique nature let both families of invariant circles be gathered into the basic toolkit for the exploration of the subgroups of \mathcal{M}; in fact, invariance prevents ambiguities and then it shows to be an essential property for building up mathematical theories.555For more information, see [7, pp. 23 et ff.]. Invariant circles are also of help to obtain graphical representations of generators (fig. 2/a and 1.1). We will deal here with subgroups of self-inverse and of non-self-inverse Möbius transformations in form (3) or not respectively. The renderings on the left and at the center inside the strip of figures (2) are known as tessellation and limit set, and technically obtained by computing all the chains/orbits (2) at p. 2 up to some given bounded depth, with the distinctive approach to rendering all the images of the starting invariant disc through each orbit or just the last in line. Kleinian is the definition for groups \mathcal{G} of LFT (1). The research on subgroups \mathcal{G} came up in the second half of the XIX century. The initial development of this theory involved the classification rules of subgroups, according to the following properties: (a) shape and topology of limit sets: a circle, a general curve or a dust of points for subgroups of Fuchsian, quasi-Fuchsian and of Schottky kind respectively (fig. 2); (b) numerical nature of the coefficients a, b, c and d; for example, the subgroup is modular if they are real integers, or defined Picard groups, after Émile Picard (1856-1941) groups, if coefficients are Gaussian integers; (c) special algebraic relations between coefficients where the above mentioned trace plays as a key tool. (a) Fuchsian (b) Quasi-Fuchsian Figure 2: Simple examples of limit sets. (a) Generators are four and mutually tangent inversion circles. The limit set is a circle; the subgroup is defined Fuchsian. (b) If not being exactly a geometrical, but a Jordan curve, such groups are known as quasi-fuchsian. The shapes of limit sets \lambda for subgroups of \mathcal{M} are generally ruled by fractal patterns.666Like it happens to Julia sets \mathcal{J}, the limits for the iteration of non-linear functions in one complex variable. These are two well-known kindred theories, based on orbits built according to the criteria of the algebraic structures which functions belong to: groups or singletons. There exist groups, defined degenerate, which escape this pattern (fig. 14 at p. 14). Other groups have limit sets which spread in ways that their complement (the set of discontinuity \Omega) consist of circles that are said to tessellate a given portion of the complex plane {\mathbb{C}}: i.e., they cover, or fill, the space through a well-ordered geometric distribution. Tessellation is part of the so-called circle packing, a collection of studies focusing on optimal777Aiming at reducing the gap between the original area and the packing to 0. patterns for filling in areas by means of circles (fig. 3). Packing could be assumed as an algorithm for approximation. We are interested here in deploying an efficient algorithm to render, on a computer screen, the limit sets of Kleinian groups that are not elementary, i.e., which include at least three points. Figure 3: Circle packings. Many limit sets for subgroups of Möbius maps spread their point around circles, which tend to pack bounded surfaces, like disks, or infinite strips. (a) Generators (b) Final rendering Figure 4: Tessellation via circle inversion. (a) A different disposition of four mutually tangent circles than fig. 2/a was adopted here to work inversion maps (3). The tangency condition is preserved along the construction and it prevents images circles from overlapping each other. The gradient of lighter shades enhances convergence as well as position and shape of the limit set. 1.2 Two kinds of rendering The circle representation of a generator could extend to that of the chain (2) because of having the same algebraic nature as of (1), according to the discussion at §1. Would it be always worth anyway? The renderings shown throughout this article are joined by the same goal: displaying the limit sets for subgroups G\subset\mathcal{M}. They show up in two kinds, and each for a precise purpose: when discs are drawn (or painted), we also chose to pick up colors out from a palette of shades being sorted by a gradient, in order to obtain a chromatic analogy for the decreasing sequence of disc sizes, for instance. Alternatively, discs are not drawn if we need to display the end points of the orbits and we are not interested in representing the whole orbit, but just (the approximation of) their final fate, the above mentioned \Lambda_{d} of the limit set. So these two kinds of rendering are intended to highlight the behavior and fate of the orbits respectively. The limit set will reveal as the consequence from the decreasing size of these discs during the generation of isometric circles. (a) Generators (b) Disc images (c) Limit set isometric circles isometric circles points Figure 5: Choice of proper strategy. (a) Generators are parabolic. The subgroup action has been rendered through (b) circles (the limit set is barely recognizable) and (c) points/pixels. Colors in (c) are associated to the starting generator of each orbit. The main reason behind the choice of rendering tesselations or limit set relies in the possibility of producing pictures that will not look as messy and confusing (fig. 5/b); this last event often happens when the disc images overlap each other. We need an optimal computation strategy because groups processing needs to skip all the chains including contiguous pairs g_{k}\circ g^{-1}_{k} of inverse generators for example, which resolve into the identity map I(z) and give rise to duplicate orbits that are represented by equivalent but formally shorter chains. The earliest renderings, via tessellations or limit sets, were already available on paper at the end of the XIX century, in the masterpiece by Fricke and Klein [8]. During the modern times of digital computing, this problem was tackled through a lexicographic approach based upon a finite state automata (see [6]) that generates all the chains by permutations, each uniquely binding to one of the orbits up to the bounded maximal length/depth l=d<\infty. We remark that this approach was not originally conceived for computational goals, as dating back to a time when there was no enough familiarity and confidence with these problems, hence its features were not geared to optimizing speed, efficiency, and to saving memory resources. Figure 6: Tessellation and pattern. 2.1 Trees of words and duplicates Letter-izedDigit-ized(abbAABB)(2330011)WordsNumbers Abstract strings Figure 7: Materialization into trees. Concatenation is a general operation for producing abstract strings (fig. 7): sequences of symbols, each being decoupled from semantic meaning. In mathematics, it finds to be useful for representing multiple application of functions, thus it can be extended to working with chains (2). It underlies the so-called lexicographic approach to the rendering of subgroups \mathcal{G}\subset\mathcal{M}, where strings are generated in the terms set out by the chains (2). Symbols are here assumed to be the letters l_{n} of the Western alphabet (a, b, c, d, …), thus abstract strings materialize into sequences of alphabetical letters, that are known as words. Namely, each generator of subgroups \mathcal{G} will be associated to one letter as follows: g_{1}=a,\hskip 14.22636ptg_{2}=b,\hskip 14.22636ptg^{-1}_{1}=A,\hskip 14.22636% ptg^{-1}_{2}=B; (4) This writing lists the bindings required for applying the lexicographic approach to so-called 2-generators subgroups.888Unless otherwise specified, the expression ‘n-generators subgroup’ does not count the inverse maps g_{n}^{-1}. Here we first notice that letters stand out as a good choice for setting up a one-to-one relation between one and distinct symbol and one only generator in order to reproduce the inversion relationship between pairs of generators: mutually inverse generators are analogously represented by the duality of the small (lowercase) and the capital (uppercase) representation of the same alphabetic symbol. Anyway, for following the next arguments, we have to remark that every such binding is just a resort and one of the possible viable choices. In more details, the lexicographic approach works upon a set \mathcal{W}_{n} of n<\infty symbols, each being conventionally associated to one and only one generator in the given subgroup G\subset\mathcal{M}. Analogously, \mathcal{W}_{n} is termed alphabet, as it serves to encode the chains (2) into the readable representation of a string of letters, which is contextually defined as word and which could be read from left to right (LR) or from right to left (RL). Throughout the present article, words will be conventionally read in RL order. It is easy to check out these biunivocal connections orbits ⇆ chains ⇆ words. & \displaystyle I\displaystyle A\displaystyle B\displaystyle a\displaystyle b\displaystyle bA\displaystyle BA\displaystyle AA\displaystyle bb\displaystyle Ab\displaystyle ab\displaystyle ba\displaystyle Ba\displaystyle aa\displaystyle BB\displaystyle AB\displaystyle aB (a) self-inverse (b) non-self-inverse Figure 8: Multi-branched trees up to depth 2. Lexicographic representation of the two initial steps in the growth for the tree models associated to 2-generators groups of (a) circles inversions or (b) not. The lexicographic approach will build up a dictionary, that is, a collection of finite length words w. Dictionaries are filled in by all the words being generated by appending single nodes up to some bounded length l. \displaystyle I\displaystyle a\displaystyle ba\displaystyle ca\displaystyle da\displaystyle aba\displaystyle cba\displaystyle dba\displaystyle Bba Figure 9: Tree growth with cancellations. The identities aA, Aa, bB, Bb stop the tree growth as they would refer to equivalent words. In this environment, the concatenation of letters into words can be modelled through a (4-1=3)-branched tree (fig. 9), where every new node gives rise to n-1 new branches. Orbits are represented by paths connecting the root to leaves; the depth of an orbit thus amounts to the number of nodes traversed up to reaching a leaf. Words here show up as a, ABab, BBaaB for example. Like ordinary ones, each encodes one meaning. The converse is not true. The existence of mutually inverse generators in the group definition opens to the possibility of building special words – such as aA, Aa, bB, Bb for 2-generators groups, which formally represent the identity map I(z)=z, which is generally obtained through the formal composition g(z)\circ g^{-1}(z) (see §1). Operatively, the identity map does not alter the action of the chain and can be safely dropped: this management is formally carried out by the cancellation, within the given word, of contiguous symbols that jointly pertain the identity map: for example, Aaaa, read from right to left, reduces to the shorter form aa, a word that was previously generated along a same process. If a word includes no identities, it is said reduced. Thus, we must separate form from action here: the possibility of cancellations within formal words hints at the existence of equivalent but shorter ones; or, similarly, to the existence of infinitely many and longer forms of a same reduced word. For what follows, we want to point out that the action of every chain (or word, or orbit) is subjected to its formal representation, for any set of symbols in use: in fact, cancellations are necessary for they involve redundant operations and computational costs if neglected. & \displaystyle I\displaystyle A\displaystyle bA\displaystyle BA\displaystyle AA\displaystyle\boldsymbol{abA}\displaystyle\boldsymbol{bbA}\displaystyle\boldsymbol{AbA} (a) Tessellation Not self-inverse (b) Limit set \displaystyle I\displaystyle a\displaystyle ba\displaystyle ca\displaystyle da\displaystyle aba\displaystyle cba\displaystyle dba \displaystyle I\displaystyle a\displaystyle ba\displaystyle ca\displaystyle da\displaystyle aba\displaystyle cba\displaystyle dba (c) Tessellation Self-inverse (d) Limit set Figure 10: Partial trees up to depth 3. The entries in bold are those to be rendered. Given a word of given length, cancellations shall be evaluated along a cascading approach: the formal word ababBAA first requires to drop bB and we get abaAA; again, we drop aA and we finally have abA which includes no more cancellations; whereas we do not need this cascading check when we are building words step by step. These remarks definitely attest that the correct processing of our subgroups requires every newly generated word to be checked for not including cancellations. 2.2 Presentations and multiplication tables In terms of our tree model, this latter task calls in the concept of phyllotaxis, i.e., the set of rules followed by the tree during its growth. Here they collect into lists that could be concise or not, depending on the degree of complication governing the tree growth. In general, rules concern how growth continues or stop. Identities are just special and simple cases of such stopping rules, all defined as cancellations for short. The expression below presents a widely used and compact form that lists generators on the left and cancellations on the right: \langle a,b\ |\ aa=bb=I\rangle (5) This refers to a subgroup of self-inverse maps, such as circle inversions (3) for instance. The letter I, meaning to the identity, is also conventionally replaced by the unit value 1, assuming the composition operator ‘\circ’ to be formally read like the arithmetic multiplication. In the next example expression, we worked with subgroups (4), where identities originate from pairs of mutually inverse generators \langle a,b,A,B\ |\ aA=Aa=bB=Bb=1\rangle. (6) Rules ensure that this is a 2-generators subgroup of non-self-inverse maps. These two examples are known as group presentations (or defining relations). Each identity detection rule on the right is said cancellation, because of symbols being deleted when identities occur within a new formal word. The goal of presentations is to obtain synthetic writings for simple groups, but they turn obsolete for more complicated actions that may feature several rules for composition and cancellation of words. We recall that, according to the remark at p. 1, there exists no unique and irreducible presentation. For example, the word aaBb resolves into the equivalent aa after the deletion of the identity Bb, according to the rules in the table (6). Figure 1.1 shows four partial trees that are built up according to the presentations (5) and (6) respectively. All the previous examples shall not suggest that the one-to-one relation from letters to generators is the sole way to detect identities. There exists a larger casuistry overcoming the formalities of the identities discussed so far, not just implemented through the concatenation of two opposite generators: for instance, a^{3}=aaa=I. (a) (b) (c) Figure 11: Tessellation and patterns. Patterns may be wrapped into orthogonal or oblique containers. a b A B a a b † B b a b A † A † b A B B a † A B a b c d a † b c d b a † c d c a b † d d a b c † (a) abABB (b) cabdc Not self-inverse subgroup Self-inverse subgroup Table 1: Trasversing the multiplication table. The tree growth in figs. 1.1 and 1.1 is driven by Cayley tables. The cancellations ‘\dagger’ appear in the presentations (6) and (5). We can follow the zig-zag path through the upper bar with the gray shades gradient. In such a more variegated scenario, it may happen that cancellation rules could be so many to no longer fit the goals behind the compact form of group presentations. This problem is settled by the so-called multiplication table or Cayley table, named after Arthur Cayley (1821–1895), which every single step along the formation of new chains of generators. Cayley tables include the same number of columns as of the generators (including the inverse ones) in the subgroup, whereas rows list all unique combinations allowed in a given group, including the cancellations. Again, every row is announced by one combination of generators on the far left column, and it is accessed by means of the combination with the cells in the other columns, following a sort of zig-zag path eventually ending at cancellation (box 1).999Presentations can be seen as synthetic versions of the analytic multiplication tables [10, p. 88], which provide specific composition rules besides cancellations. The two examples in box 1 stand out as the simplest ever, because the only directive to follow, for trasversing the table, wants to replace the current symbol by the next in line, again and again up to the chosen maximal length of the orbit or until we do not stumble into a cancellation rule. The (RL) reading of the word abA is equivalent to the following path A\underset{b}{\rightarrow}b\underset{a}{\rightarrow}a, running through three rows, one per each symbol. There exist groups whose multiplication tables include rows that are announced by words being longer than one symbol (see the example at [15, p. 359]), such as the path a\underset{b}{\rightarrow}ba\underset{A}{\rightarrow}Aba that runs over the table rows announced by a, ba, and Aba.101010 The formal word Bbba will eventually meet the cancellation rule in the row announced by the letter B. This is an excerpt of a multiplication table including longer entries than one letter [15, p. 359]: a b A B bAB I I BA B Bab ba b I I In any version, either as presentations or as tables, the tests for reduced words validation by means of cancellations cannot be exempted from implementation because of being strictly required to ensure the correct processing; otherwise said, ignoring the cancellation tests would bring inaccurate results. The lexicographic approach pre-processes words: it builds them in progression and checks if the newly appended symbol has met an identity rule and then triggered a cancellation. During the early 1990s, digital pictures of limit sets were produced through the lexicographic approach in a few works, such as in Manna and Vicsek’s [12], Bullets and Mantica’s [4], McShane, Parker and Redfern’s [14], and Parker’s [16]. None of them hit the technical details of the rendering anyway. This gap in the literature was filled inside the book Indra’s pearls by Mumford, Series and Wright [15], published in 2002 and providing a very extensive and plain discussion of the lexicographic approach. 3 Drawbacks of the lexicographic approach Given n generators and chains (2) of maximal depth d, tessellations and limit sets require \displaystyle N_{T}=\sum_{i=1}^{d}n^{i} (intermediate nodes and leaves) and N_{L}=n^{d} words (leaves only, i.e. the number of permutations) respectively. The lexicographic approach features the following additional computation costs: (1) a table for binding letters to the indexes of the generators stored in an array; (2) a table for registering the association between the letters of generators and of their inverses; (3) the implementation of bread-first (equivalently, depth-first) algorithm for trasversing the tree of words for generating the words dictionary up to a finite length; (4) the memory space required to store the dictionary; (5) the translation of symbols into indexes to pick up each generator long the chain of compositions. The following tables report the memory sizes of dictionaries including words up to length 17, which could allow some rendering quality. And these costs are doomed to dramatically grow, especially if close-ups of the limit set have to be rendered. Hence compiling the dictionary would equivalently turn into a very expensive process, that demands long computation times and very huge memory resources. words length 0 1 3 5 7 9 11 13 15 17 Tessellations process steps 1 5 53 485 4373 39365 354293 3188645 28697813 258280325 dictionary size 1B 5B 53B 485B 4.2KB 38KB 346KB 3.04MB 27MB 246.3MB Limit set process steps 1 4 9 81 729 6561 59049 531441 4782969 43046721 dictionary size 1B 4B 9B 81B 729B 6.4KB 57.7KB 519KB 4.56MB 41.06MB Table 2: Memory size for words. The upper and the lower table have been compiled for groups of four generators of self-inverse maps and of non-self-inverse maps respectively. The need of huge memory loads was already pointed out at [15, p. 141], where three approaches were provided to work around this problem: one was based upon recursion, the others on the tree model. All require considerable resources in terms of function calls stack. The recursion-based approach looks as the most onerous in this sense, as it triggers as many calls as the dictionary size, i.e. N_{T} or N_{L}; whereas the other two approaches look rather complicate and expensive, because discarding out the dictionary would imply the constant tracking the paths in tree while they have to be travelled back and forth in order to visit all the nodes therein. The lexicographic approach was not originally devised to saving resources so that the performance would eventually slow down during the running. Any alternative should then aim at giving a lighter and quicker approach, i.e. in practice, at dropping orbits storage and at devising alternatives to step-by-step orbit generation. 4 Index generation: the numerical alternative In order to get away from the lexicographic environment, we shall step back to the abstract level of composition, relying upon the abstraction of symbols, as discussed in §2 (fig. 7). We recall that letters are just meant as a choice for opening to rendering operations. A different materialization of abstract symbols consists in involving digits and numbers instead. We start from reviewing the insertion of tree nodes in fig. 1.1 under this new perspective. According to the Basis Representation Theorem [1, pp. 8–9], every numerical quantity Q can be written as a unique string q_{b} in base b\geq 2. Q acts like an abstract concept that allows travelling through arbitrary representations. Q is invariant under base conversion, and only the formal appearance changes; thus the increasing (or decreasing) trend of sequences of numbers in base 10, say 1_{10}, 2_{10}, 3_{10}, …, will be kept up the same trend under another numerical base. Given q_{2} and q_{10}, then q_{10}\rightarrow Q\rightarrow q_{2}: for every integer in base 10, there exists one and only one conversion into a different base. Base conversion represents an unambiguous and reliable approach to the formalization of orbits/chains. 0_{10} 1_{10} 2_{10} 3_{10} 4_{10} 5_{10} 6_{10} 7_{10} 8_{10} 9_{10} 10_{10} 11_{10} 12_{10} 13_{10} 14_{10} 15_{10} 0_{4} 1_{4} 2_{4} 3_{4} 10_{4} 11_{4} 12_{4} 13_{4} 20_{4} 21_{4} 22_{4} 23_{4} 30_{4} 31_{4} 32_{4} 33_{4} Table 3: Conversion from base 10 to 4. We notice that every base n is equipped with a set of exactly n distinct digits, which we could, even if improperly, call as alphabet again, because of performing homologous tasks. Generator g_{1} g_{2} g^{-1}_{1} g^{-1}_{2} Symbol a b A B Array index 0 1 2 3 Cancellations in letters aA bB Aa Bb aa bb AA BB Cancellations in digits 02 13 20 31 00 11 22 33 non-self-inverse generators self-inverse generators Table 4: Environmental arrays for 4-generators groups. The transition from the lexicographic to the indexed approach has been depicted in table 4, by comparing formal compositions. Level 0 I Level 1 A B a b 0_{4} 1_{4} 2_{4} 3_{4} Level 2 AA AB Ab 00_{4} 01_{4} 03_{4} BB BA Ba 11_{4} 10_{4} 12_{4} aa aB ab 22_{4} 21_{4} 23_{4} bb bA ba 33_{4} 30_{4} 32_{4} Level 0 I Level 1 a b c d 0_{4}=0_{10} 1_{4}=1_{10} 2_{4}=2_{10} 3_{4}=3_{10} Level 2 ab ac ad 01_{4}=1_{10} 02_{4}=2_{10} 03_{4}=3_{10} ba bc bd 10_{4}=4_{10} 12_{4}=6_{10} 13_{4}=7_{10} ca cb cd 20_{4}=8_{10} 21_{4}=9_{10} 23_{4}=11_{10} da db dc 30_{4}=12_{10} 31_{4}=13_{10} 32_{4}=14_{10} self-inverse generators non-self-inverse generators Table 5: Applications to subgroups. Some entries have been skipped because of cancellation rules. Index generation covers all combinations yielded by the lexicographic approach (fig. 1.1). At the initial stage, the difference regards the adopted symbols only. Every new number in base 4, for example, shows up as the concatenation of digits, taken from the set [0, 1, 2, 3] (table 3). In every 4-generators subgroup, the letters a, b, c, d are respectively associated to the digits of the 4-base number system: 0, 1, 2, 3. The formal expressions of numerical quantities under some given value are permutations of digits. Let the value 10000, then all smaller numbers from 0000 to 9999 are permutations of all digits in the 10-base system. Given an alphabet of cardinality n=4, the concatenation of letters up to depth d is equivalent to writing all numbers in base n up to the value n^{d}, in other terms, to compute all the permutations of n symbols up to depth d. The generation of all words in the lexicographic approach can be completely replaced by the increasing sequence of positive integers in some arbitrary base system. We no longer need to build up dictionaries and store massive bulks of data: the n-base representation can return the same information as from the combinations built on purpose through the step-by-step concatenation of letters. Numbers devolve into sequences of digits, i.e. of strings being managed through the symbols concatenation; they however keep all we need to render the subgroup action. 4.1 Questioning on zero-based arrays. Speed would be meaningless without wise management. In fact, we are switching from quantity, represented by numbers, to quality (i.e., the visual appearance) of digits, i.e. of symbols that are deprived from the original meaning. This transition moves from positional numerical systems to strings of concatenated symbols. Here we could stumble into the question to working with the ambiguous role played by the digit 0 (table 4), which, on one side, it unequivocally refers to the action of the Möbius map stored in the array at the index (111111Arrays are data structured endowed with zero-based indexing.) 0 but, on the other, the vanishing nature of 0 gives rise to several ambiguous but operatively equivalent formalizations: we mean to chains being prefixed by arbitrary many zeros, such as 1 for instance and all the infinitely many concatenations resumed into the periodic form \overline{0}1, for example. Again, The digit 0, unlike all others from 1 to 9, could relate to quantification or not, depending on the position within the string of digits: it plays the multiplicative role if appended to the far right (ex: 10000), or none if prepended to the far left (ex: 00001); we mean to trailing and of leading zeros respectively. In the formal context of symbols concatenation, the digit 0 drops the role played in the positional representation of numbers; being no longer a numerical value but just as a symbol, it is an index that refers to a generator for the given subgroup; this is another reason why leading zeros are as important as trailing ones here.121212We see that the zeros left padding does not occur for groups of self-inversions for instance, where the composition of words does not allow contiguous symbols repetition. & \displaystyle I\displaystyle 1\displaystyle 4\displaystyle 3\displaystyle 2\displaystyle 6\displaystyle 7\displaystyle 8\displaystyle 12\displaystyle 11\displaystyle 10\displaystyle 14\displaystyle 15\displaystyle 16\displaystyle 20\displaystyle 19\displaystyle 18\displaystyle 5\displaystyle 17\displaystyle 13\displaystyle 9 (a) (b) Figure 12: Viewpoints. (a) Ordinal: each integer is translated and its quantity is kept up during the translation. (b) Cardinal: orbits are counted while they are generated; every integer is translated and each new digit in the new base representation is eventually remapped to zero-based indexing for all the integers that are associated to orbits of maximal chosen length, here 2. Would it be a real or an apparent difficulty anyway? Response is mixed and mostly affected by the way we decide to deal with zero-based indexing management of data arrays. According to the above approach, managing the zero digit boils down to dealing with symbols concatenation, analogously to what we formerly did with letters, considering that we dealing with the family of zero-prefixed strings, like ‘0001’, in a hybrid form where the zero digit is simultaneously worked out as a symbol in the chain, in order to compute the orbit, and as a quantity that could be prefixed by arbitrarily many zeros, as the unit, 1, is quantitatively equivalent to 01, 001, … . In short, this scenario reads every new integer, in the sequence 0, 1, 2, 3, 4, …and as cardinal number and then to a numerical value that is open to multiple representations (fig. 1.1/a).131313The transformation from numerical values to strings of symbols is one-to-many (= multi-valued); conversely, strings with leading zeros would be encoded back to the same numerical value in the many-to-one fashion (= single-valued). Because of the aforementioned reasons, base conversion cannot cover strings with leading zeros; hence we have to implement a separate procedure for managing these special strings. Otherwise, we could read the input integers as ordinal numbers, that is, we just want to count the orbits in the order of appearance during the process of base conversion and bind an increasing number. Hence the strictly positive integers 1, 2, 3, 4, 5, 6, …, will just indicate different orbits, each of which be again represented under the chosen n-based system, 1_4, 2_4, 3_4, 10_4, 11_4, 12_4, … but regardless of the leading zeros now (fig. 1.1/b). With regard to the 4-base representation here for instance, we need to simply re-map the indexes to the zero-based indexing as follows, in order to correctly manage arrays of digital data and pick up the Mobius maps for computations: 1 2 3 4 \downarrow \downarrow \downarrow \downarrow 0 1 2 3 The cardinal version runs up to the 10-based integer n^{d}, where n counts the symbols in the new base representation and d is the maximal depth/length of the orbits, i.e., the value n^{d} in base 10, as previously remarked, represents the set of all permutations of strings including n symbols (the numerical base) and with length d. None of these two options represents the best choice: both are valid and the difference just regards about base representations filled by leading zeros and thus involving a larger number of orbits displayed; thus the cardinal approach will be more accurate and the ordinal one will be quicker. 4.2 Guidelines for Index generation. Resuming, the implementation of the index generation algorithm consists in (1^{\circ}) taking on a number in base 10, say 93_{10}; (2^{\circ}) encoding it into the new base, say 1131_{4}; (3^{\circ}/a) Cardinal approach: left padding every string yielded in the step 2 through a sequence of leading zeros up to a given finite maximal length. Suppose the latter is 8, the 4-base number obtained above would increasingly left padded in order to obtain the four strings (8-4=4): 01131_{4}, 001131_{4}, 0001131_{4}, 00001131_{4}; (3^{\circ}/b) Ordinal approach: remap every digit in the resulting base conversion from 1-based to 0-based indexing, i.e., by decrementing each digit by one. (4) feeding the resulting string in the new base to the rendering engine. All process boils down to converting numbers into the new base and processing the obtained strings. We no longer need to store them. The index generation algorithm post-processes words: the base conversion yield a new words which is checked whether there are subsets triggering cancellations. Figure 13: Borders look blurred because points in the farther regions are reached in the conclusion of the process and thus may be partially covered by the relatively longer orbits. Closer points belong to regions which are more probably covered by the chosen maximal length of the orbits. 5 The pseudo-code implementation We will give here some guidelines for the code implementation of the index generation algorithm, in order to render tessellations or limit sets. We split code into blocks, which might be of help for easily following every step of the process. We have adopted a pseudo object-oriented141414Here closer to the syntax of C-like family of imperative languages, such as Java, Javascript, …. language in order to ease the customization into the preferred environment. Generators are assumed to be instantiations of some class endowed with methods and data containers, accessed via this conventional syntax: obj.⟨method_id⟩(parameters) for methods and for obj.⟨variable_id⟩ for containers (i.e., variables) respectively. We begin from the generators listed in table 4 at p. 4: for sake of simplicity and with no loss of generalization, we will assume to work with 2-generators groups ruled by the presentation (5) or (6). The index generation algorithm is scalable and not affected by the cardinality of the generators set.151515 In what follows, the expression index generation refers to the algorithm, whereas the sole index to the position within the array of generators. The generator–index association, within the array storage, is automatically set up by the instantiation of the logical array (table 4 at p. 4) and it is zero-based. (Hence we shall choose one of the paths discussed in the previous section: cardinal or ordinal.) Some environmental variables and containers have to initialized, like in the code below. {mdframed} 1var _gens_objs = [ g1, g2, g3, g4 ], _gens_num = _gens_objs.length;2var _max_depth = 10, _max_value = power( _gens_num, _max_depth );3var _proc_str = """", _zero_fill_proc_str = """", _index, _circle;4var _str_length = 1, _rec_start = 0, _rec_end = -1;5var _b_crash_found = 0; We stress that the numerical nature of this algorithm disengages from the concept of word depth, which is more naturally tied to the transversion of the lexicographic approach. We will deal with the number of steps instead, and so we have to set up an arbitrary maximal value that stops the main loop. We opted to use the for syntax as it looks conceptually close to the increasing sequences of integers here involved by the base transformation.161616There is no impediment against the usage of while-loop syntax anyway, being, as known, the generalization of the specialized conditions in the header of for loops. We will work with inversion circles and with pixels/points for rendering tessellations or limit sets respectively. {mdframed} 1for( var _i = 0; _i < _max_value; _i++ ){2 _proc_str = _i.<convert-to-base>( _gens_num );34 //the test below may involve the multiplication table5 //or the subgroup presentation6 if ( <call-to-sub-routine-#1.x:cancellation-rule-test_of_proc_str> ) continue;78 <call-to-sub-routine-#2.x:process-the-numerical-string-in-base-n>910 <optional-call-to-sub-routine-\#3:leading-zeros-management for the ordinal reading>11} We are going to render limit sets first: a string of symbols is returned for each value of the loop counter _i and processed by composition of Möbius maps. Input points. Unlike the rasterized rendering of Julia sets, we do not need to check every point/pixel inside the region of interest (the escape time method). According to the theory of function groups (which both fuchsian and kleinian kinds belong to), it is sufficient to give an arbitrary input value: the definition of a limit point depends only on the sequence of elements of the subgroup \mathcal{G}, and not on the points belonging to the region U where the action of \mathcal{G} is freely discontinuous [13, p. 22, D.3]. Moreover, since the limit set is transformed into itself by the subgroup transformations [7, p. 43], we found worth picking up the input values from the fixed points of a generator. The generic label #2.x refers to the code block #2.1 which renders the limit set. After processing the input word from right to left, we will display on the screen the last element of every orbit exclusively, as required by the definition of limit sets. {mdframed} 1//<sub-routine-#2.1:limit set mode>2//right-to-left reading order3_index = <turn-the-symbol-to-integer>( _proc_str[ _proc_str.length-1 ] );4//initialization5_fp = _gens_objs[ _index ].get_one_fixed_point();67//process the rest of the string8for( var _wr = _proc_str.length-2; _wr >= 0; _wr-- ){9 _index = <turn-the-symbol-to-integer>( _proc_str[ _wr ] );10 _fp = _gens_objs[ _index ].map_point( _fp );11}1213<call-a-sub-routine-for-drawing-the-pixel-at-the-fixed-point-coordinates> Tessellation via disc images renderings need the reference #2.x to be replaced by the block #2.2, where every new inversion circle is plotted when a new symbol along the input word is read from right to left, and processed: {mdframed} 1//<sub-routine-#2.2:disc images (tessellation) mode>2//right-to-left reading order3_index = <turn-the-symbol-to-integer>( _proc_str[ _proc_str.length-1 ] );4//initialization5_circle = _gens_objs[ _index ].get_inversion_circle();67<call-a-sub-routine-for-drawing-the-circle>8//process the rest of the string9for( var _wr = _proc_str.length-2; _wr >= 0; _wr-- ){10 _index = <turn-the-first-symbol-to-integer>( _proc_str[ _wr ] );11 _circle = _gens_objs[ _index ].map_inversion_circle( _circle );12 <call-a-sub-routine-for-drawing-the-circle>13} The label <optional-call-to-sub-routine-#3:leading-zeros-management> refers to the pseudo-code implementing, in respect of the above cardinal approach, the elaboration of strings with leading zeros too: we simply generate them by pre-pending the 0 to every string from each one yielded in the main for-loop: for example, the input integer \texttt{5}_{10} turns into \texttt{11}_{4} in base 4 and then pad it up to maximal depth, say 6 here, by leading zeros, so to obtain: 011, 0011, 00011, 000011. {mdframed} 1//<optional-call-to-sub-routine-\#3:leading-zeros-management>2if ( _b_length_change )3{4 _rec_end = _n - 1;5 for( var _r = _rec_start; _r <= _rec_end; _r++ )6 {7 _zero_fill_proc_str = _r.toString( _n_gens );8 for( var _filler = _zero_fill_proc_str.length; _filler <= _max_depth; _filler++ )9 {10 _zero_fill_proc_str = ""0"" + _zero_fill_proc_str;11 if ( <call-to-sub-routine-#1.x:cancellation-rule-test_of_proc_str> )12 continue;13 <call-to-sub-routine-#2.x:process-the-numerical-string-in-base-n>14 }15 }1617 _rec_start = _rec_end + 1;18 _rec_end = -1;19 _b_length_change = 0;20} The ordinal version needs not to run this last subroutine. We also remark that the pseudo-code {mdframed} if ( <call-to-sub-routine-#1.x:cancellation-rule-test_of_proc_str> ) continue; refers to tests to be performed according to the Cayley table or presentation related to the given subgroup. Two examples (about the tables presented in the box 1 at p. 1) follow below. The equivalent cancellation rules are (5) and (6) in terms of presentations. Implementation is easy: instead of trasversing rows and columns in the table, these cancellation tests check whether every new input string of digits includes at least one of the rules on the right of the presentation. {mdframed} 1//<sub-routine-#1.1:group-presentation>2function __check__group_presentation__( _digitized_word = """" )3{4<let a boolean flag and set it to 0>56<for each entry inside the group presentation>7 <check if the input digitized word includes the current entry>8 <if so, set the above flag to 1 and break this loop>9<end-of-for-loop>1011<return the boolean flag>12} And now the pseudo-code for cancellation tests with regard to the multiplication table. {mdframed} 1//<sub-routine-#1.2:multiplication-table-test>2function __multiplication-table-test__( _digitized_word = """" ) {3//the goal is to check for cancellations. The return value is 1 or 0 if found or not4<let a boolean flag and set it to 0>56//we assume that the index has been converted into the new required base7<split-the-digitized-word-into-an-array-of-single-digits->8<get-the-first-digit-in-the-word>9<get a reference pointer to the related row inside the table>1011//we prevent to raise conditional if-statement in the loop12<remove the first digit from the word>1314//trasverse and explore the Cayley table15<for each digit in the rest of this array> //sequential read16 <get the next-index in the current row at the index17 referred by the current digit>18 <if the next-index refers to a cancellation, then19 (1) set the above flag to 120 (2) break this loop21 (3) return the flag to skip the processing of the word under consideration22 >23 <otherwise get a reference pointer to the row inside the-table and24 related to the next-index>25<end-of-for-loop>2627<return the flag value>28} 6 Conclusions Figure 14: This is a close-up of the limit set for a degenerate subgroup of 2-generators, whose coefficients satisfy special and delicate numerical conditions. The digital nature of the index generation algorithm could take away some of the charm tied to the theory of word processing performed by the lexicographic approach (refer to the algebraic theory of commutators at [15, p. 168]) and emanating from the related literature ([6] über alles). Anyway, for practical purposes, this new approach gives the benefit of generating every chain of generators in one only step, much faster than the lexicographic approach. The transformational character of the index generation algorithm relies upon the simpler and quicker generation of words, no longer coming from the constructive progression, like in the lexicographic approach which sets up a tortuous track disseminated by the technical drawbacks here discussed at §3, such as appending symbols, checking words and storing them. The necessary computational costs of the index generation algorithm concern cancellation rules and tests. One more drawback of lexicographic approach concerns the sequential building of words of n symbols, which are deduced from those of length n-1. At this regard, we observe that the intrinsic tree structure involves nodes dependency, which demands to start from the root and walk through the consecutive nodes in order to get to the given depth. On the contrary, the index generation algorithm enjoys the benefits of numerical sequences which is based upon, allowing to start, stop and resume the generation of strings at any arbitrary element of the sequence. Now we could jump from end to end here, instead of walking through the interval. It is known that d=\displaystyle\Bigl{\lfloor}{\frac{\log(i)}{\log(n)}}\Bigr{\rfloor} returns the number d of digits required to convert i from base 10 to base n; so we can explore limit sets inside some interval of integer values, which match with words of length l, given d\leq l\leq D for instance. The author has developed a web application that implements both the lexicographic and the index generation algorithm at http://alessandrorosa.altervista.org/circles/; a number of demos can be run for introductory purposes, or groups be built either geometrically through inversion circles or algebraically via input of arbitrary coefficients into Möbius maps. Refer to [17] for related examples. References [1] Andrews G.E., Number Theory, Saunders, 1971. [2] Beardon A., The Geometry of Discrete Groups, Springer, 1983. [3] Bessis D., Demka S. Generalized Apollonian packings, Commun. Math. Phys., 134, 1990, pp. 293–319. [4] Bullets S., Mantica G., Group theory of hyperbolic circle packings, Nonlinearity, 5, 1992, pp. 1085–1109. [5] Devaney R. L., Marotta S. M., Mandelpinski necklaces in the parameter plane of rational maps, Springer Proceedings in Mathematics and Statistics, 2021, pp. 95-119. [6] Epstein D.B.A. et alia, Word processing in Groups, Jones and Bartlett Publishers, Boston, 1992. [7] Ford L., Automorphic functions, McGraw-Hill, New York, 1929. [8] Fricke R., Klein F., Vorlesungen über die Theorie der automorphen Functionen, Teubner, Leipzig, 1897. [9] Krushkal S.L., Apanasov B.N., Gusevskiĭ N. A., Kleinian Groups and Uniformization in Examples and Problems, Translations of Mathematical Monographs, AMS, 1986. [10] Lyndon R.C., Schupp P.E., Combinatorial Group Theory, Springer, 2001. [11] Magnus W., Non-Euclidean Tesselations and their Groups, Elsevier, 1974. [12] Manna S.S., Vicsek T., Multifractality of Space-Filling Bearings and Apollonian Packings, Journal of Statistical Physics, 64, 3/4, 1991. [13] Maskit B., Kleinian Groups, Springer, 1988. [14] McShane G., Parker J.R., Redfern I., Drawing limit sets of Kleinian groups using finite state automata, Experimental Mathematics, vol. 3, 2 (1994), pp. 153–170. [15] Mumford D., Series C., Wright D., Indra’s pearls: The Vision of Felix Klein, Cambridge University Press, 2002 (reprinted in 2015). [16] Parker J.R., Kleinian circle packings, Topology, vol. 34, No. 3, 1995, pp. 489–496. [17] Rosa A., The pearls of Heavens: A gallery of Kleinian Groups, 2023, https://www.academia.edu/95460195/"
https://arxiv.org/html/2411.08673v1,ScribGen: Generating Scribble Art Through Metaheuristics,"Scribble art, arising from chaos and randomness, remains one of the exceptionally attractive forms of art. Many works bridge the gap between sketches and images, but few translate images into meaningful chaotic expressions. While deep generative networks are known for understanding images, their ability to induce scribble drawings is under-explored. Unlike GAN-based approaches that generate line drawings, sketches, and contours, our work uses metaheuristics to produce scribble art from images. We extensively analyse various metaheuristic algorithms, demonstrating their optimal balance between creativity and computational efficiency. They offer better adaptability and accuracy than state-of-the-art deep generative models for image-to-scribble generation.","1.1. Scribble Art Art has long been a medium for individuals to engage with the world. Scribble art, a form of abstract visual expression, features spontaneous, gestural strokes made with pens or brushes. These dynamic and expressive compositions, created quickly and impulsively, reveal intricate patterns and hidden meanings upon closer inspection. While scribble art is often associated with spontaneous expression and experimentation, it can also be planned and intentional. Some artists use scribble techniques as a starting point for their creative process, exploring the possibilities of line, shape, and texture before refining their work into more polished compositions. From ancient cave paintings to modern abstract sketches and doodles, scribble art has evolved with civilizations, reflecting diverse artistic movements and cultural influences. This evolution highlights its universal appeal, transcending language and cultural barriers and connecting people through the shared experience of creating art. \Description The figure lists the fundamental ideas of various metaheuristic algorithm categories based on evolution, nature, physics, behaviour and hybrid methods. Figure 2. Categories of Metaheuristic algorithms 1.2. Art and Technology Although digital platforms have overshadowed many analogue methods, they have also created new avenues for artistic experimentation. The intersection of art and technology has been explored through deep neural networks, which recreate various art forms, including shadow art (Sadekar et al., 2022) (Gangopadhyay et al., 2023). Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have shown promising results in generating realistic images from simple sketches (Chen and Hays, 2018) (Lu et al., 2018), allowing for computational creativity. Some studies have synthesized circular scribble art from grayscale images (Chiu et al., 2015), while others generate line drawings from colour images (Chan et al., 2022). These works have either been limited to circular scribbles (scribble art is much closer to quick linear strokes and random doodles) or by the availability of edge-map-like training data (often missing the artistic touch), thus highlighting the potential for further exploration in creating ""meaningfully random"" scribbles that mimic human expressions. Algorithm 1 Proposed Algorithm: ScribGA Input: Population size, N = 100 Input: Number of Generations, G_{max} Output: Set of best solutions, R Begin Generate initial population of N solutions Y_{i} (i=1,2,...,N) Set the iteration counter t=0 Initialize R as an empty set while t<G_{max} do Compute fitness of each solution as per Figure 3 Include the best solution to the set R Select parent pairs based on fitness for each pair of parents do Apply crossover to generate offspring Apply mutation to the offspring end for Evaluate the fitness of the new offspring Replace the old population with the new offspring Increment the iteration counter t=t+1 end while Return R End \Description The figure represents the process of computing solution fitness and generating scribble art using a generic metaheuristic algorithm backbone. Figure 3. Computing solution fitness and generating scribble art. Eiffel tower image generated by DALL-E 3 (Betker et al., 2023) from text prompts. 1.3. Contribution This work explores the potential of using metaheuristic algorithms to generate scribble art. Given a reference image, we define scribble art as an approximation of the image’s contours using random yet strategically placed line strokes that preserve the structure of the object in the given image. Metaheuristics (Osman and Kelly, 1997) addresses multifaceted, non-linear challenges, thereby identifying high-quality solutions. Unlike classic optimization approaches that rely on gradients, metaheuristics utilize exploration and exploitation-based strategies. These algorithms apply to diverse fields, from mathematics and engineering to healthcare and creative expression. In this work, we modify and examine a set of metaheuristic algorithms, i.e., Genetic Algorithm (Holland, 1992) (ScribGA), Differential Evolution (Feoktistov, 2006) (ScribDE), Particle Swarm Optimization (Kennedy and Eberhart, 1995) (ScribPSO), Gravitational Search Algorithm (Rashedi et al., 2009) (ScribGSA), and Harris Hawk Optimization (Heidari et al., 2019; Debnath et al., 2023) (ScribHHO), to specifically cater to scribble art generation. We also compare our results with learning-based image-to-sketch methods."
https://arxiv.org/html/2411.08037v1,Material transforms from disentangled NeRF representations,"In this paper, we first propose a novel method for transferring material transformations across different scenes. Building on disentangled Neural Radiance Field (NeRF) representations, our approach learns to map Bidirectional Reflectance Distribution Functions (BRDF) from pairs of scenes observed in varying conditions, such as dry and wet. The learned transformations can then be applied to unseen scenes with similar materials, therefore effectively rendering the transformation learned with an arbitrary level of intensity. Extensive experiments on synthetic scenes and real-world objects validate the effectiveness of our approach, showing that it can learn various transformations such as wetness, painting, coating, etc. Our results highlight not only the versatility of our method but also its potential for practical applications in computer graphics. We publish our method implementation, along with our synthetic/real datasets on https://github.com/astra-vision/BRDFTransform","In computer graphics and vision, inverse rendering is key to extracting material information and allowing re-rendering under novel conditions (viewpoint, lighting, materials, etc.). While neural representations have largely taken over the traditional Physically-Based Rendering (PBR) techniques, recent works have demonstrated that the two representations can be combined [JLX∗23], thus preserving the editability and expressivity of PBR representations along with the flexibility of neural representations. When considering the appearance of a scene, certain transformations (such as applying a coat of varnish) can alter the material properties significantly, causing the scene’s appearance to change drastically. Currently, estimating the PBR characteristics of a known material after such a transformation requires capturing the scene again in the desired target condition. This process is both complex and laborious due to the variety of possible transformations, such as wetness, dust, varnish, painting, etc. In this work, we aim to learn a BRDF transformation from a source scene and apply it to different scenes. Assuming we have paired observations of the same scene under two different conditions, say original and varnished, we propose a method to learn the transformation of materials. This transformation can then be applied to another scene composed of similar materials. This allows us to predict the appearance of that scene under this effect, effectively transferring the material transformation. Material transforms from disentangled NeRF representations illustrates that several material transformations can be learned from multiple pairs of scenes (left) and later applied on novel scenes (right), whether synthetic or real. Technically, our method relies on the joint optimization of a radiance field corresponding to a first scene captured in original and transformed (e.g., varnished) conditions, possibly with varying lighting conditions. We rely here on the disentangled NeRF representation of TensoIR [JLX∗23]—that optimizes appearance, geometry, and parametric BRDF simultaneously—while introducing two novel key components. First, we condition the transformed scene BRDF on the original scene and approximate its transformation with a Multi-Layer Perceptron (MLP). Second, we expose a limitation of TensoIR showing it fails at decomposing highly reflective materials and propose an improved light estimation scheme that better estimates low roughness components while preserving high frequencies in the illumination. As a result, our framework allows capturing a collection of transformations which can then be applied on new scenes, while controlling the intensity of the transformation. We demonstrate the performance of our method on two new datasets: a synthetic dataset with a series of custom shader transformations and a real-world dataset of figurines with varying material conditions (e.g., original, painted, varnished, etc.). On both datasets, our approach produces faithful transformations. Our method and datasets will be released publicly."
https://arxiv.org/html/2411.08033v1,: Interactive Point Cloud Latent Diffusion for 3D Generation,"While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs a Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multi-view image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing methods in both text- and image-conditioned 3D generation.","3D content generation holds great potential for transforming the virtual reality, film, and gaming industries. Current approaches typically follow one of two paths: either a 2D-lifting method or the design of native 3D diffusion models. While the 2D-lifting approach (Shi et al., 2023b; Liu et al., 2023b) benefits from leveraging 2D diffusion model priors, it is often hindered by expensive optimization, the Janus problem, and inconsistencies between views. In contrast, native 3D diffusion models (Jun & Nichol, 2023; Lan et al., 2024; Zhang et al., 2024) are trained from scratch for 3D generation, offering improved generality, efficiency, and control. Despite the progress in native 3D diffusion models, several design challenges still persist: (1) Input format to the 3D VAE. Most methods (Zhang et al., 2024; Li et al., 2024) directly adopt point cloud as input. However, it fails to encode the high-frequency details from textures. Besides, this limits the available training dataset to artist-created 3D assets, which are challenging to collect on a large scale. LN3Diff (Lan et al., 2024) adopt multi-view images as input. Though straightforward, it lacks direct 3D information input and cannot comprehensively encode the given object. (2) 3D latent space structure. Since 3D objects are diverse in geometry, color, and size, most 3D VAE models adopt the permutation-invariant set latent (Zhang et al., 2023a; Sajjadi et al., 2022; Zhang et al., 2024) to encode incoming 3D objects. Though flexible, this design lacks the image-latent correspondence as in Stable Diffusion VAE (Rombach et al., 2022), where the VAE latent code can directly serve as the proxy for editing input image (Mou et al., 2023b; a). Other methods adopt latent tri-plane (Wu et al., 2024; Lan et al., 2024) as the 3D latent representation. However, the latent tri-plane is still unsuitable for interactive 3D editing as changes in one plane may not map to the exact part of the objects that need editing. (3) Choice of 3D output representations. Existing solutions either output texture-less SDF (Wu et al., 2024; Zhang et al., 2024), which requires additional shading model for post-processing; or volumetric tri-plane (Lan et al., 2024), which struggles with high-resolution rendering due to extensive memory required by volumetric rendering (Mildenhall et al., 2020). In this study, we propose a novel 3D generation framework that resolves the problems above and enables scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. The resulting method, dubbed GaussianAnything, supports multi-modal conditional 3D generation, including point cloud, caption, and image. Specifically, we propose a 3D VAE that adopts multi-view posed RGB-D(epth)-N(ormal) renderings as the input, which are easy to render and contain comprehensive 3D attributes corresponding to the input 3D object. The information of each input view is channel-wise concatenated and efficiently encoded with the scene representation transformer (Sajjadi et al., 2022), yielding a set latent that compactly encodes the given 3D input. Instead of directly applying it for diffusion learning (Zhang et al., 2024; Li et al., 2024), our novel design concretizes the unordered tokens into the shape of the 3D input. Specifically, this is achieved by cross-attending (Huang et al., 2024b) the set latent via a sparse point cloud sampled from the input 3D shape, as visualized in Fig. 2. The resulting point-cloud structured latent space significantly facilitate shape-texture disentanglement and 3D editing. Afterward, a DiT-based 3D decoder (Peebles & Xie, 2023; Lan et al., 2024) gradually decodes and upsamples the latent point cloud into a set of dense surfel Gaussians (Huang et al., 2024a), which are rasterized to high-resolution renderings to supervise 3D VAE training. After the 3D VAE is trained, we conduct cascaded latent diffusion modeling on the latent space through flow matching (Albergo et al., 2023; Lipman et al., 2023; Liu et al., 2023c) using the DiT (Peebles & Xie, 2023) framework. To encourage better shape-texture disentanglement, a point cloud diffusion model is first trained to carve the overall layout of the input shape. Then, a point-cloud feature diffusion model is cascaded to output the corresponding feature conditioned on the generated point cloud. The generated featured point cloud is then decoded into surfel Gaussians via pre-trained VAE for downstream applications. In summary, we contribute a comprehensive 3D generation framework with a point cloud-structured 3D latent space. The redesigned 3D VAE efficiently encodes the 3D input into an interactive latent space, which is further decoded into high-quality surfel Gaussians. The diffusion models trained on the compressed latent space have shown superior performance in text-conditioned 3D generation and editing, as well as impressive image-conditioned 3D generation on general real world data."
https://arxiv.org/html/2411.07579v3,Projecting Gaussian Ellipsoids While Avoiding Affine Projection Approximation,"Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its real-time rendering speed and state-of-the-art rendering quality. However, during the rendering process, the use of the Jacobian of the affine approximation of the projection transformation leads to inevitable errors, resulting in blurriness, artifacts and a lack of scene consistency in the final rendered images. To address this issue, we introduce an ellipsoid-based projection method to calculate the projection of Gaussian ellipsoid onto the image plane, which is the primitive of 3D Gaussian Splatting. As our proposed ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera origins inside them or parts lying below z=0 plane in the camera space, we designed a pre-filtering strategy. Experiments over multiple widely adopted benchmark datasets show that our ellipsoid-based projection method can enhance the rendering quality of 3D Gaussian Splatting and its extensions.","Novel View Synthesis (NVS) plays a crucial role in computer vision and computer graphics, with numerous applications, including robotics, virtual reality, and 3D gaming. One of the most influential works in this field is the Neural Radiance Field (NeRF) [36], proposed by Mildenhall et al. in 2020. NeRF utilizes a Multilayer Perceptron (MLP) to store geometric and appearance information of a scene and employs differentiable volume rendering [9, 29, 35, 22]. Although NeRF and its extensions can render high-quality images, their training time is excessively long, and the rendering speed is far from achieving real-time rendering (\geq 30 fps). Recently, 3D Gaussian Splatting (3DGS) [24] has made a significant impact in NVS due to its real-time rendering speed, high-quality rendering results and competitive training times. Unlike NeRF, 3DGS represents scenes with a set of Gaussian ellipsoids explicitly. By projecting each Gaussian ellipsoid onto the image plane and using \alpha-blending for rendering, the properties of each Gaussian ellipsoid, position, pose, scale, transparency, and color, are optimized based on a multi-view photometric loss. Although 3D Gaussian Splatting has shown impressive results, due to the local affine approximation of the projection transformation [58] at the center of each Gaussian ellipsoid during rendering, errors are inevitably introduced, negatively affecting rendering quality and scene consistency. We observed blurriness and artifacts in distant objects in the scene, which we attribute to larger errors in the approximated projection transformation as the distance from the Gaussian ellipsoid center grows, particularly when distant Gaussian ellipsoids are generally large. Additionally, while the details rendered in the training set are of high quality, the results in the test set show a decline, likely due to the lack of scene consistency on account of the approximated projection transformation. To solve this problem, we propose an ellipsoid-based projection method. Our core idea is to calculate the ellipse equation projected onto the image plane based on the work of David Eberly [10], given the equation of the ellipsoid and the image plane. We first derive the gaussian ellipsoid equation from the covariance matrix of the 3D Gaussian function. Then we find the equation of the cone formed by lines that pass through the camera’s origin and are tangent to the ellipsoid surface. Finally, we determine the intersection line of this cone with the image plane, which gives us the projected ellipse equation. The following experiments demonstrate that there are two types of Gaussian ellipsoids that can cause the training process to diverge and negatively impact the system. The first type has the camera’s origin inside the ellipsoid, where no lines through the camera’s origin can be tangent to it. The second type consists of Gaussian ellipsoids that have a portion below z=0 plane in the camera space, the projection of these ellipsoids results in hyperbola or parabola [10] rather than an ellipse. To avoid negatively impacting the system, we designed filtering algorithms specifically for these two types of Gaussian ellipsoids. Extensive experiments demonstrated that our method not only improves rendering quality compared to 3DGS but also further accelerates rendering speed. In summary, we make the following contributions: • We proposed an ellipsoid-based projection method to eliminate the negative impact on rendering quality and scene consistency caused by approximating the projection transformation using the Jacobian of its affine approximation in 3DGS. • We design a pre-filtering strategy for Gaussian ellipsoids that cannot be projected before the rendering process, enhancing the system’s robustness and contributing to faster rendering speed. • Experiments conducted on challenging benchmark datasets demonstrated that our method surpasses 3DGS in both rendering quality and speed. • Our ellipsoid-based projection method shows improvement results on 3DGS and its extensions, and can be easily applied to them, requiring only few changes to the original code."
https://arxiv.org/html/2411.07025v1,Scaling Mesh Generation via Compressive Tokenization,"We propose a compressive yet effective mesh representation, Blocked and Patchified Tokenization (BPT), facilitating the generation of meshes exceeding 8k faces. BPT compresses mesh sequences by employing block-wise indexing and patch aggregation, reducing their length by approximately 75% compared to the original sequences. This compression milestone unlocks the potential to utilize mesh data with significantly more faces, thereby enhancing detail richness and improving generation robustness. Empowered with the BPT, we have built a foundation mesh generative model training on scaled mesh data to support flexible control for point clouds and images. Our model demonstrates the capability to generate meshes with intricate details and accurate topology, achieving SoTA performance on mesh generation and reaching the level for direct product usage.","Meshes, the cornerstone of 3D geometric representation, are widely utilized in various applications, including video games, cinematic productions, and simulations. Despite their widespread adoption, the meticulous craft of meshes with functional topologies demands substantial design effort. This labor-intensive process acts as a bottleneck, impeding the evolution of 3D content creation and the progress of immersive human-computer interaction. Recent research [36, 3, 45, 4, 5, 38] has tried to automate the mesh sculpting process via auto-regressive Transformers. These methods directly generate vertices and faces as human-crafted to maintain the high-quality mesh topology, yielding promising results on low-poly mesh generation. The foundation of modeling meshes with auto-regressive transformers is mesh tokenization, which converts the mesh into a one-dimensional sequence. PolyGen [26] and MeshXL [3] directly tokenize the mesh by converting the vertex coordinates to sorted triplets, each defining a tuple of quantized 3D coordinates. They learn the one-dimensional sequence with a joint or two separate auto-regressive transformers. MeshGPT [36] and its variants [45, 4] utilize an auto-encoder to convert meshes into latent sequences. MeshAnythingv2 [5] and Edgerunner [38] propose improved tokenization methods to compress vanilla mesh sequences further. However, these methods still convert meshes to relatively long sequences, limiting the ability of generative models to learn with high-poly meshes. To scale up mesh generation, a more compressive representation is demanded to extend the scope of the training data. In this paper, we propose a compressive yet efficient mesh representation called Blocked and Patchified Tokenization (BPT). BPT converts Cartesian coordinates to block-wise indexes, which makes the initial attempt to compress mesh tokens at the vertex level. Then, we select the vertices connected with most faces (i.e., the highest vertex degree) as the patch center. The faces around the center vertices are aggregated as patches, compressing mesh tokens at the face level. Our approach significantly reduces the length of the vanilla mesh sequence by around 75%, achieving the SoTA compression ratio across existing tokenization. Empowered with BPT, our mesh generative model can utilize millions of meshes with intricate details, significantly improving its performance and robustness. BPT facilitates a wide range of 3D applications. We demonstrate its effectiveness via conditional mesh generation on point clouds and images. Our model empowers even unprofessional users to produce meshes at the product-ready level. Its applicability spans a spectrum of practical domains of 3D content creation, revealing the dawn of a new era in 3D generation. Our contributions can be summarized as follows: • We introduce Blocked and Patchified Tokenization (BPT), a compressive yet effective tokenization with a state-of-the-art compression ratio of around 75%. • Empowered by BPT, we investigated the scaling of mesh data across diverse face configurations, revealing that incorporating extended data improves generation performance and robustness. • We build a mesh foundation model that supports conditional generation based on images and point clouds, enabling users to create product-ready 3D assets. Figure 3: The proposed Blocked and Patchified Tokenization (BPT). (a) We convert the coordinates from the Cartesian system to block-wise indexes. The coordinates are first separated equally into several blocks. Then, vertices inside each block are located with 1-dim indexes. (b) The nearby faces are aggregated as patches to compress the mesh sequence. Each patch center is set as the vertex connected with the most unvisited faces. Subsequently, other vertices within the patch are included in the subsequence to create a complete patch."
https://arxiv.org/html/2411.06471v1,Towards Voronoi Diagrams of Surface Patches,"Extraction of a high-fidelity 3D medial axis is a crucial operation in CAD. When dealing with a polygonal model as input, ensuring accuracy and tidiness becomes challenging due to discretization errors inherent in the mesh surface. Commonly, existing approaches yield medial-axis surfaces with various artifacts, including zigzag boundaries, bumpy surfaces, unwanted spikes, and non-smooth stitching curves. Considering that the surface of a CAD model can be easily decomposed into a collection of surface patches, its 3D medial axis can be extracted by computing the Voronoi diagram of these surface patches, where each surface patch serves as a generator. However, no solver currently exists for accurately computing such an extended Voronoi diagram. Under the assumption that each generator defines a linear distance field over a sufficiently small range, our approach operates by tetrahedralizing the region of interest and computing the medial axis within each tetrahedral element. Just as SurfaceVoronoi computes surface-based Voronoi diagrams by cutting a 3D prism with 3D planes (each plane encodes a linear field in a triangle), the key operation in this paper is to conduct the hyperplane cutting process in 4D, where each hyperplane encodes a linear field in a tetrahedron. In comparison with the state-of-the-art, our algorithm produces better outcomes. Furthermore, it can also be used to compute the offset surface.","Voronoi diagrams serve the purpose of partitioning a given space into subregions based on proximity. Beyond their direct applications in proximity queries and collision detection [1, 2], Voronoi diagrams find utility in a diverse range of fields, including surface reconstruction [3], robot motion planning [4], non-photorealistic rendering [5], surface simplification [6], mesh generation [7], and shape analysis [8], among others. Voronoi diagrams showcase numerous variants depending on specific domains, metrics, and generator types, with the most commonly employed version defined in Euclidean spaces using point generators. In digital geometry processing, a fundamental research task involves partitioning a 2-manifold surface into curved Voronoi cells based on geodesic distances. In this scenario, the 2-manifold surface serves as the domain, the geodesic distance functions as the metric, and the user-specified point set acts as the generators. A recent advancement by Xin et al. [9] introduces an extensible approach known as SurfaceVoronoi for computing Voronoi diagrams on surfaces and their variants. SurfaceVoronoi operates under the assumption that mesh triangles are small in size and that the triangle-wide geodesic distance field, provided by a single generator, can be considered linear. Leveraging this assumption, SurfaceVoronoi enables each generator to simultaneously propagate distances until all contributing generators for each triangle are identified. Ultimately, for each triangle, SurfaceVoronoi calculates the surface-restricted Voronoi structure by elevating each 2D linear field to a 3D plane and extracting the lower envelope of a roof-like structure. This paper explores the 3D Voronoi diagram of a set of non-intersecting surface patches. We aim to enhance SurfaceVoronoi to tackle this challenge, recognizing that the extension must contend with the increase in dimensions compared to surface-restricted Voronoi diagrams, which are inherently 2D. In practical applications, the Voronoi diagram of surface patches or even 3D objects proves significantly useful for understanding how objects interact or relate spatially. For instance, Zhao et al. [10] proposed using bisectors between two 3D objects to describe their topological relationships. In the realm of robotics and autonomous systems, the extended Voronoi diagram facilitates the rapid identification of safe paths to avoid collisions or obstacles between 3D objects [11]. While the resulting distance field from a single surface-patch generator can be arbitrarily complex in the entire \mathbb{R}^{3} space, it can be as simple as a linear function when confined to a small range. This allows us to encode the distance field within a small tetrahedral cell, contributed by a single generator, using a straightforward quadruple. In its nature, this representation signifies a 4D plane, with the first three dimensions denoting coordinates and the fourth dimension illustrating distance variation. Initially, we generate the initial 4D volume rooted at a tetrahedral element by sweeping the base tetrahedron along the fourth dimension. Our approach begins with the tetrahedralization of the space of interest. Similar to SurfaceVoronoi [9], the first stage involves propagating straight-line distances from the generators until no generator can offer a smaller distance for any tetrahedral element. Subsequently, we preserve the surviving generators and their corresponding linear distance fields for each tetrahedron. The second stage involves decomposing each tetrahedron into sub-domains through a sequence of 4D hyperplane cutting operations. Finally, the lower envelope of the 4D roof-like structure, when projected back into 3D, defines the decomposition configuration of the base tetrahedron. Given that the surface of a CAD model can be readily decomposed into a collection of simple patches, we innovatively apply the extended Voronoi diagram to compute medial-axis surfaces. Extensive experimental results demonstrate that our medial-axis extraction algorithm significantly outperforms the state-of-the-art in terms of accuracy and noise insensitivity. Furthermore, our algorithm can even be used to compute the offset surface. Our contributions are three-fold: • We extend SurfaceVoronoi to compute the Voronoi diagram of a collection of surface patches, addressing a challenging task in past research. • The fundamental operations are elevated from 3D to 4D, enabling the computation of the extended Voronoi diagram, confined within a tetrahedron, through a sequence of 4D hyperplane cutting. • We innovatively apply the new algorithm to compute medial-axis surfaces and demonstrate its superior performance. Additionally, we discuss more potential application scenarios."
https://arxiv.org/html/2411.06459v1,Learning Uniformly Distributed Embedding Clusters of Stylistic Skills for Physically Simulated Characters,"Learning natural and diverse behaviors from human motion datasets remains a significant challenge in physics-based character control. Existing conditional adversarial models often suffer from tight and biased embedding distributions where embeddings from the same motion are closely grouped in a small area and shorter motions occupy even less space. Our empirical observations indicate this limits the representational capacity and diversity under each skill. An ideal latent space should be maximally packed by all motion’s embedding clusters. Although methods that employ separate embedding space for each motion mitigate this limitation to some extent, introducing a hybrid discrete-continuous embedding space imposes a huge exploration burden on the high-level policy. To address the above limitations, we propose a versatile skill-conditioned controller that learns diverse skills with expressive variations. Our approach leverages the Neural Collapse phenomenon, a natural outcome of the classification-based encoder, to uniformly distributed cluster centers. We additionally propose a novel Embedding Expansion technique to form stylistic embedding clusters for diverse skills that are uniformly distributed on a hypersphere, maximizing the representational area occupied by each skill and minimizing unmapped regions. This maximally packed and uniformly distributed embedding space ensures that embeddings within the same cluster generate behaviors conforming to the characteristics of the corresponding motion clips, yet exhibiting noticeable variations within each cluster. Compared to existing methods, experimental results demonstrate that our controller not only generates high-quality, diverse motions covering the entire dataset but also achieves superior controllability, motion coverage, and diversity under each skill. Both qualitative and quantitative results confirm these traits, enabling our controller to be applied to a wide range of downstream tasks and serving as a cornerstone for diverse applications.<ccs2012> <concept> <concept_id>10010147.10010371.10010352</concept_id> <concept_desc>Computing methodologies Animation</concept_desc> <concept_significance>500</concept_significance> </concept> </ccs2012>[500]Computing methodologies Animation","Synthesizing natural and lifelike character behaviors from human motion datasets has emerged as a significant research focus [YSCL22, WGH22, TKG∗23, PGH∗22]. An ideal control model of motor skills should support the generation of life-like motions with a high degree of diversity, reflecting the varied expressions and unique behavioral styles exhibited by humans across different skills. Another highly desirable property is that it should offer reusability, serving as a cornerstone for a range of downstream applications. A well-qualified reusable model should learn diverse, varied, and realistic motor skills, as it will function as a low-level control model directed by high-level strategies to complete tasks. Insufficient diversity limits the synthesis of natural behaviors. Furthermore, it should be capable of performing desired motions in response to specified skill commands, to support interactive control secnerios. However, developing such a versatile, adaptable, and controllable motor skill model remains challenging for the current state-of-the-art generative controllers. Recent works have increasingly adopted Generative Adversarial Network [GPAM∗20] (GAN)-based control model [JGFP22, DCF∗23]. Some methods learn skill embeddings by projecting motions onto a unit hypersphere, which has been empirically shown to improve training stability, facilitate compact feature clustering, and ensure uniform class distribution [CHL05, XD18, WI20]. Compared to other methods that leverage explicit tracking rewards to precisely replicate each motion [ZZLH23, PALVdP18], GAN-based models can generate more diverse variations of a motion while also preserving its distributional behavioral characteristics. However, this advantage is impaired in current state-of-the-art conditional GAN-based control models due to the tight and biased embedding distribution on spherical latent space, as demonstrated in Figure 1(a), which shows a part of the region in latent space from previous work [TKG∗23] after dimensionality reduction by Principal Component Analysis (PCA). These models [TKG∗23, JGFP22] utilize segment-wise encoding to learn representations of reference motions, resulting in embeddings from the same motion being closely clustered on the surface of the spherical latent space. Notably, the spatial occupation of skill embeddings correlates with the length of the motion clips and shorter motions tend to occupy even less representational space. Our empirical observations indicate that this tight and biased clustering of skill embeddings restricts representational capacity, leading to reduced diversity for each skill. Furthermore, the embedding regions that remain unlearned could result in unnatural and meaningless movements. Both the restricted diversity of each skill and the existence of substantial unmapped regions adversely affect the performance of the controller. Methods like [DCF∗23] help alleviate the issue of limited diversity by constructing a discrete-continuous skill embedding space, allocating independent unit hyperspheres to each motion. However, as pointed out in [LTZ∗21], the introduced heterogeneity in this hybrid low-level space poses significant exploration challenges. The high-level policy struggles to navigate and explore appropriate discrete skill labels while simultaneously approximating suitable continuous skill embeddings under the predicted label. This issue is particularly pronounced in tasks requiring the operation of various heterogeneous skills, such as in the strike task [PGH∗22], hindering the synthesis of natural and fluid strategies. (a) CALM’s latent space (b) Our latent space Figure 1: The visualization of two latent spaces using PCA. (a) The tight and biased embedding distributions for each motion in CALM, with labels indicating the motion names and their respective lengths. The spatial occupation of each skill correlates with the length of the motion clip. (b) The learned skill embedding clusters in our model form a maximally packed, uniform, and length-agnostic distribution. We hypothesize that an ideal skill representation should contain clusters of skills uniformly distributed over a latent embedding space and collectively covering the entire space. We refer to it as a maximally packed and uniformly distributed space. This ensures equal representational area for each skill in the latent space while reducing the presence of unmapped areas. However, achieving such a distribution on a spherical surface is a variation of the Tammes problem [Tam30], which lacks straightforward solutions, necessitating a learning-based approach. Neural Collapse [PHD20, GGH21, HPD21] is an intriguing phenomenon observed in classification models, where the features of data points from the same class tend to converge toward their class mean, collapsing into a single point. Simultaneously, the class means themselves become equidistant and maximally-equiangular positioned in the feature space, thereby maximizing the inter-class separation and forming a simplex equiangular tight frame [LZH∗23, HPD21]. This phenomenon potentially facilitates the modeling of uniform distributions for feature means of each motion clip in high-dimensional spherical latent space. On the other hand, a naive implementation of neural collapse still fails to provide diversity within each motion, as the representation of each motion collapses to its feature mean for each skill. We propose an Embedding Expansion method to expand the areas occupied by each skill and maximally pack the skill clusters over the embedding space, maximizing the diversity potential under each skill. To this end, we develop a GAN-based skill-conditioned control policy that learns diverse skills with expressive variations from motion datasets. Through a classification-based encoder and embedding expansion, our model is compelled to learn all reference motions and construct embedding clusters for each motion. These skill-specific embedding clusters are uniformly distributed on a hypersphere with all clusters occupying equivalent areas. Skill embeddings within the same cluster generate behaviors that conform to the characteristics of the corresponding motion clips, while also exhibiting noticeable variations. Experiment results reveal that our controller is capable of generating diverse motions that cover the entire dataset, featuring high-quality and versatile variations. Compared to the existing methods, our method achieved superior controllability, motion coverage, and diversity under each skill. All these traits are further substantiated by its effectiveness in tackling complex downstream tasks and improving performance in interactive scenarios. Overall, our contributions to this paper are as follows. 1) We exploit a classification-based encoder to establish a uniform distribution prior across motion clips, creating distinctive stylistic skill cluster centers. 2) Combined with conditional generative learning, we apply the Embedding Expansion to form embedding clusters for all skills with great variations. 3) Experiment results reveal that the above steps are well synergized, and collectively promote a versatile, adaptable, and controllable policy, becoming a cornerstone for varied downstream applications."
https://arxiv.org/html/2411.06224v2,Advancing GPU IPC for Stiff Affine-Deformable Simulation,"Incremental Potential Contact (IPC) is a widely used, robust, and accurate method for simulating complex frictional contact behaviors. However, achieving high efficiency remains a major challenge, particularly as material stiffness increases, which leads to slower Preconditioned Conjugate Gradient (PCG) convergence, even with the state-of-the-art preconditioners. In this paper, we propose a fully GPU-optimized IPC simulation framework capable of handling materials across a wide range of stiffnesses, delivering consistent high performance and scalability with up to 10\times speedup over state-of-the-art GPU IPC methods. Our framework introduces three key innovations: 1) A novel connectivity-enhanced Multilevel Additive Schwarz (MAS) preconditioner on the GPU, designed to efficiently capture both stiff and soft elastodynamics and improve PCG convergence at a reduced preconditioning cost. 2) A C^{2}-continuous cubic energy with an analytic eigensystem for strain limiting, enabling more parallel-friendly simulations of stiff membranes, such as cloth, without membrane locking. 3) For extremely stiff behaviors where elastic waves are barely visible, we employ affine body dynamics (ABD) with a hash-based multi-layer reduction strategy for fast Hessian assembly and efficient affine-deformable coupling. We conduct extensive performance analyses and benchmark studies to compare our framework against state-of-the-art methods and alternative design choices. Our system consistently delivers the fastest performance across soft, stiff, and hybrid simulation scenarios, even in cases with high resolution, large deformations, and high-speed impacts. Our framework will be fully open-sourced upon acceptance.","Incremental Potential Contact (IPC) (Li et al., 2020a) is a cutting-edge elastodynamic contact simulation method widely used in computer graphics, computational mechanics, robotics, etc. Despite its robustness, accuracy, and differentiability in simulating complex frictional contact behaviors, IPC’s efficiency remains a significant bottleneck, limiting its full potential. Several variants have been proposed to address IPC’s efficiency issues, often at the expense of accuracy (Lan et al., 2021, 2023; Li et al., 2023; Lan et al., 2022b). To accelerate IPC without sacrificing accuracy, Huang et al. (2024) introduced GIPC with a GPU-friendly redesign of the numerical algorithms. This included replacing direct factorization with a Preconditioned Conjugate Gradient (PCG) solver and proposing a Gauss-Newton approximation of the barrier Hessian matrices with analytic eigensystems. While GIPC is effective, its efficiency deteriorates significantly as object stiffness increases. This is mainly due to the growing condition number of the global linear system, which requires more PCG iterations to solve. To improve the PCG convergence of stiff material, Wu et al. (2022) proposed the multilevel additive Schwarz (MAS) preconditioner (Wu et al., 2022). Their approach involves sorting the nodes based on Morton codes and building a hierarchy by grouping the nodes at each level. Despite its effectiveness, the lack of consideration for mesh connectivity during reordering leads to suboptimal domain hierarchy construction. This results in high construction costs, additional overhead for deformable simulations and challenges for GPU optimization. When simulating stiff elastic thin shells like cloth, another challenge arises, which is membrane locking. With linear triangle elements, the stiff membrane energy (Young’s modulus around 10MPa for cloth (Penava et al., 2014)) will often result in nonnegligible extra artificial bending resistance, forming sharp creases and plastic appearances in the simulation results (Figure 12 top). Simulating cloth with smaller stiffness can result in more realistic wrinkles, but it will suffer from over-elongation issues. To tackle this challenge, Li et al. (2021) propose to augment soft membrane energy with a barrier-based strain-limiting term to prevent cloth from over-stretching while avoiding membrane locking. This strategy enables realistic cloth simulation within the IPC framework, but the required exact strain limit satisfaction necessitates a backtracking-based line search filtering scheme, as the updated strain has a complicated relation to the step size, making analytic expressions unavailable. Additionally, numerical eigendecomposition is needed for computing a positive semi-definite approximation of the strain-limiting term’s Hessian matrix, which further complicates GPU optimization. For even stiffer problems where elastic waves are barely visible, objects can be treated as rigid (Ferguson et al., 2021) or stiff affine (Lan et al., 2022a) bodies in the IPC framework for a reduced number of degrees of freedom (DOF). But to accurately simulate contact behaviors, surface elements from the original input geometry are used, which also makes simulating rigid-deformable coupled scenarios convenient. Chen et al. (2022) introduced a unified Newton barrier method for stiff affine-deformable simulation, possibly with articulation constraints. However, although some components are GPU-accelerated, the primary simulation processes still execute on the CPU, leading to suboptimal performance. ZeMa (Du et al., 2024) is another GPU IPC framework for stiff affine-deformable simulation, with most processes parallelized on the GPU, except for the linear system, which is solved on the CPU using a direct solver. However, ZeMa lacks a well-optimized contact Hessian assembly algorithm, as it accumulates the 12\times 12 dense contact Hessian matrices to the affine body DOFs atomically, where conflicting operations can significantly impede the performance, especially when there are a large number of contacts. Moreover, direct solvers often fall short in large-scale simulations. In summary, there are still plenty of rooms for optimizing linear solver preconditioners, strain limiting, global Hessian matrix assembly, etc., for realizing a highly GPU-optimized IPC framework that can efficiently simulate large-scale affine-deformable coupled scenarios. In this paper, we propose such a framework, achieving up to 10\times speedup compared to GIPC via the following 3 major innovations: • A novel connectivity-enhanced MAS preconditioner on the GPU that achieves improved PCG convergence at a lower precomputation and per-iteration cost (section 4). Our preconditioner consistently performs effective and well-structured aggregations, which supports smaller blocksizes and further GPU optimizations based on warp reduction. • A C^{2}-continuous cubic strain-limiting energy with an analytic eigensystem, enabling realistic cloth simulation without membrane locking (section 5). As numerical eigendecomposition and line search filtering for the feasibility of the strain limits are not needed, our model supports highly GPU-parallelized computations. • A hash-based multi-layer reduction strategy for fast Hessian matrix assembly (section 6). Our strategy significantly reduces the number of numerical operations, and it enables the development of a memory-efficient symmetric blockwise sparse matrix-vector multiplication method to further boost PCG performance. In section 7, we perform extensive and rigorous performance analyses and benchmark studies to validate our framework and compare it to state-of-the-art GPU IPC systems and alternative design choices that may seem reasonable but suffer from suboptimal performance in practice. Our framework exhibits the fastest performance in soft, stiff, and hybrid simulation scenarios, even with high resolution, extreme deformation, and high-speed impacts. Our system will be fully open-sourced upon acceptance."
https://arxiv.org/html/2411.07232v2,Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models,"Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models’ attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed ”Additing Affordance Benchmark” for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics. Our code and data will be available at: https://research.nvidia.com/labs/par/addit/","Adding objects to images based on textual instructions is a challenging task in image editing, with numerous applications in computer graphics, content creation and synthetic data generation. A creator may want to use text-to-image models to iteratively build a complex visual scene, while autonomous driving researchers may wish to draw pedestrians in new scenarios for training their car-perception system. Despite considerable recent research efforts on text-based editing, this particular task remains a challenge . When adding objects, one needs to preserve the appearance and structure of the original scene as closely as possible, while inserting the novel objects in a way that appears natural. To do so, one must first understand affordance—the deep semantic knowledge of how people and objects interact, in order to position an object in a reasonable location. For brevity, we call this task Image Additing. Several studies (Hertz et al., 2022; Meng et al., 2022) tried addressing this task by leveraging modern text-to-image diffusion models. This is a natural choice since these models embody substantial knowledge about arrangements of objects in scenes and support open-world conditioning on text. While these methods perform well for various editing tasks, their success rate for adding objects is disappointingly low, failing to align with both the source image and the text prompt. In response, another set of methods took a more direct learning approach (Brooks et al., 2023; Zhang et al., 2023; Canberk et al., 2024). They trained deep models on large image editing datasets, pairing images with and without an object to add. However, these often struggle with generalization beyond their training data, falling short of the general nature of the original diffusion model itself. This typically manifests as a failure to insert the new object, the creation of visual artifacts, or more commonly – failing to insert the object in the correct place, i.e. struggling with affordances. Indeed, we remain far from achieving open-world object insertions from text instructions. Here we describe an open-world, training-free method that can successfully leverage the knowledge stored in text-to-image foundation models, to naturally add objects into images. As a guiding principle, we propose that addressing the affordance challenge requires methods to carefully balance between the context of the existing scene and the instructions provided in the prompt. We achieve this by: first, extending the multi-modal attention mechanism (Esser et al., 2024) of recent T2I diffusion models to also consider tokens from a source image; and second, controlling the influence of each multi-modal attention component: the source image, the target image and the text prompt. A main contribution of this paper is a mechanism to balance these three sources of attention during generation. We also apply a structure transfer step and introduce a novel subject-guided latent blending mechanism to preserve the fine details of the source image while enabling necessary adjustments, such as shadows or reflections. Our full pipeline is shown at fig. 2. We name our method Add-it. Image Additing methods typically face three main failure modes: neglect, appearance, and affordance. While current CLIP-based evaluation protocols can partially assess neglect and appearance, there is a lack of reliable methods for evaluating affordance. To address this gap, we introduce the “Additing Affordance Benchmark,” where we manually annotate suitable areas for object insertion in images and propose a new protocol specifically designed to evaluate the plausibility of object placement. Additionally, we introduce a metric to capture object neglect. Add-it outperforms all baselines, improving affordance from 47% to 83%. We also evaluate our method on an existing benchmark (Sheynin et al., 2023) with real images, as well as our newly proposed Additing Benchmark for generated images. Add-it consistently surpasses previous methods, as reflected by CLIP-based metrics, our object inclusion metric, and human preference, where our method is favored in over 80% of cases, even against methods specifically trained for this task. Our contributions are as follows: (i) We propose a training-free method that achieves state-of-the-art results on the task of object insertion, significantly outperforming previous methods, including supervised ones trained for this task. (ii) We analyze the components of attention in a modern diffusion model and introduce a novel mechanism to control their contribution, along with novel Subject Guided Latent Blending and a noise structure transfer. (iii) We introduce an affordance benchmark and a new evaluation protocol to assess the plausibility of object insertion, addressing a critical gap in current Image Additing evaluation methods."
https://arxiv.org/html/2411.07135v1,Edify 3D: Scalable High-Quality 3D Asset Generation,"We introduce Edify 3D, an advanced solution designed for high-quality 3D asset generation. Our method first synthesizes RGB and surface normal images of the described object at multiple viewpoints using a diffusion model. The multi-view observations are then used to reconstruct the shape, texture, and PBR materials of the object. Our method can generate high-quality 3D assets with detailed geometry, clean shape topologies, high-resolution textures, and materials within 2 minutes of runtime.","The creation of detailed digital 3D assets is essential for developing scenes, characters, and environments across various digital domains. This capability is invaluable to industries such as video game design, extended reality, film production, and simulation. For 3D content to be production-ready, it must meet industry standards, including precise mesh structures, high-resolution textures, and material maps. Consequently, producing such high-quality 3D content is often an exceedingly complex and time-intensive process. As demand for 3D digital experiences grows, the need for efficient, scalable solutions in 3D asset creation becomes increasingly crucial. Recently, many research works have investigated into training AI models for 3D asset generation (Lin et al., 2023). A significant challenge, however, is the limited availability of 3D assets suitable for model training. Creating 3D content requires specialized skills and expertise, making such assets much scarcer than other visual media like images and videos. This scarcity raises a key research question of how to design scalable models to generate high-quality 3D assets from such data efficiently. Edify 3D is an advanced solution designed for high-quality 3D asset generation, addressing the above challenges while meeting industry standards. Our model generates high-quality 3D assets in under 2 minutes, providing detailed geometry, clean shape topologies, organized UV maps, textures up to 4K resolution, and physically-based rendering (PBR) materials. Compared to other text-to-3D approaches, Edify 3D consistently produces superior 3D shapes and textures, with notable improvements in both efficiency and scalability. This technical report provides a detailed description of Edify 3D. Figure 2: Pipeline of Edify 3D. Given a text description, a multi-view diffusion model synthesizes the RGB appearance of the described object. The generated multi-view RGB images are then used as a condition to synthesize surface normals using a multi-view ControlNet (Zhang et al., 2023). Next, a reconstruction model takes the multi-view RGB and normal images as input and predicts the neural 3D representation using a set of latent tokens. This is followed by isosurface extraction and subsequent mesh post-processing to obtain the mesh geometry. An upscaling ControlNet is used to increase the texture resolution, conditioning on mesh rasterizations to generate high-resolution multi-view RGB images, which are then back-projected onto the texture map. Core capabilities. Edify 3D features the following capabilities: • Text-to-3D generation. Given an input text description, Edify 3D generates a digital 3D asset with the aforementioned properties. • Image-to-3D generation. Edify 3D can also create a 3D asset from a reference image of the object, automatically identifying the foreground object in the image. Model design. The core technology of Edify 3D relies on two types of neural networks: diffusion models (Song and Ermon, 2019; Ho et al., 2020) and Transformers (Vaswani et al., 2017). Both architectures have demonstrated great scalability and success in improving generation quality as more training data becomes available. Following Li et al. (2024), we train the following models: • Multi-view diffusion models. We train multiple diffusion models to synthesize the RGB appearance and surface normals of an object from multiple viewpoints (Shi et al., 2023b). The input can be a text prompt, a reference image, or both. • Reconstruction model. Using the synthesized multi-view RGB and surface normal images, a reconstruction model predicts the geometry, texture, and materials of the 3D shape. We employ a Transformer-based model (Hong et al., 2023) to predict a neural representation of the 3D object as latent tokens, followed by isosurface extraction and mesh processing. The final output of Edify 3D is a 3D asset that includes the mesh geometry, texture map, and material map. Fig. 2 illustrates the overall pipeline of Edify 3D. In this report, we provide: • A detailed discussion of the design choices in the Edify 3D pipeline. • An analysis of the scaling behaviors of model components and properties. • An application of Edify 3D to scalable 3D scene generation from input text prompts."
https://arxiv.org/html/2411.06244v1,Grasping Object: Challenges and Innovations in Robotics and Virtual Reality,"In real-life, grasping is one of the fundamental and effective forms of interaction when manipulating objects. This holds true in the physical and virtual world; however, unlike the physical world, virtual reality (VR) is grasped in a complex formulation that includes graphics, physics, and perception. In virtual reality, the user’s immersion level depends on realistic haptic feedback and high-quality graphics, which are computationally demanding and hard to achieve in real-time. Current solutions fail to produce plausible visuals and haptic feedback when simulation grasping in VR with a variety of targeted object dynamics. In this paper, we review the existing techniques for grasping in VR and robotics and indicate the main challenges that grasping faces in both domains. We aim to explore and understand the complexity of hand-grasping objects with different dynamics and inspire various ideas to improve and come up with potential solutions suitable for virtual reality applications.","Over the past decades, with the advancement in virtual reality (VR) devices and Human-computer interaction (HCI) studies, the need for simulation of human behaviour with realistic interaction has become vital for many applications (see Fig. 1), such as industrial training, medical and surgical simulation, and rehabilitation [1, 2, 3]. High-fidelity graphics in such virtual environments have been used to provide users with a relatively immersive virtual experience [4]. However, obtaining a fully immersed experience requires realistic interaction with objects within the environment, where forces and masses of the objects can be felt during the interaction [5]. Haptic devices for both fingertips and/or the whole hand enable users to feel and manipulate the 3D objects and explore the virtual environment through a kinesthetic and cutaneous perception [6]. When the user interacts with an object, grasping is one of the main intuitive action behaviours. Although grasping behaviour is natural, human hands can grab a variety of objects with different shapes, weights, and frictions. Grasping in the virtual environment is a challenging task, as an ideal grasping action must take into account the geometry and dynamic characteristics of the virtual object [7]. Despite the recent achievements in grasping techniques in VR, which have led to the emergence of some technological devices such as glove-based devices [8] and controller-based devices [9, 1]. More technical methods need to be explored in order to ensure stable, controllable grasping in virtual reality. The grasping techniques in VR differ broadly in complexity according to the virtual object’s properties, such as mass, size, and materials. Moreover, the object’s stiffness plays a significant role in making the interaction with the object complex to simulate [10]. Most existing techniques focus on reducing the complexity of the grasping targets by simulating rigid bodies or objects with relatively simple and similar properties [11]. However, unlike rigid bodies, deformable bodies have high dynamic force attributes when the fingertips make contact with them. Therefore, obtaining stable grasping of deformable bodies while achieving realistic visuals and haptic feedback in virtual reality remains an open problem. Fig 1. Realistic interaction with virtual objects within an immersed environment is crucial for many applications, such as industrial and medical training, entertainment and virtual social interaction [2]. In this paper, we aim to review the existing methods for grasping from the perspectives of haptics and visuals, including the properties of the target objects. Further, to focus on the techniques of grasping in VR, we also generally introduce robotic grasping methods to compare the grasping methods in virtual environments with real-life robotics. Through this paper, we hope to summarise the main existing challenges in grasping simulation and improve the quality of VR grasping, potentially creating a direction for a new research agenda."
https://arxiv.org/html/2411.06067v1,AI-Driven Stylization of 3D Environments,"In this our system, we discuss methods to stylize a scene of 3D primitive objects into a higher fidelity 3D scene using novel 3D representations like NeRFs and 3D Gaussian Splatting. Our approach leverages existing image stylization systems and image-to-3D generative models to create a pipeline that iterativly stylizes and composites 3D objects into scenes. We show our results on adding generated objects into a scene and discuss limitations.","The rapid evolution of 3D scene generation technologies has substantially enhanced the feasibility and quality of generating and manipulating three-dimensional environments, particularly through the advent of Neural Radiance Fields (NeRFs) and other generative models. These advancements have opened new possibilities for virtual reality (VR), augmented reality (AR), and other applications requiring rich, immersive, and dynamically editable 3D content. However, despite significant progress, the challenge remains in the accessibility and ease of use of these technologies for users without extensive technical expertise in 3D design. Recognizing this gap, we introduce a pipeline designed to empower individuals with little to no background in 3D design to furnish and restyle their living spaces effectively. Our approach leverages a user-friendly interface where users can draw basic primitives within a scanned room and input stylistic preferences through simple text prompts. The system then automates the conversion of these inputs into a fully furnished and stylized 3D room model, viewable in real-time. The major contribution of our work lies in integrating systems such as InstructPix2Pix for image stylization and SIGNeRF for seamless object integration within NeRFs, tailored to simplify the design process. This pipeline not only democratizes 3D interior design but also enhances the accessibility and usability of sophisticated 3D modeling tools. By leveraging intuitive user interaction and automation, our method opens up new avenues for personal creativity and practical application in home design, offering significant benefits to users unfamiliar with traditional 3D modeling tools. This paper outlines our methodology, discusses the implementation of our pipeline, and presents results that validate the effectiveness of our approach in creating immersive and aesthetically pleasing 3D environments tailored to user specifications. Our contribution marks a step forward in making advanced 3D scene generation accessible to a broader audience."
https://arxiv.org/html/2411.06019v1,": An “Optimizing-Sparsifying"" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting","3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view synthesis, leveraging continuous aggregations of Gaussian functions to model scene geometry. However, 3DGS suffers from substantial memory requirements to store the multitude of Gaussians, hindering its practicality. To address this challenge, we introduce GaussianSpa, an optimization-based simplification framework for compact and high-quality 3DGS. Specifically, we formulate the simplification as an optimization problem associated with the 3DGS training. Correspondingly, we propose an efficient “optimizing-sparsifying” solution that alternately solves two independent sub-problems, gradually imposing strong sparsity onto the Gaussians in the training process. Our comprehensive evaluations on various datasets show the superiority of GaussianSpa over existing state-of-the-art approaches. Notably, GaussianSpa achieves an average PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with 10\times fewer Gaussians compared to the vanilla 3DGS. Our project page is available at https://gaussianspa.github.io/.","Novel view synthesis has become a pivotal area in computer vision and graphics, driving advancements in applications such as virtual reality, augmented reality, and immersive media experiences [21]. NeRF [35] has recently gained prominence in this domain because it can generate high-quality, photorealistic images from sparse input views by representing scenes as continuous volumetric functions based on neural networks. However, NeRF often requires extensive computational resources and long training times, making it less practical for real-time applications and large-scale reconstructions. 3D Gaussian Splatting (3DGS) [25] has emerged as a powerful alternative, leveraging continuous aggregations of Gaussian functions to model scene geometry and appearance. Unlike NeRF, which relies on neural networks to approximate volumetric radiance fields, 3DGS directly represents scenes using a collection of Gaussians. This approach excels in capturing details and smooth transitions, offering significant advantages in training and rendering speed. 3DGS achieves superior visual fidelity [28] compared to NeRF while reducing computational overhead, making it more suitable for interactive applications that demand quality and performance. Despite its strengths, 3DGS suffers from significant memory requirements that hinder its practicality. The main issue is the massive memory consumption associated with storing a large number of Gaussians that are needed to represent complex scenes. Each Gaussian occupies memory space for its parameters, including position, covariance, and color attributes. In densely sampled scenes, the sheer volume of Gaussians leads to memory usage that exceeds the capacity of typical hardware, making it challenging to handle higher-resolution scenes and limiting its applicability in resource-constrained environments. Existing works, e.g., Mini-Splatting [20], LightGaussian [19], LP-3DGS [54], EfficientGS [31], and RadSplat [40], have predominantly focused on mitigating this issue by removing a certain number of Gaussians. Techniques such as pruning and sampling aim to discard unimportant Gaussians based on hand-crafted criteria such as opacity [25, 53], importance score (hit count) [20, 19, 40], dominant primitives [31], and binary mask [54, 29]. However, these criteria are generally utilized to determine the importance of Gaussian points from a single heuristic perspective, limiting robustness in dynamic scenes or under varying lighting conditions. Moreover, the sudden one-shot removal may cause permanent loss of Gaussians that are crucial to visual synthesis, making it challenging to recover the original performance after even long-term training, as shown in Figure 2. Consequently, while these methods can alleviate memory and storage burdens to some extent, they often lead to sub-optimal rendering outcomes with loss of details and visual artifacts, thereby compromising the quality of the synthesized views. In this paper, we present an optimization-based simplification framework, GaussianSpa, for compact and high-quality Gaussian Splatting. In the proposed framework, we formulate 3DGS simplification as a constrained optimization problem under a target number of Gaussians. Then, we propose an efficient “optimizing-sparsifying"" solution for the formulated problem by splitting it into two simple sub-problems that are alternately solved in the “optimizing"" step and the “sparsifying"" step. Instead of permanently removing a certain number of Gaussians, GaussianSpa incorporates the “optimizing-sparsifying” algorithm into the training process, gradually imposing a substantial sparse property onto the trained Gaussians. Hence, our GaussianSpa can simultaneously enjoy maximum information preservation from the original Gaussians and a desired number of reduced Gaussians, providing compact 3DGS models with high-quality rendering. Overall, our contributions can be summarized as follows: • We propose a general 3DGS simplification framework that formulates the simplification objective as an optimization problem and solves it in the 3DGS training process. In solving the formulated optimization problem, our proposed framework gradually restricts Gaussians to the target sparsity constraint without explicitly removing a specific number of points. Hence, GaussianSpa can maximally maintain and smoothly transfer the information to the sparse Gaussians from the original model. • We propose an efficient “optimizing-sparsifying"" solution for the formulated problem, which can be integrated into the 3DGS training with negligible costs, separately solving two sub-problems. In the “optimizing"" step, we optimize the original loss function attached by a regularization with gradient descent. In the “sparsifying"" step, we analytically project the auxiliary Gaussians onto the constrained sparse space. • We comprehensively evaluate GaussianSpa through extensive experiments on various complex scenes, demonstrating improved rendering quality compared to existing approaches. Particularly, with as high as 10\times fewer number of Gaussians than the vanilla 3DGS, GaussianSpa achieves an average 0.4 dB improvement on the Mip-NeRF 360 [4] and Tanks&Temples [27] datasets, 0.9 dB on Deep Blending [23] dataset. Furthermore, we conduct various visual quality assessments, showing that GaussianSpa exhibits high-quality rendering of details and sparse 3D Gaussian views."
https://arxiv.org/html/2411.05823v1,FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models,"Recently, there is a growing interest in creating computer-aided design (CAD) models based on user intent, known as controllable CAD generation. Existing work offers limited controllability and needs separate models for different types of control, reducing efficiency and practicality. To achieve controllable generation across all CAD construction hierarchies, such as sketch-extrusion, extrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by fine-tuning large language models (LLMs). First, to enhance comprehension by LLMs, we represent a CAD model as a structured text by abstracting each hierarchy as a sequence of text tokens. Second, to address various controllable generation tasks in a unified model, we introduce a hierarchy-aware masking strategy. Specifically, during training, we mask a hierarchy-aware field in the CAD text with a mask token. This field, composed of a sequence of tokens, can be set flexibly to represent various hierarchies. Subsequently, we ask LLMs to predict this masked field. During inference, the user intent is converted into a CAD text with a mask token replacing the part the user wants to modify, which is then fed into FlexCAD to generate new CAD models. Comprehensive experiments on public dataset demonstrate the effectiveness of FlexCAD in both generation quality and controllability. Code will be available at https://github.com/microsoft/CADGeneration/FlexCAD.","A computer-aided design (CAD) model is a digital representation of a 2D or 3D object. It has been widely used across numerous industries, including architecture, product design and manufacturing, facilitating precise, efficient, and innovative development Ganin et al. (2021); Khan et al. (2024). In commonly used CAD tools like SolidWorks and AutoCAD, sketch-and-extrude modeling (SEM) is prevalent. This involves drawing 2D sketches and then extruding them into 3D shapes. Compared to other representations, such as Constructive Solid Geometry (CSG) Yu et al. (2024), B-rep Xu et al. (2024), or voxel Li et al. (2023) and point cloud Khan et al. (2024)-based formats, SEM, incorporating multiple CAD construction hierarchies including sketch-extrusion, extrusion, sketch, face, loop and curve (see Fig. 3(a)), directly illustrates the drawing process of a 3D object. This allows for easy editing and reuse of CAD models, which is essential in CAD development. Recently, there is an increasing interest in developing generative models to automatically produce SEM of a CAD model111In the following, we will use CAD model to refer to SEM of a CAD model for brevity.. Specifically, DeepCAD Wu et al. (2021) focuses on uncontrollable generation, where a CAD model is generated from a randomly sampled vector. However, providing controllability, i.e., generating CAD models according to user intent, is crucial for the practical application of generative models. To address this, SkexGen Xu et al. (2022) and Hnc-cad Xu et al. (2023) implement disentangled codebooks to offer some levels of control. As each codebook encodes a particular construction hierarchy, their controllability is quite restricted. For instance, SkexGen does not allow selecting a specific sketch for modifications when a CAD model comprises multiple sketches, nor can it handle finer-grained hierarchies such as faces and loops. Hnc-cad lacks control over the topology and geometry of curves. In summary, existing methods face challenges in providing adequate controllability across all CAD construction hierarchies. Additionally, they require separate models to deliver different types of control, which is inefficient and less practical. The emergence of large language models (LLMs) offers insights for addressing these challenges. First, LLMs have exhibited remarkable success in handling diverse user queries with a single and unified model Chung et al. (2024). This phenomenon not only occurs in natural language tasks but also extends to other areas with domain-specific fine-tuning, such as human motion generation Jiang et al. (2024) and crystal material synthesis Gruver et al. (2024). Second, LLMs might have acquired CAD-related knowledge during the pre-training by learning CAD-specific codes, such as JSCAD codes Makatura et al. (2023). Third, prior to the rise of LLMs, small transformer-based models were explored for tasks like uncontrollable generation and image-to-sketch translation in the 2D sketch domain Ganin et al. (2021), showcasing the possibility of LLMs from a different perspective. In this work, we introduce FlexCAD, a unified model designed for controllable CAD generation across all hierarchies by fine-tuning LLMs. As shown in Fig. 1, FlexCAD receives the original CAD model along with the part the user wants to modify (highlighted in blue). Here, users can specify the part in any hierarchy. FlexCAD then generates multiple new CAD models, altering only the selected part. To achieve these abilities, first, FlexCAD translates a CAD model into a concise and structured text (see Fig. 3). Specifically, in each sketch, the curve type (e.g., a line) is directly represented as textual tokens. The numerical data indicating geometry (e.g., point coordinates in a line) is converted into decimal integers and then into textual tokens. A special token is added to mark the end of each hierarchy. Tokens from the finer-level hierarchy are concatenated to form the representation for the coarser-level hierarchy. We use a similar way to convert each extrusion. Consequently, unlike the one-hot representation used in Xu et al. (2022), FlexCAD provides a concise text representation of a CAD model, facilitating easier processing and understanding by LLMs. Second, FlexCAD introduces a hierarchy-aware masking strategy to enable fine-tuning LLMs for various controllable CAD generation tasks (see Fig. 2). During training, we replace a hierarchy-aware field, which contains a sequence of tokens in the CAD text, with a mask token. This field can be set adaptably to reflect various hierarchies. Then, we ask LLMs to predict the masked field. To achieve this, we design prompt templates for all hierarchies, where the mask tokens are tailored to match the corresponding hierarchies. These templates are uniformly sampled at each epoch during the fine-tuning of LLMs. In this way, we ensure that the generation tasks for all hierarchies are learned in a single and unified model. Besides, unlike Xu et al. (2022; 2023) that requires multi-stage training, FlexCAD achieves end-to-end training. During inference, a CAD model is represented as a CAD text with a mask token replacing the part the user wants to change. The masked CAD text is fed into the fine-tuned LLMs to get predictions. After infilling the masked text with these predictions, FlexCAD produces CAD texts that can be rendered into new CAD models. Overall, our contributions are: • We propose FlexCAD, a unified and versatile model for controllable CAD generation across all hierarchies, including sketch-extrusion, extrusion, sketch, face, loop and curve. • To the best of our knowledge, FlexCAD is the first to leverage LLMs for controllable CAD generation. It converts a CAD model into a brief, structured text and employs hierarchy-aware masking to fine-tune LLMs for various controllable CAD generation tasks. • We conduct extensive experiments on public datasets. Despite its simplicity, FlexCAD greatly improves generation quality and controllability, showing its effectiveness on the tasks presented in this work and indicating potential for other CAD generation scenarios."
