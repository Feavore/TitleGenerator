Title,Abstract,Introduction
Sparse Decomposition of Graph Neural Networks,"Graph Neural Networks (GNN) exhibit superior performance in graph representation learning, but their inference cost can be high, due to an aggregation operation that can require a memory fetch for a very large number of nodes. This inference cost is the major obstacle to deploying GNN models with online prediction to reflect the potentially dynamic node features. To address this, we propose an approach to reduce the number of nodes that are included during aggregation. We achieve this through a sparse decomposition, learning to approximate node representations using a weighted sum of linearly transformed features of a carefully selected subset of nodes within the extended neighbourhood. The approach achieves linear complexity with respect to the average node degree and the number of layers in the graph neural network. We introduce an algorithm to compute the optimal parameters for the sparse decomposition, ensuring an accurate approximation of the original GNN model, and present effective strategies to reduce the training time and improve the learning process. We demonstrate via extensive experiments that our method outperforms other baselines designed for inference speedup, achieving significant accuracy gains with comparable inference times for both node classification and spatio-temporal forecasting tasks.","Figure 1: The pipeline overview for SDGNN framework (bottom pipeline). To compute GNN embedding efficiently, we use a transformation function to adapt node features and introduce sparse vectors associated with each node to gather information from critical neighbours. The parameters in the transformation function and the sparse vectors are determined by optimization to approximate the target GNN embeddings. Graph neural networks (GNN) have demonstrated impressive performance for graph representation learning (Hamilton et al., 2017; Veličković et al., 2018; Qu et al., 2019; Rampášek et al., 2022). Although there are numerous designs for GNN models, the essential idea is to represent each node based on its features and its neighbourhood (Wu et al., 2020; Zhou et al., 2020). The procedure of aggregating features from neighbour nodes is empirically and theoretically effective (Xu et al., 2019) in representing the graph structures and blending the features of the nodes. However, deploying GNN models to process large graphs is challenging since collecting information from the neighbour nodes and computing the aggregation is extremely time-consuming (Zhang et al., 2021; Tian et al., 2023; Wu et al., 2023; Liu et al., 2024). In this work, we tackle the efficient inference problem for GNN models in the online prediction setting (Crankshaw, 2019). Specifically, we need to compute the representations of a few arbitrary nodes. The main advantage is that the prediction can reflect potential dynamic features111The features of a sample may vary over time, e.g., dynamical features from sensors (Dawson et al., 2016), dynamic features in the recommendation systems (Chu & Park, 2009). of the input. The computational complexity is dominated by the number of receptive nodes, which rapidly increases as the number of layers in the model grows, for most message-passing-based and graph-transformer-based GNNs (Zeng et al., 2020; Min et al., 2022). Our goal is to reduce the inference time to linear complexity with respect to the number of layers and the average node degree. Recently, several studies have attempted to address this problem by combining the performance of GNN and the efficiency of MLPs (Zhang et al., 2021; Hu et al., 2021; Tian et al., 2023; Wang et al., 2023; Wu et al., 2023; Liu et al., 2024; Tian et al., 2024; Winter et al., 2024; Wu et al., 2024). Knowledge distillation (Hinton et al., 2015) and feature/label smoothing are used to construct effective MLP models to eliminate the cumbersome neighbour collection and aggregation procedure. Although efficient, these methods have a fundamental limitation: the features gathered at each node are assumed to contain sufficient information to predict the node label accurately. However, to achieve their full potential, especially when features can change at inference time, GNN models should take into account the features from neighbourhood nodes and the graph structure (Battaglia et al., 2018; Pei et al., 2020). Therefore, we ask the question: given any graph neural network model that relies on both the graph structure and the features of the neighbourhood, can we infer the representation of a node in linear time? Present work. We propose sparse decomposition for graph neural networks (SDGNN), an approximation to the original GNN models that can infer node representations efficiently and effectively. The SDGNN consists of a feature transformation function and sparse weight vectors for nodes in the graph. The representation of each node is then a weighted sum of the transformed features from a small set of receptive nodes. The sparsity of the weight vectors guarantees low inference complexity. The learnable feature transformation function and the sparse weight vectors grant the SDGNN flexibility to approximate a wide range of targeted GNN models. To find the optimal parameters in SDGNN, we formulate the approximation task as an optimization problem and propose a scalable and efficient solution that iterates between the learning of the transformation function and the optimization of the sparse weight vectors. We verify the approximation power of SDGNN and the scalability of our algorithm on seven node classification datasets and demonstrate how SDGNN can be effectively applied under the online prediction setting with two spatio-temporal forecasting datasets. SDGNN consistently outperforms recent state-of-the-art models designed for GNN inference speedup."
Adversarial Environment Design via Regret-Guided Diffusion Models,"Training agents that are robust to environmental changes remains a significant challenge in deep reinforcement learning (RL). Unsupervised environment design (UED) has recently emerged to address this issue by generating a set of training environments tailored to the agent’s capabilities. While prior works demonstrate that UED has the potential to learn a robust policy, their performance is constrained by the capabilities of the environment generation. To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). The proposed method guides the diffusion-based environment generator with the regret of the agent to produce environments that the agent finds challenging but conducive to further improvement. By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy. Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments. Project page: https://github.com/rllab-snu.github.io/projects/ADD","Deep reinforcement learning (RL) has achieved great success in various challenging domains, such as Atari [1], GO [2], and real-world robotics tasks [3, 4]. Despite the progress, the deep RL agent struggles with the generalization problem; it often fails in unseen environments even with a small difference from the training environment distribution [5, 6]. To train well-generalizing policies, various prior works have used domain randomization (DR) [7, 8, 9], which provides RL agents with randomly generated environments. While DR enhances the diversity of the training environments, it requires a large number of trials to generate meaningful structures in high-dimensional domains. Curriculum reinforcement learning [10, 11] has been demonstrated to address these issues by providing instructive sequences of environments. Since manually designing an effective curriculum for complicated tasks is challenging, prior works [12, 13] focus on generating curricula that consider the current agent’s capabilities. Recently, unsupervised environment design (UED, [14]) has emerged as a scalable approach, notable for its advantage of requiring no prior knowledge. UED algorithms alternate between training the policy and designing training environments that maximize the regret of the agent. This closed-loop framework ensures the agent learns a minimax regret policy [15], assuming that the two-player game between the agent and the environment generator reaches the Nash equilibrium. There are two main approaches for UED: 1) learning-based methods, which employ an environment generator trained via reinforcement learning, and 2) replay-based methods, which selectively replay among previously generated environments. The learning-based methods [14, 16, 17] utilize an adaptive generator that controls the parameters that fully define the environment configuration. The generator receives a regret of the agent as a reward and is trained via reinforcement learning to produce environments that maximize the regret. While the learning-based methods can directly generate meaningful environments, training the generator with RL is unstable due to the moving manifold [16]. Additionally, we observe that the RL-based generator has limited environment coverage, which limits the generalization capability of the trained agent. In contrast, the replay-based methods [18, 19, 20] employ a random generator and select environments to revisit among previously generated environments. Since the random generator can produce diverse environments without additional training, they outperform the learning-based methods in zero-shot generalization tasks [20]. However, the replay-based methods are sample inefficient as they require additional episodes to evaluate the regret on the randomly generated environments. In this work, we propose a sample-efficient and robust UED algorithm by leveraging the strong representation power of diffusion models [21]. First, to make UED suitable for using a diffusion model as a generator, we introduce soft UED, which augments the regret objective of UED with an entropy regularization term, as done in maximum entropy RL [22]. By incorporating the entropy term, we can ensure the diversity of the generated environments. Then, we present adversarial environment design via regret-guided diffusion models (ADD), which guides a diffusion-based environment generator with the regret of the agent to produce environments that are conducive to the performance improvement of the agent. Enabling this regret guidance requires the gradient of the regret with respect to the environment parameter. However, since the true value of the regret is intractable and the regret estimation methods used in prior works on UED are not differentiable, a new form of regret estimation method is needed. To this end, we propose a novel method that enables the estimation of the regret in a differentiable form by utilizing an environment critic, which predicts a return distribution of the current policy on the given environment. This enables us to effectively integrate diffusion models within the UED framework, significantly enhancing the environment generation capability. Since the regret-guided diffusion does not require an additional training of the environment generator, we can preserve the ability to cover the high-dimensional environment domain as the random generator of the replay-based method. Moreover, ADD can directly generate meaningful environments via regret-guided sampling as the learning-based methods. By doing so, ADD effectively combines the strengths of previous UED methods while addressing some of their limitations. Additionally, unlike other UED methods, ADD allows us to control the difficulty levels of the environments it generates by guiding the generator with the probability of achieving a specific return. It enables the reuse of the learned generator in various applications, such as generating benchmarks. We conduct extensive experiments across challenging tasks commonly used in UED research: partially observable maze navigation and 2D bipedal locomotion over challenging terrain. Experimental results show that ADD achieves higher zero-shot generalization performance in unseen environments compared to the baselines. Furthermore, our analysis on the generated environments demonstrates that ADD produces an instructive curriculum with varying complexity while covering a large environment configuration space. As a result, it is shown that the proposed method successfully generates adversarial environments and facilitates the agent to learn a policy with solid generalization capabilities."
Super Gradient Descent: Global Optimization requires Global Gradient,"Global minimization is a fundamental challenge in optimization, especially in machine learning, where finding the global minimum of a function directly impacts model performance and convergence. This report introduces a novel optimization method that we called Super Gradient Descent, designed specifically for one-dimensional functions, guaranteeing convergence to the global minimum for any k𝑘kitalic_k-Lipschitz function defined on a closed interval [a,b]𝑎𝑏[a,b][ italic_a , italic_b ]. Our approach addresses the limitations of traditional optimization algorithms, which often get trapped in local minima. In particular, we introduce the concept of global gradient which offers a robust solution for precise and well-guided global optimization. By focusing on the global minimization problem, this work bridges a critical gap in optimization theory, offering new insights and practical advancements in different optimization problems in particular Machine Learning problems like line search.","Global optimization plays a critical role in addressing complex real-life challenges across various fields. In engineering, it is applied to structural design optimization, where minimizing weight or material use while ensuring durability is essential for cost-effective and safe construction. In financial services, portfolio optimization requires balancing risk and return by finding the global minimum or maximum in investment strategies. In logistics and transportation, global optimization is crucial for solving routing problems such as determining the shortest path or optimizing delivery routes which leads to significant cost savings and improved efficiency. Similarly, in energy systems, global optimization is key to managing and distributing power more efficiently, reducing operational costs, and optimizing renewable energy usage. In machine learning, the need for global optimization is especially pronounced. The performance of models often depends on the ability to minimize complex, non-convex loss functions. While traditional methods like gradient descent are effective in many cases, they frequently encounter the problem of getting trapped in local minima, which can hinder the model’s overall performance. This is particularly relevant in tasks that require complex models where the optimization landscape is highly non-linear and fraught with local minima. The primary contribution of this work is the introduction of a novel algorithm named Super Gradient Descent. Unlike classical gradient descent, which collects only local information making it prone to local minima, the proposed method adapts the state’s change decision based on a global detection of the function change to ensure consistent progress towards the global minimum. We evaluate its performance on various one-dimensional functions, demonstrating that it provides superior convergence behavior, particularly in avoiding local minima and achieving the global optimum. This novel approach contributes to overcoming the challenges of non-convex optimization, offering a more reliable method for finding global solutions in machine learning."
Robust Thompson Sampling Algorithms Against Reward Poisoning Attacks,"Thompson sampling is one of the most popular learning algorithms for online sequential decision-making problems and has rich real-world applications. However, current Thompson sampling algorithms are limited by the assumption that the rewards received are uncorrupted, which may not be true in real-world applications where adversarial reward poisoning exists. To make Thompson sampling more reliable, we want to make it robust against adversarial reward poisoning. The main challenge is that one can no longer compute the actual posteriors for the true reward, as the agent can only observe the rewards after corruption. In this work, we solve this problem by computing pseudo-posteriors that are less likely to be manipulated by the attack. We propose robust algorithms based on Thompson sampling for the popular stochastic and contextual linear bandit settings in both cases where the agent is aware or unaware of the budget of the attacker. We theoretically show that our algorithms guarantee near-optimal regret under any attack strategy.","The multi-armed bandit (MAB) setting is a popular learning paradigm for solving sequential decision-making problems (Slivkins et al., 2019). The stochastic and linear contextual MAB settings are the most fundamental and representative of the different bandit settings. Due to their simplicity, many industrial applications such as recommendation systems frame their problems as stochastic or contextual linear MAB (Brodén et al., 2018; Chu et al., 2011). As one of the most famous stochastic bandit algorithms, Thompson sampling has been widely applied in these applications and achieves excellent performance both empirically (Chapelle & Li, 2011; Scott, 2010) and theoretically (Agrawal & Goyal, 2013; 2017). Compared to another popular exploration strategy known as optimality in the face of uncertainty (OFUL/UCB), Thompson sampling has several advantages: • Utilizing prior information: By design, Thompson sampling algorithms utilize and benefit from the prior information about the arms. • Easy to implement: While the regret of a UCB algorithm depends critically on the specific choice of upper-confidence bound, Thompson sampling depends only on the best possible choice. This becomes an advantage when there are complicated dependencies among actions, as designing and computing with appropriate upper confidence bounds present significant challenges Russo et al. (2018). In practice, Thompson sampling is usually easier to implement Chapelle & Li (2011). • Stochastic exploration: Thompson sampling is a random exploration strategy, which could be more resilient under some bandit settings Lancewicki et al. (2021). Despite the success, Thompson sampling faces the problem of low efficacy under adversarial reward poisoning attacks Jun et al. (2018); Xu et al. (2021); Liu & Shroff (2019). Existing algorithms assume that the reward signals corresponding to selecting an arm are drawn stochastically from a fixed distribution depending on the arm. However, this assumption does not always hold in the real world. For example, a malicious user can provide an adversarial signal for an article from a recommendation system. Even under small corruption, Thompson sampling algorithms suffer from significant regret under attacks. While robust versions of the learning algorithms following other fundamental exploration strategies such as optimality in the face of uncertainty (OFUL) and ϵitalic-ϵ\epsilonitalic_ϵ-greedy were developed Lykouris et al. (2018); Neu & Olkhovskaya (2020); Ding et al. (2022); He et al. (2022); Xu et al. (2023), there has been no prior investigation of robust Thompson sampling algorithms. The main challenge is that under the reward poisoning attacks, it becomes impossible to compute the actual posteriors based on the true reward, which is essentially required by the algorithm. Naively computing the posteriors based on the corrupted reward causes the algorithm to be manipulated by the attacker arbitrarily (Xu et al., 2021). This work. We are the first to show the feasibility of making Thompson sampling algorithms robust against adversarial reward poisoning. Our main contribution is developing robust Thompson sampling algorithms for stochastic and linear contextual bandits. We consider both cases where the corruption budget of the attack is known or unknown to the learning agent. The regrets induced by our algorithms under the attack are near-optimal with theoretical guarantees. We adopt two ideas to achieve robustness against reward poisoning attacks in the two MAB settings. The first idea is ‘optimality in the face of corruption.’ In the stochastic MAB setting, we show that the Thompson sampling algorithm can maintain sufficient explorations on arms and identify the optimal arm by relying on optimistic posteriors considering potential attacks. The second idea is to adopt a weighted estimator He et al. (2023) that is less susceptible to the attack. In the linear contextual MAB setting, we show that with such an estimator, the influence of the attack on the estimation of the posteriors is limited, and the Thompson sampling algorithm can almost always identify the optimal arm at each round with a high probability. We empirically demonstrate the training process of our algorithms under the attacks and show that our algorithms are much more robust than other fundamental bandit algorithms, such as UCB, in practice. Compared to the state-of-the-art robust algorithm CW-OFUL He et al. (2022) for linear contextual bandit setting, our algorithm is as efficient, and in addition, it inherits the advantages from using Thompson sampling exploration strategy as aforementioned."
Learning the Regularization Strength for Deep Fine-Tuning via a Data-EmphasizedVariational Objective,"A number of popular transfer learning methods rely on grid search to select regularization hyperparameters that control over-fitting. This grid search requirement has several key disadvantages: the search is computationally expensive, requires carving out a validation set that reduces the size of available data for model training, and requires practitioners to specify candidate values. In this paper, we propose an alternative to grid search: directly learning regularization hyperparameters on the full training set via model selection techniques based on the evidence lower bound (“ELBo”) objective from variational methods. For deep neural networks with millions of parameters, we specifically recommend a modified ELBo that upweights the influence of the data likelihood relative to the prior while remaining a valid bound on the evidence for Bayesian model selection. Our proposed technique overcomes all three disadvantages of grid search. We demonstrate effectiveness on image classification tasks on several datasets, yielding heldout accuracy comparable to existing approaches with far less compute time.","††footnotetext: Open-source code: https://github.com/tufts-ml/data-emphasized-ELBo When fine-tuning deep neural networks (DNNs), a significant amount of computational resources are devoted to tuning hyperparameters that control model complexity to manage tradeoffs between under- and over-fitting on the target task of interest. One widespread example would be tuning the value of the scalar multiplier that controls the strength of an additive loss term computed as the sum-of-squares on weight coefficient values, known in various communities as L2 regularization [33], Ridge penalty [14, 21], or “weight decay” [24, 9]. A common technique for tuning such hyperparameters is to hold out a dedicated validation set and use grid search to find the hyperparameters that perform best on the validation set [36, 32]. While reasonably effective and in widespread use to manage over-fitting in recent transfer learning [45, 39], using grid search for hyperparameter selection has three key drawbacks. First and perhaps most important, the need to train separate models at each possible value in the grid significantly increases computational runtime and resources. Second, the need to carve out a validation set to assess performance reduces the amount of available data that can inform model training. This can cause under-fitting, especially when available data has limited size. Finally, grid search requires a list of candidate values specified in advance, yet ideal values may vary widely depending on the data and specific classification task at hand. We take another approach to hyperparameter selection, inspired by a pragmatic Bayesian perspective. Suppose we model observable dataset 𝒟𝒟\mathcal{D}caligraphic_D via a likelihood p⁢(𝒟|θ)𝑝conditional𝒟𝜃p(\mathcal{D}|\theta)italic_p ( caligraphic_D | italic_θ ), where θ𝜃\thetaitalic_θ is a high-dimensional parameter to be estimated, with prior distribution p⁢(θ|η)𝑝conditional𝜃𝜂p(\theta|\eta)italic_p ( italic_θ | italic_η ) controlled by hyperparameter η𝜂\etaitalic_η (say just 1-5 dimensions). Instead of point estimating a specific θ,η𝜃𝜂\theta,\etaitalic_θ , italic_η pair, we can instead estimate a posterior p⁢(θ|𝒟,η)𝑝conditional𝜃𝒟𝜂p(\theta|\mathcal{D},\eta)italic_p ( italic_θ | caligraphic_D , italic_η ) while simultaneously directly learning η𝜂\etaitalic_η to optimize p⁢(𝒟|η)=∫θp⁢(𝒟,θ|η)⁢𝑑θ𝑝conditional𝒟𝜂subscript𝜃𝑝𝒟conditional𝜃𝜂differential-d𝜃p(\mathcal{D}|\eta)=\int_{\theta}p(\mathcal{D},\theta|\eta)d\thetaitalic_p ( caligraphic_D | italic_η ) = ∫ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_p ( caligraphic_D , italic_θ | italic_η ) italic_d italic_θ. This latter objective p⁢(𝒟|η)𝑝conditional𝒟𝜂p(\mathcal{D}|\eta)italic_p ( caligraphic_D | italic_η ) is known as the marginal likelihood or evidence [26]. The evidence naturally encodes a notion of Occam’s razor, favoring the hyperparameter setting that leads to the simplest model that fits the data well, while penalizing complex models that over-fit the training data [18, 27, 2]. Learning η𝜂\etaitalic_η to maximize evidence (or equivalently, the logarithm of evidence) via gradient descent avoids all three issues with grid search: we need only one run of gradient descent (not separate efforts for each candidate η𝜂\etaitalic_η value in a grid), we can use all available labeled data for training without any validation set, and we can explore the full continuous range of possible η𝜂\etaitalic_η values rather than a limited discrete set that must be predefined. While elegant in theory, this vision of selecting hyperparameters via maximizing evidence is difficult in practice for most models of interest due to the intractable high-dimensional integral that defines the evidence. For modern deep image classifiers with millions of parameters, computing the evidence directly seems insurmountable even for a specifc η𝜂\etaitalic_η, let alone optimizing evidence to select a preferred η𝜂\etaitalic_η value. In this work, we use and extend tools from variational Bayesian methods [3, 19], specifically tractable lower bounds on the evidence, to make hyperparameter selection for fine-tuning deep neural image classifiers possible. Ultimately, we contribute methods that should help practitioners perform cost-effective transfer learning on custom datasets. When available data is plentiful, our experiments suggest our approach is competitive in accuracy while reducing total training time from 16 hours for L2-SP [45] and 150 hours for PTYL [39] (using the grid search ranges recommended by the original authors) to under 3 hours. When available data is limited, e.g., only 5-300 labeled examples per class, our experiments suggest our approach can be particularly effective in improving accuracy and runtime."
Spatial Shortcuts in Graph Neural Controlled Differential Equations,"rnn short=RNN, long=Recurrent Neural Network \DeclareAcronymnde short=NDE, long=Neural Differential Equation \DeclareAcronymgncde short=GNCDE, long=Graph Neural Controlled Differential Equation \DeclareAcronymncde short=NCDE, long=Neural Controlled Differential Equation \DeclareAcronymnn short=NN, long=Neural Network \DeclareAcronymnode short=NODE, long=Neural Ordinary Differential Equation \DeclareAcronympde short=PDE, long=Partial Differential Equation \DeclareAcronymgnn short=GNN, long=Graph Neural Network \DeclareAcronymlstm short=LSTM, long=Long Short Term Memory \DeclareAcronymgru short=GRU, long=Gated Recurrent Unit \DeclareAcronymagc short=AGC, long=Adaptive Graph Convolution \DeclareAcronymresnet short=ResNet, long=Residual Neural Network \DeclareAcronymmae short=MAE, long=Mean Absolute Error","Effect follows cause. When a problem is represented on a graph, its structure contains information on how causes at one spatial position are linked to effects at another. To learn about physical dynamics from spatial time series data, one can often leverage this structural information. Schölkopf [1] describes differential equations as the gold standard for understanding cause-effect structures and highlight the lack of a time component in statistical machine learning methods. \acpnde [2] are able to learn a hidden state that evolves continuously in time, and could remedy this lack. With Neural Controlled Differential Equations (NCDE) [3], one can update the hidden state continuously with data incoming at different points in time. If one also wants to account for spatial dependencies, \acpgncde [4] can be used, where a node embedding is learned to capture these spatial dependencies. We incorporate prior known graph topology information into a \acgncde to infer the future dynamics at the vertices. We therefore generate data coming from a graph advection simulation from which we know the underlying graph topology plus the temporal cause and effect relation. We will outline the close connection between the graph information in the data generated and the artificial \acnn architecture. Then we train our Informed \acpgncde and let them learn the dynamics to predict their future behavior. We start by describing advection on graphs and the theory before we explain the data generation and our Informed \acgncde. We believe this approach can lead to improvements in domains where graph information is available or where time series data is scarce or partially missing. \acpncde are an effective method in this context due to their ability to handle irregular time series data [5]. With graph information one has the potential to predict with fewer observations, as the graph structure itself does not have to be learned on top of the temporal dynamics. Promising domains for introducing known graph structure are traffic forecasting, river water level forecasting, climate and weather prediction, or disease spread (see A.4). Figure 1: Advection of an initial Gaussian pulse on a graph with 5 edges over time"
MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular Metaverse Services,"Updates of extensive Internet of Things (IoT) data are critical to the immersion of vehicular metaverse services. However, providing high-quality and sustainable data in unstable and resource-constrained vehicular networks remains a significant challenge. To address this problem, we put forth a novel immersion-aware model trading framework that incentivizes metaverse users (MUs) to contribute learning models trained by their latest local data for augmented reality (AR) services in the vehicular metaverse, while preserving their privacy through federated learning. To comprehensively evaluate the contribution of locally trained learning models provided by MUs to AR services, we design a new immersion metric that captures service immersion by considering the freshness and accuracy of learning models, as well as the amount and potential value of raw data used for training. We model the trading interactions between metaverse service providers (MSPs) and MUs as an equilibrium problem with equilibrium constraints (EPEC) to analyze and balance their costs and gains. Moreover, considering dynamic network conditions and privacy concerns, we formulate the reward decisions of MSPs as a multi-agent Markov decision process. Then, a fully distributed dynamic reward method based on deep reinforcement learning is presented, which operates without any private information about MUs and other MSPs. Experimental results demonstrate that the proposed framework can effectively provide higher-value models for object detection and classification in AR services on real AR-related vehicle datasets compared to benchmark schemes.","The metaverse, envisioned as one of the next evolution directions of the Internet, is designed to create a fully immersive, self-sustaining virtual environment in which people can engage in activities such as playing, working, and socializing [1]. This vision is propelled by advances in the fifth-generation (5G) and the forthcoming sixth-generation (6G) communication technologies, renowned for their low latency and high data throughput. These technologies play a critical role in seamlessly integrating the Internet of Things (IoT) data into metaverse services, thus bringing the once-fictional concept of immersive experiences closer to reality. Metaverse services are beginning to reveal their vast potential across a broad spectrum of industries, from gaming and autonomous driving to education and marketing. Notably, the application of vehicles within the metaverse has captured significant interest due to the enhanced safety features and immersive experiences offered by state-of-the-art augmented reality (AR) technologies. Figure 1: An example of AR services in the vehicular metaverse222Image source: https://www.jasoren.com/ar-in-automotive/. Market report [2] forecasted that the global automotive metaverse market will grow from 1.91.91.91.9 billion in 2022 to 6.56.56.56.5 billion by 2030. Automakers like BMW have recently doubled down on AR technology in their projects. As shown in Fig. 2, augmented information makes driving safer by showing potential hazards hidden behind the vehicle in front of you. Moreover, Nissan’s [3] upcoming technology utilizes a 3D AR interface that merges the real world with the virtual world to provide the driver with augmented information about the surrounding area. In addition, Vanarama has also implemented a visual parking assistance function using AR [4], which is expected to identify empty spaces in busy car parks and display useful information, such as parking fees, on the windscreen. The ability to capture information from the “real” world, particularly the ability to collect and process massive data from IoT devices, is the key to determining the success of immersive services (e.g., AR) in the vehicular metaverse. Meanwhile, the data must be processed and presented in a meaningful, responsive, and appropriately protected way. Technically, a high-quality experience with AR services relies on accurate detection and classification of real-world objects (e.g., cars and pedestrians) under complex conditions [5]. To achieve this goal, sufficient valid data needs to be collected and processed in depth to detect and classify objects accurately. Therefore, it is essential to focus on effectively collecting, processing, and protecting the data that supports a safer and more enjoyable driving experience. Motivations. An existing widely-used data collection method, as adopted by Nissan [6], involves collecting massive amounts of data through vehicle sensors, cameras, and roadside devices, and then processing all the data centrally. However, when it comes to the situation with multiple metaverse users (MUs) and metaverse service providers (MSPs) on behalf of different companies, the centralized data collection approach is not applicable and may lead to the following issues. First, AR services in the vehicular metaverse need to be highly immersive so that MUs feel fully immersed in the rendered environment, such as visualized driving. However, providing a seamless AR experience is challenging due to the latency caused by massive real-time data updates under unstable and resource-limited communication conditions. Note that the value of real-time data diminishes over time [7]. Also, delays can severely impact the MU’s experience and cause dizziness [8]. Second, the data to be transmitted may be sensitive and private, such as location, movement, and biometrics, which can create a better immersive experience but may inevitably increase the privacy risk of MUs [9]. Third, it is difficult to share data between MSPs due to competition. This leads to the same data being sampled multiple times by different MSPs in the physical world, resulting in wasted resources (e.g., sampling costs) and data redundancy. Moreover, tremendous data pressure and computational burden are imposed on MSPs. Therefore, an efficient method for collecting high-quality data while preserving privacy is needed to achieve immersive AR services. Challenges. To address the above issues, we propose an immersion-aware model trading framework designed to provide data support for AR services and ensure privacy by integrating federated learning (FL). Adopting FL enables MUs to contribute diverse learning models to multiple MSPs using local data, thus effectively protecting sensitive information [10]. Moreover, learning models are uploaded to MSPs for AR services by individual MUs, rather than centrally transmitting large amounts of data, which significantly reduces the communication burden. Nonetheless, developing such a model trading framework still faces the following challenges: • MUs are typically self-interested and are reluctant to share learning models with MSPs due to the additional computation, communication, and energy overhead. Thus, the challenge here is how to incentivize them to become contributors in providing high-value learning models that will benefit vehicular metaverse services. • It is difficult to design an appropriate metric to comprehensively evaluate the value of learning models contributed by MUs due to the diverse aspects involved in model valuation. • MUs have different sampling costs and limited computation and communication resources, while MSPs differ in their model preferences and compete with each other. Hence, the next challenge is to model the dynamic competitive interactions among MSPs and to achieve an equilibrium of interests among MUs and MSPs in the model trading. Figure 2: The outline of an immersion-aware framework for FL-assisted vehicular metaverse. Contributions. To address these challenges, we model the dynamic competitive trading interactions as an equilibrium problem with equilibrium constraints (EPEC) involving multiple leaders (MSPs) and multiple followers (MUs), which is a hierarchical optimization problem with equilibria at two levels [11]. Furthermore, to evaluate the value of learning models contributed by MUs to MSPs, we design a new metric called the immersion of the learning model (IoM). The metric jointly considers the freshness and accuracy of the learning model, as well as the amount and potential value of raw data used for training. Moreover, given the dynamic networks and the privacy concerns of MSPs, we formulate the reward decisions of MSPs as a multi-agent Markov decision process (MAMDP) and develop a multi-agent deep reinforcement learning (DRL)-based dynamic reward (MDDR) approach to obtain the reward decisions in a fully distributed manner. The outline of the proposed framework is shown in Fig. 2, and the main contributions of this paper are summarized as follows: • We propose an immersion-aware trading framework for learning models with FL assistance, incentivizing MUs to contribute high-value learning models for MSPs in a privacy-preserving and efficient manner. Moreover, a novel metric called “IoM” is designed to quantify the immersion enhancement provided by MUs for AR services. • We model the dynamic competitive trading interactions as an EPEC and provide theoretical support for the existence and uniqueness of equilibria at two levels. Moreover, a fully distributed MDDR approach is developed to adapt to dynamic environments and address complex reward decisions of MSPs without accessing any private information of MUs or MSPs. • We conduct extensive numerical simulations based on AR-related vehicle datasets to validate the efficacy and efficiency of MDDR and the proposed immersion-aware model trading framework. Compared to benchmark schemes, the framework better motivates MUs to provide higher-value learning models for object detection and classification functions of AR services effectively. The rest of this paper is organized as follows. Section II discusses the related work. In Section III, we present the system overview and design the immersion metric of the learning model. Section IV gives the game formulation, and Section V analyzes the existence of the equilibria at two levels. In Section VI, we give the detailed design of MDDR. Section VII shows numerical experiments to evaluate the framework performance, and finally Section VIII concludes the paper."
Conformal Prediction for Multimodal Regression,capbtabboxtable[][\FBwidth],"1 Background and related work The comprehension of the internals of neural networks has begun with efforts towards explainability but little has been done to exploit these internal features. The area of conformal prediction (CP) for regression is no different. The consequence is that rich multimodal input features such as images, unstructured text, and categoricals have not participated in the conformal regression prediction uncertainty quantification process. CP provides a straightforward framework for constructing statistically rigorous uncertainty intervals for model predictions. Importantly, these intervals are valid regardless of the underlying distribution, providing explicit, non-asymptotic guarantees without depending on specific distributional or model assumptions. By using conformal prediction on any pre-trained model, it is possible to create intervals that consistently contain the true value with a specified coverage level. Angelopoulos and Bates (2021). CP input assumptions are based on data exchangeability. There is no other assumption about the data distribution or the model. This condition is satisfied for the problems outlined in this paper. The more developed field of CP classification has done image-to-image uncertainty Angelopoulos et al. (2022), and even proposed changing CP regression tasks into classification problems Guha et al. (2023). Despite a comprehensive literature review, no findings similar to those proposed by our novel use of internal features in CP for multimodal regression have been uncovered. The inductive, or split, approach to creating conformal regressors, outlined by Papadopoulos et al. (2002), involves splitting the training data into a proper training set and a calibration set, both of which should accurately represent the overall data distribution. The training set is used to build the regression model, while the calibration set is used to calculate the model’s absolute residuals. Conformal predictive systems (CPS) enhance conformal regressors by generating cumulative probability distributions, over potential target values Vovk et al. (2020). This enables the creation of PIs with a specified confidence level, effectively capturing prediction uncertainty while providing a statistically grounded basis for decision-making. Typically, the more complicated multimodal inputs of images and unstructured text are processed by larger more complicated networks (e.g., convolutional neural networks (CNN), transformers, hybrid networks, etc.). These networks can be compute-intensive. Therefore, building multiple models and having PIs generated in an ensemble fashion may not be an option. The inductive implementation of the aforementioned CPSs was the chosen approach as the model is only trained once after a hold-out calibration set is removed from the training set. Multimodal architectures generally contain an internal combining stage where all the processed inputs come together. These internal inputs will be referred to as internal features within the respective architectures. Conversely, external features are features that are fed to the model at the input layer. The possible advantage with working with internal features is that they have been filtered for significance and weighted for importance which is not true of the input features. This results in a metric suitable for distance-based conformal prediction. The major contribution of this work is to address the challenges associated with conformal prediction in the context of multimodal regression tasks involving diverse input data, such as tabular, unstructured text, and image modalities. Traditional conformal prediction methods face significant obstacles when applied directly to such heterogeneous input features in regression problems. However, this paper demonstrates a novel approach that leverages an internal feature layer within the model architecture for multimodal regression. Specifically, we show that this intermediate representation can be effectively utilized to generate a calibration set suitable for conformal prediction in regression scenarios. This methodology extends the applicability of conformal prediction techniques to complex, multimodal regression tasks, thereby enhancing uncertainty quantification capabilities for a wider range of machine learning regression models and diverse data types."
Deep learning-based identification of patients at increased risk of cancer using routine laboratory markers,"Early screening for cancer has proven to improve the survival rate and spare patients from intensive and costly treatments due to late diagnosis. Cancer screening in the healthy population involves an initial risk stratification step to determine the screening method and frequency, primarily to optimize resource allocation by targeting screening towards individuals who draw most benefit. For most screening programs, age and clinical risk factors such as family history are part of the initial risk stratification algorithm. In this paper, we focus on developing a blood marker-based risk stratification approach, which could be used to identify patients with elevated cancer risk to be encouraged for taking a diagnostic test or participate in a screening program. We demonstrate that the combination of simple, widely available blood tests, such as complete blood count and complete metabolic panel, could potentially be used to identify patients at risk for colorectal, liver, and lung cancers with areas under the ROC curve of 0.76, 0.85, 0.78, respectively. Furthermore, we hypothesize that such an approach could not only be used as pre-screening risk assessment for individuals but also as population health management tool, for example to better interrogate the cancer risk in certain sub-populations.","This paper focuses on the use of multiple biomarkers for the assessment of patients, or identification of otherwise healthy individuals, who are at increased risk of cancer. With the high mortality rate associated with cancer patients, significant research has been conducted to help identify patients at higher risk, starting with identifying medical conditions that increase the risk of cancer, such as diabetes, or genetic predispositions that promote its development[1]. Furthermore, various screening procedures have been developed to help facilitate early diagnosis such as the Faecal Immunochemical Test (FIT) and colonoscopy for colorectal cancer (CRC)[2], mammography for breast cancer[3], and low-dose computed tomography (LDCT) for lung cancer[4]. However, cancer screening rates and their uptake remains lower than desired, e.g., in the US[5]. While there are several factors contributing to this low uptake, one of the key factors is the lack of awareness within the general population. This is even more important to address for people who may be at increased risk and would benefit from early and/or regular screening. In other words, there is still a need for convenient tests for early detection of rapidly progressing diseases such as cancer so that intervention can start as early as possible[6]. Several cancer risk prediction/assessment tools based on demographic, socioeconomic or blood based markers have been developed over the years, and studies have shown that cancer risk assessment algorithms could have an impact in early cancer diagnosis[7]. For instance, the Qcancer 10 year risk algorithm[8] considers the age, ethnicity, deprivation, body mass index, smoking, alcohol, previous cancer diagnoses, family history of cancer, relevant comorbidities, and medication data for a patient and predicts the cancer risk for 11 types of cancers. Nartowt et al.[9] reported high concordance in the prediction of CRC into low, medium and high groups using an artificial neural network trained on patient data comprising age, sex, and complete blood count (CBC). ColonFlag[10] can be used to identify individuals at high risk of CRC using specific blood-based markers and refer them to screening procedures such as colonoscopy. More recently, a cell-free DNA-based blood test for the early detection of CRC has been clinically validated in the ECLIPSE study[11]. Moreover, multi-cancer early detection technologies[12] such as the Galleri test[13, 14] can identify abnormal methylation patterns in cell-free DNA to detect a cancer signal and predict its origin. Besides algorithm development, the deployment of the algorithm and communication of the findings play a critical role in acceptance and clinical use of the algorithm[15], and must be taken into account to facilitate screening uptake. To this end, instead of defining a test with a specific set of ingredients catered towards a particular cancer, we propose to use commonly measured blood markers, often obtained during the annual physical exam, and obtain a risk profile for multiple cancers. Furthermore, instead of reporting a risk score, we compute the pre- and post-test odds of a patient at risk of developing cancer over the next 12 months. A key challenge in developing a model that considers several biomarkers is to deal with a significant degree of missingness in the historical data as not all markers may be obtained at each encounter. Although this issue can be partly alleviated by considering the biomarkers obtained at an annual physical exam, we observed that in real world data, there is still a significant degree of missingness, either due to a lack of awareness, insurance coverage or reimbursement, among other reasons. A standard approach to deal with missingness in input data is to impute the missing values using statistical methods, such as expectation maximization and regression[16]. However, the quality of imputed data is limited and can significantly impact the generalization ability of the trained model. In this work, we address the aforementioned challenges by training a deep learning model, Deep Profiler, which takes the age, sex, and commonly obtained blood biomarkers included in CBC and Comprehensive Metabolic Panel (CMP), and outputs a likelihood ratio of a patient to develop cancer over the period of the following 12 months (see Figure 1). The Deep Profiler architecture employs a variational autoencoder (VAE) model that is pre-trained to impute missing data similar to the masked language modeling technique. Subsequently, we train cancer-specific risk prediction models from the shared encoded latent space and compute the likelihood ratio for each patient. We validate the proposed method over screening-relevant cohorts for three different cancers - colorectal, liver, and lung. These are among the top cancers responsible for cancer related mortality rate in the US (https://seer.cancer.gov/statfacts/html/common.html, accessed April 30, 2024.). Figure 1: Workflow of using a biomarker-based pre-screening test."
Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites.,"Machine learning (ML) models benefit from large datasets. Collecting data in biomedical domains is costly and challenging, hence, combining datasets has become a common practice. However, datasets obtained under different conditions could present undesired site-specific variability. Data harmonization methods aim to remove site-specific variance while retaining biologically relevant information. This study evaluates the effectiveness of popularly used ComBat-based methods for harmonizing data in scenarios where the class balance is not equal across sites. We find that these methods struggle with data leakage issues. To overcome this problem, we propose a novel approach “PrettYharmonize”, designed to harmonize data by pretending the target labels. We validate our approach using controlled datasets designed to benchmark the utility of harmonization. Finally, using real-world MRI and clinical data, we compare leakage-prone methods with “PrettYharmonize” and show that it achieves comparable performance while avoiding data leakage, particularly in site-target-dependence scenarios.","Many research fields have greatly benefited from machine learning (ML) approaches. ML models can extract important values from large amounts of data. Having vast data benefits the model’s classification performance and helps capture the underlying patterns, promoting better generalization to new unseen data. This makes combining multiple datasets an appealing approach, especially in domains where obtaining data in a uniform setting is challenging 1. Moreover, small health or research centers that can not afford to collect a large number in-house data, using data acquired in different sites is the only possibility for train ML models. However, different datasets obtained under different conditions often present variability due to differences in the acquisition procedure that are unrelated to relevant biological information 2. This undesired variability, also known as Effects of Site (EoS), can induce biased results if present or not correctly removed 3. These differences may come from systematic differences, which can be corrected, or random variations, which can not be modeled or corrected by harmonization. This problem is of common occurrence in many biomedical domains. For example, clinical data is affected by the acquisition site, as different hospitals have different laboratory machines, procedures, and criteria. Another example is the medical imaging field, as images are affected by acquisition protocol, scanner drifts, and time of the day, just to name a few factors 3, 4. Within this field, Magnetic Resonance Imaging (MRI) images are particularly susceptible to this site-related variance, like the magnetic field strength, room temperature fluctuation or changes in the electromagnetic noise, which makes even images obtained from scanners with the same manufacturer and the same parameters exhibit different characteristics 5, 6. Many works showed that removing this undesired systematic variability, which is only related to the acquisition site and has no biological information, can benefit further analysis made with the data 7, 8, 9, 10, 11. To this end, several Methods Aiming to Remove the Effects of Site (MAREoS) have been proposed and developed 4, 12. These MAREoS methods are typically used as a pre-processing step, where the site effects are removed and the “site-effect free” data, also known as harmonized data, is used for statistical analysis or to train and evaluate ML models. Among these MAREoS, the ones based on “ComBat” are extensively used in several domains. The ComBat method was originally proposed for correcting batch differences in genomic data 13 and was later adapted to other domains like MRI data 14, 7. ComBat uses Bayesian regression to find additive (location) and multiplicative (scale) corrections for each feature in each site. Within the ComBat-based methods, “neuroHarmonize” was proposed 15 to allow for the preservation of non-linear covariate effects and has been widely used since 4, 12, 16, 17. Although ComBat and its derivations have been widely applied in medical imaging data, several concerns have been raised, mainly because ComBat’s hypothesis and assumptions only hold for genomic data, where it was originally proposed, and may not be fulfilled in other applications fields 18. Additionally, concerns had been raised on the integration of ComBat into ML models, as the location and scale parameters of the model could not be learned in a subset of data (train data) and then applied to a new unseen subset of data (test data) 19. Early implementations of ComBat 14, 20 used the whole dataset to estimate the model’s parameters and create a harmonized dataset that is then used from all the downstream analyses. This approach was used in several works 7, 8, 21, 22, 23, 24. While this approach is valid when performing statistical analyses, it is not consistent with machine learning applications where the training and test data must be separate 25, 19. Specifically, the parameters of the models, including preprocessing models, must be obtained on a training set and then applied to the test set. This separation is important to get realistic estimates of generalization performance (e.g. using cross-validation) and to ensure deployability of the model in the real world where the test data is not yet available 26, 27. ComBat-MAGA 28, neuroHarmonize 15, and ”harmonizer” 19, which is based on neuroHarmonize, allow the estimation of the model’s parameters in a training set and apply them to the test samples, however, a critical assumption of ComBat is that all variance not shared across sites is unwanted site-related variance. Consequently, ComBat removes any variance that is not common to all sites, including the relevant biological variance. This poses a new problem, as this assumption is broken when a class imbalance occurs across sites and a target-site dependence exists, for example when the control patients are acquired in one site and target patients in a different one 29. This could also be extended to other possible biological information like comorbidities or disease severity, for example, if more severe patients are consistently treated or acquired only in one site. In these cases, even though ComBat will provide harmonized data, it will remove the variance related to the target (control versus patient) as the assumption is that only non-relevant factors change between sites. ComBat allows to retain the biologically relevant variables, for example, a diagnosis, the age, or the sex of a patient, by providing these variables as covariates to be retained. Nonetheless, this information is needed both when training the model and when applying the model to the test data. Thus, if target labels need to be preserved, this inevitably leads to the model requiring the test labels preventing the model’s use in real-world applications where test labels are not available or known18. This phenomenon where information of the test set is presented to the models is commonly known as data leakage. It is also well described that leaking the test target information would produce overconfident results, which could be misleading and can jeopardize the progress of an entire research field, as researchers who avoid data leakage would not be able to outperform the models that present data leakage 26, 27, 25. In this work, we aim to empirically demonstrate a shortcoming of ComBat-based harmonization in site-target dependence scenarios, i.e., that the model can properly harmonize the data only when test labels are used and data leakage happens. To do so, we performed controlled experiments for age regression and sex classification using real MRI data for healthy control individuals. Also using MRI data, a dementia and mild cognitive impairment (MCI) classification experiment was performed. Additionally, an outcome prediction of septic patients was performed using clinical data. All experiments were conducted both in site-target dependence and independence scenarios. Several harmonization schemes were used and compared, allowing and not allowing leakage, to harmonize the data. Finally, to overcome the aforementioned problem, we propose a new harmonization method, called PRETended Target Y Harmonize (PrettYHarmonize), which allows the users to integrate ComBat in an ML pipeline, harmonizing the data and generating a prediction without using the test labels and thus avoiding data leakage. We validated our method using benchmark datasets 3. Additionally, the proposed method was compared with the other harmonization schemes on the site-target dependence and independence scenarios to comprehensively compare no harmonization, leakage, and no-leakage methods. The corresponding Python package is publicly available via GitHub https://github.com/juaml/PrettYharmonize."
Efficient Biological Data Acquisitionthrough Inference Set Design,"In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so we might hope to reduce their cost by experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of an uncertainty-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that stops running the experiments when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that deploying active learning for inference set design leads to significant reduction in experimental cost while obtaining high system performance.","Automated high-throughput screening (HTS) laboratories have enabled scientists to screen large compound libraries to find effective therapeutic compounds and screen whole-genome CRISPR knockouts to understand the effects of genes on cell function (Mayr & Bojanic, 2009; Wildey et al., 2017; Blay et al., 2020; Tom et al., 2024; Fay et al., 2023). However, conducting experiments on every compound or CRISPR guide in these vast libraries remains very resource-intensive. With typical screening libraries holding on the order of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT to 106superscript10610^{6}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT compounds (Hughes et al., 2011) and the number of possible small molecules estimated at 1060superscript106010^{60}10 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT (Bohacek et al., 1996), the disparity between our screening capabilities and all which we could explore is staggering. Reducing experimental costs without compromising the quality of the generated data would allow us to accelerate biology and pharmaceutical research and expand the set of molecules considered for testing. To avoid costs scaling with the number of perturbations, we can train a model from a subset of the target library that has been tested in the lab, and then predict experimental outcomes for the remainder of the library using the trained model (Naik et al., 2013; Reker & Schneider, 2015; Dara et al., 2022), thereby building a hybrid screen of the library. This approach entails three interrelated questions: (1) which subset of the library should we use to maximise the accuracy of the predictions?, (2) how do we select this subset without access to the experimental outcomes?, and (3) how do we ensure that we select a large enough subset to meet a target level of accuracy for the predictions? This problem is similar to an active learning problem in that we want to select examples that maximize prediction accuracy, but instead of aiming to minimize generalization error, here we only care about prediction on a particular, finite, set of experiments from a library. The fact that the library is finite introduces an important difference: the learner can influence the set of examples on which it is evaluated by strategically selecting examples. If we accept that the difficulty of predicting outcomes varies among compounds, such that some examples are inherently harder to predict, either due to their complex properties (Bengio et al., 2009), because of the partial observability of the system (Saleh et al., 2021; Krenn et al., 2020) or due to noise in the labeling function (Frénay & Verleysen, 2013; Lukasik et al., 2020), the learner can select these examples into the training set to avoid having to predict their outcomes. Conversely, if an example can be reliably predicted by the model, we can save experimental costs by not including it in the training set. We call this mechanism through which a learner can influence performance inference set design. We propose an active learning-based solution to hybrid screening that uses the model’s confidence to guide the selection of experiments and leverages the mechanism of inference set design to improve the system’s performance on the target set. Our algorithm includes a practical stopping criterion that terminates the search for new molecules once a lower bound on a target accuracy threshold is exceeded. We show in Lemma 1 that this bound provides probabilistic guarantee on the performance of the algorithm as long as the model is weakly calibrated, such that examples for which the model is more uncertain receive a lower predicted probability than those for which it is more certain. To validate our method, we conduct series of empirical studies on image and molecular datasets, as well as a real-world case study in drug discovery. The results demonstrate inference set design significantly reduces experimental costs while improving overall system performance. Importantly, this is true even when the generalization benefits of active learning-based acquisition functions are marginal compared to random search. This has important practical implications for active learning: if a problem is a hybrid screen—in the sense that one only needs good performance on a fixed, finite set of experiments—then evaluating generalization error dramatically understates the benefits of active learning. By combining simple active learning acquisition functions with an appropriate stopping criterion, it is possible to make large scale screening far more efficient."
"Analyzing Neural Network Robustness Using Graph Curvature††thanks:This material is based upon work supported by the National Science Foundation (NSF) under Award No. 2403616, as part of the NSF Cyber-Physical Systems Program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.","This paper presents a new look at the neural network (NN) robustness problem, from the point of view of graph theory analysis, specifically graph curvature. Graph curvature (e.g., Ricci curvature) has been used to analyze system dynamics and identify bottlenecks in many domains, including road traffic analysis and internet routing. We define the notion of neural Ricci curvature and use it to identify bottleneck NN edges that are heavily used to “transport data” to the NN outputs. We provide an evaluation on MNIST that illustrates that such edges indeed occur more frequently for inputs where NNs are less robust. These results will serve as the basis for an alternative method of robust training, by minimizing the number of bottleneck edges.","Autonomous systems (AS) increasingly use neural networks (NNs) due to their ability to process high-dimensional data such as camera images [1], LiDAR scans [2] and textual prompts [3]. At the same time, NNs are known to suffer from robustness vulnerabilities: a slightly perturbed or out-of-distribution input [4, 5] may lead to very different and unexpected outputs. In turn, such vulnerabilities may severely compromise the safety and predictability of NN-based AS. Since the discovery of NN robustness issues [4], there has been an impressive amount of research on this topic. Researchers have developed a number of robust training methods, including adversarial training [6], certified robustness [7, 8], knowledge distillation [9], and semi-infinite constrained learning [10]. Although significant progress has been made, training robust NNs remains largely an unsolved and very challenging problem (e.g., the current leader on the CIFAR-10 robustness leaderboard [11] can only achieve high robust accuracy for perturbations of at most 8/255). We note that the vast majority of existing methods approach the problem from an optimization point of view: e.g., in adversarial training the goal is to train a NN that minimizes the loss not only on training data but also on the worst-case bounded perturbations of that data. This bilevel non-convex optimization problem is challenging to solve and leads to suboptimal solutions, especially if gradient descent is used. We take a fresh look at NN robustness through the lens of graph theory and network science analysis, in particular graph curvature (GC). GC (e.g., Ricci curvature [12]) has been effectively applied in numerous domains that can be modeled as graphs, including road traffic analysis [13, 14], internet routing [15], machine learning [16, 17], and biological networks [18, 16], due to its ability to capture intrinsic geometric and local structure of the space, such as connectivity and robustness in networks. GC can quantify the importance of specific edges; for example, an edge with negative curvature can be considered a bottleneck and is greatly important for the overall graph functionality, e.g., such an edge may connect different communities within the graph [19, 20]. In this paper, we employ GC in order to analyze the robustness of NN classifiers. We introduce the notion of neural Ricci curvature (NRC) that captures the bottleneck intuition of standard Ricci curvature – if an edge has a negative NRC, then it is heavily used by the NN and is thus likely a source of robustness vulnerability. To calculate the NRC, we construct a neural data graph, i.e., a graph in the shape of the NN architecture, where edges are weighted by a combination of the NN weights and the magnitude of data that goes through each edge when an example is provided as input. We evaluate the significance of the NRC using NNs trained on MNIST. We show that neural data graphs corresponding to more robust examples (i.e., examples which are correctly classified even for an adversarial perturbation) indeed have fewer negative-NRC edges. The results are consistent across architectures, including adversarially trained ones. This result will serve as the basis for an alternative, graph-based, method for robust training, that minimizes the number of negative-NRC edges and promotes balanced usage of all NN edges. In summary, this paper makes two contributions: 1) we define the concepts of neural data graphs and neural Ricci curvature that can be used to identify bottleneck NN edges that contribute to robustness issues; 2) we provide an evaluation on MNIST that demonstrates that bottleneck edges indeed occur more frequently in examples where NNs are less robust."
FLiP: Privacy-Preserving Federated Learning based on the Principle of Least Privileg,"Federated Learning (FL) allows users to share knowledge instead of raw data to train a model with high accuracy. Unfortunately, during the training, users lose control over the knowledge shared, which causes serious data privacy issues. We hold that users are only willing and need to share the essential knowledge to the training task to obtain the FL model with high accuracy. However, existing efforts cannot help users minimize the shared knowledge according to the user intention in the FL training procedure. This work proposes FLiP, which aims to bring the principle of least privilege (PoLP) to FL training. The key design of FLiP is applying elaborate information reduction on the training data through a local-global dataset distillation design. We measure the privacy performance through attribute inference and membership inference attacks. Extensive experiments show that FLiP strikes a good balance between model accuracy and privacy protection.","Federated learning (FL) [1] is a deep learning (DL) training paradigm, which aims to utilize the data existing in the form of isolated islands to train DL models (Fig. 2(a)). During the training procedure, data owners (a.k.a. clients) do not share their raw data with anyone, but instead, share some information obtained from the raw data in the form of model parameters. Many solutions are proposed to protect the privacy in FL context. However, they are yet unable to strike a balance between performance and privacy protection in real-world scenarios. This is because they do not fully consider the training task itself when performing protection. Specifically, considering that users are likely only interested in obtaining a high-quality model for the target task, sharing any information unrelated to the training task during the training process could potentially lead to privacy leakage, e.g., secondary attributes inference [2], practical attribute reconstruction Attack [3]. Given the subjective nature of privacy protection, we hold that an ideal solution should fully consider the user’s training goals and only share essential information related to the training task during the training. That’s the core idea, what we call as principle of least privilege (PoLP). According to the PoLP, as shown in Fig. 1, clients should control only the essential training task-relevant information from the raw data that can be shared among participants. At first glance, it is paradoxical to determine which part of the raw data plays a role in the model training procedure before the model is trained. After empirical study and analysis, we observe that each client can only extract a portion of local data that is most relevant to the FL task in the local training. Figure 1: Comparison of PoLP and existing privacy protection solutions. Ideally, the new sample generated by PoLP only contains task-relevant information. (a) The vanilla FL paradigm. (b) The FL paradigm of FLiP. Figure 2: The training procedures of vanilla FL and FLiP. In both paradigms, no training is needed on the central server. The biggest difference between our FLiP and the vanilla FL is the carrier of information aggregation, i.e., FLiP performs distilled data aggregation, and the vanilla FL performs parameter aggregation. Compared to the vanilla method, the amount of shared information during the training in FLiP is controllable. Our work proposes a new FL framework, FLiP, to achieve the PoLP in FL training for privacy protection. FLiP can help clients to identify which part of the raw data contributes most to the FL training and, at the same time, extract the task-relevant information locally. The central server collects the task-relevant information extracted by clients and distributes the global view to help clients make better information extraction. To measure privacy protection effectiveness, we consider adversaries in a semi-honest setting and perform two attacks to infer task-irrelevant information from the data extracted during the training. Experimental results show that FLiP can achieve comparable accuracy to the vanilla FL and better protection for information irrelevant to the training task. Our contribution is fourfold: • We are the first to introduce the PoLP to the FL training for privacy protection. Data owners can control the local information shared among FL participants and minimize privacy breaches by only sharing the essential FL task-relevant information. • We design a privacy-preserving FL system, FLiP, to realize the PoLP. The key design of FLiP is a local-global dataset distillation, which can identify and extract the task-relevant information gradually by round. • We design a task-irrelevant attribute inference attack to measure the protection effectiveness. The attribute inference attack is inspired by existing secondary inference attacks and fully considers the information leakage in each round. • We implement the system and perform an extensive evaluation. The experimental results show that FLiP can prevent adversaries from inferring task-irrelevant information and preserve high data usability."
Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series,"Lately, there has been a surge in interest surrounding generative modeling of time series data. Most existing approaches are designed either to process short sequences or to handle long-range sequences. This dichotomy can be attributed to gradient issues with recurrent networks, computational costs associated with transformers, and limited expressiveness of state space models. Towards a unified generative model for varying-length time series, we propose in this work to transform sequences into images. By employing invertible transforms such as the delay embedding and the short-time Fourier transform, we unlock three main advantages: i) We can exploit advanced diffusion vision models; ii) We can remarkably process short- and long-range inputs within the same framework; and iii) We can harness recent and established tools proposed in the time series to image literature. We validate the effectiveness of our method through a comprehensive evaluation across multiple tasks, including unconditional generation, interpolation, and extrapolation. We show that our approach achieves consistently state-of-the-art results against strong baselines. In the unconditional generation tasks, we show remarkable mean improvements of 58.17%percent58.1758.17\%58.17 % over previous diffusion models in the short discriminative score and 132.61%percent132.61132.61\%132.61 % in the (ultra-)long classification scores. Code is at https://github.com/azencot-group/ImagenTime.","Generative modeling of real-world information such as images [72], texts [13], and other types of data [99, 55, 8] has drawn increased attention recently. In this work, we focus on the setting of generative modeling (GM) of general time series information. There are several factors that govern the complexity required from sequential data generators including the sequence length, its number of features, the appearance of transient vs. long-range effects, and more. Existing generative models for time series are typically designed either for multivariate short-term sequences [44, 19] or univariate long-range data [103], often resulting in separate and completely different neural network frameworks. However, a natural question arises: Can one develop a unified framework equipped to handle both high-dimensional short sequences and low-dimensional long time series? Earlier approaches for processing time series based on recurrent neural networks (RNNs) handled short sequences well [62, 3, 43, 76], however, modeling long-range dependencies turned out to be significantly more challenging. Particularly, RNNs suffer from the well-known vanishing and exploding gradient problem [9, 70] that prevents them from learning complex patterns and long-range dependencies. To address long-context modeling and memory retention, extensive research is devoted to approaches such as long short-term memory (LSTM) models [42], unitary evolution RNNs [5] and Lipschitz RNNs [24]. A different approach for processing sequential information is based on the Transformer [93], eliminating any recurrent connections. Recent remarkable results have been obtained with transformers on natural language processing [13] and time series forecasting [96, 104, 68] tasks. Alas, transformers are underexplored as generative models for long-range time series data. This may be in part due to their computational costs that scale quadratically as 𝒪⁢(L2)𝒪superscript𝐿2\mathcal{O}(L^{2})caligraphic_O ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) with the sequence length L𝐿Litalic_L, and in part because transformer forecasters are inferior to linear tools [101]. Beyond RNNs and the Transformer, recent works have considered the state space model (SSM) for modeling long-range time series information. For instance, the structured SSM (S4) [36] employed a parameterization that reduced computational costs via evaluations of Cauchy kernels. Further, the deep linear recurrent unit (LRU) is inspired by the similarities between SSMs and RNNs, and it demonstrated impressive performance in modeling long-range dependencies (LRD). Still, generative modeling of long-range sequential data via state space models remains largely underexplored. Recent work suggested LS4 [103], a latent time series generative model that builds upon linear state space equations. LS4 utilizes autoregressive dependencies to expressively model time series (potentially non-stationary) distributions. However, this model struggles with short-length sequences as we show in our study, potentially due to limited expressivity of linear SSMs. To overcome gradient issues of recurrent backbones, temporal computational costs of transformers, and expressivity problems of SSMs, we represent time series information via small-sized images. Transforming raw sequences to other encodings has been useful for processing audio [34] as well as general time series data [95, 38, 56]. Moreover, a similar approach was employed to generative modeling of time series with generative adversarial networks (GANs) [12, 39]. However, unstable training dynamics and mode collapse negatively affect the performance of GAN-based tools [59]. In contrast, transforming time series to images is underexplored in the context of generative diffusion models. There are several fundamental advantages to our approach. First, there have been remarkable advancements in diffusion models for vision data that we can exploit [81, 40, 86, 45]. Second, using images instead of sequences elegantly avoids the challenges of long-term modeling. For instance, a moderately-sized 256×256256256256\times 256256 × 256 image corresponds to a time series of length up to 65⁢k65𝑘65k65 italic_k, as we show in Sec. 3. Finally, there is a growing body of literature dealing with time series as images on generative, classification, and forecasting tasks, whose results can be applied in our work and in future studies. In this work, we propose a new diffusion-based framework for generative modeling of general time series data, designed to seamlessly process both short-, long-, and ultra-long-range sequences. To evaluate our method, we consider standard benchmarks for short to ultra-long time series focusing on unconditional generation. Our approach supports efficient sampling, and it attains state-of-the-art results in comparison to recent generative models for sequential information. As far as we know, there are no existing tools handling both short and long sequence data. In addition to its strong unconditional generation capabilities, our approach is also tested in conditional scenarios involving the interpolation of missing information and extrapolation. Overall, we obtained state-of-the-art results in such cases with respect to existing tools. We further analyze and ablate our technique to motivate some of our design choices. The contributions of our work can be summarized as follows: 1. We view generative modeling of time series as a visual challenge, allowing to harness advances in time series to image transforms as well as vision diffusion models. 2. We develop a novel generative model for time series that scales from short to very long sequence lengths without significant modifications to the neural architecture or training method. 3. Our approach achieves state-of-the-art results in comparison to strong baselines in unconditional and conditional generative benchmarks for time series of lengths in the range [24,17.5⁢k]2417.5𝑘[24,17.5k][ 24 , 17.5 italic_k ]. Particularly, we attain the best scores on a new challenging benchmark of very long sequences that we introduce."
AgentForge: A Flexible Low-Code Platform for Reinforcement Learning Agent Design,"Developing a reinforcement learning (RL) agent often involves identifying effective values for a large number of parameters, covering the policy, reward function, environment, and the agent’s internal architecture, such as parameters controlling how the peripheral vision and memory modules work. Critically, since these parameters are interrelated in complex ways, optimizing them can be viewed as a black box optimization problem, which is especially challenging for non-experts. Although existing optimization-as-a-service platforms (e.g., Vizier, Optuna) can handle such problems, they are impractical for RL systems, as users must manually map each parameter to different components, making the process cumbersome and error-prone. They also require deep understanding of the optimization process, limiting their application outside ML experts and restricting access for fields like cognitive science, which models human decision-making. To tackle these challenges, we present AgentForge, a flexible low-code framework to optimize any parameter set across an RL system. AgentForge allows the user to perform individual or joint optimization of parameter sets. An optimization problem can be defined in a few lines of code and handed to any of the interfaced optimizers. We evaluated its performance in a challenging vision-based RL problem. AgentForge enables practitioners to develop RL agents without requiring extensive coding or deep expertise in optimization.","1 INTRODUCTION Developing reinforcement learning (RL) agents is important not only for advancements in machine learning (ML) but also for fields such as cognitive sciences, which is increasingly using RL to model cognitive mechanisms [Eppe et al., 2022]. One of the challenging aspects of this development is optimizing a broad and complex array of parameters that influence agent behavior and performance, a complex black box optimization problem for non-ML experts. Embodied RL agents, which interact with their environments through sensors and actuators, are a point in case. These agents are modeled as Partially Observable Markov Decision Processes (POMDPs) [Eppe et al., 2022, Deglurkar et al., 2023] and are used in real-world tasks such as autonomous driving [De Morais et al., 2020], robotic navigation [Shahria et al., 2022], and human-computer interaction [Jiang et al., 2024]. These models involve not only the parameters of the policy but several others controlling, among others, the reward function. In constrast, optimization of models in other ML fields is often more straightforward, as practitioners need to tune a smaller set of parameters mostly related to training. Sometimes parameter optimization is not just a matter of performance. Parameters that are explainable (e.g., reward weights) are a key to engineering explainable and trustworthy AI systems. Figure 1: Three examples of RL agent applications in Robotics [Ladosz et al., 2024], Human Modeling [Shi et al., 2024], and Collaborative AI [Lingler et al., 2024], highlighting the number of parameters involved in agent design today. Parameters are categorized as Agent, Environment, or Policy. The lists of parameters are non-exhaustive, and practitioners have to choose many more. Our long-term goal is to provide non-ML expect practitioners with the ability to optimize all parameters in a RL system, either jointly or individually, based on their specific problem. This capacity is important, because evidence suggests that even subtle adjustments to the implementation of RL algorithms, like reward clipping or observation normalization, can affect performance [Engstrom et al., 2020]. Furthermore, careful selection of parameters can sometimes improve performance more than the choice of RL algorithm itself [Andrychowicz et al., 2020, Paine et al., 2020, Yang and Yin, 2021]. In embodied RL agents in particular, the deep neural networks have many parameters that are sensitive to optimization [Fetterman et al., 2023]. Figure 1 presents some examples of agents’ applications and some of the parameters that a practitioner would need to choose during their development phase. More generally, these parameters can be roughly categorized in the following classes: 1. Agent design, which control the internal functions of the RL agent, including the discount factor, the entropy coefficient, and the size of the observation window; 2. Environment settings, which define the task itself, such as the field of view, and the reward structure; 3. Policy parameters, such as the architecture of the neural networks, learning rate, and activation functions. Thus, we present AgentForge222A link to the AgentForge platform can be found here: https://github.com/feferna/AgentForge, a flexible low-code platform targeted specifically for developing complex RL systems by domain experts with no solid background in optimization or ML. The focus in a low-code platform is becoming increasingly relevant as professionals in fields outside of ML, such as cognitive and behavioral sciences, are starting to use RL systems to model human cognition and behavior [Eppe et al., 2022, Nobandegani et al., 2022]. Beyond offering a low-code specification of the optimization problem, AgentForge provides rapid iteration by allowing developers to quickly ‘prototype’ an RL design, optimize it, and examine results. Its flexibility builds on its ability to accommodate a wide range of RL systems, from simple fully observable models to more complex, embodied RL agents. Users are only required to define their custom environment, objective function, and the parameters they wish to optimize in two files. The objective function, which guides the optimization process, can be the average reward per episode or any other criterion defined by the practitioner. The platform then automatically converts these inputs into an optimization problem that defines how the agent is trained and evaluated. In this first version, the software offers three popular optimization techniques: Random Search, Bayesian Optimization (BayesOpt) and Particle Swarm Optimization (PSO); however, this set can be extended. Obviously, the challenge of parameter optimization is shared across areas of ML, which has promoted the development of numerous services and frameworks. Notable examples include BoTorch [Balandat et al., 2020], Scikit-Optimize [scikit-optimize, 2024], Optuna [Akiba et al., 2019], and Google Vizier [Song et al., 2023]. However, these platforms primarily cater to users with substantial knowledge of ML and statistics. They are also complex to use with RL agents, because it is not easy to devise an objective function when there are many parameters distributed across different system components. This motivates low-code solutions, such the one we present here. Such tools can enable users to focus on designing effective agents without the burden of mastering optimization techniques, which is especially important given the high dimensionality of sensory inputs and the dynamic environments RL agents operate in. We evaluated AgentForge by jointly optimizing the parameters of a pixel-based version of the classical Lunar Lander agent from the Gymnasium library [Towers et al., 2024], a POMDP where the agent must infer the state from raw pixel values. This agent presents parameters from all three categories cited earlier—agent, environment, and policy—demonstrating AgentForge’s ability to handle diverse and complex parameter sets. Additionally, we show how easily an optimization problem can be specified, highlighting the platform’s easiness and flexibility. The remainder of this paper presents AgentForge in detail and is structured as follows: Section 2 reviews related works on traditional parameter tuning methods, their application in ML application, and limitations. Section 3 details our proposed platform, including what inputs are required from the user and how the optimization is performed. Section LABEL:sec:evaluation describes the evaluation setup, including the application of AgentForge to the proposed pixel-based Lunar Lander agent and its configuration. Section 4 presents the results achieved, while Section 5 discusses the implications for RL agents. Finally, Section 6 summarizes the main findings, discusses current limitations, and suggests directions for future research."
Marked Temporal Bayesian Flow Point Processes,"Marked event data captures events by recording their continuous-valued occurrence timestamps along with their corresponding discrete-valued types. They have appeared in various real-world scenarios such as social media, financial transactions, and healthcare records, and have been effectively modeled through Marked Temporal Point Process (MTPP) models. Recently, developing generative models for these MTPP models have seen rapid development due to their powerful generative capability and less restrictive functional forms. However, existing generative MTPP models are usually challenged in jointly modeling events’ timestamps and types since: (1) mainstream methods design the generative mechanisms for timestamps only and do not include event types; (2) the complex interdependence between the timestamps and event types are overlooked. In this paper, we propose a novel generative MTPP model called BMTPP. Unlike existing generative MTPP models, BMTPP flexibly models marked temporal joint distributions using a parameter-based approach. Additionally, by adding joint noise to the marked temporal data space, BMTPP effectively captures and explicitly reveals the interdependence between timestamps and event types. Extensive experiments validate the superiority of our approach over other state-of-the-art models and its ability to effectively capture marked-temporal interdependence.","Marked event data is widely seen in the real world as a sequence of events, where each event is recorded with a continuous-valued occurrence timestamp and a categorical event type (a.k.a. mark). Its detailed applications include social media [1, 2], where different event types may trigger diverse event patterns; financial transactions [3, 4], where a buy or sell action would result in different transaction times; and healthcare records [5, 6], where the disease types decide the visit times. That is, timestamps and event types exhibit non-trivial interdependence in these scenarios, as they can influence each other in both directions, specified by the detailed situation. Marked Temporal Point Process (MTPP) is a stochastic process that can effectively model marked event sequences. Mainstream MTPP methods can be broadly divided into two categories: classical MTPP models, including Poisson processes [7, 8], Hawkes processes [9] and Self-correcting processes [10], which use an intensity function to characterize the instantaneous occurrence of events given their history. However, these methods often rely on strong parametric or modeling assumptions, limiting their ability to effectively capture complex patterns in event occurrences. On the other hand, neural MTPP models have emerged as a rapidly developing branch in recent years [11, 12, 13]. These models employ neural networks, such as RNNs, LSTMs, or transformers, to encode historical events [14]. In some cases, they draw on intensity functions inspired by processes like the Hawkes process to model event occurrences [15, 16]. Compared to classical MTPP models, neural MTPP models leverage the expressive power of neural networks to better capture the complex dependencies among events. However, applying these two methods to parametric MTPP models requires solving integrals to compute the likelihood function, which usually requires extremely high computational cost [11, 17]. To tackle this issue, two common techniques, namely strong model assumptions and numerical approximation techniques, are typically employed. First, certain model assumptions are introduced, such as treating the timestamp x𝑥xitalic_x and event type m𝑚mitalic_m as independent or conditionally dependent (e.g., x𝑥xitalic_x depends on m𝑚mitalic_m, or m𝑚mitalic_m depends on x𝑥xitalic_x) [18, 19, 15, 20]. This approach simplifies the integral form, thereby reducing computational complexity. However, in reality, x𝑥xitalic_x and m𝑚mitalic_m may not be independent or may have more complex dependence, which can lead to model misspecification and consequently restrict the model’s expressive capacity [21]. Second, numerical approximation techniques, such as Monte Carlo sampling or numerical integration, are used to simplify the computation of integrals when closed-form solutions are not available [22, 23]. Despite this, limitations in sample size and sampling errors mean that these methods only approximate the true solution, which can result in information loss and affect model performance [17]. To fill these gaps, generative MTPP models, which model the target distribution of timestamps using the generative models, have been proposed and have shown promising results [24, 25]. Mainstream generative models, such as diffusion models [26, 27], generative adversarial networks (GANs) [28, 29], and variational autoencoders (VAEs) [30, 31], are energy-based deep probabilistic models, where the optimization objective is an energy function corresponding to an unnormalized negative log-likelihood function. Typically, neural networks are employed to represent this energy function. Since neural networks can approximate complex functions without formal constraints, there is no need to impose model assumptions or use numerical approximation techniques to simplify computation. Consequently, using such generative models in MTPP tasks allows for flexible modeling, enhancing the model’s expressiveness and avoiding information loss due to approximation operations. However, generative MTPP models still face two main challenges. Challenge I: In MTPP tasks, we aim to model the joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ) of two heterogeneous random variables: x𝑥xitalic_x, which is continuous, and m𝑚mitalic_m, which is discrete. However, mainstream generative models, such as diffusion models, are designed for continuous random variables due to their reliance on Gaussian noise [32]. As a result, these models are not directly applicable for modeling joint distributions that include discrete random variables m𝑚mitalic_m [24]. Challenge II: The target joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ) involves two random variables, x𝑥xitalic_x and m𝑚mitalic_m, which exhibit a strong interdependence across different scenarios. For example, discussions about clothing types change with the seasons [14]. Thus, capturing the complex interdependence between timestamps x𝑥xitalic_x and different event types m𝑚mitalic_m is crucial for improving model performance. However, existing generative MTPP models are unable to directly model the joint distribution, often leading to the assumption that x𝑥xitalic_x and m𝑚mitalic_m are independent, and applying the generative model only to x𝑥xitalic_x [24]. This approach neglects the interdependence between the two random variables, ultimately compromising model performance. To tackle the above challenges, we propose a novel generative MTPP model called the Marked Temporal Bayesian Flow Point Process (BMTPP), based on the recently developed generative model, the Bayesian Flow Network (BFN) [33], to approximate the joint distribution of timestamps and event types. First, in contrast to existing generative MTPP models that model the continuous random variable x𝑥xitalic_x only, BMTPP flexibly leverages BFN in a parameter-based manner to model both the timestamps and event types. Second, by adding the joint noise to the marked temporal data, BMTPP can effectively capture the coupling correlation between timestamps and event types in MTPP tasks, explicitly revealing the complex interactions between them. We summarize the contributions as follows: • Based on the Bayesian flow network, we propose a new generative MTPP model, i.e., BMTPP, which can naturally and flexibly model marked event sequences. • BMTPP can directly model the joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ), which consists of continuous timestamps x𝑥xitalic_x and discrete event types m𝑚mitalic_m. Moreover, by employing a joint noise strategy within the marked temporal data space, it can effectively capture and explicitly reveal the interdependence between timestamps and event types. • We evaluate BMTPP on four real-world datasets, demonstrating our proposed approach outperforms other state-of-the-art models, as well as the effectiveness of our joint noise strategy in capturing marked-temporal interdependence."
DMT-HI: MOE-based Hyperbolic Interpretable Deep Manifold Transformation for Unspervised Dimensionality Reduction,"Dimensionality reduction (DR) plays a crucial role in various fields, including data engineering and visualization, by simplifying complex datasets while retaining essential information. However, the challenge of balancing DR accuracy and interpretability remains crucial, particularly for users dealing with high-dimensional data. Traditional DR methods often face a trade-off between precision and transparency, where optimizing for performance can lead to reduced interpretability, and vice versa. This limitation is especially prominent in real-world applications such as image, tabular, and text data analysis, where both accuracy and interpretability are critical. To address these challenges, this work introduces the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI). The proposed approach combines hyperbolic embeddings, which effectively capture complex hierarchical structures, with Mixture of Experts (MOE) models, which dynamically allocate tasks based on input features. DMT-HI enhances DR accuracy by leveraging hyperbolic embeddings to represent the hierarchical nature of data, while also improving interpretability by explicitly linking input data, embedding outcomes, and key features through the MOE structure. Extensive experiments demonstrate that DMT-HI consistently achieves superior performance in both DR accuracy and model interpretability, making it a robust solution for complex data analysis. The code is available at https://github.com/zangzelin/code_dmthi.","Dimensionality reduction [1, 2, 3, 4] simplifies complex datasets while preserving their intrinsic structure [5, 6]. This is crucial for managing high-dimensional data, which presents challenges in computational complexity, storage, and visualisation [7, 8]. Reducing data dimensionality allows for more efficient analysis, pattern recognition, and interpretation [9]. However, balancing high performance [10] and interpretability [11, 12] remains challenging, as efficient processing [13] often conflicts with human interpretability [14, 15]. Figure 1: Overview of the proposed DMT-HI network. The figure compares three dimensionality reduction methods, manifold-based, deep learning-based, and the proposed DMT-HI. DMT-HI leverages a Mixture of Experts (MOE) strategy to efficiently process both image [16] and tabular [17] data, offering better performance, lower training costs, and improved interpretability across different data sizes. Dimensionality reduction methods fall into two categories, manifold-based parameter-free approaches [18, 19, 20] and deep learning-based methods [21]. Manifold-based methods like t-SNE [22] and UMAP [23] are known for their speed (on small dataset) and adaptability [7], projecting high-dimensional data into low-dimensional spaces through nonlinear mappings, revealing underlying structures. Deep learning methods, such as parametric UMAP (PUMAP) [24, 25] and DMT-EV [26], handle complex data more effectively, especially high-dimensional data, leveraging neural networks to capture intricate patterns. DMT-EV, in particular, excels in both performance and explainability [27], pruning irrelevant features for clearer, more interpretable results. Deep learning methods stand out for their ability to scale and generalize well across large datasets, positioning them as central to current dimensionality reduction research. In terms of method efficiency, performance, and interpretability, manifold-based parameter-free methods and deep learning-based methods each have distinct strengths and weaknesses, driven by their theoretical and design differences [7]. Parameter-free methods are more efficient for small datasets with lower time costs since they don’t rely on parametric models and focus on optimizing the output [11]. However, their efficiency declines with increasing data size due to the rising complexity of neighborhood search and distance calculations. In contrast, deep learning methods handle large-scale data more efficiently due to model scalability and hardware acceleration, though their training on small datasets is costlier. In terms of performance, parameter-free methods excel at capturing local structures but struggle with complex global hierarchies due to their reliance on Euclidean space [28]. Deep learning methods, by contrast, can capture both local and global structures through multilayer transformations but require significant data and computational resources. Regarding interpretability, parameter-free methods rely on similarity metrics, making them harder to interpret and inconsistent globally. While deep learning methods can capture richer features, their “black-box” nature and complexity make their decision-making process harder to explain. To address these challenges in global structure characterization and interpretability, As shown in Fig. 1, we propose the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI), which integrates hyperbolic mapping and a Mixture of Experts model (MOE) [29, 30]. Hyperbolic mapping uses negative curvature to better capture complex hierarchical structures and global dependencies, preserving global information in lower dimensions. The MOE strategy enhances performance and efficiency by dynamically assigning tasks to specialized expert networks that handle different input features, thereby avoiding bottlenecks from a single model. This model provides interpretability by allowing users to track expert decisions and understand the internal model workings. Additionally, MOE serves as a bridge between raw data, embedded data, and feature subsets, enabling clear interpretation of how features influence data representations at different levels. By combining hyperbolic mapping’s structural advantages and MOE’s efficient task allocation, DMT-HI aims to improve performance, efficiency, and interpretability, offering a comprehensive solution for reducing the dimensionality of complex data. By integrating these innovations, our approach not only improves dimensionality reduction performance but also enhances interpretability, offering a comprehensive solution for handling complex data types and extracting insights from high-dimensional datasets. DMT-HI’s performance in dimensionality reduction is enhanced through the MOE strategy, which dynamically assigns tasks to the most suitable experts, improving both processing efficiency and overall model performance. Additionally, the redesigned manifold loss optimizes the training process, enabling the model to capture both local and global structures more effectively. Overall, the key contributions of this paper include, • A hyperbolic embedding and deep manifold loss function, which improve the accuracy of dimensionality reduction by better capturing the global structure of data. • The introduction of the MOE strategy, establishing a clear connection between input data, embedding results, and key features, thus enhancing model interpretability and stability. • Comprehensive tests evaluating global and local performance, time efficiency, and other dimensions to validate the advantages of the proposed model."
A neural network approach for solving the Monge-Ampère equation with transport boundary condition,"This paper introduces a novel neural network-based approach to solving the Monge-Ampère equation with the transport boundary condition, specifically targeted towards optical design applications. We leverage multilayer perceptron networks to learn approximate solutions by minimizing a loss function that encompasses the equation’s residual, boundary conditions, and convexity constraints. Our main results demonstrate the efficacy of this method, optimized using L-BFGS, through a series of test cases encompassing symmetric and asymmetric circle-to-circle, square-to-circle, and circle-to-flower reflector mapping problems. Comparative analysis with a conventional least-squares finite-difference solver reveals the competitive, and often superior, performance of our neural network approach on the test cases examined here. A comprehensive hyperparameter study further illuminates the impact of factors such as sampling density, network architecture, and optimization algorithm. While promising, further investigation is needed to verify the method’s robustness for more complicated problems and to ensure consistent convergence. Nonetheless, the simplicity and adaptability of this neural network-based approach position it as a compelling alternative to specialized partial differential equation solvers.","The Monge-Ampère equation is a nonlinear partial differential equation (PDE) with crucial applications across various fields in physics and mathematics. Its general form is given by: det(D2⁢u)=f⁢(x,u,∇u),superscript𝐷2𝑢𝑓𝑥𝑢∇𝑢\det\left(D^{2}u\right)=f(x,u,\nabla u),roman_det ( italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u ) = italic_f ( italic_x , italic_u , ∇ italic_u ) , (1) where u:ℝN→ℝ:𝑢→superscriptℝ𝑁ℝu:\mathbb{R}^{N}\to\mathbb{R}italic_u : blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT → blackboard_R, (N≥1𝑁1N\geq 1italic_N ≥ 1), is an unknown convex function, and D2⁢usuperscript𝐷2𝑢D^{2}uitalic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u represents the Hessian matrix of u𝑢uitalic_u. This equation traces its origins back to the 18th-century work of Gaspard Monge, who studied the problem of optimal resource allocation. Over time, this foundational problem has evolved into what is now known as the optimal transport problem, a concept that naturally emerges in fields such as fluid dynamics and mathematical physics. The Monge-Ampère equation effectively describes the optimal transportation of one mass distribution to another, minimizing a cost function that typically represents the distance over which each mass element must be moved 1. Several optical problems can be formulated as instances of optimal transport. A notable example is the design of a reflector that transforms a given light source distribution into a desired target distribution, a problem that inherently aligns with the principles of optimal transport. In this context, the ’mass’ represents energy, while the cost function corresponds to the optical path lengths of the light rays 2. Numerous variations of this problem can be effectively modeled using the Monge-Ampère equation or its generalized forms. For example, the light source may be planar, emitting a parallel beam, or point-based, radiating in multiple directions. Additionally, optical systems can target near-field or far-field regions and may involve point or parallel targets. Both reflectors and lenses can be described within this framework. For example, we might wish to transform a point source to a far-field target using a freeform reflector; a numerical method for solving this problem using the intersection of confocal paraboloids has been described by De Castro et al. 3. For an in-depth exploration of these variations, see Romijn 4 and Anthonissen et al. 5. To simplify our analysis, we will focus on a specific optical configuration: the parallel-to-far-field reflector system. In this setup, a planar light source emits a parallel beam of light toward a reflector, and our primary concern is the distribution of the reflected light at a significant distance from the reflector. Consequently, we only need to consider the direction of each reflected ray. By applying the Monge-Ampère equation with the transport boundary condition and solving it, we can determine the convex reflector surface that transforms a given source light distribution into the desired target distribution. It is important to note that, mathematically, this problem is identical to both the parallel-to-parallel reflector problem and the parallel-to-far-field lens problem, which can also be addressed using the method presented here. This paper introduces a novel numerical method based on artificial neural networks (ANNs) to solve the Monge-Ampère equation with transport boundary condition. Numerous studies have explored the application of neural networks and automatic differentiation to solve ordinary and partial differential equations. Dissanayake and Phan-Thien 6 pioneered this approach, demonstrating the potential of neural networks for approximating solutions to PDEs. Building on this work, Lagaris et al. 7 presented a method using ANNs to solve initial and boundary value problems by constructing trial solutions that inherently satisfy the given conditions. Aarts and Van Der Veer 8 proposed a method to solve PDEs and their boundary/initial conditions using neural networks, incorporating knowledge about the PDE in the structure and training process of the network. More recently, Michoski et al. 9 presented a comprehensive study on the solving of differential equations using deep neural networks, demonstrating their competitiveness with conventional numerical methods and their potential for parameter space exploration and model enrichment. Building on this rich body of work, Nyström and Vestberg 10 employed ANNs to solve the Dirichlet problem for the Monge-Ampère equation. We extend this approach by incorporating the transport boundary condition and compare our neural network-based method against an existing numerical solver for the Monge-Ampère equation with transport boundary condition11. Furthermore, we examine the effect of various hyperparameters of this neural network method on its performance. Section 2 provides a concise background on the Monge-Ampère equation in the context of the parallel-to-far-field reflector problem, a previously proposed finite-differences-based solver for this problem, and artificial neural networks. In Section 3, we present our extension of this method to incorporate the alternative boundary conditions required for optimal transport problems. To demonstrate the effectiveness of our proposed method, Section 4 presents results for several example problems. As is common in machine learning, numerous hyperparameters influence the accuracy of our method. Thus, Section 5 empirically examines the effects of select hyperparameters on our method’s performance. In Section 6 and Section 7, we conclude by discussing the advantages and limitations of neural network-based methods for solving the Monge-Ampère equation with the transport boundary condition, and explore potential avenues for future research to mitigate these limitations."
,,"1 INTRODUCTION Generative models have achieved impressive performance in scientific applications among many other fields (Noé et al.,, 2019; Butter and Plehn,, 2022; Cranmer et al.,, 2020; Sanchez-Lengeling and Aspuru-Guzik,, 2018). Oftentimes, such systems can depend on external control parameters, such as temperature governing the behavior of thermodynamic systems, coupling constants in physical models, or a tempered likelihood posterior in Bayesian inference (Wuttke et al.,, 2014; Friel and Pettitt,, 2008; Pawlowski et al.,, 2017). A major challenge in such cases is acquiring training data for a complete range of control parameters, which can quickly become infeasible. Figure 1: Our approach to train a conditional normalizing flow pθ⁢(x|c)subscript𝑝𝜃conditional𝑥𝑐p_{\theta}(x|c)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_c ). Left: At c=c0𝑐subscript𝑐0c=c_{0}italic_c = italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, the flow is trained using NLL. Right: By learning the gradient of the distribution with respect to c𝑐citalic_c based on prior knowledge, the distribution learned at c0subscript𝑐0c_{0}italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is propagated to other conditions c≠c0𝑐subscript𝑐0c\neq c_{0}italic_c ≠ italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT without additional training data. In this work, we focus on the task of learning generative models in the case where we have access to the functional form of the unnormalized density, such as learning the distributions of equilibrium states in physical systems like molecules (Boltzmann distributions) or variational inference. We approach this problem by learning a single generative model that takes the external condition c𝑐citalic_c as a parameter: pθ⁢(x|c)≈p⁢(x|c)subscript𝑝𝜃conditional𝑥𝑐𝑝conditional𝑥𝑐p_{\theta}(x|c)\approx p(x|c)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_c ) ≈ italic_p ( italic_x | italic_c ). Several works have attempted to address this problem before. One approach applies architectural restrictions to allow one particular functional form of external parameter dependence (Dibak et al.,, 2022). However, this restriction has recently been shown to incur severe limitations in expressivity (Draxler et al., 2024b, ). Energy-based training has been proposed as another method (Schebek et al.,, 2024; Invernizzi et al.,, 2022; Wirnsberger et al.,, 2020), but can exhibit unfavorable properties like mode-seeking behavior (Minka et al.,, 2005; Felardos et al.,, 2023), which has also been shown to be a problem in practice (Wirnsberger et al.,, 2020). Other works require data to be available at the target parameters (Wang et al., 2022b, ; Wirnsberger et al.,, 2020), which can become prohibitively expensive. We overcome these limitations: We allow learning arbitrary continuous dependencies of the target density on external parameters and train unconstrained architectures. Our central idea is to formulate the training of a conditional probability density as a boundary value problem: The boundary is learned on a fixed reference condition c=c0𝑐subscript𝑐0c=c_{0}italic_c = italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and then the resulting density is extrapolated using the known functional dependence on the condition underlying the problem. Our approach is summarized in Fig. 1. In summary, we contribute: • We introduce TRADE, a method for learning generative models with arbitrary continuous dependencies on external conditions. Learning is enabled by a novel boundary value problem formulation. • TRADE uses unconstrained architectures, facilitating the application to more complex target distributions. • TRADE can be trained data-free or with data only available at the boundary c0subscript𝑐0c_{0}italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, making it an efficient approach in cases where acquiring data for a full range of control parameters is infeasible. • TRADE achieves excellent results in a wide range of experiments including Bayesian inference, molecular simulations and physical lattice models"
Measuring memorization through probabilistic discoverable extraction,redacted \correspondingauthorjamhay@google.com,"Memorization of training data, while potentially beneficial for retaining factual information, presents significant challenges in large language models (LLMs) (Biderman et al., 2024; Duan et al., 2024b; Zhang et al., 2024; Huang et al., 2024; More et al., 2024; Smith et al., 2023; Carlini et al., 2022; Bordt et al., 2024; Duan et al., 2024a; Staab et al., 2023; Shi et al., 2023; Tang et al., 2023; Zanella-Béguelin et al., 2020) 111This paper covers a very restricted definition of “memorization”: whether a generative model can be induced to generate near-facsimiles of some training examples when prompted with appropriate instructions. Models do not “contain” bit-wise or code-wise copies of their training data. Rather, if a model can be induced to generate very close copies of certain training examples by supplying appropriate instructions to guide the model’s statistical generation processes then that model is said to have “memorized” those examples. This is an area of active ongoing research.. The undesirable consequences of memorized data can inadvertently expose sensitive information contained within the training set. This issue has garnered significant attention, leading to the now-common practice of quantifying and reporting training data memorization rates within technical reports introducing new foundation large language models (LLMs) (Reid et al., 2024; Anil et al., 2023; Chowdhery et al., 2023; Team et al., 2024; Kudugunta et al., 2024). One way to measure memorization is to quantify how easily a potential attacker could extract training data by querying the model. This often-used measure of memorization called discoverable extraction (Carlini et al., 2022; Kassem et al., 2024) essentially states that a training example is extractable (or “memorized”) if when split into a prefix and suffix, the model generates a sequence matching the suffix when given the prefix as input. Discoverable extraction is often used as an (approximate) upper-bound to an adversary that has no prior knowledge of the example to be extracted (Nasr et al., 2023). Discoverable extraction has become a popular way of measuring memorization rates of LLMs (Reid et al., 2024; Anil et al., 2023; Chowdhery et al., 2023; Team et al., 2024; Kudugunta et al., 2024), in no small part due to its simplicity; one needs only to check if a model’s completion matches an expected target string. While discoverable extraction is cheap to compute, this has its own drawbacks; by only generating a single sequence and checking for a match with the target, it may miss cases where a match could have been found if more than one sequence was generated. These nuances of what is and is not memorized are too coarsely treated by a working definition like discoverable extraction, and leads us to investigate the following question: Question 1. How can we better measure the extractability of sensitive sequences from large language models? Quantifying memorization – and the associated risks – is a subtle and context-specific problem that a single measurement likely cannot capture in isolation. Prior work has attempted to introduce more complex definitions of memorization that aim to get to the heart of what it means for a model to memorize a training example, but they are often too costly to be practically leveraged. These methods have varying levels of computational cost to empirically estimate memorization in the context of LLMs, but are all more expensive than discoverable extraction (see Section 4). Large language models are probabilistic machines, the output of the model is a probability distribution over tokens that make up a model’s vocabulary. The sequence that is generated is entirely dependent on the choice of sampling algorithm that defines how a token is selected from this distribution. There are many different sampling algorithms that one can choose from (see Fan et al. (2018); Holtzman et al. (2019); Basu et al. (2020); Vijayakumar et al. (2016); Graves (2012); Boulanger-Lewandowski et al. (2013)). Prior works have focused on greedy sampling, which iteratively selects the next token with the largest probability conditioned on the previous tokens. Although greedy sampling selects the most likely next token it may not select the overall most likely sequence; yet, the difference between discoverable extraction rates under greedy and other sampling schemes like beam search was found to be marginal (Carlini et al., 2022). However, users of production large language models are often free to decide which sampling scheme to use. These observations lead to our second research question: Question 2. How does the user-chosen sampling scheme affect extractability rates? Carlini et al. (2022) argue that focusing on sampling schemes that have higher degrees of associated randomness compared to greedy sampling, are “antithetical” to maximizing discoverable extraction, as sampling schemes that encourage diversity in sequence generation will by definition have a higher variation in the sequences that can be sampled. However, higher sequence diversity may be advantageous for extraction given that users may query the model multiple times. Extracting the secret even once out of multiple queries could be highly problematic as the adversary (say a hacker checking credit card numbers) may have external means of verifying which one is correct. It is reasonable to try and quantify the number of sequences that need to be generated before a target example becomes extractable, as this better aligns with how users could interact with a model. In particular, because production language models are deployed with non-greedy based sampling strategies that, by default, encourage diversity in sampled sequences. This idea is not a new one. Carlini et al. (2019) motivate their measure of canary memorization using rank perplexity by considering an adversary who sequentially guesses the potential canaries in order from lowest to highest perplexity. The rank of the true canary measures how many guesses such an adversary would need to make before guessing correctly. While the secret sharer attack only guesses a single canary, it is natural to consider how extraction rates scale with multiple guesses. We find that even when we measure the extraction probability after multiple guesses, the extraction rates on training data remain significantly higher than extraction rates on test data (see Section 5.2). This observation allows us to reason about the absolute risk of extraction as well as the relative risk. In this work, we introduce a probabilistic relaxation of discoverable extraction that resolves the discussed points of tension. This new definition quantifies the number of attempts n𝑛nitalic_n an adversary would need to make to extract a target with a certain probability p𝑝pitalic_p under a given sampling scheme. This provides a more nuanced quantification of memorization, which alleviates the aforementioned drawbacks of discoverable extraction without incurring any additional computational cost compared to discoverable extraction. Our contributions We propose a simple probabilistic definition of extraction called (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction that captures the risk of extracting a target after sampling n𝑛nitalic_n times from an arbitrary sampling scheme. We thoroughly benchmark (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction rates for different sampling schemes, settings of n𝑛nitalic_n and p𝑝pitalic_p, model sizes, and number of target data repetitions, and we make a number of remarkable empirical findings that demonstrate the utility of our definition: • Greedy extraction underestimates training data memorization rates compared to (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction even for modest values of n𝑛nitalic_n and p𝑝pitalic_p (Section 5.1). Moreover, the discrepancy between the two rates increases for larger models and more repetitions of the target data. (Section 5.3) • At every setting of the definition parameters we tried, extraction rates of training data far exceeded baseline extraction rates on test data (Section 5.2). • We show that (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction provides a better comparison of memorization rates across models trained on the same data compared with greedy extraction (Section 5.1). Along the way, we provide extensive discussion how our definition relates to other definitions of memorization (Section 3 and Section 4)."
Improving Inverse Folding for Peptide Design with Diversity-regularized Direct Preference Optimization,"Inverse folding models play an important role in structure-based design by predicting amino acid sequences that fold into desired reference structures. Models like ProteinMPNN, a message-passing encoder-decoder model, are trained to reliably produce new sequences from a reference structure. However, when applied to peptides, these models are prone to generating repetitive sequences that do not fold into the reference structure. To address this, we fine-tune ProteinMPNN to produce diverse and structurally consistent peptide sequences via Direct Preference Optimization (DPO). We derive two enhancements to DPO: online diversity regularization and domain-specific priors. Additionally, we develop a new understanding on improving diversity in decoder models. When conditioned on OpenFold generated structures, our fine-tuned models achieve state-of-the-art structural similarity scores, improving base ProteinMPNN by at least 8%. Compared to standard DPO, our regularized method achieves up to 20% higher sequence diversity with no loss in structural similarity score.","Engineering biopolymers that fold into desired 3D structures, a computational challenge known as inverse protein folding problem, has broad applications in drug discovery and material science (Yang et al., 2023; Dill et al., 2008; Abascal & Regan, 2018). Several approaches for inverse folding have been adopted over the past decades, from molecular dynamics simulations to machine learning approaches (Dauparas et al., 2022b; Shanker et al., 2023; Hsu et al., 2022a; Yi et al., 2023; Correa, 1990). In the standard machine learning approach, a molecular backbone chain serves as input, and a model generates sequences that adopt folding topologies compatible with the reference backbone. Sequences do not necessarily share sequence homology, as multiple diverse sequences can fold into similar structures (Hsu et al., 2022a; Yue & Dill, 1992; Godzik et al., 1993). Peptides, which are small biopolymers comprising 2-50 residues, are interesting targets for inverse folding given their role in diverse biological functions, acting as hormones, neurotransmitters, signalling molecules, or nanostructures assemblers (Chockalingam et al., 2007; Torres et al., 2019; Copolovici et al., 2014; Ulijn & Smith, 2008). Only about 225,000 protein structures have been experimentally determined111Updated figures available at https://www.rcsb.org/stats/growth/growth-released-structures. and made available via the Protein Data Bank (PDB). Training inverse-folding machine learning models in a supervised fashion is a challenging task, due to the complexity of the problem and the limited amount of experimental data. The challenge is aggravated in the peptide domain as fewer than 3.53.53.53.5% PDB structures contain 50 residues or less. In fact, applying the SCOP classification filter in the PDB to display structures labelled as “Peptide” reveals only 509 entries, circa 0.20.20.20.2% of all experimentally determined structures available. In addition to the lack of data, sequences are subject to composition bias. The incidence of certain amino acids may differ depending on the sequence length , as longer proteins have more options for accommodating multiple secondary structures and folding loops (Tiessen et al., 2012). Popular models like ProteinMPNN (Dauparas et al., 2022b), PiFold (Gao et al., 2022) and ESM-IF1 (Hsu et al., 2022b) are primarily trained on data derived from longer proteins, leading to poor performance for peptide design tasks. Additionally, shorter sequences fold into simpler structures. Indeed, Milner-White & Russell (2008) argue that short peptides are notoriously “structureless” and tend to flicker between conformations. For example, a structural conformation of a single alpha helix or beta sheet – or a combination of two or three of them – is not necessarily stable and can fluctuate. Existing inverse folding models optimize sequence residue-recovery accuracy and structural similarity via template modeling (TM) score (Zhang & Skolnick, 2004). However, they often suffer from low sampling diversity (Gao et al., 2023b). Ideally, the inverse folding model generates maximally diverse candidate sequences, as additional design filters, such as synthesizability and thermal stability, reduce the number of potential hits downstream of the design process. To address these issues, we apply Direct Preference Optimization (DPO) (Rafailov et al., 2023), a fine-tuning method, to improve inverse-folding methods for peptide design. To the authors’ knowledge, we are the first to apply DPO to this task. We propose several enhancements to DPO to address specific problems that arise in inverse folding. Particularly, we forward fold generated sequences and derive an online regularized algorithm for optimizing structural similarity to the reference and sequence diversity simultaneously. We show empirically that this algorithm targets the differential entropy in log-probability space. Furthermore, we present a simple reward scaling approach to incorporate scalar reward information, showing that reward scaling adaptively selects a KL divergence penalty (Kullback, S. and Leibler, R. A., 1951) to improve performance on harder structures."
LOCAL: Learning with Orientation Matrix to Infer Causal Structure from Time Series Data,"Discovering the underlying Directed Acyclic Graph (DAG) from time series observational data is highly challenging due to the dynamic nature and complex nonlinear interactions between variables. Existing methods often struggle with inefficiency and the handling of high-dimensional data. To address these research gap, we propose LOCAL, a highly efficient, easy-to-implement, and constraint-free method for recovering dynamic causal structures. LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic DAG equivalent to the ground truth. On this basis, we propose two adaptive modules for enhancing the algebraic characterization of acyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and Dynamic Graph Parameter Learning (DGPL). ACML generates causal masks using learnable priority vectors and the Gumbel-Sigmoid function, ensuring the creation of DAGs while optimizing computational efficiency. DGPL transforms causal learning into decomposed matrix products, capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Extensive experiments on synthetic and real-world datasets demonstrate that LOCAL significantly outperforms existing methods, and highlight LOCAL’s potential as a robust and efficient method for dynamic causal discovery. Our code will be available soon.","Exploration of the underlying causal generation process of dynamic systems is an important task (Cheng et al., 2024a; Gong et al., 2024) for trustworthy machine learning. Unfortunately, it is unethical, impossible due to technical reasons, or expensive to conduct intervention experiments on the dynamic systems of certain domains (Li et al., 2023a; Cai et al., 2023a). Another challenge is to infer about the structure which may be high dimensional and nonlinear. Some recent works (Pamfil et al., 2020; Sun et al., 2023; Gao et al., 2022; Fan et al., 2023) have made significant efforts by employing dynamic Bayesian networks (DBNs) with observational and interventional data: among dynamic systems, as illustrated in Figure 1. The variable xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at timestep t𝑡titalic_t is affected by which variables xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT at the same time step (instantaneous dependency) and which variables xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT at the previous timestep (lagged dependency)? This question highlights the crucial roles of those algorithms in the interpretable performance of the trained models. Figure 1. Illustration of instantaneous dependency (solid lines) and lagged dependency (dashed lines) dependencies in a DBN with d=3𝑑3d=3italic_d = 3 nodes and autoregression order p=2𝑝2p=2italic_p = 2. For clarity, we display edges that do not influence the variables at time t𝑡titalic_t in a lighter shade. In order to study the nonparametric DBN, DYNOTEARS (Pamfil et al., 2020) (i.e., a score-based approach to learning DBNs with a differentiable DAG constraint (Zheng et al., 2018)) was proposed as a proxy to capture the learn the parents of child variables. However, in Section 4.2, our practical analysis shows that the DYNOTEARS algorithm and its extensions (Sun et al., 2023; Gao et al., 2022; Fan et al., 2023) adopting matrix exponential constraints require an extremely long time to optimize in high-dimensional dynamic systems, even if they smartly adopt interventional data to enhance the identifiability (Li et al., 2023b, 2024). Then, it is natural to need a proxy model that can fasterly infer dynamic causal structures in high-dimensional situations, which is also the main goal of our work. Recently, much of the research on dynamic causal structure learning has concentrated on applying soft sparsity and DAG constraints. For instance, Golem (Ng et al., 2020) formulated a likelihood-based score function for handling the causality of thousands of nodes. Yu et al. (Yu et al., 2023) extended it for recovery dynamic causal structure. Concurrently, (Fang et al., 2024) verified the feasibility of further accelerating the learning of DAG based on this likelihood function both theoretically and experimentally. These approaches are aimed at enhancing flexibility and scalability in high-dimensional and circumventing rigid structural constraints. Building on these insights, we propose a novel framework for Learning with Orientation matrix to infer CAusaL structure from time series data, which we call LOCAL. We develop a quasi-maximum likelihood-based dynamic structure learning method with identifiability guarantee. Powered by this quasi-maximum likelihood-based objective, we propose to enhance the algebraic characterization of acyclicity with two adaptive modules for causal structure recovering task: 1) an Asymptotic Causal Mask Learning (ACML) module which leverages learnable priority vectors (𝒑𝒑\boldsymbol{p}bold_italic_p) and the Gumbel-Sigmoid function to generate causal masks, ensuring the creation of directed acyclic graphs (DAGs) while optimizing computational efficiency; 2) a Dynamic Graph Parameter Learning (DGPL) module to transform causal learning into decomposed matrix products (𝑾=𝑬s⁢𝑬tT𝑾subscript𝑬𝑠subscriptsuperscript𝑬𝑇𝑡\boldsymbol{W}=\boldsymbol{E}_{s}\boldsymbol{E}^{T}_{t}bold_italic_W = bold_italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_italic_E start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT), effectively capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Those leads us can exploit faster gradient-based optimization, such as Adam (Kingma and Ba, 2015), and GPU acceleration. Contribution. The main contributions of this paper are as follows: • To the best of our knowledge, LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic causal structure and shows that more robust and accurate. • We proposed two adaptive modules, ACML and DGPL, which further liberate the matrix exponential operations required for causality. • We conducted extensive experiments on several synthetic datasets and real-world benchmarks, which proved that LOCAL outperforms state-of-the-art by a significant margin."
Accelerating AI Performance usingAnderson Extrapolation on GPUs,"We present a novel approach for accelerating AI performance by leveraging Anderson extrapolation, a vector-to-vector mapping technique based on a window of historical iterations. By identifying the crossover point where a mixing penalty is incurred, the method focuses on reducing iterations to convergence, with fewer more compute-intensive but generally cacheable iterations, balancing speed and memory usage with accuracy and algorithmic stability, respectively. We demonstrate significant improvements, in both training and inference, motivated by scalability and efficiency extensions to the realm of high-performance computing (HPC).","Anderson extrapolation [1, 2, 27, 33, 16] has recently been applied to deep equilibrium models (DEQs) [7, 8, 9, 10, 24, 17]. Kolter et al. [34] found the gains not substantial due to early termination with a loose convergence tolerance. They focused on Anderson extrapolation during training. Here, we show significant acceleration of AI performance with Anderson on GPUs for both the forward pass (running inferences faster) and training (generating models faster). We demonstrate acceleration of the forward pass with standard Anderson as a baseline for future work with stochastic variants [30] and accelerating the backward pass with Jacobian-free methods like Jacobian-Free Backpropagation (JFB) and Neumann series gradient approximations [16]. Figure 2: AI carbon footprint projected to consume >2% of global electricity demand [3, 15, 28, 25], amounting to >10% of global electricity demand for data centers and infrastructure. As AI demand grows, as shown in Fig. 2 [3, 15, 28, 25], high-performance computing (HPC) is becoming critical due to economic pressures from the growth of data and AI infrastructure [29]. Low-memory acceleration techniques, like Anderson extrapolation, will be key to increasing HPC-based AI computational efficiency. This study investigates matrix-free Anderson extrapolation on GPUs, emphasizing gains from advanced computing architectures compared to CPUs. Our goal is to maximize computational efficiency while reducing iterations to convergence by reusing previous iterations to avoid unnecessary gradient calculations, gaining benefits expected from second-order methods (e.g., [32]) without manipulating Hessian matrices. The environmental impact of AI is rapidly growing [3, 15, 28, 25]. By 2030, AI is projected to account for 2% of global electricity consumption. We aim to reduce this impact by up to 90%, saving 160 terawatt-hours per year by 2030. The carbon footprint of AI exceeds the 500-megaton annual benchmark set by initiatives like Bill Gates’ Breakthrough Energy [14]. Efficiency-enhancing technologies like GPU and Anderson acceleration can reduce AI’s carbon emissions by 60 gigatons per year by 2030, as shown in Fig. 2. 1.1 Leveraging extrapolation for AI and HPC advances Anderson extrapolation, a windowing technique for accelerating nonlinear fixed point iterations, is widely applied in fields like density functional theory, kinetic theory, and climate spin-up. It is well-suited for distributed memory parallelization and GPU implementation. It is a staple of major open-source large-scale solver libraries, including PETSc [11, 12], SUNDIALS [23], Trilinos [19, 22, 21, 20], and deal.II [13, 4, 5, 6]. It can be applied to machine learning training, smoothing out standard forward iterations and achieving superior accuracy in training and testing error. Benchmarking results on CIFAR10 show expected robustness benefits and allow characterization of the temporal advantages or disadvantages from the higher cost per iteration, where a small residual minimization step is applied at each new function evaluation. Figure 3: Mathematical formulation and vector representation. Adapted from Y. He & H. De Sterck. ""Linear Asymptotic Convergence Analysis of Anderson Acceleration, with Krylov Formulation in the Linear Case"" Copper Mountain Conference (2022), ICERM Workshop (2023). Available at: https://www.bilibili.com/video/BV1Wa411i77y/ and https://icerm.brown.edu/video_archive/?play=3320 Figure 4: Deep equilibrium neural network model architecture (Source: NeurIPS Tutorial, 2020 [34]). f⁢(z,x)=norm⁢(ReLU⁢(z+norm⁢(x+W2∗(norm⁢(ReLU⁢(W1∗z))))))𝑓𝑧𝑥normReLU𝑧norm𝑥subscript𝑊2normReLUsubscript𝑊1𝑧f(z,x)=\mathrm{norm}(\mathrm{ReLU}(z+\mathrm{norm}(x+W_{2}*(\mathrm{norm}(% \mathrm{ReLU}(W_{1}*z))))))italic_f ( italic_z , italic_x ) = roman_norm ( roman_ReLU ( italic_z + roman_norm ( italic_x + italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∗ ( roman_norm ( roman_ReLU ( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∗ italic_z ) ) ) ) ) ) ""norm"" here is a group norm, representing a statistical normalization [31]. 1.2 Balancing memory and stability Fundamental tradeoffs exist between memory capacity, memory bandwidth, communication cost, and algorithmic characteristics of stability and convergence rate. The tradeoffs are generally resolved to minimize time to solution. GPUs attain high memory bandwidth advantages over CPUs at the cost of smaller memory capacity. Anderson extrapolation promotes fewer, more expensive steps, reusing cached state-vector data. In distributed memory implementations, it produces convergence with fewer interprocessor communication steps. It has tuning parameters such as window size and damping that can be tuned to application and architecture. We are assessing its utility in machine learning more broadly at a time of emergent CPU-GPU superchips. 1.3 Deep equilibrium neural network models Deep equilibrium models (DEQs) are the continuum limit of explicit neural networks as the number of layers approaches infinity [26], approximating many explicit layers with a single, implicit layer with exponentially fewer parameters using a backward pass including the output. This reduces the inverse problem in parameter space to a fixed-point iteration problem, enabling the usage of nonlinear, vector-to-vector mapping techniques to compute the fixed-point iterations that converge to the deep equilibrium state parameters by minimizing the loss function. With gains in memory and acceleration, DEQs are fit for large-scale computer vision and natural language processing tasks and benefit more from matrix-vector operation-optimized computing architectures like GPUs and CPU-GPU superchips. The standard approach using forward iteration for fixed-point iteration problems often does not efficiently converge to the fixed point and suffers from initially slow error reduction and local minimum trapping in nonlinear problems like deep neural networks. Vector-to-vector mapping techniques like Anderson extrapolation outperform standard forward iteration by combining information from previous iterations to span a searchable subspace to extrapolate the next iteration, enhancing convergence rates at the expense of memory usage in each iteration. DEQs represent any neural network at arbitrary depths and connectivities with a single implicit layer consuming vastly fewer parameters with faster forward passes for accelerated training and inferences. The implicit function theorem shows how gradients can be computed in the DEQ framework, facilitating backpropagation through the equilibrium state [8, 34]. DEQs provide a framework for accelerating deep learning, extending the capacity of deep networks within a single-layer architecture through fixed-point computations and advanced root-finding algorithms. Their amenability to convergence acceleration with techniques like Anderson positions DEQs as a robust method to reduce computation needed to build state-of-the-art models and scale up beyond current computational limitations."
Computational Bottlenecks of TrainingSmall-scale Large Language Models,"While large language models (LLMs) dominate the AI landscape, Small-scale large Language Models (SLMs) are gaining attention due to cost and efficiency demands from consumers. However, there is limited research on the training behavior and computational requirements of SLMs. In this study, we explore the computational bottlenecks of training SLMs (up to 2B parameters) by examining the effects of various hyperparameters and configurations, including GPU type, batch size, model size, communication protocol, attention type, and the number of GPUs. We assess these factors on popular cloud services using metrics such as loss per dollar and tokens per second 111We use average dollar cost ratios of cloud instance types based on publicly available pricing (Appx. A).. Our findings aim to support the broader adoption and optimization of language model training for low-resource AI research institutes.","Large Language Models (LLMs) are becoming increasingly popular in various fields due to their performance on a variety of tasks [6, 18, 8, 20, 5]. However, deploying large models widely such as on mobile hardware and edge devices is challenging due to the large memory and compute requirements [11, 12, 10]. These constraints have driven a growing interest in smaller language models (such as ≤2⁢Babsent2𝐵\leq 2B≤ 2 italic_B parameters) as a viable alternative [24, 16, 23]. Recent work refer to these models as Small-scale large Language Models (SLMs) which can work well in environments where cost-efficiency and resource limitations are of significant concern, as well as on servers where the reduced cost of inference will be a dominant factor to attract and retain customers. SLMs have demonstrated substantial potential in achieving competitive results despite their smaller size. Techniques such as pruning, distillation, and quantization have been employed to enhance their performance [2, 3, 17], allowing SLMs to perform on par with, and in some cases surpass, much larger models [4]. For example, Gemma-2B outperforms the largest OPT-175B [25], challenging the notion that sheer model size is the primary determinant of effectiveness. In addition to on par accuracy, SLMs can meet consumer demands for fast, efficient, and cost-effective AI without sacrificing task performance, making them increasingly attractive for organizations with limited computational budgets, such as small businesses and academic institutions. While prior work mostly focused on optimizing SLMs for inference [15], relatively little attention has been paid to their training dynamics. This gap is significant, as the computational and infrastructure demands of training LLMs may not translate to SLMs. Given the diverse range of hardware configurations available on cloud platforms—such as GPU type, batch size, and communication protocols—there is a need for a systematic analysis of how these factors impact the training efficiency of SLMs, particularly when measured in terms of practical metrics such as loss per dollar and tokens per second. Our findings indicate that for smaller models, more affordable options like A100-40GB GPUs and Distributed Data Parallel (DDP) can be utilized without sacrificing performance. For larger models, advanced configurations, such as A100-80GB and H100-80GB GPUs paired with Flash Attention (FA) and Fully Sharded Data Parallel (FSDP), are necessary to handle larger batch sizes and prevent memory-related issues. Recent advancements in the field underscore the importance of scaling AI systems not only for state-of-the-art performance but also for practical applications in real-world environments. The emerging trend toward SLMs suggests that a re-evaluation of hardware and computation strategies is essential. The contribution of this paper is to address the need for such evaluation, providing a systematic study on the computational bottlenecks and cost-efficiency of training SLMs up to 2B parameters on various cloud infrastructure and setups. We find that 1) FlashAttention is significantly more important for SLMs than LLMs, 2) Expensive hardware, e.g., H100-80GB and A100-80GB, is not necessarily cost effective for SLM training, 3) DDP is the best distributed training scheme for SLMs, and 4) Maximizing GPU memory utilization is not cost-optimal for SLM training."
Gradient Descent Efficiency Index,"Gradient descent is a widely used iterative algorithm for finding local minima in multivariate functions. However, the final iterations often either overshoot the minima or make minimal progress, making it challenging to determine an optimal stopping point. This study introduces a new efficiency metric, Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, designed to quantify the effectiveness of each iteration. The proposed metric accounts for both the relative change in error and the stability of the loss function across iterations. This measure is particularly valuable in resource-constrained environments, where costs are closely tied to training time. Experimental validation across multiple datasets and models demonstrates that Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT provides valuable insights into the convergence behavior of gradient descent, complementing traditional performance metrics. The index has the potential to guide more informed decisions in the selection and tuning of optimization algorithms in machine learning applications and be used to compare the “effectiveness” of models relative to each other.","In the field of machine learning, optimizing the training process of models is crucial for achieving high performance while minimizing computational resources. Gradient descent [1] is a widely used optimization algorithm due to its simplicity and effectiveness in finding local minima of differentiable functions. However, the efficiency of gradient descent can diminish with large datasets and prolonged training periods, where additional iterations provide negligible improvements. This raises the need for a robust mechanism to identify the optimal stopping point, ensuring efficient use of computational resources. Fig. 1 is a contour plot that illustrates how each step taken in gradient descent towards a local minimum is smaller than the previous one. This approach quickly returns diminishing results, making the last few steps cost more computationally than they yield in accuracy. Figure 1: Contour plot of cost b,w with path of gradient descent J⁢(w,b)𝐽𝑤𝑏J(w,b)italic_J ( italic_w , italic_b ). In Fig. 2, notice how the first 100 steps exponentially decrease (or logarithmically increase) the cost. However, if you zoom out to 100,000 steps, the curve effectively flattens out before 10,000 steps in this particular example. Figure 2: Comparison of cost with different domain restrictions. The “Gradient Descent Efficiency Index” is a novel ratio between training parameters that includes the relative change in gradient norm, the initial learning rate, the learning decay rate, absolute change in coefficients, and the number of iterations. Note that throughout this paper, I will use the name “Gradient Descent Efficiency Index”, the short form “GDEI”, and the function Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT interchangeably."
Generative Diffusion Models for Sequential Recommendations,Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).,"Recommender systems are algorithms that suggest items to users by analyzing various forms of input data. Their primary goal is to enhance the customer experience through personalized recommendations, often based on prior implicit feedback. These systems track user behaviors, such as purchase history, viewing habits, and browsing activity, to model user preferences. Sequential Recommendation, a specific type of recommendation, is particularly relevant for applications where user behavior is naturally sequential. It focuses on predicting the next item a user will interact with by considering the order of previous interactions. Mainstream solutions to Sequential Recommendation (SR) [2] represent items with fixed vectors, which have a limited ability to capture the latent aspects of items and the diversity of user preferences. Generative models like Generative Adversarial Networks (GANs) [3] and Variational Auto-Encoders (VAEs) [4] have been widely applied in personalized recommendations, using adversarial training and encoder-decoder architectures, respectively, to model user behavior and preferences. However, Diffusion Models have shown significant advantages over GANs and VAEs, such as greater stability and higher generation quality in various tasks. Diffusion Models (DMs) [5, 6, 7] have achieved state-of-the-art results in image synthesis tasks [7, 8, 9, 10, 11]. These models alleviate the trade-off between stability and quality by gradually corrupting images in a forward process and iteratively learning to reconstruct them. DMs progressively corrupt 𝐱0subscript𝐱0\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with random noise and then recover 𝐱0subscript𝐱0\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT from the corrupted state 𝐱Tsubscript𝐱𝑇\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT step by step. This forward process creates a tractable posterior [6], enabling the iterative modeling of complex distributions through flexible neural networks in the reverse generation process. The objectives of recommender models align well with DMs, as recommendation essentially involves inferring future interaction probabilities from noisy historical interactions, where noise represents false-positive and false-negative items [12, 13]. This makes DMs a promising approach for accurately modeling complex interaction patterns with strong representational ability. Despite their success in other domains, applying diffusion models to recommender systems remains underexplored. We further explore diffusion models for sequential recommendation (SR) by extending the method introduced by Li et al. (2023). Our work proposes significant enhancements to the existing architecture, resulting in a new model, DiffuRecSys111https://youtu.be/bEpDfAAGL2I. Specifically, our contributions are as follows: • Enhancing the Diffusion Recommender Model: We incorporate cross-attention mechanisms within the Approximator of the model architecture. The model isn’t just learning temporal dependencies (the sequential order of items) but also more complex relationships between past interactions and the target item by focusing on the most relevant past events. • Incorporation of Offset Noise: We introduce offset noise into the diffusion process to increase model robustness and effectively handle variability in user interactions. • Comprehensive Experimental Validation: We conduct extensive experiments across three datasets under various settings, demonstrating improvements of DiffuRec with our extensions over standard baselines."
,,"Deep generative models – for example VAEs, GANs, normalizing flows, diffusion models, and flow matching – have recently made great progress in probability density learning (DL), with flow matching achieving highest accuracy at the moment. Besides generative accuracy, human interpretability of the learned representation is highly desirable, and disentangled representation learning (DRL) is a key tool for this, see Bengio et al., (2014) and Wang et al., (2024). Intuitively, DRL means that each latent variable should effect only a single, distinct semantic property of the generated data instances. We consider the problem of measuring if and to what degree a given model has actually learned a disentangled representation. Most prior work addresses this question in a supervised setting, where the true generative factors are known (see Related Work). Since this assumption is often violated in the real world, we instead focus on the unsupervised case. That is, we do not ask if the model has learned the (unknown) true factors, but only if it has learned any disentangled representation at all. The learned representation might be close to the true one, if certain identifiability conditions are fulfilled (see Related Work), but this is beyond the scope of our paper. Our work rests on the manifold hypothesis which states that data in a D𝐷Ditalic_D-dimensional space often reside near a manifold ℳℳ\mathcal{M}caligraphic_M of much lower dimension d≪Dmuch-less-than𝑑𝐷d\ll Ditalic_d ≪ italic_D. Variations along the manifold correspond to semantically important differences between data instances, whereas off-manifold variations are considered as unimportant or noise. This is familiar from PCA, where one interprets directions of high data variability as important, whereas directions of low variability are irrelevant. PCA achieves this under the assumption that the manifold ℳℳ\mathcal{M}caligraphic_M is a linear subspace, and DRL seeks to generalize this to non-linear models. If the important dimensions indeed span the manifold ℳℳ\mathcal{M}caligraphic_M, the representation is called aligned. Our method clearly highlights Alignment by assigning a high manifold entropy to the important features and low to unimportant ones, see fig. 1. Moreover, it allows sorting of latent variables by importance so that the cut-off between important and irrelevant can be adjusted later according to the needs of an application, analogous to PCA’s ordering by variance. Figure 1: The two moons distribution illustrates how manifold entropic metrics quantify DRL in terms of alignment and disentanglement. (Top left) The latent prior and a Cartesian grid spanned by the latent variables Zcsubscript𝑍𝑐Z_{c}italic_Z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and Zdsubscript𝑍𝑑Z_{d}italic_Z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue). The latent distribution is mapped to data space by three generative models with equal accuracy, but vastly different representations. This can be seen by the differences in the transformed grid spanned by the manifold random variables 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and 𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue) in the top row, and the corresponding values of our metrics manifold entropy H⁢(𝑿c)𝐻subscript𝑿𝑐H({\boldsymbol{X}}_{c})italic_H ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ), H⁢(𝑿d)𝐻subscript𝑿𝑑H({\boldsymbol{X}}_{d})italic_H ( bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) and manifold mutual information ℐ⁢(𝑿c,𝑿d)ℐsubscript𝑿𝑐subscript𝑿𝑑\mathcal{I}({\boldsymbol{X}}_{c},{\boldsymbol{X}}_{d})caligraphic_I ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) in the bottom row. The total entropy of the distribution (gray) is the signed sum of the three terms. (A) The latent manifolds are entangled (and thus not interpretable), and our metric indicates this by high mutual information (brown). (B) The latent manifolds are locally orthogonal everywhere and have low mutual information. However, alignment is inconsistent (𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT aligns with the upper moon, 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT with the lower), resulting in comparable manifold entropy of both variables. (C) The representation is disentangled and aligned. The manifold entropy is high for the important variable 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and low for the noise variable 𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue), and their mutual information is small. Disentanglement, i.e. the statistical independence between latent factors, has been addressed by Independent Component Analysis (ICA, Comon,, 1994) under the assumption of a linear data-generating process (DGP), 𝒙=𝑨⁢𝒔𝒙𝑨𝒔{\boldsymbol{x}}=\boldsymbol{A}\boldsymbol{s}bold_italic_x = bold_italic_A bold_italic_s. When linearity holds, the true generative factors 𝒔𝒔\boldsymbol{s}bold_italic_s are identifiable if they are independent and follow a non-Gaussian distribution. However, identifiability is generally lost for non-linear DGPs 𝒙=Φ⁢(𝒔)𝒙Φ𝒔{\boldsymbol{x}}=\Phi(\boldsymbol{s})bold_italic_x = roman_Φ ( bold_italic_s ), (Hyvärinen and Pajunen,, 1999). Finding conditions on 𝒔𝒔\boldsymbol{s}bold_italic_s to restore identifiability is a major focus of current research, see (Hyvarinen et al.,, 2023) for a recent survey. Alternatively, one can restrict the class of permitted mixing functions ΦΦ\Phiroman_Φ, and this approach primarily inspired the present work. Independent Mechanism Analysis (IMA, Gresele et al.,, 2022) postulates, for example, that the Jacobian of ΦΦ\Phiroman_Φ should be orthogonal in every point, and Principal Component Flows (Cunningham et al., 2022b, ) realize this idea by adding a loss term to normalizing flow training that encourages orthogonality. It can be shown that orthogonality of the Jacobian is equivalent to minimizing the mutual information between the image of the corresponding features in data space after mapping them through the decoder (see fig. 1 and Appendix). This is crucial: In contrast to supervised disentanglement metrics, which are usually defined with respect to the encoder, meaningful unsupervised metrics must be defined in terms of the decoder mapping from latent to data space. Specifically, we make the following contributions: • We propose a set of information-theoretic metrics for DRL defined on the decoder of a generative model. • We introduce Alignment as an important complementary condition to Disentanglement in the IMA framework • We show the usefulness of our metrics at dissecting generative models in order to reveal and quantitatively measure their behaviours."
Robust Time Series Causal Discovery for Agent-Based Model Validation,"Agent-Based Model (ABM) validation is crucial as it helps ensuring the reliability of simulations, and causal discovery has become a powerful tool in this context. However, current causal discovery methods often face accuracy and robustness challenges when applied to complex and noisy time series data, which is typical in ABM scenarios. This study addresses these issues by proposing a Robust Cross-Validation (RCV) approach to enhance causal structure learning for ABM validation. We develop RCV-VarLiNGAM and RCV-PCMCI, novel extensions of two prominent causal discovery algorithms. These aim to reduce the impact of noise better and give more reliable causal relation results, even with high-dimensional, time-dependent data. The proposed approach is then integrated into an enhanced ABM validation framework, which is designed to handle diverse data and model structures. The approach is evaluated using synthetic datasets and a complex simulated fMRI dataset. The results demonstrate greater reliability in causal structure identification. The study examines how various characteristics of datasets affect the performance of established causal discovery methods. These characteristics include linearity, noise distribution, stationarity, and causal structure density. This analysis is then extended to the RCV method to see how it compares in these different situations. This examination helps confirm whether the results are consistent with existing literature and also reveals the strengths and weaknesses of the novel approaches. By tackling key methodological challenges, the study aims to enhance ABM validation with a more resilient valuation framework presented. These improvements increase the reliability of model-driven decision making processes in complex systems analysis.","Chapter 1 Introduction 1.1 Agent-Based Models: An Overview In the modern era, Agent-Based Models (ABMs) fall under the class of modelling and simulation techniques which are increasingly being used in domains such as theoretical economics, finance, social sciences, and epidemiology. It involves a bottom-up approach by putting together individual agents and their interactions so that various phenomena may be synthesised and then examined. The detailed methodologies of such models allow them to understand complexity and draw futuristic conclusions. Recent literature stresses the significant advantages of agent-based models (ABMs) over traditional economic models. Fagiolo (2019) pinpoints the major strengths of ABMs: their capacity to provide comprehensive narratives of interactions among agents with network structures, incomplete information learning processes, and competition in imperfect markets—and the flexibility they provide in validating both model inputs and outputs [1]. This characteristic has gained much attention and prompted much research activity recently. 1.2 The Importance of ABM Validation To successfully implement ABMs in reality, the “validation” of this model could be the decisive factor for its ability to truly reflect real-world reality. The validation process consists of comparing the model output with the actual data obtained in the real world to make sure it is reliable and effective. This process is utilised to establish the credibility of the model and the reliability of the predictions made. Validation is particularly challenging in fields with complex interactions and non-linear dynamics, such as finance. As Windrum et al. (2007) pointed out, significant effort is still needed to realise consistent and satisfactory techniques of ABM method implementation to real-world financial data [2]. A key component in ABM validation is to find the cause-and-effect mechanisms from data. Besides highlighting the importance of correlational testing, causal matching between the ABM outputs and real-world data has recently been emphasized in validation. These approaches aim to understand and explain the origins and propagation of observed phenomena in financial systems [3]. The details of such causal discovery methods and their application in ABM validation will be further discussed in the following chapters. 1.3 Challenges to Address Indeed, in spite of continuous progress in the ABM validation techniques, there are still the most significant challenges in such complex systems applications: 1. Insufficient Robustness of Time Series Causal Discovery Methods: The causal discovery approaches that are commonly used today, such as VAR-LiNGAM and PCMCI, can be quite susceptible to noise and variations in the data. This could create inconsistencies when the method is applied to different subsets of the same dataset or datasets with slightly different characteristics. For ABM validation purposes, the robustness of these techniques should be enhanced. False causality in this regard could be as harmful as wrong conclusions drawn up about the ABM system’s validity and, consequently, about the underlying system mechanisms. 2. Lack of Comprehensive Understanding of Dataset Characteristics’ Impact: While previous studies have examined specific dataset characteristics, there is a lack of comprehensive understanding of how various dataset properties collectively affect the performance of causal discovery methods in ABM validation. The absence of a systematic comparison across a wide range of dataset types (e.g., linear vs. non-linear, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary, sparse vs. dense causal structures) hinders our ability to select appropriate validation techniques and interpret results accurately. This deficiency stands as a huge constraint in the area of creating functional ABM validation processes as a solution to the wide spectrum of complicated problems. 3. Limitations in Existing ABM Validation Frameworks: Even though much progress has been made in ABM Validation, for instance, the framework proposed by Guerini et al. (2017) [3], current approaches still face several key limitations: (a) Insufficient Dataset Property Analysis: Existing frameworks often lack comprehensive tests for some important dataset properties. For instance, some of these may overlook important characteristics such as linearity and stationarity, which are essential for understanding the nature of the data and selecting appropriate modelling techniques. (b) Limited Options for Causal Discovery Methods: Most current frameworks rely on a single or limited set of causal discovery methods. This limitation may prevent us from obtaining optimal performance when dealing with different dataset characteristics or different priorities (such as accuracy vs. efficiency). The lack of method diversity limits the framework’s adaptability to various scenarios and data types. (c) Narrow Range of Performance Metrics: The existing validation framework typically focuses on basic similarity tests or a limited set of performance metrics. This may cause to failure to capture the full range of model performance, especially in complex financial systems where causal relationships can be intricate. Addressing these challenges is crucial for advancing ABM validation in complex systems like financial markets. Overcoming these limitations will lead to more reliable models, enhancing decision-making processes and informing policy decisions. These challenges necessitate innovative approaches in causal discovery and model validation techniques applicable to a wide range of complex systems. 1.4 Novel Approaches and Contributions This research addresses the challenges mentioned above by utilizing several new approaches and providing the following key contributions: 1. Robust Cross-Validated (RCV) Causal Discovery Method (Chapter 3): We introduce a novel approach to enhance the robustness of existing causal discovery methods. By applying cross-validation techniques to causal discovery algorithms such as VAR-LiNGAM and PCMCI, we aim to mitigate the sensitivity of these methods to noise and data variations to improve the consistency and reliability of causal structure identification in complex time series data. 2. Comprehensive Experimental Evaluation and Analysis (Chapter 4): We present a thorough empirical analysis of our proposed methods and existing approaches. This evaluation covers a wide range of characteristics, including linear vs. non-linear relationships, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary behaviour, and sparse vs. dense causal structures. We also examine the scalability of methods with varying numbers of variables and time series lengths. We ran experimental evaluations on both synthetic datasets with controlled properties and a complex simulated fMRI dataset, so as to provide insights into method performance under various conditions. 3. Context-Aware ABM Validation Framework (Chapter 5): Extending the work done by Guerini, we develop an enhanced ABM validation framework that addresses the weaknesses of existing one. In this framework, the user could choose a suitable method of causal inference based on the property of data sets or other validation needs, such as efficiency or accuracy. Another improvement we feature is to include a more comprehensive set of performance metrics for assessing the causal relations to get a more precise evaluation of model performance. This framework is designed to pre-process datasets and ensure their uniformity, analyse dataset attributes, run user-dependent or driven causal structure detection, and enhance validation evaluations. These contributions enhance ABM validation in complex systems by improving causal discovery methods and offering a flexible validation framework. This research aims to increase the accuracy and reliability of ABMs in capturing real-world dynamics across various domains. 1.5 Report Structure The remainder of this report is structured as follows: Chapter 2 provides the foundational background and reviews related work, covering Agent-Based Models in finance, existing ABM validation techniques, causal discovery methods and the current limitations. Chapter 3 introduces our novel Robust Cross-Validated (RCV) Causal Discovery Approach, detailing its theoretical foundations and implementation. Chapter 4 presents our experimental evaluation and analysis, including synthetic dataset generation, comparative analysis of causal discovery methods, and an application to a complex simulated fMRI dataset that mimics real-world neuroimaging data. Chapter 5 describes our Context-Aware ABM Validation Framework, explaining how it integrates improved causal discovery methods and enhances overall validation reliability. Finally, Chapter 6 concludes the report, summarizing key findings, discussing implications for ABM validation in complex systems and suggesting future research directions."
An Auditing Test to Detect Behavioral Shift in Language Models,"As language models (LMs) approach human-level performance, a comprehensive understanding of their behavior becomes crucial. This includes evaluating capabilities, biases, task performance, and alignment with societal values. Extensive initial evaluations, including red teaming and diverse benchmarking, can establish a model’s behavioral profile. However, subsequent fine-tuning or deployment modifications may alter these behaviors in unintended ways. We present a method for continual Behavioral Shift Auditing (BSA) in LMs. Our test compares model generations from a baseline model to those of the model under scrutiny and provides theoretical guarantees for change detection while controlling false positives. The test features a configurable tolerance parameter that adjusts sensitivity to behavioral changes for different use cases. We evaluate our approach using two case studies: monitoring changes in (a) toxicity and (b) translation performance. We find that the test is able to detect meaningful changes in behavior distributions using just hundreds of examples.","Language models (LMs) can now achieve human-level performance in a wide range of tasks, including text summarization, machine translation, coding and even acting as AI scientists: generating hypotheses and designing experiments (Achiam et al., 2023; Katz et al., 2024; Lu et al., 2024; Zhang et al., 2024). Because of this, many sectors are looking for ways to use them to improve existing systems (Kasneci et al., 2023; Felten et al., 2023). Unfortunately, one large roadblock to broad LM adoption is their propensity to generate harmful content (Weidinger et al., 2021). For example, GPT-3 has significant anti-Muslim biases (Abid et al., 2021), and GPT-4 has racial and gender biases (Zack et al., 2024). To address this, a significant effort is going into ensuring LM behavior is aligned with our societal values, spawning the field of AI alignment (Ji et al., 2023). A large portion of this effort is on developing ways to evaluate LM behavior, for example, through benchmarks (Wang et al., 2023a) and red-teaming (Perez et al., 2022). Given these evaluation techniques, how should they be used to ensure LMs stay safe? To answer this, consider the following two hypothetical settings where this question might be asked: (1) Internal Audit: At the new start-up Chasm Intellect, the LM alignment team is looking for a way to trigger an alarm when LM behavior changes. They know that LM model behavior can change unexpectedly (Li et al., 2024b). For example, fine-tuning a model that has undergone safety evaluation (e.g., GPT-3.5 Turbo (OpenAI, 2023)) can cause it to become less safe, even when using a benign fine-tuning dataset (e.g., Alpaca (Taori et al., 2023); (Qi et al., 2023)). How can the team detect meaningful changes in model behavior? (2) External Audit: A journalist has been playing around with Better geneRAtiVe language modEl (BRAVE), Chasm Intellect’s new model, and has noticed it produces highly toxic generations when asked about seemingly benign topics. Their article on this topic spurred a governmental audit of BRAVE. The government performs an initial evaluation but is worried that model behavior will change when no longer under scrutiny. How can the government regularly check the deployed model’s behavior is the same as the previously certified one? We call the general class of problems detecting changes in LM behaviors Behavioral Shift Auditing (BSA) problems. In this paper, we formalize the problem of Behavioral Shift Auditing in Language Models. We detail a test that continuously monitors behavioral shift, solely from model generations (e.g. via API calls). Under some weak assumptions, the test provably guarantees that if model generations have different behavior than those of an initial model, the test will detect it, given enough generations. At the same time, if there has not been a change, the test is guaranteed to have tight, non-asymptotic control over the false positive rate. The key insight behind our approach is that one can phrase the problem of Behavioral Shift Auditing as hypothesis testing over the relevant behavioral distribution. This framing allows our test to be applicable to any measurable aspect of model behavior, including also capabilities (e.g., dangerous capabilities (Phuong et al., 2024) or mathematical reasoning capabilities (Mishra et al., 2022a)) and biases (e.g., gender bias (Wang et al., 2023a; Kotek et al., 2023)). Using this insight, we develop a test that extends recent work on anytime-valid hypothesis testing (Pandeva et al., 2024), a state-of-the-art sequential testing method that has been successfully applied in various auditing settings (Chugg et al., 2023; Shekhar et al., 2023; Waudby-Smith et al., 2021). Our test checks for changes in model behavior distributions, comparing generations from a reference model with those of another, potentially changed model. The test has a tunable parameter that allows one to vary the strictness of the test. This allows for detecting any change in behavior, which may be more suitable for the external audit setting, to detecting a user-specified ϵitalic-ϵ\epsilonitalic_ϵ change in behavior, which could be used for the internal audit if small changes are acceptable. Similar to Pandeva et al. (2024), test performance is optimized using a learning algorithm, improving sample efficiency over prior testing methods (Lopez-Paz & Oquab, 2017; Lhéritier & Cazals, 2018; Podkopaev & Ramdas, 2024). This testing approach can complement a full evaluation when used as a warning system. Before an expensive model assessment on large-scale benchmarks (Achiam et al., 2023; Dubey et al., 2024; Zhang et al., 2024), our approach can be used to detect an initial behavior change, which can then trigger a full evaluation. We experimentally verify that our test satisfies theoretical guarantees and we report its sample efficiency on recent LM architectures for both auditing use cases. We release our code here: https://github.com/richterleo/Auditing_Test_for_LMs. Figure 1: An External Audit Example. A regulator can use the test we describe to perform an external audit: 1. The regulator initially certifies an LM by prompting and evaluating the set of generations received; 2. Later, tipped off that LM behavior may have changed, the regulator poses as a consumer and sends prompts to the model vendor, collecting the generations; 3. The regulator compares the distribution of behavior scores b⁢(⋅)𝑏⋅b(\cdot)italic_b ( ⋅ ) between the initial, certified generations and the later generations using a Behavioral Shift Auditing (BSA) test. If the distributions are sufficiently different the test triggers. Using our proposed method, the regulator can test samples sequentially without increasing the false-positive rate. The method is guaranteed to detect a change if one exists, given enough samples (more details in Section 3)."
Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression,"In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored. Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation. To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL. Technically, SCAS achieves value-aware OOD state correction, capable of correcting the agent from OOD states to high-value in-distribution states. Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS achieves excellent performance without additional hyperparameter tuning. Moreover, benefiting from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations.","Deep reinforcement learning (RL) shows promise in solving sequential decision-making problems, gaining increasing interest for real-world applications [43, 58, 64, 54, 8]. However, deploying RL algorithms in extensive scenarios poses persistent challenges, such as risk-sensitive exploration [14] and time-consuming episode collection [28]. Recent advances view offline RL as a hopeful solution to these challenges [35]. Offline RL aims to learn a policy from a fixed dataset without further interactions [33]. It can tap into existing large-scale datasets for safe and efficient learning [24, 38, 51]. In offline RL research, a well-known concern is the out-of-distribution (OOD) action issue: the evaluation of OOD actions causes extrapolation error [13], which can be exacerbated by bootstrapping and result in severe value overestimation [35]. To address this issue, a large body of work has emerged to directly or indirectly suppress OOD actions during training, employing various techniques such as policy constraint [13, 31, 11], value penalization [32, 2, 7], and in-sample learning [30, 15, 72]. (a) CQL (b) TD3BC (c) SCAS (Ours) (d) Optimal Value Figure 1: The resulting state distributions of offline RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values. Distinguished from most previous works, this paper argues that, apart from the OOD action issue, there exists an OOD state issue that also impairs performance yet has received limited attention in the field. Such an issue refers to the agent encountering states out of the offline dataset during the policy deployment phase (i.e., test phase). The occurrence of OOD states can be attributed to OOD actions, stochastic environments, and real-world perturbations. Since typical offline RL algorithms do not involve policy training in OOD states, the agent tends to behave in an uncontrolled manner once entering OOD states in the test phase. This can further exacerbate the state deviation from the offline dataset and lead to severe degradation in performance [35, 76]. In mitigating this OOD state issue, existing limited work attempts to train the policy to correct the agent from OOD states to in-distribution (ID) states [76, 23]. Technically, Zhang et al. [76] construct a dynamics model and a state transition model and align them to guide the agent to ID regions, while Jiang et al. [23] resort to an inverse dynamics model for policy constraint. However, they deal with the OOD state and OOD action issues separately, requiring extra OOD action suppression components and complex distribution modeling, which sacrifices computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, especially when the dataset contains substantial suboptimal states. As a result, the performance of prior methods also leaves considerable room for improvement. In this paper, we aim to address these two fundamental OOD issues simultaneously by proposing a simple yet effective approach for offline RL. We term our method SCAS due to its integration of OOD State Correction and OOD Action Suppression. We start with solving an analytical form of a value-aware state transition distribution, which is within the dataset support but skewed toward high-value states. Then, we align it with the dynamics induced by the trained policy on perturbed states via KL divergence. This operation intends to correct the agent from OOD states to high-value ID states, a concept we refer to as value-aware OOD state correction. Through some derivations, it also eliminates the necessity of training a multi-modal state transition model. Furthermore, we show theoretically and empirically that, while designed for OOD state correction, SCAS regularization also exhibits the effect of OOD action suppression. We evaluate SCAS on the offline RL benchmarks including D4RL [10] and NeoRL [50]. SCAS achieves excellent performance with consistent hyperparameters without additional tuning. Moreover, benefiting from its OOD state correction ability, SCAS demonstrates improved robustness against environmental perturbations. To summarize, the main contributions of this work are: • We systematically analyze the underexplored OOD state issue in offline RL and propose a simple yet effective approach SCAS unifying OOD state correction and OOD action suppression. • Our approach achieves value-aware OOD state correction, which circumvents modeling complex distributions and significantly improves performance over vanilla OOD state correction methods. • Empirically111Our code is available at https://github.com/MAOYIXIU/SCAS., our approach demonstrates superior performance on standard offline RL benchmarks and enhanced robustness in perturbed environments without additional hyperparameter tuning."
Multi-Agent Reinforcement Learning with Selective State-Space Models,"The Transformer model has demonstrated success across a wide range of domains, including in Multi-Agent Reinforcement Learning (MARL) where the Multi-Agent Transformer (MAT) has emerged as a leading algorithm in the field. However, a significant drawback of Transformer models is their quadratic computational complexity relative to input size, making them computationally expensive when scaling to larger inputs. This limitation restricts MAT’s scalability in environments with many agents. Recently, State-Space Models (SSMs) have gained attention due to their computational efficiency, but their application in MARL remains unexplored. In this work, we investigate the use of Mamba, a recent SSM, in MARL and assess whether it can match the performance of MAT while providing significant improvements in efficiency. We introduce a modified version of MAT that incorporates standard and bi-directional Mamba blocks, as well as a novel ‘cross-attention’ Mamba block. Extensive testing shows that our Multi-Agent Mamba (MAM) matches the performance of MAT across multiple standard multi-agent environments, while offering superior scalability to larger agent scenarios. This is significant for the MARL community, because it indicates that SSMs could replace Transformers without compromising performance, whilst also supporting more effective scaling to higher numbers of agents. Our project page is available at https://sites.google.com/view/multi-agent-mamba.","Figure 1: Normalised mean episode returns aggregated over all tasks and environments with 95% confidence intervals for MAM, MAT, and MAPPO. Results are obtained using ten independent seeds. MAM matches the final performance of MAT, currently state-of-the-art in MARL, and exhibits greater sample efficiency. Figure 2: Mean time (seconds) per evaluation step in smacv2 tasks with increasing numbers of agents for MAM, MAT, and MAPPO. The mean time per evaluation step for MAT increases approximately quadratically, while MAM and MAPPO scale linearly in the number of agents. Figure 3: Comparison of performance metrics for MAM, MAT, and MAPPO across various tasks. Multi-Agent Reinforcement Learning (MARL) still faces significant challenges that must be overcome to unlock its full potential; one such challenge is the ability to scale to large numbers of agents while maintaining good performance. The Multi-Agent Transformer (MAT) (Wen et al., 2022) boasts state-of-the-art performance in online MARL. MAT is an encoder-decoder framework that utilises the multi-agent advantage decomposition theorem (Kuba et al., 2022) to reframe the challenge of joint policy optimisation. It converts the problem into a sequential decision-making process, simplifying the search for optimal policies across multiple agents. However, Transformers scale quadratically in sequence length (Vaswani et al., 2023). This creates a computational bottleneck for MAT as the number of agents becomes large. Recently, State-Space Models (SSMs) (Gu et al., 2022; Gupta et al., 2022; Gu et al., 2021; Gupta et al., 2022; Smith et al., 2023; Fu et al., 2023) have offered a solution to this drawback in the Transformer architecture, with the ability to scale linearly in the sequence length. Of interest in this work is Mamba (Gu and Dao, 2024)–a selective SSM which boasts fast inference and linear scaling in the sequence length whilst matching the performance of attention architectures in the natural language processing domain. The innovations of the Mamba model are its input-dependent SSM parameters and its hardware-aware parallel algorithm in recurrent mode. In this paper, we explore replacing attention in the MAT architecture with Mamba blocks. We make use of both existing vanilla and bi-directional Mamba blocks, as well as a novel ‘cross-attention’ Mamba block we designed to replace MAT’s cross-attention. We evaluate our novel architecture, which we call the Multi-Agent Mamba (MAM), on a wide range of well-known MARL benchmark environments and compare its performance to MAT. Our core contributions can be summarised as follows: • We create an extension of the vanilla Mamba block which can be used as a cross-attention replacement. • We replace the three different attention blocks in the MAT architecture with vanilla, bi-directional and cross-attentional Mamba blocks respectively. • We empirically validate that MAM performs comparably to MAT on a wide range of MARL environments while offering significantly greater efficiency."
Toward Finding Strong Pareto Optimal Policies in Multi-Agent Reinforcement Learning,[1]\fnmViet Cuong \surTa,"Multiagent reinforcement learning (MARL) is the learning framework where multiple different agents learn to make optimal decisions in an environment through the Reinforcement Learning paradigm. MARL research advances rapidly, with the recent development of MARL achieving impressive results on a wide range of learning scenarios, notably in zero-sum games [21] and fully cooperative environments [18, 27] where all agents share the same reward function. A less studied area of research in MARL is in the environments with cooperative reward structures, in which all agents have different and possibly conflicting reward schemes [15]. In these problems, there might be multiple different but optimal policies, in the sense that none of which is better than the other. This concept is known as the Pareto optimality in multi-objective optimization. It is desirable that we can find such Pareto optimal policies, as they are equivalent to the optimal policies in the common setting of single agent problems [3]. While the current MARL methods are known to find Nash equilibrium [13], such solutions can be suboptimal [17]. In this paper, we first show that in general cooperative environments, agents need to explicitly consider the optimization of other agents to achieve Pareto optimality. Such behaviors are known as altruistic learning in RL literature [9]; altruistic learners learn to act for the benefit of other agents even though those actions do not bring about any personal gain for the actors. To learn altruistically, one agent needs to optimize not only the reward of itself but also the rewards of other agents, which involves some form of multi-objective optimization. As a result, we connect the multi-objective framework to the MARL domain. Multiple Gradient Descent Algorithm (MGDA) [5] is one of the most popular gradient-based multi-objective methods. MGDA can find arbitrary solutions in the Pareto optimal Set using first-order gradient descent. However, MGDA is known to only converge to weak Pareto optimal solutions [8]. While this problem is not significant in other learning settings, we show that MARL problems can have many weak Pareto Stationary points, which can reduce the efficacy of MGDA. To this end, we identify the effect of diminishing gradient norms as a root cause to the weak Pareto convergence issue of MGDA and propose an improved version of MGDA++ based on this observation. We demonstrate both theoretically and empirically that MGDA++ can converge to strong Pareto optimal solutions. To summarize, our contributions in this paper are: • We show that to achieve Pareto optimality in MARL, agents need to consider the objective of other agents, we connect the multi-objective optimization with MARL problems, and propose to apply MGDA to the MARL problems. • We propose MGDA++, an extension of MGDA that converges to strong Pareto Solutions with bi-objective problems in convex, smooth settings both theoretically and empirically. To our knowledge, this strong Pareto convergence result in the convex setting with gradient descent is the first in the literature. • We demonstrate the effectiveness of MGDA++ with trust region methods through several cooperative scenarios in the Gridworld benchmark. Our proposed method is able to achieve better convergence solutions across different agents in comparison with other baselines."
Notes on the Mathematical Structure of GPT LLM Architectures,"When considered from a purely mathematical point of view, the building and training of a large (transformer) language model (LLM) is the construction of a function - which can be taken to be a map from some euclidean space to another - that has certain interesting properties. And therefore, from the point of view of a mathematician, it may be frustrating to find that many key papers announcing significant new LLMs seem reluctant to simply spell out the details of the function that they have constructed in plain mathematical language or indeed even in complete pseudo-code (and the latter form of this complaint appears to be one of the motivations behind a recent article of Phuong and Hutter [1]). Here, we seek to give a relatively ‘pure’ mathematical description of the architecture of a GPT-3-style LLM.","When considered from a purely mathematical point of view, the building and training of a large (transformer) language model (LLM) is the construction of a function - which can be taken to be a map from some euclidean space to another - that has certain interesting properties. And therefore, from the point of view of a mathematician, it may be frustrating to find that many key papers announcing significant new LLMs seem reluctant to simply spell out the details of the function that they have constructed in plain mathematical language or indeed even in complete pseudo-code (and the latter form of this complaint appears to be one of the motivations behind a recent article of Phuong and Hutter [1]). Here, we seek to give a relatively ‘pure’ mathematical description of the architecture of a GPT-3-style LLM. Trainable Parameters Like all such models in machine learning, the construction really initially describes a family of functions indexed by some set Θ=𝐑NΘsuperscript𝐑𝑁\Theta=\mathbf{R}^{N}roman_Θ = bold_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT called the parameter space. There is then a separate process - the training of the model - in which a particular value θ∈Θ𝜃Θ\theta\in\Thetaitalic_θ ∈ roman_Θ is selected using a training algorithm. Each dimension of ΘΘ\Thetaroman_Θ corresponds to the possible values of an individual trainable parameter. We will draw attention to such parameters as we introduce them, as opposed to attempting to give a definition of ΘΘ\Thetaroman_Θ up front. But this short note discusses only the architecture and does not describe any training algorithms."
BitPipe: Bidirectional Interleaved Pipeline Parallelismfor Accelerating Large Models Training,"With the increasing scale of models, the need for efficient distributed training has become increasingly urgent. Recently, many synchronous pipeline parallelism approaches have been proposed to improve training throughput. However, these approaches still suffer from two major issues, i.e., pipeline bubbles caused by periodic flushing and extra communication due to the increasing number of pipeline stages. To this end, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. Specifically, a hybrid scheme of fusing interleaved pipelines with bidirectional pipelines is proposed to reduce the computational time of each single micro-batch and multiply the number of devices executing simultaneously. A V-shaped schedule with eager gradient synchronization is introduced to reduce and overlap the communication between devices. Experiments conducted on up to 32 GPUs show that BitPipe improves the training throughput of GPT-style and BERT-style models by 1.05×1.05\times1.05 ×-1.28×1.28\times1.28 × compared to the state-of-the-art synchronous approaches.","Scaling the number of parameters in contemporary deep learning models has yielded remarkable the state-of-the-art (SOTA) results. Training these large models is challenging, as the limited memory and computational capacity of a single device (e.g., GPU) pose obstacles to accommodating them within realistic timeframes. For instance, training a GPT-3 175B model demands over 3,000 GiB for storing model parameters and optimizer states, requiring an impractical 288 years with a single NVIDIA V100 GPU (Kim et al. 2023; Narayanan et al. 2021b). The urgency for parallel and distributed training (e.g., data parallelism and model parallelism) has become increasingly pronounced. While data parallelism (Li et al. 2014) allows for ideal speedup, it falters when confronted with large models that exceed the capacity of a single device. Model parallelism (Dean et al. 2012; Lee et al. 2014; Wang, Huang, and Li 2019) addresses this limitation by distributing the weight parameters of a model across multiple devices, which mitigates the memory usage per device but suffers from severe resource under-utilization. Pipeline parallelism improves resource utilization, which splits a batch into smaller micro-batches and divides a model into stages within a pipeline, allowing simultaneous execution of different micro-batches across multiple devices. Pipeline parallelism can be categorized into synchronous and asynchronous schemes based on weight update semantic. Synchronous approaches flush periodically at the end of each iteration to guarantee strict optimizer semantics, which causes device idle times (also called pipeline bubbles). Asynchronous approaches do away with flushes completely by delaying weight updates, but at the expense of strict model convergence and thus are not within the scope of our work. Figure 1: Classic synchronous pipeline schedules, with 4 pipeline devices and 8 micro-batches within a training iteration. Both schedules have the same bubble overhead and weights memory consumption (Mθsubscript𝑀θM_{\uptheta}italic_M start_POSTSUBSCRIPT roman_θ end_POSTSUBSCRIPT). The activations memory consumption (Masubscript𝑀aM_{\rm a}italic_M start_POSTSUBSCRIPT roman_a end_POSTSUBSCRIPT) of the 1F1B schedule exhibits better efficiency but existing imbalance. Early synchronous approach (e.g., GPipe (Huang et al. 2019)) focuses on reducing pipeline bubbles by increasing the number of concurrent batches in the pipeline (as shown in Figure 1(a)). As a direct consequence, there is an increase in peak activation memory demands. Subsequently, encouraged by the success of the 1F1B schedule (as shown in Figure 1(b)), researchers have proposed memory-efficient approaches (e.g., DAPPLE (Fan et al. 2021) and PipeDream-Flush (Narayanan et al. 2021a)), which further adjusts the number of micro-batches injected into devices at the beginning of pipelines. Recently approaches attempt to increase the number of devices executing simultaneously (i.e., bidirectional pipeline parallelism), or to reduce the computational time of a single micro-batch (i.e., interleaved pipeline parallelism), which shows the SOTA performance. In the bidirectional approaches (Jain et al. 2020; Li and Hoefler 2021; Zhang et al. 2023), each device stores multiple pipeline stages in different directions, which decreases bubble size and achieves a more balanced activation memory consumption. On the other hand, interleaved approaches (Narayanan et al. 2021b; Lamy-Poirier 2023; Liu et al. 2023) assign multiple smaller and nonconsecutive stages to each device, which makes each bubble size smaller accordingly. Despite the promising results, the latest synchronous approaches still face two primary issues. First, the remaining bubbles still pose the largest deficiency. Due to computation dependencies in the pipeline across different devices, bubbles are inevitable. In existing approaches, as much as 50% of the time can be spent to flush the pipeline. Second, the communication overhead remains considerable even though pipeline parallelism employs point-to-point (P2P) communication. Specifically, bidirectional pipeline parallelism requires additional weight memory and data-parallel communication to reduce pipeline bubbles, while interleaved pipeline parallelism shrinks bubble size at the expense of extra P2P communication. Moreover, if the bidirectional pipeline extends to more than two pipelines, or each device in the interleaved pipeline generalizes to have more stages, the extra communication or memory usage will increase accordingly, further degrading their performance. To address the aforementioned issues, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. To the best of our knowledge, BitPipe is the first work that incorporates the interleaved schedule into bidirectional pipeline parallelism, which reduces the computational time of each single micro-batch and doubles the number of devices executing simultaneously. BitPipe transforms the looping schedule of the interleaved pipeline to a V-shaped schedule and thus mitigates the side effect of the additional communication overhead. The contributions of BitPipe are summarized as follows: • We propose a hybrid pipeline scheme of fusing interleaved pipelines with bidirectional pipelines. This design can not only improve throughput, but also achieves a harmonious balance in memory utilization. • We introduce a V-shaped schedule of partially transforming cross-device communication to local copying, alongside an eager gradient synchronization scheme, which can reduce and overlap communication between devices. • Experiments show that BitPipe can improve the end-to-end performance by up to 1.28×1.28\times1.28 × per iteration for GPT-style and BERT-style models compared to the SOTA synchronous pipeline approaches."
FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with Ferroelectric In-Memory Computing,"In scenarios with limited training data or where explainability is crucial, conventional neural network-based machine learning models often face challenges. In contrast, Bayesian inference-based algorithms excel in providing interpretable predictions and reliable uncertainty estimation in these scenarios. While many state-of-the-art in-memory computing (IMC) architectures leverage emerging non-volatile memory (NVM) technologies to offer unparalleled computing capacity and energy efficiency for neural network workloads, their application in Bayesian inference is limited. This is because the core operations in Bayesian inference, i.e., cumulative multiplications of prior and likelihood probabilities, differ significantly from the multiplication-accumulation (MAC) operations common in neural networks, rendering them generally unsuitable for direct implementation in most existing IMC designs. In this paper, we propose FeBiM, an efficient and compact Bayesian inference engine powered by multi-bit ferroelectric field-effect transistor (FeFET)-based IMC. FeBiM effectively encodes the trained probabilities of a Bayesian inference model within a compact FeFET-based crossbar. It maps quantized logarithmic probabilities to discrete FeFET states. As a result, the accumulated outputs of the crossbar naturally represent the posterior probabilities, i.e., the Bayesian inference model’s output given a set of observations. This approach enables efficient in-memory Bayesian inference without the need for additional calculation circuitry. As the first FeFET-based in-memory Bayesian inference engine, FeBiM achieves an impressive storage density of 26.32 Mb/mm2 and a computing efficiency of 581.40 TOPS/W in a representative Bayesian classification task. These results demonstrate 10.7×\times×/43.4×\times× improvement in compactness/efficiency compared to the state-of-the-art hardware implementation of Bayesian inference.","In-memory computing (IMC) has recently emerged as a promising solution to address the memory wall issues in conventional von Neumann hardware (ielmini2018memory, ). Leveraging the compactness and high energy efficiency of emerging non-volatile memory (NVM) technologies, many leading IMC accelerators have achieved impressive computing efficiency and throughput for data-intensive machine learning models, particularly neural networks (NNs) (shafiee2016isaac, ; hu2021memory, ; yan2023improving, ; jung2022crossbar, ). While conventional NN-based algorithms are widely used, they often struggle in situations where training data is insufficient or when interpretable results are needed (qayyum2020secure, ; yang2022unbox, ). As a compelling alternative, Bayesian inference is particularly well-suited in low-data scenarios, providing explainable results with uncertainty estimation (ghahramani2015probabilistic, ; burkart2021survey, ). The primary posterior calculation in Bayesian inference, which involves the cumulative product of prior and likelihood probabilities as per Bayes’ theorem, however, differs from the multiply-and-accumulate (MAC) operations common in NN workloads. This difference renders Bayesian inference usually unsuitable for direct implementation with many existing IMC designs that typically focus on NN acceleration. In traditional complementary metal-oxide-semiconductor (CMOS)-based von Neumann implementations for Bayesian inference, such as CPU (smith2020massively, ), GPU (talbot2019parallelized, ) and field-programmable gate array (FPGA) (awano2020bynqnet, ), accessing separate memory units for stored probabilities incurs significant area and energy overhead. Efforts to exploit the non-volatility and energy efficiency of emerging devices have led to the development of Bayesian inference prototypes utilizing random number generators (RNGs) built with magnetic tunnel junction (MTJ) (vodenicarevic2017low, ), memtransistor (zheng2022hardware, ) and magnetic random-access memory (MRAM) (faria2018implementing, ). These implementations, however, are limited to Bayesian inference with binary evidence/events, and do not effectively address probability storage, rather generating required probabilities on demand, which is energy-consuming. A memristor-based Bayesian machine has been proposed (harabi2023memristor, ), using near-memory stochastic computing to reduce memory access overhead. Yet, these implementations still require additional CMOS logic and multiple clock cycles for posterior calculations and complex sensing circuitry for final inference, thus compromising computing density and inference efficiency. To address the aforementioned challenges in hardware implementation of Bayesian inference, we propose FeBiM, an efficient and compact in-memory Bayesian inference engine utilizing multi-level cell (MLC) ferroelectric field-effect transistors (FeFETs). The key contributions of this work are summarized as follows: • We propose a compact crossbar array design using one FeFET per cell as probability storage unit and a compact and scalable winner-take-all (WTA) circuit for sensing. This multi-bit FeFET array enables efficient in-memory Bayesian inference in just one clock cycle, eliminating the need for extra calculation circuitry. • We introduce a novel mapping scheme that associates quantized logarithmic probabilities with discrete FeFET states. This scheme enables the output currents of the crossbar to naturally represent the posterior probabilities, i.e., the cumulative product of priors and likelihoods given a set of observations. • We thoroughly investigate the functionality, scalability and application level performance of FeBiM. In a representative Bayesian classification task, our proposed design shows a 10.7×\times×/43.4×\times× storage density/inference efficiency improvement compared to the state-of-the-art Bayesian machine. The rest of the paper is organized as follows. Section 2 reviews the basics and relevant prior works. Section 3 introduces our FeFET-based IMC design for Bayesian inference. Section 4 presents the validation, scalability investigation and application benchmarking results of FeBiM. Finally, section 5 concludes the paper."
Interpreting Neural Networks through Mahalanobis Distance,"This paper introduces a theoretical framework that connects neural network linear layers with the Mahalanobis distance, offering a new perspective on neural network interpretability. While previous studies have explored activation functions primarily for performance optimization, our work interprets these functions through statistical distance measures, a less explored area in neural network research. By establishing this connection, we provide a foundation for developing more interpretable neural network models, which is crucial for applications requiring transparency. Although this work is theoretical and does not include empirical data, the proposed distance-based interpretation has the potential to enhance model robustness, improve generalization, and provide more intuitive explanations of neural network decisions.","Neural networks have revolutionized machine learning, achieving remarkable success across diverse applications. Central to their efficacy is the use of activation functions, which introduce non-linearity and enable the modeling of complex relationships within data. While Rectified Linear Units (ReLU) have gained prominence due to their simplicity and effectiveness (Nair and Hinton, 2010), the exploration of alternative activation functions remains an open and valuable area of research (Ramachandran et al., 2018). Neural network units are often viewed as linear separators that define decision boundaries between classes (Minsky and Papert, 1969) with larger activation values suggesting stronger contributions of features to those decisions. Our work challenges this perspective, exploring how individual neurons can be understood through the lens of statistical distance measures. Clustering techniques use distance measures. They aim to minimize the distance between data points and feature prototypes, with smaller values indicating stronger membership to the feature or cluster (MacQueen, 1967a). We explore the intersection between these perspectives on activation interpretations, leveraging the distance-minimization approach of clustering techniques to lay the groundwork for novel neural network designs based on statistical distance measures. This paper establishes a novel connection between neural network architectures and the Mahalanobis distance, a statistical measure that accounts for the covariance structure of data (Mahalanobis, 1936). We present a robust mathematical framework that bridges neural networks with this statistical distance measure and lays the groundwork for future research into neural network interpretability and design. Our key contributions are: 1. We establish a mathematical connection between neural network linear layers and the Mahalanobis distance, demonstrating how Absolute Value (Abs) activations facilitate distance-based interpretations. 2. We analyze the solution space that neural networks are likely to learn when approximating Mahalanobis distance, exploring the effects of non-uniqueness in whitening transformations and the role of Abs-activated linear nodes. 3. We discuss the broader implications of this framework for neural network design and interpretability, laying the groundwork for more interpretable models."
COAT:CompressingOptimizer states andActivation for Memory-Efficient FP8Training,"FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54× compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43× end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine’s speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT.","Foundation Models (FMs), such as Large Language Models (LLM) and Vision Language Models (VLM), have made significant breakthroughs in various tasks such as reasoning, understanding, and summarization (Dubey et al., 2024; Adler et al., 2024; Team et al., 2024; Lin et al., 2024). However, the training of such models, which often comprise billions of parameters, demands substantial computational resources and memory. This presents substantial challenges, making the training of these foundation models very challenging (Smith et al., 2022; Hoffmann et al., 2022). Low-precision training has emerged as a promising approach to make FMs training more efficient (Micikevicius et al., 2017; Wang et al., 2018; Zhu et al., 2020; Xi et al., 2023; Wortsman et al., 2023; Xi et al., 2024). By quantizing tensors used in deep neural networks into lower precision, low-precision training effectively speed up the training process and reduce the memory footprint. Currently, BF16 training (Kalamkar et al., 2019; Micikevicius et al., 2017) is the most prevalent low-precision method, and is widely adopted in large-scale training frameworks like DeepSpeed (Rasley et al., 2020) and Megatron-LM (Shoeybi et al., 2019). With the advent of Nvidia’s H100 GPU (NVIDIA, 2024a), FP8 training Micikevicius et al. (2022) is emerging as the next-generation low-precision technique. Compared to BF16, FP8 training has the potential to (1) double the speed and (2) halve the memory footprint. To achieve practical speedup, Transformer Engine (NVIDIA, 2024b) performs matrix multiplications in FP8 precision, leading to faster training. Transformer Engine’s memory footprint can be further improved by reducing optimizer states, gradients, weights, and activations to lower precision. As illustrated in Figure 1, FP8-LM (Peng et al., 2023) advances this by further quantizing the gradients, weight master copy, and first-order momentum into FP8. This reduces memory and communication overhead, partially improving memory efficiency. However, they do not tackle the memory consumption of activations and still leave the optimizer’s second-order momentum in higher precision. The memory problem of activations becomes even more critical when optimizer, gradient, and weights are sharded across multiple GPUs using ZeRO or FSDP. Besides, second-order momentum is more sensitive to quantization than first-order momentum (Fishman et al., 2024), and activations’ large spikes also make them hard to quantize to FP8 (Yang et al., 2024). This potential accuracy degradation makes them missing a crucial opportunity to optimize memory further. In this work, we propose COAT: Compressing Optimizer states and Activations for memory-efficient FP8 Training to address the aforementioned issue. COAT significantly reduces the overall memory footprint by quantizing optimizer states and activations into FP8. For optimizer states, we observe that FP8 format’s representation range is under-utilized when quantizing them, as illustrated in Figure 2(a). To address this, we introduce a novel Dynamic Range Expansion method which adjusts the distribution of optimizer states to better fit within the FP8 range, thereby minimizing quantization error. For activations, we propose Mixed-Granularity Activation Quantization to achieve efficient and accurate quantization. We apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers. Per-tensor quantization for matrix multiplications is more efficient and better suited for TensorCores, while fine-grained quantization helps maintain accuracy. These two approaches tackle high memory consumption while ensuring minimal performance degradation. We provide an overview of COAT in Figure 1(b) for demonstration. We demonstrate the accurate performance of COAT on a wide range of tasks, including LLM pretraining, LLM fine-tuning, and VLM training. COAT achieves nearly lossless performance on all of these tasks. For efficiency results, COAT achieves 1.54×1.54\times1.54 × end-to-end memory reduction compared with BF16, and 1.43×1.43\times1.43 × end-to-end training speed up on Llama 7B, 13B, and 30B models compared to BF16. COAT also doubles the batch size in all realistic distributed training settings, which is crucial for higher speedup and support for longer context length, leading to a more efficient training process for large-scale models."
Golden Ratio-Based Sufficient Dimension Reduction,"Many machine learning applications deal with high dimensional data. To make computations feasible and learning more efficient, it is often desirable to reduce the dimensionality of the input variables by finding linear combinations of the predictors that can retain as much original information as possible in the relationship between the response and the original predictors. We propose a neural network based sufficient dimension reduction method that not only identifies the structural dimension effectively, but also estimates the central space well. It takes advantages of approximation capabilities of neural networks for functions in Barron classes and leads to reduced computation cost compared to other dimension reduction methods in the literature. Additionally, the framework can be extended to fit practical dimension reduction, making the methodology more applicable in practical settings.","The curse of dimensionality poses significant challenges to statistical analysis when dealing with a large number of variables [1]. Under the supervised learning framework, sufficient dimension reduction (SDR) has emerged as a useful tool that bridges the gap between high dimensionality and traditional statistical modeling. However, current state-of-the-art statistical methods for SDR in the literature often presume the structural dimension, which is generally not the case in practice. Additionally, these methods may not be computationally feasible for handling large sample sizes or high dimensionality efficiently, which limits their usage in many real-world applications. Numerous methods have been proposed on SDR in the past decades, see e.g., [2]. For classical methods such as sliced inverse regression (SIR) [3], minimum average variance estimation (MAVE) method [4], and sliced average variance estimation (SAVE) [5], a main class of estimators for the central space is based on the inverse conditional moments of X|Yconditional𝑋𝑌X|Yitalic_X | italic_Y, where Y∈ℝ𝑌ℝY\in\mathbb{R}italic_Y ∈ blackboard_R and X∈ℝp𝑋superscriptℝ𝑝X\in\mathbb{R}^{p}italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT are the response and p𝑝pitalic_p-dimensional predictor in a regression analysis, respectively. Slicing the continuous response y𝑦yitalic_y is often used to facilitate the estimation. However, this imposes some strong probabilistic structure on X𝑋Xitalic_X. Moreover, selecting the number of slices remains an open and challenging question [6]. [7] proposed a cumulative slicing estimation methodology for SDR and developed three methods: cumulative mean estimation (CUME), cumulative directional regression (CUDR), and cumulative variance estimation (CUVE). [8] introduced a fused estimation procedure (fSIR), with observed performance improvement in some situations. [9] implemented a sliced inverse regression in an online fashion that first constructs an online estimate for the kernel matrix and then performs online singular value decomposition. [10] established consistency of estimation of the dimension reduction space in a high-dimensional setting. In a more recent work, [6] proposed an aggregate inverse mean estimation (AIME) procedure that may substantially improve estimation accuracy compared to the previous methods. It incorporates the cumulative slicing scheme into the aggregate SDR idea proposed by [11] and is much less sensitive to linearity condition violations with the localization step before aggregation. [12] proposed a real-time approach for SDR that uses a principal least squares support vector machines approach to estimate the central subspace more accurately. Their method updates the estimator efficiently as new data is collected, starting from an initial estimator obtained with currently available data. [13] proposed a method that first estimates the expectiles through kernel expectile regression and then carries out dimension reduction based on random projections of the regression expectiles. Several methods in the literature are extended under these frameworks [14, 15, 16]. There are also neural network approaches to SDR for tackling classification problems [17, 18, 19]. For regression problems, [20] proposed a nonlinear SDR method and [21] proposed a stochastic neural network that is computationally feasible to handle large scale data. [21] has proposed an algorithm that is able to obtain structural dimension, although no theoretical understanding is provided (different from this work). In this paper, we propose a golden ratio-based neural network for SDR (GRNN-SDR), a novel approach that utilizes neural networks to capture complex functional forms that are previously inaccessible with the traditional nonparametric regression tools. Our algorithm incorporates the golden ratio to dynamically search for the structural dimension, which significantly reduces computation time and complexity. Theoretical basis have demonstrated the generalization ability of multi-layer neural networks [22, 23]. Under proper conditions, we establish theoretical results that demonstrate that our approach leads to the true structural dimension with high probability. Compared to most of the existing methods, which typically presume the structural dimension to estimate the central space, extensive numerical results show that our proposed method is able to obtain the true or practical structural dimension effectively without prior knowledge. Extensive experiment comparisons show that our method estimate the central space with higher accuracy in most cases and demonstrate higher stability when the true dimensionality is not small. Furthermore, our algorithmic complexity under a fixed neural network structure is O⁢(N)𝑂𝑁O(N)italic_O ( italic_N ), where N𝑁Nitalic_N is the sample size, offering a promising solution to the challenges in SDR."
A Stock Price Prediction Approach Based on Time Series Decomposition and Multi-Scale CNN using OHLCT Images,"Stock price fluctuations are influenced by a variety of factors, including macroeconomic conditions, government policies and market sentiment, which together make price movements complex and difficult to be predicted. Despite many studies aimed at enhancing stock price prediction models, the challenges such as data noise, model overfitting and lack of interpretability are still encountered. To address these issues and improve prediction accuracy, this paper proposes a novel method, named Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), for predicting stock price movements in the China A-share market.","With the rapid development of financial markets and the acceleration of globalization, stock investment has become an essential avenue for many investors to achieve wealth appreciation [1, 2, 3]. However, the complexity and uncertainty of the stock market make accurately predicting stock trends a challenging task. The China A-share market, being one of the largest and most dynamic globally, has attracted significant quantitative analysis and research due to the growing impact of economic globalization [4, 5, 6]. Traditional stock analysis methods heavily reliant on financial data and macroeconomic indicators and often fall short in capturing the dynamic and non-linear relationships within the market [7, 8, 9, 10]. Therefore, quantitative trading has emerged as a pivotal component of modern financial strategies. By leveraging computational techniques, it enables the execution of trading strategies devoid of emotion-based decision-making, thereby uncovering patterns that may elude human analysts [11]. The potential quantitative trading in stock market forecasting has been well-documented [12]. This progress has led to the development of advanced deep models that can handle the inherent complexity, nonlinearity, and noise of financial markets, making the construction of stock trend prediction models using machine learning, deep learning, and big data techniques, which becomes a hot research topic in finance [13, 14, 15, 16, 17, 18, 19]. Despite significant advancements, many deep learning models for stock price prediction still grapple with challenges such as data noise, model overfitting, and insufficient interpretability [20]. Baek et al. [21] pointed out that the limited number of training data points often leads to overfitting in deep neural network models. Ito et al. [22] aggregated several simple, interpretable weak algorithms and assessed the importance of each weak algorithm in improving overall interpretability. In order to tackle the data noise problem, the research conducted by Liu et al. [23] employed sparse autoencoders with one-dimensional (1D) residual convolutional networks to denoise the stock prices. Convolutional neural networks (CNNs) have been employed for stock market prediction due to their efficient feature extraction capabilities [24, 25, 26]. However, traditional CNN approaches often fail to fully utilize time series information, resulting in suboptimal performance under complex market conditions [27]. Additionally, when considering multiple stocks, these models typically focus on individual stock information and neglect the inter-stock correlations that significantly influence price fluctuations [28]. Jiang et al. [29] utilized the opening prices, highest price, lowest price, closing price, and volume (OHLCV) images as inputs to predict the probability of stock price increasing or decreasing, achieving a significant accuracy. Their experimental results showed that 5-day feature maps in CNNs yield significantly better accuracy than 20-day and 60-day maps, suggesting that longer time series of stock feature maps make it challenging for CNN models to identify critical feature points, leading to local feature overfitting. Converting sequences to image features can lose the advantage of utilizing more historical data, whereas shorter image features risk sacrificing significant historical information, increasing prediction inaccuracies. Inspired by these studies, we first propose two novel methods. The first method involves replacing trading volume with stock turnover rate, which eliminates the impact of trading volume caused by ex-rights events, resulting in more stable features. The second method introduces an integration between time separator and OHLCT (Opening price, Highest price, Lowest price, Closing price, and Turnover rate), resulting a new image feature as input, named as TS-OHLCT. Specifically, we incorporate the weekend time information as separators into OHLCT images to help CNNs capture trading temporal information and learn the effects of stock trading cycles. Furthermore, two new architectures are proposed. The first is a Multi-Scale Residual Convolutional Neural Network (MSR-CNN) designed to address the overfitting problem in long sequence images. The second architecture is a Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), which addresses the problem that using only image features makes it difficult to learn information about stock price fluctuations. By integrating sequence data, SMSFR-CNN better captures stock price trends in the A-share market. We observed that traditional CNN methods often exhibit local feature overfitting and convergence issues in stock image feature extraction, with prediction performance gradually deteriorating as the time of sequence feature maps increases. This observation aligns with the findings of [29]. To address these mentioned issues, we decomposed long sequences of stock image features into multiple time periods according to different time scales and assign different feature weights based on their importance (with higher weights for features closer to the current trading day). This significantly reduces overfitting and enables CNNs to learn long sequence image features better. Additionally, considering that investors more concern about the magnitude of price fluctuations rather than the probability of price increasing or decreasing, and given that it is difficult for the image features to effectively capture the magnitude of price changes, the proposed model integrates time series information as an extra features. Our approach utilizes the CNN component to learn time series features and concatenates them with image features, simultaneously predicting both the magnitude and probability of stock price movements. This method effectively incorporates regression labels into the existing framework. Our experimental results indicate that this approach significantly improves the accuracy of stock price trend predictions, reduces the search space for image features, stabilizes and accelerates the convergence process. The paper’s major contributions can be summarized as follows: 1. This is the first time that historical open, high, low, close prices and turnover-rates are incorporated into images, which are separated by weekends and combined with time-specific information. 2. The long sequences of stock image features are decomposed into multiple time periods according to different time scales and assigned different feature weights based on their importance (with higher weights for features closer to the current trading day). 3. Combining time series with image features using CNN improves prediction accuracy, reduces the search space, stabilizes and accelerates the convergence process. 4. Comprehensive comparison experiments between different methods on 4,454 A-share stocks are provided, and the majority of A-share stocks are considered in our experiments. 5. Our proposed method, SMSFR-CNN, outperforms other advanced methods in terms of positive predictive value (PPV) and negative predictive value (NPV). The remainder of the paper is structured as follows. The related works of CNN methods in stock prediction are introduced in Section 2. Section 3 presents the dataset that serves as the basis for our study along with a detailed description of the innovative and predictive model developed in this paper. In Section 4, we describe and analyze the experimental settings and results. Finally, we provide a summary of the conclusions and main contributions of the paper in Section 5. We also highlight the significance of our results and propose potential works for future research."
Applying sparse autoencoders to unlearn knowledge in language models,"We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it language models. We demonstrate that individual interpretable biology-related SAE features can be used to unlearn biology-related knowledge with minimal side-effects. Our results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective. We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique. Current SAE quality or intervention techniques would need to improve to make SAE-based unlearning comparable to the existing fine-tuning based techniques.","Current and future language models may learn inaccurate information, produce toxic or malicious outputs, or possess dangerous capabilities that we would like to remove before deployment, such as advanced bio-weapons knowledge (Ji et al., 2023; Li et al., 2024). However, we do not yet know how to precisely and robustly remove knowledge or unlearn capabilities in these language models. The goal of this work is to investigate whether sparse autoencoders (SAEs) can be used to perform unlearning in an interpretable way. Recent work on unlearning has typically focused on fine-tuning based methods that have been applied in a variety of contexts to unlearn concepts in language models (e.g. Li et al., 2024; Zou et al., 2024; Eldan & Russinovich, 2023), going beyond prior work that aimed to unlearn specific training data points in neural networks (Bourtoule et al., 2020). While relatively successful, these fine-tuning approaches are opaque and we lack insight into what exactly is happening in the model (Łucki et al., 2024). Existing methods for removing specific facts from language models offer interpretable solutions (e.g. Meng et al., 2023), however these approaches are limited to fact-level unlearning. Having an interpretable method for unlearning is important as it can allow a higher level of confidence that the model has actually unlearned the knowledge, rather than superficially or temporarily hiding the capability to discuss a given topic. One possibility is to use sparse autoencoders to try to unlearn knowledge in an interpretable way. Sparse autoencoders (SAEs) use an unsupervised method to learn sparse reconstruction of language model activations (e.g. Ng, 2011; Bricken et al., 2023; Cunningham et al., 2023; Templeton et al., 2024; Marks et al., 2024; Gao et al., 2024). SAEs have been shown to find interpretable features in language models. SAEs appear to be a promising approach to help understand complex, abstract features that are used by language models (Templeton et al., 2024). Whether SAEs can be used to make systematic, predictable, interpretable interventions in language models in a variety of contexts remains an open question. Our work makes two key contributions: First, we attempt to develop a method for unlearning knowledge in language models in an interpretable way. Second, we apply SAEs to the task of unlearning knowledge, extending their use beyond previous work. Our approach aims to work towards more transparent and verifiable knowledge removal at a broader scale. Figure 1: An outline of how we use SAE features to intervene in the model. Selected feature activations fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are set to a negative value −c𝑐-c- italic_c when fi>0subscript𝑓𝑖0f_{i}>0italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0."
Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware Neuron Management,s m \IfBooleanF#1▷▷\triangleright▷ #2,"Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of applications (chatgpt, ; gpt4, ; llm-application-medical, ; llm-application-education, ; llm-application-finance, ; llm-application-engineer, ). Comprising millions or even billions of parameters (bert, ; opt, ; gpt3, ; llama2, ; palm, ; mistral, ; gemini, ), these models demand substantial computational and memory resources, typically available only in state-of-the-art data centers. Nonetheless, there is an increasing demand for deploying LLMs on resource-constrained devices, such as smartphones (llm-mobile-1, ; llm-mobile-2, ; llm-mobile-3, ; llm-mobile-4, ; llm-mobile-5, ; llm-mobile-6, ). On one hand, stringent privacy regulations necessitate local data processing to protect user information. On the other hand, LLMs on smartphones facilitate customization based on user habits, enabling enhanced personalization. Given the limited DRAM capacity of devices, LLMs on smartphones are typically constrained to models specially designed for mobile deployment (phi3, ; minicpm, ; gemini, ). Although these models are lightweight, the reduction in parameters inevitably leads to a compromise in their capabilities (scaling-law, ). As an alternative, many recent works (powerinfer-2, ; llm-flash, ; deja-vu, ; powerinfer, ; sparsity-mobile-survey-1, ; sparsity-mobile-survey-2, ) explore the exploitation of inherent sparsity within LLMs to address memory limitations. Specially, rather than pruning model parameters, these methods selectively activate a subset of model parameters based on the input while maintaining the original performance. By transferring only the activated parameters to DRAM for computation, larger and more powerful LLMs can be stored in external flash memory, effectively surpassing DRAM limitations of smartphones. Table 1. Breakdown of average inference latency per token when offloading 50% model parameters to flash memory. Model Compute Load Total Load Ratio OPT-350M 34 ms 87 ms 121 ms 71.9% OPT-1.3B 84 ms 273 ms 357 ms 76.5% OPT-6.7B 387 ms 1883 ms 2270 ms 82.9% Llama2-7B 450 ms 10982 ms 11432 ms 96.1% Mistral-7B 355 ms 15126 ms 15481 ms 97.7% Figure 1. Bandwidth utilization on smartphones is heavily constrained by IOPS. Ripple alleviates this bottleneck and boosts bandwidth with neuron co-activation linking. However, the efficiency of this LLM inference paradigm is significantly hindered by I/O overheads. Since different inference requests generally activate distinct sets of model parameters, frequent I/O operations are generated to swap parameters between DRAM and flash memory. As shown in Table 1, even when only half of the model parameters reside in flash memory, 71.9%-97.7% of the inference latency arises from I/O operations. More critically, the scattered activation of model parameters induces numerous small-grained read accesses, limiting transfer efficiency due to constraints in Input/Output Operations Per Second (IOPS) (iops, ). As depicted in Figure 1, this IOPS bottleneck severely restricts on-device bandwidth utilization across various LLMs. Building on these insights, this paper proposes Ripple, a novel approach to accelerating LLM inference on smartphones through I/O optimizations. While previous works (deja-vu, ; powerinfer, ) primarily focus on computation efficiency under activation sparsity, they tend to exacerbate the existing I/O overhead bottlenecks. Fewer studies (powerinfer-2, ; llm-flash, ) explore mitigating I/O overhead through enhanced caching strategies to minimize data loading. However, without directly improving bandwidth utilization, overall efficiency remains suboptimal. Orthogonal to these methods, Ripple addresses the primary bottleneck in LLM inference by maximizing bandwidth utilization via the effective reduction of I/O operations. The design of Ripple is rooted in Neuron Co-Activation, a property prevalent in activation sparsity yet underexplored in current works. Specially, neurons in LLMs exhibit strong correlations in their activation patterns. When processing real-world datasets, the activation of an individual neuron is consistently linked to the activation of a stable group of others. Given the efficiency of continuous reads, which enable the retrieval of larger data blocks with a single request, Ripple introduces a key insight: Why not establish links between neurons that are frequently co-activated in flash memory, facilitating continuous read access to reduce IOPS? However, this is not a low-hanging fruit, as both neuron co-activation patterns and storage hardware characteristics exhibit inherent complexity, complicating their effective alignment. Our comprehensive analysis identifies that three critical technical challenges must be tackled: (1) Extensive Search Space. The vast number of neurons in LLMs leads to an exponentially large space of possible neuron linking combinations. Identifying the optimized neuron linking that maximizes global benefits is exceedingly difficult and infeasible through brute-force enumeration alone. (2) Random Activation Variation. Owing to varying model inputs, the activation of model parameters exhibits intrinsic randomness. Although optimized placement strategies can spatially co-locate activated neurons, access to these neurons remains hindered by discontinuities caused by randomness. (3) Misaligned Cache Strategy. Storing frequently activated neurons in memory is critical for minimizing transfer workload. However, storing neurons individually leads to fragmentation in their placement within flash memory, potentially disrupting continuous access. To this end, Ripple employs a two-stage solution that performs hierarchical optimizations both offline and online. (1) In the Offline Phase, Ripple clusters neurons exhibiting high co-activation correlation and reorganizes their placement in flash memory. To address Challenge (1), we abstract the problem into a complete graph, reformulating it as the discovery of the globally optimal Hamiltonian Path. By leveraging graph-theoretic techniques, we propose a greedy algorithm that efficiently searches for optimized placement based on observed neuron co-activation patterns. (2) In the Online Phase, Ripple performs fine-grained refinements based on optimized neuron placement, further enhancing access continuity. To tackle Challenge (2), we devise an IOPS-friendly access collapse technique. By strategically incorporating additional neurons between two separate neuron links, we improve read access continuity with negligible overhead. In response to Challenge (3), we design a linking-aligned in-memory caching policy. Rather than individually caching the hottest neurons, we account for their interlinking relationships, ensuring efficient access patterns. We evaluate Ripple on three smartphones with distinct hardware configurations, benchmarking a diverse range of LLMs varying in structures and scales. The results demonstrate that Ripple significantly boosts on-device bandwidth, achieving improvements of up to 4.32×4.32\times4.32 ×. Moreover, this bandwidth optimization yields substantial reductions in I/O latency during inference, offering speedups of up to 5.93×5.93\times5.93 × when compared to state-of-the-art solutions. To the best of our knowledge, Ripple is the first to accelerate LLM inference on smartphones by enhancing I/O bandwidth through optimized neuron placement in flash memory. Ripple effectively bridges the performance gap between flash memory and DRAM, enabling LLM inference to exceed DRAM limitations on smartphones. Our contributions can be summarized as follows: • We identify the primary bottleneck in LLM inference on smartphones as IOPS, attributing it to the inherent misalignment between scattered activation patterns and storage hardware characteristics. • We notably exploit neuron co-activation to mitigate the IOPS bottleneck, pioneering the optimization of neuron placement in flash memory for enhancing bandwidth efficiency on smartphones. • We conduct extensive evaluations on various representative LLMs and hardware, achieving substantial improvements over state-of-the-art solutions."
"Datasheet for “Coordinated Reply Attacks in Influence Operations:
Characterization and Detection”",Motivation,
A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution Generalization to Adaptation,"Distribution shifts on graphs – the discrepancies in data distribution between training and employing a graph machine learning model – are ubiquitous and often unavoidable in real-world scenarios. These shifts may severely deteriorate model performance, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph machine learning under distribution shifts, aiming to train models to achieve satisfactory performance on out-of-distribution (OOD) test data. In our survey, we provide an up-to-date and forward-looking review of deep graph learning under distribution shifts. Specifically, we cover three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation. We begin by formally formulating the problems and discussing various types of distribution shifts that can affect graph learning, such as covariate shifts and concept shifts. To provide a better understanding of the literature, we systematically categorize the existing models based on our proposed taxonomy and investigate the adopted techniques behind. We also summarize commonly used datasets in this research area to facilitate further investigation. Finally, we point out promising research directions and the corresponding challenges to encourage further study in this vital domain. Additionally, we provide a continuously updated reading list at https://github.com/kaize0409/Awesome-Graph-OOD.","Driven by the prevalence of graph-structured data in numerous real-world scenarios, growing attention has been paid to graph machine learning, which effectively captures the relationships and dependencies among entities within graphs. In particular, Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations on graphs through message-passing [1, 2, 3], and they have demonstrated remarkable success across diverse applications, such as social networks, physics problems, and traffic networks [4, 5, 6]. While graph machine learning has achieved notable success, most of the existing efforts assume that test data follows the same distribution as training data, which is often invalid in the wild. When confronted with Out-Of-Distribution (OOD) samples, the performance of graph machine learning methods may substantially degrade, limiting their efficacy in high-stake graph applications such as finance and healthcare [7]. Although numerous transfer learning methods have been proposed to address distribution shifts for Euclidean data [8, 9, 10], their direct application to graph data is challenging. This is due to the interconnected nature of entities on graphs, which violates the independent and identically distributed (IID) assumption inherent in traditional transfer learning methods. Moreover, the various types of graph shifts introduce new challenges. These shifts occur across different modalities including features, structures, and labels, and can manifest in various forms such as variations in graph sizes, subgraph densities, and homophily [11]. Given these obstacles, increasing research efforts have been dedicated to improving the reliability of graph machine learning against distribution shifts, concentrating on three main scenarios: graph OOD generalization [7, 11], training-time graph OOD adaptation [12, 13], and test-time graph OOD adaptation [14, 15]. The primary distinction between graph OOD generalization and adaptation methods lies in their assumptions regarding the availability of target data. Graph OOD generalization methods typically assume the unavailability of target data during model training and aim to enhance the model’s generalization performance on any potential unseen test distribution. In contrast, both training-time and test-time adaptation methods assume the availability of target data and aim to improve model performance on this specific target. However, they differ in their assumptions about the source data and in how they utilize knowledge of the source distribution. Training-time adaptation assumes that both the source and target graphs are available simultaneously, allowing model adaptation to start from scratch during the training process. On the other hand, test-time adaptation typically assumes access to a model pre-trained on the source graph, rather than the source graph itself, and begins adapting the model to the target data from this pre-trained state. Although graph OOD generalization, training-time OOD adaptation, and test-time OOD adaptation are closely related, there is currently no unified framework that comprehensively discusses deep graph learning under distribution shifts across all three scenarios. With recent progress on graph OOD learning, an up-to-date and forward-looking review of this field is urgently needed. In this survey, we provide, to the best of our knowledge, the first unified and systematic review of the literature on deep graph learning under distribution shifts. We start by formally formulating the problems and discussing different types of graph distribution shifts in graph machine learning. Next, our new taxonomy is proposed, classifying existing methods into three categories based on the model learning scenario: (1) graph OOD generalization, where the generalizability is enhanced through strategic design of the model when training on source data, (2) training-time graph OOD adaptation, where the adaptation is performed when jointly training the model based on both source and target data [16, 17], and (3) test-time graph OOD adaptation, where the adaptation happens when adjusting a pre-trained source model to the target data [18, 19]. To deepen our understanding of these approaches, we further classify existing methods within each of the three categories into model-centric and data-centric strategies. Model-centric approaches focus on the learning process or the architecture of the graph model itself, enhancing the model’s inherent ability to generalize or adapt to distribution shifts by refining its structure, training objectives, or learning mechanisms. In contrast, data-centric approaches emphasize the manipulation of input graphs, improving model performance by addressing the data directly, either through preprocessing techniques or data augmentation strategies. Within each subline of research, we elaborate on the detailed techniques for enhancing the generalizability or adaptability under distribution shifts on graphs. Additionally, we provide a summary of the datasets utilized in these studies, highlighting their characteristics and relevance to the challenges posed by distribution shifts. Based on the current progress on graph OOD learning, at the end we also point out several promising research directions in this evolving field. Differences between this survey and existing ones. Despite the urgent need for an overview of graph learning under distribution shifts, existing surveys have primarily focused on subfields within this area, rather than providing a comprehensive overview from multiple scenarios. Until now, there have been several surveys in related areas, but exhibiting distinct focuses, including graph OOD generalization [7, 20], graph domain adaptation [21, 22], trustworthy graph learning [23] related to distribution shifts. Our work distinguishes itself from existing ones in the following aspects: (1) Main focus. Our survey centers on the challenges and solutions for graph learning under distribution shifts, while [23] analyzes OOD issues from a trustworthy perspective and does not delve into the methodological aspects. Conversely, [20] examines graph machine learning from a causal perspective, which is narrower than our broad examination. (2) Taxonomy. We provide a comprehensive categorization of existing methods and summarize them, whereas related work, such as [24], lacks such summaries. Other surveys like [21] and [22] primarily focus on domain adaptation without addressing the broader scope of graph OOD learning. Additionally, we provide coverage of the most recent advancements and discussions in this field. Survey structure. The general organization of this survey is presented as follows: Section 2 introduces the notations and preliminaries. Sections 3, 4 and 5 review graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation, respectively. Each section discusses model-centric and data-centric approaches within its scenario, further detailing the techniques associated with each category. Furthermore, Section 6 provides a comprehensive summary of the datasets used in the literature, highlighting popular graph datasets for evaluation and their relevance to the challenges posed by distribution shifts. Section 7 explores promising future research directions and the associated challenges. Finally, Section 8 presents the conclusion of this survey."
Spatioformer: A Geo-encoded Transformer for Large-Scale Plant Species Richness Prediction,"Earth observation data have shown promise in predicting species richness of vascular plants (α𝛼\alphaitalic_α-diversity), but extending this approach to large spatial scales is challenging because geographically distant regions may exhibit different compositions of plant species (β𝛽\betaitalic_β-diversity), resulting in a location-dependent relationship between richness and spectral measurements. In order to handle such geolocation dependency, we propose Spatioformer, where a novel geolocation encoder is coupled with the transformer model to encode geolocation context into remote sensing imagery. The Spatioformer model compares favourably to state-of-the-art models in richness predictions on a large-scale ground-truth richness dataset (HAVPlot) that consists of 68,170 in-situ richness samples covering diverse landscapes across Australia. The results demonstrate that geolocational information is advantageous in predicting species richness from satellite observations over large spatial scales. With Spatioformer, plant species richness maps over Australia are compiled from Landsat archive for the years from 2015 to 2023. The richness maps produced in this study reveal the spatiotemporal dynamics of plant species richness in Australia, providing supporting evidence to inform effective planning and policy development for plant diversity conservation. Regions of high richness prediction uncertainties are identified, highlighting the need for future in-situ surveys to be conducted in these areas to enhance the prediction accuracy.","Australia is home to a large and diverse range of plant species, with over 21,000 known native species of vascular plants and 93% of these being endemic [1, 2]. The richness of plant species, also known as α𝛼\alphaitalic_α-diversity, is highly important in maintaining the functioning of ecosystems, such as habitat provision, carbon sequestration, and water cycling [3, 4, 5, 6]. However, anthropogenic interference, such as deforestation, overgrazing, and urbanisation, has resulted in a decline in plant species richness [7, 8]. In response, conservation activities have been initialised and conducted across the country aiming to preserve plant diversity [9, 10, 11]. Accurate and up-to-date maps of plant species richness will strongly support effective planning and policy-making for these activities [12, 13]. Earth observation (EO) data provide rapid and near-real-time estimates of changes in land surface conditions across large regions [14, 15, 16, 17]. This makes remote sensing imagery a favourable data source for plant species richness modelling, as compared with another widely adopted approach where environmental variables, such as temperature, precipitation, soil texture, and topographic heterogeneity, are used as richness predictors [18]. The reason is that environmental variables drive mainly the environmental potential of plant habitats (i.e., the capacity to sustain a certain level of richness), rather than represent the actual conditions on the ground like those observed by remote sensing satellites. For example, deforestation, floods, and bushfires could cause reduction in richness [19], but such reduction might not be reflected by environmental variables. Therefore, environmental variables are often aimed at predicting the natural patterns in diversity in a pre-intensification reference state, while remote sensing data are more valuable for monitoring actual changes in those patterns. Australia covers an area of over seven million square kilometres. As a result of the relatively large geographical extents, versatile types of plant habitats are found across the country, differing in their inventories of plant species present that have been shaped by a variety of factors such as biogeographic history, climate, and geography [20]. To understand the spatiotemporal distribution of plant species richness, perseverant in-situ field surveys have been conducted over the past several decades. Via various survey campaigns, a wealth of 219,552 richness samples have been gathered across the country as of the year 2022 [18]. These samples represent a broad range of landscapes across the continent, and therefore present a unique opportunity to unravel the potentially intricate relationship between richness measurements and satellite observations. Nevertheless, geographically distant regions may exhibit distinct assemblages of plant species with differed compositional properties (i.e., β𝛽\betaitalic_β-diversity [21]), making it challenging to model richness over large spatial scales. Due to spatial variations in plant species composition, a location with a set of plant species would be expected to display quite different spectral features in remote sensing imagery from another location with a dissimilar plant composition, even if the two locations sustain the same richness of species [22, 23]. Through statistical regression analysis for two regions in southeast Australia, previous studies [13, 24] suggested that the relationship between plant species richness and hyper/multispectral satellite observations is region-specific. To account for the location dependency, we need a model that is capable of taking in geolocation context when mapping plant species richness over large spatial scales. The transformer model, first introduced in [25], is built upon the self-attention mechanism [26]. The model attends effectively to information of high importance in the input data, as the self-attention module is capable of capturing intricate data structures and dependencies [26, 27]. Initially proposed for language tasks, the transformer model has shown promise for image understanding due to its superior ability over Convolutional Neural Networks (CNNs) in capturing global dependencies between different regions of an image [28]. As a seminal work on applying transformer to image data, the Vision Transformer (ViT) model [28] first divides an image into non-overlapping patches, followed by projecting each patch into a feature vector which is then fed into the self-attention module, with state-of-the-art performance being achieved on benchmark datasets. Given remote sensing imagery captures rich features in the spectral dimension, the SpectralFormer model [29] was developed to effectively embed the spectral information. This model was later extended in [30], where FactoFormer, a factorised transformer, was introduced for the joint learning of spectral and spatial features. These studies have demonstrated the effectiveness of transformer in processing remote sensing images (e.g., [30, 29]), but to advance the model’s application to remote sensing images recorded over large spatial scales, geolocational information could be leveraged. Unlike many types of imagery whose semantics are independent of the location where they are recorded, remote sensing images are intrinsically associated with geolocations [31, 32]. Considering that the composition of plant species is location-specific, incorporating geolocation context could be helpful in modelling the location-dependent relationship between richness and remote sensing imagery. Geo-coordinates provide geographical priors that supplement geolocation context to the image data [33, 32, 34, 31, 35, 36, 37]. While a straightforward way to utilise geolocational features is to concatenate the original longitude and latitude coordinates into the model, this approach has shown to yield almost no gain in performance [34]. To deal with this problem, a geo-feature extraction approach was proposed in [34] for CNN models, where the geo-coordinates were projected into a higher dimensional feature space with a geolocation encoder, whose outputs were then merged into those of a CNN-based image network. It was observed that, by leveraging geolocation context, a 7% increase in accuracy was achieved for an image data set spanning over the continental United States [34]. This geo-encoded CNN model was later applied to global-scale vegetation canopy height mapping with satellite imagery [38], where the geolocations served as a prior. Other state-of-the-art geolocation encoders include Space2Vec [39], Sphere2Vec [40], PE-GNN [41], and a more recent algorithm that is based on the spherical harmonic basis functions [42]. Multi-scale sinusoidal functions are favoured in building these encoders (e.g., [39, 42]), thanks to their merits of being bounded in value, infinitely extended in space, and possessing a multi-resolution scalability. Geolocation encoding is demonstrated to be effective in many large-scale geospatial problems, such as animal species categorisation [38, 40], water quality prediction [43], event/activity recognition [44], and remote sensing scene classification [31, 40]. In this study, we aim to predict the spatiotemporal distribution of plant species richness in Australia from EO imagery with geolocation context being taken into account. Considering that the relationship between plant species richness and remote sensing imagery varies from one location to another due to differences in vegetation composition, we propose Spatioformer, where a novel geolocation encoder is coupled with the transformer model in order to incorporate the geolocation context. The performance of Spatioformer in richness mapping is compared with a CNN model, a ViT model, and the FactoFormer model where the geolocational information is not encoded. Through quantitative analyses, we seek to address primarily the following important questions: (1) Does Spatioformer perform better than state-of-the-art algorithms in predicting plant species richness over large spatial scales? (2) What are the spatial patterns of plant species richness in Australia inferred from remote sensing evidence? (3) Where future in-situ surveys should be conducted as suggested by the mapping results? The rest of the paper is organised as follows. Section II describes the study area and the datasets used for modelling, including the ground-truth samples of plant species richness and satellite imagery. Section III introduces the methods with a focus on the proposed Spatioformer model. This model is developed with the aim to capture the location-dependent relationships between plant species richness and remote sensing imagery over large spatial scales. Section IV describes the experimental settings for training and validation of the Spatioformer model. Section V presents the results of applying Spatioformer to plant richness mapping across Australia, and discusses the implications of these findings for biodiversity conservation and future research directions. Finally, Section VI concludes the paper."
CHESTNUT: A QoS Dataset for Mobile Edge Environments,"Quality of Service (QoS) is an important metric to measure the performance of network services. Nowadays, it is widely used in mobile edge environments to evaluate the quality of service when mobile devices request services from edge servers. QoS usually involves multiple dimensions, such as bandwidth, latency, jitter, and data packet loss rate. However, most existing QoS datasets, such as the common WS-Dream dataset, focus mainly on static QoS metrics of network services and ignore dynamic attributes such as time and geographic location. This means they should have detailed the mobile device’s location at the time of the service request or the chronological order in which the request was made. However, these dynamic attributes are crucial for understanding and predicting the actual performance of network services, as QoS performance typically fluctuates with time and geographic location. To this end, we propose a novel dataset that accurately records temporal and geographic location information on quality of service during the collection process, aiming to provide more accurate and reliable data to support future QoS prediction in mobile edge environments.","I original dataset description To create a high-quality dataset for predicting Quality of Service (QoS) in mobile edge environments, this study utilized two real-world datasets from Shanghai. One dataset is from the Shanghai Johnson Taxi, containing information such as the longitude, latitude, moving direction, and speed of the taxis on a specific day, which was used to simulate user mobile datasets. The other dataset is from Shanghai Telecom [1, 2, 3], providing the longitude and latitude of the base stations. Data from June 2014 was used to simulate the generation of edge server datasets. I-A Shanghai Johnson Taxi Dataset The Shanghai johnson taxi dataset is an important resource for traffic research, containing real-time GPS and business status information from taxis in Shanghai. Each record in the dataset includes various fields: the vehicle ID, control word (where A indicates normal and M indicates alarm), business status (0 for normal and 1 for alarm), passenger status (0 for occupied and 1 for unoccupied), top light status (with values ranging from 0 for operation to 5 for out of service), road type (0 for ground road and 1 for express road), brake status (0 for no braking and 1 for braking), meaningless fields, reception date, GPS timestamp, longitude, latitude, speed, direction, the number of satellites, and additional meaningless fields. This dataset enables the analysis of urban traffic flow, travel patterns, and traffic management strategies. In this paper, we use only the gps time, latitude, longitude, speed, and direction of the cab to generate motion information about the mobile user. I-B Shanghai Telecom Dataset This study utilized a telecommunications dataset provided by Shanghai Telecom, comprising over 7.2 million records of 9,481 mobile phones accessing the Internet through 3,233 base stations over six months.111http://sguangwang.com/TelecomDataset.html The dataset includes six parameters: month, date, start time, end time, base station location (latitude and longitude), and user ID used within Shanghai Telecom. This dataset can be used to evaluate solutions in mobile edge computing, such as edge server deployment, service migration, and service recommendation. In our research, we need to simulate the information of edge servers on base stations based on this dataset. Furthermore, considering the substantial volume of data, we only counted the data for one month and only focused on the geographic location information of Shanghai base stations."
Enhancing Exchange Rate Forecasting with Explainable Deep Learning Models,"Accurate exchange rate prediction is fundamental to financial stability and international trade, positioning it as a critical focus in economic and financial research. Traditional forecasting models often falter when addressing the inherent complexities and non-linearities of exchange rate data. This study explores the application of advanced deep learning models, including LSTM, CNN, and transformer-based architectures, to enhance the predictive accuracy of the RMB/USD exchange rate. Utilizing 40 features across 6 categories, the analysis identifies TSMixer as the most effective model for this task. A rigorous feature selection process emphasizes the inclusion of key economic indicators, such as China-U.S. trade volumes and exchange rates of other major currencies like the euro-RMB and yen-dollar pairs. The integration of grad-CAM visualization techniques further enhances model interpretability, allowing for clearer identification of the most influential features and bolstering the credibility of the predictions. These findings underscore the pivotal role of fundamental economic data in exchange rate forecasting and highlight the substantial potential of machine learning models to deliver more accurate and reliable predictions, thereby serving as a valuable tool for financial analysis and decision-making.","Since the dissolution of the Bretton Woods system, the adoption of a floating exchange rate regime has introduced significant challenges in risk management for market participants. The volatility of the RMB/USD exchange rate, particularly during periods of trade tensions, has heightened the uncertainty faced by those engaged in the foreign exchange market. The People’s Bank of China’s reform of the exchange rate fixing mechanism on August 11, 2015, further increased the marketization of the RMB exchange rate, leading to greater exchange rate volatility and increased foreign exchange risk. This has underscored the critical need for accurate exchange rate forecasting and effective risk management strategies. China’s growing role in the global supply chain, especially after its accession to the World Trade Organization (WTO), and the deepening economic ties between China and the United States, have made the RMB/USD exchange rate a focal point of global economic stability. Debates over the valuation of the RMB, particularly during periods of significant trade surpluses with the U.S., have led to multiple rounds of discussions on exchange rate policy. The RMB exchange rate reform in 2005 and the subsequent rounds of monetary policy adjustments by the Federal Reserve, especially during the 2007 financial crisis, further complicated the dynamics of the RMB/USD exchange rate. The “8.11 Exchange Rate Reform” in 2015, which introduced a more flexible exchange rate mechanism, and the intensified trade frictions since 2018, have put additional depreciation pressure on the RMB, making accurate forecasting of the RMB/USD exchange rate increasingly important [1, 2, 3, 4, 5]. Previous research on exchange rate forecasting has primarily focused on theoretical and quantitative models. Theoretical models often emphasize the equilibrium state of exchange rates, which can be difficult to achieve or maintain in practice, making short- to medium-term predictions particularly challenging. Quantitative models focus on the exchange rate’s own dynamics while often neglecting other critical influencing factors. Moreover, these models have struggled to produce consistent results across different studies. In recent years, there has been a notable shift towards using big data approaches in forecasting models, bypassing the need for complex mathematical modeling and allowing for more flexible model forms without predefined structures. The inherent complexity and non-linearity of exchange rate data have led to applying non-linear methods, such as chaos theory, non-parametric methods, and machine learning techniques, which have shown potential to improve forecasting accuracy. Studies like those of LeBaron and others have demonstrated that methods such as kernel ridge regression can significantly enhance the prediction of financial volatility, although some researchers, such as Mourer, have found that these methods do not always outperform simple autoregressive models in all contexts. Since the end of the Bretton Woods system, the transition to a floating exchange rate regime has posed significant challenges for managing risk, especially regarding the RMB/USD exchange rate. China’s integration into the global economy post-WTO accession, along with its deepening economic ties with the U.S., has made this exchange rate crucial for global economic stability. The 2015 reform of China’s exchange rate mechanism increased market-driven fluctuations, further intensified by trade tensions, which has heightened volatility. This evolving landscape has led to debates over the RMB’s valuation and spurred numerous policy discussions. Consequently, precise and reliable exchange rate forecasting has become essential for effective risk management in this volatile environment. Traditional exchange rate forecasting models, both theoretical and quantitative, have struggled with consistency and often neglect critical influencing factors. For example, theoretical models may fail to account for the real-time impact of policy changes and global economic shifts, while quantitative models may not fully capture the non-linear dynamics and intricate interactions of the variables involved. Recently, there has been a shift toward big data and machine learning approaches, which offer flexibility and improved accuracy in handling the complex, non-linear nature of exchange rate data [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]. While some methods like kernel ridge regression have shown promise, their performance varies across different contexts, highlighting the ongoing challenges in exchange rate prediction. Machine learning models, particularly deep learning models, have increasingly been applied to predicting time series and economic variables [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]. Despite their advantages in handling complex, non-linear data without requiring explicit assumptions about the underlying data distribution, these models are often criticized for their “black box” nature and lack of interpretability. Recent advancements, such as the application of Grad-CAM and attention mechanisms, have begun to address these issues, making it possible to visualize model predictions and understand the underlying decision-making processes. In related financial applications, researchers have combined advanced models like XGBoost with data balancing techniques including SMOTE to enhance predictive performance, as demonstrated by Chang et al. in their fraud detection study [60]. Their work not only showcased the effectiveness of this approach in fraud detection but also laid a foundation for developing robust models in various financial domains, including potentially exchange rate prediction [60]. However, applying these interpretability techniques has been mostly limited to fields like image recognition and natural language processing, with relatively few studies applying them to economic forecasting. Given the challenges of traditional models and the potential of machine learning approaches, this study seeks to explore the use of advanced deep learning models, including CNNs, RNNs, MLPs, and transformer-based architectures, for predicting the RMB/USD exchange rate. By incorporating a comprehensive set of features—drawn from economic indicators, trade data, and other currency pairs—and employing advanced feature selection techniques, this research aims to enhance predictive accuracy, identify the most relevant factors influencing exchange rate fluctuations, and enhance the interpretability of the model predictions. Contributions of This Study • Application of Deep Learning Models: This study provides an initial analysis of the effectiveness of deep learning models in exchange rate prediction, using MSE and MAE as key metrics to identify the best-performing models. • Enhancement of Predictive Performance: To improve the accuracy of machine learning models, this study employs various techniques, including feature selection, to reduce redundancy and retain the most relevant subset of features for exchange rate forecasting. • Analysis of Influential Factors Over Time: By applying attention mechanisms, this study enhances the interpretability of machine learning models, offering insights into how different factors influence exchange rate predictions across different periods. This analysis aims to uncover which aspects of economic data the models prioritize during the prediction process, thereby providing a more nuanced understanding of the underlying dynamics."
SHAP zero Explains All-order Feature Interactions in Black-box Genomic Models with Near-zero Query Cost,"With the rapid growth of black-box models in machine learning, Shapley values have emerged as a popular method for model explanations due to their theoretical guarantees. Shapley values locally explain a model to an input query using additive features. Yet, in genomics, extracting biological knowledge from black-box models hinges on explaining nonlinear feature interactions globally to hundreds to thousands of input query sequences. Herein, we develop SHAP zero, an algorithm that estimates all-order Shapley feature interactions with a near-zero cost per queried sequence after paying a one-time fee for model sketching. SHAP zero achieves this by establishing a surprisingly underexplored connection between the Shapley interactions and the Fourier transform of the model. Explaining two genomic models, one trained to predict guide RNA binding and the other to predict DNA repair outcomes, we demonstrate that SHAP zero achieves orders of magnitude reduction in amortized computational cost compared to state-of-the-art algorithms. SHAP zero reveals all microhomologous motifs that are predictive of DNA repair outcome, a finding previously inaccessible due to the combinatorial space of possible high-order feature interactions.","Shapley values have emerged as a theoretically robust method for explaining the local additive features of an input query to black-box models [1, 2, 3]. However, extracting biological knowledge from the emerging models in genomics demands a more global understanding, which requires explaining the nonlinear interactions among features and doing so for hundreds to thousands of input query sequences. Shapley value explanations have a high computational cost, with an exact calculation requiring an exponential number of model evaluations in the input dimension [4]. The cost is higher for nonlinear feature interactions and grows exponentially with a polynomial function of the input dimension. With the increasing growth of complex and large-scale models in genomics [5], often with only proprietary access, there is an urgent need for algorithms that can explain the nonlinear high-order interactions in these black-box models at scale–not just in a handful of sequences but among a host of query sequences. Consider the problem of explaining a black-box model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) that takes in a length-n𝑛nitalic_n DNA sequence 𝐱∈ℤ4n={A,T,C,G}n𝐱superscriptsubscriptℤ4𝑛superscript𝐴𝑇𝐶𝐺𝑛\mathbf{x}\in\mathbb{Z}_{4}^{n}=\{A,T,C,G\}^{n}bold_x ∈ blackboard_Z start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT = { italic_A , italic_T , italic_C , italic_G } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and outputs a real number. One popular method of explaining the prediction of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to the input query sequence 𝐱𝐱\mathbf{x}bold_x is using SHapley Additive exPlanations (SHAP) [6]. SHAP explains the output of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) by assigning a so-called SHAP value IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) to the ithsuperscript𝑖thi^{\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT nucleotide in 𝐱𝐱\mathbf{x}bold_x, where i𝑖iitalic_i belongs to the feature set D={1,2,…,n}𝐷12…𝑛D=\{1,2,\dots,n\}italic_D = { 1 , 2 , … , italic_n } (Fig. 1a): IS⁢V⁢(i)=∑T⊆D\{i}|T|!⁢(|D|−|T|−1)!|D|!⁢[vT∪{i}⁢(𝐱T∪{i})−vT⁢(𝐱T)].superscript𝐼𝑆𝑉𝑖subscript𝑇\𝐷𝑖𝑇𝐷𝑇1𝐷delimited-[]subscript𝑣𝑇𝑖subscript𝐱𝑇𝑖subscript𝑣𝑇subscript𝐱𝑇I^{SV}(i)=\sum_{T\subseteq D\backslash\{i\}}\frac{|T|!\,(|D|-|T|-1)!}{|D|!}% \left[v_{T\cup\{i\}}(\mathbf{x}_{T\cup\{i\}})-v_{T}(\mathbf{x}_{T})\right].italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) = ∑ start_POSTSUBSCRIPT italic_T ⊆ italic_D \ { italic_i } end_POSTSUBSCRIPT divide start_ARG | italic_T | ! ( | italic_D | - | italic_T | - 1 ) ! end_ARG start_ARG | italic_D | ! end_ARG [ italic_v start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT ) - italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ] . (1) IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) computes the marginal contribution of the ithsuperscript𝑖thi^{\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT nucleotide to the value function vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) for all the subsets T𝑇Titalic_T of features in D\{i}\𝐷𝑖D\backslash\{i\}italic_D \ { italic_i }, where the value function is the output of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to 𝐱Tsubscript𝐱𝑇\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT computed by marginalizing the absent nucleotides in D\T\𝐷𝑇D\backslash Titalic_D \ italic_T (Fig. 1a). SHAP requires computing vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) for every subset of features in T⊆D𝑇𝐷T\subseteq Ditalic_T ⊆ italic_D, which means evaluating f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) using a number of samples that grows exponentially with n𝑛nitalic_n (hereafter called sample complexity) and paying a computational cost that also grows exponentially with n𝑛nitalic_n (hereafter called computational complexity). Faithful Shapley Interaction index IF⁢S⁢I⁢(T)superscript𝐼𝐹𝑆𝐼𝑇I^{FSI}(T)italic_I start_POSTSUPERSCRIPT italic_F italic_S italic_I end_POSTSUPERSCRIPT ( italic_T ) (Faith-Shap) generalizes SHAP values defined for a single nucleotide i𝑖iitalic_i to an interaction defined over a set T⊆D𝑇𝐷T\subseteq Ditalic_T ⊆ italic_D (Equation (34)) [7]. Computing Faith-Shap requires evaluating the model for all possible subsets of features in D𝐷Ditalic_D, and then marginalizing over their power set with an exact computational cost that grows exponentially with a polynomial function of sequence length p⁢o⁢l⁢y⁢(n)𝑝𝑜𝑙𝑦𝑛poly(n)italic_p italic_o italic_l italic_y ( italic_n ). Figure 1: Schematic of the flowchart for the exact computation of SHapley Additive exPlanations (SHAP) and our approximation algorithm SHAP zero. a, Computing SHAP values exactly requires exponential model evaluations with sequence length n𝑛nitalic_n, per query sequence. This illustration shows computing one such term in IS⁢V⁢(i=1)superscript𝐼𝑆𝑉𝑖1I^{SV}(i=1)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i = 1 ), which measures the marginal contribution of the nucleotide T at site i=1𝑖1i=1italic_i = 1 to the set T={3,4,5}𝑇345T=\{3,4,5\}italic_T = { 3 , 4 , 5 } by evaluating the model for all four possible nucleotides at D\(T∪{i})={2}\𝐷𝑇𝑖2D\backslash(T\cup\{i\})=\{2\}italic_D \ ( italic_T ∪ { italic_i } ) = { 2 } in the sequence 𝐱T∪{i}subscript𝐱𝑇𝑖{\bf x}_{T\cup\{i\}}bold_x start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT and all 42=16superscript42164^{2}=164 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 16 possible nucleotide combinations at D\T={1,2}\𝐷𝑇12D\backslash T=\{1,2\}italic_D \ italic_T = { 1 , 2 } in the sequence 𝐱Tsubscript𝐱𝑇{\bf x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. b, In contrast, SHAP zero cleverly samples the sequence space and resolves a sparse bipartite graph to approximate the original model with its top-s𝑠sitalic_s Fourier coefficients. c, For each query sequence 𝐱𝐱{\bf x}bold_x, SHAP zero maps the top-s𝑠sitalic_s Fourier coefficients into the Möbius transform M⁢[𝐤]𝑀delimited-[]𝐤M[{\bf k}]italic_M [ bold_k ], where k is the feature interaction vector. The illustration shows one such sequence’s Möbius transform, composed of all the permutations of ℓ=3ℓ3\ell=3roman_ℓ = 3 nucleotides from the original sequence. d, SHAP zero computes IS⁢V⁢(i=1)superscript𝐼𝑆𝑉𝑖1I^{SV}(i=1)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i = 1 ) using a weighted sum of the Möbius coefficients with k1>0subscript𝑘10k_{1}>0italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0 (Equation (2)). SHAP zero amortizes the cost of finding the Fourier transform over the future explanations. The algorithms to approximately compute SHAP values are either stochastic estimators [6, 8, 9, 10, 11, 12] or model-based approximators [6, 13, 14, 15, 16, 17, 18]. Stochastic estimators, such as KernelSHAP [6], randomly subsample the feature subsets (T𝑇Titalic_T,vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )) and approximately solve a weighted-least squares problem to estimate IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ). These algorithms require many model evaluations in practice and impose an undesirable trade-off between sample complexity and accuracy [19]. Model-based approximators, such as DeepSHAP [6], take advantage of the model architecture (e.g., neural networks) to estimate Shapley values. These methods are often faster but still require many model evaluations and only work for white-box models. Algorithms to compute Shapley interactions in black-box models [20, 7, 21, 22], such as SHAP-IQ [20], subsample high-order interactions for efficiency but similar to stochastic estimators they need many model evaluations. To empirically demonstrate how current SHAP algorithms scale, let us consider TIGER [23], a recent model that predicts the efficiency of CRISPR-Cas13d guide RNA from their sequence and context. To explain which region in the guide RNA is the most determinant of efficiency, we estimated SHAP values of 1038103810381038 input query sequences. Finding the KernelSHAP [6] values of all the nucleotides in these sequences took about one day on our single NVIDIA RTX A6000 machine. Worse yet, finding only up to 3rdsuperscript3rd3^{\text{rd}}3 start_POSTSUPERSCRIPT rd end_POSTSUPERSCRIPT order feature interactions using SHAP-IQ took more than 81 days—revealing a severe scalability issue in current explainability algorithms. We posit that the model evaluations needed to estimate the SHAP values of an input query sequence have information that can be used to estimate the SHAP values for a new query sequence. Therefore, instead of independently evaluating the model for each query sequence, one can “recycle” the model evaluations to slash the sample and computational cost of explaining the model. Taking this idea to the extreme, we propose to do initial query-agnostic model evaluations to sketch the model and then use the sketch for model explanation. How can a model be sketched to be efficiently mapped to SHAP values and Shapley interactions? We discover a surprisingly underexplored connection between SHAP values and interactions and the model’s Fourier transform, enabling us to sketch the model and use the sketch for fast Shapley explanations. The Fourier transform provides a global sketching of the model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) irrespective of the query sequence or even the training distribution. Moreover, since Fourier is an orthonormal basis, it enables fast and sample-efficient algorithms for sketching black-box models in genomics that are compressible or near-sparse in the Fourier domain [24, 25, 26, 27]. Herein, we develop SHAP zero, an algorithm that estimates SHAP values and Shapley interactions with a near-zero additional cost per new query sequence after paying an initial up-front cost for model sketching (Fig. 1b). In developing SHAP zero, we make three distinct and interconnected contributions: First, we build on the existing algorithms in sparse Fourier transforms [27, 28] and develop a method to sketch a black-box model f⁢(𝐱)𝑓𝐱f({\bf x})italic_f ( bold_x ) with sequence input (defined over q𝑞qitalic_q alphabets) and real-valued output, in terms of its top-s𝑠sitalic_s Fourier coefficients with a sample complexity of 𝒪⁢(s⁢n2)𝒪𝑠superscript𝑛2\mathcal{O}(sn^{2})caligraphic_O ( italic_s italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and a computational complexity of 𝒪⁢(s⁢n3)𝒪𝑠superscript𝑛3\mathcal{O}(sn^{3})caligraphic_O ( italic_s italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). Second, we establish a mathematical formalism to map from (i) the Fourier transform to a transform from algebraic geometry called the Möbius transform and (ii) from the Möbius transform to Shapley-based explanations. The Möbius transform [29, 30] enables us to map the top-s𝑠sitalic_s Fourier coefficients of the model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to Shapley-based explanations in 𝒪⁢(s2⁢(2⁢q)ℓ)𝒪superscript𝑠2superscript2𝑞ℓ\mathcal{O}(s^{2}(2q)^{\ell})caligraphic_O ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 2 italic_q ) start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) time per query sequence, where ℓ≤5ℓ5\ell\leq 5roman_ℓ ≤ 5 caps the maximum order of model interactions in practice [31]. Third, we conduct large-scale experiments to explain two genomic models, TIGER [23] and inDelphi [32], with SHAP zero. We demonstrate that SHAP zero estimates all-order feature interactions with an amortized computational cost up to 1000-fold faster than current algorithms. SHAP zero reveals the GC content of the seed region in TIGER and microhomologous motifs in inDelphi as predictive high-order features, a task previously inaccessible due to the combinatorial space of possible feature interactions."
Sparse Decomposition of Graph Neural Networks,"Graph Neural Networks (GNN) exhibit superior performance in graph representation learning, but their inference cost can be high, due to an aggregation operation that can require a memory fetch for a very large number of nodes. This inference cost is the major obstacle to deploying GNN models with online prediction to reflect the potentially dynamic node features. To address this, we propose an approach to reduce the number of nodes that are included during aggregation. We achieve this through a sparse decomposition, learning to approximate node representations using a weighted sum of linearly transformed features of a carefully selected subset of nodes within the extended neighbourhood. The approach achieves linear complexity with respect to the average node degree and the number of layers in the graph neural network. We introduce an algorithm to compute the optimal parameters for the sparse decomposition, ensuring an accurate approximation of the original GNN model, and present effective strategies to reduce the training time and improve the learning process. We demonstrate via extensive experiments that our method outperforms other baselines designed for inference speedup, achieving significant accuracy gains with comparable inference times for both node classification and spatio-temporal forecasting tasks.","Figure 1: The pipeline overview for SDGNN framework (bottom pipeline). To compute GNN embedding efficiently, we use a transformation function to adapt node features and introduce sparse vectors associated with each node to gather information from critical neighbours. The parameters in the transformation function and the sparse vectors are determined by optimization to approximate the target GNN embeddings. Graph neural networks (GNN) have demonstrated impressive performance for graph representation learning (Hamilton et al., 2017; Veličković et al., 2018; Qu et al., 2019; Rampášek et al., 2022). Although there are numerous designs for GNN models, the essential idea is to represent each node based on its features and its neighbourhood (Wu et al., 2020; Zhou et al., 2020). The procedure of aggregating features from neighbour nodes is empirically and theoretically effective (Xu et al., 2019) in representing the graph structures and blending the features of the nodes. However, deploying GNN models to process large graphs is challenging since collecting information from the neighbour nodes and computing the aggregation is extremely time-consuming (Zhang et al., 2021; Tian et al., 2023; Wu et al., 2023; Liu et al., 2024). In this work, we tackle the efficient inference problem for GNN models in the online prediction setting (Crankshaw, 2019). Specifically, we need to compute the representations of a few arbitrary nodes. The main advantage is that the prediction can reflect potential dynamic features111The features of a sample may vary over time, e.g., dynamical features from sensors (Dawson et al., 2016), dynamic features in the recommendation systems (Chu & Park, 2009). of the input. The computational complexity is dominated by the number of receptive nodes, which rapidly increases as the number of layers in the model grows, for most message-passing-based and graph-transformer-based GNNs (Zeng et al., 2020; Min et al., 2022). Our goal is to reduce the inference time to linear complexity with respect to the number of layers and the average node degree. Recently, several studies have attempted to address this problem by combining the performance of GNN and the efficiency of MLPs (Zhang et al., 2021; Hu et al., 2021; Tian et al., 2023; Wang et al., 2023; Wu et al., 2023; Liu et al., 2024; Tian et al., 2024; Winter et al., 2024; Wu et al., 2024). Knowledge distillation (Hinton et al., 2015) and feature/label smoothing are used to construct effective MLP models to eliminate the cumbersome neighbour collection and aggregation procedure. Although efficient, these methods have a fundamental limitation: the features gathered at each node are assumed to contain sufficient information to predict the node label accurately. However, to achieve their full potential, especially when features can change at inference time, GNN models should take into account the features from neighbourhood nodes and the graph structure (Battaglia et al., 2018; Pei et al., 2020). Therefore, we ask the question: given any graph neural network model that relies on both the graph structure and the features of the neighbourhood, can we infer the representation of a node in linear time? Present work. We propose sparse decomposition for graph neural networks (SDGNN), an approximation to the original GNN models that can infer node representations efficiently and effectively. The SDGNN consists of a feature transformation function and sparse weight vectors for nodes in the graph. The representation of each node is then a weighted sum of the transformed features from a small set of receptive nodes. The sparsity of the weight vectors guarantees low inference complexity. The learnable feature transformation function and the sparse weight vectors grant the SDGNN flexibility to approximate a wide range of targeted GNN models. To find the optimal parameters in SDGNN, we formulate the approximation task as an optimization problem and propose a scalable and efficient solution that iterates between the learning of the transformation function and the optimization of the sparse weight vectors. We verify the approximation power of SDGNN and the scalability of our algorithm on seven node classification datasets and demonstrate how SDGNN can be effectively applied under the online prediction setting with two spatio-temporal forecasting datasets. SDGNN consistently outperforms recent state-of-the-art models designed for GNN inference speedup."
Adversarial Environment Design via Regret-Guided Diffusion Models,"Training agents that are robust to environmental changes remains a significant challenge in deep reinforcement learning (RL). Unsupervised environment design (UED) has recently emerged to address this issue by generating a set of training environments tailored to the agent’s capabilities. While prior works demonstrate that UED has the potential to learn a robust policy, their performance is constrained by the capabilities of the environment generation. To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). The proposed method guides the diffusion-based environment generator with the regret of the agent to produce environments that the agent finds challenging but conducive to further improvement. By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy. Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments. Project page: https://github.com/rllab-snu.github.io/projects/ADD","Deep reinforcement learning (RL) has achieved great success in various challenging domains, such as Atari [1], GO [2], and real-world robotics tasks [3, 4]. Despite the progress, the deep RL agent struggles with the generalization problem; it often fails in unseen environments even with a small difference from the training environment distribution [5, 6]. To train well-generalizing policies, various prior works have used domain randomization (DR) [7, 8, 9], which provides RL agents with randomly generated environments. While DR enhances the diversity of the training environments, it requires a large number of trials to generate meaningful structures in high-dimensional domains. Curriculum reinforcement learning [10, 11] has been demonstrated to address these issues by providing instructive sequences of environments. Since manually designing an effective curriculum for complicated tasks is challenging, prior works [12, 13] focus on generating curricula that consider the current agent’s capabilities. Recently, unsupervised environment design (UED, [14]) has emerged as a scalable approach, notable for its advantage of requiring no prior knowledge. UED algorithms alternate between training the policy and designing training environments that maximize the regret of the agent. This closed-loop framework ensures the agent learns a minimax regret policy [15], assuming that the two-player game between the agent and the environment generator reaches the Nash equilibrium. There are two main approaches for UED: 1) learning-based methods, which employ an environment generator trained via reinforcement learning, and 2) replay-based methods, which selectively replay among previously generated environments. The learning-based methods [14, 16, 17] utilize an adaptive generator that controls the parameters that fully define the environment configuration. The generator receives a regret of the agent as a reward and is trained via reinforcement learning to produce environments that maximize the regret. While the learning-based methods can directly generate meaningful environments, training the generator with RL is unstable due to the moving manifold [16]. Additionally, we observe that the RL-based generator has limited environment coverage, which limits the generalization capability of the trained agent. In contrast, the replay-based methods [18, 19, 20] employ a random generator and select environments to revisit among previously generated environments. Since the random generator can produce diverse environments without additional training, they outperform the learning-based methods in zero-shot generalization tasks [20]. However, the replay-based methods are sample inefficient as they require additional episodes to evaluate the regret on the randomly generated environments. In this work, we propose a sample-efficient and robust UED algorithm by leveraging the strong representation power of diffusion models [21]. First, to make UED suitable for using a diffusion model as a generator, we introduce soft UED, which augments the regret objective of UED with an entropy regularization term, as done in maximum entropy RL [22]. By incorporating the entropy term, we can ensure the diversity of the generated environments. Then, we present adversarial environment design via regret-guided diffusion models (ADD), which guides a diffusion-based environment generator with the regret of the agent to produce environments that are conducive to the performance improvement of the agent. Enabling this regret guidance requires the gradient of the regret with respect to the environment parameter. However, since the true value of the regret is intractable and the regret estimation methods used in prior works on UED are not differentiable, a new form of regret estimation method is needed. To this end, we propose a novel method that enables the estimation of the regret in a differentiable form by utilizing an environment critic, which predicts a return distribution of the current policy on the given environment. This enables us to effectively integrate diffusion models within the UED framework, significantly enhancing the environment generation capability. Since the regret-guided diffusion does not require an additional training of the environment generator, we can preserve the ability to cover the high-dimensional environment domain as the random generator of the replay-based method. Moreover, ADD can directly generate meaningful environments via regret-guided sampling as the learning-based methods. By doing so, ADD effectively combines the strengths of previous UED methods while addressing some of their limitations. Additionally, unlike other UED methods, ADD allows us to control the difficulty levels of the environments it generates by guiding the generator with the probability of achieving a specific return. It enables the reuse of the learned generator in various applications, such as generating benchmarks. We conduct extensive experiments across challenging tasks commonly used in UED research: partially observable maze navigation and 2D bipedal locomotion over challenging terrain. Experimental results show that ADD achieves higher zero-shot generalization performance in unseen environments compared to the baselines. Furthermore, our analysis on the generated environments demonstrates that ADD produces an instructive curriculum with varying complexity while covering a large environment configuration space. As a result, it is shown that the proposed method successfully generates adversarial environments and facilitates the agent to learn a policy with solid generalization capabilities."
Super Gradient Descent: Global Optimization requires Global Gradient,"Global minimization is a fundamental challenge in optimization, especially in machine learning, where finding the global minimum of a function directly impacts model performance and convergence. This report introduces a novel optimization method that we called Super Gradient Descent, designed specifically for one-dimensional functions, guaranteeing convergence to the global minimum for any k𝑘kitalic_k-Lipschitz function defined on a closed interval [a,b]𝑎𝑏[a,b][ italic_a , italic_b ]. Our approach addresses the limitations of traditional optimization algorithms, which often get trapped in local minima. In particular, we introduce the concept of global gradient which offers a robust solution for precise and well-guided global optimization. By focusing on the global minimization problem, this work bridges a critical gap in optimization theory, offering new insights and practical advancements in different optimization problems in particular Machine Learning problems like line search.","Global optimization plays a critical role in addressing complex real-life challenges across various fields. In engineering, it is applied to structural design optimization, where minimizing weight or material use while ensuring durability is essential for cost-effective and safe construction. In financial services, portfolio optimization requires balancing risk and return by finding the global minimum or maximum in investment strategies. In logistics and transportation, global optimization is crucial for solving routing problems such as determining the shortest path or optimizing delivery routes which leads to significant cost savings and improved efficiency. Similarly, in energy systems, global optimization is key to managing and distributing power more efficiently, reducing operational costs, and optimizing renewable energy usage. In machine learning, the need for global optimization is especially pronounced. The performance of models often depends on the ability to minimize complex, non-convex loss functions. While traditional methods like gradient descent are effective in many cases, they frequently encounter the problem of getting trapped in local minima, which can hinder the model’s overall performance. This is particularly relevant in tasks that require complex models where the optimization landscape is highly non-linear and fraught with local minima. The primary contribution of this work is the introduction of a novel algorithm named Super Gradient Descent. Unlike classical gradient descent, which collects only local information making it prone to local minima, the proposed method adapts the state’s change decision based on a global detection of the function change to ensure consistent progress towards the global minimum. We evaluate its performance on various one-dimensional functions, demonstrating that it provides superior convergence behavior, particularly in avoiding local minima and achieving the global optimum. This novel approach contributes to overcoming the challenges of non-convex optimization, offering a more reliable method for finding global solutions in machine learning."
Robust Thompson Sampling Algorithms Against Reward Poisoning Attacks,"Thompson sampling is one of the most popular learning algorithms for online sequential decision-making problems and has rich real-world applications. However, current Thompson sampling algorithms are limited by the assumption that the rewards received are uncorrupted, which may not be true in real-world applications where adversarial reward poisoning exists. To make Thompson sampling more reliable, we want to make it robust against adversarial reward poisoning. The main challenge is that one can no longer compute the actual posteriors for the true reward, as the agent can only observe the rewards after corruption. In this work, we solve this problem by computing pseudo-posteriors that are less likely to be manipulated by the attack. We propose robust algorithms based on Thompson sampling for the popular stochastic and contextual linear bandit settings in both cases where the agent is aware or unaware of the budget of the attacker. We theoretically show that our algorithms guarantee near-optimal regret under any attack strategy.","The multi-armed bandit (MAB) setting is a popular learning paradigm for solving sequential decision-making problems (Slivkins et al., 2019). The stochastic and linear contextual MAB settings are the most fundamental and representative of the different bandit settings. Due to their simplicity, many industrial applications such as recommendation systems frame their problems as stochastic or contextual linear MAB (Brodén et al., 2018; Chu et al., 2011). As one of the most famous stochastic bandit algorithms, Thompson sampling has been widely applied in these applications and achieves excellent performance both empirically (Chapelle & Li, 2011; Scott, 2010) and theoretically (Agrawal & Goyal, 2013; 2017). Compared to another popular exploration strategy known as optimality in the face of uncertainty (OFUL/UCB), Thompson sampling has several advantages: • Utilizing prior information: By design, Thompson sampling algorithms utilize and benefit from the prior information about the arms. • Easy to implement: While the regret of a UCB algorithm depends critically on the specific choice of upper-confidence bound, Thompson sampling depends only on the best possible choice. This becomes an advantage when there are complicated dependencies among actions, as designing and computing with appropriate upper confidence bounds present significant challenges Russo et al. (2018). In practice, Thompson sampling is usually easier to implement Chapelle & Li (2011). • Stochastic exploration: Thompson sampling is a random exploration strategy, which could be more resilient under some bandit settings Lancewicki et al. (2021). Despite the success, Thompson sampling faces the problem of low efficacy under adversarial reward poisoning attacks Jun et al. (2018); Xu et al. (2021); Liu & Shroff (2019). Existing algorithms assume that the reward signals corresponding to selecting an arm are drawn stochastically from a fixed distribution depending on the arm. However, this assumption does not always hold in the real world. For example, a malicious user can provide an adversarial signal for an article from a recommendation system. Even under small corruption, Thompson sampling algorithms suffer from significant regret under attacks. While robust versions of the learning algorithms following other fundamental exploration strategies such as optimality in the face of uncertainty (OFUL) and ϵitalic-ϵ\epsilonitalic_ϵ-greedy were developed Lykouris et al. (2018); Neu & Olkhovskaya (2020); Ding et al. (2022); He et al. (2022); Xu et al. (2023), there has been no prior investigation of robust Thompson sampling algorithms. The main challenge is that under the reward poisoning attacks, it becomes impossible to compute the actual posteriors based on the true reward, which is essentially required by the algorithm. Naively computing the posteriors based on the corrupted reward causes the algorithm to be manipulated by the attacker arbitrarily (Xu et al., 2021). This work. We are the first to show the feasibility of making Thompson sampling algorithms robust against adversarial reward poisoning. Our main contribution is developing robust Thompson sampling algorithms for stochastic and linear contextual bandits. We consider both cases where the corruption budget of the attack is known or unknown to the learning agent. The regrets induced by our algorithms under the attack are near-optimal with theoretical guarantees. We adopt two ideas to achieve robustness against reward poisoning attacks in the two MAB settings. The first idea is ‘optimality in the face of corruption.’ In the stochastic MAB setting, we show that the Thompson sampling algorithm can maintain sufficient explorations on arms and identify the optimal arm by relying on optimistic posteriors considering potential attacks. The second idea is to adopt a weighted estimator He et al. (2023) that is less susceptible to the attack. In the linear contextual MAB setting, we show that with such an estimator, the influence of the attack on the estimation of the posteriors is limited, and the Thompson sampling algorithm can almost always identify the optimal arm at each round with a high probability. We empirically demonstrate the training process of our algorithms under the attacks and show that our algorithms are much more robust than other fundamental bandit algorithms, such as UCB, in practice. Compared to the state-of-the-art robust algorithm CW-OFUL He et al. (2022) for linear contextual bandit setting, our algorithm is as efficient, and in addition, it inherits the advantages from using Thompson sampling exploration strategy as aforementioned."
Learning the Regularization Strength for Deep Fine-Tuning via a Data-EmphasizedVariational Objective,"A number of popular transfer learning methods rely on grid search to select regularization hyperparameters that control over-fitting. This grid search requirement has several key disadvantages: the search is computationally expensive, requires carving out a validation set that reduces the size of available data for model training, and requires practitioners to specify candidate values. In this paper, we propose an alternative to grid search: directly learning regularization hyperparameters on the full training set via model selection techniques based on the evidence lower bound (“ELBo”) objective from variational methods. For deep neural networks with millions of parameters, we specifically recommend a modified ELBo that upweights the influence of the data likelihood relative to the prior while remaining a valid bound on the evidence for Bayesian model selection. Our proposed technique overcomes all three disadvantages of grid search. We demonstrate effectiveness on image classification tasks on several datasets, yielding heldout accuracy comparable to existing approaches with far less compute time.","††footnotetext: Open-source code: https://github.com/tufts-ml/data-emphasized-ELBo When fine-tuning deep neural networks (DNNs), a significant amount of computational resources are devoted to tuning hyperparameters that control model complexity to manage tradeoffs between under- and over-fitting on the target task of interest. One widespread example would be tuning the value of the scalar multiplier that controls the strength of an additive loss term computed as the sum-of-squares on weight coefficient values, known in various communities as L2 regularization [33], Ridge penalty [14, 21], or “weight decay” [24, 9]. A common technique for tuning such hyperparameters is to hold out a dedicated validation set and use grid search to find the hyperparameters that perform best on the validation set [36, 32]. While reasonably effective and in widespread use to manage over-fitting in recent transfer learning [45, 39], using grid search for hyperparameter selection has three key drawbacks. First and perhaps most important, the need to train separate models at each possible value in the grid significantly increases computational runtime and resources. Second, the need to carve out a validation set to assess performance reduces the amount of available data that can inform model training. This can cause under-fitting, especially when available data has limited size. Finally, grid search requires a list of candidate values specified in advance, yet ideal values may vary widely depending on the data and specific classification task at hand. We take another approach to hyperparameter selection, inspired by a pragmatic Bayesian perspective. Suppose we model observable dataset 𝒟𝒟\mathcal{D}caligraphic_D via a likelihood p⁢(𝒟|θ)𝑝conditional𝒟𝜃p(\mathcal{D}|\theta)italic_p ( caligraphic_D | italic_θ ), where θ𝜃\thetaitalic_θ is a high-dimensional parameter to be estimated, with prior distribution p⁢(θ|η)𝑝conditional𝜃𝜂p(\theta|\eta)italic_p ( italic_θ | italic_η ) controlled by hyperparameter η𝜂\etaitalic_η (say just 1-5 dimensions). Instead of point estimating a specific θ,η𝜃𝜂\theta,\etaitalic_θ , italic_η pair, we can instead estimate a posterior p⁢(θ|𝒟,η)𝑝conditional𝜃𝒟𝜂p(\theta|\mathcal{D},\eta)italic_p ( italic_θ | caligraphic_D , italic_η ) while simultaneously directly learning η𝜂\etaitalic_η to optimize p⁢(𝒟|η)=∫θp⁢(𝒟,θ|η)⁢𝑑θ𝑝conditional𝒟𝜂subscript𝜃𝑝𝒟conditional𝜃𝜂differential-d𝜃p(\mathcal{D}|\eta)=\int_{\theta}p(\mathcal{D},\theta|\eta)d\thetaitalic_p ( caligraphic_D | italic_η ) = ∫ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_p ( caligraphic_D , italic_θ | italic_η ) italic_d italic_θ. This latter objective p⁢(𝒟|η)𝑝conditional𝒟𝜂p(\mathcal{D}|\eta)italic_p ( caligraphic_D | italic_η ) is known as the marginal likelihood or evidence [26]. The evidence naturally encodes a notion of Occam’s razor, favoring the hyperparameter setting that leads to the simplest model that fits the data well, while penalizing complex models that over-fit the training data [18, 27, 2]. Learning η𝜂\etaitalic_η to maximize evidence (or equivalently, the logarithm of evidence) via gradient descent avoids all three issues with grid search: we need only one run of gradient descent (not separate efforts for each candidate η𝜂\etaitalic_η value in a grid), we can use all available labeled data for training without any validation set, and we can explore the full continuous range of possible η𝜂\etaitalic_η values rather than a limited discrete set that must be predefined. While elegant in theory, this vision of selecting hyperparameters via maximizing evidence is difficult in practice for most models of interest due to the intractable high-dimensional integral that defines the evidence. For modern deep image classifiers with millions of parameters, computing the evidence directly seems insurmountable even for a specifc η𝜂\etaitalic_η, let alone optimizing evidence to select a preferred η𝜂\etaitalic_η value. In this work, we use and extend tools from variational Bayesian methods [3, 19], specifically tractable lower bounds on the evidence, to make hyperparameter selection for fine-tuning deep neural image classifiers possible. Ultimately, we contribute methods that should help practitioners perform cost-effective transfer learning on custom datasets. When available data is plentiful, our experiments suggest our approach is competitive in accuracy while reducing total training time from 16 hours for L2-SP [45] and 150 hours for PTYL [39] (using the grid search ranges recommended by the original authors) to under 3 hours. When available data is limited, e.g., only 5-300 labeled examples per class, our experiments suggest our approach can be particularly effective in improving accuracy and runtime."
Spatial Shortcuts in Graph Neural Controlled Differential Equations,"rnn short=RNN, long=Recurrent Neural Network \DeclareAcronymnde short=NDE, long=Neural Differential Equation \DeclareAcronymgncde short=GNCDE, long=Graph Neural Controlled Differential Equation \DeclareAcronymncde short=NCDE, long=Neural Controlled Differential Equation \DeclareAcronymnn short=NN, long=Neural Network \DeclareAcronymnode short=NODE, long=Neural Ordinary Differential Equation \DeclareAcronympde short=PDE, long=Partial Differential Equation \DeclareAcronymgnn short=GNN, long=Graph Neural Network \DeclareAcronymlstm short=LSTM, long=Long Short Term Memory \DeclareAcronymgru short=GRU, long=Gated Recurrent Unit \DeclareAcronymagc short=AGC, long=Adaptive Graph Convolution \DeclareAcronymresnet short=ResNet, long=Residual Neural Network \DeclareAcronymmae short=MAE, long=Mean Absolute Error","Effect follows cause. When a problem is represented on a graph, its structure contains information on how causes at one spatial position are linked to effects at another. To learn about physical dynamics from spatial time series data, one can often leverage this structural information. Schölkopf [1] describes differential equations as the gold standard for understanding cause-effect structures and highlight the lack of a time component in statistical machine learning methods. \acpnde [2] are able to learn a hidden state that evolves continuously in time, and could remedy this lack. With Neural Controlled Differential Equations (NCDE) [3], one can update the hidden state continuously with data incoming at different points in time. If one also wants to account for spatial dependencies, \acpgncde [4] can be used, where a node embedding is learned to capture these spatial dependencies. We incorporate prior known graph topology information into a \acgncde to infer the future dynamics at the vertices. We therefore generate data coming from a graph advection simulation from which we know the underlying graph topology plus the temporal cause and effect relation. We will outline the close connection between the graph information in the data generated and the artificial \acnn architecture. Then we train our Informed \acpgncde and let them learn the dynamics to predict their future behavior. We start by describing advection on graphs and the theory before we explain the data generation and our Informed \acgncde. We believe this approach can lead to improvements in domains where graph information is available or where time series data is scarce or partially missing. \acpncde are an effective method in this context due to their ability to handle irregular time series data [5]. With graph information one has the potential to predict with fewer observations, as the graph structure itself does not have to be learned on top of the temporal dynamics. Promising domains for introducing known graph structure are traffic forecasting, river water level forecasting, climate and weather prediction, or disease spread (see A.4). Figure 1: Advection of an initial Gaussian pulse on a graph with 5 edges over time"
MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular Metaverse Services,"Updates of extensive Internet of Things (IoT) data are critical to the immersion of vehicular metaverse services. However, providing high-quality and sustainable data in unstable and resource-constrained vehicular networks remains a significant challenge. To address this problem, we put forth a novel immersion-aware model trading framework that incentivizes metaverse users (MUs) to contribute learning models trained by their latest local data for augmented reality (AR) services in the vehicular metaverse, while preserving their privacy through federated learning. To comprehensively evaluate the contribution of locally trained learning models provided by MUs to AR services, we design a new immersion metric that captures service immersion by considering the freshness and accuracy of learning models, as well as the amount and potential value of raw data used for training. We model the trading interactions between metaverse service providers (MSPs) and MUs as an equilibrium problem with equilibrium constraints (EPEC) to analyze and balance their costs and gains. Moreover, considering dynamic network conditions and privacy concerns, we formulate the reward decisions of MSPs as a multi-agent Markov decision process. Then, a fully distributed dynamic reward method based on deep reinforcement learning is presented, which operates without any private information about MUs and other MSPs. Experimental results demonstrate that the proposed framework can effectively provide higher-value models for object detection and classification in AR services on real AR-related vehicle datasets compared to benchmark schemes.","The metaverse, envisioned as one of the next evolution directions of the Internet, is designed to create a fully immersive, self-sustaining virtual environment in which people can engage in activities such as playing, working, and socializing [1]. This vision is propelled by advances in the fifth-generation (5G) and the forthcoming sixth-generation (6G) communication technologies, renowned for their low latency and high data throughput. These technologies play a critical role in seamlessly integrating the Internet of Things (IoT) data into metaverse services, thus bringing the once-fictional concept of immersive experiences closer to reality. Metaverse services are beginning to reveal their vast potential across a broad spectrum of industries, from gaming and autonomous driving to education and marketing. Notably, the application of vehicles within the metaverse has captured significant interest due to the enhanced safety features and immersive experiences offered by state-of-the-art augmented reality (AR) technologies. Figure 1: An example of AR services in the vehicular metaverse222Image source: https://www.jasoren.com/ar-in-automotive/. Market report [2] forecasted that the global automotive metaverse market will grow from 1.91.91.91.9 billion in 2022 to 6.56.56.56.5 billion by 2030. Automakers like BMW have recently doubled down on AR technology in their projects. As shown in Fig. 2, augmented information makes driving safer by showing potential hazards hidden behind the vehicle in front of you. Moreover, Nissan’s [3] upcoming technology utilizes a 3D AR interface that merges the real world with the virtual world to provide the driver with augmented information about the surrounding area. In addition, Vanarama has also implemented a visual parking assistance function using AR [4], which is expected to identify empty spaces in busy car parks and display useful information, such as parking fees, on the windscreen. The ability to capture information from the “real” world, particularly the ability to collect and process massive data from IoT devices, is the key to determining the success of immersive services (e.g., AR) in the vehicular metaverse. Meanwhile, the data must be processed and presented in a meaningful, responsive, and appropriately protected way. Technically, a high-quality experience with AR services relies on accurate detection and classification of real-world objects (e.g., cars and pedestrians) under complex conditions [5]. To achieve this goal, sufficient valid data needs to be collected and processed in depth to detect and classify objects accurately. Therefore, it is essential to focus on effectively collecting, processing, and protecting the data that supports a safer and more enjoyable driving experience. Motivations. An existing widely-used data collection method, as adopted by Nissan [6], involves collecting massive amounts of data through vehicle sensors, cameras, and roadside devices, and then processing all the data centrally. However, when it comes to the situation with multiple metaverse users (MUs) and metaverse service providers (MSPs) on behalf of different companies, the centralized data collection approach is not applicable and may lead to the following issues. First, AR services in the vehicular metaverse need to be highly immersive so that MUs feel fully immersed in the rendered environment, such as visualized driving. However, providing a seamless AR experience is challenging due to the latency caused by massive real-time data updates under unstable and resource-limited communication conditions. Note that the value of real-time data diminishes over time [7]. Also, delays can severely impact the MU’s experience and cause dizziness [8]. Second, the data to be transmitted may be sensitive and private, such as location, movement, and biometrics, which can create a better immersive experience but may inevitably increase the privacy risk of MUs [9]. Third, it is difficult to share data between MSPs due to competition. This leads to the same data being sampled multiple times by different MSPs in the physical world, resulting in wasted resources (e.g., sampling costs) and data redundancy. Moreover, tremendous data pressure and computational burden are imposed on MSPs. Therefore, an efficient method for collecting high-quality data while preserving privacy is needed to achieve immersive AR services. Challenges. To address the above issues, we propose an immersion-aware model trading framework designed to provide data support for AR services and ensure privacy by integrating federated learning (FL). Adopting FL enables MUs to contribute diverse learning models to multiple MSPs using local data, thus effectively protecting sensitive information [10]. Moreover, learning models are uploaded to MSPs for AR services by individual MUs, rather than centrally transmitting large amounts of data, which significantly reduces the communication burden. Nonetheless, developing such a model trading framework still faces the following challenges: • MUs are typically self-interested and are reluctant to share learning models with MSPs due to the additional computation, communication, and energy overhead. Thus, the challenge here is how to incentivize them to become contributors in providing high-value learning models that will benefit vehicular metaverse services. • It is difficult to design an appropriate metric to comprehensively evaluate the value of learning models contributed by MUs due to the diverse aspects involved in model valuation. • MUs have different sampling costs and limited computation and communication resources, while MSPs differ in their model preferences and compete with each other. Hence, the next challenge is to model the dynamic competitive interactions among MSPs and to achieve an equilibrium of interests among MUs and MSPs in the model trading. Figure 2: The outline of an immersion-aware framework for FL-assisted vehicular metaverse. Contributions. To address these challenges, we model the dynamic competitive trading interactions as an equilibrium problem with equilibrium constraints (EPEC) involving multiple leaders (MSPs) and multiple followers (MUs), which is a hierarchical optimization problem with equilibria at two levels [11]. Furthermore, to evaluate the value of learning models contributed by MUs to MSPs, we design a new metric called the immersion of the learning model (IoM). The metric jointly considers the freshness and accuracy of the learning model, as well as the amount and potential value of raw data used for training. Moreover, given the dynamic networks and the privacy concerns of MSPs, we formulate the reward decisions of MSPs as a multi-agent Markov decision process (MAMDP) and develop a multi-agent deep reinforcement learning (DRL)-based dynamic reward (MDDR) approach to obtain the reward decisions in a fully distributed manner. The outline of the proposed framework is shown in Fig. 2, and the main contributions of this paper are summarized as follows: • We propose an immersion-aware trading framework for learning models with FL assistance, incentivizing MUs to contribute high-value learning models for MSPs in a privacy-preserving and efficient manner. Moreover, a novel metric called “IoM” is designed to quantify the immersion enhancement provided by MUs for AR services. • We model the dynamic competitive trading interactions as an EPEC and provide theoretical support for the existence and uniqueness of equilibria at two levels. Moreover, a fully distributed MDDR approach is developed to adapt to dynamic environments and address complex reward decisions of MSPs without accessing any private information of MUs or MSPs. • We conduct extensive numerical simulations based on AR-related vehicle datasets to validate the efficacy and efficiency of MDDR and the proposed immersion-aware model trading framework. Compared to benchmark schemes, the framework better motivates MUs to provide higher-value learning models for object detection and classification functions of AR services effectively. The rest of this paper is organized as follows. Section II discusses the related work. In Section III, we present the system overview and design the immersion metric of the learning model. Section IV gives the game formulation, and Section V analyzes the existence of the equilibria at two levels. In Section VI, we give the detailed design of MDDR. Section VII shows numerical experiments to evaluate the framework performance, and finally Section VIII concludes the paper."
Conformal Prediction for Multimodal Regression,capbtabboxtable[][\FBwidth],"1 Background and related work The comprehension of the internals of neural networks has begun with efforts towards explainability but little has been done to exploit these internal features. The area of conformal prediction (CP) for regression is no different. The consequence is that rich multimodal input features such as images, unstructured text, and categoricals have not participated in the conformal regression prediction uncertainty quantification process. CP provides a straightforward framework for constructing statistically rigorous uncertainty intervals for model predictions. Importantly, these intervals are valid regardless of the underlying distribution, providing explicit, non-asymptotic guarantees without depending on specific distributional or model assumptions. By using conformal prediction on any pre-trained model, it is possible to create intervals that consistently contain the true value with a specified coverage level. Angelopoulos and Bates (2021). CP input assumptions are based on data exchangeability. There is no other assumption about the data distribution or the model. This condition is satisfied for the problems outlined in this paper. The more developed field of CP classification has done image-to-image uncertainty Angelopoulos et al. (2022), and even proposed changing CP regression tasks into classification problems Guha et al. (2023). Despite a comprehensive literature review, no findings similar to those proposed by our novel use of internal features in CP for multimodal regression have been uncovered. The inductive, or split, approach to creating conformal regressors, outlined by Papadopoulos et al. (2002), involves splitting the training data into a proper training set and a calibration set, both of which should accurately represent the overall data distribution. The training set is used to build the regression model, while the calibration set is used to calculate the model’s absolute residuals. Conformal predictive systems (CPS) enhance conformal regressors by generating cumulative probability distributions, over potential target values Vovk et al. (2020). This enables the creation of PIs with a specified confidence level, effectively capturing prediction uncertainty while providing a statistically grounded basis for decision-making. Typically, the more complicated multimodal inputs of images and unstructured text are processed by larger more complicated networks (e.g., convolutional neural networks (CNN), transformers, hybrid networks, etc.). These networks can be compute-intensive. Therefore, building multiple models and having PIs generated in an ensemble fashion may not be an option. The inductive implementation of the aforementioned CPSs was the chosen approach as the model is only trained once after a hold-out calibration set is removed from the training set. Multimodal architectures generally contain an internal combining stage where all the processed inputs come together. These internal inputs will be referred to as internal features within the respective architectures. Conversely, external features are features that are fed to the model at the input layer. The possible advantage with working with internal features is that they have been filtered for significance and weighted for importance which is not true of the input features. This results in a metric suitable for distance-based conformal prediction. The major contribution of this work is to address the challenges associated with conformal prediction in the context of multimodal regression tasks involving diverse input data, such as tabular, unstructured text, and image modalities. Traditional conformal prediction methods face significant obstacles when applied directly to such heterogeneous input features in regression problems. However, this paper demonstrates a novel approach that leverages an internal feature layer within the model architecture for multimodal regression. Specifically, we show that this intermediate representation can be effectively utilized to generate a calibration set suitable for conformal prediction in regression scenarios. This methodology extends the applicability of conformal prediction techniques to complex, multimodal regression tasks, thereby enhancing uncertainty quantification capabilities for a wider range of machine learning regression models and diverse data types."
Deep learning-based identification of patients at increased risk of cancer using routine laboratory markers,"Early screening for cancer has proven to improve the survival rate and spare patients from intensive and costly treatments due to late diagnosis. Cancer screening in the healthy population involves an initial risk stratification step to determine the screening method and frequency, primarily to optimize resource allocation by targeting screening towards individuals who draw most benefit. For most screening programs, age and clinical risk factors such as family history are part of the initial risk stratification algorithm. In this paper, we focus on developing a blood marker-based risk stratification approach, which could be used to identify patients with elevated cancer risk to be encouraged for taking a diagnostic test or participate in a screening program. We demonstrate that the combination of simple, widely available blood tests, such as complete blood count and complete metabolic panel, could potentially be used to identify patients at risk for colorectal, liver, and lung cancers with areas under the ROC curve of 0.76, 0.85, 0.78, respectively. Furthermore, we hypothesize that such an approach could not only be used as pre-screening risk assessment for individuals but also as population health management tool, for example to better interrogate the cancer risk in certain sub-populations.","This paper focuses on the use of multiple biomarkers for the assessment of patients, or identification of otherwise healthy individuals, who are at increased risk of cancer. With the high mortality rate associated with cancer patients, significant research has been conducted to help identify patients at higher risk, starting with identifying medical conditions that increase the risk of cancer, such as diabetes, or genetic predispositions that promote its development[1]. Furthermore, various screening procedures have been developed to help facilitate early diagnosis such as the Faecal Immunochemical Test (FIT) and colonoscopy for colorectal cancer (CRC)[2], mammography for breast cancer[3], and low-dose computed tomography (LDCT) for lung cancer[4]. However, cancer screening rates and their uptake remains lower than desired, e.g., in the US[5]. While there are several factors contributing to this low uptake, one of the key factors is the lack of awareness within the general population. This is even more important to address for people who may be at increased risk and would benefit from early and/or regular screening. In other words, there is still a need for convenient tests for early detection of rapidly progressing diseases such as cancer so that intervention can start as early as possible[6]. Several cancer risk prediction/assessment tools based on demographic, socioeconomic or blood based markers have been developed over the years, and studies have shown that cancer risk assessment algorithms could have an impact in early cancer diagnosis[7]. For instance, the Qcancer 10 year risk algorithm[8] considers the age, ethnicity, deprivation, body mass index, smoking, alcohol, previous cancer diagnoses, family history of cancer, relevant comorbidities, and medication data for a patient and predicts the cancer risk for 11 types of cancers. Nartowt et al.[9] reported high concordance in the prediction of CRC into low, medium and high groups using an artificial neural network trained on patient data comprising age, sex, and complete blood count (CBC). ColonFlag[10] can be used to identify individuals at high risk of CRC using specific blood-based markers and refer them to screening procedures such as colonoscopy. More recently, a cell-free DNA-based blood test for the early detection of CRC has been clinically validated in the ECLIPSE study[11]. Moreover, multi-cancer early detection technologies[12] such as the Galleri test[13, 14] can identify abnormal methylation patterns in cell-free DNA to detect a cancer signal and predict its origin. Besides algorithm development, the deployment of the algorithm and communication of the findings play a critical role in acceptance and clinical use of the algorithm[15], and must be taken into account to facilitate screening uptake. To this end, instead of defining a test with a specific set of ingredients catered towards a particular cancer, we propose to use commonly measured blood markers, often obtained during the annual physical exam, and obtain a risk profile for multiple cancers. Furthermore, instead of reporting a risk score, we compute the pre- and post-test odds of a patient at risk of developing cancer over the next 12 months. A key challenge in developing a model that considers several biomarkers is to deal with a significant degree of missingness in the historical data as not all markers may be obtained at each encounter. Although this issue can be partly alleviated by considering the biomarkers obtained at an annual physical exam, we observed that in real world data, there is still a significant degree of missingness, either due to a lack of awareness, insurance coverage or reimbursement, among other reasons. A standard approach to deal with missingness in input data is to impute the missing values using statistical methods, such as expectation maximization and regression[16]. However, the quality of imputed data is limited and can significantly impact the generalization ability of the trained model. In this work, we address the aforementioned challenges by training a deep learning model, Deep Profiler, which takes the age, sex, and commonly obtained blood biomarkers included in CBC and Comprehensive Metabolic Panel (CMP), and outputs a likelihood ratio of a patient to develop cancer over the period of the following 12 months (see Figure 1). The Deep Profiler architecture employs a variational autoencoder (VAE) model that is pre-trained to impute missing data similar to the masked language modeling technique. Subsequently, we train cancer-specific risk prediction models from the shared encoded latent space and compute the likelihood ratio for each patient. We validate the proposed method over screening-relevant cohorts for three different cancers - colorectal, liver, and lung. These are among the top cancers responsible for cancer related mortality rate in the US (https://seer.cancer.gov/statfacts/html/common.html, accessed April 30, 2024.). Figure 1: Workflow of using a biomarker-based pre-screening test."
Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites.,"Machine learning (ML) models benefit from large datasets. Collecting data in biomedical domains is costly and challenging, hence, combining datasets has become a common practice. However, datasets obtained under different conditions could present undesired site-specific variability. Data harmonization methods aim to remove site-specific variance while retaining biologically relevant information. This study evaluates the effectiveness of popularly used ComBat-based methods for harmonizing data in scenarios where the class balance is not equal across sites. We find that these methods struggle with data leakage issues. To overcome this problem, we propose a novel approach “PrettYharmonize”, designed to harmonize data by pretending the target labels. We validate our approach using controlled datasets designed to benchmark the utility of harmonization. Finally, using real-world MRI and clinical data, we compare leakage-prone methods with “PrettYharmonize” and show that it achieves comparable performance while avoiding data leakage, particularly in site-target-dependence scenarios.","Many research fields have greatly benefited from machine learning (ML) approaches. ML models can extract important values from large amounts of data. Having vast data benefits the model’s classification performance and helps capture the underlying patterns, promoting better generalization to new unseen data. This makes combining multiple datasets an appealing approach, especially in domains where obtaining data in a uniform setting is challenging 1. Moreover, small health or research centers that can not afford to collect a large number in-house data, using data acquired in different sites is the only possibility for train ML models. However, different datasets obtained under different conditions often present variability due to differences in the acquisition procedure that are unrelated to relevant biological information 2. This undesired variability, also known as Effects of Site (EoS), can induce biased results if present or not correctly removed 3. These differences may come from systematic differences, which can be corrected, or random variations, which can not be modeled or corrected by harmonization. This problem is of common occurrence in many biomedical domains. For example, clinical data is affected by the acquisition site, as different hospitals have different laboratory machines, procedures, and criteria. Another example is the medical imaging field, as images are affected by acquisition protocol, scanner drifts, and time of the day, just to name a few factors 3, 4. Within this field, Magnetic Resonance Imaging (MRI) images are particularly susceptible to this site-related variance, like the magnetic field strength, room temperature fluctuation or changes in the electromagnetic noise, which makes even images obtained from scanners with the same manufacturer and the same parameters exhibit different characteristics 5, 6. Many works showed that removing this undesired systematic variability, which is only related to the acquisition site and has no biological information, can benefit further analysis made with the data 7, 8, 9, 10, 11. To this end, several Methods Aiming to Remove the Effects of Site (MAREoS) have been proposed and developed 4, 12. These MAREoS methods are typically used as a pre-processing step, where the site effects are removed and the “site-effect free” data, also known as harmonized data, is used for statistical analysis or to train and evaluate ML models. Among these MAREoS, the ones based on “ComBat” are extensively used in several domains. The ComBat method was originally proposed for correcting batch differences in genomic data 13 and was later adapted to other domains like MRI data 14, 7. ComBat uses Bayesian regression to find additive (location) and multiplicative (scale) corrections for each feature in each site. Within the ComBat-based methods, “neuroHarmonize” was proposed 15 to allow for the preservation of non-linear covariate effects and has been widely used since 4, 12, 16, 17. Although ComBat and its derivations have been widely applied in medical imaging data, several concerns have been raised, mainly because ComBat’s hypothesis and assumptions only hold for genomic data, where it was originally proposed, and may not be fulfilled in other applications fields 18. Additionally, concerns had been raised on the integration of ComBat into ML models, as the location and scale parameters of the model could not be learned in a subset of data (train data) and then applied to a new unseen subset of data (test data) 19. Early implementations of ComBat 14, 20 used the whole dataset to estimate the model’s parameters and create a harmonized dataset that is then used from all the downstream analyses. This approach was used in several works 7, 8, 21, 22, 23, 24. While this approach is valid when performing statistical analyses, it is not consistent with machine learning applications where the training and test data must be separate 25, 19. Specifically, the parameters of the models, including preprocessing models, must be obtained on a training set and then applied to the test set. This separation is important to get realistic estimates of generalization performance (e.g. using cross-validation) and to ensure deployability of the model in the real world where the test data is not yet available 26, 27. ComBat-MAGA 28, neuroHarmonize 15, and ”harmonizer” 19, which is based on neuroHarmonize, allow the estimation of the model’s parameters in a training set and apply them to the test samples, however, a critical assumption of ComBat is that all variance not shared across sites is unwanted site-related variance. Consequently, ComBat removes any variance that is not common to all sites, including the relevant biological variance. This poses a new problem, as this assumption is broken when a class imbalance occurs across sites and a target-site dependence exists, for example when the control patients are acquired in one site and target patients in a different one 29. This could also be extended to other possible biological information like comorbidities or disease severity, for example, if more severe patients are consistently treated or acquired only in one site. In these cases, even though ComBat will provide harmonized data, it will remove the variance related to the target (control versus patient) as the assumption is that only non-relevant factors change between sites. ComBat allows to retain the biologically relevant variables, for example, a diagnosis, the age, or the sex of a patient, by providing these variables as covariates to be retained. Nonetheless, this information is needed both when training the model and when applying the model to the test data. Thus, if target labels need to be preserved, this inevitably leads to the model requiring the test labels preventing the model’s use in real-world applications where test labels are not available or known18. This phenomenon where information of the test set is presented to the models is commonly known as data leakage. It is also well described that leaking the test target information would produce overconfident results, which could be misleading and can jeopardize the progress of an entire research field, as researchers who avoid data leakage would not be able to outperform the models that present data leakage 26, 27, 25. In this work, we aim to empirically demonstrate a shortcoming of ComBat-based harmonization in site-target dependence scenarios, i.e., that the model can properly harmonize the data only when test labels are used and data leakage happens. To do so, we performed controlled experiments for age regression and sex classification using real MRI data for healthy control individuals. Also using MRI data, a dementia and mild cognitive impairment (MCI) classification experiment was performed. Additionally, an outcome prediction of septic patients was performed using clinical data. All experiments were conducted both in site-target dependence and independence scenarios. Several harmonization schemes were used and compared, allowing and not allowing leakage, to harmonize the data. Finally, to overcome the aforementioned problem, we propose a new harmonization method, called PRETended Target Y Harmonize (PrettYHarmonize), which allows the users to integrate ComBat in an ML pipeline, harmonizing the data and generating a prediction without using the test labels and thus avoiding data leakage. We validated our method using benchmark datasets 3. Additionally, the proposed method was compared with the other harmonization schemes on the site-target dependence and independence scenarios to comprehensively compare no harmonization, leakage, and no-leakage methods. The corresponding Python package is publicly available via GitHub https://github.com/juaml/PrettYharmonize."
Efficient Biological Data Acquisitionthrough Inference Set Design,"In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so we might hope to reduce their cost by experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of an uncertainty-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that stops running the experiments when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that deploying active learning for inference set design leads to significant reduction in experimental cost while obtaining high system performance.","Automated high-throughput screening (HTS) laboratories have enabled scientists to screen large compound libraries to find effective therapeutic compounds and screen whole-genome CRISPR knockouts to understand the effects of genes on cell function (Mayr & Bojanic, 2009; Wildey et al., 2017; Blay et al., 2020; Tom et al., 2024; Fay et al., 2023). However, conducting experiments on every compound or CRISPR guide in these vast libraries remains very resource-intensive. With typical screening libraries holding on the order of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT to 106superscript10610^{6}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT compounds (Hughes et al., 2011) and the number of possible small molecules estimated at 1060superscript106010^{60}10 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT (Bohacek et al., 1996), the disparity between our screening capabilities and all which we could explore is staggering. Reducing experimental costs without compromising the quality of the generated data would allow us to accelerate biology and pharmaceutical research and expand the set of molecules considered for testing. To avoid costs scaling with the number of perturbations, we can train a model from a subset of the target library that has been tested in the lab, and then predict experimental outcomes for the remainder of the library using the trained model (Naik et al., 2013; Reker & Schneider, 2015; Dara et al., 2022), thereby building a hybrid screen of the library. This approach entails three interrelated questions: (1) which subset of the library should we use to maximise the accuracy of the predictions?, (2) how do we select this subset without access to the experimental outcomes?, and (3) how do we ensure that we select a large enough subset to meet a target level of accuracy for the predictions? This problem is similar to an active learning problem in that we want to select examples that maximize prediction accuracy, but instead of aiming to minimize generalization error, here we only care about prediction on a particular, finite, set of experiments from a library. The fact that the library is finite introduces an important difference: the learner can influence the set of examples on which it is evaluated by strategically selecting examples. If we accept that the difficulty of predicting outcomes varies among compounds, such that some examples are inherently harder to predict, either due to their complex properties (Bengio et al., 2009), because of the partial observability of the system (Saleh et al., 2021; Krenn et al., 2020) or due to noise in the labeling function (Frénay & Verleysen, 2013; Lukasik et al., 2020), the learner can select these examples into the training set to avoid having to predict their outcomes. Conversely, if an example can be reliably predicted by the model, we can save experimental costs by not including it in the training set. We call this mechanism through which a learner can influence performance inference set design. We propose an active learning-based solution to hybrid screening that uses the model’s confidence to guide the selection of experiments and leverages the mechanism of inference set design to improve the system’s performance on the target set. Our algorithm includes a practical stopping criterion that terminates the search for new molecules once a lower bound on a target accuracy threshold is exceeded. We show in Lemma 1 that this bound provides probabilistic guarantee on the performance of the algorithm as long as the model is weakly calibrated, such that examples for which the model is more uncertain receive a lower predicted probability than those for which it is more certain. To validate our method, we conduct series of empirical studies on image and molecular datasets, as well as a real-world case study in drug discovery. The results demonstrate inference set design significantly reduces experimental costs while improving overall system performance. Importantly, this is true even when the generalization benefits of active learning-based acquisition functions are marginal compared to random search. This has important practical implications for active learning: if a problem is a hybrid screen—in the sense that one only needs good performance on a fixed, finite set of experiments—then evaluating generalization error dramatically understates the benefits of active learning. By combining simple active learning acquisition functions with an appropriate stopping criterion, it is possible to make large scale screening far more efficient."
"Analyzing Neural Network Robustness Using Graph Curvature††thanks:This material is based upon work supported by the National Science Foundation (NSF) under Award No. 2403616, as part of the NSF Cyber-Physical Systems Program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.","This paper presents a new look at the neural network (NN) robustness problem, from the point of view of graph theory analysis, specifically graph curvature. Graph curvature (e.g., Ricci curvature) has been used to analyze system dynamics and identify bottlenecks in many domains, including road traffic analysis and internet routing. We define the notion of neural Ricci curvature and use it to identify bottleneck NN edges that are heavily used to “transport data” to the NN outputs. We provide an evaluation on MNIST that illustrates that such edges indeed occur more frequently for inputs where NNs are less robust. These results will serve as the basis for an alternative method of robust training, by minimizing the number of bottleneck edges.","Autonomous systems (AS) increasingly use neural networks (NNs) due to their ability to process high-dimensional data such as camera images [1], LiDAR scans [2] and textual prompts [3]. At the same time, NNs are known to suffer from robustness vulnerabilities: a slightly perturbed or out-of-distribution input [4, 5] may lead to very different and unexpected outputs. In turn, such vulnerabilities may severely compromise the safety and predictability of NN-based AS. Since the discovery of NN robustness issues [4], there has been an impressive amount of research on this topic. Researchers have developed a number of robust training methods, including adversarial training [6], certified robustness [7, 8], knowledge distillation [9], and semi-infinite constrained learning [10]. Although significant progress has been made, training robust NNs remains largely an unsolved and very challenging problem (e.g., the current leader on the CIFAR-10 robustness leaderboard [11] can only achieve high robust accuracy for perturbations of at most 8/255). We note that the vast majority of existing methods approach the problem from an optimization point of view: e.g., in adversarial training the goal is to train a NN that minimizes the loss not only on training data but also on the worst-case bounded perturbations of that data. This bilevel non-convex optimization problem is challenging to solve and leads to suboptimal solutions, especially if gradient descent is used. We take a fresh look at NN robustness through the lens of graph theory and network science analysis, in particular graph curvature (GC). GC (e.g., Ricci curvature [12]) has been effectively applied in numerous domains that can be modeled as graphs, including road traffic analysis [13, 14], internet routing [15], machine learning [16, 17], and biological networks [18, 16], due to its ability to capture intrinsic geometric and local structure of the space, such as connectivity and robustness in networks. GC can quantify the importance of specific edges; for example, an edge with negative curvature can be considered a bottleneck and is greatly important for the overall graph functionality, e.g., such an edge may connect different communities within the graph [19, 20]. In this paper, we employ GC in order to analyze the robustness of NN classifiers. We introduce the notion of neural Ricci curvature (NRC) that captures the bottleneck intuition of standard Ricci curvature – if an edge has a negative NRC, then it is heavily used by the NN and is thus likely a source of robustness vulnerability. To calculate the NRC, we construct a neural data graph, i.e., a graph in the shape of the NN architecture, where edges are weighted by a combination of the NN weights and the magnitude of data that goes through each edge when an example is provided as input. We evaluate the significance of the NRC using NNs trained on MNIST. We show that neural data graphs corresponding to more robust examples (i.e., examples which are correctly classified even for an adversarial perturbation) indeed have fewer negative-NRC edges. The results are consistent across architectures, including adversarially trained ones. This result will serve as the basis for an alternative, graph-based, method for robust training, that minimizes the number of negative-NRC edges and promotes balanced usage of all NN edges. In summary, this paper makes two contributions: 1) we define the concepts of neural data graphs and neural Ricci curvature that can be used to identify bottleneck NN edges that contribute to robustness issues; 2) we provide an evaluation on MNIST that demonstrates that bottleneck edges indeed occur more frequently in examples where NNs are less robust."
FLiP: Privacy-Preserving Federated Learning based on the Principle of Least Privileg,"Federated Learning (FL) allows users to share knowledge instead of raw data to train a model with high accuracy. Unfortunately, during the training, users lose control over the knowledge shared, which causes serious data privacy issues. We hold that users are only willing and need to share the essential knowledge to the training task to obtain the FL model with high accuracy. However, existing efforts cannot help users minimize the shared knowledge according to the user intention in the FL training procedure. This work proposes FLiP, which aims to bring the principle of least privilege (PoLP) to FL training. The key design of FLiP is applying elaborate information reduction on the training data through a local-global dataset distillation design. We measure the privacy performance through attribute inference and membership inference attacks. Extensive experiments show that FLiP strikes a good balance between model accuracy and privacy protection.","Federated learning (FL) [1] is a deep learning (DL) training paradigm, which aims to utilize the data existing in the form of isolated islands to train DL models (Fig. 2(a)). During the training procedure, data owners (a.k.a. clients) do not share their raw data with anyone, but instead, share some information obtained from the raw data in the form of model parameters. Many solutions are proposed to protect the privacy in FL context. However, they are yet unable to strike a balance between performance and privacy protection in real-world scenarios. This is because they do not fully consider the training task itself when performing protection. Specifically, considering that users are likely only interested in obtaining a high-quality model for the target task, sharing any information unrelated to the training task during the training process could potentially lead to privacy leakage, e.g., secondary attributes inference [2], practical attribute reconstruction Attack [3]. Given the subjective nature of privacy protection, we hold that an ideal solution should fully consider the user’s training goals and only share essential information related to the training task during the training. That’s the core idea, what we call as principle of least privilege (PoLP). According to the PoLP, as shown in Fig. 1, clients should control only the essential training task-relevant information from the raw data that can be shared among participants. At first glance, it is paradoxical to determine which part of the raw data plays a role in the model training procedure before the model is trained. After empirical study and analysis, we observe that each client can only extract a portion of local data that is most relevant to the FL task in the local training. Figure 1: Comparison of PoLP and existing privacy protection solutions. Ideally, the new sample generated by PoLP only contains task-relevant information. (a) The vanilla FL paradigm. (b) The FL paradigm of FLiP. Figure 2: The training procedures of vanilla FL and FLiP. In both paradigms, no training is needed on the central server. The biggest difference between our FLiP and the vanilla FL is the carrier of information aggregation, i.e., FLiP performs distilled data aggregation, and the vanilla FL performs parameter aggregation. Compared to the vanilla method, the amount of shared information during the training in FLiP is controllable. Our work proposes a new FL framework, FLiP, to achieve the PoLP in FL training for privacy protection. FLiP can help clients to identify which part of the raw data contributes most to the FL training and, at the same time, extract the task-relevant information locally. The central server collects the task-relevant information extracted by clients and distributes the global view to help clients make better information extraction. To measure privacy protection effectiveness, we consider adversaries in a semi-honest setting and perform two attacks to infer task-irrelevant information from the data extracted during the training. Experimental results show that FLiP can achieve comparable accuracy to the vanilla FL and better protection for information irrelevant to the training task. Our contribution is fourfold: • We are the first to introduce the PoLP to the FL training for privacy protection. Data owners can control the local information shared among FL participants and minimize privacy breaches by only sharing the essential FL task-relevant information. • We design a privacy-preserving FL system, FLiP, to realize the PoLP. The key design of FLiP is a local-global dataset distillation, which can identify and extract the task-relevant information gradually by round. • We design a task-irrelevant attribute inference attack to measure the protection effectiveness. The attribute inference attack is inspired by existing secondary inference attacks and fully considers the information leakage in each round. • We implement the system and perform an extensive evaluation. The experimental results show that FLiP can prevent adversaries from inferring task-irrelevant information and preserve high data usability."
Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series,"Lately, there has been a surge in interest surrounding generative modeling of time series data. Most existing approaches are designed either to process short sequences or to handle long-range sequences. This dichotomy can be attributed to gradient issues with recurrent networks, computational costs associated with transformers, and limited expressiveness of state space models. Towards a unified generative model for varying-length time series, we propose in this work to transform sequences into images. By employing invertible transforms such as the delay embedding and the short-time Fourier transform, we unlock three main advantages: i) We can exploit advanced diffusion vision models; ii) We can remarkably process short- and long-range inputs within the same framework; and iii) We can harness recent and established tools proposed in the time series to image literature. We validate the effectiveness of our method through a comprehensive evaluation across multiple tasks, including unconditional generation, interpolation, and extrapolation. We show that our approach achieves consistently state-of-the-art results against strong baselines. In the unconditional generation tasks, we show remarkable mean improvements of 58.17%percent58.1758.17\%58.17 % over previous diffusion models in the short discriminative score and 132.61%percent132.61132.61\%132.61 % in the (ultra-)long classification scores. Code is at https://github.com/azencot-group/ImagenTime.","Generative modeling of real-world information such as images [72], texts [13], and other types of data [99, 55, 8] has drawn increased attention recently. In this work, we focus on the setting of generative modeling (GM) of general time series information. There are several factors that govern the complexity required from sequential data generators including the sequence length, its number of features, the appearance of transient vs. long-range effects, and more. Existing generative models for time series are typically designed either for multivariate short-term sequences [44, 19] or univariate long-range data [103], often resulting in separate and completely different neural network frameworks. However, a natural question arises: Can one develop a unified framework equipped to handle both high-dimensional short sequences and low-dimensional long time series? Earlier approaches for processing time series based on recurrent neural networks (RNNs) handled short sequences well [62, 3, 43, 76], however, modeling long-range dependencies turned out to be significantly more challenging. Particularly, RNNs suffer from the well-known vanishing and exploding gradient problem [9, 70] that prevents them from learning complex patterns and long-range dependencies. To address long-context modeling and memory retention, extensive research is devoted to approaches such as long short-term memory (LSTM) models [42], unitary evolution RNNs [5] and Lipschitz RNNs [24]. A different approach for processing sequential information is based on the Transformer [93], eliminating any recurrent connections. Recent remarkable results have been obtained with transformers on natural language processing [13] and time series forecasting [96, 104, 68] tasks. Alas, transformers are underexplored as generative models for long-range time series data. This may be in part due to their computational costs that scale quadratically as 𝒪⁢(L2)𝒪superscript𝐿2\mathcal{O}(L^{2})caligraphic_O ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) with the sequence length L𝐿Litalic_L, and in part because transformer forecasters are inferior to linear tools [101]. Beyond RNNs and the Transformer, recent works have considered the state space model (SSM) for modeling long-range time series information. For instance, the structured SSM (S4) [36] employed a parameterization that reduced computational costs via evaluations of Cauchy kernels. Further, the deep linear recurrent unit (LRU) is inspired by the similarities between SSMs and RNNs, and it demonstrated impressive performance in modeling long-range dependencies (LRD). Still, generative modeling of long-range sequential data via state space models remains largely underexplored. Recent work suggested LS4 [103], a latent time series generative model that builds upon linear state space equations. LS4 utilizes autoregressive dependencies to expressively model time series (potentially non-stationary) distributions. However, this model struggles with short-length sequences as we show in our study, potentially due to limited expressivity of linear SSMs. To overcome gradient issues of recurrent backbones, temporal computational costs of transformers, and expressivity problems of SSMs, we represent time series information via small-sized images. Transforming raw sequences to other encodings has been useful for processing audio [34] as well as general time series data [95, 38, 56]. Moreover, a similar approach was employed to generative modeling of time series with generative adversarial networks (GANs) [12, 39]. However, unstable training dynamics and mode collapse negatively affect the performance of GAN-based tools [59]. In contrast, transforming time series to images is underexplored in the context of generative diffusion models. There are several fundamental advantages to our approach. First, there have been remarkable advancements in diffusion models for vision data that we can exploit [81, 40, 86, 45]. Second, using images instead of sequences elegantly avoids the challenges of long-term modeling. For instance, a moderately-sized 256×256256256256\times 256256 × 256 image corresponds to a time series of length up to 65⁢k65𝑘65k65 italic_k, as we show in Sec. 3. Finally, there is a growing body of literature dealing with time series as images on generative, classification, and forecasting tasks, whose results can be applied in our work and in future studies. In this work, we propose a new diffusion-based framework for generative modeling of general time series data, designed to seamlessly process both short-, long-, and ultra-long-range sequences. To evaluate our method, we consider standard benchmarks for short to ultra-long time series focusing on unconditional generation. Our approach supports efficient sampling, and it attains state-of-the-art results in comparison to recent generative models for sequential information. As far as we know, there are no existing tools handling both short and long sequence data. In addition to its strong unconditional generation capabilities, our approach is also tested in conditional scenarios involving the interpolation of missing information and extrapolation. Overall, we obtained state-of-the-art results in such cases with respect to existing tools. We further analyze and ablate our technique to motivate some of our design choices. The contributions of our work can be summarized as follows: 1. We view generative modeling of time series as a visual challenge, allowing to harness advances in time series to image transforms as well as vision diffusion models. 2. We develop a novel generative model for time series that scales from short to very long sequence lengths without significant modifications to the neural architecture or training method. 3. Our approach achieves state-of-the-art results in comparison to strong baselines in unconditional and conditional generative benchmarks for time series of lengths in the range [24,17.5⁢k]2417.5𝑘[24,17.5k][ 24 , 17.5 italic_k ]. Particularly, we attain the best scores on a new challenging benchmark of very long sequences that we introduce."
AgentForge: A Flexible Low-Code Platform for Reinforcement Learning Agent Design,"Developing a reinforcement learning (RL) agent often involves identifying effective values for a large number of parameters, covering the policy, reward function, environment, and the agent’s internal architecture, such as parameters controlling how the peripheral vision and memory modules work. Critically, since these parameters are interrelated in complex ways, optimizing them can be viewed as a black box optimization problem, which is especially challenging for non-experts. Although existing optimization-as-a-service platforms (e.g., Vizier, Optuna) can handle such problems, they are impractical for RL systems, as users must manually map each parameter to different components, making the process cumbersome and error-prone. They also require deep understanding of the optimization process, limiting their application outside ML experts and restricting access for fields like cognitive science, which models human decision-making. To tackle these challenges, we present AgentForge, a flexible low-code framework to optimize any parameter set across an RL system. AgentForge allows the user to perform individual or joint optimization of parameter sets. An optimization problem can be defined in a few lines of code and handed to any of the interfaced optimizers. We evaluated its performance in a challenging vision-based RL problem. AgentForge enables practitioners to develop RL agents without requiring extensive coding or deep expertise in optimization.","1 INTRODUCTION Developing reinforcement learning (RL) agents is important not only for advancements in machine learning (ML) but also for fields such as cognitive sciences, which is increasingly using RL to model cognitive mechanisms [Eppe et al., 2022]. One of the challenging aspects of this development is optimizing a broad and complex array of parameters that influence agent behavior and performance, a complex black box optimization problem for non-ML experts. Embodied RL agents, which interact with their environments through sensors and actuators, are a point in case. These agents are modeled as Partially Observable Markov Decision Processes (POMDPs) [Eppe et al., 2022, Deglurkar et al., 2023] and are used in real-world tasks such as autonomous driving [De Morais et al., 2020], robotic navigation [Shahria et al., 2022], and human-computer interaction [Jiang et al., 2024]. These models involve not only the parameters of the policy but several others controlling, among others, the reward function. In constrast, optimization of models in other ML fields is often more straightforward, as practitioners need to tune a smaller set of parameters mostly related to training. Sometimes parameter optimization is not just a matter of performance. Parameters that are explainable (e.g., reward weights) are a key to engineering explainable and trustworthy AI systems. Figure 1: Three examples of RL agent applications in Robotics [Ladosz et al., 2024], Human Modeling [Shi et al., 2024], and Collaborative AI [Lingler et al., 2024], highlighting the number of parameters involved in agent design today. Parameters are categorized as Agent, Environment, or Policy. The lists of parameters are non-exhaustive, and practitioners have to choose many more. Our long-term goal is to provide non-ML expect practitioners with the ability to optimize all parameters in a RL system, either jointly or individually, based on their specific problem. This capacity is important, because evidence suggests that even subtle adjustments to the implementation of RL algorithms, like reward clipping or observation normalization, can affect performance [Engstrom et al., 2020]. Furthermore, careful selection of parameters can sometimes improve performance more than the choice of RL algorithm itself [Andrychowicz et al., 2020, Paine et al., 2020, Yang and Yin, 2021]. In embodied RL agents in particular, the deep neural networks have many parameters that are sensitive to optimization [Fetterman et al., 2023]. Figure 1 presents some examples of agents’ applications and some of the parameters that a practitioner would need to choose during their development phase. More generally, these parameters can be roughly categorized in the following classes: 1. Agent design, which control the internal functions of the RL agent, including the discount factor, the entropy coefficient, and the size of the observation window; 2. Environment settings, which define the task itself, such as the field of view, and the reward structure; 3. Policy parameters, such as the architecture of the neural networks, learning rate, and activation functions. Thus, we present AgentForge222A link to the AgentForge platform can be found here: https://github.com/feferna/AgentForge, a flexible low-code platform targeted specifically for developing complex RL systems by domain experts with no solid background in optimization or ML. The focus in a low-code platform is becoming increasingly relevant as professionals in fields outside of ML, such as cognitive and behavioral sciences, are starting to use RL systems to model human cognition and behavior [Eppe et al., 2022, Nobandegani et al., 2022]. Beyond offering a low-code specification of the optimization problem, AgentForge provides rapid iteration by allowing developers to quickly ‘prototype’ an RL design, optimize it, and examine results. Its flexibility builds on its ability to accommodate a wide range of RL systems, from simple fully observable models to more complex, embodied RL agents. Users are only required to define their custom environment, objective function, and the parameters they wish to optimize in two files. The objective function, which guides the optimization process, can be the average reward per episode or any other criterion defined by the practitioner. The platform then automatically converts these inputs into an optimization problem that defines how the agent is trained and evaluated. In this first version, the software offers three popular optimization techniques: Random Search, Bayesian Optimization (BayesOpt) and Particle Swarm Optimization (PSO); however, this set can be extended. Obviously, the challenge of parameter optimization is shared across areas of ML, which has promoted the development of numerous services and frameworks. Notable examples include BoTorch [Balandat et al., 2020], Scikit-Optimize [scikit-optimize, 2024], Optuna [Akiba et al., 2019], and Google Vizier [Song et al., 2023]. However, these platforms primarily cater to users with substantial knowledge of ML and statistics. They are also complex to use with RL agents, because it is not easy to devise an objective function when there are many parameters distributed across different system components. This motivates low-code solutions, such the one we present here. Such tools can enable users to focus on designing effective agents without the burden of mastering optimization techniques, which is especially important given the high dimensionality of sensory inputs and the dynamic environments RL agents operate in. We evaluated AgentForge by jointly optimizing the parameters of a pixel-based version of the classical Lunar Lander agent from the Gymnasium library [Towers et al., 2024], a POMDP where the agent must infer the state from raw pixel values. This agent presents parameters from all three categories cited earlier—agent, environment, and policy—demonstrating AgentForge’s ability to handle diverse and complex parameter sets. Additionally, we show how easily an optimization problem can be specified, highlighting the platform’s easiness and flexibility. The remainder of this paper presents AgentForge in detail and is structured as follows: Section 2 reviews related works on traditional parameter tuning methods, their application in ML application, and limitations. Section 3 details our proposed platform, including what inputs are required from the user and how the optimization is performed. Section LABEL:sec:evaluation describes the evaluation setup, including the application of AgentForge to the proposed pixel-based Lunar Lander agent and its configuration. Section 4 presents the results achieved, while Section 5 discusses the implications for RL agents. Finally, Section 6 summarizes the main findings, discusses current limitations, and suggests directions for future research."
Marked Temporal Bayesian Flow Point Processes,"Marked event data captures events by recording their continuous-valued occurrence timestamps along with their corresponding discrete-valued types. They have appeared in various real-world scenarios such as social media, financial transactions, and healthcare records, and have been effectively modeled through Marked Temporal Point Process (MTPP) models. Recently, developing generative models for these MTPP models have seen rapid development due to their powerful generative capability and less restrictive functional forms. However, existing generative MTPP models are usually challenged in jointly modeling events’ timestamps and types since: (1) mainstream methods design the generative mechanisms for timestamps only and do not include event types; (2) the complex interdependence between the timestamps and event types are overlooked. In this paper, we propose a novel generative MTPP model called BMTPP. Unlike existing generative MTPP models, BMTPP flexibly models marked temporal joint distributions using a parameter-based approach. Additionally, by adding joint noise to the marked temporal data space, BMTPP effectively captures and explicitly reveals the interdependence between timestamps and event types. Extensive experiments validate the superiority of our approach over other state-of-the-art models and its ability to effectively capture marked-temporal interdependence.","Marked event data is widely seen in the real world as a sequence of events, where each event is recorded with a continuous-valued occurrence timestamp and a categorical event type (a.k.a. mark). Its detailed applications include social media [1, 2], where different event types may trigger diverse event patterns; financial transactions [3, 4], where a buy or sell action would result in different transaction times; and healthcare records [5, 6], where the disease types decide the visit times. That is, timestamps and event types exhibit non-trivial interdependence in these scenarios, as they can influence each other in both directions, specified by the detailed situation. Marked Temporal Point Process (MTPP) is a stochastic process that can effectively model marked event sequences. Mainstream MTPP methods can be broadly divided into two categories: classical MTPP models, including Poisson processes [7, 8], Hawkes processes [9] and Self-correcting processes [10], which use an intensity function to characterize the instantaneous occurrence of events given their history. However, these methods often rely on strong parametric or modeling assumptions, limiting their ability to effectively capture complex patterns in event occurrences. On the other hand, neural MTPP models have emerged as a rapidly developing branch in recent years [11, 12, 13]. These models employ neural networks, such as RNNs, LSTMs, or transformers, to encode historical events [14]. In some cases, they draw on intensity functions inspired by processes like the Hawkes process to model event occurrences [15, 16]. Compared to classical MTPP models, neural MTPP models leverage the expressive power of neural networks to better capture the complex dependencies among events. However, applying these two methods to parametric MTPP models requires solving integrals to compute the likelihood function, which usually requires extremely high computational cost [11, 17]. To tackle this issue, two common techniques, namely strong model assumptions and numerical approximation techniques, are typically employed. First, certain model assumptions are introduced, such as treating the timestamp x𝑥xitalic_x and event type m𝑚mitalic_m as independent or conditionally dependent (e.g., x𝑥xitalic_x depends on m𝑚mitalic_m, or m𝑚mitalic_m depends on x𝑥xitalic_x) [18, 19, 15, 20]. This approach simplifies the integral form, thereby reducing computational complexity. However, in reality, x𝑥xitalic_x and m𝑚mitalic_m may not be independent or may have more complex dependence, which can lead to model misspecification and consequently restrict the model’s expressive capacity [21]. Second, numerical approximation techniques, such as Monte Carlo sampling or numerical integration, are used to simplify the computation of integrals when closed-form solutions are not available [22, 23]. Despite this, limitations in sample size and sampling errors mean that these methods only approximate the true solution, which can result in information loss and affect model performance [17]. To fill these gaps, generative MTPP models, which model the target distribution of timestamps using the generative models, have been proposed and have shown promising results [24, 25]. Mainstream generative models, such as diffusion models [26, 27], generative adversarial networks (GANs) [28, 29], and variational autoencoders (VAEs) [30, 31], are energy-based deep probabilistic models, where the optimization objective is an energy function corresponding to an unnormalized negative log-likelihood function. Typically, neural networks are employed to represent this energy function. Since neural networks can approximate complex functions without formal constraints, there is no need to impose model assumptions or use numerical approximation techniques to simplify computation. Consequently, using such generative models in MTPP tasks allows for flexible modeling, enhancing the model’s expressiveness and avoiding information loss due to approximation operations. However, generative MTPP models still face two main challenges. Challenge I: In MTPP tasks, we aim to model the joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ) of two heterogeneous random variables: x𝑥xitalic_x, which is continuous, and m𝑚mitalic_m, which is discrete. However, mainstream generative models, such as diffusion models, are designed for continuous random variables due to their reliance on Gaussian noise [32]. As a result, these models are not directly applicable for modeling joint distributions that include discrete random variables m𝑚mitalic_m [24]. Challenge II: The target joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ) involves two random variables, x𝑥xitalic_x and m𝑚mitalic_m, which exhibit a strong interdependence across different scenarios. For example, discussions about clothing types change with the seasons [14]. Thus, capturing the complex interdependence between timestamps x𝑥xitalic_x and different event types m𝑚mitalic_m is crucial for improving model performance. However, existing generative MTPP models are unable to directly model the joint distribution, often leading to the assumption that x𝑥xitalic_x and m𝑚mitalic_m are independent, and applying the generative model only to x𝑥xitalic_x [24]. This approach neglects the interdependence between the two random variables, ultimately compromising model performance. To tackle the above challenges, we propose a novel generative MTPP model called the Marked Temporal Bayesian Flow Point Process (BMTPP), based on the recently developed generative model, the Bayesian Flow Network (BFN) [33], to approximate the joint distribution of timestamps and event types. First, in contrast to existing generative MTPP models that model the continuous random variable x𝑥xitalic_x only, BMTPP flexibly leverages BFN in a parameter-based manner to model both the timestamps and event types. Second, by adding the joint noise to the marked temporal data, BMTPP can effectively capture the coupling correlation between timestamps and event types in MTPP tasks, explicitly revealing the complex interactions between them. We summarize the contributions as follows: • Based on the Bayesian flow network, we propose a new generative MTPP model, i.e., BMTPP, which can naturally and flexibly model marked event sequences. • BMTPP can directly model the joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ), which consists of continuous timestamps x𝑥xitalic_x and discrete event types m𝑚mitalic_m. Moreover, by employing a joint noise strategy within the marked temporal data space, it can effectively capture and explicitly reveal the interdependence between timestamps and event types. • We evaluate BMTPP on four real-world datasets, demonstrating our proposed approach outperforms other state-of-the-art models, as well as the effectiveness of our joint noise strategy in capturing marked-temporal interdependence."
DMT-HI: MOE-based Hyperbolic Interpretable Deep Manifold Transformation for Unspervised Dimensionality Reduction,"Dimensionality reduction (DR) plays a crucial role in various fields, including data engineering and visualization, by simplifying complex datasets while retaining essential information. However, the challenge of balancing DR accuracy and interpretability remains crucial, particularly for users dealing with high-dimensional data. Traditional DR methods often face a trade-off between precision and transparency, where optimizing for performance can lead to reduced interpretability, and vice versa. This limitation is especially prominent in real-world applications such as image, tabular, and text data analysis, where both accuracy and interpretability are critical. To address these challenges, this work introduces the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI). The proposed approach combines hyperbolic embeddings, which effectively capture complex hierarchical structures, with Mixture of Experts (MOE) models, which dynamically allocate tasks based on input features. DMT-HI enhances DR accuracy by leveraging hyperbolic embeddings to represent the hierarchical nature of data, while also improving interpretability by explicitly linking input data, embedding outcomes, and key features through the MOE structure. Extensive experiments demonstrate that DMT-HI consistently achieves superior performance in both DR accuracy and model interpretability, making it a robust solution for complex data analysis. The code is available at https://github.com/zangzelin/code_dmthi.","Dimensionality reduction [1, 2, 3, 4] simplifies complex datasets while preserving their intrinsic structure [5, 6]. This is crucial for managing high-dimensional data, which presents challenges in computational complexity, storage, and visualisation [7, 8]. Reducing data dimensionality allows for more efficient analysis, pattern recognition, and interpretation [9]. However, balancing high performance [10] and interpretability [11, 12] remains challenging, as efficient processing [13] often conflicts with human interpretability [14, 15]. Figure 1: Overview of the proposed DMT-HI network. The figure compares three dimensionality reduction methods, manifold-based, deep learning-based, and the proposed DMT-HI. DMT-HI leverages a Mixture of Experts (MOE) strategy to efficiently process both image [16] and tabular [17] data, offering better performance, lower training costs, and improved interpretability across different data sizes. Dimensionality reduction methods fall into two categories, manifold-based parameter-free approaches [18, 19, 20] and deep learning-based methods [21]. Manifold-based methods like t-SNE [22] and UMAP [23] are known for their speed (on small dataset) and adaptability [7], projecting high-dimensional data into low-dimensional spaces through nonlinear mappings, revealing underlying structures. Deep learning methods, such as parametric UMAP (PUMAP) [24, 25] and DMT-EV [26], handle complex data more effectively, especially high-dimensional data, leveraging neural networks to capture intricate patterns. DMT-EV, in particular, excels in both performance and explainability [27], pruning irrelevant features for clearer, more interpretable results. Deep learning methods stand out for their ability to scale and generalize well across large datasets, positioning them as central to current dimensionality reduction research. In terms of method efficiency, performance, and interpretability, manifold-based parameter-free methods and deep learning-based methods each have distinct strengths and weaknesses, driven by their theoretical and design differences [7]. Parameter-free methods are more efficient for small datasets with lower time costs since they don’t rely on parametric models and focus on optimizing the output [11]. However, their efficiency declines with increasing data size due to the rising complexity of neighborhood search and distance calculations. In contrast, deep learning methods handle large-scale data more efficiently due to model scalability and hardware acceleration, though their training on small datasets is costlier. In terms of performance, parameter-free methods excel at capturing local structures but struggle with complex global hierarchies due to their reliance on Euclidean space [28]. Deep learning methods, by contrast, can capture both local and global structures through multilayer transformations but require significant data and computational resources. Regarding interpretability, parameter-free methods rely on similarity metrics, making them harder to interpret and inconsistent globally. While deep learning methods can capture richer features, their “black-box” nature and complexity make their decision-making process harder to explain. To address these challenges in global structure characterization and interpretability, As shown in Fig. 1, we propose the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI), which integrates hyperbolic mapping and a Mixture of Experts model (MOE) [29, 30]. Hyperbolic mapping uses negative curvature to better capture complex hierarchical structures and global dependencies, preserving global information in lower dimensions. The MOE strategy enhances performance and efficiency by dynamically assigning tasks to specialized expert networks that handle different input features, thereby avoiding bottlenecks from a single model. This model provides interpretability by allowing users to track expert decisions and understand the internal model workings. Additionally, MOE serves as a bridge between raw data, embedded data, and feature subsets, enabling clear interpretation of how features influence data representations at different levels. By combining hyperbolic mapping’s structural advantages and MOE’s efficient task allocation, DMT-HI aims to improve performance, efficiency, and interpretability, offering a comprehensive solution for reducing the dimensionality of complex data. By integrating these innovations, our approach not only improves dimensionality reduction performance but also enhances interpretability, offering a comprehensive solution for handling complex data types and extracting insights from high-dimensional datasets. DMT-HI’s performance in dimensionality reduction is enhanced through the MOE strategy, which dynamically assigns tasks to the most suitable experts, improving both processing efficiency and overall model performance. Additionally, the redesigned manifold loss optimizes the training process, enabling the model to capture both local and global structures more effectively. Overall, the key contributions of this paper include, • A hyperbolic embedding and deep manifold loss function, which improve the accuracy of dimensionality reduction by better capturing the global structure of data. • The introduction of the MOE strategy, establishing a clear connection between input data, embedding results, and key features, thus enhancing model interpretability and stability. • Comprehensive tests evaluating global and local performance, time efficiency, and other dimensions to validate the advantages of the proposed model."
A neural network approach for solving the Monge-Ampère equation with transport boundary condition,"This paper introduces a novel neural network-based approach to solving the Monge-Ampère equation with the transport boundary condition, specifically targeted towards optical design applications. We leverage multilayer perceptron networks to learn approximate solutions by minimizing a loss function that encompasses the equation’s residual, boundary conditions, and convexity constraints. Our main results demonstrate the efficacy of this method, optimized using L-BFGS, through a series of test cases encompassing symmetric and asymmetric circle-to-circle, square-to-circle, and circle-to-flower reflector mapping problems. Comparative analysis with a conventional least-squares finite-difference solver reveals the competitive, and often superior, performance of our neural network approach on the test cases examined here. A comprehensive hyperparameter study further illuminates the impact of factors such as sampling density, network architecture, and optimization algorithm. While promising, further investigation is needed to verify the method’s robustness for more complicated problems and to ensure consistent convergence. Nonetheless, the simplicity and adaptability of this neural network-based approach position it as a compelling alternative to specialized partial differential equation solvers.","The Monge-Ampère equation is a nonlinear partial differential equation (PDE) with crucial applications across various fields in physics and mathematics. Its general form is given by: det(D2⁢u)=f⁢(x,u,∇u),superscript𝐷2𝑢𝑓𝑥𝑢∇𝑢\det\left(D^{2}u\right)=f(x,u,\nabla u),roman_det ( italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u ) = italic_f ( italic_x , italic_u , ∇ italic_u ) , (1) where u:ℝN→ℝ:𝑢→superscriptℝ𝑁ℝu:\mathbb{R}^{N}\to\mathbb{R}italic_u : blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT → blackboard_R, (N≥1𝑁1N\geq 1italic_N ≥ 1), is an unknown convex function, and D2⁢usuperscript𝐷2𝑢D^{2}uitalic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u represents the Hessian matrix of u𝑢uitalic_u. This equation traces its origins back to the 18th-century work of Gaspard Monge, who studied the problem of optimal resource allocation. Over time, this foundational problem has evolved into what is now known as the optimal transport problem, a concept that naturally emerges in fields such as fluid dynamics and mathematical physics. The Monge-Ampère equation effectively describes the optimal transportation of one mass distribution to another, minimizing a cost function that typically represents the distance over which each mass element must be moved 1. Several optical problems can be formulated as instances of optimal transport. A notable example is the design of a reflector that transforms a given light source distribution into a desired target distribution, a problem that inherently aligns with the principles of optimal transport. In this context, the ’mass’ represents energy, while the cost function corresponds to the optical path lengths of the light rays 2. Numerous variations of this problem can be effectively modeled using the Monge-Ampère equation or its generalized forms. For example, the light source may be planar, emitting a parallel beam, or point-based, radiating in multiple directions. Additionally, optical systems can target near-field or far-field regions and may involve point or parallel targets. Both reflectors and lenses can be described within this framework. For example, we might wish to transform a point source to a far-field target using a freeform reflector; a numerical method for solving this problem using the intersection of confocal paraboloids has been described by De Castro et al. 3. For an in-depth exploration of these variations, see Romijn 4 and Anthonissen et al. 5. To simplify our analysis, we will focus on a specific optical configuration: the parallel-to-far-field reflector system. In this setup, a planar light source emits a parallel beam of light toward a reflector, and our primary concern is the distribution of the reflected light at a significant distance from the reflector. Consequently, we only need to consider the direction of each reflected ray. By applying the Monge-Ampère equation with the transport boundary condition and solving it, we can determine the convex reflector surface that transforms a given source light distribution into the desired target distribution. It is important to note that, mathematically, this problem is identical to both the parallel-to-parallel reflector problem and the parallel-to-far-field lens problem, which can also be addressed using the method presented here. This paper introduces a novel numerical method based on artificial neural networks (ANNs) to solve the Monge-Ampère equation with transport boundary condition. Numerous studies have explored the application of neural networks and automatic differentiation to solve ordinary and partial differential equations. Dissanayake and Phan-Thien 6 pioneered this approach, demonstrating the potential of neural networks for approximating solutions to PDEs. Building on this work, Lagaris et al. 7 presented a method using ANNs to solve initial and boundary value problems by constructing trial solutions that inherently satisfy the given conditions. Aarts and Van Der Veer 8 proposed a method to solve PDEs and their boundary/initial conditions using neural networks, incorporating knowledge about the PDE in the structure and training process of the network. More recently, Michoski et al. 9 presented a comprehensive study on the solving of differential equations using deep neural networks, demonstrating their competitiveness with conventional numerical methods and their potential for parameter space exploration and model enrichment. Building on this rich body of work, Nyström and Vestberg 10 employed ANNs to solve the Dirichlet problem for the Monge-Ampère equation. We extend this approach by incorporating the transport boundary condition and compare our neural network-based method against an existing numerical solver for the Monge-Ampère equation with transport boundary condition11. Furthermore, we examine the effect of various hyperparameters of this neural network method on its performance. Section 2 provides a concise background on the Monge-Ampère equation in the context of the parallel-to-far-field reflector problem, a previously proposed finite-differences-based solver for this problem, and artificial neural networks. In Section 3, we present our extension of this method to incorporate the alternative boundary conditions required for optimal transport problems. To demonstrate the effectiveness of our proposed method, Section 4 presents results for several example problems. As is common in machine learning, numerous hyperparameters influence the accuracy of our method. Thus, Section 5 empirically examines the effects of select hyperparameters on our method’s performance. In Section 6 and Section 7, we conclude by discussing the advantages and limitations of neural network-based methods for solving the Monge-Ampère equation with the transport boundary condition, and explore potential avenues for future research to mitigate these limitations."
,,"1 INTRODUCTION Generative models have achieved impressive performance in scientific applications among many other fields (Noé et al.,, 2019; Butter and Plehn,, 2022; Cranmer et al.,, 2020; Sanchez-Lengeling and Aspuru-Guzik,, 2018). Oftentimes, such systems can depend on external control parameters, such as temperature governing the behavior of thermodynamic systems, coupling constants in physical models, or a tempered likelihood posterior in Bayesian inference (Wuttke et al.,, 2014; Friel and Pettitt,, 2008; Pawlowski et al.,, 2017). A major challenge in such cases is acquiring training data for a complete range of control parameters, which can quickly become infeasible. Figure 1: Our approach to train a conditional normalizing flow pθ⁢(x|c)subscript𝑝𝜃conditional𝑥𝑐p_{\theta}(x|c)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_c ). Left: At c=c0𝑐subscript𝑐0c=c_{0}italic_c = italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, the flow is trained using NLL. Right: By learning the gradient of the distribution with respect to c𝑐citalic_c based on prior knowledge, the distribution learned at c0subscript𝑐0c_{0}italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is propagated to other conditions c≠c0𝑐subscript𝑐0c\neq c_{0}italic_c ≠ italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT without additional training data. In this work, we focus on the task of learning generative models in the case where we have access to the functional form of the unnormalized density, such as learning the distributions of equilibrium states in physical systems like molecules (Boltzmann distributions) or variational inference. We approach this problem by learning a single generative model that takes the external condition c𝑐citalic_c as a parameter: pθ⁢(x|c)≈p⁢(x|c)subscript𝑝𝜃conditional𝑥𝑐𝑝conditional𝑥𝑐p_{\theta}(x|c)\approx p(x|c)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_c ) ≈ italic_p ( italic_x | italic_c ). Several works have attempted to address this problem before. One approach applies architectural restrictions to allow one particular functional form of external parameter dependence (Dibak et al.,, 2022). However, this restriction has recently been shown to incur severe limitations in expressivity (Draxler et al., 2024b, ). Energy-based training has been proposed as another method (Schebek et al.,, 2024; Invernizzi et al.,, 2022; Wirnsberger et al.,, 2020), but can exhibit unfavorable properties like mode-seeking behavior (Minka et al.,, 2005; Felardos et al.,, 2023), which has also been shown to be a problem in practice (Wirnsberger et al.,, 2020). Other works require data to be available at the target parameters (Wang et al., 2022b, ; Wirnsberger et al.,, 2020), which can become prohibitively expensive. We overcome these limitations: We allow learning arbitrary continuous dependencies of the target density on external parameters and train unconstrained architectures. Our central idea is to formulate the training of a conditional probability density as a boundary value problem: The boundary is learned on a fixed reference condition c=c0𝑐subscript𝑐0c=c_{0}italic_c = italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and then the resulting density is extrapolated using the known functional dependence on the condition underlying the problem. Our approach is summarized in Fig. 1. In summary, we contribute: • We introduce TRADE, a method for learning generative models with arbitrary continuous dependencies on external conditions. Learning is enabled by a novel boundary value problem formulation. • TRADE uses unconstrained architectures, facilitating the application to more complex target distributions. • TRADE can be trained data-free or with data only available at the boundary c0subscript𝑐0c_{0}italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, making it an efficient approach in cases where acquiring data for a full range of control parameters is infeasible. • TRADE achieves excellent results in a wide range of experiments including Bayesian inference, molecular simulations and physical lattice models"
Measuring memorization through probabilistic discoverable extraction,redacted \correspondingauthorjamhay@google.com,"Memorization of training data, while potentially beneficial for retaining factual information, presents significant challenges in large language models (LLMs) (Biderman et al., 2024; Duan et al., 2024b; Zhang et al., 2024; Huang et al., 2024; More et al., 2024; Smith et al., 2023; Carlini et al., 2022; Bordt et al., 2024; Duan et al., 2024a; Staab et al., 2023; Shi et al., 2023; Tang et al., 2023; Zanella-Béguelin et al., 2020) 111This paper covers a very restricted definition of “memorization”: whether a generative model can be induced to generate near-facsimiles of some training examples when prompted with appropriate instructions. Models do not “contain” bit-wise or code-wise copies of their training data. Rather, if a model can be induced to generate very close copies of certain training examples by supplying appropriate instructions to guide the model’s statistical generation processes then that model is said to have “memorized” those examples. This is an area of active ongoing research.. The undesirable consequences of memorized data can inadvertently expose sensitive information contained within the training set. This issue has garnered significant attention, leading to the now-common practice of quantifying and reporting training data memorization rates within technical reports introducing new foundation large language models (LLMs) (Reid et al., 2024; Anil et al., 2023; Chowdhery et al., 2023; Team et al., 2024; Kudugunta et al., 2024). One way to measure memorization is to quantify how easily a potential attacker could extract training data by querying the model. This often-used measure of memorization called discoverable extraction (Carlini et al., 2022; Kassem et al., 2024) essentially states that a training example is extractable (or “memorized”) if when split into a prefix and suffix, the model generates a sequence matching the suffix when given the prefix as input. Discoverable extraction is often used as an (approximate) upper-bound to an adversary that has no prior knowledge of the example to be extracted (Nasr et al., 2023). Discoverable extraction has become a popular way of measuring memorization rates of LLMs (Reid et al., 2024; Anil et al., 2023; Chowdhery et al., 2023; Team et al., 2024; Kudugunta et al., 2024), in no small part due to its simplicity; one needs only to check if a model’s completion matches an expected target string. While discoverable extraction is cheap to compute, this has its own drawbacks; by only generating a single sequence and checking for a match with the target, it may miss cases where a match could have been found if more than one sequence was generated. These nuances of what is and is not memorized are too coarsely treated by a working definition like discoverable extraction, and leads us to investigate the following question: Question 1. How can we better measure the extractability of sensitive sequences from large language models? Quantifying memorization – and the associated risks – is a subtle and context-specific problem that a single measurement likely cannot capture in isolation. Prior work has attempted to introduce more complex definitions of memorization that aim to get to the heart of what it means for a model to memorize a training example, but they are often too costly to be practically leveraged. These methods have varying levels of computational cost to empirically estimate memorization in the context of LLMs, but are all more expensive than discoverable extraction (see Section 4). Large language models are probabilistic machines, the output of the model is a probability distribution over tokens that make up a model’s vocabulary. The sequence that is generated is entirely dependent on the choice of sampling algorithm that defines how a token is selected from this distribution. There are many different sampling algorithms that one can choose from (see Fan et al. (2018); Holtzman et al. (2019); Basu et al. (2020); Vijayakumar et al. (2016); Graves (2012); Boulanger-Lewandowski et al. (2013)). Prior works have focused on greedy sampling, which iteratively selects the next token with the largest probability conditioned on the previous tokens. Although greedy sampling selects the most likely next token it may not select the overall most likely sequence; yet, the difference between discoverable extraction rates under greedy and other sampling schemes like beam search was found to be marginal (Carlini et al., 2022). However, users of production large language models are often free to decide which sampling scheme to use. These observations lead to our second research question: Question 2. How does the user-chosen sampling scheme affect extractability rates? Carlini et al. (2022) argue that focusing on sampling schemes that have higher degrees of associated randomness compared to greedy sampling, are “antithetical” to maximizing discoverable extraction, as sampling schemes that encourage diversity in sequence generation will by definition have a higher variation in the sequences that can be sampled. However, higher sequence diversity may be advantageous for extraction given that users may query the model multiple times. Extracting the secret even once out of multiple queries could be highly problematic as the adversary (say a hacker checking credit card numbers) may have external means of verifying which one is correct. It is reasonable to try and quantify the number of sequences that need to be generated before a target example becomes extractable, as this better aligns with how users could interact with a model. In particular, because production language models are deployed with non-greedy based sampling strategies that, by default, encourage diversity in sampled sequences. This idea is not a new one. Carlini et al. (2019) motivate their measure of canary memorization using rank perplexity by considering an adversary who sequentially guesses the potential canaries in order from lowest to highest perplexity. The rank of the true canary measures how many guesses such an adversary would need to make before guessing correctly. While the secret sharer attack only guesses a single canary, it is natural to consider how extraction rates scale with multiple guesses. We find that even when we measure the extraction probability after multiple guesses, the extraction rates on training data remain significantly higher than extraction rates on test data (see Section 5.2). This observation allows us to reason about the absolute risk of extraction as well as the relative risk. In this work, we introduce a probabilistic relaxation of discoverable extraction that resolves the discussed points of tension. This new definition quantifies the number of attempts n𝑛nitalic_n an adversary would need to make to extract a target with a certain probability p𝑝pitalic_p under a given sampling scheme. This provides a more nuanced quantification of memorization, which alleviates the aforementioned drawbacks of discoverable extraction without incurring any additional computational cost compared to discoverable extraction. Our contributions We propose a simple probabilistic definition of extraction called (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction that captures the risk of extracting a target after sampling n𝑛nitalic_n times from an arbitrary sampling scheme. We thoroughly benchmark (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction rates for different sampling schemes, settings of n𝑛nitalic_n and p𝑝pitalic_p, model sizes, and number of target data repetitions, and we make a number of remarkable empirical findings that demonstrate the utility of our definition: • Greedy extraction underestimates training data memorization rates compared to (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction even for modest values of n𝑛nitalic_n and p𝑝pitalic_p (Section 5.1). Moreover, the discrepancy between the two rates increases for larger models and more repetitions of the target data. (Section 5.3) • At every setting of the definition parameters we tried, extraction rates of training data far exceeded baseline extraction rates on test data (Section 5.2). • We show that (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction provides a better comparison of memorization rates across models trained on the same data compared with greedy extraction (Section 5.1). Along the way, we provide extensive discussion how our definition relates to other definitions of memorization (Section 3 and Section 4)."
Improving Inverse Folding for Peptide Design with Diversity-regularized Direct Preference Optimization,"Inverse folding models play an important role in structure-based design by predicting amino acid sequences that fold into desired reference structures. Models like ProteinMPNN, a message-passing encoder-decoder model, are trained to reliably produce new sequences from a reference structure. However, when applied to peptides, these models are prone to generating repetitive sequences that do not fold into the reference structure. To address this, we fine-tune ProteinMPNN to produce diverse and structurally consistent peptide sequences via Direct Preference Optimization (DPO). We derive two enhancements to DPO: online diversity regularization and domain-specific priors. Additionally, we develop a new understanding on improving diversity in decoder models. When conditioned on OpenFold generated structures, our fine-tuned models achieve state-of-the-art structural similarity scores, improving base ProteinMPNN by at least 8%. Compared to standard DPO, our regularized method achieves up to 20% higher sequence diversity with no loss in structural similarity score.","Engineering biopolymers that fold into desired 3D structures, a computational challenge known as inverse protein folding problem, has broad applications in drug discovery and material science (Yang et al., 2023; Dill et al., 2008; Abascal & Regan, 2018). Several approaches for inverse folding have been adopted over the past decades, from molecular dynamics simulations to machine learning approaches (Dauparas et al., 2022b; Shanker et al., 2023; Hsu et al., 2022a; Yi et al., 2023; Correa, 1990). In the standard machine learning approach, a molecular backbone chain serves as input, and a model generates sequences that adopt folding topologies compatible with the reference backbone. Sequences do not necessarily share sequence homology, as multiple diverse sequences can fold into similar structures (Hsu et al., 2022a; Yue & Dill, 1992; Godzik et al., 1993). Peptides, which are small biopolymers comprising 2-50 residues, are interesting targets for inverse folding given their role in diverse biological functions, acting as hormones, neurotransmitters, signalling molecules, or nanostructures assemblers (Chockalingam et al., 2007; Torres et al., 2019; Copolovici et al., 2014; Ulijn & Smith, 2008). Only about 225,000 protein structures have been experimentally determined111Updated figures available at https://www.rcsb.org/stats/growth/growth-released-structures. and made available via the Protein Data Bank (PDB). Training inverse-folding machine learning models in a supervised fashion is a challenging task, due to the complexity of the problem and the limited amount of experimental data. The challenge is aggravated in the peptide domain as fewer than 3.53.53.53.5% PDB structures contain 50 residues or less. In fact, applying the SCOP classification filter in the PDB to display structures labelled as “Peptide” reveals only 509 entries, circa 0.20.20.20.2% of all experimentally determined structures available. In addition to the lack of data, sequences are subject to composition bias. The incidence of certain amino acids may differ depending on the sequence length , as longer proteins have more options for accommodating multiple secondary structures and folding loops (Tiessen et al., 2012). Popular models like ProteinMPNN (Dauparas et al., 2022b), PiFold (Gao et al., 2022) and ESM-IF1 (Hsu et al., 2022b) are primarily trained on data derived from longer proteins, leading to poor performance for peptide design tasks. Additionally, shorter sequences fold into simpler structures. Indeed, Milner-White & Russell (2008) argue that short peptides are notoriously “structureless” and tend to flicker between conformations. For example, a structural conformation of a single alpha helix or beta sheet – or a combination of two or three of them – is not necessarily stable and can fluctuate. Existing inverse folding models optimize sequence residue-recovery accuracy and structural similarity via template modeling (TM) score (Zhang & Skolnick, 2004). However, they often suffer from low sampling diversity (Gao et al., 2023b). Ideally, the inverse folding model generates maximally diverse candidate sequences, as additional design filters, such as synthesizability and thermal stability, reduce the number of potential hits downstream of the design process. To address these issues, we apply Direct Preference Optimization (DPO) (Rafailov et al., 2023), a fine-tuning method, to improve inverse-folding methods for peptide design. To the authors’ knowledge, we are the first to apply DPO to this task. We propose several enhancements to DPO to address specific problems that arise in inverse folding. Particularly, we forward fold generated sequences and derive an online regularized algorithm for optimizing structural similarity to the reference and sequence diversity simultaneously. We show empirically that this algorithm targets the differential entropy in log-probability space. Furthermore, we present a simple reward scaling approach to incorporate scalar reward information, showing that reward scaling adaptively selects a KL divergence penalty (Kullback, S. and Leibler, R. A., 1951) to improve performance on harder structures."
LOCAL: Learning with Orientation Matrix to Infer Causal Structure from Time Series Data,"Discovering the underlying Directed Acyclic Graph (DAG) from time series observational data is highly challenging due to the dynamic nature and complex nonlinear interactions between variables. Existing methods often struggle with inefficiency and the handling of high-dimensional data. To address these research gap, we propose LOCAL, a highly efficient, easy-to-implement, and constraint-free method for recovering dynamic causal structures. LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic DAG equivalent to the ground truth. On this basis, we propose two adaptive modules for enhancing the algebraic characterization of acyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and Dynamic Graph Parameter Learning (DGPL). ACML generates causal masks using learnable priority vectors and the Gumbel-Sigmoid function, ensuring the creation of DAGs while optimizing computational efficiency. DGPL transforms causal learning into decomposed matrix products, capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Extensive experiments on synthetic and real-world datasets demonstrate that LOCAL significantly outperforms existing methods, and highlight LOCAL’s potential as a robust and efficient method for dynamic causal discovery. Our code will be available soon.","Exploration of the underlying causal generation process of dynamic systems is an important task (Cheng et al., 2024a; Gong et al., 2024) for trustworthy machine learning. Unfortunately, it is unethical, impossible due to technical reasons, or expensive to conduct intervention experiments on the dynamic systems of certain domains (Li et al., 2023a; Cai et al., 2023a). Another challenge is to infer about the structure which may be high dimensional and nonlinear. Some recent works (Pamfil et al., 2020; Sun et al., 2023; Gao et al., 2022; Fan et al., 2023) have made significant efforts by employing dynamic Bayesian networks (DBNs) with observational and interventional data: among dynamic systems, as illustrated in Figure 1. The variable xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at timestep t𝑡titalic_t is affected by which variables xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT at the same time step (instantaneous dependency) and which variables xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT at the previous timestep (lagged dependency)? This question highlights the crucial roles of those algorithms in the interpretable performance of the trained models. Figure 1. Illustration of instantaneous dependency (solid lines) and lagged dependency (dashed lines) dependencies in a DBN with d=3𝑑3d=3italic_d = 3 nodes and autoregression order p=2𝑝2p=2italic_p = 2. For clarity, we display edges that do not influence the variables at time t𝑡titalic_t in a lighter shade. In order to study the nonparametric DBN, DYNOTEARS (Pamfil et al., 2020) (i.e., a score-based approach to learning DBNs with a differentiable DAG constraint (Zheng et al., 2018)) was proposed as a proxy to capture the learn the parents of child variables. However, in Section 4.2, our practical analysis shows that the DYNOTEARS algorithm and its extensions (Sun et al., 2023; Gao et al., 2022; Fan et al., 2023) adopting matrix exponential constraints require an extremely long time to optimize in high-dimensional dynamic systems, even if they smartly adopt interventional data to enhance the identifiability (Li et al., 2023b, 2024). Then, it is natural to need a proxy model that can fasterly infer dynamic causal structures in high-dimensional situations, which is also the main goal of our work. Recently, much of the research on dynamic causal structure learning has concentrated on applying soft sparsity and DAG constraints. For instance, Golem (Ng et al., 2020) formulated a likelihood-based score function for handling the causality of thousands of nodes. Yu et al. (Yu et al., 2023) extended it for recovery dynamic causal structure. Concurrently, (Fang et al., 2024) verified the feasibility of further accelerating the learning of DAG based on this likelihood function both theoretically and experimentally. These approaches are aimed at enhancing flexibility and scalability in high-dimensional and circumventing rigid structural constraints. Building on these insights, we propose a novel framework for Learning with Orientation matrix to infer CAusaL structure from time series data, which we call LOCAL. We develop a quasi-maximum likelihood-based dynamic structure learning method with identifiability guarantee. Powered by this quasi-maximum likelihood-based objective, we propose to enhance the algebraic characterization of acyclicity with two adaptive modules for causal structure recovering task: 1) an Asymptotic Causal Mask Learning (ACML) module which leverages learnable priority vectors (𝒑𝒑\boldsymbol{p}bold_italic_p) and the Gumbel-Sigmoid function to generate causal masks, ensuring the creation of directed acyclic graphs (DAGs) while optimizing computational efficiency; 2) a Dynamic Graph Parameter Learning (DGPL) module to transform causal learning into decomposed matrix products (𝑾=𝑬s⁢𝑬tT𝑾subscript𝑬𝑠subscriptsuperscript𝑬𝑇𝑡\boldsymbol{W}=\boldsymbol{E}_{s}\boldsymbol{E}^{T}_{t}bold_italic_W = bold_italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_italic_E start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT), effectively capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Those leads us can exploit faster gradient-based optimization, such as Adam (Kingma and Ba, 2015), and GPU acceleration. Contribution. The main contributions of this paper are as follows: • To the best of our knowledge, LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic causal structure and shows that more robust and accurate. • We proposed two adaptive modules, ACML and DGPL, which further liberate the matrix exponential operations required for causality. • We conducted extensive experiments on several synthetic datasets and real-world benchmarks, which proved that LOCAL outperforms state-of-the-art by a significant margin."
Accelerating AI Performance usingAnderson Extrapolation on GPUs,"We present a novel approach for accelerating AI performance by leveraging Anderson extrapolation, a vector-to-vector mapping technique based on a window of historical iterations. By identifying the crossover point where a mixing penalty is incurred, the method focuses on reducing iterations to convergence, with fewer more compute-intensive but generally cacheable iterations, balancing speed and memory usage with accuracy and algorithmic stability, respectively. We demonstrate significant improvements, in both training and inference, motivated by scalability and efficiency extensions to the realm of high-performance computing (HPC).","Anderson extrapolation [1, 2, 27, 33, 16] has recently been applied to deep equilibrium models (DEQs) [7, 8, 9, 10, 24, 17]. Kolter et al. [34] found the gains not substantial due to early termination with a loose convergence tolerance. They focused on Anderson extrapolation during training. Here, we show significant acceleration of AI performance with Anderson on GPUs for both the forward pass (running inferences faster) and training (generating models faster). We demonstrate acceleration of the forward pass with standard Anderson as a baseline for future work with stochastic variants [30] and accelerating the backward pass with Jacobian-free methods like Jacobian-Free Backpropagation (JFB) and Neumann series gradient approximations [16]. Figure 2: AI carbon footprint projected to consume >2% of global electricity demand [3, 15, 28, 25], amounting to >10% of global electricity demand for data centers and infrastructure. As AI demand grows, as shown in Fig. 2 [3, 15, 28, 25], high-performance computing (HPC) is becoming critical due to economic pressures from the growth of data and AI infrastructure [29]. Low-memory acceleration techniques, like Anderson extrapolation, will be key to increasing HPC-based AI computational efficiency. This study investigates matrix-free Anderson extrapolation on GPUs, emphasizing gains from advanced computing architectures compared to CPUs. Our goal is to maximize computational efficiency while reducing iterations to convergence by reusing previous iterations to avoid unnecessary gradient calculations, gaining benefits expected from second-order methods (e.g., [32]) without manipulating Hessian matrices. The environmental impact of AI is rapidly growing [3, 15, 28, 25]. By 2030, AI is projected to account for 2% of global electricity consumption. We aim to reduce this impact by up to 90%, saving 160 terawatt-hours per year by 2030. The carbon footprint of AI exceeds the 500-megaton annual benchmark set by initiatives like Bill Gates’ Breakthrough Energy [14]. Efficiency-enhancing technologies like GPU and Anderson acceleration can reduce AI’s carbon emissions by 60 gigatons per year by 2030, as shown in Fig. 2. 1.1 Leveraging extrapolation for AI and HPC advances Anderson extrapolation, a windowing technique for accelerating nonlinear fixed point iterations, is widely applied in fields like density functional theory, kinetic theory, and climate spin-up. It is well-suited for distributed memory parallelization and GPU implementation. It is a staple of major open-source large-scale solver libraries, including PETSc [11, 12], SUNDIALS [23], Trilinos [19, 22, 21, 20], and deal.II [13, 4, 5, 6]. It can be applied to machine learning training, smoothing out standard forward iterations and achieving superior accuracy in training and testing error. Benchmarking results on CIFAR10 show expected robustness benefits and allow characterization of the temporal advantages or disadvantages from the higher cost per iteration, where a small residual minimization step is applied at each new function evaluation. Figure 3: Mathematical formulation and vector representation. Adapted from Y. He & H. De Sterck. ""Linear Asymptotic Convergence Analysis of Anderson Acceleration, with Krylov Formulation in the Linear Case"" Copper Mountain Conference (2022), ICERM Workshop (2023). Available at: https://www.bilibili.com/video/BV1Wa411i77y/ and https://icerm.brown.edu/video_archive/?play=3320 Figure 4: Deep equilibrium neural network model architecture (Source: NeurIPS Tutorial, 2020 [34]). f⁢(z,x)=norm⁢(ReLU⁢(z+norm⁢(x+W2∗(norm⁢(ReLU⁢(W1∗z))))))𝑓𝑧𝑥normReLU𝑧norm𝑥subscript𝑊2normReLUsubscript𝑊1𝑧f(z,x)=\mathrm{norm}(\mathrm{ReLU}(z+\mathrm{norm}(x+W_{2}*(\mathrm{norm}(% \mathrm{ReLU}(W_{1}*z))))))italic_f ( italic_z , italic_x ) = roman_norm ( roman_ReLU ( italic_z + roman_norm ( italic_x + italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∗ ( roman_norm ( roman_ReLU ( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∗ italic_z ) ) ) ) ) ) ""norm"" here is a group norm, representing a statistical normalization [31]. 1.2 Balancing memory and stability Fundamental tradeoffs exist between memory capacity, memory bandwidth, communication cost, and algorithmic characteristics of stability and convergence rate. The tradeoffs are generally resolved to minimize time to solution. GPUs attain high memory bandwidth advantages over CPUs at the cost of smaller memory capacity. Anderson extrapolation promotes fewer, more expensive steps, reusing cached state-vector data. In distributed memory implementations, it produces convergence with fewer interprocessor communication steps. It has tuning parameters such as window size and damping that can be tuned to application and architecture. We are assessing its utility in machine learning more broadly at a time of emergent CPU-GPU superchips. 1.3 Deep equilibrium neural network models Deep equilibrium models (DEQs) are the continuum limit of explicit neural networks as the number of layers approaches infinity [26], approximating many explicit layers with a single, implicit layer with exponentially fewer parameters using a backward pass including the output. This reduces the inverse problem in parameter space to a fixed-point iteration problem, enabling the usage of nonlinear, vector-to-vector mapping techniques to compute the fixed-point iterations that converge to the deep equilibrium state parameters by minimizing the loss function. With gains in memory and acceleration, DEQs are fit for large-scale computer vision and natural language processing tasks and benefit more from matrix-vector operation-optimized computing architectures like GPUs and CPU-GPU superchips. The standard approach using forward iteration for fixed-point iteration problems often does not efficiently converge to the fixed point and suffers from initially slow error reduction and local minimum trapping in nonlinear problems like deep neural networks. Vector-to-vector mapping techniques like Anderson extrapolation outperform standard forward iteration by combining information from previous iterations to span a searchable subspace to extrapolate the next iteration, enhancing convergence rates at the expense of memory usage in each iteration. DEQs represent any neural network at arbitrary depths and connectivities with a single implicit layer consuming vastly fewer parameters with faster forward passes for accelerated training and inferences. The implicit function theorem shows how gradients can be computed in the DEQ framework, facilitating backpropagation through the equilibrium state [8, 34]. DEQs provide a framework for accelerating deep learning, extending the capacity of deep networks within a single-layer architecture through fixed-point computations and advanced root-finding algorithms. Their amenability to convergence acceleration with techniques like Anderson positions DEQs as a robust method to reduce computation needed to build state-of-the-art models and scale up beyond current computational limitations."
Computational Bottlenecks of TrainingSmall-scale Large Language Models,"While large language models (LLMs) dominate the AI landscape, Small-scale large Language Models (SLMs) are gaining attention due to cost and efficiency demands from consumers. However, there is limited research on the training behavior and computational requirements of SLMs. In this study, we explore the computational bottlenecks of training SLMs (up to 2B parameters) by examining the effects of various hyperparameters and configurations, including GPU type, batch size, model size, communication protocol, attention type, and the number of GPUs. We assess these factors on popular cloud services using metrics such as loss per dollar and tokens per second 111We use average dollar cost ratios of cloud instance types based on publicly available pricing (Appx. A).. Our findings aim to support the broader adoption and optimization of language model training for low-resource AI research institutes.","Large Language Models (LLMs) are becoming increasingly popular in various fields due to their performance on a variety of tasks [6, 18, 8, 20, 5]. However, deploying large models widely such as on mobile hardware and edge devices is challenging due to the large memory and compute requirements [11, 12, 10]. These constraints have driven a growing interest in smaller language models (such as ≤2⁢Babsent2𝐵\leq 2B≤ 2 italic_B parameters) as a viable alternative [24, 16, 23]. Recent work refer to these models as Small-scale large Language Models (SLMs) which can work well in environments where cost-efficiency and resource limitations are of significant concern, as well as on servers where the reduced cost of inference will be a dominant factor to attract and retain customers. SLMs have demonstrated substantial potential in achieving competitive results despite their smaller size. Techniques such as pruning, distillation, and quantization have been employed to enhance their performance [2, 3, 17], allowing SLMs to perform on par with, and in some cases surpass, much larger models [4]. For example, Gemma-2B outperforms the largest OPT-175B [25], challenging the notion that sheer model size is the primary determinant of effectiveness. In addition to on par accuracy, SLMs can meet consumer demands for fast, efficient, and cost-effective AI without sacrificing task performance, making them increasingly attractive for organizations with limited computational budgets, such as small businesses and academic institutions. While prior work mostly focused on optimizing SLMs for inference [15], relatively little attention has been paid to their training dynamics. This gap is significant, as the computational and infrastructure demands of training LLMs may not translate to SLMs. Given the diverse range of hardware configurations available on cloud platforms—such as GPU type, batch size, and communication protocols—there is a need for a systematic analysis of how these factors impact the training efficiency of SLMs, particularly when measured in terms of practical metrics such as loss per dollar and tokens per second. Our findings indicate that for smaller models, more affordable options like A100-40GB GPUs and Distributed Data Parallel (DDP) can be utilized without sacrificing performance. For larger models, advanced configurations, such as A100-80GB and H100-80GB GPUs paired with Flash Attention (FA) and Fully Sharded Data Parallel (FSDP), are necessary to handle larger batch sizes and prevent memory-related issues. Recent advancements in the field underscore the importance of scaling AI systems not only for state-of-the-art performance but also for practical applications in real-world environments. The emerging trend toward SLMs suggests that a re-evaluation of hardware and computation strategies is essential. The contribution of this paper is to address the need for such evaluation, providing a systematic study on the computational bottlenecks and cost-efficiency of training SLMs up to 2B parameters on various cloud infrastructure and setups. We find that 1) FlashAttention is significantly more important for SLMs than LLMs, 2) Expensive hardware, e.g., H100-80GB and A100-80GB, is not necessarily cost effective for SLM training, 3) DDP is the best distributed training scheme for SLMs, and 4) Maximizing GPU memory utilization is not cost-optimal for SLM training."
Gradient Descent Efficiency Index,"Gradient descent is a widely used iterative algorithm for finding local minima in multivariate functions. However, the final iterations often either overshoot the minima or make minimal progress, making it challenging to determine an optimal stopping point. This study introduces a new efficiency metric, Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, designed to quantify the effectiveness of each iteration. The proposed metric accounts for both the relative change in error and the stability of the loss function across iterations. This measure is particularly valuable in resource-constrained environments, where costs are closely tied to training time. Experimental validation across multiple datasets and models demonstrates that Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT provides valuable insights into the convergence behavior of gradient descent, complementing traditional performance metrics. The index has the potential to guide more informed decisions in the selection and tuning of optimization algorithms in machine learning applications and be used to compare the “effectiveness” of models relative to each other.","In the field of machine learning, optimizing the training process of models is crucial for achieving high performance while minimizing computational resources. Gradient descent [1] is a widely used optimization algorithm due to its simplicity and effectiveness in finding local minima of differentiable functions. However, the efficiency of gradient descent can diminish with large datasets and prolonged training periods, where additional iterations provide negligible improvements. This raises the need for a robust mechanism to identify the optimal stopping point, ensuring efficient use of computational resources. Fig. 1 is a contour plot that illustrates how each step taken in gradient descent towards a local minimum is smaller than the previous one. This approach quickly returns diminishing results, making the last few steps cost more computationally than they yield in accuracy. Figure 1: Contour plot of cost b,w with path of gradient descent J⁢(w,b)𝐽𝑤𝑏J(w,b)italic_J ( italic_w , italic_b ). In Fig. 2, notice how the first 100 steps exponentially decrease (or logarithmically increase) the cost. However, if you zoom out to 100,000 steps, the curve effectively flattens out before 10,000 steps in this particular example. Figure 2: Comparison of cost with different domain restrictions. The “Gradient Descent Efficiency Index” is a novel ratio between training parameters that includes the relative change in gradient norm, the initial learning rate, the learning decay rate, absolute change in coefficients, and the number of iterations. Note that throughout this paper, I will use the name “Gradient Descent Efficiency Index”, the short form “GDEI”, and the function Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT interchangeably."
Generative Diffusion Models for Sequential Recommendations,Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).,"Recommender systems are algorithms that suggest items to users by analyzing various forms of input data. Their primary goal is to enhance the customer experience through personalized recommendations, often based on prior implicit feedback. These systems track user behaviors, such as purchase history, viewing habits, and browsing activity, to model user preferences. Sequential Recommendation, a specific type of recommendation, is particularly relevant for applications where user behavior is naturally sequential. It focuses on predicting the next item a user will interact with by considering the order of previous interactions. Mainstream solutions to Sequential Recommendation (SR) [2] represent items with fixed vectors, which have a limited ability to capture the latent aspects of items and the diversity of user preferences. Generative models like Generative Adversarial Networks (GANs) [3] and Variational Auto-Encoders (VAEs) [4] have been widely applied in personalized recommendations, using adversarial training and encoder-decoder architectures, respectively, to model user behavior and preferences. However, Diffusion Models have shown significant advantages over GANs and VAEs, such as greater stability and higher generation quality in various tasks. Diffusion Models (DMs) [5, 6, 7] have achieved state-of-the-art results in image synthesis tasks [7, 8, 9, 10, 11]. These models alleviate the trade-off between stability and quality by gradually corrupting images in a forward process and iteratively learning to reconstruct them. DMs progressively corrupt 𝐱0subscript𝐱0\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with random noise and then recover 𝐱0subscript𝐱0\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT from the corrupted state 𝐱Tsubscript𝐱𝑇\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT step by step. This forward process creates a tractable posterior [6], enabling the iterative modeling of complex distributions through flexible neural networks in the reverse generation process. The objectives of recommender models align well with DMs, as recommendation essentially involves inferring future interaction probabilities from noisy historical interactions, where noise represents false-positive and false-negative items [12, 13]. This makes DMs a promising approach for accurately modeling complex interaction patterns with strong representational ability. Despite their success in other domains, applying diffusion models to recommender systems remains underexplored. We further explore diffusion models for sequential recommendation (SR) by extending the method introduced by Li et al. (2023). Our work proposes significant enhancements to the existing architecture, resulting in a new model, DiffuRecSys111https://youtu.be/bEpDfAAGL2I. Specifically, our contributions are as follows: • Enhancing the Diffusion Recommender Model: We incorporate cross-attention mechanisms within the Approximator of the model architecture. The model isn’t just learning temporal dependencies (the sequential order of items) but also more complex relationships between past interactions and the target item by focusing on the most relevant past events. • Incorporation of Offset Noise: We introduce offset noise into the diffusion process to increase model robustness and effectively handle variability in user interactions. • Comprehensive Experimental Validation: We conduct extensive experiments across three datasets under various settings, demonstrating improvements of DiffuRec with our extensions over standard baselines."
,,"Deep generative models – for example VAEs, GANs, normalizing flows, diffusion models, and flow matching – have recently made great progress in probability density learning (DL), with flow matching achieving highest accuracy at the moment. Besides generative accuracy, human interpretability of the learned representation is highly desirable, and disentangled representation learning (DRL) is a key tool for this, see Bengio et al., (2014) and Wang et al., (2024). Intuitively, DRL means that each latent variable should effect only a single, distinct semantic property of the generated data instances. We consider the problem of measuring if and to what degree a given model has actually learned a disentangled representation. Most prior work addresses this question in a supervised setting, where the true generative factors are known (see Related Work). Since this assumption is often violated in the real world, we instead focus on the unsupervised case. That is, we do not ask if the model has learned the (unknown) true factors, but only if it has learned any disentangled representation at all. The learned representation might be close to the true one, if certain identifiability conditions are fulfilled (see Related Work), but this is beyond the scope of our paper. Our work rests on the manifold hypothesis which states that data in a D𝐷Ditalic_D-dimensional space often reside near a manifold ℳℳ\mathcal{M}caligraphic_M of much lower dimension d≪Dmuch-less-than𝑑𝐷d\ll Ditalic_d ≪ italic_D. Variations along the manifold correspond to semantically important differences between data instances, whereas off-manifold variations are considered as unimportant or noise. This is familiar from PCA, where one interprets directions of high data variability as important, whereas directions of low variability are irrelevant. PCA achieves this under the assumption that the manifold ℳℳ\mathcal{M}caligraphic_M is a linear subspace, and DRL seeks to generalize this to non-linear models. If the important dimensions indeed span the manifold ℳℳ\mathcal{M}caligraphic_M, the representation is called aligned. Our method clearly highlights Alignment by assigning a high manifold entropy to the important features and low to unimportant ones, see fig. 1. Moreover, it allows sorting of latent variables by importance so that the cut-off between important and irrelevant can be adjusted later according to the needs of an application, analogous to PCA’s ordering by variance. Figure 1: The two moons distribution illustrates how manifold entropic metrics quantify DRL in terms of alignment and disentanglement. (Top left) The latent prior and a Cartesian grid spanned by the latent variables Zcsubscript𝑍𝑐Z_{c}italic_Z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and Zdsubscript𝑍𝑑Z_{d}italic_Z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue). The latent distribution is mapped to data space by three generative models with equal accuracy, but vastly different representations. This can be seen by the differences in the transformed grid spanned by the manifold random variables 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and 𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue) in the top row, and the corresponding values of our metrics manifold entropy H⁢(𝑿c)𝐻subscript𝑿𝑐H({\boldsymbol{X}}_{c})italic_H ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ), H⁢(𝑿d)𝐻subscript𝑿𝑑H({\boldsymbol{X}}_{d})italic_H ( bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) and manifold mutual information ℐ⁢(𝑿c,𝑿d)ℐsubscript𝑿𝑐subscript𝑿𝑑\mathcal{I}({\boldsymbol{X}}_{c},{\boldsymbol{X}}_{d})caligraphic_I ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) in the bottom row. The total entropy of the distribution (gray) is the signed sum of the three terms. (A) The latent manifolds are entangled (and thus not interpretable), and our metric indicates this by high mutual information (brown). (B) The latent manifolds are locally orthogonal everywhere and have low mutual information. However, alignment is inconsistent (𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT aligns with the upper moon, 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT with the lower), resulting in comparable manifold entropy of both variables. (C) The representation is disentangled and aligned. The manifold entropy is high for the important variable 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and low for the noise variable 𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue), and their mutual information is small. Disentanglement, i.e. the statistical independence between latent factors, has been addressed by Independent Component Analysis (ICA, Comon,, 1994) under the assumption of a linear data-generating process (DGP), 𝒙=𝑨⁢𝒔𝒙𝑨𝒔{\boldsymbol{x}}=\boldsymbol{A}\boldsymbol{s}bold_italic_x = bold_italic_A bold_italic_s. When linearity holds, the true generative factors 𝒔𝒔\boldsymbol{s}bold_italic_s are identifiable if they are independent and follow a non-Gaussian distribution. However, identifiability is generally lost for non-linear DGPs 𝒙=Φ⁢(𝒔)𝒙Φ𝒔{\boldsymbol{x}}=\Phi(\boldsymbol{s})bold_italic_x = roman_Φ ( bold_italic_s ), (Hyvärinen and Pajunen,, 1999). Finding conditions on 𝒔𝒔\boldsymbol{s}bold_italic_s to restore identifiability is a major focus of current research, see (Hyvarinen et al.,, 2023) for a recent survey. Alternatively, one can restrict the class of permitted mixing functions ΦΦ\Phiroman_Φ, and this approach primarily inspired the present work. Independent Mechanism Analysis (IMA, Gresele et al.,, 2022) postulates, for example, that the Jacobian of ΦΦ\Phiroman_Φ should be orthogonal in every point, and Principal Component Flows (Cunningham et al., 2022b, ) realize this idea by adding a loss term to normalizing flow training that encourages orthogonality. It can be shown that orthogonality of the Jacobian is equivalent to minimizing the mutual information between the image of the corresponding features in data space after mapping them through the decoder (see fig. 1 and Appendix). This is crucial: In contrast to supervised disentanglement metrics, which are usually defined with respect to the encoder, meaningful unsupervised metrics must be defined in terms of the decoder mapping from latent to data space. Specifically, we make the following contributions: • We propose a set of information-theoretic metrics for DRL defined on the decoder of a generative model. • We introduce Alignment as an important complementary condition to Disentanglement in the IMA framework • We show the usefulness of our metrics at dissecting generative models in order to reveal and quantitatively measure their behaviours."
Robust Time Series Causal Discovery for Agent-Based Model Validation,"Agent-Based Model (ABM) validation is crucial as it helps ensuring the reliability of simulations, and causal discovery has become a powerful tool in this context. However, current causal discovery methods often face accuracy and robustness challenges when applied to complex and noisy time series data, which is typical in ABM scenarios. This study addresses these issues by proposing a Robust Cross-Validation (RCV) approach to enhance causal structure learning for ABM validation. We develop RCV-VarLiNGAM and RCV-PCMCI, novel extensions of two prominent causal discovery algorithms. These aim to reduce the impact of noise better and give more reliable causal relation results, even with high-dimensional, time-dependent data. The proposed approach is then integrated into an enhanced ABM validation framework, which is designed to handle diverse data and model structures. The approach is evaluated using synthetic datasets and a complex simulated fMRI dataset. The results demonstrate greater reliability in causal structure identification. The study examines how various characteristics of datasets affect the performance of established causal discovery methods. These characteristics include linearity, noise distribution, stationarity, and causal structure density. This analysis is then extended to the RCV method to see how it compares in these different situations. This examination helps confirm whether the results are consistent with existing literature and also reveals the strengths and weaknesses of the novel approaches. By tackling key methodological challenges, the study aims to enhance ABM validation with a more resilient valuation framework presented. These improvements increase the reliability of model-driven decision making processes in complex systems analysis.","Chapter 1 Introduction 1.1 Agent-Based Models: An Overview In the modern era, Agent-Based Models (ABMs) fall under the class of modelling and simulation techniques which are increasingly being used in domains such as theoretical economics, finance, social sciences, and epidemiology. It involves a bottom-up approach by putting together individual agents and their interactions so that various phenomena may be synthesised and then examined. The detailed methodologies of such models allow them to understand complexity and draw futuristic conclusions. Recent literature stresses the significant advantages of agent-based models (ABMs) over traditional economic models. Fagiolo (2019) pinpoints the major strengths of ABMs: their capacity to provide comprehensive narratives of interactions among agents with network structures, incomplete information learning processes, and competition in imperfect markets—and the flexibility they provide in validating both model inputs and outputs [1]. This characteristic has gained much attention and prompted much research activity recently. 1.2 The Importance of ABM Validation To successfully implement ABMs in reality, the “validation” of this model could be the decisive factor for its ability to truly reflect real-world reality. The validation process consists of comparing the model output with the actual data obtained in the real world to make sure it is reliable and effective. This process is utilised to establish the credibility of the model and the reliability of the predictions made. Validation is particularly challenging in fields with complex interactions and non-linear dynamics, such as finance. As Windrum et al. (2007) pointed out, significant effort is still needed to realise consistent and satisfactory techniques of ABM method implementation to real-world financial data [2]. A key component in ABM validation is to find the cause-and-effect mechanisms from data. Besides highlighting the importance of correlational testing, causal matching between the ABM outputs and real-world data has recently been emphasized in validation. These approaches aim to understand and explain the origins and propagation of observed phenomena in financial systems [3]. The details of such causal discovery methods and their application in ABM validation will be further discussed in the following chapters. 1.3 Challenges to Address Indeed, in spite of continuous progress in the ABM validation techniques, there are still the most significant challenges in such complex systems applications: 1. Insufficient Robustness of Time Series Causal Discovery Methods: The causal discovery approaches that are commonly used today, such as VAR-LiNGAM and PCMCI, can be quite susceptible to noise and variations in the data. This could create inconsistencies when the method is applied to different subsets of the same dataset or datasets with slightly different characteristics. For ABM validation purposes, the robustness of these techniques should be enhanced. False causality in this regard could be as harmful as wrong conclusions drawn up about the ABM system’s validity and, consequently, about the underlying system mechanisms. 2. Lack of Comprehensive Understanding of Dataset Characteristics’ Impact: While previous studies have examined specific dataset characteristics, there is a lack of comprehensive understanding of how various dataset properties collectively affect the performance of causal discovery methods in ABM validation. The absence of a systematic comparison across a wide range of dataset types (e.g., linear vs. non-linear, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary, sparse vs. dense causal structures) hinders our ability to select appropriate validation techniques and interpret results accurately. This deficiency stands as a huge constraint in the area of creating functional ABM validation processes as a solution to the wide spectrum of complicated problems. 3. Limitations in Existing ABM Validation Frameworks: Even though much progress has been made in ABM Validation, for instance, the framework proposed by Guerini et al. (2017) [3], current approaches still face several key limitations: (a) Insufficient Dataset Property Analysis: Existing frameworks often lack comprehensive tests for some important dataset properties. For instance, some of these may overlook important characteristics such as linearity and stationarity, which are essential for understanding the nature of the data and selecting appropriate modelling techniques. (b) Limited Options for Causal Discovery Methods: Most current frameworks rely on a single or limited set of causal discovery methods. This limitation may prevent us from obtaining optimal performance when dealing with different dataset characteristics or different priorities (such as accuracy vs. efficiency). The lack of method diversity limits the framework’s adaptability to various scenarios and data types. (c) Narrow Range of Performance Metrics: The existing validation framework typically focuses on basic similarity tests or a limited set of performance metrics. This may cause to failure to capture the full range of model performance, especially in complex financial systems where causal relationships can be intricate. Addressing these challenges is crucial for advancing ABM validation in complex systems like financial markets. Overcoming these limitations will lead to more reliable models, enhancing decision-making processes and informing policy decisions. These challenges necessitate innovative approaches in causal discovery and model validation techniques applicable to a wide range of complex systems. 1.4 Novel Approaches and Contributions This research addresses the challenges mentioned above by utilizing several new approaches and providing the following key contributions: 1. Robust Cross-Validated (RCV) Causal Discovery Method (Chapter 3): We introduce a novel approach to enhance the robustness of existing causal discovery methods. By applying cross-validation techniques to causal discovery algorithms such as VAR-LiNGAM and PCMCI, we aim to mitigate the sensitivity of these methods to noise and data variations to improve the consistency and reliability of causal structure identification in complex time series data. 2. Comprehensive Experimental Evaluation and Analysis (Chapter 4): We present a thorough empirical analysis of our proposed methods and existing approaches. This evaluation covers a wide range of characteristics, including linear vs. non-linear relationships, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary behaviour, and sparse vs. dense causal structures. We also examine the scalability of methods with varying numbers of variables and time series lengths. We ran experimental evaluations on both synthetic datasets with controlled properties and a complex simulated fMRI dataset, so as to provide insights into method performance under various conditions. 3. Context-Aware ABM Validation Framework (Chapter 5): Extending the work done by Guerini, we develop an enhanced ABM validation framework that addresses the weaknesses of existing one. In this framework, the user could choose a suitable method of causal inference based on the property of data sets or other validation needs, such as efficiency or accuracy. Another improvement we feature is to include a more comprehensive set of performance metrics for assessing the causal relations to get a more precise evaluation of model performance. This framework is designed to pre-process datasets and ensure their uniformity, analyse dataset attributes, run user-dependent or driven causal structure detection, and enhance validation evaluations. These contributions enhance ABM validation in complex systems by improving causal discovery methods and offering a flexible validation framework. This research aims to increase the accuracy and reliability of ABMs in capturing real-world dynamics across various domains. 1.5 Report Structure The remainder of this report is structured as follows: Chapter 2 provides the foundational background and reviews related work, covering Agent-Based Models in finance, existing ABM validation techniques, causal discovery methods and the current limitations. Chapter 3 introduces our novel Robust Cross-Validated (RCV) Causal Discovery Approach, detailing its theoretical foundations and implementation. Chapter 4 presents our experimental evaluation and analysis, including synthetic dataset generation, comparative analysis of causal discovery methods, and an application to a complex simulated fMRI dataset that mimics real-world neuroimaging data. Chapter 5 describes our Context-Aware ABM Validation Framework, explaining how it integrates improved causal discovery methods and enhances overall validation reliability. Finally, Chapter 6 concludes the report, summarizing key findings, discussing implications for ABM validation in complex systems and suggesting future research directions."
An Auditing Test to Detect Behavioral Shift in Language Models,"As language models (LMs) approach human-level performance, a comprehensive understanding of their behavior becomes crucial. This includes evaluating capabilities, biases, task performance, and alignment with societal values. Extensive initial evaluations, including red teaming and diverse benchmarking, can establish a model’s behavioral profile. However, subsequent fine-tuning or deployment modifications may alter these behaviors in unintended ways. We present a method for continual Behavioral Shift Auditing (BSA) in LMs. Our test compares model generations from a baseline model to those of the model under scrutiny and provides theoretical guarantees for change detection while controlling false positives. The test features a configurable tolerance parameter that adjusts sensitivity to behavioral changes for different use cases. We evaluate our approach using two case studies: monitoring changes in (a) toxicity and (b) translation performance. We find that the test is able to detect meaningful changes in behavior distributions using just hundreds of examples.","Language models (LMs) can now achieve human-level performance in a wide range of tasks, including text summarization, machine translation, coding and even acting as AI scientists: generating hypotheses and designing experiments (Achiam et al., 2023; Katz et al., 2024; Lu et al., 2024; Zhang et al., 2024). Because of this, many sectors are looking for ways to use them to improve existing systems (Kasneci et al., 2023; Felten et al., 2023). Unfortunately, one large roadblock to broad LM adoption is their propensity to generate harmful content (Weidinger et al., 2021). For example, GPT-3 has significant anti-Muslim biases (Abid et al., 2021), and GPT-4 has racial and gender biases (Zack et al., 2024). To address this, a significant effort is going into ensuring LM behavior is aligned with our societal values, spawning the field of AI alignment (Ji et al., 2023). A large portion of this effort is on developing ways to evaluate LM behavior, for example, through benchmarks (Wang et al., 2023a) and red-teaming (Perez et al., 2022). Given these evaluation techniques, how should they be used to ensure LMs stay safe? To answer this, consider the following two hypothetical settings where this question might be asked: (1) Internal Audit: At the new start-up Chasm Intellect, the LM alignment team is looking for a way to trigger an alarm when LM behavior changes. They know that LM model behavior can change unexpectedly (Li et al., 2024b). For example, fine-tuning a model that has undergone safety evaluation (e.g., GPT-3.5 Turbo (OpenAI, 2023)) can cause it to become less safe, even when using a benign fine-tuning dataset (e.g., Alpaca (Taori et al., 2023); (Qi et al., 2023)). How can the team detect meaningful changes in model behavior? (2) External Audit: A journalist has been playing around with Better geneRAtiVe language modEl (BRAVE), Chasm Intellect’s new model, and has noticed it produces highly toxic generations when asked about seemingly benign topics. Their article on this topic spurred a governmental audit of BRAVE. The government performs an initial evaluation but is worried that model behavior will change when no longer under scrutiny. How can the government regularly check the deployed model’s behavior is the same as the previously certified one? We call the general class of problems detecting changes in LM behaviors Behavioral Shift Auditing (BSA) problems. In this paper, we formalize the problem of Behavioral Shift Auditing in Language Models. We detail a test that continuously monitors behavioral shift, solely from model generations (e.g. via API calls). Under some weak assumptions, the test provably guarantees that if model generations have different behavior than those of an initial model, the test will detect it, given enough generations. At the same time, if there has not been a change, the test is guaranteed to have tight, non-asymptotic control over the false positive rate. The key insight behind our approach is that one can phrase the problem of Behavioral Shift Auditing as hypothesis testing over the relevant behavioral distribution. This framing allows our test to be applicable to any measurable aspect of model behavior, including also capabilities (e.g., dangerous capabilities (Phuong et al., 2024) or mathematical reasoning capabilities (Mishra et al., 2022a)) and biases (e.g., gender bias (Wang et al., 2023a; Kotek et al., 2023)). Using this insight, we develop a test that extends recent work on anytime-valid hypothesis testing (Pandeva et al., 2024), a state-of-the-art sequential testing method that has been successfully applied in various auditing settings (Chugg et al., 2023; Shekhar et al., 2023; Waudby-Smith et al., 2021). Our test checks for changes in model behavior distributions, comparing generations from a reference model with those of another, potentially changed model. The test has a tunable parameter that allows one to vary the strictness of the test. This allows for detecting any change in behavior, which may be more suitable for the external audit setting, to detecting a user-specified ϵitalic-ϵ\epsilonitalic_ϵ change in behavior, which could be used for the internal audit if small changes are acceptable. Similar to Pandeva et al. (2024), test performance is optimized using a learning algorithm, improving sample efficiency over prior testing methods (Lopez-Paz & Oquab, 2017; Lhéritier & Cazals, 2018; Podkopaev & Ramdas, 2024). This testing approach can complement a full evaluation when used as a warning system. Before an expensive model assessment on large-scale benchmarks (Achiam et al., 2023; Dubey et al., 2024; Zhang et al., 2024), our approach can be used to detect an initial behavior change, which can then trigger a full evaluation. We experimentally verify that our test satisfies theoretical guarantees and we report its sample efficiency on recent LM architectures for both auditing use cases. We release our code here: https://github.com/richterleo/Auditing_Test_for_LMs. Figure 1: An External Audit Example. A regulator can use the test we describe to perform an external audit: 1. The regulator initially certifies an LM by prompting and evaluating the set of generations received; 2. Later, tipped off that LM behavior may have changed, the regulator poses as a consumer and sends prompts to the model vendor, collecting the generations; 3. The regulator compares the distribution of behavior scores b⁢(⋅)𝑏⋅b(\cdot)italic_b ( ⋅ ) between the initial, certified generations and the later generations using a Behavioral Shift Auditing (BSA) test. If the distributions are sufficiently different the test triggers. Using our proposed method, the regulator can test samples sequentially without increasing the false-positive rate. The method is guaranteed to detect a change if one exists, given enough samples (more details in Section 3)."
Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression,"In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored. Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation. To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL. Technically, SCAS achieves value-aware OOD state correction, capable of correcting the agent from OOD states to high-value in-distribution states. Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS achieves excellent performance without additional hyperparameter tuning. Moreover, benefiting from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations.","Deep reinforcement learning (RL) shows promise in solving sequential decision-making problems, gaining increasing interest for real-world applications [43, 58, 64, 54, 8]. However, deploying RL algorithms in extensive scenarios poses persistent challenges, such as risk-sensitive exploration [14] and time-consuming episode collection [28]. Recent advances view offline RL as a hopeful solution to these challenges [35]. Offline RL aims to learn a policy from a fixed dataset without further interactions [33]. It can tap into existing large-scale datasets for safe and efficient learning [24, 38, 51]. In offline RL research, a well-known concern is the out-of-distribution (OOD) action issue: the evaluation of OOD actions causes extrapolation error [13], which can be exacerbated by bootstrapping and result in severe value overestimation [35]. To address this issue, a large body of work has emerged to directly or indirectly suppress OOD actions during training, employing various techniques such as policy constraint [13, 31, 11], value penalization [32, 2, 7], and in-sample learning [30, 15, 72]. (a) CQL (b) TD3BC (c) SCAS (Ours) (d) Optimal Value Figure 1: The resulting state distributions of offline RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values. Distinguished from most previous works, this paper argues that, apart from the OOD action issue, there exists an OOD state issue that also impairs performance yet has received limited attention in the field. Such an issue refers to the agent encountering states out of the offline dataset during the policy deployment phase (i.e., test phase). The occurrence of OOD states can be attributed to OOD actions, stochastic environments, and real-world perturbations. Since typical offline RL algorithms do not involve policy training in OOD states, the agent tends to behave in an uncontrolled manner once entering OOD states in the test phase. This can further exacerbate the state deviation from the offline dataset and lead to severe degradation in performance [35, 76]. In mitigating this OOD state issue, existing limited work attempts to train the policy to correct the agent from OOD states to in-distribution (ID) states [76, 23]. Technically, Zhang et al. [76] construct a dynamics model and a state transition model and align them to guide the agent to ID regions, while Jiang et al. [23] resort to an inverse dynamics model for policy constraint. However, they deal with the OOD state and OOD action issues separately, requiring extra OOD action suppression components and complex distribution modeling, which sacrifices computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, especially when the dataset contains substantial suboptimal states. As a result, the performance of prior methods also leaves considerable room for improvement. In this paper, we aim to address these two fundamental OOD issues simultaneously by proposing a simple yet effective approach for offline RL. We term our method SCAS due to its integration of OOD State Correction and OOD Action Suppression. We start with solving an analytical form of a value-aware state transition distribution, which is within the dataset support but skewed toward high-value states. Then, we align it with the dynamics induced by the trained policy on perturbed states via KL divergence. This operation intends to correct the agent from OOD states to high-value ID states, a concept we refer to as value-aware OOD state correction. Through some derivations, it also eliminates the necessity of training a multi-modal state transition model. Furthermore, we show theoretically and empirically that, while designed for OOD state correction, SCAS regularization also exhibits the effect of OOD action suppression. We evaluate SCAS on the offline RL benchmarks including D4RL [10] and NeoRL [50]. SCAS achieves excellent performance with consistent hyperparameters without additional tuning. Moreover, benefiting from its OOD state correction ability, SCAS demonstrates improved robustness against environmental perturbations. To summarize, the main contributions of this work are: • We systematically analyze the underexplored OOD state issue in offline RL and propose a simple yet effective approach SCAS unifying OOD state correction and OOD action suppression. • Our approach achieves value-aware OOD state correction, which circumvents modeling complex distributions and significantly improves performance over vanilla OOD state correction methods. • Empirically111Our code is available at https://github.com/MAOYIXIU/SCAS., our approach demonstrates superior performance on standard offline RL benchmarks and enhanced robustness in perturbed environments without additional hyperparameter tuning."
Multi-Agent Reinforcement Learning with Selective State-Space Models,"The Transformer model has demonstrated success across a wide range of domains, including in Multi-Agent Reinforcement Learning (MARL) where the Multi-Agent Transformer (MAT) has emerged as a leading algorithm in the field. However, a significant drawback of Transformer models is their quadratic computational complexity relative to input size, making them computationally expensive when scaling to larger inputs. This limitation restricts MAT’s scalability in environments with many agents. Recently, State-Space Models (SSMs) have gained attention due to their computational efficiency, but their application in MARL remains unexplored. In this work, we investigate the use of Mamba, a recent SSM, in MARL and assess whether it can match the performance of MAT while providing significant improvements in efficiency. We introduce a modified version of MAT that incorporates standard and bi-directional Mamba blocks, as well as a novel ‘cross-attention’ Mamba block. Extensive testing shows that our Multi-Agent Mamba (MAM) matches the performance of MAT across multiple standard multi-agent environments, while offering superior scalability to larger agent scenarios. This is significant for the MARL community, because it indicates that SSMs could replace Transformers without compromising performance, whilst also supporting more effective scaling to higher numbers of agents. Our project page is available at https://sites.google.com/view/multi-agent-mamba.","Figure 1: Normalised mean episode returns aggregated over all tasks and environments with 95% confidence intervals for MAM, MAT, and MAPPO. Results are obtained using ten independent seeds. MAM matches the final performance of MAT, currently state-of-the-art in MARL, and exhibits greater sample efficiency. Figure 2: Mean time (seconds) per evaluation step in smacv2 tasks with increasing numbers of agents for MAM, MAT, and MAPPO. The mean time per evaluation step for MAT increases approximately quadratically, while MAM and MAPPO scale linearly in the number of agents. Figure 3: Comparison of performance metrics for MAM, MAT, and MAPPO across various tasks. Multi-Agent Reinforcement Learning (MARL) still faces significant challenges that must be overcome to unlock its full potential; one such challenge is the ability to scale to large numbers of agents while maintaining good performance. The Multi-Agent Transformer (MAT) (Wen et al., 2022) boasts state-of-the-art performance in online MARL. MAT is an encoder-decoder framework that utilises the multi-agent advantage decomposition theorem (Kuba et al., 2022) to reframe the challenge of joint policy optimisation. It converts the problem into a sequential decision-making process, simplifying the search for optimal policies across multiple agents. However, Transformers scale quadratically in sequence length (Vaswani et al., 2023). This creates a computational bottleneck for MAT as the number of agents becomes large. Recently, State-Space Models (SSMs) (Gu et al., 2022; Gupta et al., 2022; Gu et al., 2021; Gupta et al., 2022; Smith et al., 2023; Fu et al., 2023) have offered a solution to this drawback in the Transformer architecture, with the ability to scale linearly in the sequence length. Of interest in this work is Mamba (Gu and Dao, 2024)–a selective SSM which boasts fast inference and linear scaling in the sequence length whilst matching the performance of attention architectures in the natural language processing domain. The innovations of the Mamba model are its input-dependent SSM parameters and its hardware-aware parallel algorithm in recurrent mode. In this paper, we explore replacing attention in the MAT architecture with Mamba blocks. We make use of both existing vanilla and bi-directional Mamba blocks, as well as a novel ‘cross-attention’ Mamba block we designed to replace MAT’s cross-attention. We evaluate our novel architecture, which we call the Multi-Agent Mamba (MAM), on a wide range of well-known MARL benchmark environments and compare its performance to MAT. Our core contributions can be summarised as follows: • We create an extension of the vanilla Mamba block which can be used as a cross-attention replacement. • We replace the three different attention blocks in the MAT architecture with vanilla, bi-directional and cross-attentional Mamba blocks respectively. • We empirically validate that MAM performs comparably to MAT on a wide range of MARL environments while offering significantly greater efficiency."
Toward Finding Strong Pareto Optimal Policies in Multi-Agent Reinforcement Learning,[1]\fnmViet Cuong \surTa,"Multiagent reinforcement learning (MARL) is the learning framework where multiple different agents learn to make optimal decisions in an environment through the Reinforcement Learning paradigm. MARL research advances rapidly, with the recent development of MARL achieving impressive results on a wide range of learning scenarios, notably in zero-sum games [21] and fully cooperative environments [18, 27] where all agents share the same reward function. A less studied area of research in MARL is in the environments with cooperative reward structures, in which all agents have different and possibly conflicting reward schemes [15]. In these problems, there might be multiple different but optimal policies, in the sense that none of which is better than the other. This concept is known as the Pareto optimality in multi-objective optimization. It is desirable that we can find such Pareto optimal policies, as they are equivalent to the optimal policies in the common setting of single agent problems [3]. While the current MARL methods are known to find Nash equilibrium [13], such solutions can be suboptimal [17]. In this paper, we first show that in general cooperative environments, agents need to explicitly consider the optimization of other agents to achieve Pareto optimality. Such behaviors are known as altruistic learning in RL literature [9]; altruistic learners learn to act for the benefit of other agents even though those actions do not bring about any personal gain for the actors. To learn altruistically, one agent needs to optimize not only the reward of itself but also the rewards of other agents, which involves some form of multi-objective optimization. As a result, we connect the multi-objective framework to the MARL domain. Multiple Gradient Descent Algorithm (MGDA) [5] is one of the most popular gradient-based multi-objective methods. MGDA can find arbitrary solutions in the Pareto optimal Set using first-order gradient descent. However, MGDA is known to only converge to weak Pareto optimal solutions [8]. While this problem is not significant in other learning settings, we show that MARL problems can have many weak Pareto Stationary points, which can reduce the efficacy of MGDA. To this end, we identify the effect of diminishing gradient norms as a root cause to the weak Pareto convergence issue of MGDA and propose an improved version of MGDA++ based on this observation. We demonstrate both theoretically and empirically that MGDA++ can converge to strong Pareto optimal solutions. To summarize, our contributions in this paper are: • We show that to achieve Pareto optimality in MARL, agents need to consider the objective of other agents, we connect the multi-objective optimization with MARL problems, and propose to apply MGDA to the MARL problems. • We propose MGDA++, an extension of MGDA that converges to strong Pareto Solutions with bi-objective problems in convex, smooth settings both theoretically and empirically. To our knowledge, this strong Pareto convergence result in the convex setting with gradient descent is the first in the literature. • We demonstrate the effectiveness of MGDA++ with trust region methods through several cooperative scenarios in the Gridworld benchmark. Our proposed method is able to achieve better convergence solutions across different agents in comparison with other baselines."
Notes on the Mathematical Structure of GPT LLM Architectures,"When considered from a purely mathematical point of view, the building and training of a large (transformer) language model (LLM) is the construction of a function - which can be taken to be a map from some euclidean space to another - that has certain interesting properties. And therefore, from the point of view of a mathematician, it may be frustrating to find that many key papers announcing significant new LLMs seem reluctant to simply spell out the details of the function that they have constructed in plain mathematical language or indeed even in complete pseudo-code (and the latter form of this complaint appears to be one of the motivations behind a recent article of Phuong and Hutter [1]). Here, we seek to give a relatively ‘pure’ mathematical description of the architecture of a GPT-3-style LLM.","When considered from a purely mathematical point of view, the building and training of a large (transformer) language model (LLM) is the construction of a function - which can be taken to be a map from some euclidean space to another - that has certain interesting properties. And therefore, from the point of view of a mathematician, it may be frustrating to find that many key papers announcing significant new LLMs seem reluctant to simply spell out the details of the function that they have constructed in plain mathematical language or indeed even in complete pseudo-code (and the latter form of this complaint appears to be one of the motivations behind a recent article of Phuong and Hutter [1]). Here, we seek to give a relatively ‘pure’ mathematical description of the architecture of a GPT-3-style LLM. Trainable Parameters Like all such models in machine learning, the construction really initially describes a family of functions indexed by some set Θ=𝐑NΘsuperscript𝐑𝑁\Theta=\mathbf{R}^{N}roman_Θ = bold_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT called the parameter space. There is then a separate process - the training of the model - in which a particular value θ∈Θ𝜃Θ\theta\in\Thetaitalic_θ ∈ roman_Θ is selected using a training algorithm. Each dimension of ΘΘ\Thetaroman_Θ corresponds to the possible values of an individual trainable parameter. We will draw attention to such parameters as we introduce them, as opposed to attempting to give a definition of ΘΘ\Thetaroman_Θ up front. But this short note discusses only the architecture and does not describe any training algorithms."
BitPipe: Bidirectional Interleaved Pipeline Parallelismfor Accelerating Large Models Training,"With the increasing scale of models, the need for efficient distributed training has become increasingly urgent. Recently, many synchronous pipeline parallelism approaches have been proposed to improve training throughput. However, these approaches still suffer from two major issues, i.e., pipeline bubbles caused by periodic flushing and extra communication due to the increasing number of pipeline stages. To this end, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. Specifically, a hybrid scheme of fusing interleaved pipelines with bidirectional pipelines is proposed to reduce the computational time of each single micro-batch and multiply the number of devices executing simultaneously. A V-shaped schedule with eager gradient synchronization is introduced to reduce and overlap the communication between devices. Experiments conducted on up to 32 GPUs show that BitPipe improves the training throughput of GPT-style and BERT-style models by 1.05×1.05\times1.05 ×-1.28×1.28\times1.28 × compared to the state-of-the-art synchronous approaches.","Scaling the number of parameters in contemporary deep learning models has yielded remarkable the state-of-the-art (SOTA) results. Training these large models is challenging, as the limited memory and computational capacity of a single device (e.g., GPU) pose obstacles to accommodating them within realistic timeframes. For instance, training a GPT-3 175B model demands over 3,000 GiB for storing model parameters and optimizer states, requiring an impractical 288 years with a single NVIDIA V100 GPU (Kim et al. 2023; Narayanan et al. 2021b). The urgency for parallel and distributed training (e.g., data parallelism and model parallelism) has become increasingly pronounced. While data parallelism (Li et al. 2014) allows for ideal speedup, it falters when confronted with large models that exceed the capacity of a single device. Model parallelism (Dean et al. 2012; Lee et al. 2014; Wang, Huang, and Li 2019) addresses this limitation by distributing the weight parameters of a model across multiple devices, which mitigates the memory usage per device but suffers from severe resource under-utilization. Pipeline parallelism improves resource utilization, which splits a batch into smaller micro-batches and divides a model into stages within a pipeline, allowing simultaneous execution of different micro-batches across multiple devices. Pipeline parallelism can be categorized into synchronous and asynchronous schemes based on weight update semantic. Synchronous approaches flush periodically at the end of each iteration to guarantee strict optimizer semantics, which causes device idle times (also called pipeline bubbles). Asynchronous approaches do away with flushes completely by delaying weight updates, but at the expense of strict model convergence and thus are not within the scope of our work. Figure 1: Classic synchronous pipeline schedules, with 4 pipeline devices and 8 micro-batches within a training iteration. Both schedules have the same bubble overhead and weights memory consumption (Mθsubscript𝑀θM_{\uptheta}italic_M start_POSTSUBSCRIPT roman_θ end_POSTSUBSCRIPT). The activations memory consumption (Masubscript𝑀aM_{\rm a}italic_M start_POSTSUBSCRIPT roman_a end_POSTSUBSCRIPT) of the 1F1B schedule exhibits better efficiency but existing imbalance. Early synchronous approach (e.g., GPipe (Huang et al. 2019)) focuses on reducing pipeline bubbles by increasing the number of concurrent batches in the pipeline (as shown in Figure 1(a)). As a direct consequence, there is an increase in peak activation memory demands. Subsequently, encouraged by the success of the 1F1B schedule (as shown in Figure 1(b)), researchers have proposed memory-efficient approaches (e.g., DAPPLE (Fan et al. 2021) and PipeDream-Flush (Narayanan et al. 2021a)), which further adjusts the number of micro-batches injected into devices at the beginning of pipelines. Recently approaches attempt to increase the number of devices executing simultaneously (i.e., bidirectional pipeline parallelism), or to reduce the computational time of a single micro-batch (i.e., interleaved pipeline parallelism), which shows the SOTA performance. In the bidirectional approaches (Jain et al. 2020; Li and Hoefler 2021; Zhang et al. 2023), each device stores multiple pipeline stages in different directions, which decreases bubble size and achieves a more balanced activation memory consumption. On the other hand, interleaved approaches (Narayanan et al. 2021b; Lamy-Poirier 2023; Liu et al. 2023) assign multiple smaller and nonconsecutive stages to each device, which makes each bubble size smaller accordingly. Despite the promising results, the latest synchronous approaches still face two primary issues. First, the remaining bubbles still pose the largest deficiency. Due to computation dependencies in the pipeline across different devices, bubbles are inevitable. In existing approaches, as much as 50% of the time can be spent to flush the pipeline. Second, the communication overhead remains considerable even though pipeline parallelism employs point-to-point (P2P) communication. Specifically, bidirectional pipeline parallelism requires additional weight memory and data-parallel communication to reduce pipeline bubbles, while interleaved pipeline parallelism shrinks bubble size at the expense of extra P2P communication. Moreover, if the bidirectional pipeline extends to more than two pipelines, or each device in the interleaved pipeline generalizes to have more stages, the extra communication or memory usage will increase accordingly, further degrading their performance. To address the aforementioned issues, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. To the best of our knowledge, BitPipe is the first work that incorporates the interleaved schedule into bidirectional pipeline parallelism, which reduces the computational time of each single micro-batch and doubles the number of devices executing simultaneously. BitPipe transforms the looping schedule of the interleaved pipeline to a V-shaped schedule and thus mitigates the side effect of the additional communication overhead. The contributions of BitPipe are summarized as follows: • We propose a hybrid pipeline scheme of fusing interleaved pipelines with bidirectional pipelines. This design can not only improve throughput, but also achieves a harmonious balance in memory utilization. • We introduce a V-shaped schedule of partially transforming cross-device communication to local copying, alongside an eager gradient synchronization scheme, which can reduce and overlap communication between devices. • Experiments show that BitPipe can improve the end-to-end performance by up to 1.28×1.28\times1.28 × per iteration for GPT-style and BERT-style models compared to the SOTA synchronous pipeline approaches."
FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with Ferroelectric In-Memory Computing,"In scenarios with limited training data or where explainability is crucial, conventional neural network-based machine learning models often face challenges. In contrast, Bayesian inference-based algorithms excel in providing interpretable predictions and reliable uncertainty estimation in these scenarios. While many state-of-the-art in-memory computing (IMC) architectures leverage emerging non-volatile memory (NVM) technologies to offer unparalleled computing capacity and energy efficiency for neural network workloads, their application in Bayesian inference is limited. This is because the core operations in Bayesian inference, i.e., cumulative multiplications of prior and likelihood probabilities, differ significantly from the multiplication-accumulation (MAC) operations common in neural networks, rendering them generally unsuitable for direct implementation in most existing IMC designs. In this paper, we propose FeBiM, an efficient and compact Bayesian inference engine powered by multi-bit ferroelectric field-effect transistor (FeFET)-based IMC. FeBiM effectively encodes the trained probabilities of a Bayesian inference model within a compact FeFET-based crossbar. It maps quantized logarithmic probabilities to discrete FeFET states. As a result, the accumulated outputs of the crossbar naturally represent the posterior probabilities, i.e., the Bayesian inference model’s output given a set of observations. This approach enables efficient in-memory Bayesian inference without the need for additional calculation circuitry. As the first FeFET-based in-memory Bayesian inference engine, FeBiM achieves an impressive storage density of 26.32 Mb/mm2 and a computing efficiency of 581.40 TOPS/W in a representative Bayesian classification task. These results demonstrate 10.7×\times×/43.4×\times× improvement in compactness/efficiency compared to the state-of-the-art hardware implementation of Bayesian inference.","In-memory computing (IMC) has recently emerged as a promising solution to address the memory wall issues in conventional von Neumann hardware (ielmini2018memory, ). Leveraging the compactness and high energy efficiency of emerging non-volatile memory (NVM) technologies, many leading IMC accelerators have achieved impressive computing efficiency and throughput for data-intensive machine learning models, particularly neural networks (NNs) (shafiee2016isaac, ; hu2021memory, ; yan2023improving, ; jung2022crossbar, ). While conventional NN-based algorithms are widely used, they often struggle in situations where training data is insufficient or when interpretable results are needed (qayyum2020secure, ; yang2022unbox, ). As a compelling alternative, Bayesian inference is particularly well-suited in low-data scenarios, providing explainable results with uncertainty estimation (ghahramani2015probabilistic, ; burkart2021survey, ). The primary posterior calculation in Bayesian inference, which involves the cumulative product of prior and likelihood probabilities as per Bayes’ theorem, however, differs from the multiply-and-accumulate (MAC) operations common in NN workloads. This difference renders Bayesian inference usually unsuitable for direct implementation with many existing IMC designs that typically focus on NN acceleration. In traditional complementary metal-oxide-semiconductor (CMOS)-based von Neumann implementations for Bayesian inference, such as CPU (smith2020massively, ), GPU (talbot2019parallelized, ) and field-programmable gate array (FPGA) (awano2020bynqnet, ), accessing separate memory units for stored probabilities incurs significant area and energy overhead. Efforts to exploit the non-volatility and energy efficiency of emerging devices have led to the development of Bayesian inference prototypes utilizing random number generators (RNGs) built with magnetic tunnel junction (MTJ) (vodenicarevic2017low, ), memtransistor (zheng2022hardware, ) and magnetic random-access memory (MRAM) (faria2018implementing, ). These implementations, however, are limited to Bayesian inference with binary evidence/events, and do not effectively address probability storage, rather generating required probabilities on demand, which is energy-consuming. A memristor-based Bayesian machine has been proposed (harabi2023memristor, ), using near-memory stochastic computing to reduce memory access overhead. Yet, these implementations still require additional CMOS logic and multiple clock cycles for posterior calculations and complex sensing circuitry for final inference, thus compromising computing density and inference efficiency. To address the aforementioned challenges in hardware implementation of Bayesian inference, we propose FeBiM, an efficient and compact in-memory Bayesian inference engine utilizing multi-level cell (MLC) ferroelectric field-effect transistors (FeFETs). The key contributions of this work are summarized as follows: • We propose a compact crossbar array design using one FeFET per cell as probability storage unit and a compact and scalable winner-take-all (WTA) circuit for sensing. This multi-bit FeFET array enables efficient in-memory Bayesian inference in just one clock cycle, eliminating the need for extra calculation circuitry. • We introduce a novel mapping scheme that associates quantized logarithmic probabilities with discrete FeFET states. This scheme enables the output currents of the crossbar to naturally represent the posterior probabilities, i.e., the cumulative product of priors and likelihoods given a set of observations. • We thoroughly investigate the functionality, scalability and application level performance of FeBiM. In a representative Bayesian classification task, our proposed design shows a 10.7×\times×/43.4×\times× storage density/inference efficiency improvement compared to the state-of-the-art Bayesian machine. The rest of the paper is organized as follows. Section 2 reviews the basics and relevant prior works. Section 3 introduces our FeFET-based IMC design for Bayesian inference. Section 4 presents the validation, scalability investigation and application benchmarking results of FeBiM. Finally, section 5 concludes the paper."
Interpreting Neural Networks through Mahalanobis Distance,"This paper introduces a theoretical framework that connects neural network linear layers with the Mahalanobis distance, offering a new perspective on neural network interpretability. While previous studies have explored activation functions primarily for performance optimization, our work interprets these functions through statistical distance measures, a less explored area in neural network research. By establishing this connection, we provide a foundation for developing more interpretable neural network models, which is crucial for applications requiring transparency. Although this work is theoretical and does not include empirical data, the proposed distance-based interpretation has the potential to enhance model robustness, improve generalization, and provide more intuitive explanations of neural network decisions.","Neural networks have revolutionized machine learning, achieving remarkable success across diverse applications. Central to their efficacy is the use of activation functions, which introduce non-linearity and enable the modeling of complex relationships within data. While Rectified Linear Units (ReLU) have gained prominence due to their simplicity and effectiveness (Nair and Hinton, 2010), the exploration of alternative activation functions remains an open and valuable area of research (Ramachandran et al., 2018). Neural network units are often viewed as linear separators that define decision boundaries between classes (Minsky and Papert, 1969) with larger activation values suggesting stronger contributions of features to those decisions. Our work challenges this perspective, exploring how individual neurons can be understood through the lens of statistical distance measures. Clustering techniques use distance measures. They aim to minimize the distance between data points and feature prototypes, with smaller values indicating stronger membership to the feature or cluster (MacQueen, 1967a). We explore the intersection between these perspectives on activation interpretations, leveraging the distance-minimization approach of clustering techniques to lay the groundwork for novel neural network designs based on statistical distance measures. This paper establishes a novel connection between neural network architectures and the Mahalanobis distance, a statistical measure that accounts for the covariance structure of data (Mahalanobis, 1936). We present a robust mathematical framework that bridges neural networks with this statistical distance measure and lays the groundwork for future research into neural network interpretability and design. Our key contributions are: 1. We establish a mathematical connection between neural network linear layers and the Mahalanobis distance, demonstrating how Absolute Value (Abs) activations facilitate distance-based interpretations. 2. We analyze the solution space that neural networks are likely to learn when approximating Mahalanobis distance, exploring the effects of non-uniqueness in whitening transformations and the role of Abs-activated linear nodes. 3. We discuss the broader implications of this framework for neural network design and interpretability, laying the groundwork for more interpretable models."
COAT:CompressingOptimizer states andActivation for Memory-Efficient FP8Training,"FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54× compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43× end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine’s speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT.","Foundation Models (FMs), such as Large Language Models (LLM) and Vision Language Models (VLM), have made significant breakthroughs in various tasks such as reasoning, understanding, and summarization (Dubey et al., 2024; Adler et al., 2024; Team et al., 2024; Lin et al., 2024). However, the training of such models, which often comprise billions of parameters, demands substantial computational resources and memory. This presents substantial challenges, making the training of these foundation models very challenging (Smith et al., 2022; Hoffmann et al., 2022). Low-precision training has emerged as a promising approach to make FMs training more efficient (Micikevicius et al., 2017; Wang et al., 2018; Zhu et al., 2020; Xi et al., 2023; Wortsman et al., 2023; Xi et al., 2024). By quantizing tensors used in deep neural networks into lower precision, low-precision training effectively speed up the training process and reduce the memory footprint. Currently, BF16 training (Kalamkar et al., 2019; Micikevicius et al., 2017) is the most prevalent low-precision method, and is widely adopted in large-scale training frameworks like DeepSpeed (Rasley et al., 2020) and Megatron-LM (Shoeybi et al., 2019). With the advent of Nvidia’s H100 GPU (NVIDIA, 2024a), FP8 training Micikevicius et al. (2022) is emerging as the next-generation low-precision technique. Compared to BF16, FP8 training has the potential to (1) double the speed and (2) halve the memory footprint. To achieve practical speedup, Transformer Engine (NVIDIA, 2024b) performs matrix multiplications in FP8 precision, leading to faster training. Transformer Engine’s memory footprint can be further improved by reducing optimizer states, gradients, weights, and activations to lower precision. As illustrated in Figure 1, FP8-LM (Peng et al., 2023) advances this by further quantizing the gradients, weight master copy, and first-order momentum into FP8. This reduces memory and communication overhead, partially improving memory efficiency. However, they do not tackle the memory consumption of activations and still leave the optimizer’s second-order momentum in higher precision. The memory problem of activations becomes even more critical when optimizer, gradient, and weights are sharded across multiple GPUs using ZeRO or FSDP. Besides, second-order momentum is more sensitive to quantization than first-order momentum (Fishman et al., 2024), and activations’ large spikes also make them hard to quantize to FP8 (Yang et al., 2024). This potential accuracy degradation makes them missing a crucial opportunity to optimize memory further. In this work, we propose COAT: Compressing Optimizer states and Activations for memory-efficient FP8 Training to address the aforementioned issue. COAT significantly reduces the overall memory footprint by quantizing optimizer states and activations into FP8. For optimizer states, we observe that FP8 format’s representation range is under-utilized when quantizing them, as illustrated in Figure 2(a). To address this, we introduce a novel Dynamic Range Expansion method which adjusts the distribution of optimizer states to better fit within the FP8 range, thereby minimizing quantization error. For activations, we propose Mixed-Granularity Activation Quantization to achieve efficient and accurate quantization. We apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers. Per-tensor quantization for matrix multiplications is more efficient and better suited for TensorCores, while fine-grained quantization helps maintain accuracy. These two approaches tackle high memory consumption while ensuring minimal performance degradation. We provide an overview of COAT in Figure 1(b) for demonstration. We demonstrate the accurate performance of COAT on a wide range of tasks, including LLM pretraining, LLM fine-tuning, and VLM training. COAT achieves nearly lossless performance on all of these tasks. For efficiency results, COAT achieves 1.54×1.54\times1.54 × end-to-end memory reduction compared with BF16, and 1.43×1.43\times1.43 × end-to-end training speed up on Llama 7B, 13B, and 30B models compared to BF16. COAT also doubles the batch size in all realistic distributed training settings, which is crucial for higher speedup and support for longer context length, leading to a more efficient training process for large-scale models."
Golden Ratio-Based Sufficient Dimension Reduction,"Many machine learning applications deal with high dimensional data. To make computations feasible and learning more efficient, it is often desirable to reduce the dimensionality of the input variables by finding linear combinations of the predictors that can retain as much original information as possible in the relationship between the response and the original predictors. We propose a neural network based sufficient dimension reduction method that not only identifies the structural dimension effectively, but also estimates the central space well. It takes advantages of approximation capabilities of neural networks for functions in Barron classes and leads to reduced computation cost compared to other dimension reduction methods in the literature. Additionally, the framework can be extended to fit practical dimension reduction, making the methodology more applicable in practical settings.","The curse of dimensionality poses significant challenges to statistical analysis when dealing with a large number of variables [1]. Under the supervised learning framework, sufficient dimension reduction (SDR) has emerged as a useful tool that bridges the gap between high dimensionality and traditional statistical modeling. However, current state-of-the-art statistical methods for SDR in the literature often presume the structural dimension, which is generally not the case in practice. Additionally, these methods may not be computationally feasible for handling large sample sizes or high dimensionality efficiently, which limits their usage in many real-world applications. Numerous methods have been proposed on SDR in the past decades, see e.g., [2]. For classical methods such as sliced inverse regression (SIR) [3], minimum average variance estimation (MAVE) method [4], and sliced average variance estimation (SAVE) [5], a main class of estimators for the central space is based on the inverse conditional moments of X|Yconditional𝑋𝑌X|Yitalic_X | italic_Y, where Y∈ℝ𝑌ℝY\in\mathbb{R}italic_Y ∈ blackboard_R and X∈ℝp𝑋superscriptℝ𝑝X\in\mathbb{R}^{p}italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT are the response and p𝑝pitalic_p-dimensional predictor in a regression analysis, respectively. Slicing the continuous response y𝑦yitalic_y is often used to facilitate the estimation. However, this imposes some strong probabilistic structure on X𝑋Xitalic_X. Moreover, selecting the number of slices remains an open and challenging question [6]. [7] proposed a cumulative slicing estimation methodology for SDR and developed three methods: cumulative mean estimation (CUME), cumulative directional regression (CUDR), and cumulative variance estimation (CUVE). [8] introduced a fused estimation procedure (fSIR), with observed performance improvement in some situations. [9] implemented a sliced inverse regression in an online fashion that first constructs an online estimate for the kernel matrix and then performs online singular value decomposition. [10] established consistency of estimation of the dimension reduction space in a high-dimensional setting. In a more recent work, [6] proposed an aggregate inverse mean estimation (AIME) procedure that may substantially improve estimation accuracy compared to the previous methods. It incorporates the cumulative slicing scheme into the aggregate SDR idea proposed by [11] and is much less sensitive to linearity condition violations with the localization step before aggregation. [12] proposed a real-time approach for SDR that uses a principal least squares support vector machines approach to estimate the central subspace more accurately. Their method updates the estimator efficiently as new data is collected, starting from an initial estimator obtained with currently available data. [13] proposed a method that first estimates the expectiles through kernel expectile regression and then carries out dimension reduction based on random projections of the regression expectiles. Several methods in the literature are extended under these frameworks [14, 15, 16]. There are also neural network approaches to SDR for tackling classification problems [17, 18, 19]. For regression problems, [20] proposed a nonlinear SDR method and [21] proposed a stochastic neural network that is computationally feasible to handle large scale data. [21] has proposed an algorithm that is able to obtain structural dimension, although no theoretical understanding is provided (different from this work). In this paper, we propose a golden ratio-based neural network for SDR (GRNN-SDR), a novel approach that utilizes neural networks to capture complex functional forms that are previously inaccessible with the traditional nonparametric regression tools. Our algorithm incorporates the golden ratio to dynamically search for the structural dimension, which significantly reduces computation time and complexity. Theoretical basis have demonstrated the generalization ability of multi-layer neural networks [22, 23]. Under proper conditions, we establish theoretical results that demonstrate that our approach leads to the true structural dimension with high probability. Compared to most of the existing methods, which typically presume the structural dimension to estimate the central space, extensive numerical results show that our proposed method is able to obtain the true or practical structural dimension effectively without prior knowledge. Extensive experiment comparisons show that our method estimate the central space with higher accuracy in most cases and demonstrate higher stability when the true dimensionality is not small. Furthermore, our algorithmic complexity under a fixed neural network structure is O⁢(N)𝑂𝑁O(N)italic_O ( italic_N ), where N𝑁Nitalic_N is the sample size, offering a promising solution to the challenges in SDR."
A Stock Price Prediction Approach Based on Time Series Decomposition and Multi-Scale CNN using OHLCT Images,"Stock price fluctuations are influenced by a variety of factors, including macroeconomic conditions, government policies and market sentiment, which together make price movements complex and difficult to be predicted. Despite many studies aimed at enhancing stock price prediction models, the challenges such as data noise, model overfitting and lack of interpretability are still encountered. To address these issues and improve prediction accuracy, this paper proposes a novel method, named Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), for predicting stock price movements in the China A-share market.","With the rapid development of financial markets and the acceleration of globalization, stock investment has become an essential avenue for many investors to achieve wealth appreciation [1, 2, 3]. However, the complexity and uncertainty of the stock market make accurately predicting stock trends a challenging task. The China A-share market, being one of the largest and most dynamic globally, has attracted significant quantitative analysis and research due to the growing impact of economic globalization [4, 5, 6]. Traditional stock analysis methods heavily reliant on financial data and macroeconomic indicators and often fall short in capturing the dynamic and non-linear relationships within the market [7, 8, 9, 10]. Therefore, quantitative trading has emerged as a pivotal component of modern financial strategies. By leveraging computational techniques, it enables the execution of trading strategies devoid of emotion-based decision-making, thereby uncovering patterns that may elude human analysts [11]. The potential quantitative trading in stock market forecasting has been well-documented [12]. This progress has led to the development of advanced deep models that can handle the inherent complexity, nonlinearity, and noise of financial markets, making the construction of stock trend prediction models using machine learning, deep learning, and big data techniques, which becomes a hot research topic in finance [13, 14, 15, 16, 17, 18, 19]. Despite significant advancements, many deep learning models for stock price prediction still grapple with challenges such as data noise, model overfitting, and insufficient interpretability [20]. Baek et al. [21] pointed out that the limited number of training data points often leads to overfitting in deep neural network models. Ito et al. [22] aggregated several simple, interpretable weak algorithms and assessed the importance of each weak algorithm in improving overall interpretability. In order to tackle the data noise problem, the research conducted by Liu et al. [23] employed sparse autoencoders with one-dimensional (1D) residual convolutional networks to denoise the stock prices. Convolutional neural networks (CNNs) have been employed for stock market prediction due to their efficient feature extraction capabilities [24, 25, 26]. However, traditional CNN approaches often fail to fully utilize time series information, resulting in suboptimal performance under complex market conditions [27]. Additionally, when considering multiple stocks, these models typically focus on individual stock information and neglect the inter-stock correlations that significantly influence price fluctuations [28]. Jiang et al. [29] utilized the opening prices, highest price, lowest price, closing price, and volume (OHLCV) images as inputs to predict the probability of stock price increasing or decreasing, achieving a significant accuracy. Their experimental results showed that 5-day feature maps in CNNs yield significantly better accuracy than 20-day and 60-day maps, suggesting that longer time series of stock feature maps make it challenging for CNN models to identify critical feature points, leading to local feature overfitting. Converting sequences to image features can lose the advantage of utilizing more historical data, whereas shorter image features risk sacrificing significant historical information, increasing prediction inaccuracies. Inspired by these studies, we first propose two novel methods. The first method involves replacing trading volume with stock turnover rate, which eliminates the impact of trading volume caused by ex-rights events, resulting in more stable features. The second method introduces an integration between time separator and OHLCT (Opening price, Highest price, Lowest price, Closing price, and Turnover rate), resulting a new image feature as input, named as TS-OHLCT. Specifically, we incorporate the weekend time information as separators into OHLCT images to help CNNs capture trading temporal information and learn the effects of stock trading cycles. Furthermore, two new architectures are proposed. The first is a Multi-Scale Residual Convolutional Neural Network (MSR-CNN) designed to address the overfitting problem in long sequence images. The second architecture is a Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), which addresses the problem that using only image features makes it difficult to learn information about stock price fluctuations. By integrating sequence data, SMSFR-CNN better captures stock price trends in the A-share market. We observed that traditional CNN methods often exhibit local feature overfitting and convergence issues in stock image feature extraction, with prediction performance gradually deteriorating as the time of sequence feature maps increases. This observation aligns with the findings of [29]. To address these mentioned issues, we decomposed long sequences of stock image features into multiple time periods according to different time scales and assign different feature weights based on their importance (with higher weights for features closer to the current trading day). This significantly reduces overfitting and enables CNNs to learn long sequence image features better. Additionally, considering that investors more concern about the magnitude of price fluctuations rather than the probability of price increasing or decreasing, and given that it is difficult for the image features to effectively capture the magnitude of price changes, the proposed model integrates time series information as an extra features. Our approach utilizes the CNN component to learn time series features and concatenates them with image features, simultaneously predicting both the magnitude and probability of stock price movements. This method effectively incorporates regression labels into the existing framework. Our experimental results indicate that this approach significantly improves the accuracy of stock price trend predictions, reduces the search space for image features, stabilizes and accelerates the convergence process. The paper’s major contributions can be summarized as follows: 1. This is the first time that historical open, high, low, close prices and turnover-rates are incorporated into images, which are separated by weekends and combined with time-specific information. 2. The long sequences of stock image features are decomposed into multiple time periods according to different time scales and assigned different feature weights based on their importance (with higher weights for features closer to the current trading day). 3. Combining time series with image features using CNN improves prediction accuracy, reduces the search space, stabilizes and accelerates the convergence process. 4. Comprehensive comparison experiments between different methods on 4,454 A-share stocks are provided, and the majority of A-share stocks are considered in our experiments. 5. Our proposed method, SMSFR-CNN, outperforms other advanced methods in terms of positive predictive value (PPV) and negative predictive value (NPV). The remainder of the paper is structured as follows. The related works of CNN methods in stock prediction are introduced in Section 2. Section 3 presents the dataset that serves as the basis for our study along with a detailed description of the innovative and predictive model developed in this paper. In Section 4, we describe and analyze the experimental settings and results. Finally, we provide a summary of the conclusions and main contributions of the paper in Section 5. We also highlight the significance of our results and propose potential works for future research."
Applying sparse autoencoders to unlearn knowledge in language models,"We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it language models. We demonstrate that individual interpretable biology-related SAE features can be used to unlearn biology-related knowledge with minimal side-effects. Our results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective. We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique. Current SAE quality or intervention techniques would need to improve to make SAE-based unlearning comparable to the existing fine-tuning based techniques.","Current and future language models may learn inaccurate information, produce toxic or malicious outputs, or possess dangerous capabilities that we would like to remove before deployment, such as advanced bio-weapons knowledge (Ji et al., 2023; Li et al., 2024). However, we do not yet know how to precisely and robustly remove knowledge or unlearn capabilities in these language models. The goal of this work is to investigate whether sparse autoencoders (SAEs) can be used to perform unlearning in an interpretable way. Recent work on unlearning has typically focused on fine-tuning based methods that have been applied in a variety of contexts to unlearn concepts in language models (e.g. Li et al., 2024; Zou et al., 2024; Eldan & Russinovich, 2023), going beyond prior work that aimed to unlearn specific training data points in neural networks (Bourtoule et al., 2020). While relatively successful, these fine-tuning approaches are opaque and we lack insight into what exactly is happening in the model (Łucki et al., 2024). Existing methods for removing specific facts from language models offer interpretable solutions (e.g. Meng et al., 2023), however these approaches are limited to fact-level unlearning. Having an interpretable method for unlearning is important as it can allow a higher level of confidence that the model has actually unlearned the knowledge, rather than superficially or temporarily hiding the capability to discuss a given topic. One possibility is to use sparse autoencoders to try to unlearn knowledge in an interpretable way. Sparse autoencoders (SAEs) use an unsupervised method to learn sparse reconstruction of language model activations (e.g. Ng, 2011; Bricken et al., 2023; Cunningham et al., 2023; Templeton et al., 2024; Marks et al., 2024; Gao et al., 2024). SAEs have been shown to find interpretable features in language models. SAEs appear to be a promising approach to help understand complex, abstract features that are used by language models (Templeton et al., 2024). Whether SAEs can be used to make systematic, predictable, interpretable interventions in language models in a variety of contexts remains an open question. Our work makes two key contributions: First, we attempt to develop a method for unlearning knowledge in language models in an interpretable way. Second, we apply SAEs to the task of unlearning knowledge, extending their use beyond previous work. Our approach aims to work towards more transparent and verifiable knowledge removal at a broader scale. Figure 1: An outline of how we use SAE features to intervene in the model. Selected feature activations fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are set to a negative value −c𝑐-c- italic_c when fi>0subscript𝑓𝑖0f_{i}>0italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0."
Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware Neuron Management,s m \IfBooleanF#1▷▷\triangleright▷ #2,"Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of applications (chatgpt, ; gpt4, ; llm-application-medical, ; llm-application-education, ; llm-application-finance, ; llm-application-engineer, ). Comprising millions or even billions of parameters (bert, ; opt, ; gpt3, ; llama2, ; palm, ; mistral, ; gemini, ), these models demand substantial computational and memory resources, typically available only in state-of-the-art data centers. Nonetheless, there is an increasing demand for deploying LLMs on resource-constrained devices, such as smartphones (llm-mobile-1, ; llm-mobile-2, ; llm-mobile-3, ; llm-mobile-4, ; llm-mobile-5, ; llm-mobile-6, ). On one hand, stringent privacy regulations necessitate local data processing to protect user information. On the other hand, LLMs on smartphones facilitate customization based on user habits, enabling enhanced personalization. Given the limited DRAM capacity of devices, LLMs on smartphones are typically constrained to models specially designed for mobile deployment (phi3, ; minicpm, ; gemini, ). Although these models are lightweight, the reduction in parameters inevitably leads to a compromise in their capabilities (scaling-law, ). As an alternative, many recent works (powerinfer-2, ; llm-flash, ; deja-vu, ; powerinfer, ; sparsity-mobile-survey-1, ; sparsity-mobile-survey-2, ) explore the exploitation of inherent sparsity within LLMs to address memory limitations. Specially, rather than pruning model parameters, these methods selectively activate a subset of model parameters based on the input while maintaining the original performance. By transferring only the activated parameters to DRAM for computation, larger and more powerful LLMs can be stored in external flash memory, effectively surpassing DRAM limitations of smartphones. Table 1. Breakdown of average inference latency per token when offloading 50% model parameters to flash memory. Model Compute Load Total Load Ratio OPT-350M 34 ms 87 ms 121 ms 71.9% OPT-1.3B 84 ms 273 ms 357 ms 76.5% OPT-6.7B 387 ms 1883 ms 2270 ms 82.9% Llama2-7B 450 ms 10982 ms 11432 ms 96.1% Mistral-7B 355 ms 15126 ms 15481 ms 97.7% Figure 1. Bandwidth utilization on smartphones is heavily constrained by IOPS. Ripple alleviates this bottleneck and boosts bandwidth with neuron co-activation linking. However, the efficiency of this LLM inference paradigm is significantly hindered by I/O overheads. Since different inference requests generally activate distinct sets of model parameters, frequent I/O operations are generated to swap parameters between DRAM and flash memory. As shown in Table 1, even when only half of the model parameters reside in flash memory, 71.9%-97.7% of the inference latency arises from I/O operations. More critically, the scattered activation of model parameters induces numerous small-grained read accesses, limiting transfer efficiency due to constraints in Input/Output Operations Per Second (IOPS) (iops, ). As depicted in Figure 1, this IOPS bottleneck severely restricts on-device bandwidth utilization across various LLMs. Building on these insights, this paper proposes Ripple, a novel approach to accelerating LLM inference on smartphones through I/O optimizations. While previous works (deja-vu, ; powerinfer, ) primarily focus on computation efficiency under activation sparsity, they tend to exacerbate the existing I/O overhead bottlenecks. Fewer studies (powerinfer-2, ; llm-flash, ) explore mitigating I/O overhead through enhanced caching strategies to minimize data loading. However, without directly improving bandwidth utilization, overall efficiency remains suboptimal. Orthogonal to these methods, Ripple addresses the primary bottleneck in LLM inference by maximizing bandwidth utilization via the effective reduction of I/O operations. The design of Ripple is rooted in Neuron Co-Activation, a property prevalent in activation sparsity yet underexplored in current works. Specially, neurons in LLMs exhibit strong correlations in their activation patterns. When processing real-world datasets, the activation of an individual neuron is consistently linked to the activation of a stable group of others. Given the efficiency of continuous reads, which enable the retrieval of larger data blocks with a single request, Ripple introduces a key insight: Why not establish links between neurons that are frequently co-activated in flash memory, facilitating continuous read access to reduce IOPS? However, this is not a low-hanging fruit, as both neuron co-activation patterns and storage hardware characteristics exhibit inherent complexity, complicating their effective alignment. Our comprehensive analysis identifies that three critical technical challenges must be tackled: (1) Extensive Search Space. The vast number of neurons in LLMs leads to an exponentially large space of possible neuron linking combinations. Identifying the optimized neuron linking that maximizes global benefits is exceedingly difficult and infeasible through brute-force enumeration alone. (2) Random Activation Variation. Owing to varying model inputs, the activation of model parameters exhibits intrinsic randomness. Although optimized placement strategies can spatially co-locate activated neurons, access to these neurons remains hindered by discontinuities caused by randomness. (3) Misaligned Cache Strategy. Storing frequently activated neurons in memory is critical for minimizing transfer workload. However, storing neurons individually leads to fragmentation in their placement within flash memory, potentially disrupting continuous access. To this end, Ripple employs a two-stage solution that performs hierarchical optimizations both offline and online. (1) In the Offline Phase, Ripple clusters neurons exhibiting high co-activation correlation and reorganizes their placement in flash memory. To address Challenge (1), we abstract the problem into a complete graph, reformulating it as the discovery of the globally optimal Hamiltonian Path. By leveraging graph-theoretic techniques, we propose a greedy algorithm that efficiently searches for optimized placement based on observed neuron co-activation patterns. (2) In the Online Phase, Ripple performs fine-grained refinements based on optimized neuron placement, further enhancing access continuity. To tackle Challenge (2), we devise an IOPS-friendly access collapse technique. By strategically incorporating additional neurons between two separate neuron links, we improve read access continuity with negligible overhead. In response to Challenge (3), we design a linking-aligned in-memory caching policy. Rather than individually caching the hottest neurons, we account for their interlinking relationships, ensuring efficient access patterns. We evaluate Ripple on three smartphones with distinct hardware configurations, benchmarking a diverse range of LLMs varying in structures and scales. The results demonstrate that Ripple significantly boosts on-device bandwidth, achieving improvements of up to 4.32×4.32\times4.32 ×. Moreover, this bandwidth optimization yields substantial reductions in I/O latency during inference, offering speedups of up to 5.93×5.93\times5.93 × when compared to state-of-the-art solutions. To the best of our knowledge, Ripple is the first to accelerate LLM inference on smartphones by enhancing I/O bandwidth through optimized neuron placement in flash memory. Ripple effectively bridges the performance gap between flash memory and DRAM, enabling LLM inference to exceed DRAM limitations on smartphones. Our contributions can be summarized as follows: • We identify the primary bottleneck in LLM inference on smartphones as IOPS, attributing it to the inherent misalignment between scattered activation patterns and storage hardware characteristics. • We notably exploit neuron co-activation to mitigate the IOPS bottleneck, pioneering the optimization of neuron placement in flash memory for enhancing bandwidth efficiency on smartphones. • We conduct extensive evaluations on various representative LLMs and hardware, achieving substantial improvements over state-of-the-art solutions."
"Datasheet for “Coordinated Reply Attacks in Influence Operations:
Characterization and Detection”",Motivation,
A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution Generalization to Adaptation,"Distribution shifts on graphs – the discrepancies in data distribution between training and employing a graph machine learning model – are ubiquitous and often unavoidable in real-world scenarios. These shifts may severely deteriorate model performance, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph machine learning under distribution shifts, aiming to train models to achieve satisfactory performance on out-of-distribution (OOD) test data. In our survey, we provide an up-to-date and forward-looking review of deep graph learning under distribution shifts. Specifically, we cover three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation. We begin by formally formulating the problems and discussing various types of distribution shifts that can affect graph learning, such as covariate shifts and concept shifts. To provide a better understanding of the literature, we systematically categorize the existing models based on our proposed taxonomy and investigate the adopted techniques behind. We also summarize commonly used datasets in this research area to facilitate further investigation. Finally, we point out promising research directions and the corresponding challenges to encourage further study in this vital domain. Additionally, we provide a continuously updated reading list at https://github.com/kaize0409/Awesome-Graph-OOD.","Driven by the prevalence of graph-structured data in numerous real-world scenarios, growing attention has been paid to graph machine learning, which effectively captures the relationships and dependencies among entities within graphs. In particular, Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations on graphs through message-passing [1, 2, 3], and they have demonstrated remarkable success across diverse applications, such as social networks, physics problems, and traffic networks [4, 5, 6]. While graph machine learning has achieved notable success, most of the existing efforts assume that test data follows the same distribution as training data, which is often invalid in the wild. When confronted with Out-Of-Distribution (OOD) samples, the performance of graph machine learning methods may substantially degrade, limiting their efficacy in high-stake graph applications such as finance and healthcare [7]. Although numerous transfer learning methods have been proposed to address distribution shifts for Euclidean data [8, 9, 10], their direct application to graph data is challenging. This is due to the interconnected nature of entities on graphs, which violates the independent and identically distributed (IID) assumption inherent in traditional transfer learning methods. Moreover, the various types of graph shifts introduce new challenges. These shifts occur across different modalities including features, structures, and labels, and can manifest in various forms such as variations in graph sizes, subgraph densities, and homophily [11]. Given these obstacles, increasing research efforts have been dedicated to improving the reliability of graph machine learning against distribution shifts, concentrating on three main scenarios: graph OOD generalization [7, 11], training-time graph OOD adaptation [12, 13], and test-time graph OOD adaptation [14, 15]. The primary distinction between graph OOD generalization and adaptation methods lies in their assumptions regarding the availability of target data. Graph OOD generalization methods typically assume the unavailability of target data during model training and aim to enhance the model’s generalization performance on any potential unseen test distribution. In contrast, both training-time and test-time adaptation methods assume the availability of target data and aim to improve model performance on this specific target. However, they differ in their assumptions about the source data and in how they utilize knowledge of the source distribution. Training-time adaptation assumes that both the source and target graphs are available simultaneously, allowing model adaptation to start from scratch during the training process. On the other hand, test-time adaptation typically assumes access to a model pre-trained on the source graph, rather than the source graph itself, and begins adapting the model to the target data from this pre-trained state. Although graph OOD generalization, training-time OOD adaptation, and test-time OOD adaptation are closely related, there is currently no unified framework that comprehensively discusses deep graph learning under distribution shifts across all three scenarios. With recent progress on graph OOD learning, an up-to-date and forward-looking review of this field is urgently needed. In this survey, we provide, to the best of our knowledge, the first unified and systematic review of the literature on deep graph learning under distribution shifts. We start by formally formulating the problems and discussing different types of graph distribution shifts in graph machine learning. Next, our new taxonomy is proposed, classifying existing methods into three categories based on the model learning scenario: (1) graph OOD generalization, where the generalizability is enhanced through strategic design of the model when training on source data, (2) training-time graph OOD adaptation, where the adaptation is performed when jointly training the model based on both source and target data [16, 17], and (3) test-time graph OOD adaptation, where the adaptation happens when adjusting a pre-trained source model to the target data [18, 19]. To deepen our understanding of these approaches, we further classify existing methods within each of the three categories into model-centric and data-centric strategies. Model-centric approaches focus on the learning process or the architecture of the graph model itself, enhancing the model’s inherent ability to generalize or adapt to distribution shifts by refining its structure, training objectives, or learning mechanisms. In contrast, data-centric approaches emphasize the manipulation of input graphs, improving model performance by addressing the data directly, either through preprocessing techniques or data augmentation strategies. Within each subline of research, we elaborate on the detailed techniques for enhancing the generalizability or adaptability under distribution shifts on graphs. Additionally, we provide a summary of the datasets utilized in these studies, highlighting their characteristics and relevance to the challenges posed by distribution shifts. Based on the current progress on graph OOD learning, at the end we also point out several promising research directions in this evolving field. Differences between this survey and existing ones. Despite the urgent need for an overview of graph learning under distribution shifts, existing surveys have primarily focused on subfields within this area, rather than providing a comprehensive overview from multiple scenarios. Until now, there have been several surveys in related areas, but exhibiting distinct focuses, including graph OOD generalization [7, 20], graph domain adaptation [21, 22], trustworthy graph learning [23] related to distribution shifts. Our work distinguishes itself from existing ones in the following aspects: (1) Main focus. Our survey centers on the challenges and solutions for graph learning under distribution shifts, while [23] analyzes OOD issues from a trustworthy perspective and does not delve into the methodological aspects. Conversely, [20] examines graph machine learning from a causal perspective, which is narrower than our broad examination. (2) Taxonomy. We provide a comprehensive categorization of existing methods and summarize them, whereas related work, such as [24], lacks such summaries. Other surveys like [21] and [22] primarily focus on domain adaptation without addressing the broader scope of graph OOD learning. Additionally, we provide coverage of the most recent advancements and discussions in this field. Survey structure. The general organization of this survey is presented as follows: Section 2 introduces the notations and preliminaries. Sections 3, 4 and 5 review graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation, respectively. Each section discusses model-centric and data-centric approaches within its scenario, further detailing the techniques associated with each category. Furthermore, Section 6 provides a comprehensive summary of the datasets used in the literature, highlighting popular graph datasets for evaluation and their relevance to the challenges posed by distribution shifts. Section 7 explores promising future research directions and the associated challenges. Finally, Section 8 presents the conclusion of this survey."
Spatioformer: A Geo-encoded Transformer for Large-Scale Plant Species Richness Prediction,"Earth observation data have shown promise in predicting species richness of vascular plants (α𝛼\alphaitalic_α-diversity), but extending this approach to large spatial scales is challenging because geographically distant regions may exhibit different compositions of plant species (β𝛽\betaitalic_β-diversity), resulting in a location-dependent relationship between richness and spectral measurements. In order to handle such geolocation dependency, we propose Spatioformer, where a novel geolocation encoder is coupled with the transformer model to encode geolocation context into remote sensing imagery. The Spatioformer model compares favourably to state-of-the-art models in richness predictions on a large-scale ground-truth richness dataset (HAVPlot) that consists of 68,170 in-situ richness samples covering diverse landscapes across Australia. The results demonstrate that geolocational information is advantageous in predicting species richness from satellite observations over large spatial scales. With Spatioformer, plant species richness maps over Australia are compiled from Landsat archive for the years from 2015 to 2023. The richness maps produced in this study reveal the spatiotemporal dynamics of plant species richness in Australia, providing supporting evidence to inform effective planning and policy development for plant diversity conservation. Regions of high richness prediction uncertainties are identified, highlighting the need for future in-situ surveys to be conducted in these areas to enhance the prediction accuracy.","Australia is home to a large and diverse range of plant species, with over 21,000 known native species of vascular plants and 93% of these being endemic [1, 2]. The richness of plant species, also known as α𝛼\alphaitalic_α-diversity, is highly important in maintaining the functioning of ecosystems, such as habitat provision, carbon sequestration, and water cycling [3, 4, 5, 6]. However, anthropogenic interference, such as deforestation, overgrazing, and urbanisation, has resulted in a decline in plant species richness [7, 8]. In response, conservation activities have been initialised and conducted across the country aiming to preserve plant diversity [9, 10, 11]. Accurate and up-to-date maps of plant species richness will strongly support effective planning and policy-making for these activities [12, 13]. Earth observation (EO) data provide rapid and near-real-time estimates of changes in land surface conditions across large regions [14, 15, 16, 17]. This makes remote sensing imagery a favourable data source for plant species richness modelling, as compared with another widely adopted approach where environmental variables, such as temperature, precipitation, soil texture, and topographic heterogeneity, are used as richness predictors [18]. The reason is that environmental variables drive mainly the environmental potential of plant habitats (i.e., the capacity to sustain a certain level of richness), rather than represent the actual conditions on the ground like those observed by remote sensing satellites. For example, deforestation, floods, and bushfires could cause reduction in richness [19], but such reduction might not be reflected by environmental variables. Therefore, environmental variables are often aimed at predicting the natural patterns in diversity in a pre-intensification reference state, while remote sensing data are more valuable for monitoring actual changes in those patterns. Australia covers an area of over seven million square kilometres. As a result of the relatively large geographical extents, versatile types of plant habitats are found across the country, differing in their inventories of plant species present that have been shaped by a variety of factors such as biogeographic history, climate, and geography [20]. To understand the spatiotemporal distribution of plant species richness, perseverant in-situ field surveys have been conducted over the past several decades. Via various survey campaigns, a wealth of 219,552 richness samples have been gathered across the country as of the year 2022 [18]. These samples represent a broad range of landscapes across the continent, and therefore present a unique opportunity to unravel the potentially intricate relationship between richness measurements and satellite observations. Nevertheless, geographically distant regions may exhibit distinct assemblages of plant species with differed compositional properties (i.e., β𝛽\betaitalic_β-diversity [21]), making it challenging to model richness over large spatial scales. Due to spatial variations in plant species composition, a location with a set of plant species would be expected to display quite different spectral features in remote sensing imagery from another location with a dissimilar plant composition, even if the two locations sustain the same richness of species [22, 23]. Through statistical regression analysis for two regions in southeast Australia, previous studies [13, 24] suggested that the relationship between plant species richness and hyper/multispectral satellite observations is region-specific. To account for the location dependency, we need a model that is capable of taking in geolocation context when mapping plant species richness over large spatial scales. The transformer model, first introduced in [25], is built upon the self-attention mechanism [26]. The model attends effectively to information of high importance in the input data, as the self-attention module is capable of capturing intricate data structures and dependencies [26, 27]. Initially proposed for language tasks, the transformer model has shown promise for image understanding due to its superior ability over Convolutional Neural Networks (CNNs) in capturing global dependencies between different regions of an image [28]. As a seminal work on applying transformer to image data, the Vision Transformer (ViT) model [28] first divides an image into non-overlapping patches, followed by projecting each patch into a feature vector which is then fed into the self-attention module, with state-of-the-art performance being achieved on benchmark datasets. Given remote sensing imagery captures rich features in the spectral dimension, the SpectralFormer model [29] was developed to effectively embed the spectral information. This model was later extended in [30], where FactoFormer, a factorised transformer, was introduced for the joint learning of spectral and spatial features. These studies have demonstrated the effectiveness of transformer in processing remote sensing images (e.g., [30, 29]), but to advance the model’s application to remote sensing images recorded over large spatial scales, geolocational information could be leveraged. Unlike many types of imagery whose semantics are independent of the location where they are recorded, remote sensing images are intrinsically associated with geolocations [31, 32]. Considering that the composition of plant species is location-specific, incorporating geolocation context could be helpful in modelling the location-dependent relationship between richness and remote sensing imagery. Geo-coordinates provide geographical priors that supplement geolocation context to the image data [33, 32, 34, 31, 35, 36, 37]. While a straightforward way to utilise geolocational features is to concatenate the original longitude and latitude coordinates into the model, this approach has shown to yield almost no gain in performance [34]. To deal with this problem, a geo-feature extraction approach was proposed in [34] for CNN models, where the geo-coordinates were projected into a higher dimensional feature space with a geolocation encoder, whose outputs were then merged into those of a CNN-based image network. It was observed that, by leveraging geolocation context, a 7% increase in accuracy was achieved for an image data set spanning over the continental United States [34]. This geo-encoded CNN model was later applied to global-scale vegetation canopy height mapping with satellite imagery [38], where the geolocations served as a prior. Other state-of-the-art geolocation encoders include Space2Vec [39], Sphere2Vec [40], PE-GNN [41], and a more recent algorithm that is based on the spherical harmonic basis functions [42]. Multi-scale sinusoidal functions are favoured in building these encoders (e.g., [39, 42]), thanks to their merits of being bounded in value, infinitely extended in space, and possessing a multi-resolution scalability. Geolocation encoding is demonstrated to be effective in many large-scale geospatial problems, such as animal species categorisation [38, 40], water quality prediction [43], event/activity recognition [44], and remote sensing scene classification [31, 40]. In this study, we aim to predict the spatiotemporal distribution of plant species richness in Australia from EO imagery with geolocation context being taken into account. Considering that the relationship between plant species richness and remote sensing imagery varies from one location to another due to differences in vegetation composition, we propose Spatioformer, where a novel geolocation encoder is coupled with the transformer model in order to incorporate the geolocation context. The performance of Spatioformer in richness mapping is compared with a CNN model, a ViT model, and the FactoFormer model where the geolocational information is not encoded. Through quantitative analyses, we seek to address primarily the following important questions: (1) Does Spatioformer perform better than state-of-the-art algorithms in predicting plant species richness over large spatial scales? (2) What are the spatial patterns of plant species richness in Australia inferred from remote sensing evidence? (3) Where future in-situ surveys should be conducted as suggested by the mapping results? The rest of the paper is organised as follows. Section II describes the study area and the datasets used for modelling, including the ground-truth samples of plant species richness and satellite imagery. Section III introduces the methods with a focus on the proposed Spatioformer model. This model is developed with the aim to capture the location-dependent relationships between plant species richness and remote sensing imagery over large spatial scales. Section IV describes the experimental settings for training and validation of the Spatioformer model. Section V presents the results of applying Spatioformer to plant richness mapping across Australia, and discusses the implications of these findings for biodiversity conservation and future research directions. Finally, Section VI concludes the paper."
CHESTNUT: A QoS Dataset for Mobile Edge Environments,"Quality of Service (QoS) is an important metric to measure the performance of network services. Nowadays, it is widely used in mobile edge environments to evaluate the quality of service when mobile devices request services from edge servers. QoS usually involves multiple dimensions, such as bandwidth, latency, jitter, and data packet loss rate. However, most existing QoS datasets, such as the common WS-Dream dataset, focus mainly on static QoS metrics of network services and ignore dynamic attributes such as time and geographic location. This means they should have detailed the mobile device’s location at the time of the service request or the chronological order in which the request was made. However, these dynamic attributes are crucial for understanding and predicting the actual performance of network services, as QoS performance typically fluctuates with time and geographic location. To this end, we propose a novel dataset that accurately records temporal and geographic location information on quality of service during the collection process, aiming to provide more accurate and reliable data to support future QoS prediction in mobile edge environments.","I original dataset description To create a high-quality dataset for predicting Quality of Service (QoS) in mobile edge environments, this study utilized two real-world datasets from Shanghai. One dataset is from the Shanghai Johnson Taxi, containing information such as the longitude, latitude, moving direction, and speed of the taxis on a specific day, which was used to simulate user mobile datasets. The other dataset is from Shanghai Telecom [1, 2, 3], providing the longitude and latitude of the base stations. Data from June 2014 was used to simulate the generation of edge server datasets. I-A Shanghai Johnson Taxi Dataset The Shanghai johnson taxi dataset is an important resource for traffic research, containing real-time GPS and business status information from taxis in Shanghai. Each record in the dataset includes various fields: the vehicle ID, control word (where A indicates normal and M indicates alarm), business status (0 for normal and 1 for alarm), passenger status (0 for occupied and 1 for unoccupied), top light status (with values ranging from 0 for operation to 5 for out of service), road type (0 for ground road and 1 for express road), brake status (0 for no braking and 1 for braking), meaningless fields, reception date, GPS timestamp, longitude, latitude, speed, direction, the number of satellites, and additional meaningless fields. This dataset enables the analysis of urban traffic flow, travel patterns, and traffic management strategies. In this paper, we use only the gps time, latitude, longitude, speed, and direction of the cab to generate motion information about the mobile user. I-B Shanghai Telecom Dataset This study utilized a telecommunications dataset provided by Shanghai Telecom, comprising over 7.2 million records of 9,481 mobile phones accessing the Internet through 3,233 base stations over six months.111http://sguangwang.com/TelecomDataset.html The dataset includes six parameters: month, date, start time, end time, base station location (latitude and longitude), and user ID used within Shanghai Telecom. This dataset can be used to evaluate solutions in mobile edge computing, such as edge server deployment, service migration, and service recommendation. In our research, we need to simulate the information of edge servers on base stations based on this dataset. Furthermore, considering the substantial volume of data, we only counted the data for one month and only focused on the geographic location information of Shanghai base stations."
Enhancing Exchange Rate Forecasting with Explainable Deep Learning Models,"Accurate exchange rate prediction is fundamental to financial stability and international trade, positioning it as a critical focus in economic and financial research. Traditional forecasting models often falter when addressing the inherent complexities and non-linearities of exchange rate data. This study explores the application of advanced deep learning models, including LSTM, CNN, and transformer-based architectures, to enhance the predictive accuracy of the RMB/USD exchange rate. Utilizing 40 features across 6 categories, the analysis identifies TSMixer as the most effective model for this task. A rigorous feature selection process emphasizes the inclusion of key economic indicators, such as China-U.S. trade volumes and exchange rates of other major currencies like the euro-RMB and yen-dollar pairs. The integration of grad-CAM visualization techniques further enhances model interpretability, allowing for clearer identification of the most influential features and bolstering the credibility of the predictions. These findings underscore the pivotal role of fundamental economic data in exchange rate forecasting and highlight the substantial potential of machine learning models to deliver more accurate and reliable predictions, thereby serving as a valuable tool for financial analysis and decision-making.","Since the dissolution of the Bretton Woods system, the adoption of a floating exchange rate regime has introduced significant challenges in risk management for market participants. The volatility of the RMB/USD exchange rate, particularly during periods of trade tensions, has heightened the uncertainty faced by those engaged in the foreign exchange market. The People’s Bank of China’s reform of the exchange rate fixing mechanism on August 11, 2015, further increased the marketization of the RMB exchange rate, leading to greater exchange rate volatility and increased foreign exchange risk. This has underscored the critical need for accurate exchange rate forecasting and effective risk management strategies. China’s growing role in the global supply chain, especially after its accession to the World Trade Organization (WTO), and the deepening economic ties between China and the United States, have made the RMB/USD exchange rate a focal point of global economic stability. Debates over the valuation of the RMB, particularly during periods of significant trade surpluses with the U.S., have led to multiple rounds of discussions on exchange rate policy. The RMB exchange rate reform in 2005 and the subsequent rounds of monetary policy adjustments by the Federal Reserve, especially during the 2007 financial crisis, further complicated the dynamics of the RMB/USD exchange rate. The “8.11 Exchange Rate Reform” in 2015, which introduced a more flexible exchange rate mechanism, and the intensified trade frictions since 2018, have put additional depreciation pressure on the RMB, making accurate forecasting of the RMB/USD exchange rate increasingly important [1, 2, 3, 4, 5]. Previous research on exchange rate forecasting has primarily focused on theoretical and quantitative models. Theoretical models often emphasize the equilibrium state of exchange rates, which can be difficult to achieve or maintain in practice, making short- to medium-term predictions particularly challenging. Quantitative models focus on the exchange rate’s own dynamics while often neglecting other critical influencing factors. Moreover, these models have struggled to produce consistent results across different studies. In recent years, there has been a notable shift towards using big data approaches in forecasting models, bypassing the need for complex mathematical modeling and allowing for more flexible model forms without predefined structures. The inherent complexity and non-linearity of exchange rate data have led to applying non-linear methods, such as chaos theory, non-parametric methods, and machine learning techniques, which have shown potential to improve forecasting accuracy. Studies like those of LeBaron and others have demonstrated that methods such as kernel ridge regression can significantly enhance the prediction of financial volatility, although some researchers, such as Mourer, have found that these methods do not always outperform simple autoregressive models in all contexts. Since the end of the Bretton Woods system, the transition to a floating exchange rate regime has posed significant challenges for managing risk, especially regarding the RMB/USD exchange rate. China’s integration into the global economy post-WTO accession, along with its deepening economic ties with the U.S., has made this exchange rate crucial for global economic stability. The 2015 reform of China’s exchange rate mechanism increased market-driven fluctuations, further intensified by trade tensions, which has heightened volatility. This evolving landscape has led to debates over the RMB’s valuation and spurred numerous policy discussions. Consequently, precise and reliable exchange rate forecasting has become essential for effective risk management in this volatile environment. Traditional exchange rate forecasting models, both theoretical and quantitative, have struggled with consistency and often neglect critical influencing factors. For example, theoretical models may fail to account for the real-time impact of policy changes and global economic shifts, while quantitative models may not fully capture the non-linear dynamics and intricate interactions of the variables involved. Recently, there has been a shift toward big data and machine learning approaches, which offer flexibility and improved accuracy in handling the complex, non-linear nature of exchange rate data [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]. While some methods like kernel ridge regression have shown promise, their performance varies across different contexts, highlighting the ongoing challenges in exchange rate prediction. Machine learning models, particularly deep learning models, have increasingly been applied to predicting time series and economic variables [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]. Despite their advantages in handling complex, non-linear data without requiring explicit assumptions about the underlying data distribution, these models are often criticized for their “black box” nature and lack of interpretability. Recent advancements, such as the application of Grad-CAM and attention mechanisms, have begun to address these issues, making it possible to visualize model predictions and understand the underlying decision-making processes. In related financial applications, researchers have combined advanced models like XGBoost with data balancing techniques including SMOTE to enhance predictive performance, as demonstrated by Chang et al. in their fraud detection study [60]. Their work not only showcased the effectiveness of this approach in fraud detection but also laid a foundation for developing robust models in various financial domains, including potentially exchange rate prediction [60]. However, applying these interpretability techniques has been mostly limited to fields like image recognition and natural language processing, with relatively few studies applying them to economic forecasting. Given the challenges of traditional models and the potential of machine learning approaches, this study seeks to explore the use of advanced deep learning models, including CNNs, RNNs, MLPs, and transformer-based architectures, for predicting the RMB/USD exchange rate. By incorporating a comprehensive set of features—drawn from economic indicators, trade data, and other currency pairs—and employing advanced feature selection techniques, this research aims to enhance predictive accuracy, identify the most relevant factors influencing exchange rate fluctuations, and enhance the interpretability of the model predictions. Contributions of This Study • Application of Deep Learning Models: This study provides an initial analysis of the effectiveness of deep learning models in exchange rate prediction, using MSE and MAE as key metrics to identify the best-performing models. • Enhancement of Predictive Performance: To improve the accuracy of machine learning models, this study employs various techniques, including feature selection, to reduce redundancy and retain the most relevant subset of features for exchange rate forecasting. • Analysis of Influential Factors Over Time: By applying attention mechanisms, this study enhances the interpretability of machine learning models, offering insights into how different factors influence exchange rate predictions across different periods. This analysis aims to uncover which aspects of economic data the models prioritize during the prediction process, thereby providing a more nuanced understanding of the underlying dynamics."
SHAP zero Explains All-order Feature Interactions in Black-box Genomic Models with Near-zero Query Cost,"With the rapid growth of black-box models in machine learning, Shapley values have emerged as a popular method for model explanations due to their theoretical guarantees. Shapley values locally explain a model to an input query using additive features. Yet, in genomics, extracting biological knowledge from black-box models hinges on explaining nonlinear feature interactions globally to hundreds to thousands of input query sequences. Herein, we develop SHAP zero, an algorithm that estimates all-order Shapley feature interactions with a near-zero cost per queried sequence after paying a one-time fee for model sketching. SHAP zero achieves this by establishing a surprisingly underexplored connection between the Shapley interactions and the Fourier transform of the model. Explaining two genomic models, one trained to predict guide RNA binding and the other to predict DNA repair outcomes, we demonstrate that SHAP zero achieves orders of magnitude reduction in amortized computational cost compared to state-of-the-art algorithms. SHAP zero reveals all microhomologous motifs that are predictive of DNA repair outcome, a finding previously inaccessible due to the combinatorial space of possible high-order feature interactions.","Shapley values have emerged as a theoretically robust method for explaining the local additive features of an input query to black-box models [1, 2, 3]. However, extracting biological knowledge from the emerging models in genomics demands a more global understanding, which requires explaining the nonlinear interactions among features and doing so for hundreds to thousands of input query sequences. Shapley value explanations have a high computational cost, with an exact calculation requiring an exponential number of model evaluations in the input dimension [4]. The cost is higher for nonlinear feature interactions and grows exponentially with a polynomial function of the input dimension. With the increasing growth of complex and large-scale models in genomics [5], often with only proprietary access, there is an urgent need for algorithms that can explain the nonlinear high-order interactions in these black-box models at scale–not just in a handful of sequences but among a host of query sequences. Consider the problem of explaining a black-box model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) that takes in a length-n𝑛nitalic_n DNA sequence 𝐱∈ℤ4n={A,T,C,G}n𝐱superscriptsubscriptℤ4𝑛superscript𝐴𝑇𝐶𝐺𝑛\mathbf{x}\in\mathbb{Z}_{4}^{n}=\{A,T,C,G\}^{n}bold_x ∈ blackboard_Z start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT = { italic_A , italic_T , italic_C , italic_G } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and outputs a real number. One popular method of explaining the prediction of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to the input query sequence 𝐱𝐱\mathbf{x}bold_x is using SHapley Additive exPlanations (SHAP) [6]. SHAP explains the output of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) by assigning a so-called SHAP value IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) to the ithsuperscript𝑖thi^{\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT nucleotide in 𝐱𝐱\mathbf{x}bold_x, where i𝑖iitalic_i belongs to the feature set D={1,2,…,n}𝐷12…𝑛D=\{1,2,\dots,n\}italic_D = { 1 , 2 , … , italic_n } (Fig. 1a): IS⁢V⁢(i)=∑T⊆D\{i}|T|!⁢(|D|−|T|−1)!|D|!⁢[vT∪{i}⁢(𝐱T∪{i})−vT⁢(𝐱T)].superscript𝐼𝑆𝑉𝑖subscript𝑇\𝐷𝑖𝑇𝐷𝑇1𝐷delimited-[]subscript𝑣𝑇𝑖subscript𝐱𝑇𝑖subscript𝑣𝑇subscript𝐱𝑇I^{SV}(i)=\sum_{T\subseteq D\backslash\{i\}}\frac{|T|!\,(|D|-|T|-1)!}{|D|!}% \left[v_{T\cup\{i\}}(\mathbf{x}_{T\cup\{i\}})-v_{T}(\mathbf{x}_{T})\right].italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) = ∑ start_POSTSUBSCRIPT italic_T ⊆ italic_D \ { italic_i } end_POSTSUBSCRIPT divide start_ARG | italic_T | ! ( | italic_D | - | italic_T | - 1 ) ! end_ARG start_ARG | italic_D | ! end_ARG [ italic_v start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT ) - italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ] . (1) IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) computes the marginal contribution of the ithsuperscript𝑖thi^{\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT nucleotide to the value function vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) for all the subsets T𝑇Titalic_T of features in D\{i}\𝐷𝑖D\backslash\{i\}italic_D \ { italic_i }, where the value function is the output of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to 𝐱Tsubscript𝐱𝑇\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT computed by marginalizing the absent nucleotides in D\T\𝐷𝑇D\backslash Titalic_D \ italic_T (Fig. 1a). SHAP requires computing vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) for every subset of features in T⊆D𝑇𝐷T\subseteq Ditalic_T ⊆ italic_D, which means evaluating f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) using a number of samples that grows exponentially with n𝑛nitalic_n (hereafter called sample complexity) and paying a computational cost that also grows exponentially with n𝑛nitalic_n (hereafter called computational complexity). Faithful Shapley Interaction index IF⁢S⁢I⁢(T)superscript𝐼𝐹𝑆𝐼𝑇I^{FSI}(T)italic_I start_POSTSUPERSCRIPT italic_F italic_S italic_I end_POSTSUPERSCRIPT ( italic_T ) (Faith-Shap) generalizes SHAP values defined for a single nucleotide i𝑖iitalic_i to an interaction defined over a set T⊆D𝑇𝐷T\subseteq Ditalic_T ⊆ italic_D (Equation (34)) [7]. Computing Faith-Shap requires evaluating the model for all possible subsets of features in D𝐷Ditalic_D, and then marginalizing over their power set with an exact computational cost that grows exponentially with a polynomial function of sequence length p⁢o⁢l⁢y⁢(n)𝑝𝑜𝑙𝑦𝑛poly(n)italic_p italic_o italic_l italic_y ( italic_n ). Figure 1: Schematic of the flowchart for the exact computation of SHapley Additive exPlanations (SHAP) and our approximation algorithm SHAP zero. a, Computing SHAP values exactly requires exponential model evaluations with sequence length n𝑛nitalic_n, per query sequence. This illustration shows computing one such term in IS⁢V⁢(i=1)superscript𝐼𝑆𝑉𝑖1I^{SV}(i=1)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i = 1 ), which measures the marginal contribution of the nucleotide T at site i=1𝑖1i=1italic_i = 1 to the set T={3,4,5}𝑇345T=\{3,4,5\}italic_T = { 3 , 4 , 5 } by evaluating the model for all four possible nucleotides at D\(T∪{i})={2}\𝐷𝑇𝑖2D\backslash(T\cup\{i\})=\{2\}italic_D \ ( italic_T ∪ { italic_i } ) = { 2 } in the sequence 𝐱T∪{i}subscript𝐱𝑇𝑖{\bf x}_{T\cup\{i\}}bold_x start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT and all 42=16superscript42164^{2}=164 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 16 possible nucleotide combinations at D\T={1,2}\𝐷𝑇12D\backslash T=\{1,2\}italic_D \ italic_T = { 1 , 2 } in the sequence 𝐱Tsubscript𝐱𝑇{\bf x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. b, In contrast, SHAP zero cleverly samples the sequence space and resolves a sparse bipartite graph to approximate the original model with its top-s𝑠sitalic_s Fourier coefficients. c, For each query sequence 𝐱𝐱{\bf x}bold_x, SHAP zero maps the top-s𝑠sitalic_s Fourier coefficients into the Möbius transform M⁢[𝐤]𝑀delimited-[]𝐤M[{\bf k}]italic_M [ bold_k ], where k is the feature interaction vector. The illustration shows one such sequence’s Möbius transform, composed of all the permutations of ℓ=3ℓ3\ell=3roman_ℓ = 3 nucleotides from the original sequence. d, SHAP zero computes IS⁢V⁢(i=1)superscript𝐼𝑆𝑉𝑖1I^{SV}(i=1)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i = 1 ) using a weighted sum of the Möbius coefficients with k1>0subscript𝑘10k_{1}>0italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0 (Equation (2)). SHAP zero amortizes the cost of finding the Fourier transform over the future explanations. The algorithms to approximately compute SHAP values are either stochastic estimators [6, 8, 9, 10, 11, 12] or model-based approximators [6, 13, 14, 15, 16, 17, 18]. Stochastic estimators, such as KernelSHAP [6], randomly subsample the feature subsets (T𝑇Titalic_T,vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )) and approximately solve a weighted-least squares problem to estimate IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ). These algorithms require many model evaluations in practice and impose an undesirable trade-off between sample complexity and accuracy [19]. Model-based approximators, such as DeepSHAP [6], take advantage of the model architecture (e.g., neural networks) to estimate Shapley values. These methods are often faster but still require many model evaluations and only work for white-box models. Algorithms to compute Shapley interactions in black-box models [20, 7, 21, 22], such as SHAP-IQ [20], subsample high-order interactions for efficiency but similar to stochastic estimators they need many model evaluations. To empirically demonstrate how current SHAP algorithms scale, let us consider TIGER [23], a recent model that predicts the efficiency of CRISPR-Cas13d guide RNA from their sequence and context. To explain which region in the guide RNA is the most determinant of efficiency, we estimated SHAP values of 1038103810381038 input query sequences. Finding the KernelSHAP [6] values of all the nucleotides in these sequences took about one day on our single NVIDIA RTX A6000 machine. Worse yet, finding only up to 3rdsuperscript3rd3^{\text{rd}}3 start_POSTSUPERSCRIPT rd end_POSTSUPERSCRIPT order feature interactions using SHAP-IQ took more than 81 days—revealing a severe scalability issue in current explainability algorithms. We posit that the model evaluations needed to estimate the SHAP values of an input query sequence have information that can be used to estimate the SHAP values for a new query sequence. Therefore, instead of independently evaluating the model for each query sequence, one can “recycle” the model evaluations to slash the sample and computational cost of explaining the model. Taking this idea to the extreme, we propose to do initial query-agnostic model evaluations to sketch the model and then use the sketch for model explanation. How can a model be sketched to be efficiently mapped to SHAP values and Shapley interactions? We discover a surprisingly underexplored connection between SHAP values and interactions and the model’s Fourier transform, enabling us to sketch the model and use the sketch for fast Shapley explanations. The Fourier transform provides a global sketching of the model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) irrespective of the query sequence or even the training distribution. Moreover, since Fourier is an orthonormal basis, it enables fast and sample-efficient algorithms for sketching black-box models in genomics that are compressible or near-sparse in the Fourier domain [24, 25, 26, 27]. Herein, we develop SHAP zero, an algorithm that estimates SHAP values and Shapley interactions with a near-zero additional cost per new query sequence after paying an initial up-front cost for model sketching (Fig. 1b). In developing SHAP zero, we make three distinct and interconnected contributions: First, we build on the existing algorithms in sparse Fourier transforms [27, 28] and develop a method to sketch a black-box model f⁢(𝐱)𝑓𝐱f({\bf x})italic_f ( bold_x ) with sequence input (defined over q𝑞qitalic_q alphabets) and real-valued output, in terms of its top-s𝑠sitalic_s Fourier coefficients with a sample complexity of 𝒪⁢(s⁢n2)𝒪𝑠superscript𝑛2\mathcal{O}(sn^{2})caligraphic_O ( italic_s italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and a computational complexity of 𝒪⁢(s⁢n3)𝒪𝑠superscript𝑛3\mathcal{O}(sn^{3})caligraphic_O ( italic_s italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). Second, we establish a mathematical formalism to map from (i) the Fourier transform to a transform from algebraic geometry called the Möbius transform and (ii) from the Möbius transform to Shapley-based explanations. The Möbius transform [29, 30] enables us to map the top-s𝑠sitalic_s Fourier coefficients of the model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to Shapley-based explanations in 𝒪⁢(s2⁢(2⁢q)ℓ)𝒪superscript𝑠2superscript2𝑞ℓ\mathcal{O}(s^{2}(2q)^{\ell})caligraphic_O ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 2 italic_q ) start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) time per query sequence, where ℓ≤5ℓ5\ell\leq 5roman_ℓ ≤ 5 caps the maximum order of model interactions in practice [31]. Third, we conduct large-scale experiments to explain two genomic models, TIGER [23] and inDelphi [32], with SHAP zero. We demonstrate that SHAP zero estimates all-order feature interactions with an amortized computational cost up to 1000-fold faster than current algorithms. SHAP zero reveals the GC content of the seed region in TIGER and microhomologous motifs in inDelphi as predictive high-order features, a task previously inaccessible due to the combinatorial space of possible feature interactions."
Sparse Decomposition of Graph Neural Networks,"Graph Neural Networks (GNN) exhibit superior performance in graph representation learning, but their inference cost can be high, due to an aggregation operation that can require a memory fetch for a very large number of nodes. This inference cost is the major obstacle to deploying GNN models with online prediction to reflect the potentially dynamic node features. To address this, we propose an approach to reduce the number of nodes that are included during aggregation. We achieve this through a sparse decomposition, learning to approximate node representations using a weighted sum of linearly transformed features of a carefully selected subset of nodes within the extended neighbourhood. The approach achieves linear complexity with respect to the average node degree and the number of layers in the graph neural network. We introduce an algorithm to compute the optimal parameters for the sparse decomposition, ensuring an accurate approximation of the original GNN model, and present effective strategies to reduce the training time and improve the learning process. We demonstrate via extensive experiments that our method outperforms other baselines designed for inference speedup, achieving significant accuracy gains with comparable inference times for both node classification and spatio-temporal forecasting tasks.","Figure 1: The pipeline overview for SDGNN framework (bottom pipeline). To compute GNN embedding efficiently, we use a transformation function to adapt node features and introduce sparse vectors associated with each node to gather information from critical neighbours. The parameters in the transformation function and the sparse vectors are determined by optimization to approximate the target GNN embeddings. Graph neural networks (GNN) have demonstrated impressive performance for graph representation learning (Hamilton et al., 2017; Veličković et al., 2018; Qu et al., 2019; Rampášek et al., 2022). Although there are numerous designs for GNN models, the essential idea is to represent each node based on its features and its neighbourhood (Wu et al., 2020; Zhou et al., 2020). The procedure of aggregating features from neighbour nodes is empirically and theoretically effective (Xu et al., 2019) in representing the graph structures and blending the features of the nodes. However, deploying GNN models to process large graphs is challenging since collecting information from the neighbour nodes and computing the aggregation is extremely time-consuming (Zhang et al., 2021; Tian et al., 2023; Wu et al., 2023; Liu et al., 2024). In this work, we tackle the efficient inference problem for GNN models in the online prediction setting (Crankshaw, 2019). Specifically, we need to compute the representations of a few arbitrary nodes. The main advantage is that the prediction can reflect potential dynamic features111The features of a sample may vary over time, e.g., dynamical features from sensors (Dawson et al., 2016), dynamic features in the recommendation systems (Chu & Park, 2009). of the input. The computational complexity is dominated by the number of receptive nodes, which rapidly increases as the number of layers in the model grows, for most message-passing-based and graph-transformer-based GNNs (Zeng et al., 2020; Min et al., 2022). Our goal is to reduce the inference time to linear complexity with respect to the number of layers and the average node degree. Recently, several studies have attempted to address this problem by combining the performance of GNN and the efficiency of MLPs (Zhang et al., 2021; Hu et al., 2021; Tian et al., 2023; Wang et al., 2023; Wu et al., 2023; Liu et al., 2024; Tian et al., 2024; Winter et al., 2024; Wu et al., 2024). Knowledge distillation (Hinton et al., 2015) and feature/label smoothing are used to construct effective MLP models to eliminate the cumbersome neighbour collection and aggregation procedure. Although efficient, these methods have a fundamental limitation: the features gathered at each node are assumed to contain sufficient information to predict the node label accurately. However, to achieve their full potential, especially when features can change at inference time, GNN models should take into account the features from neighbourhood nodes and the graph structure (Battaglia et al., 2018; Pei et al., 2020). Therefore, we ask the question: given any graph neural network model that relies on both the graph structure and the features of the neighbourhood, can we infer the representation of a node in linear time? Present work. We propose sparse decomposition for graph neural networks (SDGNN), an approximation to the original GNN models that can infer node representations efficiently and effectively. The SDGNN consists of a feature transformation function and sparse weight vectors for nodes in the graph. The representation of each node is then a weighted sum of the transformed features from a small set of receptive nodes. The sparsity of the weight vectors guarantees low inference complexity. The learnable feature transformation function and the sparse weight vectors grant the SDGNN flexibility to approximate a wide range of targeted GNN models. To find the optimal parameters in SDGNN, we formulate the approximation task as an optimization problem and propose a scalable and efficient solution that iterates between the learning of the transformation function and the optimization of the sparse weight vectors. We verify the approximation power of SDGNN and the scalability of our algorithm on seven node classification datasets and demonstrate how SDGNN can be effectively applied under the online prediction setting with two spatio-temporal forecasting datasets. SDGNN consistently outperforms recent state-of-the-art models designed for GNN inference speedup."
Adversarial Environment Design via Regret-Guided Diffusion Models,"Training agents that are robust to environmental changes remains a significant challenge in deep reinforcement learning (RL). Unsupervised environment design (UED) has recently emerged to address this issue by generating a set of training environments tailored to the agent’s capabilities. While prior works demonstrate that UED has the potential to learn a robust policy, their performance is constrained by the capabilities of the environment generation. To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). The proposed method guides the diffusion-based environment generator with the regret of the agent to produce environments that the agent finds challenging but conducive to further improvement. By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy. Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments. Project page: https://github.com/rllab-snu.github.io/projects/ADD","Deep reinforcement learning (RL) has achieved great success in various challenging domains, such as Atari [1], GO [2], and real-world robotics tasks [3, 4]. Despite the progress, the deep RL agent struggles with the generalization problem; it often fails in unseen environments even with a small difference from the training environment distribution [5, 6]. To train well-generalizing policies, various prior works have used domain randomization (DR) [7, 8, 9], which provides RL agents with randomly generated environments. While DR enhances the diversity of the training environments, it requires a large number of trials to generate meaningful structures in high-dimensional domains. Curriculum reinforcement learning [10, 11] has been demonstrated to address these issues by providing instructive sequences of environments. Since manually designing an effective curriculum for complicated tasks is challenging, prior works [12, 13] focus on generating curricula that consider the current agent’s capabilities. Recently, unsupervised environment design (UED, [14]) has emerged as a scalable approach, notable for its advantage of requiring no prior knowledge. UED algorithms alternate between training the policy and designing training environments that maximize the regret of the agent. This closed-loop framework ensures the agent learns a minimax regret policy [15], assuming that the two-player game between the agent and the environment generator reaches the Nash equilibrium. There are two main approaches for UED: 1) learning-based methods, which employ an environment generator trained via reinforcement learning, and 2) replay-based methods, which selectively replay among previously generated environments. The learning-based methods [14, 16, 17] utilize an adaptive generator that controls the parameters that fully define the environment configuration. The generator receives a regret of the agent as a reward and is trained via reinforcement learning to produce environments that maximize the regret. While the learning-based methods can directly generate meaningful environments, training the generator with RL is unstable due to the moving manifold [16]. Additionally, we observe that the RL-based generator has limited environment coverage, which limits the generalization capability of the trained agent. In contrast, the replay-based methods [18, 19, 20] employ a random generator and select environments to revisit among previously generated environments. Since the random generator can produce diverse environments without additional training, they outperform the learning-based methods in zero-shot generalization tasks [20]. However, the replay-based methods are sample inefficient as they require additional episodes to evaluate the regret on the randomly generated environments. In this work, we propose a sample-efficient and robust UED algorithm by leveraging the strong representation power of diffusion models [21]. First, to make UED suitable for using a diffusion model as a generator, we introduce soft UED, which augments the regret objective of UED with an entropy regularization term, as done in maximum entropy RL [22]. By incorporating the entropy term, we can ensure the diversity of the generated environments. Then, we present adversarial environment design via regret-guided diffusion models (ADD), which guides a diffusion-based environment generator with the regret of the agent to produce environments that are conducive to the performance improvement of the agent. Enabling this regret guidance requires the gradient of the regret with respect to the environment parameter. However, since the true value of the regret is intractable and the regret estimation methods used in prior works on UED are not differentiable, a new form of regret estimation method is needed. To this end, we propose a novel method that enables the estimation of the regret in a differentiable form by utilizing an environment critic, which predicts a return distribution of the current policy on the given environment. This enables us to effectively integrate diffusion models within the UED framework, significantly enhancing the environment generation capability. Since the regret-guided diffusion does not require an additional training of the environment generator, we can preserve the ability to cover the high-dimensional environment domain as the random generator of the replay-based method. Moreover, ADD can directly generate meaningful environments via regret-guided sampling as the learning-based methods. By doing so, ADD effectively combines the strengths of previous UED methods while addressing some of their limitations. Additionally, unlike other UED methods, ADD allows us to control the difficulty levels of the environments it generates by guiding the generator with the probability of achieving a specific return. It enables the reuse of the learned generator in various applications, such as generating benchmarks. We conduct extensive experiments across challenging tasks commonly used in UED research: partially observable maze navigation and 2D bipedal locomotion over challenging terrain. Experimental results show that ADD achieves higher zero-shot generalization performance in unseen environments compared to the baselines. Furthermore, our analysis on the generated environments demonstrates that ADD produces an instructive curriculum with varying complexity while covering a large environment configuration space. As a result, it is shown that the proposed method successfully generates adversarial environments and facilitates the agent to learn a policy with solid generalization capabilities."
Super Gradient Descent: Global Optimization requires Global Gradient,"Global minimization is a fundamental challenge in optimization, especially in machine learning, where finding the global minimum of a function directly impacts model performance and convergence. This report introduces a novel optimization method that we called Super Gradient Descent, designed specifically for one-dimensional functions, guaranteeing convergence to the global minimum for any k𝑘kitalic_k-Lipschitz function defined on a closed interval [a,b]𝑎𝑏[a,b][ italic_a , italic_b ]. Our approach addresses the limitations of traditional optimization algorithms, which often get trapped in local minima. In particular, we introduce the concept of global gradient which offers a robust solution for precise and well-guided global optimization. By focusing on the global minimization problem, this work bridges a critical gap in optimization theory, offering new insights and practical advancements in different optimization problems in particular Machine Learning problems like line search.","Global optimization plays a critical role in addressing complex real-life challenges across various fields. In engineering, it is applied to structural design optimization, where minimizing weight or material use while ensuring durability is essential for cost-effective and safe construction. In financial services, portfolio optimization requires balancing risk and return by finding the global minimum or maximum in investment strategies. In logistics and transportation, global optimization is crucial for solving routing problems such as determining the shortest path or optimizing delivery routes which leads to significant cost savings and improved efficiency. Similarly, in energy systems, global optimization is key to managing and distributing power more efficiently, reducing operational costs, and optimizing renewable energy usage. In machine learning, the need for global optimization is especially pronounced. The performance of models often depends on the ability to minimize complex, non-convex loss functions. While traditional methods like gradient descent are effective in many cases, they frequently encounter the problem of getting trapped in local minima, which can hinder the model’s overall performance. This is particularly relevant in tasks that require complex models where the optimization landscape is highly non-linear and fraught with local minima. The primary contribution of this work is the introduction of a novel algorithm named Super Gradient Descent. Unlike classical gradient descent, which collects only local information making it prone to local minima, the proposed method adapts the state’s change decision based on a global detection of the function change to ensure consistent progress towards the global minimum. We evaluate its performance on various one-dimensional functions, demonstrating that it provides superior convergence behavior, particularly in avoiding local minima and achieving the global optimum. This novel approach contributes to overcoming the challenges of non-convex optimization, offering a more reliable method for finding global solutions in machine learning."
Robust Thompson Sampling Algorithms Against Reward Poisoning Attacks,"Thompson sampling is one of the most popular learning algorithms for online sequential decision-making problems and has rich real-world applications. However, current Thompson sampling algorithms are limited by the assumption that the rewards received are uncorrupted, which may not be true in real-world applications where adversarial reward poisoning exists. To make Thompson sampling more reliable, we want to make it robust against adversarial reward poisoning. The main challenge is that one can no longer compute the actual posteriors for the true reward, as the agent can only observe the rewards after corruption. In this work, we solve this problem by computing pseudo-posteriors that are less likely to be manipulated by the attack. We propose robust algorithms based on Thompson sampling for the popular stochastic and contextual linear bandit settings in both cases where the agent is aware or unaware of the budget of the attacker. We theoretically show that our algorithms guarantee near-optimal regret under any attack strategy.","The multi-armed bandit (MAB) setting is a popular learning paradigm for solving sequential decision-making problems (Slivkins et al., 2019). The stochastic and linear contextual MAB settings are the most fundamental and representative of the different bandit settings. Due to their simplicity, many industrial applications such as recommendation systems frame their problems as stochastic or contextual linear MAB (Brodén et al., 2018; Chu et al., 2011). As one of the most famous stochastic bandit algorithms, Thompson sampling has been widely applied in these applications and achieves excellent performance both empirically (Chapelle & Li, 2011; Scott, 2010) and theoretically (Agrawal & Goyal, 2013; 2017). Compared to another popular exploration strategy known as optimality in the face of uncertainty (OFUL/UCB), Thompson sampling has several advantages: • Utilizing prior information: By design, Thompson sampling algorithms utilize and benefit from the prior information about the arms. • Easy to implement: While the regret of a UCB algorithm depends critically on the specific choice of upper-confidence bound, Thompson sampling depends only on the best possible choice. This becomes an advantage when there are complicated dependencies among actions, as designing and computing with appropriate upper confidence bounds present significant challenges Russo et al. (2018). In practice, Thompson sampling is usually easier to implement Chapelle & Li (2011). • Stochastic exploration: Thompson sampling is a random exploration strategy, which could be more resilient under some bandit settings Lancewicki et al. (2021). Despite the success, Thompson sampling faces the problem of low efficacy under adversarial reward poisoning attacks Jun et al. (2018); Xu et al. (2021); Liu & Shroff (2019). Existing algorithms assume that the reward signals corresponding to selecting an arm are drawn stochastically from a fixed distribution depending on the arm. However, this assumption does not always hold in the real world. For example, a malicious user can provide an adversarial signal for an article from a recommendation system. Even under small corruption, Thompson sampling algorithms suffer from significant regret under attacks. While robust versions of the learning algorithms following other fundamental exploration strategies such as optimality in the face of uncertainty (OFUL) and ϵitalic-ϵ\epsilonitalic_ϵ-greedy were developed Lykouris et al. (2018); Neu & Olkhovskaya (2020); Ding et al. (2022); He et al. (2022); Xu et al. (2023), there has been no prior investigation of robust Thompson sampling algorithms. The main challenge is that under the reward poisoning attacks, it becomes impossible to compute the actual posteriors based on the true reward, which is essentially required by the algorithm. Naively computing the posteriors based on the corrupted reward causes the algorithm to be manipulated by the attacker arbitrarily (Xu et al., 2021). This work. We are the first to show the feasibility of making Thompson sampling algorithms robust against adversarial reward poisoning. Our main contribution is developing robust Thompson sampling algorithms for stochastic and linear contextual bandits. We consider both cases where the corruption budget of the attack is known or unknown to the learning agent. The regrets induced by our algorithms under the attack are near-optimal with theoretical guarantees. We adopt two ideas to achieve robustness against reward poisoning attacks in the two MAB settings. The first idea is ‘optimality in the face of corruption.’ In the stochastic MAB setting, we show that the Thompson sampling algorithm can maintain sufficient explorations on arms and identify the optimal arm by relying on optimistic posteriors considering potential attacks. The second idea is to adopt a weighted estimator He et al. (2023) that is less susceptible to the attack. In the linear contextual MAB setting, we show that with such an estimator, the influence of the attack on the estimation of the posteriors is limited, and the Thompson sampling algorithm can almost always identify the optimal arm at each round with a high probability. We empirically demonstrate the training process of our algorithms under the attacks and show that our algorithms are much more robust than other fundamental bandit algorithms, such as UCB, in practice. Compared to the state-of-the-art robust algorithm CW-OFUL He et al. (2022) for linear contextual bandit setting, our algorithm is as efficient, and in addition, it inherits the advantages from using Thompson sampling exploration strategy as aforementioned."
Learning the Regularization Strength for Deep Fine-Tuning via a Data-EmphasizedVariational Objective,"A number of popular transfer learning methods rely on grid search to select regularization hyperparameters that control over-fitting. This grid search requirement has several key disadvantages: the search is computationally expensive, requires carving out a validation set that reduces the size of available data for model training, and requires practitioners to specify candidate values. In this paper, we propose an alternative to grid search: directly learning regularization hyperparameters on the full training set via model selection techniques based on the evidence lower bound (“ELBo”) objective from variational methods. For deep neural networks with millions of parameters, we specifically recommend a modified ELBo that upweights the influence of the data likelihood relative to the prior while remaining a valid bound on the evidence for Bayesian model selection. Our proposed technique overcomes all three disadvantages of grid search. We demonstrate effectiveness on image classification tasks on several datasets, yielding heldout accuracy comparable to existing approaches with far less compute time.","††footnotetext: Open-source code: https://github.com/tufts-ml/data-emphasized-ELBo When fine-tuning deep neural networks (DNNs), a significant amount of computational resources are devoted to tuning hyperparameters that control model complexity to manage tradeoffs between under- and over-fitting on the target task of interest. One widespread example would be tuning the value of the scalar multiplier that controls the strength of an additive loss term computed as the sum-of-squares on weight coefficient values, known in various communities as L2 regularization [33], Ridge penalty [14, 21], or “weight decay” [24, 9]. A common technique for tuning such hyperparameters is to hold out a dedicated validation set and use grid search to find the hyperparameters that perform best on the validation set [36, 32]. While reasonably effective and in widespread use to manage over-fitting in recent transfer learning [45, 39], using grid search for hyperparameter selection has three key drawbacks. First and perhaps most important, the need to train separate models at each possible value in the grid significantly increases computational runtime and resources. Second, the need to carve out a validation set to assess performance reduces the amount of available data that can inform model training. This can cause under-fitting, especially when available data has limited size. Finally, grid search requires a list of candidate values specified in advance, yet ideal values may vary widely depending on the data and specific classification task at hand. We take another approach to hyperparameter selection, inspired by a pragmatic Bayesian perspective. Suppose we model observable dataset 𝒟𝒟\mathcal{D}caligraphic_D via a likelihood p⁢(𝒟|θ)𝑝conditional𝒟𝜃p(\mathcal{D}|\theta)italic_p ( caligraphic_D | italic_θ ), where θ𝜃\thetaitalic_θ is a high-dimensional parameter to be estimated, with prior distribution p⁢(θ|η)𝑝conditional𝜃𝜂p(\theta|\eta)italic_p ( italic_θ | italic_η ) controlled by hyperparameter η𝜂\etaitalic_η (say just 1-5 dimensions). Instead of point estimating a specific θ,η𝜃𝜂\theta,\etaitalic_θ , italic_η pair, we can instead estimate a posterior p⁢(θ|𝒟,η)𝑝conditional𝜃𝒟𝜂p(\theta|\mathcal{D},\eta)italic_p ( italic_θ | caligraphic_D , italic_η ) while simultaneously directly learning η𝜂\etaitalic_η to optimize p⁢(𝒟|η)=∫θp⁢(𝒟,θ|η)⁢𝑑θ𝑝conditional𝒟𝜂subscript𝜃𝑝𝒟conditional𝜃𝜂differential-d𝜃p(\mathcal{D}|\eta)=\int_{\theta}p(\mathcal{D},\theta|\eta)d\thetaitalic_p ( caligraphic_D | italic_η ) = ∫ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_p ( caligraphic_D , italic_θ | italic_η ) italic_d italic_θ. This latter objective p⁢(𝒟|η)𝑝conditional𝒟𝜂p(\mathcal{D}|\eta)italic_p ( caligraphic_D | italic_η ) is known as the marginal likelihood or evidence [26]. The evidence naturally encodes a notion of Occam’s razor, favoring the hyperparameter setting that leads to the simplest model that fits the data well, while penalizing complex models that over-fit the training data [18, 27, 2]. Learning η𝜂\etaitalic_η to maximize evidence (or equivalently, the logarithm of evidence) via gradient descent avoids all three issues with grid search: we need only one run of gradient descent (not separate efforts for each candidate η𝜂\etaitalic_η value in a grid), we can use all available labeled data for training without any validation set, and we can explore the full continuous range of possible η𝜂\etaitalic_η values rather than a limited discrete set that must be predefined. While elegant in theory, this vision of selecting hyperparameters via maximizing evidence is difficult in practice for most models of interest due to the intractable high-dimensional integral that defines the evidence. For modern deep image classifiers with millions of parameters, computing the evidence directly seems insurmountable even for a specifc η𝜂\etaitalic_η, let alone optimizing evidence to select a preferred η𝜂\etaitalic_η value. In this work, we use and extend tools from variational Bayesian methods [3, 19], specifically tractable lower bounds on the evidence, to make hyperparameter selection for fine-tuning deep neural image classifiers possible. Ultimately, we contribute methods that should help practitioners perform cost-effective transfer learning on custom datasets. When available data is plentiful, our experiments suggest our approach is competitive in accuracy while reducing total training time from 16 hours for L2-SP [45] and 150 hours for PTYL [39] (using the grid search ranges recommended by the original authors) to under 3 hours. When available data is limited, e.g., only 5-300 labeled examples per class, our experiments suggest our approach can be particularly effective in improving accuracy and runtime."
Spatial Shortcuts in Graph Neural Controlled Differential Equations,"rnn short=RNN, long=Recurrent Neural Network \DeclareAcronymnde short=NDE, long=Neural Differential Equation \DeclareAcronymgncde short=GNCDE, long=Graph Neural Controlled Differential Equation \DeclareAcronymncde short=NCDE, long=Neural Controlled Differential Equation \DeclareAcronymnn short=NN, long=Neural Network \DeclareAcronymnode short=NODE, long=Neural Ordinary Differential Equation \DeclareAcronympde short=PDE, long=Partial Differential Equation \DeclareAcronymgnn short=GNN, long=Graph Neural Network \DeclareAcronymlstm short=LSTM, long=Long Short Term Memory \DeclareAcronymgru short=GRU, long=Gated Recurrent Unit \DeclareAcronymagc short=AGC, long=Adaptive Graph Convolution \DeclareAcronymresnet short=ResNet, long=Residual Neural Network \DeclareAcronymmae short=MAE, long=Mean Absolute Error","Effect follows cause. When a problem is represented on a graph, its structure contains information on how causes at one spatial position are linked to effects at another. To learn about physical dynamics from spatial time series data, one can often leverage this structural information. Schölkopf [1] describes differential equations as the gold standard for understanding cause-effect structures and highlight the lack of a time component in statistical machine learning methods. \acpnde [2] are able to learn a hidden state that evolves continuously in time, and could remedy this lack. With Neural Controlled Differential Equations (NCDE) [3], one can update the hidden state continuously with data incoming at different points in time. If one also wants to account for spatial dependencies, \acpgncde [4] can be used, where a node embedding is learned to capture these spatial dependencies. We incorporate prior known graph topology information into a \acgncde to infer the future dynamics at the vertices. We therefore generate data coming from a graph advection simulation from which we know the underlying graph topology plus the temporal cause and effect relation. We will outline the close connection between the graph information in the data generated and the artificial \acnn architecture. Then we train our Informed \acpgncde and let them learn the dynamics to predict their future behavior. We start by describing advection on graphs and the theory before we explain the data generation and our Informed \acgncde. We believe this approach can lead to improvements in domains where graph information is available or where time series data is scarce or partially missing. \acpncde are an effective method in this context due to their ability to handle irregular time series data [5]. With graph information one has the potential to predict with fewer observations, as the graph structure itself does not have to be learned on top of the temporal dynamics. Promising domains for introducing known graph structure are traffic forecasting, river water level forecasting, climate and weather prediction, or disease spread (see A.4). Figure 1: Advection of an initial Gaussian pulse on a graph with 5 edges over time"
MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular Metaverse Services,"Updates of extensive Internet of Things (IoT) data are critical to the immersion of vehicular metaverse services. However, providing high-quality and sustainable data in unstable and resource-constrained vehicular networks remains a significant challenge. To address this problem, we put forth a novel immersion-aware model trading framework that incentivizes metaverse users (MUs) to contribute learning models trained by their latest local data for augmented reality (AR) services in the vehicular metaverse, while preserving their privacy through federated learning. To comprehensively evaluate the contribution of locally trained learning models provided by MUs to AR services, we design a new immersion metric that captures service immersion by considering the freshness and accuracy of learning models, as well as the amount and potential value of raw data used for training. We model the trading interactions between metaverse service providers (MSPs) and MUs as an equilibrium problem with equilibrium constraints (EPEC) to analyze and balance their costs and gains. Moreover, considering dynamic network conditions and privacy concerns, we formulate the reward decisions of MSPs as a multi-agent Markov decision process. Then, a fully distributed dynamic reward method based on deep reinforcement learning is presented, which operates without any private information about MUs and other MSPs. Experimental results demonstrate that the proposed framework can effectively provide higher-value models for object detection and classification in AR services on real AR-related vehicle datasets compared to benchmark schemes.","The metaverse, envisioned as one of the next evolution directions of the Internet, is designed to create a fully immersive, self-sustaining virtual environment in which people can engage in activities such as playing, working, and socializing [1]. This vision is propelled by advances in the fifth-generation (5G) and the forthcoming sixth-generation (6G) communication technologies, renowned for their low latency and high data throughput. These technologies play a critical role in seamlessly integrating the Internet of Things (IoT) data into metaverse services, thus bringing the once-fictional concept of immersive experiences closer to reality. Metaverse services are beginning to reveal their vast potential across a broad spectrum of industries, from gaming and autonomous driving to education and marketing. Notably, the application of vehicles within the metaverse has captured significant interest due to the enhanced safety features and immersive experiences offered by state-of-the-art augmented reality (AR) technologies. Figure 1: An example of AR services in the vehicular metaverse222Image source: https://www.jasoren.com/ar-in-automotive/. Market report [2] forecasted that the global automotive metaverse market will grow from 1.91.91.91.9 billion in 2022 to 6.56.56.56.5 billion by 2030. Automakers like BMW have recently doubled down on AR technology in their projects. As shown in Fig. 2, augmented information makes driving safer by showing potential hazards hidden behind the vehicle in front of you. Moreover, Nissan’s [3] upcoming technology utilizes a 3D AR interface that merges the real world with the virtual world to provide the driver with augmented information about the surrounding area. In addition, Vanarama has also implemented a visual parking assistance function using AR [4], which is expected to identify empty spaces in busy car parks and display useful information, such as parking fees, on the windscreen. The ability to capture information from the “real” world, particularly the ability to collect and process massive data from IoT devices, is the key to determining the success of immersive services (e.g., AR) in the vehicular metaverse. Meanwhile, the data must be processed and presented in a meaningful, responsive, and appropriately protected way. Technically, a high-quality experience with AR services relies on accurate detection and classification of real-world objects (e.g., cars and pedestrians) under complex conditions [5]. To achieve this goal, sufficient valid data needs to be collected and processed in depth to detect and classify objects accurately. Therefore, it is essential to focus on effectively collecting, processing, and protecting the data that supports a safer and more enjoyable driving experience. Motivations. An existing widely-used data collection method, as adopted by Nissan [6], involves collecting massive amounts of data through vehicle sensors, cameras, and roadside devices, and then processing all the data centrally. However, when it comes to the situation with multiple metaverse users (MUs) and metaverse service providers (MSPs) on behalf of different companies, the centralized data collection approach is not applicable and may lead to the following issues. First, AR services in the vehicular metaverse need to be highly immersive so that MUs feel fully immersed in the rendered environment, such as visualized driving. However, providing a seamless AR experience is challenging due to the latency caused by massive real-time data updates under unstable and resource-limited communication conditions. Note that the value of real-time data diminishes over time [7]. Also, delays can severely impact the MU’s experience and cause dizziness [8]. Second, the data to be transmitted may be sensitive and private, such as location, movement, and biometrics, which can create a better immersive experience but may inevitably increase the privacy risk of MUs [9]. Third, it is difficult to share data between MSPs due to competition. This leads to the same data being sampled multiple times by different MSPs in the physical world, resulting in wasted resources (e.g., sampling costs) and data redundancy. Moreover, tremendous data pressure and computational burden are imposed on MSPs. Therefore, an efficient method for collecting high-quality data while preserving privacy is needed to achieve immersive AR services. Challenges. To address the above issues, we propose an immersion-aware model trading framework designed to provide data support for AR services and ensure privacy by integrating federated learning (FL). Adopting FL enables MUs to contribute diverse learning models to multiple MSPs using local data, thus effectively protecting sensitive information [10]. Moreover, learning models are uploaded to MSPs for AR services by individual MUs, rather than centrally transmitting large amounts of data, which significantly reduces the communication burden. Nonetheless, developing such a model trading framework still faces the following challenges: • MUs are typically self-interested and are reluctant to share learning models with MSPs due to the additional computation, communication, and energy overhead. Thus, the challenge here is how to incentivize them to become contributors in providing high-value learning models that will benefit vehicular metaverse services. • It is difficult to design an appropriate metric to comprehensively evaluate the value of learning models contributed by MUs due to the diverse aspects involved in model valuation. • MUs have different sampling costs and limited computation and communication resources, while MSPs differ in their model preferences and compete with each other. Hence, the next challenge is to model the dynamic competitive interactions among MSPs and to achieve an equilibrium of interests among MUs and MSPs in the model trading. Figure 2: The outline of an immersion-aware framework for FL-assisted vehicular metaverse. Contributions. To address these challenges, we model the dynamic competitive trading interactions as an equilibrium problem with equilibrium constraints (EPEC) involving multiple leaders (MSPs) and multiple followers (MUs), which is a hierarchical optimization problem with equilibria at two levels [11]. Furthermore, to evaluate the value of learning models contributed by MUs to MSPs, we design a new metric called the immersion of the learning model (IoM). The metric jointly considers the freshness and accuracy of the learning model, as well as the amount and potential value of raw data used for training. Moreover, given the dynamic networks and the privacy concerns of MSPs, we formulate the reward decisions of MSPs as a multi-agent Markov decision process (MAMDP) and develop a multi-agent deep reinforcement learning (DRL)-based dynamic reward (MDDR) approach to obtain the reward decisions in a fully distributed manner. The outline of the proposed framework is shown in Fig. 2, and the main contributions of this paper are summarized as follows: • We propose an immersion-aware trading framework for learning models with FL assistance, incentivizing MUs to contribute high-value learning models for MSPs in a privacy-preserving and efficient manner. Moreover, a novel metric called “IoM” is designed to quantify the immersion enhancement provided by MUs for AR services. • We model the dynamic competitive trading interactions as an EPEC and provide theoretical support for the existence and uniqueness of equilibria at two levels. Moreover, a fully distributed MDDR approach is developed to adapt to dynamic environments and address complex reward decisions of MSPs without accessing any private information of MUs or MSPs. • We conduct extensive numerical simulations based on AR-related vehicle datasets to validate the efficacy and efficiency of MDDR and the proposed immersion-aware model trading framework. Compared to benchmark schemes, the framework better motivates MUs to provide higher-value learning models for object detection and classification functions of AR services effectively. The rest of this paper is organized as follows. Section II discusses the related work. In Section III, we present the system overview and design the immersion metric of the learning model. Section IV gives the game formulation, and Section V analyzes the existence of the equilibria at two levels. In Section VI, we give the detailed design of MDDR. Section VII shows numerical experiments to evaluate the framework performance, and finally Section VIII concludes the paper."
Conformal Prediction for Multimodal Regression,capbtabboxtable[][\FBwidth],"1 Background and related work The comprehension of the internals of neural networks has begun with efforts towards explainability but little has been done to exploit these internal features. The area of conformal prediction (CP) for regression is no different. The consequence is that rich multimodal input features such as images, unstructured text, and categoricals have not participated in the conformal regression prediction uncertainty quantification process. CP provides a straightforward framework for constructing statistically rigorous uncertainty intervals for model predictions. Importantly, these intervals are valid regardless of the underlying distribution, providing explicit, non-asymptotic guarantees without depending on specific distributional or model assumptions. By using conformal prediction on any pre-trained model, it is possible to create intervals that consistently contain the true value with a specified coverage level. Angelopoulos and Bates (2021). CP input assumptions are based on data exchangeability. There is no other assumption about the data distribution or the model. This condition is satisfied for the problems outlined in this paper. The more developed field of CP classification has done image-to-image uncertainty Angelopoulos et al. (2022), and even proposed changing CP regression tasks into classification problems Guha et al. (2023). Despite a comprehensive literature review, no findings similar to those proposed by our novel use of internal features in CP for multimodal regression have been uncovered. The inductive, or split, approach to creating conformal regressors, outlined by Papadopoulos et al. (2002), involves splitting the training data into a proper training set and a calibration set, both of which should accurately represent the overall data distribution. The training set is used to build the regression model, while the calibration set is used to calculate the model’s absolute residuals. Conformal predictive systems (CPS) enhance conformal regressors by generating cumulative probability distributions, over potential target values Vovk et al. (2020). This enables the creation of PIs with a specified confidence level, effectively capturing prediction uncertainty while providing a statistically grounded basis for decision-making. Typically, the more complicated multimodal inputs of images and unstructured text are processed by larger more complicated networks (e.g., convolutional neural networks (CNN), transformers, hybrid networks, etc.). These networks can be compute-intensive. Therefore, building multiple models and having PIs generated in an ensemble fashion may not be an option. The inductive implementation of the aforementioned CPSs was the chosen approach as the model is only trained once after a hold-out calibration set is removed from the training set. Multimodal architectures generally contain an internal combining stage where all the processed inputs come together. These internal inputs will be referred to as internal features within the respective architectures. Conversely, external features are features that are fed to the model at the input layer. The possible advantage with working with internal features is that they have been filtered for significance and weighted for importance which is not true of the input features. This results in a metric suitable for distance-based conformal prediction. The major contribution of this work is to address the challenges associated with conformal prediction in the context of multimodal regression tasks involving diverse input data, such as tabular, unstructured text, and image modalities. Traditional conformal prediction methods face significant obstacles when applied directly to such heterogeneous input features in regression problems. However, this paper demonstrates a novel approach that leverages an internal feature layer within the model architecture for multimodal regression. Specifically, we show that this intermediate representation can be effectively utilized to generate a calibration set suitable for conformal prediction in regression scenarios. This methodology extends the applicability of conformal prediction techniques to complex, multimodal regression tasks, thereby enhancing uncertainty quantification capabilities for a wider range of machine learning regression models and diverse data types."
Deep learning-based identification of patients at increased risk of cancer using routine laboratory markers,"Early screening for cancer has proven to improve the survival rate and spare patients from intensive and costly treatments due to late diagnosis. Cancer screening in the healthy population involves an initial risk stratification step to determine the screening method and frequency, primarily to optimize resource allocation by targeting screening towards individuals who draw most benefit. For most screening programs, age and clinical risk factors such as family history are part of the initial risk stratification algorithm. In this paper, we focus on developing a blood marker-based risk stratification approach, which could be used to identify patients with elevated cancer risk to be encouraged for taking a diagnostic test or participate in a screening program. We demonstrate that the combination of simple, widely available blood tests, such as complete blood count and complete metabolic panel, could potentially be used to identify patients at risk for colorectal, liver, and lung cancers with areas under the ROC curve of 0.76, 0.85, 0.78, respectively. Furthermore, we hypothesize that such an approach could not only be used as pre-screening risk assessment for individuals but also as population health management tool, for example to better interrogate the cancer risk in certain sub-populations.","This paper focuses on the use of multiple biomarkers for the assessment of patients, or identification of otherwise healthy individuals, who are at increased risk of cancer. With the high mortality rate associated with cancer patients, significant research has been conducted to help identify patients at higher risk, starting with identifying medical conditions that increase the risk of cancer, such as diabetes, or genetic predispositions that promote its development[1]. Furthermore, various screening procedures have been developed to help facilitate early diagnosis such as the Faecal Immunochemical Test (FIT) and colonoscopy for colorectal cancer (CRC)[2], mammography for breast cancer[3], and low-dose computed tomography (LDCT) for lung cancer[4]. However, cancer screening rates and their uptake remains lower than desired, e.g., in the US[5]. While there are several factors contributing to this low uptake, one of the key factors is the lack of awareness within the general population. This is even more important to address for people who may be at increased risk and would benefit from early and/or regular screening. In other words, there is still a need for convenient tests for early detection of rapidly progressing diseases such as cancer so that intervention can start as early as possible[6]. Several cancer risk prediction/assessment tools based on demographic, socioeconomic or blood based markers have been developed over the years, and studies have shown that cancer risk assessment algorithms could have an impact in early cancer diagnosis[7]. For instance, the Qcancer 10 year risk algorithm[8] considers the age, ethnicity, deprivation, body mass index, smoking, alcohol, previous cancer diagnoses, family history of cancer, relevant comorbidities, and medication data for a patient and predicts the cancer risk for 11 types of cancers. Nartowt et al.[9] reported high concordance in the prediction of CRC into low, medium and high groups using an artificial neural network trained on patient data comprising age, sex, and complete blood count (CBC). ColonFlag[10] can be used to identify individuals at high risk of CRC using specific blood-based markers and refer them to screening procedures such as colonoscopy. More recently, a cell-free DNA-based blood test for the early detection of CRC has been clinically validated in the ECLIPSE study[11]. Moreover, multi-cancer early detection technologies[12] such as the Galleri test[13, 14] can identify abnormal methylation patterns in cell-free DNA to detect a cancer signal and predict its origin. Besides algorithm development, the deployment of the algorithm and communication of the findings play a critical role in acceptance and clinical use of the algorithm[15], and must be taken into account to facilitate screening uptake. To this end, instead of defining a test with a specific set of ingredients catered towards a particular cancer, we propose to use commonly measured blood markers, often obtained during the annual physical exam, and obtain a risk profile for multiple cancers. Furthermore, instead of reporting a risk score, we compute the pre- and post-test odds of a patient at risk of developing cancer over the next 12 months. A key challenge in developing a model that considers several biomarkers is to deal with a significant degree of missingness in the historical data as not all markers may be obtained at each encounter. Although this issue can be partly alleviated by considering the biomarkers obtained at an annual physical exam, we observed that in real world data, there is still a significant degree of missingness, either due to a lack of awareness, insurance coverage or reimbursement, among other reasons. A standard approach to deal with missingness in input data is to impute the missing values using statistical methods, such as expectation maximization and regression[16]. However, the quality of imputed data is limited and can significantly impact the generalization ability of the trained model. In this work, we address the aforementioned challenges by training a deep learning model, Deep Profiler, which takes the age, sex, and commonly obtained blood biomarkers included in CBC and Comprehensive Metabolic Panel (CMP), and outputs a likelihood ratio of a patient to develop cancer over the period of the following 12 months (see Figure 1). The Deep Profiler architecture employs a variational autoencoder (VAE) model that is pre-trained to impute missing data similar to the masked language modeling technique. Subsequently, we train cancer-specific risk prediction models from the shared encoded latent space and compute the likelihood ratio for each patient. We validate the proposed method over screening-relevant cohorts for three different cancers - colorectal, liver, and lung. These are among the top cancers responsible for cancer related mortality rate in the US (https://seer.cancer.gov/statfacts/html/common.html, accessed April 30, 2024.). Figure 1: Workflow of using a biomarker-based pre-screening test."
Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites.,"Machine learning (ML) models benefit from large datasets. Collecting data in biomedical domains is costly and challenging, hence, combining datasets has become a common practice. However, datasets obtained under different conditions could present undesired site-specific variability. Data harmonization methods aim to remove site-specific variance while retaining biologically relevant information. This study evaluates the effectiveness of popularly used ComBat-based methods for harmonizing data in scenarios where the class balance is not equal across sites. We find that these methods struggle with data leakage issues. To overcome this problem, we propose a novel approach “PrettYharmonize”, designed to harmonize data by pretending the target labels. We validate our approach using controlled datasets designed to benchmark the utility of harmonization. Finally, using real-world MRI and clinical data, we compare leakage-prone methods with “PrettYharmonize” and show that it achieves comparable performance while avoiding data leakage, particularly in site-target-dependence scenarios.","Many research fields have greatly benefited from machine learning (ML) approaches. ML models can extract important values from large amounts of data. Having vast data benefits the model’s classification performance and helps capture the underlying patterns, promoting better generalization to new unseen data. This makes combining multiple datasets an appealing approach, especially in domains where obtaining data in a uniform setting is challenging 1. Moreover, small health or research centers that can not afford to collect a large number in-house data, using data acquired in different sites is the only possibility for train ML models. However, different datasets obtained under different conditions often present variability due to differences in the acquisition procedure that are unrelated to relevant biological information 2. This undesired variability, also known as Effects of Site (EoS), can induce biased results if present or not correctly removed 3. These differences may come from systematic differences, which can be corrected, or random variations, which can not be modeled or corrected by harmonization. This problem is of common occurrence in many biomedical domains. For example, clinical data is affected by the acquisition site, as different hospitals have different laboratory machines, procedures, and criteria. Another example is the medical imaging field, as images are affected by acquisition protocol, scanner drifts, and time of the day, just to name a few factors 3, 4. Within this field, Magnetic Resonance Imaging (MRI) images are particularly susceptible to this site-related variance, like the magnetic field strength, room temperature fluctuation or changes in the electromagnetic noise, which makes even images obtained from scanners with the same manufacturer and the same parameters exhibit different characteristics 5, 6. Many works showed that removing this undesired systematic variability, which is only related to the acquisition site and has no biological information, can benefit further analysis made with the data 7, 8, 9, 10, 11. To this end, several Methods Aiming to Remove the Effects of Site (MAREoS) have been proposed and developed 4, 12. These MAREoS methods are typically used as a pre-processing step, where the site effects are removed and the “site-effect free” data, also known as harmonized data, is used for statistical analysis or to train and evaluate ML models. Among these MAREoS, the ones based on “ComBat” are extensively used in several domains. The ComBat method was originally proposed for correcting batch differences in genomic data 13 and was later adapted to other domains like MRI data 14, 7. ComBat uses Bayesian regression to find additive (location) and multiplicative (scale) corrections for each feature in each site. Within the ComBat-based methods, “neuroHarmonize” was proposed 15 to allow for the preservation of non-linear covariate effects and has been widely used since 4, 12, 16, 17. Although ComBat and its derivations have been widely applied in medical imaging data, several concerns have been raised, mainly because ComBat’s hypothesis and assumptions only hold for genomic data, where it was originally proposed, and may not be fulfilled in other applications fields 18. Additionally, concerns had been raised on the integration of ComBat into ML models, as the location and scale parameters of the model could not be learned in a subset of data (train data) and then applied to a new unseen subset of data (test data) 19. Early implementations of ComBat 14, 20 used the whole dataset to estimate the model’s parameters and create a harmonized dataset that is then used from all the downstream analyses. This approach was used in several works 7, 8, 21, 22, 23, 24. While this approach is valid when performing statistical analyses, it is not consistent with machine learning applications where the training and test data must be separate 25, 19. Specifically, the parameters of the models, including preprocessing models, must be obtained on a training set and then applied to the test set. This separation is important to get realistic estimates of generalization performance (e.g. using cross-validation) and to ensure deployability of the model in the real world where the test data is not yet available 26, 27. ComBat-MAGA 28, neuroHarmonize 15, and ”harmonizer” 19, which is based on neuroHarmonize, allow the estimation of the model’s parameters in a training set and apply them to the test samples, however, a critical assumption of ComBat is that all variance not shared across sites is unwanted site-related variance. Consequently, ComBat removes any variance that is not common to all sites, including the relevant biological variance. This poses a new problem, as this assumption is broken when a class imbalance occurs across sites and a target-site dependence exists, for example when the control patients are acquired in one site and target patients in a different one 29. This could also be extended to other possible biological information like comorbidities or disease severity, for example, if more severe patients are consistently treated or acquired only in one site. In these cases, even though ComBat will provide harmonized data, it will remove the variance related to the target (control versus patient) as the assumption is that only non-relevant factors change between sites. ComBat allows to retain the biologically relevant variables, for example, a diagnosis, the age, or the sex of a patient, by providing these variables as covariates to be retained. Nonetheless, this information is needed both when training the model and when applying the model to the test data. Thus, if target labels need to be preserved, this inevitably leads to the model requiring the test labels preventing the model’s use in real-world applications where test labels are not available or known18. This phenomenon where information of the test set is presented to the models is commonly known as data leakage. It is also well described that leaking the test target information would produce overconfident results, which could be misleading and can jeopardize the progress of an entire research field, as researchers who avoid data leakage would not be able to outperform the models that present data leakage 26, 27, 25. In this work, we aim to empirically demonstrate a shortcoming of ComBat-based harmonization in site-target dependence scenarios, i.e., that the model can properly harmonize the data only when test labels are used and data leakage happens. To do so, we performed controlled experiments for age regression and sex classification using real MRI data for healthy control individuals. Also using MRI data, a dementia and mild cognitive impairment (MCI) classification experiment was performed. Additionally, an outcome prediction of septic patients was performed using clinical data. All experiments were conducted both in site-target dependence and independence scenarios. Several harmonization schemes were used and compared, allowing and not allowing leakage, to harmonize the data. Finally, to overcome the aforementioned problem, we propose a new harmonization method, called PRETended Target Y Harmonize (PrettYHarmonize), which allows the users to integrate ComBat in an ML pipeline, harmonizing the data and generating a prediction without using the test labels and thus avoiding data leakage. We validated our method using benchmark datasets 3. Additionally, the proposed method was compared with the other harmonization schemes on the site-target dependence and independence scenarios to comprehensively compare no harmonization, leakage, and no-leakage methods. The corresponding Python package is publicly available via GitHub https://github.com/juaml/PrettYharmonize."
Efficient Biological Data Acquisitionthrough Inference Set Design,"In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so we might hope to reduce their cost by experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of an uncertainty-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that stops running the experiments when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that deploying active learning for inference set design leads to significant reduction in experimental cost while obtaining high system performance.","Automated high-throughput screening (HTS) laboratories have enabled scientists to screen large compound libraries to find effective therapeutic compounds and screen whole-genome CRISPR knockouts to understand the effects of genes on cell function (Mayr & Bojanic, 2009; Wildey et al., 2017; Blay et al., 2020; Tom et al., 2024; Fay et al., 2023). However, conducting experiments on every compound or CRISPR guide in these vast libraries remains very resource-intensive. With typical screening libraries holding on the order of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT to 106superscript10610^{6}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT compounds (Hughes et al., 2011) and the number of possible small molecules estimated at 1060superscript106010^{60}10 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT (Bohacek et al., 1996), the disparity between our screening capabilities and all which we could explore is staggering. Reducing experimental costs without compromising the quality of the generated data would allow us to accelerate biology and pharmaceutical research and expand the set of molecules considered for testing. To avoid costs scaling with the number of perturbations, we can train a model from a subset of the target library that has been tested in the lab, and then predict experimental outcomes for the remainder of the library using the trained model (Naik et al., 2013; Reker & Schneider, 2015; Dara et al., 2022), thereby building a hybrid screen of the library. This approach entails three interrelated questions: (1) which subset of the library should we use to maximise the accuracy of the predictions?, (2) how do we select this subset without access to the experimental outcomes?, and (3) how do we ensure that we select a large enough subset to meet a target level of accuracy for the predictions? This problem is similar to an active learning problem in that we want to select examples that maximize prediction accuracy, but instead of aiming to minimize generalization error, here we only care about prediction on a particular, finite, set of experiments from a library. The fact that the library is finite introduces an important difference: the learner can influence the set of examples on which it is evaluated by strategically selecting examples. If we accept that the difficulty of predicting outcomes varies among compounds, such that some examples are inherently harder to predict, either due to their complex properties (Bengio et al., 2009), because of the partial observability of the system (Saleh et al., 2021; Krenn et al., 2020) or due to noise in the labeling function (Frénay & Verleysen, 2013; Lukasik et al., 2020), the learner can select these examples into the training set to avoid having to predict their outcomes. Conversely, if an example can be reliably predicted by the model, we can save experimental costs by not including it in the training set. We call this mechanism through which a learner can influence performance inference set design. We propose an active learning-based solution to hybrid screening that uses the model’s confidence to guide the selection of experiments and leverages the mechanism of inference set design to improve the system’s performance on the target set. Our algorithm includes a practical stopping criterion that terminates the search for new molecules once a lower bound on a target accuracy threshold is exceeded. We show in Lemma 1 that this bound provides probabilistic guarantee on the performance of the algorithm as long as the model is weakly calibrated, such that examples for which the model is more uncertain receive a lower predicted probability than those for which it is more certain. To validate our method, we conduct series of empirical studies on image and molecular datasets, as well as a real-world case study in drug discovery. The results demonstrate inference set design significantly reduces experimental costs while improving overall system performance. Importantly, this is true even when the generalization benefits of active learning-based acquisition functions are marginal compared to random search. This has important practical implications for active learning: if a problem is a hybrid screen—in the sense that one only needs good performance on a fixed, finite set of experiments—then evaluating generalization error dramatically understates the benefits of active learning. By combining simple active learning acquisition functions with an appropriate stopping criterion, it is possible to make large scale screening far more efficient."
"Analyzing Neural Network Robustness Using Graph Curvature††thanks:This material is based upon work supported by the National Science Foundation (NSF) under Award No. 2403616, as part of the NSF Cyber-Physical Systems Program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.","This paper presents a new look at the neural network (NN) robustness problem, from the point of view of graph theory analysis, specifically graph curvature. Graph curvature (e.g., Ricci curvature) has been used to analyze system dynamics and identify bottlenecks in many domains, including road traffic analysis and internet routing. We define the notion of neural Ricci curvature and use it to identify bottleneck NN edges that are heavily used to “transport data” to the NN outputs. We provide an evaluation on MNIST that illustrates that such edges indeed occur more frequently for inputs where NNs are less robust. These results will serve as the basis for an alternative method of robust training, by minimizing the number of bottleneck edges.","Autonomous systems (AS) increasingly use neural networks (NNs) due to their ability to process high-dimensional data such as camera images [1], LiDAR scans [2] and textual prompts [3]. At the same time, NNs are known to suffer from robustness vulnerabilities: a slightly perturbed or out-of-distribution input [4, 5] may lead to very different and unexpected outputs. In turn, such vulnerabilities may severely compromise the safety and predictability of NN-based AS. Since the discovery of NN robustness issues [4], there has been an impressive amount of research on this topic. Researchers have developed a number of robust training methods, including adversarial training [6], certified robustness [7, 8], knowledge distillation [9], and semi-infinite constrained learning [10]. Although significant progress has been made, training robust NNs remains largely an unsolved and very challenging problem (e.g., the current leader on the CIFAR-10 robustness leaderboard [11] can only achieve high robust accuracy for perturbations of at most 8/255). We note that the vast majority of existing methods approach the problem from an optimization point of view: e.g., in adversarial training the goal is to train a NN that minimizes the loss not only on training data but also on the worst-case bounded perturbations of that data. This bilevel non-convex optimization problem is challenging to solve and leads to suboptimal solutions, especially if gradient descent is used. We take a fresh look at NN robustness through the lens of graph theory and network science analysis, in particular graph curvature (GC). GC (e.g., Ricci curvature [12]) has been effectively applied in numerous domains that can be modeled as graphs, including road traffic analysis [13, 14], internet routing [15], machine learning [16, 17], and biological networks [18, 16], due to its ability to capture intrinsic geometric and local structure of the space, such as connectivity and robustness in networks. GC can quantify the importance of specific edges; for example, an edge with negative curvature can be considered a bottleneck and is greatly important for the overall graph functionality, e.g., such an edge may connect different communities within the graph [19, 20]. In this paper, we employ GC in order to analyze the robustness of NN classifiers. We introduce the notion of neural Ricci curvature (NRC) that captures the bottleneck intuition of standard Ricci curvature – if an edge has a negative NRC, then it is heavily used by the NN and is thus likely a source of robustness vulnerability. To calculate the NRC, we construct a neural data graph, i.e., a graph in the shape of the NN architecture, where edges are weighted by a combination of the NN weights and the magnitude of data that goes through each edge when an example is provided as input. We evaluate the significance of the NRC using NNs trained on MNIST. We show that neural data graphs corresponding to more robust examples (i.e., examples which are correctly classified even for an adversarial perturbation) indeed have fewer negative-NRC edges. The results are consistent across architectures, including adversarially trained ones. This result will serve as the basis for an alternative, graph-based, method for robust training, that minimizes the number of negative-NRC edges and promotes balanced usage of all NN edges. In summary, this paper makes two contributions: 1) we define the concepts of neural data graphs and neural Ricci curvature that can be used to identify bottleneck NN edges that contribute to robustness issues; 2) we provide an evaluation on MNIST that demonstrates that bottleneck edges indeed occur more frequently in examples where NNs are less robust."
FLiP: Privacy-Preserving Federated Learning based on the Principle of Least Privileg,"Federated Learning (FL) allows users to share knowledge instead of raw data to train a model with high accuracy. Unfortunately, during the training, users lose control over the knowledge shared, which causes serious data privacy issues. We hold that users are only willing and need to share the essential knowledge to the training task to obtain the FL model with high accuracy. However, existing efforts cannot help users minimize the shared knowledge according to the user intention in the FL training procedure. This work proposes FLiP, which aims to bring the principle of least privilege (PoLP) to FL training. The key design of FLiP is applying elaborate information reduction on the training data through a local-global dataset distillation design. We measure the privacy performance through attribute inference and membership inference attacks. Extensive experiments show that FLiP strikes a good balance between model accuracy and privacy protection.","Federated learning (FL) [1] is a deep learning (DL) training paradigm, which aims to utilize the data existing in the form of isolated islands to train DL models (Fig. 2(a)). During the training procedure, data owners (a.k.a. clients) do not share their raw data with anyone, but instead, share some information obtained from the raw data in the form of model parameters. Many solutions are proposed to protect the privacy in FL context. However, they are yet unable to strike a balance between performance and privacy protection in real-world scenarios. This is because they do not fully consider the training task itself when performing protection. Specifically, considering that users are likely only interested in obtaining a high-quality model for the target task, sharing any information unrelated to the training task during the training process could potentially lead to privacy leakage, e.g., secondary attributes inference [2], practical attribute reconstruction Attack [3]. Given the subjective nature of privacy protection, we hold that an ideal solution should fully consider the user’s training goals and only share essential information related to the training task during the training. That’s the core idea, what we call as principle of least privilege (PoLP). According to the PoLP, as shown in Fig. 1, clients should control only the essential training task-relevant information from the raw data that can be shared among participants. At first glance, it is paradoxical to determine which part of the raw data plays a role in the model training procedure before the model is trained. After empirical study and analysis, we observe that each client can only extract a portion of local data that is most relevant to the FL task in the local training. Figure 1: Comparison of PoLP and existing privacy protection solutions. Ideally, the new sample generated by PoLP only contains task-relevant information. (a) The vanilla FL paradigm. (b) The FL paradigm of FLiP. Figure 2: The training procedures of vanilla FL and FLiP. In both paradigms, no training is needed on the central server. The biggest difference between our FLiP and the vanilla FL is the carrier of information aggregation, i.e., FLiP performs distilled data aggregation, and the vanilla FL performs parameter aggregation. Compared to the vanilla method, the amount of shared information during the training in FLiP is controllable. Our work proposes a new FL framework, FLiP, to achieve the PoLP in FL training for privacy protection. FLiP can help clients to identify which part of the raw data contributes most to the FL training and, at the same time, extract the task-relevant information locally. The central server collects the task-relevant information extracted by clients and distributes the global view to help clients make better information extraction. To measure privacy protection effectiveness, we consider adversaries in a semi-honest setting and perform two attacks to infer task-irrelevant information from the data extracted during the training. Experimental results show that FLiP can achieve comparable accuracy to the vanilla FL and better protection for information irrelevant to the training task. Our contribution is fourfold: • We are the first to introduce the PoLP to the FL training for privacy protection. Data owners can control the local information shared among FL participants and minimize privacy breaches by only sharing the essential FL task-relevant information. • We design a privacy-preserving FL system, FLiP, to realize the PoLP. The key design of FLiP is a local-global dataset distillation, which can identify and extract the task-relevant information gradually by round. • We design a task-irrelevant attribute inference attack to measure the protection effectiveness. The attribute inference attack is inspired by existing secondary inference attacks and fully considers the information leakage in each round. • We implement the system and perform an extensive evaluation. The experimental results show that FLiP can prevent adversaries from inferring task-irrelevant information and preserve high data usability."
Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series,"Lately, there has been a surge in interest surrounding generative modeling of time series data. Most existing approaches are designed either to process short sequences or to handle long-range sequences. This dichotomy can be attributed to gradient issues with recurrent networks, computational costs associated with transformers, and limited expressiveness of state space models. Towards a unified generative model for varying-length time series, we propose in this work to transform sequences into images. By employing invertible transforms such as the delay embedding and the short-time Fourier transform, we unlock three main advantages: i) We can exploit advanced diffusion vision models; ii) We can remarkably process short- and long-range inputs within the same framework; and iii) We can harness recent and established tools proposed in the time series to image literature. We validate the effectiveness of our method through a comprehensive evaluation across multiple tasks, including unconditional generation, interpolation, and extrapolation. We show that our approach achieves consistently state-of-the-art results against strong baselines. In the unconditional generation tasks, we show remarkable mean improvements of 58.17%percent58.1758.17\%58.17 % over previous diffusion models in the short discriminative score and 132.61%percent132.61132.61\%132.61 % in the (ultra-)long classification scores. Code is at https://github.com/azencot-group/ImagenTime.","Generative modeling of real-world information such as images [72], texts [13], and other types of data [99, 55, 8] has drawn increased attention recently. In this work, we focus on the setting of generative modeling (GM) of general time series information. There are several factors that govern the complexity required from sequential data generators including the sequence length, its number of features, the appearance of transient vs. long-range effects, and more. Existing generative models for time series are typically designed either for multivariate short-term sequences [44, 19] or univariate long-range data [103], often resulting in separate and completely different neural network frameworks. However, a natural question arises: Can one develop a unified framework equipped to handle both high-dimensional short sequences and low-dimensional long time series? Earlier approaches for processing time series based on recurrent neural networks (RNNs) handled short sequences well [62, 3, 43, 76], however, modeling long-range dependencies turned out to be significantly more challenging. Particularly, RNNs suffer from the well-known vanishing and exploding gradient problem [9, 70] that prevents them from learning complex patterns and long-range dependencies. To address long-context modeling and memory retention, extensive research is devoted to approaches such as long short-term memory (LSTM) models [42], unitary evolution RNNs [5] and Lipschitz RNNs [24]. A different approach for processing sequential information is based on the Transformer [93], eliminating any recurrent connections. Recent remarkable results have been obtained with transformers on natural language processing [13] and time series forecasting [96, 104, 68] tasks. Alas, transformers are underexplored as generative models for long-range time series data. This may be in part due to their computational costs that scale quadratically as 𝒪⁢(L2)𝒪superscript𝐿2\mathcal{O}(L^{2})caligraphic_O ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) with the sequence length L𝐿Litalic_L, and in part because transformer forecasters are inferior to linear tools [101]. Beyond RNNs and the Transformer, recent works have considered the state space model (SSM) for modeling long-range time series information. For instance, the structured SSM (S4) [36] employed a parameterization that reduced computational costs via evaluations of Cauchy kernels. Further, the deep linear recurrent unit (LRU) is inspired by the similarities between SSMs and RNNs, and it demonstrated impressive performance in modeling long-range dependencies (LRD). Still, generative modeling of long-range sequential data via state space models remains largely underexplored. Recent work suggested LS4 [103], a latent time series generative model that builds upon linear state space equations. LS4 utilizes autoregressive dependencies to expressively model time series (potentially non-stationary) distributions. However, this model struggles with short-length sequences as we show in our study, potentially due to limited expressivity of linear SSMs. To overcome gradient issues of recurrent backbones, temporal computational costs of transformers, and expressivity problems of SSMs, we represent time series information via small-sized images. Transforming raw sequences to other encodings has been useful for processing audio [34] as well as general time series data [95, 38, 56]. Moreover, a similar approach was employed to generative modeling of time series with generative adversarial networks (GANs) [12, 39]. However, unstable training dynamics and mode collapse negatively affect the performance of GAN-based tools [59]. In contrast, transforming time series to images is underexplored in the context of generative diffusion models. There are several fundamental advantages to our approach. First, there have been remarkable advancements in diffusion models for vision data that we can exploit [81, 40, 86, 45]. Second, using images instead of sequences elegantly avoids the challenges of long-term modeling. For instance, a moderately-sized 256×256256256256\times 256256 × 256 image corresponds to a time series of length up to 65⁢k65𝑘65k65 italic_k, as we show in Sec. 3. Finally, there is a growing body of literature dealing with time series as images on generative, classification, and forecasting tasks, whose results can be applied in our work and in future studies. In this work, we propose a new diffusion-based framework for generative modeling of general time series data, designed to seamlessly process both short-, long-, and ultra-long-range sequences. To evaluate our method, we consider standard benchmarks for short to ultra-long time series focusing on unconditional generation. Our approach supports efficient sampling, and it attains state-of-the-art results in comparison to recent generative models for sequential information. As far as we know, there are no existing tools handling both short and long sequence data. In addition to its strong unconditional generation capabilities, our approach is also tested in conditional scenarios involving the interpolation of missing information and extrapolation. Overall, we obtained state-of-the-art results in such cases with respect to existing tools. We further analyze and ablate our technique to motivate some of our design choices. The contributions of our work can be summarized as follows: 1. We view generative modeling of time series as a visual challenge, allowing to harness advances in time series to image transforms as well as vision diffusion models. 2. We develop a novel generative model for time series that scales from short to very long sequence lengths without significant modifications to the neural architecture or training method. 3. Our approach achieves state-of-the-art results in comparison to strong baselines in unconditional and conditional generative benchmarks for time series of lengths in the range [24,17.5⁢k]2417.5𝑘[24,17.5k][ 24 , 17.5 italic_k ]. Particularly, we attain the best scores on a new challenging benchmark of very long sequences that we introduce."
AgentForge: A Flexible Low-Code Platform for Reinforcement Learning Agent Design,"Developing a reinforcement learning (RL) agent often involves identifying effective values for a large number of parameters, covering the policy, reward function, environment, and the agent’s internal architecture, such as parameters controlling how the peripheral vision and memory modules work. Critically, since these parameters are interrelated in complex ways, optimizing them can be viewed as a black box optimization problem, which is especially challenging for non-experts. Although existing optimization-as-a-service platforms (e.g., Vizier, Optuna) can handle such problems, they are impractical for RL systems, as users must manually map each parameter to different components, making the process cumbersome and error-prone. They also require deep understanding of the optimization process, limiting their application outside ML experts and restricting access for fields like cognitive science, which models human decision-making. To tackle these challenges, we present AgentForge, a flexible low-code framework to optimize any parameter set across an RL system. AgentForge allows the user to perform individual or joint optimization of parameter sets. An optimization problem can be defined in a few lines of code and handed to any of the interfaced optimizers. We evaluated its performance in a challenging vision-based RL problem. AgentForge enables practitioners to develop RL agents without requiring extensive coding or deep expertise in optimization.","1 INTRODUCTION Developing reinforcement learning (RL) agents is important not only for advancements in machine learning (ML) but also for fields such as cognitive sciences, which is increasingly using RL to model cognitive mechanisms [Eppe et al., 2022]. One of the challenging aspects of this development is optimizing a broad and complex array of parameters that influence agent behavior and performance, a complex black box optimization problem for non-ML experts. Embodied RL agents, which interact with their environments through sensors and actuators, are a point in case. These agents are modeled as Partially Observable Markov Decision Processes (POMDPs) [Eppe et al., 2022, Deglurkar et al., 2023] and are used in real-world tasks such as autonomous driving [De Morais et al., 2020], robotic navigation [Shahria et al., 2022], and human-computer interaction [Jiang et al., 2024]. These models involve not only the parameters of the policy but several others controlling, among others, the reward function. In constrast, optimization of models in other ML fields is often more straightforward, as practitioners need to tune a smaller set of parameters mostly related to training. Sometimes parameter optimization is not just a matter of performance. Parameters that are explainable (e.g., reward weights) are a key to engineering explainable and trustworthy AI systems. Figure 1: Three examples of RL agent applications in Robotics [Ladosz et al., 2024], Human Modeling [Shi et al., 2024], and Collaborative AI [Lingler et al., 2024], highlighting the number of parameters involved in agent design today. Parameters are categorized as Agent, Environment, or Policy. The lists of parameters are non-exhaustive, and practitioners have to choose many more. Our long-term goal is to provide non-ML expect practitioners with the ability to optimize all parameters in a RL system, either jointly or individually, based on their specific problem. This capacity is important, because evidence suggests that even subtle adjustments to the implementation of RL algorithms, like reward clipping or observation normalization, can affect performance [Engstrom et al., 2020]. Furthermore, careful selection of parameters can sometimes improve performance more than the choice of RL algorithm itself [Andrychowicz et al., 2020, Paine et al., 2020, Yang and Yin, 2021]. In embodied RL agents in particular, the deep neural networks have many parameters that are sensitive to optimization [Fetterman et al., 2023]. Figure 1 presents some examples of agents’ applications and some of the parameters that a practitioner would need to choose during their development phase. More generally, these parameters can be roughly categorized in the following classes: 1. Agent design, which control the internal functions of the RL agent, including the discount factor, the entropy coefficient, and the size of the observation window; 2. Environment settings, which define the task itself, such as the field of view, and the reward structure; 3. Policy parameters, such as the architecture of the neural networks, learning rate, and activation functions. Thus, we present AgentForge222A link to the AgentForge platform can be found here: https://github.com/feferna/AgentForge, a flexible low-code platform targeted specifically for developing complex RL systems by domain experts with no solid background in optimization or ML. The focus in a low-code platform is becoming increasingly relevant as professionals in fields outside of ML, such as cognitive and behavioral sciences, are starting to use RL systems to model human cognition and behavior [Eppe et al., 2022, Nobandegani et al., 2022]. Beyond offering a low-code specification of the optimization problem, AgentForge provides rapid iteration by allowing developers to quickly ‘prototype’ an RL design, optimize it, and examine results. Its flexibility builds on its ability to accommodate a wide range of RL systems, from simple fully observable models to more complex, embodied RL agents. Users are only required to define their custom environment, objective function, and the parameters they wish to optimize in two files. The objective function, which guides the optimization process, can be the average reward per episode or any other criterion defined by the practitioner. The platform then automatically converts these inputs into an optimization problem that defines how the agent is trained and evaluated. In this first version, the software offers three popular optimization techniques: Random Search, Bayesian Optimization (BayesOpt) and Particle Swarm Optimization (PSO); however, this set can be extended. Obviously, the challenge of parameter optimization is shared across areas of ML, which has promoted the development of numerous services and frameworks. Notable examples include BoTorch [Balandat et al., 2020], Scikit-Optimize [scikit-optimize, 2024], Optuna [Akiba et al., 2019], and Google Vizier [Song et al., 2023]. However, these platforms primarily cater to users with substantial knowledge of ML and statistics. They are also complex to use with RL agents, because it is not easy to devise an objective function when there are many parameters distributed across different system components. This motivates low-code solutions, such the one we present here. Such tools can enable users to focus on designing effective agents without the burden of mastering optimization techniques, which is especially important given the high dimensionality of sensory inputs and the dynamic environments RL agents operate in. We evaluated AgentForge by jointly optimizing the parameters of a pixel-based version of the classical Lunar Lander agent from the Gymnasium library [Towers et al., 2024], a POMDP where the agent must infer the state from raw pixel values. This agent presents parameters from all three categories cited earlier—agent, environment, and policy—demonstrating AgentForge’s ability to handle diverse and complex parameter sets. Additionally, we show how easily an optimization problem can be specified, highlighting the platform’s easiness and flexibility. The remainder of this paper presents AgentForge in detail and is structured as follows: Section 2 reviews related works on traditional parameter tuning methods, their application in ML application, and limitations. Section 3 details our proposed platform, including what inputs are required from the user and how the optimization is performed. Section LABEL:sec:evaluation describes the evaluation setup, including the application of AgentForge to the proposed pixel-based Lunar Lander agent and its configuration. Section 4 presents the results achieved, while Section 5 discusses the implications for RL agents. Finally, Section 6 summarizes the main findings, discusses current limitations, and suggests directions for future research."
Marked Temporal Bayesian Flow Point Processes,"Marked event data captures events by recording their continuous-valued occurrence timestamps along with their corresponding discrete-valued types. They have appeared in various real-world scenarios such as social media, financial transactions, and healthcare records, and have been effectively modeled through Marked Temporal Point Process (MTPP) models. Recently, developing generative models for these MTPP models have seen rapid development due to their powerful generative capability and less restrictive functional forms. However, existing generative MTPP models are usually challenged in jointly modeling events’ timestamps and types since: (1) mainstream methods design the generative mechanisms for timestamps only and do not include event types; (2) the complex interdependence between the timestamps and event types are overlooked. In this paper, we propose a novel generative MTPP model called BMTPP. Unlike existing generative MTPP models, BMTPP flexibly models marked temporal joint distributions using a parameter-based approach. Additionally, by adding joint noise to the marked temporal data space, BMTPP effectively captures and explicitly reveals the interdependence between timestamps and event types. Extensive experiments validate the superiority of our approach over other state-of-the-art models and its ability to effectively capture marked-temporal interdependence.","Marked event data is widely seen in the real world as a sequence of events, where each event is recorded with a continuous-valued occurrence timestamp and a categorical event type (a.k.a. mark). Its detailed applications include social media [1, 2], where different event types may trigger diverse event patterns; financial transactions [3, 4], where a buy or sell action would result in different transaction times; and healthcare records [5, 6], where the disease types decide the visit times. That is, timestamps and event types exhibit non-trivial interdependence in these scenarios, as they can influence each other in both directions, specified by the detailed situation. Marked Temporal Point Process (MTPP) is a stochastic process that can effectively model marked event sequences. Mainstream MTPP methods can be broadly divided into two categories: classical MTPP models, including Poisson processes [7, 8], Hawkes processes [9] and Self-correcting processes [10], which use an intensity function to characterize the instantaneous occurrence of events given their history. However, these methods often rely on strong parametric or modeling assumptions, limiting their ability to effectively capture complex patterns in event occurrences. On the other hand, neural MTPP models have emerged as a rapidly developing branch in recent years [11, 12, 13]. These models employ neural networks, such as RNNs, LSTMs, or transformers, to encode historical events [14]. In some cases, they draw on intensity functions inspired by processes like the Hawkes process to model event occurrences [15, 16]. Compared to classical MTPP models, neural MTPP models leverage the expressive power of neural networks to better capture the complex dependencies among events. However, applying these two methods to parametric MTPP models requires solving integrals to compute the likelihood function, which usually requires extremely high computational cost [11, 17]. To tackle this issue, two common techniques, namely strong model assumptions and numerical approximation techniques, are typically employed. First, certain model assumptions are introduced, such as treating the timestamp x𝑥xitalic_x and event type m𝑚mitalic_m as independent or conditionally dependent (e.g., x𝑥xitalic_x depends on m𝑚mitalic_m, or m𝑚mitalic_m depends on x𝑥xitalic_x) [18, 19, 15, 20]. This approach simplifies the integral form, thereby reducing computational complexity. However, in reality, x𝑥xitalic_x and m𝑚mitalic_m may not be independent or may have more complex dependence, which can lead to model misspecification and consequently restrict the model’s expressive capacity [21]. Second, numerical approximation techniques, such as Monte Carlo sampling or numerical integration, are used to simplify the computation of integrals when closed-form solutions are not available [22, 23]. Despite this, limitations in sample size and sampling errors mean that these methods only approximate the true solution, which can result in information loss and affect model performance [17]. To fill these gaps, generative MTPP models, which model the target distribution of timestamps using the generative models, have been proposed and have shown promising results [24, 25]. Mainstream generative models, such as diffusion models [26, 27], generative adversarial networks (GANs) [28, 29], and variational autoencoders (VAEs) [30, 31], are energy-based deep probabilistic models, where the optimization objective is an energy function corresponding to an unnormalized negative log-likelihood function. Typically, neural networks are employed to represent this energy function. Since neural networks can approximate complex functions without formal constraints, there is no need to impose model assumptions or use numerical approximation techniques to simplify computation. Consequently, using such generative models in MTPP tasks allows for flexible modeling, enhancing the model’s expressiveness and avoiding information loss due to approximation operations. However, generative MTPP models still face two main challenges. Challenge I: In MTPP tasks, we aim to model the joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ) of two heterogeneous random variables: x𝑥xitalic_x, which is continuous, and m𝑚mitalic_m, which is discrete. However, mainstream generative models, such as diffusion models, are designed for continuous random variables due to their reliance on Gaussian noise [32]. As a result, these models are not directly applicable for modeling joint distributions that include discrete random variables m𝑚mitalic_m [24]. Challenge II: The target joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ) involves two random variables, x𝑥xitalic_x and m𝑚mitalic_m, which exhibit a strong interdependence across different scenarios. For example, discussions about clothing types change with the seasons [14]. Thus, capturing the complex interdependence between timestamps x𝑥xitalic_x and different event types m𝑚mitalic_m is crucial for improving model performance. However, existing generative MTPP models are unable to directly model the joint distribution, often leading to the assumption that x𝑥xitalic_x and m𝑚mitalic_m are independent, and applying the generative model only to x𝑥xitalic_x [24]. This approach neglects the interdependence between the two random variables, ultimately compromising model performance. To tackle the above challenges, we propose a novel generative MTPP model called the Marked Temporal Bayesian Flow Point Process (BMTPP), based on the recently developed generative model, the Bayesian Flow Network (BFN) [33], to approximate the joint distribution of timestamps and event types. First, in contrast to existing generative MTPP models that model the continuous random variable x𝑥xitalic_x only, BMTPP flexibly leverages BFN in a parameter-based manner to model both the timestamps and event types. Second, by adding the joint noise to the marked temporal data, BMTPP can effectively capture the coupling correlation between timestamps and event types in MTPP tasks, explicitly revealing the complex interactions between them. We summarize the contributions as follows: • Based on the Bayesian flow network, we propose a new generative MTPP model, i.e., BMTPP, which can naturally and flexibly model marked event sequences. • BMTPP can directly model the joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ), which consists of continuous timestamps x𝑥xitalic_x and discrete event types m𝑚mitalic_m. Moreover, by employing a joint noise strategy within the marked temporal data space, it can effectively capture and explicitly reveal the interdependence between timestamps and event types. • We evaluate BMTPP on four real-world datasets, demonstrating our proposed approach outperforms other state-of-the-art models, as well as the effectiveness of our joint noise strategy in capturing marked-temporal interdependence."
DMT-HI: MOE-based Hyperbolic Interpretable Deep Manifold Transformation for Unspervised Dimensionality Reduction,"Dimensionality reduction (DR) plays a crucial role in various fields, including data engineering and visualization, by simplifying complex datasets while retaining essential information. However, the challenge of balancing DR accuracy and interpretability remains crucial, particularly for users dealing with high-dimensional data. Traditional DR methods often face a trade-off between precision and transparency, where optimizing for performance can lead to reduced interpretability, and vice versa. This limitation is especially prominent in real-world applications such as image, tabular, and text data analysis, where both accuracy and interpretability are critical. To address these challenges, this work introduces the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI). The proposed approach combines hyperbolic embeddings, which effectively capture complex hierarchical structures, with Mixture of Experts (MOE) models, which dynamically allocate tasks based on input features. DMT-HI enhances DR accuracy by leveraging hyperbolic embeddings to represent the hierarchical nature of data, while also improving interpretability by explicitly linking input data, embedding outcomes, and key features through the MOE structure. Extensive experiments demonstrate that DMT-HI consistently achieves superior performance in both DR accuracy and model interpretability, making it a robust solution for complex data analysis. The code is available at https://github.com/zangzelin/code_dmthi.","Dimensionality reduction [1, 2, 3, 4] simplifies complex datasets while preserving their intrinsic structure [5, 6]. This is crucial for managing high-dimensional data, which presents challenges in computational complexity, storage, and visualisation [7, 8]. Reducing data dimensionality allows for more efficient analysis, pattern recognition, and interpretation [9]. However, balancing high performance [10] and interpretability [11, 12] remains challenging, as efficient processing [13] often conflicts with human interpretability [14, 15]. Figure 1: Overview of the proposed DMT-HI network. The figure compares three dimensionality reduction methods, manifold-based, deep learning-based, and the proposed DMT-HI. DMT-HI leverages a Mixture of Experts (MOE) strategy to efficiently process both image [16] and tabular [17] data, offering better performance, lower training costs, and improved interpretability across different data sizes. Dimensionality reduction methods fall into two categories, manifold-based parameter-free approaches [18, 19, 20] and deep learning-based methods [21]. Manifold-based methods like t-SNE [22] and UMAP [23] are known for their speed (on small dataset) and adaptability [7], projecting high-dimensional data into low-dimensional spaces through nonlinear mappings, revealing underlying structures. Deep learning methods, such as parametric UMAP (PUMAP) [24, 25] and DMT-EV [26], handle complex data more effectively, especially high-dimensional data, leveraging neural networks to capture intricate patterns. DMT-EV, in particular, excels in both performance and explainability [27], pruning irrelevant features for clearer, more interpretable results. Deep learning methods stand out for their ability to scale and generalize well across large datasets, positioning them as central to current dimensionality reduction research. In terms of method efficiency, performance, and interpretability, manifold-based parameter-free methods and deep learning-based methods each have distinct strengths and weaknesses, driven by their theoretical and design differences [7]. Parameter-free methods are more efficient for small datasets with lower time costs since they don’t rely on parametric models and focus on optimizing the output [11]. However, their efficiency declines with increasing data size due to the rising complexity of neighborhood search and distance calculations. In contrast, deep learning methods handle large-scale data more efficiently due to model scalability and hardware acceleration, though their training on small datasets is costlier. In terms of performance, parameter-free methods excel at capturing local structures but struggle with complex global hierarchies due to their reliance on Euclidean space [28]. Deep learning methods, by contrast, can capture both local and global structures through multilayer transformations but require significant data and computational resources. Regarding interpretability, parameter-free methods rely on similarity metrics, making them harder to interpret and inconsistent globally. While deep learning methods can capture richer features, their “black-box” nature and complexity make their decision-making process harder to explain. To address these challenges in global structure characterization and interpretability, As shown in Fig. 1, we propose the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI), which integrates hyperbolic mapping and a Mixture of Experts model (MOE) [29, 30]. Hyperbolic mapping uses negative curvature to better capture complex hierarchical structures and global dependencies, preserving global information in lower dimensions. The MOE strategy enhances performance and efficiency by dynamically assigning tasks to specialized expert networks that handle different input features, thereby avoiding bottlenecks from a single model. This model provides interpretability by allowing users to track expert decisions and understand the internal model workings. Additionally, MOE serves as a bridge between raw data, embedded data, and feature subsets, enabling clear interpretation of how features influence data representations at different levels. By combining hyperbolic mapping’s structural advantages and MOE’s efficient task allocation, DMT-HI aims to improve performance, efficiency, and interpretability, offering a comprehensive solution for reducing the dimensionality of complex data. By integrating these innovations, our approach not only improves dimensionality reduction performance but also enhances interpretability, offering a comprehensive solution for handling complex data types and extracting insights from high-dimensional datasets. DMT-HI’s performance in dimensionality reduction is enhanced through the MOE strategy, which dynamically assigns tasks to the most suitable experts, improving both processing efficiency and overall model performance. Additionally, the redesigned manifold loss optimizes the training process, enabling the model to capture both local and global structures more effectively. Overall, the key contributions of this paper include, • A hyperbolic embedding and deep manifold loss function, which improve the accuracy of dimensionality reduction by better capturing the global structure of data. • The introduction of the MOE strategy, establishing a clear connection between input data, embedding results, and key features, thus enhancing model interpretability and stability. • Comprehensive tests evaluating global and local performance, time efficiency, and other dimensions to validate the advantages of the proposed model."
A neural network approach for solving the Monge-Ampère equation with transport boundary condition,"This paper introduces a novel neural network-based approach to solving the Monge-Ampère equation with the transport boundary condition, specifically targeted towards optical design applications. We leverage multilayer perceptron networks to learn approximate solutions by minimizing a loss function that encompasses the equation’s residual, boundary conditions, and convexity constraints. Our main results demonstrate the efficacy of this method, optimized using L-BFGS, through a series of test cases encompassing symmetric and asymmetric circle-to-circle, square-to-circle, and circle-to-flower reflector mapping problems. Comparative analysis with a conventional least-squares finite-difference solver reveals the competitive, and often superior, performance of our neural network approach on the test cases examined here. A comprehensive hyperparameter study further illuminates the impact of factors such as sampling density, network architecture, and optimization algorithm. While promising, further investigation is needed to verify the method’s robustness for more complicated problems and to ensure consistent convergence. Nonetheless, the simplicity and adaptability of this neural network-based approach position it as a compelling alternative to specialized partial differential equation solvers.","The Monge-Ampère equation is a nonlinear partial differential equation (PDE) with crucial applications across various fields in physics and mathematics. Its general form is given by: det(D2⁢u)=f⁢(x,u,∇u),superscript𝐷2𝑢𝑓𝑥𝑢∇𝑢\det\left(D^{2}u\right)=f(x,u,\nabla u),roman_det ( italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u ) = italic_f ( italic_x , italic_u , ∇ italic_u ) , (1) where u:ℝN→ℝ:𝑢→superscriptℝ𝑁ℝu:\mathbb{R}^{N}\to\mathbb{R}italic_u : blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT → blackboard_R, (N≥1𝑁1N\geq 1italic_N ≥ 1), is an unknown convex function, and D2⁢usuperscript𝐷2𝑢D^{2}uitalic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u represents the Hessian matrix of u𝑢uitalic_u. This equation traces its origins back to the 18th-century work of Gaspard Monge, who studied the problem of optimal resource allocation. Over time, this foundational problem has evolved into what is now known as the optimal transport problem, a concept that naturally emerges in fields such as fluid dynamics and mathematical physics. The Monge-Ampère equation effectively describes the optimal transportation of one mass distribution to another, minimizing a cost function that typically represents the distance over which each mass element must be moved 1. Several optical problems can be formulated as instances of optimal transport. A notable example is the design of a reflector that transforms a given light source distribution into a desired target distribution, a problem that inherently aligns with the principles of optimal transport. In this context, the ’mass’ represents energy, while the cost function corresponds to the optical path lengths of the light rays 2. Numerous variations of this problem can be effectively modeled using the Monge-Ampère equation or its generalized forms. For example, the light source may be planar, emitting a parallel beam, or point-based, radiating in multiple directions. Additionally, optical systems can target near-field or far-field regions and may involve point or parallel targets. Both reflectors and lenses can be described within this framework. For example, we might wish to transform a point source to a far-field target using a freeform reflector; a numerical method for solving this problem using the intersection of confocal paraboloids has been described by De Castro et al. 3. For an in-depth exploration of these variations, see Romijn 4 and Anthonissen et al. 5. To simplify our analysis, we will focus on a specific optical configuration: the parallel-to-far-field reflector system. In this setup, a planar light source emits a parallel beam of light toward a reflector, and our primary concern is the distribution of the reflected light at a significant distance from the reflector. Consequently, we only need to consider the direction of each reflected ray. By applying the Monge-Ampère equation with the transport boundary condition and solving it, we can determine the convex reflector surface that transforms a given source light distribution into the desired target distribution. It is important to note that, mathematically, this problem is identical to both the parallel-to-parallel reflector problem and the parallel-to-far-field lens problem, which can also be addressed using the method presented here. This paper introduces a novel numerical method based on artificial neural networks (ANNs) to solve the Monge-Ampère equation with transport boundary condition. Numerous studies have explored the application of neural networks and automatic differentiation to solve ordinary and partial differential equations. Dissanayake and Phan-Thien 6 pioneered this approach, demonstrating the potential of neural networks for approximating solutions to PDEs. Building on this work, Lagaris et al. 7 presented a method using ANNs to solve initial and boundary value problems by constructing trial solutions that inherently satisfy the given conditions. Aarts and Van Der Veer 8 proposed a method to solve PDEs and their boundary/initial conditions using neural networks, incorporating knowledge about the PDE in the structure and training process of the network. More recently, Michoski et al. 9 presented a comprehensive study on the solving of differential equations using deep neural networks, demonstrating their competitiveness with conventional numerical methods and their potential for parameter space exploration and model enrichment. Building on this rich body of work, Nyström and Vestberg 10 employed ANNs to solve the Dirichlet problem for the Monge-Ampère equation. We extend this approach by incorporating the transport boundary condition and compare our neural network-based method against an existing numerical solver for the Monge-Ampère equation with transport boundary condition11. Furthermore, we examine the effect of various hyperparameters of this neural network method on its performance. Section 2 provides a concise background on the Monge-Ampère equation in the context of the parallel-to-far-field reflector problem, a previously proposed finite-differences-based solver for this problem, and artificial neural networks. In Section 3, we present our extension of this method to incorporate the alternative boundary conditions required for optimal transport problems. To demonstrate the effectiveness of our proposed method, Section 4 presents results for several example problems. As is common in machine learning, numerous hyperparameters influence the accuracy of our method. Thus, Section 5 empirically examines the effects of select hyperparameters on our method’s performance. In Section 6 and Section 7, we conclude by discussing the advantages and limitations of neural network-based methods for solving the Monge-Ampère equation with the transport boundary condition, and explore potential avenues for future research to mitigate these limitations."
,,"1 INTRODUCTION Generative models have achieved impressive performance in scientific applications among many other fields (Noé et al.,, 2019; Butter and Plehn,, 2022; Cranmer et al.,, 2020; Sanchez-Lengeling and Aspuru-Guzik,, 2018). Oftentimes, such systems can depend on external control parameters, such as temperature governing the behavior of thermodynamic systems, coupling constants in physical models, or a tempered likelihood posterior in Bayesian inference (Wuttke et al.,, 2014; Friel and Pettitt,, 2008; Pawlowski et al.,, 2017). A major challenge in such cases is acquiring training data for a complete range of control parameters, which can quickly become infeasible. Figure 1: Our approach to train a conditional normalizing flow pθ⁢(x|c)subscript𝑝𝜃conditional𝑥𝑐p_{\theta}(x|c)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_c ). Left: At c=c0𝑐subscript𝑐0c=c_{0}italic_c = italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, the flow is trained using NLL. Right: By learning the gradient of the distribution with respect to c𝑐citalic_c based on prior knowledge, the distribution learned at c0subscript𝑐0c_{0}italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is propagated to other conditions c≠c0𝑐subscript𝑐0c\neq c_{0}italic_c ≠ italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT without additional training data. In this work, we focus on the task of learning generative models in the case where we have access to the functional form of the unnormalized density, such as learning the distributions of equilibrium states in physical systems like molecules (Boltzmann distributions) or variational inference. We approach this problem by learning a single generative model that takes the external condition c𝑐citalic_c as a parameter: pθ⁢(x|c)≈p⁢(x|c)subscript𝑝𝜃conditional𝑥𝑐𝑝conditional𝑥𝑐p_{\theta}(x|c)\approx p(x|c)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_c ) ≈ italic_p ( italic_x | italic_c ). Several works have attempted to address this problem before. One approach applies architectural restrictions to allow one particular functional form of external parameter dependence (Dibak et al.,, 2022). However, this restriction has recently been shown to incur severe limitations in expressivity (Draxler et al., 2024b, ). Energy-based training has been proposed as another method (Schebek et al.,, 2024; Invernizzi et al.,, 2022; Wirnsberger et al.,, 2020), but can exhibit unfavorable properties like mode-seeking behavior (Minka et al.,, 2005; Felardos et al.,, 2023), which has also been shown to be a problem in practice (Wirnsberger et al.,, 2020). Other works require data to be available at the target parameters (Wang et al., 2022b, ; Wirnsberger et al.,, 2020), which can become prohibitively expensive. We overcome these limitations: We allow learning arbitrary continuous dependencies of the target density on external parameters and train unconstrained architectures. Our central idea is to formulate the training of a conditional probability density as a boundary value problem: The boundary is learned on a fixed reference condition c=c0𝑐subscript𝑐0c=c_{0}italic_c = italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and then the resulting density is extrapolated using the known functional dependence on the condition underlying the problem. Our approach is summarized in Fig. 1. In summary, we contribute: • We introduce TRADE, a method for learning generative models with arbitrary continuous dependencies on external conditions. Learning is enabled by a novel boundary value problem formulation. • TRADE uses unconstrained architectures, facilitating the application to more complex target distributions. • TRADE can be trained data-free or with data only available at the boundary c0subscript𝑐0c_{0}italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, making it an efficient approach in cases where acquiring data for a full range of control parameters is infeasible. • TRADE achieves excellent results in a wide range of experiments including Bayesian inference, molecular simulations and physical lattice models"
Measuring memorization through probabilistic discoverable extraction,redacted \correspondingauthorjamhay@google.com,"Memorization of training data, while potentially beneficial for retaining factual information, presents significant challenges in large language models (LLMs) (Biderman et al., 2024; Duan et al., 2024b; Zhang et al., 2024; Huang et al., 2024; More et al., 2024; Smith et al., 2023; Carlini et al., 2022; Bordt et al., 2024; Duan et al., 2024a; Staab et al., 2023; Shi et al., 2023; Tang et al., 2023; Zanella-Béguelin et al., 2020) 111This paper covers a very restricted definition of “memorization”: whether a generative model can be induced to generate near-facsimiles of some training examples when prompted with appropriate instructions. Models do not “contain” bit-wise or code-wise copies of their training data. Rather, if a model can be induced to generate very close copies of certain training examples by supplying appropriate instructions to guide the model’s statistical generation processes then that model is said to have “memorized” those examples. This is an area of active ongoing research.. The undesirable consequences of memorized data can inadvertently expose sensitive information contained within the training set. This issue has garnered significant attention, leading to the now-common practice of quantifying and reporting training data memorization rates within technical reports introducing new foundation large language models (LLMs) (Reid et al., 2024; Anil et al., 2023; Chowdhery et al., 2023; Team et al., 2024; Kudugunta et al., 2024). One way to measure memorization is to quantify how easily a potential attacker could extract training data by querying the model. This often-used measure of memorization called discoverable extraction (Carlini et al., 2022; Kassem et al., 2024) essentially states that a training example is extractable (or “memorized”) if when split into a prefix and suffix, the model generates a sequence matching the suffix when given the prefix as input. Discoverable extraction is often used as an (approximate) upper-bound to an adversary that has no prior knowledge of the example to be extracted (Nasr et al., 2023). Discoverable extraction has become a popular way of measuring memorization rates of LLMs (Reid et al., 2024; Anil et al., 2023; Chowdhery et al., 2023; Team et al., 2024; Kudugunta et al., 2024), in no small part due to its simplicity; one needs only to check if a model’s completion matches an expected target string. While discoverable extraction is cheap to compute, this has its own drawbacks; by only generating a single sequence and checking for a match with the target, it may miss cases where a match could have been found if more than one sequence was generated. These nuances of what is and is not memorized are too coarsely treated by a working definition like discoverable extraction, and leads us to investigate the following question: Question 1. How can we better measure the extractability of sensitive sequences from large language models? Quantifying memorization – and the associated risks – is a subtle and context-specific problem that a single measurement likely cannot capture in isolation. Prior work has attempted to introduce more complex definitions of memorization that aim to get to the heart of what it means for a model to memorize a training example, but they are often too costly to be practically leveraged. These methods have varying levels of computational cost to empirically estimate memorization in the context of LLMs, but are all more expensive than discoverable extraction (see Section 4). Large language models are probabilistic machines, the output of the model is a probability distribution over tokens that make up a model’s vocabulary. The sequence that is generated is entirely dependent on the choice of sampling algorithm that defines how a token is selected from this distribution. There are many different sampling algorithms that one can choose from (see Fan et al. (2018); Holtzman et al. (2019); Basu et al. (2020); Vijayakumar et al. (2016); Graves (2012); Boulanger-Lewandowski et al. (2013)). Prior works have focused on greedy sampling, which iteratively selects the next token with the largest probability conditioned on the previous tokens. Although greedy sampling selects the most likely next token it may not select the overall most likely sequence; yet, the difference between discoverable extraction rates under greedy and other sampling schemes like beam search was found to be marginal (Carlini et al., 2022). However, users of production large language models are often free to decide which sampling scheme to use. These observations lead to our second research question: Question 2. How does the user-chosen sampling scheme affect extractability rates? Carlini et al. (2022) argue that focusing on sampling schemes that have higher degrees of associated randomness compared to greedy sampling, are “antithetical” to maximizing discoverable extraction, as sampling schemes that encourage diversity in sequence generation will by definition have a higher variation in the sequences that can be sampled. However, higher sequence diversity may be advantageous for extraction given that users may query the model multiple times. Extracting the secret even once out of multiple queries could be highly problematic as the adversary (say a hacker checking credit card numbers) may have external means of verifying which one is correct. It is reasonable to try and quantify the number of sequences that need to be generated before a target example becomes extractable, as this better aligns with how users could interact with a model. In particular, because production language models are deployed with non-greedy based sampling strategies that, by default, encourage diversity in sampled sequences. This idea is not a new one. Carlini et al. (2019) motivate their measure of canary memorization using rank perplexity by considering an adversary who sequentially guesses the potential canaries in order from lowest to highest perplexity. The rank of the true canary measures how many guesses such an adversary would need to make before guessing correctly. While the secret sharer attack only guesses a single canary, it is natural to consider how extraction rates scale with multiple guesses. We find that even when we measure the extraction probability after multiple guesses, the extraction rates on training data remain significantly higher than extraction rates on test data (see Section 5.2). This observation allows us to reason about the absolute risk of extraction as well as the relative risk. In this work, we introduce a probabilistic relaxation of discoverable extraction that resolves the discussed points of tension. This new definition quantifies the number of attempts n𝑛nitalic_n an adversary would need to make to extract a target with a certain probability p𝑝pitalic_p under a given sampling scheme. This provides a more nuanced quantification of memorization, which alleviates the aforementioned drawbacks of discoverable extraction without incurring any additional computational cost compared to discoverable extraction. Our contributions We propose a simple probabilistic definition of extraction called (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction that captures the risk of extracting a target after sampling n𝑛nitalic_n times from an arbitrary sampling scheme. We thoroughly benchmark (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction rates for different sampling schemes, settings of n𝑛nitalic_n and p𝑝pitalic_p, model sizes, and number of target data repetitions, and we make a number of remarkable empirical findings that demonstrate the utility of our definition: • Greedy extraction underestimates training data memorization rates compared to (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction even for modest values of n𝑛nitalic_n and p𝑝pitalic_p (Section 5.1). Moreover, the discrepancy between the two rates increases for larger models and more repetitions of the target data. (Section 5.3) • At every setting of the definition parameters we tried, extraction rates of training data far exceeded baseline extraction rates on test data (Section 5.2). • We show that (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction provides a better comparison of memorization rates across models trained on the same data compared with greedy extraction (Section 5.1). Along the way, we provide extensive discussion how our definition relates to other definitions of memorization (Section 3 and Section 4)."
Improving Inverse Folding for Peptide Design with Diversity-regularized Direct Preference Optimization,"Inverse folding models play an important role in structure-based design by predicting amino acid sequences that fold into desired reference structures. Models like ProteinMPNN, a message-passing encoder-decoder model, are trained to reliably produce new sequences from a reference structure. However, when applied to peptides, these models are prone to generating repetitive sequences that do not fold into the reference structure. To address this, we fine-tune ProteinMPNN to produce diverse and structurally consistent peptide sequences via Direct Preference Optimization (DPO). We derive two enhancements to DPO: online diversity regularization and domain-specific priors. Additionally, we develop a new understanding on improving diversity in decoder models. When conditioned on OpenFold generated structures, our fine-tuned models achieve state-of-the-art structural similarity scores, improving base ProteinMPNN by at least 8%. Compared to standard DPO, our regularized method achieves up to 20% higher sequence diversity with no loss in structural similarity score.","Engineering biopolymers that fold into desired 3D structures, a computational challenge known as inverse protein folding problem, has broad applications in drug discovery and material science (Yang et al., 2023; Dill et al., 2008; Abascal & Regan, 2018). Several approaches for inverse folding have been adopted over the past decades, from molecular dynamics simulations to machine learning approaches (Dauparas et al., 2022b; Shanker et al., 2023; Hsu et al., 2022a; Yi et al., 2023; Correa, 1990). In the standard machine learning approach, a molecular backbone chain serves as input, and a model generates sequences that adopt folding topologies compatible with the reference backbone. Sequences do not necessarily share sequence homology, as multiple diverse sequences can fold into similar structures (Hsu et al., 2022a; Yue & Dill, 1992; Godzik et al., 1993). Peptides, which are small biopolymers comprising 2-50 residues, are interesting targets for inverse folding given their role in diverse biological functions, acting as hormones, neurotransmitters, signalling molecules, or nanostructures assemblers (Chockalingam et al., 2007; Torres et al., 2019; Copolovici et al., 2014; Ulijn & Smith, 2008). Only about 225,000 protein structures have been experimentally determined111Updated figures available at https://www.rcsb.org/stats/growth/growth-released-structures. and made available via the Protein Data Bank (PDB). Training inverse-folding machine learning models in a supervised fashion is a challenging task, due to the complexity of the problem and the limited amount of experimental data. The challenge is aggravated in the peptide domain as fewer than 3.53.53.53.5% PDB structures contain 50 residues or less. In fact, applying the SCOP classification filter in the PDB to display structures labelled as “Peptide” reveals only 509 entries, circa 0.20.20.20.2% of all experimentally determined structures available. In addition to the lack of data, sequences are subject to composition bias. The incidence of certain amino acids may differ depending on the sequence length , as longer proteins have more options for accommodating multiple secondary structures and folding loops (Tiessen et al., 2012). Popular models like ProteinMPNN (Dauparas et al., 2022b), PiFold (Gao et al., 2022) and ESM-IF1 (Hsu et al., 2022b) are primarily trained on data derived from longer proteins, leading to poor performance for peptide design tasks. Additionally, shorter sequences fold into simpler structures. Indeed, Milner-White & Russell (2008) argue that short peptides are notoriously “structureless” and tend to flicker between conformations. For example, a structural conformation of a single alpha helix or beta sheet – or a combination of two or three of them – is not necessarily stable and can fluctuate. Existing inverse folding models optimize sequence residue-recovery accuracy and structural similarity via template modeling (TM) score (Zhang & Skolnick, 2004). However, they often suffer from low sampling diversity (Gao et al., 2023b). Ideally, the inverse folding model generates maximally diverse candidate sequences, as additional design filters, such as synthesizability and thermal stability, reduce the number of potential hits downstream of the design process. To address these issues, we apply Direct Preference Optimization (DPO) (Rafailov et al., 2023), a fine-tuning method, to improve inverse-folding methods for peptide design. To the authors’ knowledge, we are the first to apply DPO to this task. We propose several enhancements to DPO to address specific problems that arise in inverse folding. Particularly, we forward fold generated sequences and derive an online regularized algorithm for optimizing structural similarity to the reference and sequence diversity simultaneously. We show empirically that this algorithm targets the differential entropy in log-probability space. Furthermore, we present a simple reward scaling approach to incorporate scalar reward information, showing that reward scaling adaptively selects a KL divergence penalty (Kullback, S. and Leibler, R. A., 1951) to improve performance on harder structures."
LOCAL: Learning with Orientation Matrix to Infer Causal Structure from Time Series Data,"Discovering the underlying Directed Acyclic Graph (DAG) from time series observational data is highly challenging due to the dynamic nature and complex nonlinear interactions between variables. Existing methods often struggle with inefficiency and the handling of high-dimensional data. To address these research gap, we propose LOCAL, a highly efficient, easy-to-implement, and constraint-free method for recovering dynamic causal structures. LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic DAG equivalent to the ground truth. On this basis, we propose two adaptive modules for enhancing the algebraic characterization of acyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and Dynamic Graph Parameter Learning (DGPL). ACML generates causal masks using learnable priority vectors and the Gumbel-Sigmoid function, ensuring the creation of DAGs while optimizing computational efficiency. DGPL transforms causal learning into decomposed matrix products, capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Extensive experiments on synthetic and real-world datasets demonstrate that LOCAL significantly outperforms existing methods, and highlight LOCAL’s potential as a robust and efficient method for dynamic causal discovery. Our code will be available soon.","Exploration of the underlying causal generation process of dynamic systems is an important task (Cheng et al., 2024a; Gong et al., 2024) for trustworthy machine learning. Unfortunately, it is unethical, impossible due to technical reasons, or expensive to conduct intervention experiments on the dynamic systems of certain domains (Li et al., 2023a; Cai et al., 2023a). Another challenge is to infer about the structure which may be high dimensional and nonlinear. Some recent works (Pamfil et al., 2020; Sun et al., 2023; Gao et al., 2022; Fan et al., 2023) have made significant efforts by employing dynamic Bayesian networks (DBNs) with observational and interventional data: among dynamic systems, as illustrated in Figure 1. The variable xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at timestep t𝑡titalic_t is affected by which variables xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT at the same time step (instantaneous dependency) and which variables xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT at the previous timestep (lagged dependency)? This question highlights the crucial roles of those algorithms in the interpretable performance of the trained models. Figure 1. Illustration of instantaneous dependency (solid lines) and lagged dependency (dashed lines) dependencies in a DBN with d=3𝑑3d=3italic_d = 3 nodes and autoregression order p=2𝑝2p=2italic_p = 2. For clarity, we display edges that do not influence the variables at time t𝑡titalic_t in a lighter shade. In order to study the nonparametric DBN, DYNOTEARS (Pamfil et al., 2020) (i.e., a score-based approach to learning DBNs with a differentiable DAG constraint (Zheng et al., 2018)) was proposed as a proxy to capture the learn the parents of child variables. However, in Section 4.2, our practical analysis shows that the DYNOTEARS algorithm and its extensions (Sun et al., 2023; Gao et al., 2022; Fan et al., 2023) adopting matrix exponential constraints require an extremely long time to optimize in high-dimensional dynamic systems, even if they smartly adopt interventional data to enhance the identifiability (Li et al., 2023b, 2024). Then, it is natural to need a proxy model that can fasterly infer dynamic causal structures in high-dimensional situations, which is also the main goal of our work. Recently, much of the research on dynamic causal structure learning has concentrated on applying soft sparsity and DAG constraints. For instance, Golem (Ng et al., 2020) formulated a likelihood-based score function for handling the causality of thousands of nodes. Yu et al. (Yu et al., 2023) extended it for recovery dynamic causal structure. Concurrently, (Fang et al., 2024) verified the feasibility of further accelerating the learning of DAG based on this likelihood function both theoretically and experimentally. These approaches are aimed at enhancing flexibility and scalability in high-dimensional and circumventing rigid structural constraints. Building on these insights, we propose a novel framework for Learning with Orientation matrix to infer CAusaL structure from time series data, which we call LOCAL. We develop a quasi-maximum likelihood-based dynamic structure learning method with identifiability guarantee. Powered by this quasi-maximum likelihood-based objective, we propose to enhance the algebraic characterization of acyclicity with two adaptive modules for causal structure recovering task: 1) an Asymptotic Causal Mask Learning (ACML) module which leverages learnable priority vectors (𝒑𝒑\boldsymbol{p}bold_italic_p) and the Gumbel-Sigmoid function to generate causal masks, ensuring the creation of directed acyclic graphs (DAGs) while optimizing computational efficiency; 2) a Dynamic Graph Parameter Learning (DGPL) module to transform causal learning into decomposed matrix products (𝑾=𝑬s⁢𝑬tT𝑾subscript𝑬𝑠subscriptsuperscript𝑬𝑇𝑡\boldsymbol{W}=\boldsymbol{E}_{s}\boldsymbol{E}^{T}_{t}bold_italic_W = bold_italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_italic_E start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT), effectively capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Those leads us can exploit faster gradient-based optimization, such as Adam (Kingma and Ba, 2015), and GPU acceleration. Contribution. The main contributions of this paper are as follows: • To the best of our knowledge, LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic causal structure and shows that more robust and accurate. • We proposed two adaptive modules, ACML and DGPL, which further liberate the matrix exponential operations required for causality. • We conducted extensive experiments on several synthetic datasets and real-world benchmarks, which proved that LOCAL outperforms state-of-the-art by a significant margin."
Accelerating AI Performance usingAnderson Extrapolation on GPUs,"We present a novel approach for accelerating AI performance by leveraging Anderson extrapolation, a vector-to-vector mapping technique based on a window of historical iterations. By identifying the crossover point where a mixing penalty is incurred, the method focuses on reducing iterations to convergence, with fewer more compute-intensive but generally cacheable iterations, balancing speed and memory usage with accuracy and algorithmic stability, respectively. We demonstrate significant improvements, in both training and inference, motivated by scalability and efficiency extensions to the realm of high-performance computing (HPC).","Anderson extrapolation [1, 2, 27, 33, 16] has recently been applied to deep equilibrium models (DEQs) [7, 8, 9, 10, 24, 17]. Kolter et al. [34] found the gains not substantial due to early termination with a loose convergence tolerance. They focused on Anderson extrapolation during training. Here, we show significant acceleration of AI performance with Anderson on GPUs for both the forward pass (running inferences faster) and training (generating models faster). We demonstrate acceleration of the forward pass with standard Anderson as a baseline for future work with stochastic variants [30] and accelerating the backward pass with Jacobian-free methods like Jacobian-Free Backpropagation (JFB) and Neumann series gradient approximations [16]. Figure 2: AI carbon footprint projected to consume >2% of global electricity demand [3, 15, 28, 25], amounting to >10% of global electricity demand for data centers and infrastructure. As AI demand grows, as shown in Fig. 2 [3, 15, 28, 25], high-performance computing (HPC) is becoming critical due to economic pressures from the growth of data and AI infrastructure [29]. Low-memory acceleration techniques, like Anderson extrapolation, will be key to increasing HPC-based AI computational efficiency. This study investigates matrix-free Anderson extrapolation on GPUs, emphasizing gains from advanced computing architectures compared to CPUs. Our goal is to maximize computational efficiency while reducing iterations to convergence by reusing previous iterations to avoid unnecessary gradient calculations, gaining benefits expected from second-order methods (e.g., [32]) without manipulating Hessian matrices. The environmental impact of AI is rapidly growing [3, 15, 28, 25]. By 2030, AI is projected to account for 2% of global electricity consumption. We aim to reduce this impact by up to 90%, saving 160 terawatt-hours per year by 2030. The carbon footprint of AI exceeds the 500-megaton annual benchmark set by initiatives like Bill Gates’ Breakthrough Energy [14]. Efficiency-enhancing technologies like GPU and Anderson acceleration can reduce AI’s carbon emissions by 60 gigatons per year by 2030, as shown in Fig. 2. 1.1 Leveraging extrapolation for AI and HPC advances Anderson extrapolation, a windowing technique for accelerating nonlinear fixed point iterations, is widely applied in fields like density functional theory, kinetic theory, and climate spin-up. It is well-suited for distributed memory parallelization and GPU implementation. It is a staple of major open-source large-scale solver libraries, including PETSc [11, 12], SUNDIALS [23], Trilinos [19, 22, 21, 20], and deal.II [13, 4, 5, 6]. It can be applied to machine learning training, smoothing out standard forward iterations and achieving superior accuracy in training and testing error. Benchmarking results on CIFAR10 show expected robustness benefits and allow characterization of the temporal advantages or disadvantages from the higher cost per iteration, where a small residual minimization step is applied at each new function evaluation. Figure 3: Mathematical formulation and vector representation. Adapted from Y. He & H. De Sterck. ""Linear Asymptotic Convergence Analysis of Anderson Acceleration, with Krylov Formulation in the Linear Case"" Copper Mountain Conference (2022), ICERM Workshop (2023). Available at: https://www.bilibili.com/video/BV1Wa411i77y/ and https://icerm.brown.edu/video_archive/?play=3320 Figure 4: Deep equilibrium neural network model architecture (Source: NeurIPS Tutorial, 2020 [34]). f⁢(z,x)=norm⁢(ReLU⁢(z+norm⁢(x+W2∗(norm⁢(ReLU⁢(W1∗z))))))𝑓𝑧𝑥normReLU𝑧norm𝑥subscript𝑊2normReLUsubscript𝑊1𝑧f(z,x)=\mathrm{norm}(\mathrm{ReLU}(z+\mathrm{norm}(x+W_{2}*(\mathrm{norm}(% \mathrm{ReLU}(W_{1}*z))))))italic_f ( italic_z , italic_x ) = roman_norm ( roman_ReLU ( italic_z + roman_norm ( italic_x + italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∗ ( roman_norm ( roman_ReLU ( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∗ italic_z ) ) ) ) ) ) ""norm"" here is a group norm, representing a statistical normalization [31]. 1.2 Balancing memory and stability Fundamental tradeoffs exist between memory capacity, memory bandwidth, communication cost, and algorithmic characteristics of stability and convergence rate. The tradeoffs are generally resolved to minimize time to solution. GPUs attain high memory bandwidth advantages over CPUs at the cost of smaller memory capacity. Anderson extrapolation promotes fewer, more expensive steps, reusing cached state-vector data. In distributed memory implementations, it produces convergence with fewer interprocessor communication steps. It has tuning parameters such as window size and damping that can be tuned to application and architecture. We are assessing its utility in machine learning more broadly at a time of emergent CPU-GPU superchips. 1.3 Deep equilibrium neural network models Deep equilibrium models (DEQs) are the continuum limit of explicit neural networks as the number of layers approaches infinity [26], approximating many explicit layers with a single, implicit layer with exponentially fewer parameters using a backward pass including the output. This reduces the inverse problem in parameter space to a fixed-point iteration problem, enabling the usage of nonlinear, vector-to-vector mapping techniques to compute the fixed-point iterations that converge to the deep equilibrium state parameters by minimizing the loss function. With gains in memory and acceleration, DEQs are fit for large-scale computer vision and natural language processing tasks and benefit more from matrix-vector operation-optimized computing architectures like GPUs and CPU-GPU superchips. The standard approach using forward iteration for fixed-point iteration problems often does not efficiently converge to the fixed point and suffers from initially slow error reduction and local minimum trapping in nonlinear problems like deep neural networks. Vector-to-vector mapping techniques like Anderson extrapolation outperform standard forward iteration by combining information from previous iterations to span a searchable subspace to extrapolate the next iteration, enhancing convergence rates at the expense of memory usage in each iteration. DEQs represent any neural network at arbitrary depths and connectivities with a single implicit layer consuming vastly fewer parameters with faster forward passes for accelerated training and inferences. The implicit function theorem shows how gradients can be computed in the DEQ framework, facilitating backpropagation through the equilibrium state [8, 34]. DEQs provide a framework for accelerating deep learning, extending the capacity of deep networks within a single-layer architecture through fixed-point computations and advanced root-finding algorithms. Their amenability to convergence acceleration with techniques like Anderson positions DEQs as a robust method to reduce computation needed to build state-of-the-art models and scale up beyond current computational limitations."
Computational Bottlenecks of TrainingSmall-scale Large Language Models,"While large language models (LLMs) dominate the AI landscape, Small-scale large Language Models (SLMs) are gaining attention due to cost and efficiency demands from consumers. However, there is limited research on the training behavior and computational requirements of SLMs. In this study, we explore the computational bottlenecks of training SLMs (up to 2B parameters) by examining the effects of various hyperparameters and configurations, including GPU type, batch size, model size, communication protocol, attention type, and the number of GPUs. We assess these factors on popular cloud services using metrics such as loss per dollar and tokens per second 111We use average dollar cost ratios of cloud instance types based on publicly available pricing (Appx. A).. Our findings aim to support the broader adoption and optimization of language model training for low-resource AI research institutes.","Large Language Models (LLMs) are becoming increasingly popular in various fields due to their performance on a variety of tasks [6, 18, 8, 20, 5]. However, deploying large models widely such as on mobile hardware and edge devices is challenging due to the large memory and compute requirements [11, 12, 10]. These constraints have driven a growing interest in smaller language models (such as ≤2⁢Babsent2𝐵\leq 2B≤ 2 italic_B parameters) as a viable alternative [24, 16, 23]. Recent work refer to these models as Small-scale large Language Models (SLMs) which can work well in environments where cost-efficiency and resource limitations are of significant concern, as well as on servers where the reduced cost of inference will be a dominant factor to attract and retain customers. SLMs have demonstrated substantial potential in achieving competitive results despite their smaller size. Techniques such as pruning, distillation, and quantization have been employed to enhance their performance [2, 3, 17], allowing SLMs to perform on par with, and in some cases surpass, much larger models [4]. For example, Gemma-2B outperforms the largest OPT-175B [25], challenging the notion that sheer model size is the primary determinant of effectiveness. In addition to on par accuracy, SLMs can meet consumer demands for fast, efficient, and cost-effective AI without sacrificing task performance, making them increasingly attractive for organizations with limited computational budgets, such as small businesses and academic institutions. While prior work mostly focused on optimizing SLMs for inference [15], relatively little attention has been paid to their training dynamics. This gap is significant, as the computational and infrastructure demands of training LLMs may not translate to SLMs. Given the diverse range of hardware configurations available on cloud platforms—such as GPU type, batch size, and communication protocols—there is a need for a systematic analysis of how these factors impact the training efficiency of SLMs, particularly when measured in terms of practical metrics such as loss per dollar and tokens per second. Our findings indicate that for smaller models, more affordable options like A100-40GB GPUs and Distributed Data Parallel (DDP) can be utilized without sacrificing performance. For larger models, advanced configurations, such as A100-80GB and H100-80GB GPUs paired with Flash Attention (FA) and Fully Sharded Data Parallel (FSDP), are necessary to handle larger batch sizes and prevent memory-related issues. Recent advancements in the field underscore the importance of scaling AI systems not only for state-of-the-art performance but also for practical applications in real-world environments. The emerging trend toward SLMs suggests that a re-evaluation of hardware and computation strategies is essential. The contribution of this paper is to address the need for such evaluation, providing a systematic study on the computational bottlenecks and cost-efficiency of training SLMs up to 2B parameters on various cloud infrastructure and setups. We find that 1) FlashAttention is significantly more important for SLMs than LLMs, 2) Expensive hardware, e.g., H100-80GB and A100-80GB, is not necessarily cost effective for SLM training, 3) DDP is the best distributed training scheme for SLMs, and 4) Maximizing GPU memory utilization is not cost-optimal for SLM training."
Gradient Descent Efficiency Index,"Gradient descent is a widely used iterative algorithm for finding local minima in multivariate functions. However, the final iterations often either overshoot the minima or make minimal progress, making it challenging to determine an optimal stopping point. This study introduces a new efficiency metric, Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, designed to quantify the effectiveness of each iteration. The proposed metric accounts for both the relative change in error and the stability of the loss function across iterations. This measure is particularly valuable in resource-constrained environments, where costs are closely tied to training time. Experimental validation across multiple datasets and models demonstrates that Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT provides valuable insights into the convergence behavior of gradient descent, complementing traditional performance metrics. The index has the potential to guide more informed decisions in the selection and tuning of optimization algorithms in machine learning applications and be used to compare the “effectiveness” of models relative to each other.","In the field of machine learning, optimizing the training process of models is crucial for achieving high performance while minimizing computational resources. Gradient descent [1] is a widely used optimization algorithm due to its simplicity and effectiveness in finding local minima of differentiable functions. However, the efficiency of gradient descent can diminish with large datasets and prolonged training periods, where additional iterations provide negligible improvements. This raises the need for a robust mechanism to identify the optimal stopping point, ensuring efficient use of computational resources. Fig. 1 is a contour plot that illustrates how each step taken in gradient descent towards a local minimum is smaller than the previous one. This approach quickly returns diminishing results, making the last few steps cost more computationally than they yield in accuracy. Figure 1: Contour plot of cost b,w with path of gradient descent J⁢(w,b)𝐽𝑤𝑏J(w,b)italic_J ( italic_w , italic_b ). In Fig. 2, notice how the first 100 steps exponentially decrease (or logarithmically increase) the cost. However, if you zoom out to 100,000 steps, the curve effectively flattens out before 10,000 steps in this particular example. Figure 2: Comparison of cost with different domain restrictions. The “Gradient Descent Efficiency Index” is a novel ratio between training parameters that includes the relative change in gradient norm, the initial learning rate, the learning decay rate, absolute change in coefficients, and the number of iterations. Note that throughout this paper, I will use the name “Gradient Descent Efficiency Index”, the short form “GDEI”, and the function Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT interchangeably."
Generative Diffusion Models for Sequential Recommendations,Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).,"Recommender systems are algorithms that suggest items to users by analyzing various forms of input data. Their primary goal is to enhance the customer experience through personalized recommendations, often based on prior implicit feedback. These systems track user behaviors, such as purchase history, viewing habits, and browsing activity, to model user preferences. Sequential Recommendation, a specific type of recommendation, is particularly relevant for applications where user behavior is naturally sequential. It focuses on predicting the next item a user will interact with by considering the order of previous interactions. Mainstream solutions to Sequential Recommendation (SR) [2] represent items with fixed vectors, which have a limited ability to capture the latent aspects of items and the diversity of user preferences. Generative models like Generative Adversarial Networks (GANs) [3] and Variational Auto-Encoders (VAEs) [4] have been widely applied in personalized recommendations, using adversarial training and encoder-decoder architectures, respectively, to model user behavior and preferences. However, Diffusion Models have shown significant advantages over GANs and VAEs, such as greater stability and higher generation quality in various tasks. Diffusion Models (DMs) [5, 6, 7] have achieved state-of-the-art results in image synthesis tasks [7, 8, 9, 10, 11]. These models alleviate the trade-off between stability and quality by gradually corrupting images in a forward process and iteratively learning to reconstruct them. DMs progressively corrupt 𝐱0subscript𝐱0\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with random noise and then recover 𝐱0subscript𝐱0\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT from the corrupted state 𝐱Tsubscript𝐱𝑇\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT step by step. This forward process creates a tractable posterior [6], enabling the iterative modeling of complex distributions through flexible neural networks in the reverse generation process. The objectives of recommender models align well with DMs, as recommendation essentially involves inferring future interaction probabilities from noisy historical interactions, where noise represents false-positive and false-negative items [12, 13]. This makes DMs a promising approach for accurately modeling complex interaction patterns with strong representational ability. Despite their success in other domains, applying diffusion models to recommender systems remains underexplored. We further explore diffusion models for sequential recommendation (SR) by extending the method introduced by Li et al. (2023). Our work proposes significant enhancements to the existing architecture, resulting in a new model, DiffuRecSys111https://youtu.be/bEpDfAAGL2I. Specifically, our contributions are as follows: • Enhancing the Diffusion Recommender Model: We incorporate cross-attention mechanisms within the Approximator of the model architecture. The model isn’t just learning temporal dependencies (the sequential order of items) but also more complex relationships between past interactions and the target item by focusing on the most relevant past events. • Incorporation of Offset Noise: We introduce offset noise into the diffusion process to increase model robustness and effectively handle variability in user interactions. • Comprehensive Experimental Validation: We conduct extensive experiments across three datasets under various settings, demonstrating improvements of DiffuRec with our extensions over standard baselines."
,,"Deep generative models – for example VAEs, GANs, normalizing flows, diffusion models, and flow matching – have recently made great progress in probability density learning (DL), with flow matching achieving highest accuracy at the moment. Besides generative accuracy, human interpretability of the learned representation is highly desirable, and disentangled representation learning (DRL) is a key tool for this, see Bengio et al., (2014) and Wang et al., (2024). Intuitively, DRL means that each latent variable should effect only a single, distinct semantic property of the generated data instances. We consider the problem of measuring if and to what degree a given model has actually learned a disentangled representation. Most prior work addresses this question in a supervised setting, where the true generative factors are known (see Related Work). Since this assumption is often violated in the real world, we instead focus on the unsupervised case. That is, we do not ask if the model has learned the (unknown) true factors, but only if it has learned any disentangled representation at all. The learned representation might be close to the true one, if certain identifiability conditions are fulfilled (see Related Work), but this is beyond the scope of our paper. Our work rests on the manifold hypothesis which states that data in a D𝐷Ditalic_D-dimensional space often reside near a manifold ℳℳ\mathcal{M}caligraphic_M of much lower dimension d≪Dmuch-less-than𝑑𝐷d\ll Ditalic_d ≪ italic_D. Variations along the manifold correspond to semantically important differences between data instances, whereas off-manifold variations are considered as unimportant or noise. This is familiar from PCA, where one interprets directions of high data variability as important, whereas directions of low variability are irrelevant. PCA achieves this under the assumption that the manifold ℳℳ\mathcal{M}caligraphic_M is a linear subspace, and DRL seeks to generalize this to non-linear models. If the important dimensions indeed span the manifold ℳℳ\mathcal{M}caligraphic_M, the representation is called aligned. Our method clearly highlights Alignment by assigning a high manifold entropy to the important features and low to unimportant ones, see fig. 1. Moreover, it allows sorting of latent variables by importance so that the cut-off between important and irrelevant can be adjusted later according to the needs of an application, analogous to PCA’s ordering by variance. Figure 1: The two moons distribution illustrates how manifold entropic metrics quantify DRL in terms of alignment and disentanglement. (Top left) The latent prior and a Cartesian grid spanned by the latent variables Zcsubscript𝑍𝑐Z_{c}italic_Z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and Zdsubscript𝑍𝑑Z_{d}italic_Z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue). The latent distribution is mapped to data space by three generative models with equal accuracy, but vastly different representations. This can be seen by the differences in the transformed grid spanned by the manifold random variables 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and 𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue) in the top row, and the corresponding values of our metrics manifold entropy H⁢(𝑿c)𝐻subscript𝑿𝑐H({\boldsymbol{X}}_{c})italic_H ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ), H⁢(𝑿d)𝐻subscript𝑿𝑑H({\boldsymbol{X}}_{d})italic_H ( bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) and manifold mutual information ℐ⁢(𝑿c,𝑿d)ℐsubscript𝑿𝑐subscript𝑿𝑑\mathcal{I}({\boldsymbol{X}}_{c},{\boldsymbol{X}}_{d})caligraphic_I ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) in the bottom row. The total entropy of the distribution (gray) is the signed sum of the three terms. (A) The latent manifolds are entangled (and thus not interpretable), and our metric indicates this by high mutual information (brown). (B) The latent manifolds are locally orthogonal everywhere and have low mutual information. However, alignment is inconsistent (𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT aligns with the upper moon, 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT with the lower), resulting in comparable manifold entropy of both variables. (C) The representation is disentangled and aligned. The manifold entropy is high for the important variable 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and low for the noise variable 𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue), and their mutual information is small. Disentanglement, i.e. the statistical independence between latent factors, has been addressed by Independent Component Analysis (ICA, Comon,, 1994) under the assumption of a linear data-generating process (DGP), 𝒙=𝑨⁢𝒔𝒙𝑨𝒔{\boldsymbol{x}}=\boldsymbol{A}\boldsymbol{s}bold_italic_x = bold_italic_A bold_italic_s. When linearity holds, the true generative factors 𝒔𝒔\boldsymbol{s}bold_italic_s are identifiable if they are independent and follow a non-Gaussian distribution. However, identifiability is generally lost for non-linear DGPs 𝒙=Φ⁢(𝒔)𝒙Φ𝒔{\boldsymbol{x}}=\Phi(\boldsymbol{s})bold_italic_x = roman_Φ ( bold_italic_s ), (Hyvärinen and Pajunen,, 1999). Finding conditions on 𝒔𝒔\boldsymbol{s}bold_italic_s to restore identifiability is a major focus of current research, see (Hyvarinen et al.,, 2023) for a recent survey. Alternatively, one can restrict the class of permitted mixing functions ΦΦ\Phiroman_Φ, and this approach primarily inspired the present work. Independent Mechanism Analysis (IMA, Gresele et al.,, 2022) postulates, for example, that the Jacobian of ΦΦ\Phiroman_Φ should be orthogonal in every point, and Principal Component Flows (Cunningham et al., 2022b, ) realize this idea by adding a loss term to normalizing flow training that encourages orthogonality. It can be shown that orthogonality of the Jacobian is equivalent to minimizing the mutual information between the image of the corresponding features in data space after mapping them through the decoder (see fig. 1 and Appendix). This is crucial: In contrast to supervised disentanglement metrics, which are usually defined with respect to the encoder, meaningful unsupervised metrics must be defined in terms of the decoder mapping from latent to data space. Specifically, we make the following contributions: • We propose a set of information-theoretic metrics for DRL defined on the decoder of a generative model. • We introduce Alignment as an important complementary condition to Disentanglement in the IMA framework • We show the usefulness of our metrics at dissecting generative models in order to reveal and quantitatively measure their behaviours."
Robust Time Series Causal Discovery for Agent-Based Model Validation,"Agent-Based Model (ABM) validation is crucial as it helps ensuring the reliability of simulations, and causal discovery has become a powerful tool in this context. However, current causal discovery methods often face accuracy and robustness challenges when applied to complex and noisy time series data, which is typical in ABM scenarios. This study addresses these issues by proposing a Robust Cross-Validation (RCV) approach to enhance causal structure learning for ABM validation. We develop RCV-VarLiNGAM and RCV-PCMCI, novel extensions of two prominent causal discovery algorithms. These aim to reduce the impact of noise better and give more reliable causal relation results, even with high-dimensional, time-dependent data. The proposed approach is then integrated into an enhanced ABM validation framework, which is designed to handle diverse data and model structures. The approach is evaluated using synthetic datasets and a complex simulated fMRI dataset. The results demonstrate greater reliability in causal structure identification. The study examines how various characteristics of datasets affect the performance of established causal discovery methods. These characteristics include linearity, noise distribution, stationarity, and causal structure density. This analysis is then extended to the RCV method to see how it compares in these different situations. This examination helps confirm whether the results are consistent with existing literature and also reveals the strengths and weaknesses of the novel approaches. By tackling key methodological challenges, the study aims to enhance ABM validation with a more resilient valuation framework presented. These improvements increase the reliability of model-driven decision making processes in complex systems analysis.","Chapter 1 Introduction 1.1 Agent-Based Models: An Overview In the modern era, Agent-Based Models (ABMs) fall under the class of modelling and simulation techniques which are increasingly being used in domains such as theoretical economics, finance, social sciences, and epidemiology. It involves a bottom-up approach by putting together individual agents and their interactions so that various phenomena may be synthesised and then examined. The detailed methodologies of such models allow them to understand complexity and draw futuristic conclusions. Recent literature stresses the significant advantages of agent-based models (ABMs) over traditional economic models. Fagiolo (2019) pinpoints the major strengths of ABMs: their capacity to provide comprehensive narratives of interactions among agents with network structures, incomplete information learning processes, and competition in imperfect markets—and the flexibility they provide in validating both model inputs and outputs [1]. This characteristic has gained much attention and prompted much research activity recently. 1.2 The Importance of ABM Validation To successfully implement ABMs in reality, the “validation” of this model could be the decisive factor for its ability to truly reflect real-world reality. The validation process consists of comparing the model output with the actual data obtained in the real world to make sure it is reliable and effective. This process is utilised to establish the credibility of the model and the reliability of the predictions made. Validation is particularly challenging in fields with complex interactions and non-linear dynamics, such as finance. As Windrum et al. (2007) pointed out, significant effort is still needed to realise consistent and satisfactory techniques of ABM method implementation to real-world financial data [2]. A key component in ABM validation is to find the cause-and-effect mechanisms from data. Besides highlighting the importance of correlational testing, causal matching between the ABM outputs and real-world data has recently been emphasized in validation. These approaches aim to understand and explain the origins and propagation of observed phenomena in financial systems [3]. The details of such causal discovery methods and their application in ABM validation will be further discussed in the following chapters. 1.3 Challenges to Address Indeed, in spite of continuous progress in the ABM validation techniques, there are still the most significant challenges in such complex systems applications: 1. Insufficient Robustness of Time Series Causal Discovery Methods: The causal discovery approaches that are commonly used today, such as VAR-LiNGAM and PCMCI, can be quite susceptible to noise and variations in the data. This could create inconsistencies when the method is applied to different subsets of the same dataset or datasets with slightly different characteristics. For ABM validation purposes, the robustness of these techniques should be enhanced. False causality in this regard could be as harmful as wrong conclusions drawn up about the ABM system’s validity and, consequently, about the underlying system mechanisms. 2. Lack of Comprehensive Understanding of Dataset Characteristics’ Impact: While previous studies have examined specific dataset characteristics, there is a lack of comprehensive understanding of how various dataset properties collectively affect the performance of causal discovery methods in ABM validation. The absence of a systematic comparison across a wide range of dataset types (e.g., linear vs. non-linear, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary, sparse vs. dense causal structures) hinders our ability to select appropriate validation techniques and interpret results accurately. This deficiency stands as a huge constraint in the area of creating functional ABM validation processes as a solution to the wide spectrum of complicated problems. 3. Limitations in Existing ABM Validation Frameworks: Even though much progress has been made in ABM Validation, for instance, the framework proposed by Guerini et al. (2017) [3], current approaches still face several key limitations: (a) Insufficient Dataset Property Analysis: Existing frameworks often lack comprehensive tests for some important dataset properties. For instance, some of these may overlook important characteristics such as linearity and stationarity, which are essential for understanding the nature of the data and selecting appropriate modelling techniques. (b) Limited Options for Causal Discovery Methods: Most current frameworks rely on a single or limited set of causal discovery methods. This limitation may prevent us from obtaining optimal performance when dealing with different dataset characteristics or different priorities (such as accuracy vs. efficiency). The lack of method diversity limits the framework’s adaptability to various scenarios and data types. (c) Narrow Range of Performance Metrics: The existing validation framework typically focuses on basic similarity tests or a limited set of performance metrics. This may cause to failure to capture the full range of model performance, especially in complex financial systems where causal relationships can be intricate. Addressing these challenges is crucial for advancing ABM validation in complex systems like financial markets. Overcoming these limitations will lead to more reliable models, enhancing decision-making processes and informing policy decisions. These challenges necessitate innovative approaches in causal discovery and model validation techniques applicable to a wide range of complex systems. 1.4 Novel Approaches and Contributions This research addresses the challenges mentioned above by utilizing several new approaches and providing the following key contributions: 1. Robust Cross-Validated (RCV) Causal Discovery Method (Chapter 3): We introduce a novel approach to enhance the robustness of existing causal discovery methods. By applying cross-validation techniques to causal discovery algorithms such as VAR-LiNGAM and PCMCI, we aim to mitigate the sensitivity of these methods to noise and data variations to improve the consistency and reliability of causal structure identification in complex time series data. 2. Comprehensive Experimental Evaluation and Analysis (Chapter 4): We present a thorough empirical analysis of our proposed methods and existing approaches. This evaluation covers a wide range of characteristics, including linear vs. non-linear relationships, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary behaviour, and sparse vs. dense causal structures. We also examine the scalability of methods with varying numbers of variables and time series lengths. We ran experimental evaluations on both synthetic datasets with controlled properties and a complex simulated fMRI dataset, so as to provide insights into method performance under various conditions. 3. Context-Aware ABM Validation Framework (Chapter 5): Extending the work done by Guerini, we develop an enhanced ABM validation framework that addresses the weaknesses of existing one. In this framework, the user could choose a suitable method of causal inference based on the property of data sets or other validation needs, such as efficiency or accuracy. Another improvement we feature is to include a more comprehensive set of performance metrics for assessing the causal relations to get a more precise evaluation of model performance. This framework is designed to pre-process datasets and ensure their uniformity, analyse dataset attributes, run user-dependent or driven causal structure detection, and enhance validation evaluations. These contributions enhance ABM validation in complex systems by improving causal discovery methods and offering a flexible validation framework. This research aims to increase the accuracy and reliability of ABMs in capturing real-world dynamics across various domains. 1.5 Report Structure The remainder of this report is structured as follows: Chapter 2 provides the foundational background and reviews related work, covering Agent-Based Models in finance, existing ABM validation techniques, causal discovery methods and the current limitations. Chapter 3 introduces our novel Robust Cross-Validated (RCV) Causal Discovery Approach, detailing its theoretical foundations and implementation. Chapter 4 presents our experimental evaluation and analysis, including synthetic dataset generation, comparative analysis of causal discovery methods, and an application to a complex simulated fMRI dataset that mimics real-world neuroimaging data. Chapter 5 describes our Context-Aware ABM Validation Framework, explaining how it integrates improved causal discovery methods and enhances overall validation reliability. Finally, Chapter 6 concludes the report, summarizing key findings, discussing implications for ABM validation in complex systems and suggesting future research directions."
An Auditing Test to Detect Behavioral Shift in Language Models,"As language models (LMs) approach human-level performance, a comprehensive understanding of their behavior becomes crucial. This includes evaluating capabilities, biases, task performance, and alignment with societal values. Extensive initial evaluations, including red teaming and diverse benchmarking, can establish a model’s behavioral profile. However, subsequent fine-tuning or deployment modifications may alter these behaviors in unintended ways. We present a method for continual Behavioral Shift Auditing (BSA) in LMs. Our test compares model generations from a baseline model to those of the model under scrutiny and provides theoretical guarantees for change detection while controlling false positives. The test features a configurable tolerance parameter that adjusts sensitivity to behavioral changes for different use cases. We evaluate our approach using two case studies: monitoring changes in (a) toxicity and (b) translation performance. We find that the test is able to detect meaningful changes in behavior distributions using just hundreds of examples.","Language models (LMs) can now achieve human-level performance in a wide range of tasks, including text summarization, machine translation, coding and even acting as AI scientists: generating hypotheses and designing experiments (Achiam et al., 2023; Katz et al., 2024; Lu et al., 2024; Zhang et al., 2024). Because of this, many sectors are looking for ways to use them to improve existing systems (Kasneci et al., 2023; Felten et al., 2023). Unfortunately, one large roadblock to broad LM adoption is their propensity to generate harmful content (Weidinger et al., 2021). For example, GPT-3 has significant anti-Muslim biases (Abid et al., 2021), and GPT-4 has racial and gender biases (Zack et al., 2024). To address this, a significant effort is going into ensuring LM behavior is aligned with our societal values, spawning the field of AI alignment (Ji et al., 2023). A large portion of this effort is on developing ways to evaluate LM behavior, for example, through benchmarks (Wang et al., 2023a) and red-teaming (Perez et al., 2022). Given these evaluation techniques, how should they be used to ensure LMs stay safe? To answer this, consider the following two hypothetical settings where this question might be asked: (1) Internal Audit: At the new start-up Chasm Intellect, the LM alignment team is looking for a way to trigger an alarm when LM behavior changes. They know that LM model behavior can change unexpectedly (Li et al., 2024b). For example, fine-tuning a model that has undergone safety evaluation (e.g., GPT-3.5 Turbo (OpenAI, 2023)) can cause it to become less safe, even when using a benign fine-tuning dataset (e.g., Alpaca (Taori et al., 2023); (Qi et al., 2023)). How can the team detect meaningful changes in model behavior? (2) External Audit: A journalist has been playing around with Better geneRAtiVe language modEl (BRAVE), Chasm Intellect’s new model, and has noticed it produces highly toxic generations when asked about seemingly benign topics. Their article on this topic spurred a governmental audit of BRAVE. The government performs an initial evaluation but is worried that model behavior will change when no longer under scrutiny. How can the government regularly check the deployed model’s behavior is the same as the previously certified one? We call the general class of problems detecting changes in LM behaviors Behavioral Shift Auditing (BSA) problems. In this paper, we formalize the problem of Behavioral Shift Auditing in Language Models. We detail a test that continuously monitors behavioral shift, solely from model generations (e.g. via API calls). Under some weak assumptions, the test provably guarantees that if model generations have different behavior than those of an initial model, the test will detect it, given enough generations. At the same time, if there has not been a change, the test is guaranteed to have tight, non-asymptotic control over the false positive rate. The key insight behind our approach is that one can phrase the problem of Behavioral Shift Auditing as hypothesis testing over the relevant behavioral distribution. This framing allows our test to be applicable to any measurable aspect of model behavior, including also capabilities (e.g., dangerous capabilities (Phuong et al., 2024) or mathematical reasoning capabilities (Mishra et al., 2022a)) and biases (e.g., gender bias (Wang et al., 2023a; Kotek et al., 2023)). Using this insight, we develop a test that extends recent work on anytime-valid hypothesis testing (Pandeva et al., 2024), a state-of-the-art sequential testing method that has been successfully applied in various auditing settings (Chugg et al., 2023; Shekhar et al., 2023; Waudby-Smith et al., 2021). Our test checks for changes in model behavior distributions, comparing generations from a reference model with those of another, potentially changed model. The test has a tunable parameter that allows one to vary the strictness of the test. This allows for detecting any change in behavior, which may be more suitable for the external audit setting, to detecting a user-specified ϵitalic-ϵ\epsilonitalic_ϵ change in behavior, which could be used for the internal audit if small changes are acceptable. Similar to Pandeva et al. (2024), test performance is optimized using a learning algorithm, improving sample efficiency over prior testing methods (Lopez-Paz & Oquab, 2017; Lhéritier & Cazals, 2018; Podkopaev & Ramdas, 2024). This testing approach can complement a full evaluation when used as a warning system. Before an expensive model assessment on large-scale benchmarks (Achiam et al., 2023; Dubey et al., 2024; Zhang et al., 2024), our approach can be used to detect an initial behavior change, which can then trigger a full evaluation. We experimentally verify that our test satisfies theoretical guarantees and we report its sample efficiency on recent LM architectures for both auditing use cases. We release our code here: https://github.com/richterleo/Auditing_Test_for_LMs. Figure 1: An External Audit Example. A regulator can use the test we describe to perform an external audit: 1. The regulator initially certifies an LM by prompting and evaluating the set of generations received; 2. Later, tipped off that LM behavior may have changed, the regulator poses as a consumer and sends prompts to the model vendor, collecting the generations; 3. The regulator compares the distribution of behavior scores b⁢(⋅)𝑏⋅b(\cdot)italic_b ( ⋅ ) between the initial, certified generations and the later generations using a Behavioral Shift Auditing (BSA) test. If the distributions are sufficiently different the test triggers. Using our proposed method, the regulator can test samples sequentially without increasing the false-positive rate. The method is guaranteed to detect a change if one exists, given enough samples (more details in Section 3)."
Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression,"In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored. Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation. To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL. Technically, SCAS achieves value-aware OOD state correction, capable of correcting the agent from OOD states to high-value in-distribution states. Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS achieves excellent performance without additional hyperparameter tuning. Moreover, benefiting from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations.","Deep reinforcement learning (RL) shows promise in solving sequential decision-making problems, gaining increasing interest for real-world applications [43, 58, 64, 54, 8]. However, deploying RL algorithms in extensive scenarios poses persistent challenges, such as risk-sensitive exploration [14] and time-consuming episode collection [28]. Recent advances view offline RL as a hopeful solution to these challenges [35]. Offline RL aims to learn a policy from a fixed dataset without further interactions [33]. It can tap into existing large-scale datasets for safe and efficient learning [24, 38, 51]. In offline RL research, a well-known concern is the out-of-distribution (OOD) action issue: the evaluation of OOD actions causes extrapolation error [13], which can be exacerbated by bootstrapping and result in severe value overestimation [35]. To address this issue, a large body of work has emerged to directly or indirectly suppress OOD actions during training, employing various techniques such as policy constraint [13, 31, 11], value penalization [32, 2, 7], and in-sample learning [30, 15, 72]. (a) CQL (b) TD3BC (c) SCAS (Ours) (d) Optimal Value Figure 1: The resulting state distributions of offline RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values. Distinguished from most previous works, this paper argues that, apart from the OOD action issue, there exists an OOD state issue that also impairs performance yet has received limited attention in the field. Such an issue refers to the agent encountering states out of the offline dataset during the policy deployment phase (i.e., test phase). The occurrence of OOD states can be attributed to OOD actions, stochastic environments, and real-world perturbations. Since typical offline RL algorithms do not involve policy training in OOD states, the agent tends to behave in an uncontrolled manner once entering OOD states in the test phase. This can further exacerbate the state deviation from the offline dataset and lead to severe degradation in performance [35, 76]. In mitigating this OOD state issue, existing limited work attempts to train the policy to correct the agent from OOD states to in-distribution (ID) states [76, 23]. Technically, Zhang et al. [76] construct a dynamics model and a state transition model and align them to guide the agent to ID regions, while Jiang et al. [23] resort to an inverse dynamics model for policy constraint. However, they deal with the OOD state and OOD action issues separately, requiring extra OOD action suppression components and complex distribution modeling, which sacrifices computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, especially when the dataset contains substantial suboptimal states. As a result, the performance of prior methods also leaves considerable room for improvement. In this paper, we aim to address these two fundamental OOD issues simultaneously by proposing a simple yet effective approach for offline RL. We term our method SCAS due to its integration of OOD State Correction and OOD Action Suppression. We start with solving an analytical form of a value-aware state transition distribution, which is within the dataset support but skewed toward high-value states. Then, we align it with the dynamics induced by the trained policy on perturbed states via KL divergence. This operation intends to correct the agent from OOD states to high-value ID states, a concept we refer to as value-aware OOD state correction. Through some derivations, it also eliminates the necessity of training a multi-modal state transition model. Furthermore, we show theoretically and empirically that, while designed for OOD state correction, SCAS regularization also exhibits the effect of OOD action suppression. We evaluate SCAS on the offline RL benchmarks including D4RL [10] and NeoRL [50]. SCAS achieves excellent performance with consistent hyperparameters without additional tuning. Moreover, benefiting from its OOD state correction ability, SCAS demonstrates improved robustness against environmental perturbations. To summarize, the main contributions of this work are: • We systematically analyze the underexplored OOD state issue in offline RL and propose a simple yet effective approach SCAS unifying OOD state correction and OOD action suppression. • Our approach achieves value-aware OOD state correction, which circumvents modeling complex distributions and significantly improves performance over vanilla OOD state correction methods. • Empirically111Our code is available at https://github.com/MAOYIXIU/SCAS., our approach demonstrates superior performance on standard offline RL benchmarks and enhanced robustness in perturbed environments without additional hyperparameter tuning."
Multi-Agent Reinforcement Learning with Selective State-Space Models,"The Transformer model has demonstrated success across a wide range of domains, including in Multi-Agent Reinforcement Learning (MARL) where the Multi-Agent Transformer (MAT) has emerged as a leading algorithm in the field. However, a significant drawback of Transformer models is their quadratic computational complexity relative to input size, making them computationally expensive when scaling to larger inputs. This limitation restricts MAT’s scalability in environments with many agents. Recently, State-Space Models (SSMs) have gained attention due to their computational efficiency, but their application in MARL remains unexplored. In this work, we investigate the use of Mamba, a recent SSM, in MARL and assess whether it can match the performance of MAT while providing significant improvements in efficiency. We introduce a modified version of MAT that incorporates standard and bi-directional Mamba blocks, as well as a novel ‘cross-attention’ Mamba block. Extensive testing shows that our Multi-Agent Mamba (MAM) matches the performance of MAT across multiple standard multi-agent environments, while offering superior scalability to larger agent scenarios. This is significant for the MARL community, because it indicates that SSMs could replace Transformers without compromising performance, whilst also supporting more effective scaling to higher numbers of agents. Our project page is available at https://sites.google.com/view/multi-agent-mamba.","Figure 1: Normalised mean episode returns aggregated over all tasks and environments with 95% confidence intervals for MAM, MAT, and MAPPO. Results are obtained using ten independent seeds. MAM matches the final performance of MAT, currently state-of-the-art in MARL, and exhibits greater sample efficiency. Figure 2: Mean time (seconds) per evaluation step in smacv2 tasks with increasing numbers of agents for MAM, MAT, and MAPPO. The mean time per evaluation step for MAT increases approximately quadratically, while MAM and MAPPO scale linearly in the number of agents. Figure 3: Comparison of performance metrics for MAM, MAT, and MAPPO across various tasks. Multi-Agent Reinforcement Learning (MARL) still faces significant challenges that must be overcome to unlock its full potential; one such challenge is the ability to scale to large numbers of agents while maintaining good performance. The Multi-Agent Transformer (MAT) (Wen et al., 2022) boasts state-of-the-art performance in online MARL. MAT is an encoder-decoder framework that utilises the multi-agent advantage decomposition theorem (Kuba et al., 2022) to reframe the challenge of joint policy optimisation. It converts the problem into a sequential decision-making process, simplifying the search for optimal policies across multiple agents. However, Transformers scale quadratically in sequence length (Vaswani et al., 2023). This creates a computational bottleneck for MAT as the number of agents becomes large. Recently, State-Space Models (SSMs) (Gu et al., 2022; Gupta et al., 2022; Gu et al., 2021; Gupta et al., 2022; Smith et al., 2023; Fu et al., 2023) have offered a solution to this drawback in the Transformer architecture, with the ability to scale linearly in the sequence length. Of interest in this work is Mamba (Gu and Dao, 2024)–a selective SSM which boasts fast inference and linear scaling in the sequence length whilst matching the performance of attention architectures in the natural language processing domain. The innovations of the Mamba model are its input-dependent SSM parameters and its hardware-aware parallel algorithm in recurrent mode. In this paper, we explore replacing attention in the MAT architecture with Mamba blocks. We make use of both existing vanilla and bi-directional Mamba blocks, as well as a novel ‘cross-attention’ Mamba block we designed to replace MAT’s cross-attention. We evaluate our novel architecture, which we call the Multi-Agent Mamba (MAM), on a wide range of well-known MARL benchmark environments and compare its performance to MAT. Our core contributions can be summarised as follows: • We create an extension of the vanilla Mamba block which can be used as a cross-attention replacement. • We replace the three different attention blocks in the MAT architecture with vanilla, bi-directional and cross-attentional Mamba blocks respectively. • We empirically validate that MAM performs comparably to MAT on a wide range of MARL environments while offering significantly greater efficiency."
Toward Finding Strong Pareto Optimal Policies in Multi-Agent Reinforcement Learning,[1]\fnmViet Cuong \surTa,"Multiagent reinforcement learning (MARL) is the learning framework where multiple different agents learn to make optimal decisions in an environment through the Reinforcement Learning paradigm. MARL research advances rapidly, with the recent development of MARL achieving impressive results on a wide range of learning scenarios, notably in zero-sum games [21] and fully cooperative environments [18, 27] where all agents share the same reward function. A less studied area of research in MARL is in the environments with cooperative reward structures, in which all agents have different and possibly conflicting reward schemes [15]. In these problems, there might be multiple different but optimal policies, in the sense that none of which is better than the other. This concept is known as the Pareto optimality in multi-objective optimization. It is desirable that we can find such Pareto optimal policies, as they are equivalent to the optimal policies in the common setting of single agent problems [3]. While the current MARL methods are known to find Nash equilibrium [13], such solutions can be suboptimal [17]. In this paper, we first show that in general cooperative environments, agents need to explicitly consider the optimization of other agents to achieve Pareto optimality. Such behaviors are known as altruistic learning in RL literature [9]; altruistic learners learn to act for the benefit of other agents even though those actions do not bring about any personal gain for the actors. To learn altruistically, one agent needs to optimize not only the reward of itself but also the rewards of other agents, which involves some form of multi-objective optimization. As a result, we connect the multi-objective framework to the MARL domain. Multiple Gradient Descent Algorithm (MGDA) [5] is one of the most popular gradient-based multi-objective methods. MGDA can find arbitrary solutions in the Pareto optimal Set using first-order gradient descent. However, MGDA is known to only converge to weak Pareto optimal solutions [8]. While this problem is not significant in other learning settings, we show that MARL problems can have many weak Pareto Stationary points, which can reduce the efficacy of MGDA. To this end, we identify the effect of diminishing gradient norms as a root cause to the weak Pareto convergence issue of MGDA and propose an improved version of MGDA++ based on this observation. We demonstrate both theoretically and empirically that MGDA++ can converge to strong Pareto optimal solutions. To summarize, our contributions in this paper are: • We show that to achieve Pareto optimality in MARL, agents need to consider the objective of other agents, we connect the multi-objective optimization with MARL problems, and propose to apply MGDA to the MARL problems. • We propose MGDA++, an extension of MGDA that converges to strong Pareto Solutions with bi-objective problems in convex, smooth settings both theoretically and empirically. To our knowledge, this strong Pareto convergence result in the convex setting with gradient descent is the first in the literature. • We demonstrate the effectiveness of MGDA++ with trust region methods through several cooperative scenarios in the Gridworld benchmark. Our proposed method is able to achieve better convergence solutions across different agents in comparison with other baselines."
Notes on the Mathematical Structure of GPT LLM Architectures,"When considered from a purely mathematical point of view, the building and training of a large (transformer) language model (LLM) is the construction of a function - which can be taken to be a map from some euclidean space to another - that has certain interesting properties. And therefore, from the point of view of a mathematician, it may be frustrating to find that many key papers announcing significant new LLMs seem reluctant to simply spell out the details of the function that they have constructed in plain mathematical language or indeed even in complete pseudo-code (and the latter form of this complaint appears to be one of the motivations behind a recent article of Phuong and Hutter [1]). Here, we seek to give a relatively ‘pure’ mathematical description of the architecture of a GPT-3-style LLM.","When considered from a purely mathematical point of view, the building and training of a large (transformer) language model (LLM) is the construction of a function - which can be taken to be a map from some euclidean space to another - that has certain interesting properties. And therefore, from the point of view of a mathematician, it may be frustrating to find that many key papers announcing significant new LLMs seem reluctant to simply spell out the details of the function that they have constructed in plain mathematical language or indeed even in complete pseudo-code (and the latter form of this complaint appears to be one of the motivations behind a recent article of Phuong and Hutter [1]). Here, we seek to give a relatively ‘pure’ mathematical description of the architecture of a GPT-3-style LLM. Trainable Parameters Like all such models in machine learning, the construction really initially describes a family of functions indexed by some set Θ=𝐑NΘsuperscript𝐑𝑁\Theta=\mathbf{R}^{N}roman_Θ = bold_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT called the parameter space. There is then a separate process - the training of the model - in which a particular value θ∈Θ𝜃Θ\theta\in\Thetaitalic_θ ∈ roman_Θ is selected using a training algorithm. Each dimension of ΘΘ\Thetaroman_Θ corresponds to the possible values of an individual trainable parameter. We will draw attention to such parameters as we introduce them, as opposed to attempting to give a definition of ΘΘ\Thetaroman_Θ up front. But this short note discusses only the architecture and does not describe any training algorithms."
BitPipe: Bidirectional Interleaved Pipeline Parallelismfor Accelerating Large Models Training,"With the increasing scale of models, the need for efficient distributed training has become increasingly urgent. Recently, many synchronous pipeline parallelism approaches have been proposed to improve training throughput. However, these approaches still suffer from two major issues, i.e., pipeline bubbles caused by periodic flushing and extra communication due to the increasing number of pipeline stages. To this end, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. Specifically, a hybrid scheme of fusing interleaved pipelines with bidirectional pipelines is proposed to reduce the computational time of each single micro-batch and multiply the number of devices executing simultaneously. A V-shaped schedule with eager gradient synchronization is introduced to reduce and overlap the communication between devices. Experiments conducted on up to 32 GPUs show that BitPipe improves the training throughput of GPT-style and BERT-style models by 1.05×1.05\times1.05 ×-1.28×1.28\times1.28 × compared to the state-of-the-art synchronous approaches.","Scaling the number of parameters in contemporary deep learning models has yielded remarkable the state-of-the-art (SOTA) results. Training these large models is challenging, as the limited memory and computational capacity of a single device (e.g., GPU) pose obstacles to accommodating them within realistic timeframes. For instance, training a GPT-3 175B model demands over 3,000 GiB for storing model parameters and optimizer states, requiring an impractical 288 years with a single NVIDIA V100 GPU (Kim et al. 2023; Narayanan et al. 2021b). The urgency for parallel and distributed training (e.g., data parallelism and model parallelism) has become increasingly pronounced. While data parallelism (Li et al. 2014) allows for ideal speedup, it falters when confronted with large models that exceed the capacity of a single device. Model parallelism (Dean et al. 2012; Lee et al. 2014; Wang, Huang, and Li 2019) addresses this limitation by distributing the weight parameters of a model across multiple devices, which mitigates the memory usage per device but suffers from severe resource under-utilization. Pipeline parallelism improves resource utilization, which splits a batch into smaller micro-batches and divides a model into stages within a pipeline, allowing simultaneous execution of different micro-batches across multiple devices. Pipeline parallelism can be categorized into synchronous and asynchronous schemes based on weight update semantic. Synchronous approaches flush periodically at the end of each iteration to guarantee strict optimizer semantics, which causes device idle times (also called pipeline bubbles). Asynchronous approaches do away with flushes completely by delaying weight updates, but at the expense of strict model convergence and thus are not within the scope of our work. Figure 1: Classic synchronous pipeline schedules, with 4 pipeline devices and 8 micro-batches within a training iteration. Both schedules have the same bubble overhead and weights memory consumption (Mθsubscript𝑀θM_{\uptheta}italic_M start_POSTSUBSCRIPT roman_θ end_POSTSUBSCRIPT). The activations memory consumption (Masubscript𝑀aM_{\rm a}italic_M start_POSTSUBSCRIPT roman_a end_POSTSUBSCRIPT) of the 1F1B schedule exhibits better efficiency but existing imbalance. Early synchronous approach (e.g., GPipe (Huang et al. 2019)) focuses on reducing pipeline bubbles by increasing the number of concurrent batches in the pipeline (as shown in Figure 1(a)). As a direct consequence, there is an increase in peak activation memory demands. Subsequently, encouraged by the success of the 1F1B schedule (as shown in Figure 1(b)), researchers have proposed memory-efficient approaches (e.g., DAPPLE (Fan et al. 2021) and PipeDream-Flush (Narayanan et al. 2021a)), which further adjusts the number of micro-batches injected into devices at the beginning of pipelines. Recently approaches attempt to increase the number of devices executing simultaneously (i.e., bidirectional pipeline parallelism), or to reduce the computational time of a single micro-batch (i.e., interleaved pipeline parallelism), which shows the SOTA performance. In the bidirectional approaches (Jain et al. 2020; Li and Hoefler 2021; Zhang et al. 2023), each device stores multiple pipeline stages in different directions, which decreases bubble size and achieves a more balanced activation memory consumption. On the other hand, interleaved approaches (Narayanan et al. 2021b; Lamy-Poirier 2023; Liu et al. 2023) assign multiple smaller and nonconsecutive stages to each device, which makes each bubble size smaller accordingly. Despite the promising results, the latest synchronous approaches still face two primary issues. First, the remaining bubbles still pose the largest deficiency. Due to computation dependencies in the pipeline across different devices, bubbles are inevitable. In existing approaches, as much as 50% of the time can be spent to flush the pipeline. Second, the communication overhead remains considerable even though pipeline parallelism employs point-to-point (P2P) communication. Specifically, bidirectional pipeline parallelism requires additional weight memory and data-parallel communication to reduce pipeline bubbles, while interleaved pipeline parallelism shrinks bubble size at the expense of extra P2P communication. Moreover, if the bidirectional pipeline extends to more than two pipelines, or each device in the interleaved pipeline generalizes to have more stages, the extra communication or memory usage will increase accordingly, further degrading their performance. To address the aforementioned issues, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. To the best of our knowledge, BitPipe is the first work that incorporates the interleaved schedule into bidirectional pipeline parallelism, which reduces the computational time of each single micro-batch and doubles the number of devices executing simultaneously. BitPipe transforms the looping schedule of the interleaved pipeline to a V-shaped schedule and thus mitigates the side effect of the additional communication overhead. The contributions of BitPipe are summarized as follows: • We propose a hybrid pipeline scheme of fusing interleaved pipelines with bidirectional pipelines. This design can not only improve throughput, but also achieves a harmonious balance in memory utilization. • We introduce a V-shaped schedule of partially transforming cross-device communication to local copying, alongside an eager gradient synchronization scheme, which can reduce and overlap communication between devices. • Experiments show that BitPipe can improve the end-to-end performance by up to 1.28×1.28\times1.28 × per iteration for GPT-style and BERT-style models compared to the SOTA synchronous pipeline approaches."
FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with Ferroelectric In-Memory Computing,"In scenarios with limited training data or where explainability is crucial, conventional neural network-based machine learning models often face challenges. In contrast, Bayesian inference-based algorithms excel in providing interpretable predictions and reliable uncertainty estimation in these scenarios. While many state-of-the-art in-memory computing (IMC) architectures leverage emerging non-volatile memory (NVM) technologies to offer unparalleled computing capacity and energy efficiency for neural network workloads, their application in Bayesian inference is limited. This is because the core operations in Bayesian inference, i.e., cumulative multiplications of prior and likelihood probabilities, differ significantly from the multiplication-accumulation (MAC) operations common in neural networks, rendering them generally unsuitable for direct implementation in most existing IMC designs. In this paper, we propose FeBiM, an efficient and compact Bayesian inference engine powered by multi-bit ferroelectric field-effect transistor (FeFET)-based IMC. FeBiM effectively encodes the trained probabilities of a Bayesian inference model within a compact FeFET-based crossbar. It maps quantized logarithmic probabilities to discrete FeFET states. As a result, the accumulated outputs of the crossbar naturally represent the posterior probabilities, i.e., the Bayesian inference model’s output given a set of observations. This approach enables efficient in-memory Bayesian inference without the need for additional calculation circuitry. As the first FeFET-based in-memory Bayesian inference engine, FeBiM achieves an impressive storage density of 26.32 Mb/mm2 and a computing efficiency of 581.40 TOPS/W in a representative Bayesian classification task. These results demonstrate 10.7×\times×/43.4×\times× improvement in compactness/efficiency compared to the state-of-the-art hardware implementation of Bayesian inference.","In-memory computing (IMC) has recently emerged as a promising solution to address the memory wall issues in conventional von Neumann hardware (ielmini2018memory, ). Leveraging the compactness and high energy efficiency of emerging non-volatile memory (NVM) technologies, many leading IMC accelerators have achieved impressive computing efficiency and throughput for data-intensive machine learning models, particularly neural networks (NNs) (shafiee2016isaac, ; hu2021memory, ; yan2023improving, ; jung2022crossbar, ). While conventional NN-based algorithms are widely used, they often struggle in situations where training data is insufficient or when interpretable results are needed (qayyum2020secure, ; yang2022unbox, ). As a compelling alternative, Bayesian inference is particularly well-suited in low-data scenarios, providing explainable results with uncertainty estimation (ghahramani2015probabilistic, ; burkart2021survey, ). The primary posterior calculation in Bayesian inference, which involves the cumulative product of prior and likelihood probabilities as per Bayes’ theorem, however, differs from the multiply-and-accumulate (MAC) operations common in NN workloads. This difference renders Bayesian inference usually unsuitable for direct implementation with many existing IMC designs that typically focus on NN acceleration. In traditional complementary metal-oxide-semiconductor (CMOS)-based von Neumann implementations for Bayesian inference, such as CPU (smith2020massively, ), GPU (talbot2019parallelized, ) and field-programmable gate array (FPGA) (awano2020bynqnet, ), accessing separate memory units for stored probabilities incurs significant area and energy overhead. Efforts to exploit the non-volatility and energy efficiency of emerging devices have led to the development of Bayesian inference prototypes utilizing random number generators (RNGs) built with magnetic tunnel junction (MTJ) (vodenicarevic2017low, ), memtransistor (zheng2022hardware, ) and magnetic random-access memory (MRAM) (faria2018implementing, ). These implementations, however, are limited to Bayesian inference with binary evidence/events, and do not effectively address probability storage, rather generating required probabilities on demand, which is energy-consuming. A memristor-based Bayesian machine has been proposed (harabi2023memristor, ), using near-memory stochastic computing to reduce memory access overhead. Yet, these implementations still require additional CMOS logic and multiple clock cycles for posterior calculations and complex sensing circuitry for final inference, thus compromising computing density and inference efficiency. To address the aforementioned challenges in hardware implementation of Bayesian inference, we propose FeBiM, an efficient and compact in-memory Bayesian inference engine utilizing multi-level cell (MLC) ferroelectric field-effect transistors (FeFETs). The key contributions of this work are summarized as follows: • We propose a compact crossbar array design using one FeFET per cell as probability storage unit and a compact and scalable winner-take-all (WTA) circuit for sensing. This multi-bit FeFET array enables efficient in-memory Bayesian inference in just one clock cycle, eliminating the need for extra calculation circuitry. • We introduce a novel mapping scheme that associates quantized logarithmic probabilities with discrete FeFET states. This scheme enables the output currents of the crossbar to naturally represent the posterior probabilities, i.e., the cumulative product of priors and likelihoods given a set of observations. • We thoroughly investigate the functionality, scalability and application level performance of FeBiM. In a representative Bayesian classification task, our proposed design shows a 10.7×\times×/43.4×\times× storage density/inference efficiency improvement compared to the state-of-the-art Bayesian machine. The rest of the paper is organized as follows. Section 2 reviews the basics and relevant prior works. Section 3 introduces our FeFET-based IMC design for Bayesian inference. Section 4 presents the validation, scalability investigation and application benchmarking results of FeBiM. Finally, section 5 concludes the paper."
Interpreting Neural Networks through Mahalanobis Distance,"This paper introduces a theoretical framework that connects neural network linear layers with the Mahalanobis distance, offering a new perspective on neural network interpretability. While previous studies have explored activation functions primarily for performance optimization, our work interprets these functions through statistical distance measures, a less explored area in neural network research. By establishing this connection, we provide a foundation for developing more interpretable neural network models, which is crucial for applications requiring transparency. Although this work is theoretical and does not include empirical data, the proposed distance-based interpretation has the potential to enhance model robustness, improve generalization, and provide more intuitive explanations of neural network decisions.","Neural networks have revolutionized machine learning, achieving remarkable success across diverse applications. Central to their efficacy is the use of activation functions, which introduce non-linearity and enable the modeling of complex relationships within data. While Rectified Linear Units (ReLU) have gained prominence due to their simplicity and effectiveness (Nair and Hinton, 2010), the exploration of alternative activation functions remains an open and valuable area of research (Ramachandran et al., 2018). Neural network units are often viewed as linear separators that define decision boundaries between classes (Minsky and Papert, 1969) with larger activation values suggesting stronger contributions of features to those decisions. Our work challenges this perspective, exploring how individual neurons can be understood through the lens of statistical distance measures. Clustering techniques use distance measures. They aim to minimize the distance between data points and feature prototypes, with smaller values indicating stronger membership to the feature or cluster (MacQueen, 1967a). We explore the intersection between these perspectives on activation interpretations, leveraging the distance-minimization approach of clustering techniques to lay the groundwork for novel neural network designs based on statistical distance measures. This paper establishes a novel connection between neural network architectures and the Mahalanobis distance, a statistical measure that accounts for the covariance structure of data (Mahalanobis, 1936). We present a robust mathematical framework that bridges neural networks with this statistical distance measure and lays the groundwork for future research into neural network interpretability and design. Our key contributions are: 1. We establish a mathematical connection between neural network linear layers and the Mahalanobis distance, demonstrating how Absolute Value (Abs) activations facilitate distance-based interpretations. 2. We analyze the solution space that neural networks are likely to learn when approximating Mahalanobis distance, exploring the effects of non-uniqueness in whitening transformations and the role of Abs-activated linear nodes. 3. We discuss the broader implications of this framework for neural network design and interpretability, laying the groundwork for more interpretable models."
COAT:CompressingOptimizer states andActivation for Memory-Efficient FP8Training,"FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54× compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43× end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine’s speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT.","Foundation Models (FMs), such as Large Language Models (LLM) and Vision Language Models (VLM), have made significant breakthroughs in various tasks such as reasoning, understanding, and summarization (Dubey et al., 2024; Adler et al., 2024; Team et al., 2024; Lin et al., 2024). However, the training of such models, which often comprise billions of parameters, demands substantial computational resources and memory. This presents substantial challenges, making the training of these foundation models very challenging (Smith et al., 2022; Hoffmann et al., 2022). Low-precision training has emerged as a promising approach to make FMs training more efficient (Micikevicius et al., 2017; Wang et al., 2018; Zhu et al., 2020; Xi et al., 2023; Wortsman et al., 2023; Xi et al., 2024). By quantizing tensors used in deep neural networks into lower precision, low-precision training effectively speed up the training process and reduce the memory footprint. Currently, BF16 training (Kalamkar et al., 2019; Micikevicius et al., 2017) is the most prevalent low-precision method, and is widely adopted in large-scale training frameworks like DeepSpeed (Rasley et al., 2020) and Megatron-LM (Shoeybi et al., 2019). With the advent of Nvidia’s H100 GPU (NVIDIA, 2024a), FP8 training Micikevicius et al. (2022) is emerging as the next-generation low-precision technique. Compared to BF16, FP8 training has the potential to (1) double the speed and (2) halve the memory footprint. To achieve practical speedup, Transformer Engine (NVIDIA, 2024b) performs matrix multiplications in FP8 precision, leading to faster training. Transformer Engine’s memory footprint can be further improved by reducing optimizer states, gradients, weights, and activations to lower precision. As illustrated in Figure 1, FP8-LM (Peng et al., 2023) advances this by further quantizing the gradients, weight master copy, and first-order momentum into FP8. This reduces memory and communication overhead, partially improving memory efficiency. However, they do not tackle the memory consumption of activations and still leave the optimizer’s second-order momentum in higher precision. The memory problem of activations becomes even more critical when optimizer, gradient, and weights are sharded across multiple GPUs using ZeRO or FSDP. Besides, second-order momentum is more sensitive to quantization than first-order momentum (Fishman et al., 2024), and activations’ large spikes also make them hard to quantize to FP8 (Yang et al., 2024). This potential accuracy degradation makes them missing a crucial opportunity to optimize memory further. In this work, we propose COAT: Compressing Optimizer states and Activations for memory-efficient FP8 Training to address the aforementioned issue. COAT significantly reduces the overall memory footprint by quantizing optimizer states and activations into FP8. For optimizer states, we observe that FP8 format’s representation range is under-utilized when quantizing them, as illustrated in Figure 2(a). To address this, we introduce a novel Dynamic Range Expansion method which adjusts the distribution of optimizer states to better fit within the FP8 range, thereby minimizing quantization error. For activations, we propose Mixed-Granularity Activation Quantization to achieve efficient and accurate quantization. We apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers. Per-tensor quantization for matrix multiplications is more efficient and better suited for TensorCores, while fine-grained quantization helps maintain accuracy. These two approaches tackle high memory consumption while ensuring minimal performance degradation. We provide an overview of COAT in Figure 1(b) for demonstration. We demonstrate the accurate performance of COAT on a wide range of tasks, including LLM pretraining, LLM fine-tuning, and VLM training. COAT achieves nearly lossless performance on all of these tasks. For efficiency results, COAT achieves 1.54×1.54\times1.54 × end-to-end memory reduction compared with BF16, and 1.43×1.43\times1.43 × end-to-end training speed up on Llama 7B, 13B, and 30B models compared to BF16. COAT also doubles the batch size in all realistic distributed training settings, which is crucial for higher speedup and support for longer context length, leading to a more efficient training process for large-scale models."
Golden Ratio-Based Sufficient Dimension Reduction,"Many machine learning applications deal with high dimensional data. To make computations feasible and learning more efficient, it is often desirable to reduce the dimensionality of the input variables by finding linear combinations of the predictors that can retain as much original information as possible in the relationship between the response and the original predictors. We propose a neural network based sufficient dimension reduction method that not only identifies the structural dimension effectively, but also estimates the central space well. It takes advantages of approximation capabilities of neural networks for functions in Barron classes and leads to reduced computation cost compared to other dimension reduction methods in the literature. Additionally, the framework can be extended to fit practical dimension reduction, making the methodology more applicable in practical settings.","The curse of dimensionality poses significant challenges to statistical analysis when dealing with a large number of variables [1]. Under the supervised learning framework, sufficient dimension reduction (SDR) has emerged as a useful tool that bridges the gap between high dimensionality and traditional statistical modeling. However, current state-of-the-art statistical methods for SDR in the literature often presume the structural dimension, which is generally not the case in practice. Additionally, these methods may not be computationally feasible for handling large sample sizes or high dimensionality efficiently, which limits their usage in many real-world applications. Numerous methods have been proposed on SDR in the past decades, see e.g., [2]. For classical methods such as sliced inverse regression (SIR) [3], minimum average variance estimation (MAVE) method [4], and sliced average variance estimation (SAVE) [5], a main class of estimators for the central space is based on the inverse conditional moments of X|Yconditional𝑋𝑌X|Yitalic_X | italic_Y, where Y∈ℝ𝑌ℝY\in\mathbb{R}italic_Y ∈ blackboard_R and X∈ℝp𝑋superscriptℝ𝑝X\in\mathbb{R}^{p}italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT are the response and p𝑝pitalic_p-dimensional predictor in a regression analysis, respectively. Slicing the continuous response y𝑦yitalic_y is often used to facilitate the estimation. However, this imposes some strong probabilistic structure on X𝑋Xitalic_X. Moreover, selecting the number of slices remains an open and challenging question [6]. [7] proposed a cumulative slicing estimation methodology for SDR and developed three methods: cumulative mean estimation (CUME), cumulative directional regression (CUDR), and cumulative variance estimation (CUVE). [8] introduced a fused estimation procedure (fSIR), with observed performance improvement in some situations. [9] implemented a sliced inverse regression in an online fashion that first constructs an online estimate for the kernel matrix and then performs online singular value decomposition. [10] established consistency of estimation of the dimension reduction space in a high-dimensional setting. In a more recent work, [6] proposed an aggregate inverse mean estimation (AIME) procedure that may substantially improve estimation accuracy compared to the previous methods. It incorporates the cumulative slicing scheme into the aggregate SDR idea proposed by [11] and is much less sensitive to linearity condition violations with the localization step before aggregation. [12] proposed a real-time approach for SDR that uses a principal least squares support vector machines approach to estimate the central subspace more accurately. Their method updates the estimator efficiently as new data is collected, starting from an initial estimator obtained with currently available data. [13] proposed a method that first estimates the expectiles through kernel expectile regression and then carries out dimension reduction based on random projections of the regression expectiles. Several methods in the literature are extended under these frameworks [14, 15, 16]. There are also neural network approaches to SDR for tackling classification problems [17, 18, 19]. For regression problems, [20] proposed a nonlinear SDR method and [21] proposed a stochastic neural network that is computationally feasible to handle large scale data. [21] has proposed an algorithm that is able to obtain structural dimension, although no theoretical understanding is provided (different from this work). In this paper, we propose a golden ratio-based neural network for SDR (GRNN-SDR), a novel approach that utilizes neural networks to capture complex functional forms that are previously inaccessible with the traditional nonparametric regression tools. Our algorithm incorporates the golden ratio to dynamically search for the structural dimension, which significantly reduces computation time and complexity. Theoretical basis have demonstrated the generalization ability of multi-layer neural networks [22, 23]. Under proper conditions, we establish theoretical results that demonstrate that our approach leads to the true structural dimension with high probability. Compared to most of the existing methods, which typically presume the structural dimension to estimate the central space, extensive numerical results show that our proposed method is able to obtain the true or practical structural dimension effectively without prior knowledge. Extensive experiment comparisons show that our method estimate the central space with higher accuracy in most cases and demonstrate higher stability when the true dimensionality is not small. Furthermore, our algorithmic complexity under a fixed neural network structure is O⁢(N)𝑂𝑁O(N)italic_O ( italic_N ), where N𝑁Nitalic_N is the sample size, offering a promising solution to the challenges in SDR."
A Stock Price Prediction Approach Based on Time Series Decomposition and Multi-Scale CNN using OHLCT Images,"Stock price fluctuations are influenced by a variety of factors, including macroeconomic conditions, government policies and market sentiment, which together make price movements complex and difficult to be predicted. Despite many studies aimed at enhancing stock price prediction models, the challenges such as data noise, model overfitting and lack of interpretability are still encountered. To address these issues and improve prediction accuracy, this paper proposes a novel method, named Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), for predicting stock price movements in the China A-share market.","With the rapid development of financial markets and the acceleration of globalization, stock investment has become an essential avenue for many investors to achieve wealth appreciation [1, 2, 3]. However, the complexity and uncertainty of the stock market make accurately predicting stock trends a challenging task. The China A-share market, being one of the largest and most dynamic globally, has attracted significant quantitative analysis and research due to the growing impact of economic globalization [4, 5, 6]. Traditional stock analysis methods heavily reliant on financial data and macroeconomic indicators and often fall short in capturing the dynamic and non-linear relationships within the market [7, 8, 9, 10]. Therefore, quantitative trading has emerged as a pivotal component of modern financial strategies. By leveraging computational techniques, it enables the execution of trading strategies devoid of emotion-based decision-making, thereby uncovering patterns that may elude human analysts [11]. The potential quantitative trading in stock market forecasting has been well-documented [12]. This progress has led to the development of advanced deep models that can handle the inherent complexity, nonlinearity, and noise of financial markets, making the construction of stock trend prediction models using machine learning, deep learning, and big data techniques, which becomes a hot research topic in finance [13, 14, 15, 16, 17, 18, 19]. Despite significant advancements, many deep learning models for stock price prediction still grapple with challenges such as data noise, model overfitting, and insufficient interpretability [20]. Baek et al. [21] pointed out that the limited number of training data points often leads to overfitting in deep neural network models. Ito et al. [22] aggregated several simple, interpretable weak algorithms and assessed the importance of each weak algorithm in improving overall interpretability. In order to tackle the data noise problem, the research conducted by Liu et al. [23] employed sparse autoencoders with one-dimensional (1D) residual convolutional networks to denoise the stock prices. Convolutional neural networks (CNNs) have been employed for stock market prediction due to their efficient feature extraction capabilities [24, 25, 26]. However, traditional CNN approaches often fail to fully utilize time series information, resulting in suboptimal performance under complex market conditions [27]. Additionally, when considering multiple stocks, these models typically focus on individual stock information and neglect the inter-stock correlations that significantly influence price fluctuations [28]. Jiang et al. [29] utilized the opening prices, highest price, lowest price, closing price, and volume (OHLCV) images as inputs to predict the probability of stock price increasing or decreasing, achieving a significant accuracy. Their experimental results showed that 5-day feature maps in CNNs yield significantly better accuracy than 20-day and 60-day maps, suggesting that longer time series of stock feature maps make it challenging for CNN models to identify critical feature points, leading to local feature overfitting. Converting sequences to image features can lose the advantage of utilizing more historical data, whereas shorter image features risk sacrificing significant historical information, increasing prediction inaccuracies. Inspired by these studies, we first propose two novel methods. The first method involves replacing trading volume with stock turnover rate, which eliminates the impact of trading volume caused by ex-rights events, resulting in more stable features. The second method introduces an integration between time separator and OHLCT (Opening price, Highest price, Lowest price, Closing price, and Turnover rate), resulting a new image feature as input, named as TS-OHLCT. Specifically, we incorporate the weekend time information as separators into OHLCT images to help CNNs capture trading temporal information and learn the effects of stock trading cycles. Furthermore, two new architectures are proposed. The first is a Multi-Scale Residual Convolutional Neural Network (MSR-CNN) designed to address the overfitting problem in long sequence images. The second architecture is a Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), which addresses the problem that using only image features makes it difficult to learn information about stock price fluctuations. By integrating sequence data, SMSFR-CNN better captures stock price trends in the A-share market. We observed that traditional CNN methods often exhibit local feature overfitting and convergence issues in stock image feature extraction, with prediction performance gradually deteriorating as the time of sequence feature maps increases. This observation aligns with the findings of [29]. To address these mentioned issues, we decomposed long sequences of stock image features into multiple time periods according to different time scales and assign different feature weights based on their importance (with higher weights for features closer to the current trading day). This significantly reduces overfitting and enables CNNs to learn long sequence image features better. Additionally, considering that investors more concern about the magnitude of price fluctuations rather than the probability of price increasing or decreasing, and given that it is difficult for the image features to effectively capture the magnitude of price changes, the proposed model integrates time series information as an extra features. Our approach utilizes the CNN component to learn time series features and concatenates them with image features, simultaneously predicting both the magnitude and probability of stock price movements. This method effectively incorporates regression labels into the existing framework. Our experimental results indicate that this approach significantly improves the accuracy of stock price trend predictions, reduces the search space for image features, stabilizes and accelerates the convergence process. The paper’s major contributions can be summarized as follows: 1. This is the first time that historical open, high, low, close prices and turnover-rates are incorporated into images, which are separated by weekends and combined with time-specific information. 2. The long sequences of stock image features are decomposed into multiple time periods according to different time scales and assigned different feature weights based on their importance (with higher weights for features closer to the current trading day). 3. Combining time series with image features using CNN improves prediction accuracy, reduces the search space, stabilizes and accelerates the convergence process. 4. Comprehensive comparison experiments between different methods on 4,454 A-share stocks are provided, and the majority of A-share stocks are considered in our experiments. 5. Our proposed method, SMSFR-CNN, outperforms other advanced methods in terms of positive predictive value (PPV) and negative predictive value (NPV). The remainder of the paper is structured as follows. The related works of CNN methods in stock prediction are introduced in Section 2. Section 3 presents the dataset that serves as the basis for our study along with a detailed description of the innovative and predictive model developed in this paper. In Section 4, we describe and analyze the experimental settings and results. Finally, we provide a summary of the conclusions and main contributions of the paper in Section 5. We also highlight the significance of our results and propose potential works for future research."
Applying sparse autoencoders to unlearn knowledge in language models,"We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it language models. We demonstrate that individual interpretable biology-related SAE features can be used to unlearn biology-related knowledge with minimal side-effects. Our results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective. We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique. Current SAE quality or intervention techniques would need to improve to make SAE-based unlearning comparable to the existing fine-tuning based techniques.","Current and future language models may learn inaccurate information, produce toxic or malicious outputs, or possess dangerous capabilities that we would like to remove before deployment, such as advanced bio-weapons knowledge (Ji et al., 2023; Li et al., 2024). However, we do not yet know how to precisely and robustly remove knowledge or unlearn capabilities in these language models. The goal of this work is to investigate whether sparse autoencoders (SAEs) can be used to perform unlearning in an interpretable way. Recent work on unlearning has typically focused on fine-tuning based methods that have been applied in a variety of contexts to unlearn concepts in language models (e.g. Li et al., 2024; Zou et al., 2024; Eldan & Russinovich, 2023), going beyond prior work that aimed to unlearn specific training data points in neural networks (Bourtoule et al., 2020). While relatively successful, these fine-tuning approaches are opaque and we lack insight into what exactly is happening in the model (Łucki et al., 2024). Existing methods for removing specific facts from language models offer interpretable solutions (e.g. Meng et al., 2023), however these approaches are limited to fact-level unlearning. Having an interpretable method for unlearning is important as it can allow a higher level of confidence that the model has actually unlearned the knowledge, rather than superficially or temporarily hiding the capability to discuss a given topic. One possibility is to use sparse autoencoders to try to unlearn knowledge in an interpretable way. Sparse autoencoders (SAEs) use an unsupervised method to learn sparse reconstruction of language model activations (e.g. Ng, 2011; Bricken et al., 2023; Cunningham et al., 2023; Templeton et al., 2024; Marks et al., 2024; Gao et al., 2024). SAEs have been shown to find interpretable features in language models. SAEs appear to be a promising approach to help understand complex, abstract features that are used by language models (Templeton et al., 2024). Whether SAEs can be used to make systematic, predictable, interpretable interventions in language models in a variety of contexts remains an open question. Our work makes two key contributions: First, we attempt to develop a method for unlearning knowledge in language models in an interpretable way. Second, we apply SAEs to the task of unlearning knowledge, extending their use beyond previous work. Our approach aims to work towards more transparent and verifiable knowledge removal at a broader scale. Figure 1: An outline of how we use SAE features to intervene in the model. Selected feature activations fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are set to a negative value −c𝑐-c- italic_c when fi>0subscript𝑓𝑖0f_{i}>0italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0."
Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware Neuron Management,s m \IfBooleanF#1▷▷\triangleright▷ #2,"Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of applications (chatgpt, ; gpt4, ; llm-application-medical, ; llm-application-education, ; llm-application-finance, ; llm-application-engineer, ). Comprising millions or even billions of parameters (bert, ; opt, ; gpt3, ; llama2, ; palm, ; mistral, ; gemini, ), these models demand substantial computational and memory resources, typically available only in state-of-the-art data centers. Nonetheless, there is an increasing demand for deploying LLMs on resource-constrained devices, such as smartphones (llm-mobile-1, ; llm-mobile-2, ; llm-mobile-3, ; llm-mobile-4, ; llm-mobile-5, ; llm-mobile-6, ). On one hand, stringent privacy regulations necessitate local data processing to protect user information. On the other hand, LLMs on smartphones facilitate customization based on user habits, enabling enhanced personalization. Given the limited DRAM capacity of devices, LLMs on smartphones are typically constrained to models specially designed for mobile deployment (phi3, ; minicpm, ; gemini, ). Although these models are lightweight, the reduction in parameters inevitably leads to a compromise in their capabilities (scaling-law, ). As an alternative, many recent works (powerinfer-2, ; llm-flash, ; deja-vu, ; powerinfer, ; sparsity-mobile-survey-1, ; sparsity-mobile-survey-2, ) explore the exploitation of inherent sparsity within LLMs to address memory limitations. Specially, rather than pruning model parameters, these methods selectively activate a subset of model parameters based on the input while maintaining the original performance. By transferring only the activated parameters to DRAM for computation, larger and more powerful LLMs can be stored in external flash memory, effectively surpassing DRAM limitations of smartphones. Table 1. Breakdown of average inference latency per token when offloading 50% model parameters to flash memory. Model Compute Load Total Load Ratio OPT-350M 34 ms 87 ms 121 ms 71.9% OPT-1.3B 84 ms 273 ms 357 ms 76.5% OPT-6.7B 387 ms 1883 ms 2270 ms 82.9% Llama2-7B 450 ms 10982 ms 11432 ms 96.1% Mistral-7B 355 ms 15126 ms 15481 ms 97.7% Figure 1. Bandwidth utilization on smartphones is heavily constrained by IOPS. Ripple alleviates this bottleneck and boosts bandwidth with neuron co-activation linking. However, the efficiency of this LLM inference paradigm is significantly hindered by I/O overheads. Since different inference requests generally activate distinct sets of model parameters, frequent I/O operations are generated to swap parameters between DRAM and flash memory. As shown in Table 1, even when only half of the model parameters reside in flash memory, 71.9%-97.7% of the inference latency arises from I/O operations. More critically, the scattered activation of model parameters induces numerous small-grained read accesses, limiting transfer efficiency due to constraints in Input/Output Operations Per Second (IOPS) (iops, ). As depicted in Figure 1, this IOPS bottleneck severely restricts on-device bandwidth utilization across various LLMs. Building on these insights, this paper proposes Ripple, a novel approach to accelerating LLM inference on smartphones through I/O optimizations. While previous works (deja-vu, ; powerinfer, ) primarily focus on computation efficiency under activation sparsity, they tend to exacerbate the existing I/O overhead bottlenecks. Fewer studies (powerinfer-2, ; llm-flash, ) explore mitigating I/O overhead through enhanced caching strategies to minimize data loading. However, without directly improving bandwidth utilization, overall efficiency remains suboptimal. Orthogonal to these methods, Ripple addresses the primary bottleneck in LLM inference by maximizing bandwidth utilization via the effective reduction of I/O operations. The design of Ripple is rooted in Neuron Co-Activation, a property prevalent in activation sparsity yet underexplored in current works. Specially, neurons in LLMs exhibit strong correlations in their activation patterns. When processing real-world datasets, the activation of an individual neuron is consistently linked to the activation of a stable group of others. Given the efficiency of continuous reads, which enable the retrieval of larger data blocks with a single request, Ripple introduces a key insight: Why not establish links between neurons that are frequently co-activated in flash memory, facilitating continuous read access to reduce IOPS? However, this is not a low-hanging fruit, as both neuron co-activation patterns and storage hardware characteristics exhibit inherent complexity, complicating their effective alignment. Our comprehensive analysis identifies that three critical technical challenges must be tackled: (1) Extensive Search Space. The vast number of neurons in LLMs leads to an exponentially large space of possible neuron linking combinations. Identifying the optimized neuron linking that maximizes global benefits is exceedingly difficult and infeasible through brute-force enumeration alone. (2) Random Activation Variation. Owing to varying model inputs, the activation of model parameters exhibits intrinsic randomness. Although optimized placement strategies can spatially co-locate activated neurons, access to these neurons remains hindered by discontinuities caused by randomness. (3) Misaligned Cache Strategy. Storing frequently activated neurons in memory is critical for minimizing transfer workload. However, storing neurons individually leads to fragmentation in their placement within flash memory, potentially disrupting continuous access. To this end, Ripple employs a two-stage solution that performs hierarchical optimizations both offline and online. (1) In the Offline Phase, Ripple clusters neurons exhibiting high co-activation correlation and reorganizes their placement in flash memory. To address Challenge (1), we abstract the problem into a complete graph, reformulating it as the discovery of the globally optimal Hamiltonian Path. By leveraging graph-theoretic techniques, we propose a greedy algorithm that efficiently searches for optimized placement based on observed neuron co-activation patterns. (2) In the Online Phase, Ripple performs fine-grained refinements based on optimized neuron placement, further enhancing access continuity. To tackle Challenge (2), we devise an IOPS-friendly access collapse technique. By strategically incorporating additional neurons between two separate neuron links, we improve read access continuity with negligible overhead. In response to Challenge (3), we design a linking-aligned in-memory caching policy. Rather than individually caching the hottest neurons, we account for their interlinking relationships, ensuring efficient access patterns. We evaluate Ripple on three smartphones with distinct hardware configurations, benchmarking a diverse range of LLMs varying in structures and scales. The results demonstrate that Ripple significantly boosts on-device bandwidth, achieving improvements of up to 4.32×4.32\times4.32 ×. Moreover, this bandwidth optimization yields substantial reductions in I/O latency during inference, offering speedups of up to 5.93×5.93\times5.93 × when compared to state-of-the-art solutions. To the best of our knowledge, Ripple is the first to accelerate LLM inference on smartphones by enhancing I/O bandwidth through optimized neuron placement in flash memory. Ripple effectively bridges the performance gap between flash memory and DRAM, enabling LLM inference to exceed DRAM limitations on smartphones. Our contributions can be summarized as follows: • We identify the primary bottleneck in LLM inference on smartphones as IOPS, attributing it to the inherent misalignment between scattered activation patterns and storage hardware characteristics. • We notably exploit neuron co-activation to mitigate the IOPS bottleneck, pioneering the optimization of neuron placement in flash memory for enhancing bandwidth efficiency on smartphones. • We conduct extensive evaluations on various representative LLMs and hardware, achieving substantial improvements over state-of-the-art solutions."
"Datasheet for “Coordinated Reply Attacks in Influence Operations:
Characterization and Detection”",Motivation,
A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution Generalization to Adaptation,"Distribution shifts on graphs – the discrepancies in data distribution between training and employing a graph machine learning model – are ubiquitous and often unavoidable in real-world scenarios. These shifts may severely deteriorate model performance, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph machine learning under distribution shifts, aiming to train models to achieve satisfactory performance on out-of-distribution (OOD) test data. In our survey, we provide an up-to-date and forward-looking review of deep graph learning under distribution shifts. Specifically, we cover three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation. We begin by formally formulating the problems and discussing various types of distribution shifts that can affect graph learning, such as covariate shifts and concept shifts. To provide a better understanding of the literature, we systematically categorize the existing models based on our proposed taxonomy and investigate the adopted techniques behind. We also summarize commonly used datasets in this research area to facilitate further investigation. Finally, we point out promising research directions and the corresponding challenges to encourage further study in this vital domain. Additionally, we provide a continuously updated reading list at https://github.com/kaize0409/Awesome-Graph-OOD.","Driven by the prevalence of graph-structured data in numerous real-world scenarios, growing attention has been paid to graph machine learning, which effectively captures the relationships and dependencies among entities within graphs. In particular, Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations on graphs through message-passing [1, 2, 3], and they have demonstrated remarkable success across diverse applications, such as social networks, physics problems, and traffic networks [4, 5, 6]. While graph machine learning has achieved notable success, most of the existing efforts assume that test data follows the same distribution as training data, which is often invalid in the wild. When confronted with Out-Of-Distribution (OOD) samples, the performance of graph machine learning methods may substantially degrade, limiting their efficacy in high-stake graph applications such as finance and healthcare [7]. Although numerous transfer learning methods have been proposed to address distribution shifts for Euclidean data [8, 9, 10], their direct application to graph data is challenging. This is due to the interconnected nature of entities on graphs, which violates the independent and identically distributed (IID) assumption inherent in traditional transfer learning methods. Moreover, the various types of graph shifts introduce new challenges. These shifts occur across different modalities including features, structures, and labels, and can manifest in various forms such as variations in graph sizes, subgraph densities, and homophily [11]. Given these obstacles, increasing research efforts have been dedicated to improving the reliability of graph machine learning against distribution shifts, concentrating on three main scenarios: graph OOD generalization [7, 11], training-time graph OOD adaptation [12, 13], and test-time graph OOD adaptation [14, 15]. The primary distinction between graph OOD generalization and adaptation methods lies in their assumptions regarding the availability of target data. Graph OOD generalization methods typically assume the unavailability of target data during model training and aim to enhance the model’s generalization performance on any potential unseen test distribution. In contrast, both training-time and test-time adaptation methods assume the availability of target data and aim to improve model performance on this specific target. However, they differ in their assumptions about the source data and in how they utilize knowledge of the source distribution. Training-time adaptation assumes that both the source and target graphs are available simultaneously, allowing model adaptation to start from scratch during the training process. On the other hand, test-time adaptation typically assumes access to a model pre-trained on the source graph, rather than the source graph itself, and begins adapting the model to the target data from this pre-trained state. Although graph OOD generalization, training-time OOD adaptation, and test-time OOD adaptation are closely related, there is currently no unified framework that comprehensively discusses deep graph learning under distribution shifts across all three scenarios. With recent progress on graph OOD learning, an up-to-date and forward-looking review of this field is urgently needed. In this survey, we provide, to the best of our knowledge, the first unified and systematic review of the literature on deep graph learning under distribution shifts. We start by formally formulating the problems and discussing different types of graph distribution shifts in graph machine learning. Next, our new taxonomy is proposed, classifying existing methods into three categories based on the model learning scenario: (1) graph OOD generalization, where the generalizability is enhanced through strategic design of the model when training on source data, (2) training-time graph OOD adaptation, where the adaptation is performed when jointly training the model based on both source and target data [16, 17], and (3) test-time graph OOD adaptation, where the adaptation happens when adjusting a pre-trained source model to the target data [18, 19]. To deepen our understanding of these approaches, we further classify existing methods within each of the three categories into model-centric and data-centric strategies. Model-centric approaches focus on the learning process or the architecture of the graph model itself, enhancing the model’s inherent ability to generalize or adapt to distribution shifts by refining its structure, training objectives, or learning mechanisms. In contrast, data-centric approaches emphasize the manipulation of input graphs, improving model performance by addressing the data directly, either through preprocessing techniques or data augmentation strategies. Within each subline of research, we elaborate on the detailed techniques for enhancing the generalizability or adaptability under distribution shifts on graphs. Additionally, we provide a summary of the datasets utilized in these studies, highlighting their characteristics and relevance to the challenges posed by distribution shifts. Based on the current progress on graph OOD learning, at the end we also point out several promising research directions in this evolving field. Differences between this survey and existing ones. Despite the urgent need for an overview of graph learning under distribution shifts, existing surveys have primarily focused on subfields within this area, rather than providing a comprehensive overview from multiple scenarios. Until now, there have been several surveys in related areas, but exhibiting distinct focuses, including graph OOD generalization [7, 20], graph domain adaptation [21, 22], trustworthy graph learning [23] related to distribution shifts. Our work distinguishes itself from existing ones in the following aspects: (1) Main focus. Our survey centers on the challenges and solutions for graph learning under distribution shifts, while [23] analyzes OOD issues from a trustworthy perspective and does not delve into the methodological aspects. Conversely, [20] examines graph machine learning from a causal perspective, which is narrower than our broad examination. (2) Taxonomy. We provide a comprehensive categorization of existing methods and summarize them, whereas related work, such as [24], lacks such summaries. Other surveys like [21] and [22] primarily focus on domain adaptation without addressing the broader scope of graph OOD learning. Additionally, we provide coverage of the most recent advancements and discussions in this field. Survey structure. The general organization of this survey is presented as follows: Section 2 introduces the notations and preliminaries. Sections 3, 4 and 5 review graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation, respectively. Each section discusses model-centric and data-centric approaches within its scenario, further detailing the techniques associated with each category. Furthermore, Section 6 provides a comprehensive summary of the datasets used in the literature, highlighting popular graph datasets for evaluation and their relevance to the challenges posed by distribution shifts. Section 7 explores promising future research directions and the associated challenges. Finally, Section 8 presents the conclusion of this survey."
Spatioformer: A Geo-encoded Transformer for Large-Scale Plant Species Richness Prediction,"Earth observation data have shown promise in predicting species richness of vascular plants (α𝛼\alphaitalic_α-diversity), but extending this approach to large spatial scales is challenging because geographically distant regions may exhibit different compositions of plant species (β𝛽\betaitalic_β-diversity), resulting in a location-dependent relationship between richness and spectral measurements. In order to handle such geolocation dependency, we propose Spatioformer, where a novel geolocation encoder is coupled with the transformer model to encode geolocation context into remote sensing imagery. The Spatioformer model compares favourably to state-of-the-art models in richness predictions on a large-scale ground-truth richness dataset (HAVPlot) that consists of 68,170 in-situ richness samples covering diverse landscapes across Australia. The results demonstrate that geolocational information is advantageous in predicting species richness from satellite observations over large spatial scales. With Spatioformer, plant species richness maps over Australia are compiled from Landsat archive for the years from 2015 to 2023. The richness maps produced in this study reveal the spatiotemporal dynamics of plant species richness in Australia, providing supporting evidence to inform effective planning and policy development for plant diversity conservation. Regions of high richness prediction uncertainties are identified, highlighting the need for future in-situ surveys to be conducted in these areas to enhance the prediction accuracy.","Australia is home to a large and diverse range of plant species, with over 21,000 known native species of vascular plants and 93% of these being endemic [1, 2]. The richness of plant species, also known as α𝛼\alphaitalic_α-diversity, is highly important in maintaining the functioning of ecosystems, such as habitat provision, carbon sequestration, and water cycling [3, 4, 5, 6]. However, anthropogenic interference, such as deforestation, overgrazing, and urbanisation, has resulted in a decline in plant species richness [7, 8]. In response, conservation activities have been initialised and conducted across the country aiming to preserve plant diversity [9, 10, 11]. Accurate and up-to-date maps of plant species richness will strongly support effective planning and policy-making for these activities [12, 13]. Earth observation (EO) data provide rapid and near-real-time estimates of changes in land surface conditions across large regions [14, 15, 16, 17]. This makes remote sensing imagery a favourable data source for plant species richness modelling, as compared with another widely adopted approach where environmental variables, such as temperature, precipitation, soil texture, and topographic heterogeneity, are used as richness predictors [18]. The reason is that environmental variables drive mainly the environmental potential of plant habitats (i.e., the capacity to sustain a certain level of richness), rather than represent the actual conditions on the ground like those observed by remote sensing satellites. For example, deforestation, floods, and bushfires could cause reduction in richness [19], but such reduction might not be reflected by environmental variables. Therefore, environmental variables are often aimed at predicting the natural patterns in diversity in a pre-intensification reference state, while remote sensing data are more valuable for monitoring actual changes in those patterns. Australia covers an area of over seven million square kilometres. As a result of the relatively large geographical extents, versatile types of plant habitats are found across the country, differing in their inventories of plant species present that have been shaped by a variety of factors such as biogeographic history, climate, and geography [20]. To understand the spatiotemporal distribution of plant species richness, perseverant in-situ field surveys have been conducted over the past several decades. Via various survey campaigns, a wealth of 219,552 richness samples have been gathered across the country as of the year 2022 [18]. These samples represent a broad range of landscapes across the continent, and therefore present a unique opportunity to unravel the potentially intricate relationship between richness measurements and satellite observations. Nevertheless, geographically distant regions may exhibit distinct assemblages of plant species with differed compositional properties (i.e., β𝛽\betaitalic_β-diversity [21]), making it challenging to model richness over large spatial scales. Due to spatial variations in plant species composition, a location with a set of plant species would be expected to display quite different spectral features in remote sensing imagery from another location with a dissimilar plant composition, even if the two locations sustain the same richness of species [22, 23]. Through statistical regression analysis for two regions in southeast Australia, previous studies [13, 24] suggested that the relationship between plant species richness and hyper/multispectral satellite observations is region-specific. To account for the location dependency, we need a model that is capable of taking in geolocation context when mapping plant species richness over large spatial scales. The transformer model, first introduced in [25], is built upon the self-attention mechanism [26]. The model attends effectively to information of high importance in the input data, as the self-attention module is capable of capturing intricate data structures and dependencies [26, 27]. Initially proposed for language tasks, the transformer model has shown promise for image understanding due to its superior ability over Convolutional Neural Networks (CNNs) in capturing global dependencies between different regions of an image [28]. As a seminal work on applying transformer to image data, the Vision Transformer (ViT) model [28] first divides an image into non-overlapping patches, followed by projecting each patch into a feature vector which is then fed into the self-attention module, with state-of-the-art performance being achieved on benchmark datasets. Given remote sensing imagery captures rich features in the spectral dimension, the SpectralFormer model [29] was developed to effectively embed the spectral information. This model was later extended in [30], where FactoFormer, a factorised transformer, was introduced for the joint learning of spectral and spatial features. These studies have demonstrated the effectiveness of transformer in processing remote sensing images (e.g., [30, 29]), but to advance the model’s application to remote sensing images recorded over large spatial scales, geolocational information could be leveraged. Unlike many types of imagery whose semantics are independent of the location where they are recorded, remote sensing images are intrinsically associated with geolocations [31, 32]. Considering that the composition of plant species is location-specific, incorporating geolocation context could be helpful in modelling the location-dependent relationship between richness and remote sensing imagery. Geo-coordinates provide geographical priors that supplement geolocation context to the image data [33, 32, 34, 31, 35, 36, 37]. While a straightforward way to utilise geolocational features is to concatenate the original longitude and latitude coordinates into the model, this approach has shown to yield almost no gain in performance [34]. To deal with this problem, a geo-feature extraction approach was proposed in [34] for CNN models, where the geo-coordinates were projected into a higher dimensional feature space with a geolocation encoder, whose outputs were then merged into those of a CNN-based image network. It was observed that, by leveraging geolocation context, a 7% increase in accuracy was achieved for an image data set spanning over the continental United States [34]. This geo-encoded CNN model was later applied to global-scale vegetation canopy height mapping with satellite imagery [38], where the geolocations served as a prior. Other state-of-the-art geolocation encoders include Space2Vec [39], Sphere2Vec [40], PE-GNN [41], and a more recent algorithm that is based on the spherical harmonic basis functions [42]. Multi-scale sinusoidal functions are favoured in building these encoders (e.g., [39, 42]), thanks to their merits of being bounded in value, infinitely extended in space, and possessing a multi-resolution scalability. Geolocation encoding is demonstrated to be effective in many large-scale geospatial problems, such as animal species categorisation [38, 40], water quality prediction [43], event/activity recognition [44], and remote sensing scene classification [31, 40]. In this study, we aim to predict the spatiotemporal distribution of plant species richness in Australia from EO imagery with geolocation context being taken into account. Considering that the relationship between plant species richness and remote sensing imagery varies from one location to another due to differences in vegetation composition, we propose Spatioformer, where a novel geolocation encoder is coupled with the transformer model in order to incorporate the geolocation context. The performance of Spatioformer in richness mapping is compared with a CNN model, a ViT model, and the FactoFormer model where the geolocational information is not encoded. Through quantitative analyses, we seek to address primarily the following important questions: (1) Does Spatioformer perform better than state-of-the-art algorithms in predicting plant species richness over large spatial scales? (2) What are the spatial patterns of plant species richness in Australia inferred from remote sensing evidence? (3) Where future in-situ surveys should be conducted as suggested by the mapping results? The rest of the paper is organised as follows. Section II describes the study area and the datasets used for modelling, including the ground-truth samples of plant species richness and satellite imagery. Section III introduces the methods with a focus on the proposed Spatioformer model. This model is developed with the aim to capture the location-dependent relationships between plant species richness and remote sensing imagery over large spatial scales. Section IV describes the experimental settings for training and validation of the Spatioformer model. Section V presents the results of applying Spatioformer to plant richness mapping across Australia, and discusses the implications of these findings for biodiversity conservation and future research directions. Finally, Section VI concludes the paper."
CHESTNUT: A QoS Dataset for Mobile Edge Environments,"Quality of Service (QoS) is an important metric to measure the performance of network services. Nowadays, it is widely used in mobile edge environments to evaluate the quality of service when mobile devices request services from edge servers. QoS usually involves multiple dimensions, such as bandwidth, latency, jitter, and data packet loss rate. However, most existing QoS datasets, such as the common WS-Dream dataset, focus mainly on static QoS metrics of network services and ignore dynamic attributes such as time and geographic location. This means they should have detailed the mobile device’s location at the time of the service request or the chronological order in which the request was made. However, these dynamic attributes are crucial for understanding and predicting the actual performance of network services, as QoS performance typically fluctuates with time and geographic location. To this end, we propose a novel dataset that accurately records temporal and geographic location information on quality of service during the collection process, aiming to provide more accurate and reliable data to support future QoS prediction in mobile edge environments.","I original dataset description To create a high-quality dataset for predicting Quality of Service (QoS) in mobile edge environments, this study utilized two real-world datasets from Shanghai. One dataset is from the Shanghai Johnson Taxi, containing information such as the longitude, latitude, moving direction, and speed of the taxis on a specific day, which was used to simulate user mobile datasets. The other dataset is from Shanghai Telecom [1, 2, 3], providing the longitude and latitude of the base stations. Data from June 2014 was used to simulate the generation of edge server datasets. I-A Shanghai Johnson Taxi Dataset The Shanghai johnson taxi dataset is an important resource for traffic research, containing real-time GPS and business status information from taxis in Shanghai. Each record in the dataset includes various fields: the vehicle ID, control word (where A indicates normal and M indicates alarm), business status (0 for normal and 1 for alarm), passenger status (0 for occupied and 1 for unoccupied), top light status (with values ranging from 0 for operation to 5 for out of service), road type (0 for ground road and 1 for express road), brake status (0 for no braking and 1 for braking), meaningless fields, reception date, GPS timestamp, longitude, latitude, speed, direction, the number of satellites, and additional meaningless fields. This dataset enables the analysis of urban traffic flow, travel patterns, and traffic management strategies. In this paper, we use only the gps time, latitude, longitude, speed, and direction of the cab to generate motion information about the mobile user. I-B Shanghai Telecom Dataset This study utilized a telecommunications dataset provided by Shanghai Telecom, comprising over 7.2 million records of 9,481 mobile phones accessing the Internet through 3,233 base stations over six months.111http://sguangwang.com/TelecomDataset.html The dataset includes six parameters: month, date, start time, end time, base station location (latitude and longitude), and user ID used within Shanghai Telecom. This dataset can be used to evaluate solutions in mobile edge computing, such as edge server deployment, service migration, and service recommendation. In our research, we need to simulate the information of edge servers on base stations based on this dataset. Furthermore, considering the substantial volume of data, we only counted the data for one month and only focused on the geographic location information of Shanghai base stations."
Enhancing Exchange Rate Forecasting with Explainable Deep Learning Models,"Accurate exchange rate prediction is fundamental to financial stability and international trade, positioning it as a critical focus in economic and financial research. Traditional forecasting models often falter when addressing the inherent complexities and non-linearities of exchange rate data. This study explores the application of advanced deep learning models, including LSTM, CNN, and transformer-based architectures, to enhance the predictive accuracy of the RMB/USD exchange rate. Utilizing 40 features across 6 categories, the analysis identifies TSMixer as the most effective model for this task. A rigorous feature selection process emphasizes the inclusion of key economic indicators, such as China-U.S. trade volumes and exchange rates of other major currencies like the euro-RMB and yen-dollar pairs. The integration of grad-CAM visualization techniques further enhances model interpretability, allowing for clearer identification of the most influential features and bolstering the credibility of the predictions. These findings underscore the pivotal role of fundamental economic data in exchange rate forecasting and highlight the substantial potential of machine learning models to deliver more accurate and reliable predictions, thereby serving as a valuable tool for financial analysis and decision-making.","Since the dissolution of the Bretton Woods system, the adoption of a floating exchange rate regime has introduced significant challenges in risk management for market participants. The volatility of the RMB/USD exchange rate, particularly during periods of trade tensions, has heightened the uncertainty faced by those engaged in the foreign exchange market. The People’s Bank of China’s reform of the exchange rate fixing mechanism on August 11, 2015, further increased the marketization of the RMB exchange rate, leading to greater exchange rate volatility and increased foreign exchange risk. This has underscored the critical need for accurate exchange rate forecasting and effective risk management strategies. China’s growing role in the global supply chain, especially after its accession to the World Trade Organization (WTO), and the deepening economic ties between China and the United States, have made the RMB/USD exchange rate a focal point of global economic stability. Debates over the valuation of the RMB, particularly during periods of significant trade surpluses with the U.S., have led to multiple rounds of discussions on exchange rate policy. The RMB exchange rate reform in 2005 and the subsequent rounds of monetary policy adjustments by the Federal Reserve, especially during the 2007 financial crisis, further complicated the dynamics of the RMB/USD exchange rate. The “8.11 Exchange Rate Reform” in 2015, which introduced a more flexible exchange rate mechanism, and the intensified trade frictions since 2018, have put additional depreciation pressure on the RMB, making accurate forecasting of the RMB/USD exchange rate increasingly important [1, 2, 3, 4, 5]. Previous research on exchange rate forecasting has primarily focused on theoretical and quantitative models. Theoretical models often emphasize the equilibrium state of exchange rates, which can be difficult to achieve or maintain in practice, making short- to medium-term predictions particularly challenging. Quantitative models focus on the exchange rate’s own dynamics while often neglecting other critical influencing factors. Moreover, these models have struggled to produce consistent results across different studies. In recent years, there has been a notable shift towards using big data approaches in forecasting models, bypassing the need for complex mathematical modeling and allowing for more flexible model forms without predefined structures. The inherent complexity and non-linearity of exchange rate data have led to applying non-linear methods, such as chaos theory, non-parametric methods, and machine learning techniques, which have shown potential to improve forecasting accuracy. Studies like those of LeBaron and others have demonstrated that methods such as kernel ridge regression can significantly enhance the prediction of financial volatility, although some researchers, such as Mourer, have found that these methods do not always outperform simple autoregressive models in all contexts. Since the end of the Bretton Woods system, the transition to a floating exchange rate regime has posed significant challenges for managing risk, especially regarding the RMB/USD exchange rate. China’s integration into the global economy post-WTO accession, along with its deepening economic ties with the U.S., has made this exchange rate crucial for global economic stability. The 2015 reform of China’s exchange rate mechanism increased market-driven fluctuations, further intensified by trade tensions, which has heightened volatility. This evolving landscape has led to debates over the RMB’s valuation and spurred numerous policy discussions. Consequently, precise and reliable exchange rate forecasting has become essential for effective risk management in this volatile environment. Traditional exchange rate forecasting models, both theoretical and quantitative, have struggled with consistency and often neglect critical influencing factors. For example, theoretical models may fail to account for the real-time impact of policy changes and global economic shifts, while quantitative models may not fully capture the non-linear dynamics and intricate interactions of the variables involved. Recently, there has been a shift toward big data and machine learning approaches, which offer flexibility and improved accuracy in handling the complex, non-linear nature of exchange rate data [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]. While some methods like kernel ridge regression have shown promise, their performance varies across different contexts, highlighting the ongoing challenges in exchange rate prediction. Machine learning models, particularly deep learning models, have increasingly been applied to predicting time series and economic variables [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]. Despite their advantages in handling complex, non-linear data without requiring explicit assumptions about the underlying data distribution, these models are often criticized for their “black box” nature and lack of interpretability. Recent advancements, such as the application of Grad-CAM and attention mechanisms, have begun to address these issues, making it possible to visualize model predictions and understand the underlying decision-making processes. In related financial applications, researchers have combined advanced models like XGBoost with data balancing techniques including SMOTE to enhance predictive performance, as demonstrated by Chang et al. in their fraud detection study [60]. Their work not only showcased the effectiveness of this approach in fraud detection but also laid a foundation for developing robust models in various financial domains, including potentially exchange rate prediction [60]. However, applying these interpretability techniques has been mostly limited to fields like image recognition and natural language processing, with relatively few studies applying them to economic forecasting. Given the challenges of traditional models and the potential of machine learning approaches, this study seeks to explore the use of advanced deep learning models, including CNNs, RNNs, MLPs, and transformer-based architectures, for predicting the RMB/USD exchange rate. By incorporating a comprehensive set of features—drawn from economic indicators, trade data, and other currency pairs—and employing advanced feature selection techniques, this research aims to enhance predictive accuracy, identify the most relevant factors influencing exchange rate fluctuations, and enhance the interpretability of the model predictions. Contributions of This Study • Application of Deep Learning Models: This study provides an initial analysis of the effectiveness of deep learning models in exchange rate prediction, using MSE and MAE as key metrics to identify the best-performing models. • Enhancement of Predictive Performance: To improve the accuracy of machine learning models, this study employs various techniques, including feature selection, to reduce redundancy and retain the most relevant subset of features for exchange rate forecasting. • Analysis of Influential Factors Over Time: By applying attention mechanisms, this study enhances the interpretability of machine learning models, offering insights into how different factors influence exchange rate predictions across different periods. This analysis aims to uncover which aspects of economic data the models prioritize during the prediction process, thereby providing a more nuanced understanding of the underlying dynamics."
SHAP zero Explains All-order Feature Interactions in Black-box Genomic Models with Near-zero Query Cost,"With the rapid growth of black-box models in machine learning, Shapley values have emerged as a popular method for model explanations due to their theoretical guarantees. Shapley values locally explain a model to an input query using additive features. Yet, in genomics, extracting biological knowledge from black-box models hinges on explaining nonlinear feature interactions globally to hundreds to thousands of input query sequences. Herein, we develop SHAP zero, an algorithm that estimates all-order Shapley feature interactions with a near-zero cost per queried sequence after paying a one-time fee for model sketching. SHAP zero achieves this by establishing a surprisingly underexplored connection between the Shapley interactions and the Fourier transform of the model. Explaining two genomic models, one trained to predict guide RNA binding and the other to predict DNA repair outcomes, we demonstrate that SHAP zero achieves orders of magnitude reduction in amortized computational cost compared to state-of-the-art algorithms. SHAP zero reveals all microhomologous motifs that are predictive of DNA repair outcome, a finding previously inaccessible due to the combinatorial space of possible high-order feature interactions.","Shapley values have emerged as a theoretically robust method for explaining the local additive features of an input query to black-box models [1, 2, 3]. However, extracting biological knowledge from the emerging models in genomics demands a more global understanding, which requires explaining the nonlinear interactions among features and doing so for hundreds to thousands of input query sequences. Shapley value explanations have a high computational cost, with an exact calculation requiring an exponential number of model evaluations in the input dimension [4]. The cost is higher for nonlinear feature interactions and grows exponentially with a polynomial function of the input dimension. With the increasing growth of complex and large-scale models in genomics [5], often with only proprietary access, there is an urgent need for algorithms that can explain the nonlinear high-order interactions in these black-box models at scale–not just in a handful of sequences but among a host of query sequences. Consider the problem of explaining a black-box model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) that takes in a length-n𝑛nitalic_n DNA sequence 𝐱∈ℤ4n={A,T,C,G}n𝐱superscriptsubscriptℤ4𝑛superscript𝐴𝑇𝐶𝐺𝑛\mathbf{x}\in\mathbb{Z}_{4}^{n}=\{A,T,C,G\}^{n}bold_x ∈ blackboard_Z start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT = { italic_A , italic_T , italic_C , italic_G } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and outputs a real number. One popular method of explaining the prediction of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to the input query sequence 𝐱𝐱\mathbf{x}bold_x is using SHapley Additive exPlanations (SHAP) [6]. SHAP explains the output of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) by assigning a so-called SHAP value IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) to the ithsuperscript𝑖thi^{\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT nucleotide in 𝐱𝐱\mathbf{x}bold_x, where i𝑖iitalic_i belongs to the feature set D={1,2,…,n}𝐷12…𝑛D=\{1,2,\dots,n\}italic_D = { 1 , 2 , … , italic_n } (Fig. 1a): IS⁢V⁢(i)=∑T⊆D\{i}|T|!⁢(|D|−|T|−1)!|D|!⁢[vT∪{i}⁢(𝐱T∪{i})−vT⁢(𝐱T)].superscript𝐼𝑆𝑉𝑖subscript𝑇\𝐷𝑖𝑇𝐷𝑇1𝐷delimited-[]subscript𝑣𝑇𝑖subscript𝐱𝑇𝑖subscript𝑣𝑇subscript𝐱𝑇I^{SV}(i)=\sum_{T\subseteq D\backslash\{i\}}\frac{|T|!\,(|D|-|T|-1)!}{|D|!}% \left[v_{T\cup\{i\}}(\mathbf{x}_{T\cup\{i\}})-v_{T}(\mathbf{x}_{T})\right].italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) = ∑ start_POSTSUBSCRIPT italic_T ⊆ italic_D \ { italic_i } end_POSTSUBSCRIPT divide start_ARG | italic_T | ! ( | italic_D | - | italic_T | - 1 ) ! end_ARG start_ARG | italic_D | ! end_ARG [ italic_v start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT ) - italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ] . (1) IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) computes the marginal contribution of the ithsuperscript𝑖thi^{\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT nucleotide to the value function vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) for all the subsets T𝑇Titalic_T of features in D\{i}\𝐷𝑖D\backslash\{i\}italic_D \ { italic_i }, where the value function is the output of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to 𝐱Tsubscript𝐱𝑇\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT computed by marginalizing the absent nucleotides in D\T\𝐷𝑇D\backslash Titalic_D \ italic_T (Fig. 1a). SHAP requires computing vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) for every subset of features in T⊆D𝑇𝐷T\subseteq Ditalic_T ⊆ italic_D, which means evaluating f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) using a number of samples that grows exponentially with n𝑛nitalic_n (hereafter called sample complexity) and paying a computational cost that also grows exponentially with n𝑛nitalic_n (hereafter called computational complexity). Faithful Shapley Interaction index IF⁢S⁢I⁢(T)superscript𝐼𝐹𝑆𝐼𝑇I^{FSI}(T)italic_I start_POSTSUPERSCRIPT italic_F italic_S italic_I end_POSTSUPERSCRIPT ( italic_T ) (Faith-Shap) generalizes SHAP values defined for a single nucleotide i𝑖iitalic_i to an interaction defined over a set T⊆D𝑇𝐷T\subseteq Ditalic_T ⊆ italic_D (Equation (34)) [7]. Computing Faith-Shap requires evaluating the model for all possible subsets of features in D𝐷Ditalic_D, and then marginalizing over their power set with an exact computational cost that grows exponentially with a polynomial function of sequence length p⁢o⁢l⁢y⁢(n)𝑝𝑜𝑙𝑦𝑛poly(n)italic_p italic_o italic_l italic_y ( italic_n ). Figure 1: Schematic of the flowchart for the exact computation of SHapley Additive exPlanations (SHAP) and our approximation algorithm SHAP zero. a, Computing SHAP values exactly requires exponential model evaluations with sequence length n𝑛nitalic_n, per query sequence. This illustration shows computing one such term in IS⁢V⁢(i=1)superscript𝐼𝑆𝑉𝑖1I^{SV}(i=1)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i = 1 ), which measures the marginal contribution of the nucleotide T at site i=1𝑖1i=1italic_i = 1 to the set T={3,4,5}𝑇345T=\{3,4,5\}italic_T = { 3 , 4 , 5 } by evaluating the model for all four possible nucleotides at D\(T∪{i})={2}\𝐷𝑇𝑖2D\backslash(T\cup\{i\})=\{2\}italic_D \ ( italic_T ∪ { italic_i } ) = { 2 } in the sequence 𝐱T∪{i}subscript𝐱𝑇𝑖{\bf x}_{T\cup\{i\}}bold_x start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT and all 42=16superscript42164^{2}=164 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 16 possible nucleotide combinations at D\T={1,2}\𝐷𝑇12D\backslash T=\{1,2\}italic_D \ italic_T = { 1 , 2 } in the sequence 𝐱Tsubscript𝐱𝑇{\bf x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. b, In contrast, SHAP zero cleverly samples the sequence space and resolves a sparse bipartite graph to approximate the original model with its top-s𝑠sitalic_s Fourier coefficients. c, For each query sequence 𝐱𝐱{\bf x}bold_x, SHAP zero maps the top-s𝑠sitalic_s Fourier coefficients into the Möbius transform M⁢[𝐤]𝑀delimited-[]𝐤M[{\bf k}]italic_M [ bold_k ], where k is the feature interaction vector. The illustration shows one such sequence’s Möbius transform, composed of all the permutations of ℓ=3ℓ3\ell=3roman_ℓ = 3 nucleotides from the original sequence. d, SHAP zero computes IS⁢V⁢(i=1)superscript𝐼𝑆𝑉𝑖1I^{SV}(i=1)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i = 1 ) using a weighted sum of the Möbius coefficients with k1>0subscript𝑘10k_{1}>0italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0 (Equation (2)). SHAP zero amortizes the cost of finding the Fourier transform over the future explanations. The algorithms to approximately compute SHAP values are either stochastic estimators [6, 8, 9, 10, 11, 12] or model-based approximators [6, 13, 14, 15, 16, 17, 18]. Stochastic estimators, such as KernelSHAP [6], randomly subsample the feature subsets (T𝑇Titalic_T,vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )) and approximately solve a weighted-least squares problem to estimate IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ). These algorithms require many model evaluations in practice and impose an undesirable trade-off between sample complexity and accuracy [19]. Model-based approximators, such as DeepSHAP [6], take advantage of the model architecture (e.g., neural networks) to estimate Shapley values. These methods are often faster but still require many model evaluations and only work for white-box models. Algorithms to compute Shapley interactions in black-box models [20, 7, 21, 22], such as SHAP-IQ [20], subsample high-order interactions for efficiency but similar to stochastic estimators they need many model evaluations. To empirically demonstrate how current SHAP algorithms scale, let us consider TIGER [23], a recent model that predicts the efficiency of CRISPR-Cas13d guide RNA from their sequence and context. To explain which region in the guide RNA is the most determinant of efficiency, we estimated SHAP values of 1038103810381038 input query sequences. Finding the KernelSHAP [6] values of all the nucleotides in these sequences took about one day on our single NVIDIA RTX A6000 machine. Worse yet, finding only up to 3rdsuperscript3rd3^{\text{rd}}3 start_POSTSUPERSCRIPT rd end_POSTSUPERSCRIPT order feature interactions using SHAP-IQ took more than 81 days—revealing a severe scalability issue in current explainability algorithms. We posit that the model evaluations needed to estimate the SHAP values of an input query sequence have information that can be used to estimate the SHAP values for a new query sequence. Therefore, instead of independently evaluating the model for each query sequence, one can “recycle” the model evaluations to slash the sample and computational cost of explaining the model. Taking this idea to the extreme, we propose to do initial query-agnostic model evaluations to sketch the model and then use the sketch for model explanation. How can a model be sketched to be efficiently mapped to SHAP values and Shapley interactions? We discover a surprisingly underexplored connection between SHAP values and interactions and the model’s Fourier transform, enabling us to sketch the model and use the sketch for fast Shapley explanations. The Fourier transform provides a global sketching of the model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) irrespective of the query sequence or even the training distribution. Moreover, since Fourier is an orthonormal basis, it enables fast and sample-efficient algorithms for sketching black-box models in genomics that are compressible or near-sparse in the Fourier domain [24, 25, 26, 27]. Herein, we develop SHAP zero, an algorithm that estimates SHAP values and Shapley interactions with a near-zero additional cost per new query sequence after paying an initial up-front cost for model sketching (Fig. 1b). In developing SHAP zero, we make three distinct and interconnected contributions: First, we build on the existing algorithms in sparse Fourier transforms [27, 28] and develop a method to sketch a black-box model f⁢(𝐱)𝑓𝐱f({\bf x})italic_f ( bold_x ) with sequence input (defined over q𝑞qitalic_q alphabets) and real-valued output, in terms of its top-s𝑠sitalic_s Fourier coefficients with a sample complexity of 𝒪⁢(s⁢n2)𝒪𝑠superscript𝑛2\mathcal{O}(sn^{2})caligraphic_O ( italic_s italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and a computational complexity of 𝒪⁢(s⁢n3)𝒪𝑠superscript𝑛3\mathcal{O}(sn^{3})caligraphic_O ( italic_s italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). Second, we establish a mathematical formalism to map from (i) the Fourier transform to a transform from algebraic geometry called the Möbius transform and (ii) from the Möbius transform to Shapley-based explanations. The Möbius transform [29, 30] enables us to map the top-s𝑠sitalic_s Fourier coefficients of the model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to Shapley-based explanations in 𝒪⁢(s2⁢(2⁢q)ℓ)𝒪superscript𝑠2superscript2𝑞ℓ\mathcal{O}(s^{2}(2q)^{\ell})caligraphic_O ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 2 italic_q ) start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) time per query sequence, where ℓ≤5ℓ5\ell\leq 5roman_ℓ ≤ 5 caps the maximum order of model interactions in practice [31]. Third, we conduct large-scale experiments to explain two genomic models, TIGER [23] and inDelphi [32], with SHAP zero. We demonstrate that SHAP zero estimates all-order feature interactions with an amortized computational cost up to 1000-fold faster than current algorithms. SHAP zero reveals the GC content of the seed region in TIGER and microhomologous motifs in inDelphi as predictive high-order features, a task previously inaccessible due to the combinatorial space of possible feature interactions."
Sparse Decomposition of Graph Neural Networks,"Graph Neural Networks (GNN) exhibit superior performance in graph representation learning, but their inference cost can be high, due to an aggregation operation that can require a memory fetch for a very large number of nodes. This inference cost is the major obstacle to deploying GNN models with online prediction to reflect the potentially dynamic node features. To address this, we propose an approach to reduce the number of nodes that are included during aggregation. We achieve this through a sparse decomposition, learning to approximate node representations using a weighted sum of linearly transformed features of a carefully selected subset of nodes within the extended neighbourhood. The approach achieves linear complexity with respect to the average node degree and the number of layers in the graph neural network. We introduce an algorithm to compute the optimal parameters for the sparse decomposition, ensuring an accurate approximation of the original GNN model, and present effective strategies to reduce the training time and improve the learning process. We demonstrate via extensive experiments that our method outperforms other baselines designed for inference speedup, achieving significant accuracy gains with comparable inference times for both node classification and spatio-temporal forecasting tasks.","Figure 1: The pipeline overview for SDGNN framework (bottom pipeline). To compute GNN embedding efficiently, we use a transformation function to adapt node features and introduce sparse vectors associated with each node to gather information from critical neighbours. The parameters in the transformation function and the sparse vectors are determined by optimization to approximate the target GNN embeddings. Graph neural networks (GNN) have demonstrated impressive performance for graph representation learning (Hamilton et al., 2017; Veličković et al., 2018; Qu et al., 2019; Rampášek et al., 2022). Although there are numerous designs for GNN models, the essential idea is to represent each node based on its features and its neighbourhood (Wu et al., 2020; Zhou et al., 2020). The procedure of aggregating features from neighbour nodes is empirically and theoretically effective (Xu et al., 2019) in representing the graph structures and blending the features of the nodes. However, deploying GNN models to process large graphs is challenging since collecting information from the neighbour nodes and computing the aggregation is extremely time-consuming (Zhang et al., 2021; Tian et al., 2023; Wu et al., 2023; Liu et al., 2024). In this work, we tackle the efficient inference problem for GNN models in the online prediction setting (Crankshaw, 2019). Specifically, we need to compute the representations of a few arbitrary nodes. The main advantage is that the prediction can reflect potential dynamic features111The features of a sample may vary over time, e.g., dynamical features from sensors (Dawson et al., 2016), dynamic features in the recommendation systems (Chu & Park, 2009). of the input. The computational complexity is dominated by the number of receptive nodes, which rapidly increases as the number of layers in the model grows, for most message-passing-based and graph-transformer-based GNNs (Zeng et al., 2020; Min et al., 2022). Our goal is to reduce the inference time to linear complexity with respect to the number of layers and the average node degree. Recently, several studies have attempted to address this problem by combining the performance of GNN and the efficiency of MLPs (Zhang et al., 2021; Hu et al., 2021; Tian et al., 2023; Wang et al., 2023; Wu et al., 2023; Liu et al., 2024; Tian et al., 2024; Winter et al., 2024; Wu et al., 2024). Knowledge distillation (Hinton et al., 2015) and feature/label smoothing are used to construct effective MLP models to eliminate the cumbersome neighbour collection and aggregation procedure. Although efficient, these methods have a fundamental limitation: the features gathered at each node are assumed to contain sufficient information to predict the node label accurately. However, to achieve their full potential, especially when features can change at inference time, GNN models should take into account the features from neighbourhood nodes and the graph structure (Battaglia et al., 2018; Pei et al., 2020). Therefore, we ask the question: given any graph neural network model that relies on both the graph structure and the features of the neighbourhood, can we infer the representation of a node in linear time? Present work. We propose sparse decomposition for graph neural networks (SDGNN), an approximation to the original GNN models that can infer node representations efficiently and effectively. The SDGNN consists of a feature transformation function and sparse weight vectors for nodes in the graph. The representation of each node is then a weighted sum of the transformed features from a small set of receptive nodes. The sparsity of the weight vectors guarantees low inference complexity. The learnable feature transformation function and the sparse weight vectors grant the SDGNN flexibility to approximate a wide range of targeted GNN models. To find the optimal parameters in SDGNN, we formulate the approximation task as an optimization problem and propose a scalable and efficient solution that iterates between the learning of the transformation function and the optimization of the sparse weight vectors. We verify the approximation power of SDGNN and the scalability of our algorithm on seven node classification datasets and demonstrate how SDGNN can be effectively applied under the online prediction setting with two spatio-temporal forecasting datasets. SDGNN consistently outperforms recent state-of-the-art models designed for GNN inference speedup."
Adversarial Environment Design via Regret-Guided Diffusion Models,"Training agents that are robust to environmental changes remains a significant challenge in deep reinforcement learning (RL). Unsupervised environment design (UED) has recently emerged to address this issue by generating a set of training environments tailored to the agent’s capabilities. While prior works demonstrate that UED has the potential to learn a robust policy, their performance is constrained by the capabilities of the environment generation. To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). The proposed method guides the diffusion-based environment generator with the regret of the agent to produce environments that the agent finds challenging but conducive to further improvement. By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy. Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments. Project page: https://github.com/rllab-snu.github.io/projects/ADD","Deep reinforcement learning (RL) has achieved great success in various challenging domains, such as Atari [1], GO [2], and real-world robotics tasks [3, 4]. Despite the progress, the deep RL agent struggles with the generalization problem; it often fails in unseen environments even with a small difference from the training environment distribution [5, 6]. To train well-generalizing policies, various prior works have used domain randomization (DR) [7, 8, 9], which provides RL agents with randomly generated environments. While DR enhances the diversity of the training environments, it requires a large number of trials to generate meaningful structures in high-dimensional domains. Curriculum reinforcement learning [10, 11] has been demonstrated to address these issues by providing instructive sequences of environments. Since manually designing an effective curriculum for complicated tasks is challenging, prior works [12, 13] focus on generating curricula that consider the current agent’s capabilities. Recently, unsupervised environment design (UED, [14]) has emerged as a scalable approach, notable for its advantage of requiring no prior knowledge. UED algorithms alternate between training the policy and designing training environments that maximize the regret of the agent. This closed-loop framework ensures the agent learns a minimax regret policy [15], assuming that the two-player game between the agent and the environment generator reaches the Nash equilibrium. There are two main approaches for UED: 1) learning-based methods, which employ an environment generator trained via reinforcement learning, and 2) replay-based methods, which selectively replay among previously generated environments. The learning-based methods [14, 16, 17] utilize an adaptive generator that controls the parameters that fully define the environment configuration. The generator receives a regret of the agent as a reward and is trained via reinforcement learning to produce environments that maximize the regret. While the learning-based methods can directly generate meaningful environments, training the generator with RL is unstable due to the moving manifold [16]. Additionally, we observe that the RL-based generator has limited environment coverage, which limits the generalization capability of the trained agent. In contrast, the replay-based methods [18, 19, 20] employ a random generator and select environments to revisit among previously generated environments. Since the random generator can produce diverse environments without additional training, they outperform the learning-based methods in zero-shot generalization tasks [20]. However, the replay-based methods are sample inefficient as they require additional episodes to evaluate the regret on the randomly generated environments. In this work, we propose a sample-efficient and robust UED algorithm by leveraging the strong representation power of diffusion models [21]. First, to make UED suitable for using a diffusion model as a generator, we introduce soft UED, which augments the regret objective of UED with an entropy regularization term, as done in maximum entropy RL [22]. By incorporating the entropy term, we can ensure the diversity of the generated environments. Then, we present adversarial environment design via regret-guided diffusion models (ADD), which guides a diffusion-based environment generator with the regret of the agent to produce environments that are conducive to the performance improvement of the agent. Enabling this regret guidance requires the gradient of the regret with respect to the environment parameter. However, since the true value of the regret is intractable and the regret estimation methods used in prior works on UED are not differentiable, a new form of regret estimation method is needed. To this end, we propose a novel method that enables the estimation of the regret in a differentiable form by utilizing an environment critic, which predicts a return distribution of the current policy on the given environment. This enables us to effectively integrate diffusion models within the UED framework, significantly enhancing the environment generation capability. Since the regret-guided diffusion does not require an additional training of the environment generator, we can preserve the ability to cover the high-dimensional environment domain as the random generator of the replay-based method. Moreover, ADD can directly generate meaningful environments via regret-guided sampling as the learning-based methods. By doing so, ADD effectively combines the strengths of previous UED methods while addressing some of their limitations. Additionally, unlike other UED methods, ADD allows us to control the difficulty levels of the environments it generates by guiding the generator with the probability of achieving a specific return. It enables the reuse of the learned generator in various applications, such as generating benchmarks. We conduct extensive experiments across challenging tasks commonly used in UED research: partially observable maze navigation and 2D bipedal locomotion over challenging terrain. Experimental results show that ADD achieves higher zero-shot generalization performance in unseen environments compared to the baselines. Furthermore, our analysis on the generated environments demonstrates that ADD produces an instructive curriculum with varying complexity while covering a large environment configuration space. As a result, it is shown that the proposed method successfully generates adversarial environments and facilitates the agent to learn a policy with solid generalization capabilities."
Super Gradient Descent: Global Optimization requires Global Gradient,"Global minimization is a fundamental challenge in optimization, especially in machine learning, where finding the global minimum of a function directly impacts model performance and convergence. This report introduces a novel optimization method that we called Super Gradient Descent, designed specifically for one-dimensional functions, guaranteeing convergence to the global minimum for any k𝑘kitalic_k-Lipschitz function defined on a closed interval [a,b]𝑎𝑏[a,b][ italic_a , italic_b ]. Our approach addresses the limitations of traditional optimization algorithms, which often get trapped in local minima. In particular, we introduce the concept of global gradient which offers a robust solution for precise and well-guided global optimization. By focusing on the global minimization problem, this work bridges a critical gap in optimization theory, offering new insights and practical advancements in different optimization problems in particular Machine Learning problems like line search.","Global optimization plays a critical role in addressing complex real-life challenges across various fields. In engineering, it is applied to structural design optimization, where minimizing weight or material use while ensuring durability is essential for cost-effective and safe construction. In financial services, portfolio optimization requires balancing risk and return by finding the global minimum or maximum in investment strategies. In logistics and transportation, global optimization is crucial for solving routing problems such as determining the shortest path or optimizing delivery routes which leads to significant cost savings and improved efficiency. Similarly, in energy systems, global optimization is key to managing and distributing power more efficiently, reducing operational costs, and optimizing renewable energy usage. In machine learning, the need for global optimization is especially pronounced. The performance of models often depends on the ability to minimize complex, non-convex loss functions. While traditional methods like gradient descent are effective in many cases, they frequently encounter the problem of getting trapped in local minima, which can hinder the model’s overall performance. This is particularly relevant in tasks that require complex models where the optimization landscape is highly non-linear and fraught with local minima. The primary contribution of this work is the introduction of a novel algorithm named Super Gradient Descent. Unlike classical gradient descent, which collects only local information making it prone to local minima, the proposed method adapts the state’s change decision based on a global detection of the function change to ensure consistent progress towards the global minimum. We evaluate its performance on various one-dimensional functions, demonstrating that it provides superior convergence behavior, particularly in avoiding local minima and achieving the global optimum. This novel approach contributes to overcoming the challenges of non-convex optimization, offering a more reliable method for finding global solutions in machine learning."
Robust Thompson Sampling Algorithms Against Reward Poisoning Attacks,"Thompson sampling is one of the most popular learning algorithms for online sequential decision-making problems and has rich real-world applications. However, current Thompson sampling algorithms are limited by the assumption that the rewards received are uncorrupted, which may not be true in real-world applications where adversarial reward poisoning exists. To make Thompson sampling more reliable, we want to make it robust against adversarial reward poisoning. The main challenge is that one can no longer compute the actual posteriors for the true reward, as the agent can only observe the rewards after corruption. In this work, we solve this problem by computing pseudo-posteriors that are less likely to be manipulated by the attack. We propose robust algorithms based on Thompson sampling for the popular stochastic and contextual linear bandit settings in both cases where the agent is aware or unaware of the budget of the attacker. We theoretically show that our algorithms guarantee near-optimal regret under any attack strategy.","The multi-armed bandit (MAB) setting is a popular learning paradigm for solving sequential decision-making problems (Slivkins et al., 2019). The stochastic and linear contextual MAB settings are the most fundamental and representative of the different bandit settings. Due to their simplicity, many industrial applications such as recommendation systems frame their problems as stochastic or contextual linear MAB (Brodén et al., 2018; Chu et al., 2011). As one of the most famous stochastic bandit algorithms, Thompson sampling has been widely applied in these applications and achieves excellent performance both empirically (Chapelle & Li, 2011; Scott, 2010) and theoretically (Agrawal & Goyal, 2013; 2017). Compared to another popular exploration strategy known as optimality in the face of uncertainty (OFUL/UCB), Thompson sampling has several advantages: • Utilizing prior information: By design, Thompson sampling algorithms utilize and benefit from the prior information about the arms. • Easy to implement: While the regret of a UCB algorithm depends critically on the specific choice of upper-confidence bound, Thompson sampling depends only on the best possible choice. This becomes an advantage when there are complicated dependencies among actions, as designing and computing with appropriate upper confidence bounds present significant challenges Russo et al. (2018). In practice, Thompson sampling is usually easier to implement Chapelle & Li (2011). • Stochastic exploration: Thompson sampling is a random exploration strategy, which could be more resilient under some bandit settings Lancewicki et al. (2021). Despite the success, Thompson sampling faces the problem of low efficacy under adversarial reward poisoning attacks Jun et al. (2018); Xu et al. (2021); Liu & Shroff (2019). Existing algorithms assume that the reward signals corresponding to selecting an arm are drawn stochastically from a fixed distribution depending on the arm. However, this assumption does not always hold in the real world. For example, a malicious user can provide an adversarial signal for an article from a recommendation system. Even under small corruption, Thompson sampling algorithms suffer from significant regret under attacks. While robust versions of the learning algorithms following other fundamental exploration strategies such as optimality in the face of uncertainty (OFUL) and ϵitalic-ϵ\epsilonitalic_ϵ-greedy were developed Lykouris et al. (2018); Neu & Olkhovskaya (2020); Ding et al. (2022); He et al. (2022); Xu et al. (2023), there has been no prior investigation of robust Thompson sampling algorithms. The main challenge is that under the reward poisoning attacks, it becomes impossible to compute the actual posteriors based on the true reward, which is essentially required by the algorithm. Naively computing the posteriors based on the corrupted reward causes the algorithm to be manipulated by the attacker arbitrarily (Xu et al., 2021). This work. We are the first to show the feasibility of making Thompson sampling algorithms robust against adversarial reward poisoning. Our main contribution is developing robust Thompson sampling algorithms for stochastic and linear contextual bandits. We consider both cases where the corruption budget of the attack is known or unknown to the learning agent. The regrets induced by our algorithms under the attack are near-optimal with theoretical guarantees. We adopt two ideas to achieve robustness against reward poisoning attacks in the two MAB settings. The first idea is ‘optimality in the face of corruption.’ In the stochastic MAB setting, we show that the Thompson sampling algorithm can maintain sufficient explorations on arms and identify the optimal arm by relying on optimistic posteriors considering potential attacks. The second idea is to adopt a weighted estimator He et al. (2023) that is less susceptible to the attack. In the linear contextual MAB setting, we show that with such an estimator, the influence of the attack on the estimation of the posteriors is limited, and the Thompson sampling algorithm can almost always identify the optimal arm at each round with a high probability. We empirically demonstrate the training process of our algorithms under the attacks and show that our algorithms are much more robust than other fundamental bandit algorithms, such as UCB, in practice. Compared to the state-of-the-art robust algorithm CW-OFUL He et al. (2022) for linear contextual bandit setting, our algorithm is as efficient, and in addition, it inherits the advantages from using Thompson sampling exploration strategy as aforementioned."
Learning the Regularization Strength for Deep Fine-Tuning via a Data-EmphasizedVariational Objective,"A number of popular transfer learning methods rely on grid search to select regularization hyperparameters that control over-fitting. This grid search requirement has several key disadvantages: the search is computationally expensive, requires carving out a validation set that reduces the size of available data for model training, and requires practitioners to specify candidate values. In this paper, we propose an alternative to grid search: directly learning regularization hyperparameters on the full training set via model selection techniques based on the evidence lower bound (“ELBo”) objective from variational methods. For deep neural networks with millions of parameters, we specifically recommend a modified ELBo that upweights the influence of the data likelihood relative to the prior while remaining a valid bound on the evidence for Bayesian model selection. Our proposed technique overcomes all three disadvantages of grid search. We demonstrate effectiveness on image classification tasks on several datasets, yielding heldout accuracy comparable to existing approaches with far less compute time.","††footnotetext: Open-source code: https://github.com/tufts-ml/data-emphasized-ELBo When fine-tuning deep neural networks (DNNs), a significant amount of computational resources are devoted to tuning hyperparameters that control model complexity to manage tradeoffs between under- and over-fitting on the target task of interest. One widespread example would be tuning the value of the scalar multiplier that controls the strength of an additive loss term computed as the sum-of-squares on weight coefficient values, known in various communities as L2 regularization [33], Ridge penalty [14, 21], or “weight decay” [24, 9]. A common technique for tuning such hyperparameters is to hold out a dedicated validation set and use grid search to find the hyperparameters that perform best on the validation set [36, 32]. While reasonably effective and in widespread use to manage over-fitting in recent transfer learning [45, 39], using grid search for hyperparameter selection has three key drawbacks. First and perhaps most important, the need to train separate models at each possible value in the grid significantly increases computational runtime and resources. Second, the need to carve out a validation set to assess performance reduces the amount of available data that can inform model training. This can cause under-fitting, especially when available data has limited size. Finally, grid search requires a list of candidate values specified in advance, yet ideal values may vary widely depending on the data and specific classification task at hand. We take another approach to hyperparameter selection, inspired by a pragmatic Bayesian perspective. Suppose we model observable dataset 𝒟𝒟\mathcal{D}caligraphic_D via a likelihood p⁢(𝒟|θ)𝑝conditional𝒟𝜃p(\mathcal{D}|\theta)italic_p ( caligraphic_D | italic_θ ), where θ𝜃\thetaitalic_θ is a high-dimensional parameter to be estimated, with prior distribution p⁢(θ|η)𝑝conditional𝜃𝜂p(\theta|\eta)italic_p ( italic_θ | italic_η ) controlled by hyperparameter η𝜂\etaitalic_η (say just 1-5 dimensions). Instead of point estimating a specific θ,η𝜃𝜂\theta,\etaitalic_θ , italic_η pair, we can instead estimate a posterior p⁢(θ|𝒟,η)𝑝conditional𝜃𝒟𝜂p(\theta|\mathcal{D},\eta)italic_p ( italic_θ | caligraphic_D , italic_η ) while simultaneously directly learning η𝜂\etaitalic_η to optimize p⁢(𝒟|η)=∫θp⁢(𝒟,θ|η)⁢𝑑θ𝑝conditional𝒟𝜂subscript𝜃𝑝𝒟conditional𝜃𝜂differential-d𝜃p(\mathcal{D}|\eta)=\int_{\theta}p(\mathcal{D},\theta|\eta)d\thetaitalic_p ( caligraphic_D | italic_η ) = ∫ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_p ( caligraphic_D , italic_θ | italic_η ) italic_d italic_θ. This latter objective p⁢(𝒟|η)𝑝conditional𝒟𝜂p(\mathcal{D}|\eta)italic_p ( caligraphic_D | italic_η ) is known as the marginal likelihood or evidence [26]. The evidence naturally encodes a notion of Occam’s razor, favoring the hyperparameter setting that leads to the simplest model that fits the data well, while penalizing complex models that over-fit the training data [18, 27, 2]. Learning η𝜂\etaitalic_η to maximize evidence (or equivalently, the logarithm of evidence) via gradient descent avoids all three issues with grid search: we need only one run of gradient descent (not separate efforts for each candidate η𝜂\etaitalic_η value in a grid), we can use all available labeled data for training without any validation set, and we can explore the full continuous range of possible η𝜂\etaitalic_η values rather than a limited discrete set that must be predefined. While elegant in theory, this vision of selecting hyperparameters via maximizing evidence is difficult in practice for most models of interest due to the intractable high-dimensional integral that defines the evidence. For modern deep image classifiers with millions of parameters, computing the evidence directly seems insurmountable even for a specifc η𝜂\etaitalic_η, let alone optimizing evidence to select a preferred η𝜂\etaitalic_η value. In this work, we use and extend tools from variational Bayesian methods [3, 19], specifically tractable lower bounds on the evidence, to make hyperparameter selection for fine-tuning deep neural image classifiers possible. Ultimately, we contribute methods that should help practitioners perform cost-effective transfer learning on custom datasets. When available data is plentiful, our experiments suggest our approach is competitive in accuracy while reducing total training time from 16 hours for L2-SP [45] and 150 hours for PTYL [39] (using the grid search ranges recommended by the original authors) to under 3 hours. When available data is limited, e.g., only 5-300 labeled examples per class, our experiments suggest our approach can be particularly effective in improving accuracy and runtime."
Spatial Shortcuts in Graph Neural Controlled Differential Equations,"rnn short=RNN, long=Recurrent Neural Network \DeclareAcronymnde short=NDE, long=Neural Differential Equation \DeclareAcronymgncde short=GNCDE, long=Graph Neural Controlled Differential Equation \DeclareAcronymncde short=NCDE, long=Neural Controlled Differential Equation \DeclareAcronymnn short=NN, long=Neural Network \DeclareAcronymnode short=NODE, long=Neural Ordinary Differential Equation \DeclareAcronympde short=PDE, long=Partial Differential Equation \DeclareAcronymgnn short=GNN, long=Graph Neural Network \DeclareAcronymlstm short=LSTM, long=Long Short Term Memory \DeclareAcronymgru short=GRU, long=Gated Recurrent Unit \DeclareAcronymagc short=AGC, long=Adaptive Graph Convolution \DeclareAcronymresnet short=ResNet, long=Residual Neural Network \DeclareAcronymmae short=MAE, long=Mean Absolute Error","Effect follows cause. When a problem is represented on a graph, its structure contains information on how causes at one spatial position are linked to effects at another. To learn about physical dynamics from spatial time series data, one can often leverage this structural information. Schölkopf [1] describes differential equations as the gold standard for understanding cause-effect structures and highlight the lack of a time component in statistical machine learning methods. \acpnde [2] are able to learn a hidden state that evolves continuously in time, and could remedy this lack. With Neural Controlled Differential Equations (NCDE) [3], one can update the hidden state continuously with data incoming at different points in time. If one also wants to account for spatial dependencies, \acpgncde [4] can be used, where a node embedding is learned to capture these spatial dependencies. We incorporate prior known graph topology information into a \acgncde to infer the future dynamics at the vertices. We therefore generate data coming from a graph advection simulation from which we know the underlying graph topology plus the temporal cause and effect relation. We will outline the close connection between the graph information in the data generated and the artificial \acnn architecture. Then we train our Informed \acpgncde and let them learn the dynamics to predict their future behavior. We start by describing advection on graphs and the theory before we explain the data generation and our Informed \acgncde. We believe this approach can lead to improvements in domains where graph information is available or where time series data is scarce or partially missing. \acpncde are an effective method in this context due to their ability to handle irregular time series data [5]. With graph information one has the potential to predict with fewer observations, as the graph structure itself does not have to be learned on top of the temporal dynamics. Promising domains for introducing known graph structure are traffic forecasting, river water level forecasting, climate and weather prediction, or disease spread (see A.4). Figure 1: Advection of an initial Gaussian pulse on a graph with 5 edges over time"
MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular Metaverse Services,"Updates of extensive Internet of Things (IoT) data are critical to the immersion of vehicular metaverse services. However, providing high-quality and sustainable data in unstable and resource-constrained vehicular networks remains a significant challenge. To address this problem, we put forth a novel immersion-aware model trading framework that incentivizes metaverse users (MUs) to contribute learning models trained by their latest local data for augmented reality (AR) services in the vehicular metaverse, while preserving their privacy through federated learning. To comprehensively evaluate the contribution of locally trained learning models provided by MUs to AR services, we design a new immersion metric that captures service immersion by considering the freshness and accuracy of learning models, as well as the amount and potential value of raw data used for training. We model the trading interactions between metaverse service providers (MSPs) and MUs as an equilibrium problem with equilibrium constraints (EPEC) to analyze and balance their costs and gains. Moreover, considering dynamic network conditions and privacy concerns, we formulate the reward decisions of MSPs as a multi-agent Markov decision process. Then, a fully distributed dynamic reward method based on deep reinforcement learning is presented, which operates without any private information about MUs and other MSPs. Experimental results demonstrate that the proposed framework can effectively provide higher-value models for object detection and classification in AR services on real AR-related vehicle datasets compared to benchmark schemes.","The metaverse, envisioned as one of the next evolution directions of the Internet, is designed to create a fully immersive, self-sustaining virtual environment in which people can engage in activities such as playing, working, and socializing [1]. This vision is propelled by advances in the fifth-generation (5G) and the forthcoming sixth-generation (6G) communication technologies, renowned for their low latency and high data throughput. These technologies play a critical role in seamlessly integrating the Internet of Things (IoT) data into metaverse services, thus bringing the once-fictional concept of immersive experiences closer to reality. Metaverse services are beginning to reveal their vast potential across a broad spectrum of industries, from gaming and autonomous driving to education and marketing. Notably, the application of vehicles within the metaverse has captured significant interest due to the enhanced safety features and immersive experiences offered by state-of-the-art augmented reality (AR) technologies. Figure 1: An example of AR services in the vehicular metaverse222Image source: https://www.jasoren.com/ar-in-automotive/. Market report [2] forecasted that the global automotive metaverse market will grow from 1.91.91.91.9 billion in 2022 to 6.56.56.56.5 billion by 2030. Automakers like BMW have recently doubled down on AR technology in their projects. As shown in Fig. 2, augmented information makes driving safer by showing potential hazards hidden behind the vehicle in front of you. Moreover, Nissan’s [3] upcoming technology utilizes a 3D AR interface that merges the real world with the virtual world to provide the driver with augmented information about the surrounding area. In addition, Vanarama has also implemented a visual parking assistance function using AR [4], which is expected to identify empty spaces in busy car parks and display useful information, such as parking fees, on the windscreen. The ability to capture information from the “real” world, particularly the ability to collect and process massive data from IoT devices, is the key to determining the success of immersive services (e.g., AR) in the vehicular metaverse. Meanwhile, the data must be processed and presented in a meaningful, responsive, and appropriately protected way. Technically, a high-quality experience with AR services relies on accurate detection and classification of real-world objects (e.g., cars and pedestrians) under complex conditions [5]. To achieve this goal, sufficient valid data needs to be collected and processed in depth to detect and classify objects accurately. Therefore, it is essential to focus on effectively collecting, processing, and protecting the data that supports a safer and more enjoyable driving experience. Motivations. An existing widely-used data collection method, as adopted by Nissan [6], involves collecting massive amounts of data through vehicle sensors, cameras, and roadside devices, and then processing all the data centrally. However, when it comes to the situation with multiple metaverse users (MUs) and metaverse service providers (MSPs) on behalf of different companies, the centralized data collection approach is not applicable and may lead to the following issues. First, AR services in the vehicular metaverse need to be highly immersive so that MUs feel fully immersed in the rendered environment, such as visualized driving. However, providing a seamless AR experience is challenging due to the latency caused by massive real-time data updates under unstable and resource-limited communication conditions. Note that the value of real-time data diminishes over time [7]. Also, delays can severely impact the MU’s experience and cause dizziness [8]. Second, the data to be transmitted may be sensitive and private, such as location, movement, and biometrics, which can create a better immersive experience but may inevitably increase the privacy risk of MUs [9]. Third, it is difficult to share data between MSPs due to competition. This leads to the same data being sampled multiple times by different MSPs in the physical world, resulting in wasted resources (e.g., sampling costs) and data redundancy. Moreover, tremendous data pressure and computational burden are imposed on MSPs. Therefore, an efficient method for collecting high-quality data while preserving privacy is needed to achieve immersive AR services. Challenges. To address the above issues, we propose an immersion-aware model trading framework designed to provide data support for AR services and ensure privacy by integrating federated learning (FL). Adopting FL enables MUs to contribute diverse learning models to multiple MSPs using local data, thus effectively protecting sensitive information [10]. Moreover, learning models are uploaded to MSPs for AR services by individual MUs, rather than centrally transmitting large amounts of data, which significantly reduces the communication burden. Nonetheless, developing such a model trading framework still faces the following challenges: • MUs are typically self-interested and are reluctant to share learning models with MSPs due to the additional computation, communication, and energy overhead. Thus, the challenge here is how to incentivize them to become contributors in providing high-value learning models that will benefit vehicular metaverse services. • It is difficult to design an appropriate metric to comprehensively evaluate the value of learning models contributed by MUs due to the diverse aspects involved in model valuation. • MUs have different sampling costs and limited computation and communication resources, while MSPs differ in their model preferences and compete with each other. Hence, the next challenge is to model the dynamic competitive interactions among MSPs and to achieve an equilibrium of interests among MUs and MSPs in the model trading. Figure 2: The outline of an immersion-aware framework for FL-assisted vehicular metaverse. Contributions. To address these challenges, we model the dynamic competitive trading interactions as an equilibrium problem with equilibrium constraints (EPEC) involving multiple leaders (MSPs) and multiple followers (MUs), which is a hierarchical optimization problem with equilibria at two levels [11]. Furthermore, to evaluate the value of learning models contributed by MUs to MSPs, we design a new metric called the immersion of the learning model (IoM). The metric jointly considers the freshness and accuracy of the learning model, as well as the amount and potential value of raw data used for training. Moreover, given the dynamic networks and the privacy concerns of MSPs, we formulate the reward decisions of MSPs as a multi-agent Markov decision process (MAMDP) and develop a multi-agent deep reinforcement learning (DRL)-based dynamic reward (MDDR) approach to obtain the reward decisions in a fully distributed manner. The outline of the proposed framework is shown in Fig. 2, and the main contributions of this paper are summarized as follows: • We propose an immersion-aware trading framework for learning models with FL assistance, incentivizing MUs to contribute high-value learning models for MSPs in a privacy-preserving and efficient manner. Moreover, a novel metric called “IoM” is designed to quantify the immersion enhancement provided by MUs for AR services. • We model the dynamic competitive trading interactions as an EPEC and provide theoretical support for the existence and uniqueness of equilibria at two levels. Moreover, a fully distributed MDDR approach is developed to adapt to dynamic environments and address complex reward decisions of MSPs without accessing any private information of MUs or MSPs. • We conduct extensive numerical simulations based on AR-related vehicle datasets to validate the efficacy and efficiency of MDDR and the proposed immersion-aware model trading framework. Compared to benchmark schemes, the framework better motivates MUs to provide higher-value learning models for object detection and classification functions of AR services effectively. The rest of this paper is organized as follows. Section II discusses the related work. In Section III, we present the system overview and design the immersion metric of the learning model. Section IV gives the game formulation, and Section V analyzes the existence of the equilibria at two levels. In Section VI, we give the detailed design of MDDR. Section VII shows numerical experiments to evaluate the framework performance, and finally Section VIII concludes the paper."
Conformal Prediction for Multimodal Regression,capbtabboxtable[][\FBwidth],"1 Background and related work The comprehension of the internals of neural networks has begun with efforts towards explainability but little has been done to exploit these internal features. The area of conformal prediction (CP) for regression is no different. The consequence is that rich multimodal input features such as images, unstructured text, and categoricals have not participated in the conformal regression prediction uncertainty quantification process. CP provides a straightforward framework for constructing statistically rigorous uncertainty intervals for model predictions. Importantly, these intervals are valid regardless of the underlying distribution, providing explicit, non-asymptotic guarantees without depending on specific distributional or model assumptions. By using conformal prediction on any pre-trained model, it is possible to create intervals that consistently contain the true value with a specified coverage level. Angelopoulos and Bates (2021). CP input assumptions are based on data exchangeability. There is no other assumption about the data distribution or the model. This condition is satisfied for the problems outlined in this paper. The more developed field of CP classification has done image-to-image uncertainty Angelopoulos et al. (2022), and even proposed changing CP regression tasks into classification problems Guha et al. (2023). Despite a comprehensive literature review, no findings similar to those proposed by our novel use of internal features in CP for multimodal regression have been uncovered. The inductive, or split, approach to creating conformal regressors, outlined by Papadopoulos et al. (2002), involves splitting the training data into a proper training set and a calibration set, both of which should accurately represent the overall data distribution. The training set is used to build the regression model, while the calibration set is used to calculate the model’s absolute residuals. Conformal predictive systems (CPS) enhance conformal regressors by generating cumulative probability distributions, over potential target values Vovk et al. (2020). This enables the creation of PIs with a specified confidence level, effectively capturing prediction uncertainty while providing a statistically grounded basis for decision-making. Typically, the more complicated multimodal inputs of images and unstructured text are processed by larger more complicated networks (e.g., convolutional neural networks (CNN), transformers, hybrid networks, etc.). These networks can be compute-intensive. Therefore, building multiple models and having PIs generated in an ensemble fashion may not be an option. The inductive implementation of the aforementioned CPSs was the chosen approach as the model is only trained once after a hold-out calibration set is removed from the training set. Multimodal architectures generally contain an internal combining stage where all the processed inputs come together. These internal inputs will be referred to as internal features within the respective architectures. Conversely, external features are features that are fed to the model at the input layer. The possible advantage with working with internal features is that they have been filtered for significance and weighted for importance which is not true of the input features. This results in a metric suitable for distance-based conformal prediction. The major contribution of this work is to address the challenges associated with conformal prediction in the context of multimodal regression tasks involving diverse input data, such as tabular, unstructured text, and image modalities. Traditional conformal prediction methods face significant obstacles when applied directly to such heterogeneous input features in regression problems. However, this paper demonstrates a novel approach that leverages an internal feature layer within the model architecture for multimodal regression. Specifically, we show that this intermediate representation can be effectively utilized to generate a calibration set suitable for conformal prediction in regression scenarios. This methodology extends the applicability of conformal prediction techniques to complex, multimodal regression tasks, thereby enhancing uncertainty quantification capabilities for a wider range of machine learning regression models and diverse data types."
Deep learning-based identification of patients at increased risk of cancer using routine laboratory markers,"Early screening for cancer has proven to improve the survival rate and spare patients from intensive and costly treatments due to late diagnosis. Cancer screening in the healthy population involves an initial risk stratification step to determine the screening method and frequency, primarily to optimize resource allocation by targeting screening towards individuals who draw most benefit. For most screening programs, age and clinical risk factors such as family history are part of the initial risk stratification algorithm. In this paper, we focus on developing a blood marker-based risk stratification approach, which could be used to identify patients with elevated cancer risk to be encouraged for taking a diagnostic test or participate in a screening program. We demonstrate that the combination of simple, widely available blood tests, such as complete blood count and complete metabolic panel, could potentially be used to identify patients at risk for colorectal, liver, and lung cancers with areas under the ROC curve of 0.76, 0.85, 0.78, respectively. Furthermore, we hypothesize that such an approach could not only be used as pre-screening risk assessment for individuals but also as population health management tool, for example to better interrogate the cancer risk in certain sub-populations.","This paper focuses on the use of multiple biomarkers for the assessment of patients, or identification of otherwise healthy individuals, who are at increased risk of cancer. With the high mortality rate associated with cancer patients, significant research has been conducted to help identify patients at higher risk, starting with identifying medical conditions that increase the risk of cancer, such as diabetes, or genetic predispositions that promote its development[1]. Furthermore, various screening procedures have been developed to help facilitate early diagnosis such as the Faecal Immunochemical Test (FIT) and colonoscopy for colorectal cancer (CRC)[2], mammography for breast cancer[3], and low-dose computed tomography (LDCT) for lung cancer[4]. However, cancer screening rates and their uptake remains lower than desired, e.g., in the US[5]. While there are several factors contributing to this low uptake, one of the key factors is the lack of awareness within the general population. This is even more important to address for people who may be at increased risk and would benefit from early and/or regular screening. In other words, there is still a need for convenient tests for early detection of rapidly progressing diseases such as cancer so that intervention can start as early as possible[6]. Several cancer risk prediction/assessment tools based on demographic, socioeconomic or blood based markers have been developed over the years, and studies have shown that cancer risk assessment algorithms could have an impact in early cancer diagnosis[7]. For instance, the Qcancer 10 year risk algorithm[8] considers the age, ethnicity, deprivation, body mass index, smoking, alcohol, previous cancer diagnoses, family history of cancer, relevant comorbidities, and medication data for a patient and predicts the cancer risk for 11 types of cancers. Nartowt et al.[9] reported high concordance in the prediction of CRC into low, medium and high groups using an artificial neural network trained on patient data comprising age, sex, and complete blood count (CBC). ColonFlag[10] can be used to identify individuals at high risk of CRC using specific blood-based markers and refer them to screening procedures such as colonoscopy. More recently, a cell-free DNA-based blood test for the early detection of CRC has been clinically validated in the ECLIPSE study[11]. Moreover, multi-cancer early detection technologies[12] such as the Galleri test[13, 14] can identify abnormal methylation patterns in cell-free DNA to detect a cancer signal and predict its origin. Besides algorithm development, the deployment of the algorithm and communication of the findings play a critical role in acceptance and clinical use of the algorithm[15], and must be taken into account to facilitate screening uptake. To this end, instead of defining a test with a specific set of ingredients catered towards a particular cancer, we propose to use commonly measured blood markers, often obtained during the annual physical exam, and obtain a risk profile for multiple cancers. Furthermore, instead of reporting a risk score, we compute the pre- and post-test odds of a patient at risk of developing cancer over the next 12 months. A key challenge in developing a model that considers several biomarkers is to deal with a significant degree of missingness in the historical data as not all markers may be obtained at each encounter. Although this issue can be partly alleviated by considering the biomarkers obtained at an annual physical exam, we observed that in real world data, there is still a significant degree of missingness, either due to a lack of awareness, insurance coverage or reimbursement, among other reasons. A standard approach to deal with missingness in input data is to impute the missing values using statistical methods, such as expectation maximization and regression[16]. However, the quality of imputed data is limited and can significantly impact the generalization ability of the trained model. In this work, we address the aforementioned challenges by training a deep learning model, Deep Profiler, which takes the age, sex, and commonly obtained blood biomarkers included in CBC and Comprehensive Metabolic Panel (CMP), and outputs a likelihood ratio of a patient to develop cancer over the period of the following 12 months (see Figure 1). The Deep Profiler architecture employs a variational autoencoder (VAE) model that is pre-trained to impute missing data similar to the masked language modeling technique. Subsequently, we train cancer-specific risk prediction models from the shared encoded latent space and compute the likelihood ratio for each patient. We validate the proposed method over screening-relevant cohorts for three different cancers - colorectal, liver, and lung. These are among the top cancers responsible for cancer related mortality rate in the US (https://seer.cancer.gov/statfacts/html/common.html, accessed April 30, 2024.). Figure 1: Workflow of using a biomarker-based pre-screening test."
Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites.,"Machine learning (ML) models benefit from large datasets. Collecting data in biomedical domains is costly and challenging, hence, combining datasets has become a common practice. However, datasets obtained under different conditions could present undesired site-specific variability. Data harmonization methods aim to remove site-specific variance while retaining biologically relevant information. This study evaluates the effectiveness of popularly used ComBat-based methods for harmonizing data in scenarios where the class balance is not equal across sites. We find that these methods struggle with data leakage issues. To overcome this problem, we propose a novel approach “PrettYharmonize”, designed to harmonize data by pretending the target labels. We validate our approach using controlled datasets designed to benchmark the utility of harmonization. Finally, using real-world MRI and clinical data, we compare leakage-prone methods with “PrettYharmonize” and show that it achieves comparable performance while avoiding data leakage, particularly in site-target-dependence scenarios.","Many research fields have greatly benefited from machine learning (ML) approaches. ML models can extract important values from large amounts of data. Having vast data benefits the model’s classification performance and helps capture the underlying patterns, promoting better generalization to new unseen data. This makes combining multiple datasets an appealing approach, especially in domains where obtaining data in a uniform setting is challenging 1. Moreover, small health or research centers that can not afford to collect a large number in-house data, using data acquired in different sites is the only possibility for train ML models. However, different datasets obtained under different conditions often present variability due to differences in the acquisition procedure that are unrelated to relevant biological information 2. This undesired variability, also known as Effects of Site (EoS), can induce biased results if present or not correctly removed 3. These differences may come from systematic differences, which can be corrected, or random variations, which can not be modeled or corrected by harmonization. This problem is of common occurrence in many biomedical domains. For example, clinical data is affected by the acquisition site, as different hospitals have different laboratory machines, procedures, and criteria. Another example is the medical imaging field, as images are affected by acquisition protocol, scanner drifts, and time of the day, just to name a few factors 3, 4. Within this field, Magnetic Resonance Imaging (MRI) images are particularly susceptible to this site-related variance, like the magnetic field strength, room temperature fluctuation or changes in the electromagnetic noise, which makes even images obtained from scanners with the same manufacturer and the same parameters exhibit different characteristics 5, 6. Many works showed that removing this undesired systematic variability, which is only related to the acquisition site and has no biological information, can benefit further analysis made with the data 7, 8, 9, 10, 11. To this end, several Methods Aiming to Remove the Effects of Site (MAREoS) have been proposed and developed 4, 12. These MAREoS methods are typically used as a pre-processing step, where the site effects are removed and the “site-effect free” data, also known as harmonized data, is used for statistical analysis or to train and evaluate ML models. Among these MAREoS, the ones based on “ComBat” are extensively used in several domains. The ComBat method was originally proposed for correcting batch differences in genomic data 13 and was later adapted to other domains like MRI data 14, 7. ComBat uses Bayesian regression to find additive (location) and multiplicative (scale) corrections for each feature in each site. Within the ComBat-based methods, “neuroHarmonize” was proposed 15 to allow for the preservation of non-linear covariate effects and has been widely used since 4, 12, 16, 17. Although ComBat and its derivations have been widely applied in medical imaging data, several concerns have been raised, mainly because ComBat’s hypothesis and assumptions only hold for genomic data, where it was originally proposed, and may not be fulfilled in other applications fields 18. Additionally, concerns had been raised on the integration of ComBat into ML models, as the location and scale parameters of the model could not be learned in a subset of data (train data) and then applied to a new unseen subset of data (test data) 19. Early implementations of ComBat 14, 20 used the whole dataset to estimate the model’s parameters and create a harmonized dataset that is then used from all the downstream analyses. This approach was used in several works 7, 8, 21, 22, 23, 24. While this approach is valid when performing statistical analyses, it is not consistent with machine learning applications where the training and test data must be separate 25, 19. Specifically, the parameters of the models, including preprocessing models, must be obtained on a training set and then applied to the test set. This separation is important to get realistic estimates of generalization performance (e.g. using cross-validation) and to ensure deployability of the model in the real world where the test data is not yet available 26, 27. ComBat-MAGA 28, neuroHarmonize 15, and ”harmonizer” 19, which is based on neuroHarmonize, allow the estimation of the model’s parameters in a training set and apply them to the test samples, however, a critical assumption of ComBat is that all variance not shared across sites is unwanted site-related variance. Consequently, ComBat removes any variance that is not common to all sites, including the relevant biological variance. This poses a new problem, as this assumption is broken when a class imbalance occurs across sites and a target-site dependence exists, for example when the control patients are acquired in one site and target patients in a different one 29. This could also be extended to other possible biological information like comorbidities or disease severity, for example, if more severe patients are consistently treated or acquired only in one site. In these cases, even though ComBat will provide harmonized data, it will remove the variance related to the target (control versus patient) as the assumption is that only non-relevant factors change between sites. ComBat allows to retain the biologically relevant variables, for example, a diagnosis, the age, or the sex of a patient, by providing these variables as covariates to be retained. Nonetheless, this information is needed both when training the model and when applying the model to the test data. Thus, if target labels need to be preserved, this inevitably leads to the model requiring the test labels preventing the model’s use in real-world applications where test labels are not available or known18. This phenomenon where information of the test set is presented to the models is commonly known as data leakage. It is also well described that leaking the test target information would produce overconfident results, which could be misleading and can jeopardize the progress of an entire research field, as researchers who avoid data leakage would not be able to outperform the models that present data leakage 26, 27, 25. In this work, we aim to empirically demonstrate a shortcoming of ComBat-based harmonization in site-target dependence scenarios, i.e., that the model can properly harmonize the data only when test labels are used and data leakage happens. To do so, we performed controlled experiments for age regression and sex classification using real MRI data for healthy control individuals. Also using MRI data, a dementia and mild cognitive impairment (MCI) classification experiment was performed. Additionally, an outcome prediction of septic patients was performed using clinical data. All experiments were conducted both in site-target dependence and independence scenarios. Several harmonization schemes were used and compared, allowing and not allowing leakage, to harmonize the data. Finally, to overcome the aforementioned problem, we propose a new harmonization method, called PRETended Target Y Harmonize (PrettYHarmonize), which allows the users to integrate ComBat in an ML pipeline, harmonizing the data and generating a prediction without using the test labels and thus avoiding data leakage. We validated our method using benchmark datasets 3. Additionally, the proposed method was compared with the other harmonization schemes on the site-target dependence and independence scenarios to comprehensively compare no harmonization, leakage, and no-leakage methods. The corresponding Python package is publicly available via GitHub https://github.com/juaml/PrettYharmonize."
Efficient Biological Data Acquisitionthrough Inference Set Design,"In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so we might hope to reduce their cost by experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of an uncertainty-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that stops running the experiments when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that deploying active learning for inference set design leads to significant reduction in experimental cost while obtaining high system performance.","Automated high-throughput screening (HTS) laboratories have enabled scientists to screen large compound libraries to find effective therapeutic compounds and screen whole-genome CRISPR knockouts to understand the effects of genes on cell function (Mayr & Bojanic, 2009; Wildey et al., 2017; Blay et al., 2020; Tom et al., 2024; Fay et al., 2023). However, conducting experiments on every compound or CRISPR guide in these vast libraries remains very resource-intensive. With typical screening libraries holding on the order of 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT to 106superscript10610^{6}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT compounds (Hughes et al., 2011) and the number of possible small molecules estimated at 1060superscript106010^{60}10 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT (Bohacek et al., 1996), the disparity between our screening capabilities and all which we could explore is staggering. Reducing experimental costs without compromising the quality of the generated data would allow us to accelerate biology and pharmaceutical research and expand the set of molecules considered for testing. To avoid costs scaling with the number of perturbations, we can train a model from a subset of the target library that has been tested in the lab, and then predict experimental outcomes for the remainder of the library using the trained model (Naik et al., 2013; Reker & Schneider, 2015; Dara et al., 2022), thereby building a hybrid screen of the library. This approach entails three interrelated questions: (1) which subset of the library should we use to maximise the accuracy of the predictions?, (2) how do we select this subset without access to the experimental outcomes?, and (3) how do we ensure that we select a large enough subset to meet a target level of accuracy for the predictions? This problem is similar to an active learning problem in that we want to select examples that maximize prediction accuracy, but instead of aiming to minimize generalization error, here we only care about prediction on a particular, finite, set of experiments from a library. The fact that the library is finite introduces an important difference: the learner can influence the set of examples on which it is evaluated by strategically selecting examples. If we accept that the difficulty of predicting outcomes varies among compounds, such that some examples are inherently harder to predict, either due to their complex properties (Bengio et al., 2009), because of the partial observability of the system (Saleh et al., 2021; Krenn et al., 2020) or due to noise in the labeling function (Frénay & Verleysen, 2013; Lukasik et al., 2020), the learner can select these examples into the training set to avoid having to predict their outcomes. Conversely, if an example can be reliably predicted by the model, we can save experimental costs by not including it in the training set. We call this mechanism through which a learner can influence performance inference set design. We propose an active learning-based solution to hybrid screening that uses the model’s confidence to guide the selection of experiments and leverages the mechanism of inference set design to improve the system’s performance on the target set. Our algorithm includes a practical stopping criterion that terminates the search for new molecules once a lower bound on a target accuracy threshold is exceeded. We show in Lemma 1 that this bound provides probabilistic guarantee on the performance of the algorithm as long as the model is weakly calibrated, such that examples for which the model is more uncertain receive a lower predicted probability than those for which it is more certain. To validate our method, we conduct series of empirical studies on image and molecular datasets, as well as a real-world case study in drug discovery. The results demonstrate inference set design significantly reduces experimental costs while improving overall system performance. Importantly, this is true even when the generalization benefits of active learning-based acquisition functions are marginal compared to random search. This has important practical implications for active learning: if a problem is a hybrid screen—in the sense that one only needs good performance on a fixed, finite set of experiments—then evaluating generalization error dramatically understates the benefits of active learning. By combining simple active learning acquisition functions with an appropriate stopping criterion, it is possible to make large scale screening far more efficient."
"Analyzing Neural Network Robustness Using Graph Curvature††thanks:This material is based upon work supported by the National Science Foundation (NSF) under Award No. 2403616, as part of the NSF Cyber-Physical Systems Program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.","This paper presents a new look at the neural network (NN) robustness problem, from the point of view of graph theory analysis, specifically graph curvature. Graph curvature (e.g., Ricci curvature) has been used to analyze system dynamics and identify bottlenecks in many domains, including road traffic analysis and internet routing. We define the notion of neural Ricci curvature and use it to identify bottleneck NN edges that are heavily used to “transport data” to the NN outputs. We provide an evaluation on MNIST that illustrates that such edges indeed occur more frequently for inputs where NNs are less robust. These results will serve as the basis for an alternative method of robust training, by minimizing the number of bottleneck edges.","Autonomous systems (AS) increasingly use neural networks (NNs) due to their ability to process high-dimensional data such as camera images [1], LiDAR scans [2] and textual prompts [3]. At the same time, NNs are known to suffer from robustness vulnerabilities: a slightly perturbed or out-of-distribution input [4, 5] may lead to very different and unexpected outputs. In turn, such vulnerabilities may severely compromise the safety and predictability of NN-based AS. Since the discovery of NN robustness issues [4], there has been an impressive amount of research on this topic. Researchers have developed a number of robust training methods, including adversarial training [6], certified robustness [7, 8], knowledge distillation [9], and semi-infinite constrained learning [10]. Although significant progress has been made, training robust NNs remains largely an unsolved and very challenging problem (e.g., the current leader on the CIFAR-10 robustness leaderboard [11] can only achieve high robust accuracy for perturbations of at most 8/255). We note that the vast majority of existing methods approach the problem from an optimization point of view: e.g., in adversarial training the goal is to train a NN that minimizes the loss not only on training data but also on the worst-case bounded perturbations of that data. This bilevel non-convex optimization problem is challenging to solve and leads to suboptimal solutions, especially if gradient descent is used. We take a fresh look at NN robustness through the lens of graph theory and network science analysis, in particular graph curvature (GC). GC (e.g., Ricci curvature [12]) has been effectively applied in numerous domains that can be modeled as graphs, including road traffic analysis [13, 14], internet routing [15], machine learning [16, 17], and biological networks [18, 16], due to its ability to capture intrinsic geometric and local structure of the space, such as connectivity and robustness in networks. GC can quantify the importance of specific edges; for example, an edge with negative curvature can be considered a bottleneck and is greatly important for the overall graph functionality, e.g., such an edge may connect different communities within the graph [19, 20]. In this paper, we employ GC in order to analyze the robustness of NN classifiers. We introduce the notion of neural Ricci curvature (NRC) that captures the bottleneck intuition of standard Ricci curvature – if an edge has a negative NRC, then it is heavily used by the NN and is thus likely a source of robustness vulnerability. To calculate the NRC, we construct a neural data graph, i.e., a graph in the shape of the NN architecture, where edges are weighted by a combination of the NN weights and the magnitude of data that goes through each edge when an example is provided as input. We evaluate the significance of the NRC using NNs trained on MNIST. We show that neural data graphs corresponding to more robust examples (i.e., examples which are correctly classified even for an adversarial perturbation) indeed have fewer negative-NRC edges. The results are consistent across architectures, including adversarially trained ones. This result will serve as the basis for an alternative, graph-based, method for robust training, that minimizes the number of negative-NRC edges and promotes balanced usage of all NN edges. In summary, this paper makes two contributions: 1) we define the concepts of neural data graphs and neural Ricci curvature that can be used to identify bottleneck NN edges that contribute to robustness issues; 2) we provide an evaluation on MNIST that demonstrates that bottleneck edges indeed occur more frequently in examples where NNs are less robust."
FLiP: Privacy-Preserving Federated Learning based on the Principle of Least Privileg,"Federated Learning (FL) allows users to share knowledge instead of raw data to train a model with high accuracy. Unfortunately, during the training, users lose control over the knowledge shared, which causes serious data privacy issues. We hold that users are only willing and need to share the essential knowledge to the training task to obtain the FL model with high accuracy. However, existing efforts cannot help users minimize the shared knowledge according to the user intention in the FL training procedure. This work proposes FLiP, which aims to bring the principle of least privilege (PoLP) to FL training. The key design of FLiP is applying elaborate information reduction on the training data through a local-global dataset distillation design. We measure the privacy performance through attribute inference and membership inference attacks. Extensive experiments show that FLiP strikes a good balance between model accuracy and privacy protection.","Federated learning (FL) [1] is a deep learning (DL) training paradigm, which aims to utilize the data existing in the form of isolated islands to train DL models (Fig. 2(a)). During the training procedure, data owners (a.k.a. clients) do not share their raw data with anyone, but instead, share some information obtained from the raw data in the form of model parameters. Many solutions are proposed to protect the privacy in FL context. However, they are yet unable to strike a balance between performance and privacy protection in real-world scenarios. This is because they do not fully consider the training task itself when performing protection. Specifically, considering that users are likely only interested in obtaining a high-quality model for the target task, sharing any information unrelated to the training task during the training process could potentially lead to privacy leakage, e.g., secondary attributes inference [2], practical attribute reconstruction Attack [3]. Given the subjective nature of privacy protection, we hold that an ideal solution should fully consider the user’s training goals and only share essential information related to the training task during the training. That’s the core idea, what we call as principle of least privilege (PoLP). According to the PoLP, as shown in Fig. 1, clients should control only the essential training task-relevant information from the raw data that can be shared among participants. At first glance, it is paradoxical to determine which part of the raw data plays a role in the model training procedure before the model is trained. After empirical study and analysis, we observe that each client can only extract a portion of local data that is most relevant to the FL task in the local training. Figure 1: Comparison of PoLP and existing privacy protection solutions. Ideally, the new sample generated by PoLP only contains task-relevant information. (a) The vanilla FL paradigm. (b) The FL paradigm of FLiP. Figure 2: The training procedures of vanilla FL and FLiP. In both paradigms, no training is needed on the central server. The biggest difference between our FLiP and the vanilla FL is the carrier of information aggregation, i.e., FLiP performs distilled data aggregation, and the vanilla FL performs parameter aggregation. Compared to the vanilla method, the amount of shared information during the training in FLiP is controllable. Our work proposes a new FL framework, FLiP, to achieve the PoLP in FL training for privacy protection. FLiP can help clients to identify which part of the raw data contributes most to the FL training and, at the same time, extract the task-relevant information locally. The central server collects the task-relevant information extracted by clients and distributes the global view to help clients make better information extraction. To measure privacy protection effectiveness, we consider adversaries in a semi-honest setting and perform two attacks to infer task-irrelevant information from the data extracted during the training. Experimental results show that FLiP can achieve comparable accuracy to the vanilla FL and better protection for information irrelevant to the training task. Our contribution is fourfold: • We are the first to introduce the PoLP to the FL training for privacy protection. Data owners can control the local information shared among FL participants and minimize privacy breaches by only sharing the essential FL task-relevant information. • We design a privacy-preserving FL system, FLiP, to realize the PoLP. The key design of FLiP is a local-global dataset distillation, which can identify and extract the task-relevant information gradually by round. • We design a task-irrelevant attribute inference attack to measure the protection effectiveness. The attribute inference attack is inspired by existing secondary inference attacks and fully considers the information leakage in each round. • We implement the system and perform an extensive evaluation. The experimental results show that FLiP can prevent adversaries from inferring task-irrelevant information and preserve high data usability."
Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series,"Lately, there has been a surge in interest surrounding generative modeling of time series data. Most existing approaches are designed either to process short sequences or to handle long-range sequences. This dichotomy can be attributed to gradient issues with recurrent networks, computational costs associated with transformers, and limited expressiveness of state space models. Towards a unified generative model for varying-length time series, we propose in this work to transform sequences into images. By employing invertible transforms such as the delay embedding and the short-time Fourier transform, we unlock three main advantages: i) We can exploit advanced diffusion vision models; ii) We can remarkably process short- and long-range inputs within the same framework; and iii) We can harness recent and established tools proposed in the time series to image literature. We validate the effectiveness of our method through a comprehensive evaluation across multiple tasks, including unconditional generation, interpolation, and extrapolation. We show that our approach achieves consistently state-of-the-art results against strong baselines. In the unconditional generation tasks, we show remarkable mean improvements of 58.17%percent58.1758.17\%58.17 % over previous diffusion models in the short discriminative score and 132.61%percent132.61132.61\%132.61 % in the (ultra-)long classification scores. Code is at https://github.com/azencot-group/ImagenTime.","Generative modeling of real-world information such as images [72], texts [13], and other types of data [99, 55, 8] has drawn increased attention recently. In this work, we focus on the setting of generative modeling (GM) of general time series information. There are several factors that govern the complexity required from sequential data generators including the sequence length, its number of features, the appearance of transient vs. long-range effects, and more. Existing generative models for time series are typically designed either for multivariate short-term sequences [44, 19] or univariate long-range data [103], often resulting in separate and completely different neural network frameworks. However, a natural question arises: Can one develop a unified framework equipped to handle both high-dimensional short sequences and low-dimensional long time series? Earlier approaches for processing time series based on recurrent neural networks (RNNs) handled short sequences well [62, 3, 43, 76], however, modeling long-range dependencies turned out to be significantly more challenging. Particularly, RNNs suffer from the well-known vanishing and exploding gradient problem [9, 70] that prevents them from learning complex patterns and long-range dependencies. To address long-context modeling and memory retention, extensive research is devoted to approaches such as long short-term memory (LSTM) models [42], unitary evolution RNNs [5] and Lipschitz RNNs [24]. A different approach for processing sequential information is based on the Transformer [93], eliminating any recurrent connections. Recent remarkable results have been obtained with transformers on natural language processing [13] and time series forecasting [96, 104, 68] tasks. Alas, transformers are underexplored as generative models for long-range time series data. This may be in part due to their computational costs that scale quadratically as 𝒪⁢(L2)𝒪superscript𝐿2\mathcal{O}(L^{2})caligraphic_O ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) with the sequence length L𝐿Litalic_L, and in part because transformer forecasters are inferior to linear tools [101]. Beyond RNNs and the Transformer, recent works have considered the state space model (SSM) for modeling long-range time series information. For instance, the structured SSM (S4) [36] employed a parameterization that reduced computational costs via evaluations of Cauchy kernels. Further, the deep linear recurrent unit (LRU) is inspired by the similarities between SSMs and RNNs, and it demonstrated impressive performance in modeling long-range dependencies (LRD). Still, generative modeling of long-range sequential data via state space models remains largely underexplored. Recent work suggested LS4 [103], a latent time series generative model that builds upon linear state space equations. LS4 utilizes autoregressive dependencies to expressively model time series (potentially non-stationary) distributions. However, this model struggles with short-length sequences as we show in our study, potentially due to limited expressivity of linear SSMs. To overcome gradient issues of recurrent backbones, temporal computational costs of transformers, and expressivity problems of SSMs, we represent time series information via small-sized images. Transforming raw sequences to other encodings has been useful for processing audio [34] as well as general time series data [95, 38, 56]. Moreover, a similar approach was employed to generative modeling of time series with generative adversarial networks (GANs) [12, 39]. However, unstable training dynamics and mode collapse negatively affect the performance of GAN-based tools [59]. In contrast, transforming time series to images is underexplored in the context of generative diffusion models. There are several fundamental advantages to our approach. First, there have been remarkable advancements in diffusion models for vision data that we can exploit [81, 40, 86, 45]. Second, using images instead of sequences elegantly avoids the challenges of long-term modeling. For instance, a moderately-sized 256×256256256256\times 256256 × 256 image corresponds to a time series of length up to 65⁢k65𝑘65k65 italic_k, as we show in Sec. 3. Finally, there is a growing body of literature dealing with time series as images on generative, classification, and forecasting tasks, whose results can be applied in our work and in future studies. In this work, we propose a new diffusion-based framework for generative modeling of general time series data, designed to seamlessly process both short-, long-, and ultra-long-range sequences. To evaluate our method, we consider standard benchmarks for short to ultra-long time series focusing on unconditional generation. Our approach supports efficient sampling, and it attains state-of-the-art results in comparison to recent generative models for sequential information. As far as we know, there are no existing tools handling both short and long sequence data. In addition to its strong unconditional generation capabilities, our approach is also tested in conditional scenarios involving the interpolation of missing information and extrapolation. Overall, we obtained state-of-the-art results in such cases with respect to existing tools. We further analyze and ablate our technique to motivate some of our design choices. The contributions of our work can be summarized as follows: 1. We view generative modeling of time series as a visual challenge, allowing to harness advances in time series to image transforms as well as vision diffusion models. 2. We develop a novel generative model for time series that scales from short to very long sequence lengths without significant modifications to the neural architecture or training method. 3. Our approach achieves state-of-the-art results in comparison to strong baselines in unconditional and conditional generative benchmarks for time series of lengths in the range [24,17.5⁢k]2417.5𝑘[24,17.5k][ 24 , 17.5 italic_k ]. Particularly, we attain the best scores on a new challenging benchmark of very long sequences that we introduce."
AgentForge: A Flexible Low-Code Platform for Reinforcement Learning Agent Design,"Developing a reinforcement learning (RL) agent often involves identifying effective values for a large number of parameters, covering the policy, reward function, environment, and the agent’s internal architecture, such as parameters controlling how the peripheral vision and memory modules work. Critically, since these parameters are interrelated in complex ways, optimizing them can be viewed as a black box optimization problem, which is especially challenging for non-experts. Although existing optimization-as-a-service platforms (e.g., Vizier, Optuna) can handle such problems, they are impractical for RL systems, as users must manually map each parameter to different components, making the process cumbersome and error-prone. They also require deep understanding of the optimization process, limiting their application outside ML experts and restricting access for fields like cognitive science, which models human decision-making. To tackle these challenges, we present AgentForge, a flexible low-code framework to optimize any parameter set across an RL system. AgentForge allows the user to perform individual or joint optimization of parameter sets. An optimization problem can be defined in a few lines of code and handed to any of the interfaced optimizers. We evaluated its performance in a challenging vision-based RL problem. AgentForge enables practitioners to develop RL agents without requiring extensive coding or deep expertise in optimization.","1 INTRODUCTION Developing reinforcement learning (RL) agents is important not only for advancements in machine learning (ML) but also for fields such as cognitive sciences, which is increasingly using RL to model cognitive mechanisms [Eppe et al., 2022]. One of the challenging aspects of this development is optimizing a broad and complex array of parameters that influence agent behavior and performance, a complex black box optimization problem for non-ML experts. Embodied RL agents, which interact with their environments through sensors and actuators, are a point in case. These agents are modeled as Partially Observable Markov Decision Processes (POMDPs) [Eppe et al., 2022, Deglurkar et al., 2023] and are used in real-world tasks such as autonomous driving [De Morais et al., 2020], robotic navigation [Shahria et al., 2022], and human-computer interaction [Jiang et al., 2024]. These models involve not only the parameters of the policy but several others controlling, among others, the reward function. In constrast, optimization of models in other ML fields is often more straightforward, as practitioners need to tune a smaller set of parameters mostly related to training. Sometimes parameter optimization is not just a matter of performance. Parameters that are explainable (e.g., reward weights) are a key to engineering explainable and trustworthy AI systems. Figure 1: Three examples of RL agent applications in Robotics [Ladosz et al., 2024], Human Modeling [Shi et al., 2024], and Collaborative AI [Lingler et al., 2024], highlighting the number of parameters involved in agent design today. Parameters are categorized as Agent, Environment, or Policy. The lists of parameters are non-exhaustive, and practitioners have to choose many more. Our long-term goal is to provide non-ML expect practitioners with the ability to optimize all parameters in a RL system, either jointly or individually, based on their specific problem. This capacity is important, because evidence suggests that even subtle adjustments to the implementation of RL algorithms, like reward clipping or observation normalization, can affect performance [Engstrom et al., 2020]. Furthermore, careful selection of parameters can sometimes improve performance more than the choice of RL algorithm itself [Andrychowicz et al., 2020, Paine et al., 2020, Yang and Yin, 2021]. In embodied RL agents in particular, the deep neural networks have many parameters that are sensitive to optimization [Fetterman et al., 2023]. Figure 1 presents some examples of agents’ applications and some of the parameters that a practitioner would need to choose during their development phase. More generally, these parameters can be roughly categorized in the following classes: 1. Agent design, which control the internal functions of the RL agent, including the discount factor, the entropy coefficient, and the size of the observation window; 2. Environment settings, which define the task itself, such as the field of view, and the reward structure; 3. Policy parameters, such as the architecture of the neural networks, learning rate, and activation functions. Thus, we present AgentForge222A link to the AgentForge platform can be found here: https://github.com/feferna/AgentForge, a flexible low-code platform targeted specifically for developing complex RL systems by domain experts with no solid background in optimization or ML. The focus in a low-code platform is becoming increasingly relevant as professionals in fields outside of ML, such as cognitive and behavioral sciences, are starting to use RL systems to model human cognition and behavior [Eppe et al., 2022, Nobandegani et al., 2022]. Beyond offering a low-code specification of the optimization problem, AgentForge provides rapid iteration by allowing developers to quickly ‘prototype’ an RL design, optimize it, and examine results. Its flexibility builds on its ability to accommodate a wide range of RL systems, from simple fully observable models to more complex, embodied RL agents. Users are only required to define their custom environment, objective function, and the parameters they wish to optimize in two files. The objective function, which guides the optimization process, can be the average reward per episode or any other criterion defined by the practitioner. The platform then automatically converts these inputs into an optimization problem that defines how the agent is trained and evaluated. In this first version, the software offers three popular optimization techniques: Random Search, Bayesian Optimization (BayesOpt) and Particle Swarm Optimization (PSO); however, this set can be extended. Obviously, the challenge of parameter optimization is shared across areas of ML, which has promoted the development of numerous services and frameworks. Notable examples include BoTorch [Balandat et al., 2020], Scikit-Optimize [scikit-optimize, 2024], Optuna [Akiba et al., 2019], and Google Vizier [Song et al., 2023]. However, these platforms primarily cater to users with substantial knowledge of ML and statistics. They are also complex to use with RL agents, because it is not easy to devise an objective function when there are many parameters distributed across different system components. This motivates low-code solutions, such the one we present here. Such tools can enable users to focus on designing effective agents without the burden of mastering optimization techniques, which is especially important given the high dimensionality of sensory inputs and the dynamic environments RL agents operate in. We evaluated AgentForge by jointly optimizing the parameters of a pixel-based version of the classical Lunar Lander agent from the Gymnasium library [Towers et al., 2024], a POMDP where the agent must infer the state from raw pixel values. This agent presents parameters from all three categories cited earlier—agent, environment, and policy—demonstrating AgentForge’s ability to handle diverse and complex parameter sets. Additionally, we show how easily an optimization problem can be specified, highlighting the platform’s easiness and flexibility. The remainder of this paper presents AgentForge in detail and is structured as follows: Section 2 reviews related works on traditional parameter tuning methods, their application in ML application, and limitations. Section 3 details our proposed platform, including what inputs are required from the user and how the optimization is performed. Section LABEL:sec:evaluation describes the evaluation setup, including the application of AgentForge to the proposed pixel-based Lunar Lander agent and its configuration. Section 4 presents the results achieved, while Section 5 discusses the implications for RL agents. Finally, Section 6 summarizes the main findings, discusses current limitations, and suggests directions for future research."
Marked Temporal Bayesian Flow Point Processes,"Marked event data captures events by recording their continuous-valued occurrence timestamps along with their corresponding discrete-valued types. They have appeared in various real-world scenarios such as social media, financial transactions, and healthcare records, and have been effectively modeled through Marked Temporal Point Process (MTPP) models. Recently, developing generative models for these MTPP models have seen rapid development due to their powerful generative capability and less restrictive functional forms. However, existing generative MTPP models are usually challenged in jointly modeling events’ timestamps and types since: (1) mainstream methods design the generative mechanisms for timestamps only and do not include event types; (2) the complex interdependence between the timestamps and event types are overlooked. In this paper, we propose a novel generative MTPP model called BMTPP. Unlike existing generative MTPP models, BMTPP flexibly models marked temporal joint distributions using a parameter-based approach. Additionally, by adding joint noise to the marked temporal data space, BMTPP effectively captures and explicitly reveals the interdependence between timestamps and event types. Extensive experiments validate the superiority of our approach over other state-of-the-art models and its ability to effectively capture marked-temporal interdependence.","Marked event data is widely seen in the real world as a sequence of events, where each event is recorded with a continuous-valued occurrence timestamp and a categorical event type (a.k.a. mark). Its detailed applications include social media [1, 2], where different event types may trigger diverse event patterns; financial transactions [3, 4], where a buy or sell action would result in different transaction times; and healthcare records [5, 6], where the disease types decide the visit times. That is, timestamps and event types exhibit non-trivial interdependence in these scenarios, as they can influence each other in both directions, specified by the detailed situation. Marked Temporal Point Process (MTPP) is a stochastic process that can effectively model marked event sequences. Mainstream MTPP methods can be broadly divided into two categories: classical MTPP models, including Poisson processes [7, 8], Hawkes processes [9] and Self-correcting processes [10], which use an intensity function to characterize the instantaneous occurrence of events given their history. However, these methods often rely on strong parametric or modeling assumptions, limiting their ability to effectively capture complex patterns in event occurrences. On the other hand, neural MTPP models have emerged as a rapidly developing branch in recent years [11, 12, 13]. These models employ neural networks, such as RNNs, LSTMs, or transformers, to encode historical events [14]. In some cases, they draw on intensity functions inspired by processes like the Hawkes process to model event occurrences [15, 16]. Compared to classical MTPP models, neural MTPP models leverage the expressive power of neural networks to better capture the complex dependencies among events. However, applying these two methods to parametric MTPP models requires solving integrals to compute the likelihood function, which usually requires extremely high computational cost [11, 17]. To tackle this issue, two common techniques, namely strong model assumptions and numerical approximation techniques, are typically employed. First, certain model assumptions are introduced, such as treating the timestamp x𝑥xitalic_x and event type m𝑚mitalic_m as independent or conditionally dependent (e.g., x𝑥xitalic_x depends on m𝑚mitalic_m, or m𝑚mitalic_m depends on x𝑥xitalic_x) [18, 19, 15, 20]. This approach simplifies the integral form, thereby reducing computational complexity. However, in reality, x𝑥xitalic_x and m𝑚mitalic_m may not be independent or may have more complex dependence, which can lead to model misspecification and consequently restrict the model’s expressive capacity [21]. Second, numerical approximation techniques, such as Monte Carlo sampling or numerical integration, are used to simplify the computation of integrals when closed-form solutions are not available [22, 23]. Despite this, limitations in sample size and sampling errors mean that these methods only approximate the true solution, which can result in information loss and affect model performance [17]. To fill these gaps, generative MTPP models, which model the target distribution of timestamps using the generative models, have been proposed and have shown promising results [24, 25]. Mainstream generative models, such as diffusion models [26, 27], generative adversarial networks (GANs) [28, 29], and variational autoencoders (VAEs) [30, 31], are energy-based deep probabilistic models, where the optimization objective is an energy function corresponding to an unnormalized negative log-likelihood function. Typically, neural networks are employed to represent this energy function. Since neural networks can approximate complex functions without formal constraints, there is no need to impose model assumptions or use numerical approximation techniques to simplify computation. Consequently, using such generative models in MTPP tasks allows for flexible modeling, enhancing the model’s expressiveness and avoiding information loss due to approximation operations. However, generative MTPP models still face two main challenges. Challenge I: In MTPP tasks, we aim to model the joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ) of two heterogeneous random variables: x𝑥xitalic_x, which is continuous, and m𝑚mitalic_m, which is discrete. However, mainstream generative models, such as diffusion models, are designed for continuous random variables due to their reliance on Gaussian noise [32]. As a result, these models are not directly applicable for modeling joint distributions that include discrete random variables m𝑚mitalic_m [24]. Challenge II: The target joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ) involves two random variables, x𝑥xitalic_x and m𝑚mitalic_m, which exhibit a strong interdependence across different scenarios. For example, discussions about clothing types change with the seasons [14]. Thus, capturing the complex interdependence between timestamps x𝑥xitalic_x and different event types m𝑚mitalic_m is crucial for improving model performance. However, existing generative MTPP models are unable to directly model the joint distribution, often leading to the assumption that x𝑥xitalic_x and m𝑚mitalic_m are independent, and applying the generative model only to x𝑥xitalic_x [24]. This approach neglects the interdependence between the two random variables, ultimately compromising model performance. To tackle the above challenges, we propose a novel generative MTPP model called the Marked Temporal Bayesian Flow Point Process (BMTPP), based on the recently developed generative model, the Bayesian Flow Network (BFN) [33], to approximate the joint distribution of timestamps and event types. First, in contrast to existing generative MTPP models that model the continuous random variable x𝑥xitalic_x only, BMTPP flexibly leverages BFN in a parameter-based manner to model both the timestamps and event types. Second, by adding the joint noise to the marked temporal data, BMTPP can effectively capture the coupling correlation between timestamps and event types in MTPP tasks, explicitly revealing the complex interactions between them. We summarize the contributions as follows: • Based on the Bayesian flow network, we propose a new generative MTPP model, i.e., BMTPP, which can naturally and flexibly model marked event sequences. • BMTPP can directly model the joint distribution p⁢(x,m)𝑝𝑥𝑚p(x,m)italic_p ( italic_x , italic_m ), which consists of continuous timestamps x𝑥xitalic_x and discrete event types m𝑚mitalic_m. Moreover, by employing a joint noise strategy within the marked temporal data space, it can effectively capture and explicitly reveal the interdependence between timestamps and event types. • We evaluate BMTPP on four real-world datasets, demonstrating our proposed approach outperforms other state-of-the-art models, as well as the effectiveness of our joint noise strategy in capturing marked-temporal interdependence."
DMT-HI: MOE-based Hyperbolic Interpretable Deep Manifold Transformation for Unspervised Dimensionality Reduction,"Dimensionality reduction (DR) plays a crucial role in various fields, including data engineering and visualization, by simplifying complex datasets while retaining essential information. However, the challenge of balancing DR accuracy and interpretability remains crucial, particularly for users dealing with high-dimensional data. Traditional DR methods often face a trade-off between precision and transparency, where optimizing for performance can lead to reduced interpretability, and vice versa. This limitation is especially prominent in real-world applications such as image, tabular, and text data analysis, where both accuracy and interpretability are critical. To address these challenges, this work introduces the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI). The proposed approach combines hyperbolic embeddings, which effectively capture complex hierarchical structures, with Mixture of Experts (MOE) models, which dynamically allocate tasks based on input features. DMT-HI enhances DR accuracy by leveraging hyperbolic embeddings to represent the hierarchical nature of data, while also improving interpretability by explicitly linking input data, embedding outcomes, and key features through the MOE structure. Extensive experiments demonstrate that DMT-HI consistently achieves superior performance in both DR accuracy and model interpretability, making it a robust solution for complex data analysis. The code is available at https://github.com/zangzelin/code_dmthi.","Dimensionality reduction [1, 2, 3, 4] simplifies complex datasets while preserving their intrinsic structure [5, 6]. This is crucial for managing high-dimensional data, which presents challenges in computational complexity, storage, and visualisation [7, 8]. Reducing data dimensionality allows for more efficient analysis, pattern recognition, and interpretation [9]. However, balancing high performance [10] and interpretability [11, 12] remains challenging, as efficient processing [13] often conflicts with human interpretability [14, 15]. Figure 1: Overview of the proposed DMT-HI network. The figure compares three dimensionality reduction methods, manifold-based, deep learning-based, and the proposed DMT-HI. DMT-HI leverages a Mixture of Experts (MOE) strategy to efficiently process both image [16] and tabular [17] data, offering better performance, lower training costs, and improved interpretability across different data sizes. Dimensionality reduction methods fall into two categories, manifold-based parameter-free approaches [18, 19, 20] and deep learning-based methods [21]. Manifold-based methods like t-SNE [22] and UMAP [23] are known for their speed (on small dataset) and adaptability [7], projecting high-dimensional data into low-dimensional spaces through nonlinear mappings, revealing underlying structures. Deep learning methods, such as parametric UMAP (PUMAP) [24, 25] and DMT-EV [26], handle complex data more effectively, especially high-dimensional data, leveraging neural networks to capture intricate patterns. DMT-EV, in particular, excels in both performance and explainability [27], pruning irrelevant features for clearer, more interpretable results. Deep learning methods stand out for their ability to scale and generalize well across large datasets, positioning them as central to current dimensionality reduction research. In terms of method efficiency, performance, and interpretability, manifold-based parameter-free methods and deep learning-based methods each have distinct strengths and weaknesses, driven by their theoretical and design differences [7]. Parameter-free methods are more efficient for small datasets with lower time costs since they don’t rely on parametric models and focus on optimizing the output [11]. However, their efficiency declines with increasing data size due to the rising complexity of neighborhood search and distance calculations. In contrast, deep learning methods handle large-scale data more efficiently due to model scalability and hardware acceleration, though their training on small datasets is costlier. In terms of performance, parameter-free methods excel at capturing local structures but struggle with complex global hierarchies due to their reliance on Euclidean space [28]. Deep learning methods, by contrast, can capture both local and global structures through multilayer transformations but require significant data and computational resources. Regarding interpretability, parameter-free methods rely on similarity metrics, making them harder to interpret and inconsistent globally. While deep learning methods can capture richer features, their “black-box” nature and complexity make their decision-making process harder to explain. To address these challenges in global structure characterization and interpretability, As shown in Fig. 1, we propose the MOE-based Hyperbolic Interpretable Deep Manifold Transformation (DMT-HI), which integrates hyperbolic mapping and a Mixture of Experts model (MOE) [29, 30]. Hyperbolic mapping uses negative curvature to better capture complex hierarchical structures and global dependencies, preserving global information in lower dimensions. The MOE strategy enhances performance and efficiency by dynamically assigning tasks to specialized expert networks that handle different input features, thereby avoiding bottlenecks from a single model. This model provides interpretability by allowing users to track expert decisions and understand the internal model workings. Additionally, MOE serves as a bridge between raw data, embedded data, and feature subsets, enabling clear interpretation of how features influence data representations at different levels. By combining hyperbolic mapping’s structural advantages and MOE’s efficient task allocation, DMT-HI aims to improve performance, efficiency, and interpretability, offering a comprehensive solution for reducing the dimensionality of complex data. By integrating these innovations, our approach not only improves dimensionality reduction performance but also enhances interpretability, offering a comprehensive solution for handling complex data types and extracting insights from high-dimensional datasets. DMT-HI’s performance in dimensionality reduction is enhanced through the MOE strategy, which dynamically assigns tasks to the most suitable experts, improving both processing efficiency and overall model performance. Additionally, the redesigned manifold loss optimizes the training process, enabling the model to capture both local and global structures more effectively. Overall, the key contributions of this paper include, • A hyperbolic embedding and deep manifold loss function, which improve the accuracy of dimensionality reduction by better capturing the global structure of data. • The introduction of the MOE strategy, establishing a clear connection between input data, embedding results, and key features, thus enhancing model interpretability and stability. • Comprehensive tests evaluating global and local performance, time efficiency, and other dimensions to validate the advantages of the proposed model."
A neural network approach for solving the Monge-Ampère equation with transport boundary condition,"This paper introduces a novel neural network-based approach to solving the Monge-Ampère equation with the transport boundary condition, specifically targeted towards optical design applications. We leverage multilayer perceptron networks to learn approximate solutions by minimizing a loss function that encompasses the equation’s residual, boundary conditions, and convexity constraints. Our main results demonstrate the efficacy of this method, optimized using L-BFGS, through a series of test cases encompassing symmetric and asymmetric circle-to-circle, square-to-circle, and circle-to-flower reflector mapping problems. Comparative analysis with a conventional least-squares finite-difference solver reveals the competitive, and often superior, performance of our neural network approach on the test cases examined here. A comprehensive hyperparameter study further illuminates the impact of factors such as sampling density, network architecture, and optimization algorithm. While promising, further investigation is needed to verify the method’s robustness for more complicated problems and to ensure consistent convergence. Nonetheless, the simplicity and adaptability of this neural network-based approach position it as a compelling alternative to specialized partial differential equation solvers.","The Monge-Ampère equation is a nonlinear partial differential equation (PDE) with crucial applications across various fields in physics and mathematics. Its general form is given by: det(D2⁢u)=f⁢(x,u,∇u),superscript𝐷2𝑢𝑓𝑥𝑢∇𝑢\det\left(D^{2}u\right)=f(x,u,\nabla u),roman_det ( italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u ) = italic_f ( italic_x , italic_u , ∇ italic_u ) , (1) where u:ℝN→ℝ:𝑢→superscriptℝ𝑁ℝu:\mathbb{R}^{N}\to\mathbb{R}italic_u : blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT → blackboard_R, (N≥1𝑁1N\geq 1italic_N ≥ 1), is an unknown convex function, and D2⁢usuperscript𝐷2𝑢D^{2}uitalic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u represents the Hessian matrix of u𝑢uitalic_u. This equation traces its origins back to the 18th-century work of Gaspard Monge, who studied the problem of optimal resource allocation. Over time, this foundational problem has evolved into what is now known as the optimal transport problem, a concept that naturally emerges in fields such as fluid dynamics and mathematical physics. The Monge-Ampère equation effectively describes the optimal transportation of one mass distribution to another, minimizing a cost function that typically represents the distance over which each mass element must be moved 1. Several optical problems can be formulated as instances of optimal transport. A notable example is the design of a reflector that transforms a given light source distribution into a desired target distribution, a problem that inherently aligns with the principles of optimal transport. In this context, the ’mass’ represents energy, while the cost function corresponds to the optical path lengths of the light rays 2. Numerous variations of this problem can be effectively modeled using the Monge-Ampère equation or its generalized forms. For example, the light source may be planar, emitting a parallel beam, or point-based, radiating in multiple directions. Additionally, optical systems can target near-field or far-field regions and may involve point or parallel targets. Both reflectors and lenses can be described within this framework. For example, we might wish to transform a point source to a far-field target using a freeform reflector; a numerical method for solving this problem using the intersection of confocal paraboloids has been described by De Castro et al. 3. For an in-depth exploration of these variations, see Romijn 4 and Anthonissen et al. 5. To simplify our analysis, we will focus on a specific optical configuration: the parallel-to-far-field reflector system. In this setup, a planar light source emits a parallel beam of light toward a reflector, and our primary concern is the distribution of the reflected light at a significant distance from the reflector. Consequently, we only need to consider the direction of each reflected ray. By applying the Monge-Ampère equation with the transport boundary condition and solving it, we can determine the convex reflector surface that transforms a given source light distribution into the desired target distribution. It is important to note that, mathematically, this problem is identical to both the parallel-to-parallel reflector problem and the parallel-to-far-field lens problem, which can also be addressed using the method presented here. This paper introduces a novel numerical method based on artificial neural networks (ANNs) to solve the Monge-Ampère equation with transport boundary condition. Numerous studies have explored the application of neural networks and automatic differentiation to solve ordinary and partial differential equations. Dissanayake and Phan-Thien 6 pioneered this approach, demonstrating the potential of neural networks for approximating solutions to PDEs. Building on this work, Lagaris et al. 7 presented a method using ANNs to solve initial and boundary value problems by constructing trial solutions that inherently satisfy the given conditions. Aarts and Van Der Veer 8 proposed a method to solve PDEs and their boundary/initial conditions using neural networks, incorporating knowledge about the PDE in the structure and training process of the network. More recently, Michoski et al. 9 presented a comprehensive study on the solving of differential equations using deep neural networks, demonstrating their competitiveness with conventional numerical methods and their potential for parameter space exploration and model enrichment. Building on this rich body of work, Nyström and Vestberg 10 employed ANNs to solve the Dirichlet problem for the Monge-Ampère equation. We extend this approach by incorporating the transport boundary condition and compare our neural network-based method against an existing numerical solver for the Monge-Ampère equation with transport boundary condition11. Furthermore, we examine the effect of various hyperparameters of this neural network method on its performance. Section 2 provides a concise background on the Monge-Ampère equation in the context of the parallel-to-far-field reflector problem, a previously proposed finite-differences-based solver for this problem, and artificial neural networks. In Section 3, we present our extension of this method to incorporate the alternative boundary conditions required for optimal transport problems. To demonstrate the effectiveness of our proposed method, Section 4 presents results for several example problems. As is common in machine learning, numerous hyperparameters influence the accuracy of our method. Thus, Section 5 empirically examines the effects of select hyperparameters on our method’s performance. In Section 6 and Section 7, we conclude by discussing the advantages and limitations of neural network-based methods for solving the Monge-Ampère equation with the transport boundary condition, and explore potential avenues for future research to mitigate these limitations."
,,"1 INTRODUCTION Generative models have achieved impressive performance in scientific applications among many other fields (Noé et al.,, 2019; Butter and Plehn,, 2022; Cranmer et al.,, 2020; Sanchez-Lengeling and Aspuru-Guzik,, 2018). Oftentimes, such systems can depend on external control parameters, such as temperature governing the behavior of thermodynamic systems, coupling constants in physical models, or a tempered likelihood posterior in Bayesian inference (Wuttke et al.,, 2014; Friel and Pettitt,, 2008; Pawlowski et al.,, 2017). A major challenge in such cases is acquiring training data for a complete range of control parameters, which can quickly become infeasible. Figure 1: Our approach to train a conditional normalizing flow pθ⁢(x|c)subscript𝑝𝜃conditional𝑥𝑐p_{\theta}(x|c)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_c ). Left: At c=c0𝑐subscript𝑐0c=c_{0}italic_c = italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, the flow is trained using NLL. Right: By learning the gradient of the distribution with respect to c𝑐citalic_c based on prior knowledge, the distribution learned at c0subscript𝑐0c_{0}italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is propagated to other conditions c≠c0𝑐subscript𝑐0c\neq c_{0}italic_c ≠ italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT without additional training data. In this work, we focus on the task of learning generative models in the case where we have access to the functional form of the unnormalized density, such as learning the distributions of equilibrium states in physical systems like molecules (Boltzmann distributions) or variational inference. We approach this problem by learning a single generative model that takes the external condition c𝑐citalic_c as a parameter: pθ⁢(x|c)≈p⁢(x|c)subscript𝑝𝜃conditional𝑥𝑐𝑝conditional𝑥𝑐p_{\theta}(x|c)\approx p(x|c)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_c ) ≈ italic_p ( italic_x | italic_c ). Several works have attempted to address this problem before. One approach applies architectural restrictions to allow one particular functional form of external parameter dependence (Dibak et al.,, 2022). However, this restriction has recently been shown to incur severe limitations in expressivity (Draxler et al., 2024b, ). Energy-based training has been proposed as another method (Schebek et al.,, 2024; Invernizzi et al.,, 2022; Wirnsberger et al.,, 2020), but can exhibit unfavorable properties like mode-seeking behavior (Minka et al.,, 2005; Felardos et al.,, 2023), which has also been shown to be a problem in practice (Wirnsberger et al.,, 2020). Other works require data to be available at the target parameters (Wang et al., 2022b, ; Wirnsberger et al.,, 2020), which can become prohibitively expensive. We overcome these limitations: We allow learning arbitrary continuous dependencies of the target density on external parameters and train unconstrained architectures. Our central idea is to formulate the training of a conditional probability density as a boundary value problem: The boundary is learned on a fixed reference condition c=c0𝑐subscript𝑐0c=c_{0}italic_c = italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and then the resulting density is extrapolated using the known functional dependence on the condition underlying the problem. Our approach is summarized in Fig. 1. In summary, we contribute: • We introduce TRADE, a method for learning generative models with arbitrary continuous dependencies on external conditions. Learning is enabled by a novel boundary value problem formulation. • TRADE uses unconstrained architectures, facilitating the application to more complex target distributions. • TRADE can be trained data-free or with data only available at the boundary c0subscript𝑐0c_{0}italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, making it an efficient approach in cases where acquiring data for a full range of control parameters is infeasible. • TRADE achieves excellent results in a wide range of experiments including Bayesian inference, molecular simulations and physical lattice models"
Measuring memorization through probabilistic discoverable extraction,redacted \correspondingauthorjamhay@google.com,"Memorization of training data, while potentially beneficial for retaining factual information, presents significant challenges in large language models (LLMs) (Biderman et al., 2024; Duan et al., 2024b; Zhang et al., 2024; Huang et al., 2024; More et al., 2024; Smith et al., 2023; Carlini et al., 2022; Bordt et al., 2024; Duan et al., 2024a; Staab et al., 2023; Shi et al., 2023; Tang et al., 2023; Zanella-Béguelin et al., 2020) 111This paper covers a very restricted definition of “memorization”: whether a generative model can be induced to generate near-facsimiles of some training examples when prompted with appropriate instructions. Models do not “contain” bit-wise or code-wise copies of their training data. Rather, if a model can be induced to generate very close copies of certain training examples by supplying appropriate instructions to guide the model’s statistical generation processes then that model is said to have “memorized” those examples. This is an area of active ongoing research.. The undesirable consequences of memorized data can inadvertently expose sensitive information contained within the training set. This issue has garnered significant attention, leading to the now-common practice of quantifying and reporting training data memorization rates within technical reports introducing new foundation large language models (LLMs) (Reid et al., 2024; Anil et al., 2023; Chowdhery et al., 2023; Team et al., 2024; Kudugunta et al., 2024). One way to measure memorization is to quantify how easily a potential attacker could extract training data by querying the model. This often-used measure of memorization called discoverable extraction (Carlini et al., 2022; Kassem et al., 2024) essentially states that a training example is extractable (or “memorized”) if when split into a prefix and suffix, the model generates a sequence matching the suffix when given the prefix as input. Discoverable extraction is often used as an (approximate) upper-bound to an adversary that has no prior knowledge of the example to be extracted (Nasr et al., 2023). Discoverable extraction has become a popular way of measuring memorization rates of LLMs (Reid et al., 2024; Anil et al., 2023; Chowdhery et al., 2023; Team et al., 2024; Kudugunta et al., 2024), in no small part due to its simplicity; one needs only to check if a model’s completion matches an expected target string. While discoverable extraction is cheap to compute, this has its own drawbacks; by only generating a single sequence and checking for a match with the target, it may miss cases where a match could have been found if more than one sequence was generated. These nuances of what is and is not memorized are too coarsely treated by a working definition like discoverable extraction, and leads us to investigate the following question: Question 1. How can we better measure the extractability of sensitive sequences from large language models? Quantifying memorization – and the associated risks – is a subtle and context-specific problem that a single measurement likely cannot capture in isolation. Prior work has attempted to introduce more complex definitions of memorization that aim to get to the heart of what it means for a model to memorize a training example, but they are often too costly to be practically leveraged. These methods have varying levels of computational cost to empirically estimate memorization in the context of LLMs, but are all more expensive than discoverable extraction (see Section 4). Large language models are probabilistic machines, the output of the model is a probability distribution over tokens that make up a model’s vocabulary. The sequence that is generated is entirely dependent on the choice of sampling algorithm that defines how a token is selected from this distribution. There are many different sampling algorithms that one can choose from (see Fan et al. (2018); Holtzman et al. (2019); Basu et al. (2020); Vijayakumar et al. (2016); Graves (2012); Boulanger-Lewandowski et al. (2013)). Prior works have focused on greedy sampling, which iteratively selects the next token with the largest probability conditioned on the previous tokens. Although greedy sampling selects the most likely next token it may not select the overall most likely sequence; yet, the difference between discoverable extraction rates under greedy and other sampling schemes like beam search was found to be marginal (Carlini et al., 2022). However, users of production large language models are often free to decide which sampling scheme to use. These observations lead to our second research question: Question 2. How does the user-chosen sampling scheme affect extractability rates? Carlini et al. (2022) argue that focusing on sampling schemes that have higher degrees of associated randomness compared to greedy sampling, are “antithetical” to maximizing discoverable extraction, as sampling schemes that encourage diversity in sequence generation will by definition have a higher variation in the sequences that can be sampled. However, higher sequence diversity may be advantageous for extraction given that users may query the model multiple times. Extracting the secret even once out of multiple queries could be highly problematic as the adversary (say a hacker checking credit card numbers) may have external means of verifying which one is correct. It is reasonable to try and quantify the number of sequences that need to be generated before a target example becomes extractable, as this better aligns with how users could interact with a model. In particular, because production language models are deployed with non-greedy based sampling strategies that, by default, encourage diversity in sampled sequences. This idea is not a new one. Carlini et al. (2019) motivate their measure of canary memorization using rank perplexity by considering an adversary who sequentially guesses the potential canaries in order from lowest to highest perplexity. The rank of the true canary measures how many guesses such an adversary would need to make before guessing correctly. While the secret sharer attack only guesses a single canary, it is natural to consider how extraction rates scale with multiple guesses. We find that even when we measure the extraction probability after multiple guesses, the extraction rates on training data remain significantly higher than extraction rates on test data (see Section 5.2). This observation allows us to reason about the absolute risk of extraction as well as the relative risk. In this work, we introduce a probabilistic relaxation of discoverable extraction that resolves the discussed points of tension. This new definition quantifies the number of attempts n𝑛nitalic_n an adversary would need to make to extract a target with a certain probability p𝑝pitalic_p under a given sampling scheme. This provides a more nuanced quantification of memorization, which alleviates the aforementioned drawbacks of discoverable extraction without incurring any additional computational cost compared to discoverable extraction. Our contributions We propose a simple probabilistic definition of extraction called (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction that captures the risk of extracting a target after sampling n𝑛nitalic_n times from an arbitrary sampling scheme. We thoroughly benchmark (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction rates for different sampling schemes, settings of n𝑛nitalic_n and p𝑝pitalic_p, model sizes, and number of target data repetitions, and we make a number of remarkable empirical findings that demonstrate the utility of our definition: • Greedy extraction underestimates training data memorization rates compared to (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction even for modest values of n𝑛nitalic_n and p𝑝pitalic_p (Section 5.1). Moreover, the discrepancy between the two rates increases for larger models and more repetitions of the target data. (Section 5.3) • At every setting of the definition parameters we tried, extraction rates of training data far exceeded baseline extraction rates on test data (Section 5.2). • We show that (n,p)𝑛𝑝(n,p)( italic_n , italic_p )-discoverable extraction provides a better comparison of memorization rates across models trained on the same data compared with greedy extraction (Section 5.1). Along the way, we provide extensive discussion how our definition relates to other definitions of memorization (Section 3 and Section 4)."
Improving Inverse Folding for Peptide Design with Diversity-regularized Direct Preference Optimization,"Inverse folding models play an important role in structure-based design by predicting amino acid sequences that fold into desired reference structures. Models like ProteinMPNN, a message-passing encoder-decoder model, are trained to reliably produce new sequences from a reference structure. However, when applied to peptides, these models are prone to generating repetitive sequences that do not fold into the reference structure. To address this, we fine-tune ProteinMPNN to produce diverse and structurally consistent peptide sequences via Direct Preference Optimization (DPO). We derive two enhancements to DPO: online diversity regularization and domain-specific priors. Additionally, we develop a new understanding on improving diversity in decoder models. When conditioned on OpenFold generated structures, our fine-tuned models achieve state-of-the-art structural similarity scores, improving base ProteinMPNN by at least 8%. Compared to standard DPO, our regularized method achieves up to 20% higher sequence diversity with no loss in structural similarity score.","Engineering biopolymers that fold into desired 3D structures, a computational challenge known as inverse protein folding problem, has broad applications in drug discovery and material science (Yang et al., 2023; Dill et al., 2008; Abascal & Regan, 2018). Several approaches for inverse folding have been adopted over the past decades, from molecular dynamics simulations to machine learning approaches (Dauparas et al., 2022b; Shanker et al., 2023; Hsu et al., 2022a; Yi et al., 2023; Correa, 1990). In the standard machine learning approach, a molecular backbone chain serves as input, and a model generates sequences that adopt folding topologies compatible with the reference backbone. Sequences do not necessarily share sequence homology, as multiple diverse sequences can fold into similar structures (Hsu et al., 2022a; Yue & Dill, 1992; Godzik et al., 1993). Peptides, which are small biopolymers comprising 2-50 residues, are interesting targets for inverse folding given their role in diverse biological functions, acting as hormones, neurotransmitters, signalling molecules, or nanostructures assemblers (Chockalingam et al., 2007; Torres et al., 2019; Copolovici et al., 2014; Ulijn & Smith, 2008). Only about 225,000 protein structures have been experimentally determined111Updated figures available at https://www.rcsb.org/stats/growth/growth-released-structures. and made available via the Protein Data Bank (PDB). Training inverse-folding machine learning models in a supervised fashion is a challenging task, due to the complexity of the problem and the limited amount of experimental data. The challenge is aggravated in the peptide domain as fewer than 3.53.53.53.5% PDB structures contain 50 residues or less. In fact, applying the SCOP classification filter in the PDB to display structures labelled as “Peptide” reveals only 509 entries, circa 0.20.20.20.2% of all experimentally determined structures available. In addition to the lack of data, sequences are subject to composition bias. The incidence of certain amino acids may differ depending on the sequence length , as longer proteins have more options for accommodating multiple secondary structures and folding loops (Tiessen et al., 2012). Popular models like ProteinMPNN (Dauparas et al., 2022b), PiFold (Gao et al., 2022) and ESM-IF1 (Hsu et al., 2022b) are primarily trained on data derived from longer proteins, leading to poor performance for peptide design tasks. Additionally, shorter sequences fold into simpler structures. Indeed, Milner-White & Russell (2008) argue that short peptides are notoriously “structureless” and tend to flicker between conformations. For example, a structural conformation of a single alpha helix or beta sheet – or a combination of two or three of them – is not necessarily stable and can fluctuate. Existing inverse folding models optimize sequence residue-recovery accuracy and structural similarity via template modeling (TM) score (Zhang & Skolnick, 2004). However, they often suffer from low sampling diversity (Gao et al., 2023b). Ideally, the inverse folding model generates maximally diverse candidate sequences, as additional design filters, such as synthesizability and thermal stability, reduce the number of potential hits downstream of the design process. To address these issues, we apply Direct Preference Optimization (DPO) (Rafailov et al., 2023), a fine-tuning method, to improve inverse-folding methods for peptide design. To the authors’ knowledge, we are the first to apply DPO to this task. We propose several enhancements to DPO to address specific problems that arise in inverse folding. Particularly, we forward fold generated sequences and derive an online regularized algorithm for optimizing structural similarity to the reference and sequence diversity simultaneously. We show empirically that this algorithm targets the differential entropy in log-probability space. Furthermore, we present a simple reward scaling approach to incorporate scalar reward information, showing that reward scaling adaptively selects a KL divergence penalty (Kullback, S. and Leibler, R. A., 1951) to improve performance on harder structures."
LOCAL: Learning with Orientation Matrix to Infer Causal Structure from Time Series Data,"Discovering the underlying Directed Acyclic Graph (DAG) from time series observational data is highly challenging due to the dynamic nature and complex nonlinear interactions between variables. Existing methods often struggle with inefficiency and the handling of high-dimensional data. To address these research gap, we propose LOCAL, a highly efficient, easy-to-implement, and constraint-free method for recovering dynamic causal structures. LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic DAG equivalent to the ground truth. On this basis, we propose two adaptive modules for enhancing the algebraic characterization of acyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and Dynamic Graph Parameter Learning (DGPL). ACML generates causal masks using learnable priority vectors and the Gumbel-Sigmoid function, ensuring the creation of DAGs while optimizing computational efficiency. DGPL transforms causal learning into decomposed matrix products, capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Extensive experiments on synthetic and real-world datasets demonstrate that LOCAL significantly outperforms existing methods, and highlight LOCAL’s potential as a robust and efficient method for dynamic causal discovery. Our code will be available soon.","Exploration of the underlying causal generation process of dynamic systems is an important task (Cheng et al., 2024a; Gong et al., 2024) for trustworthy machine learning. Unfortunately, it is unethical, impossible due to technical reasons, or expensive to conduct intervention experiments on the dynamic systems of certain domains (Li et al., 2023a; Cai et al., 2023a). Another challenge is to infer about the structure which may be high dimensional and nonlinear. Some recent works (Pamfil et al., 2020; Sun et al., 2023; Gao et al., 2022; Fan et al., 2023) have made significant efforts by employing dynamic Bayesian networks (DBNs) with observational and interventional data: among dynamic systems, as illustrated in Figure 1. The variable xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at timestep t𝑡titalic_t is affected by which variables xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT at the same time step (instantaneous dependency) and which variables xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT at the previous timestep (lagged dependency)? This question highlights the crucial roles of those algorithms in the interpretable performance of the trained models. Figure 1. Illustration of instantaneous dependency (solid lines) and lagged dependency (dashed lines) dependencies in a DBN with d=3𝑑3d=3italic_d = 3 nodes and autoregression order p=2𝑝2p=2italic_p = 2. For clarity, we display edges that do not influence the variables at time t𝑡titalic_t in a lighter shade. In order to study the nonparametric DBN, DYNOTEARS (Pamfil et al., 2020) (i.e., a score-based approach to learning DBNs with a differentiable DAG constraint (Zheng et al., 2018)) was proposed as a proxy to capture the learn the parents of child variables. However, in Section 4.2, our practical analysis shows that the DYNOTEARS algorithm and its extensions (Sun et al., 2023; Gao et al., 2022; Fan et al., 2023) adopting matrix exponential constraints require an extremely long time to optimize in high-dimensional dynamic systems, even if they smartly adopt interventional data to enhance the identifiability (Li et al., 2023b, 2024). Then, it is natural to need a proxy model that can fasterly infer dynamic causal structures in high-dimensional situations, which is also the main goal of our work. Recently, much of the research on dynamic causal structure learning has concentrated on applying soft sparsity and DAG constraints. For instance, Golem (Ng et al., 2020) formulated a likelihood-based score function for handling the causality of thousands of nodes. Yu et al. (Yu et al., 2023) extended it for recovery dynamic causal structure. Concurrently, (Fang et al., 2024) verified the feasibility of further accelerating the learning of DAG based on this likelihood function both theoretically and experimentally. These approaches are aimed at enhancing flexibility and scalability in high-dimensional and circumventing rigid structural constraints. Building on these insights, we propose a novel framework for Learning with Orientation matrix to infer CAusaL structure from time series data, which we call LOCAL. We develop a quasi-maximum likelihood-based dynamic structure learning method with identifiability guarantee. Powered by this quasi-maximum likelihood-based objective, we propose to enhance the algebraic characterization of acyclicity with two adaptive modules for causal structure recovering task: 1) an Asymptotic Causal Mask Learning (ACML) module which leverages learnable priority vectors (𝒑𝒑\boldsymbol{p}bold_italic_p) and the Gumbel-Sigmoid function to generate causal masks, ensuring the creation of directed acyclic graphs (DAGs) while optimizing computational efficiency; 2) a Dynamic Graph Parameter Learning (DGPL) module to transform causal learning into decomposed matrix products (𝑾=𝑬s⁢𝑬tT𝑾subscript𝑬𝑠subscriptsuperscript𝑬𝑇𝑡\boldsymbol{W}=\boldsymbol{E}_{s}\boldsymbol{E}^{T}_{t}bold_italic_W = bold_italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_italic_E start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT), effectively capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Those leads us can exploit faster gradient-based optimization, such as Adam (Kingma and Ba, 2015), and GPU acceleration. Contribution. The main contributions of this paper are as follows: • To the best of our knowledge, LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic causal structure and shows that more robust and accurate. • We proposed two adaptive modules, ACML and DGPL, which further liberate the matrix exponential operations required for causality. • We conducted extensive experiments on several synthetic datasets and real-world benchmarks, which proved that LOCAL outperforms state-of-the-art by a significant margin."
Accelerating AI Performance usingAnderson Extrapolation on GPUs,"We present a novel approach for accelerating AI performance by leveraging Anderson extrapolation, a vector-to-vector mapping technique based on a window of historical iterations. By identifying the crossover point where a mixing penalty is incurred, the method focuses on reducing iterations to convergence, with fewer more compute-intensive but generally cacheable iterations, balancing speed and memory usage with accuracy and algorithmic stability, respectively. We demonstrate significant improvements, in both training and inference, motivated by scalability and efficiency extensions to the realm of high-performance computing (HPC).","Anderson extrapolation [1, 2, 27, 33, 16] has recently been applied to deep equilibrium models (DEQs) [7, 8, 9, 10, 24, 17]. Kolter et al. [34] found the gains not substantial due to early termination with a loose convergence tolerance. They focused on Anderson extrapolation during training. Here, we show significant acceleration of AI performance with Anderson on GPUs for both the forward pass (running inferences faster) and training (generating models faster). We demonstrate acceleration of the forward pass with standard Anderson as a baseline for future work with stochastic variants [30] and accelerating the backward pass with Jacobian-free methods like Jacobian-Free Backpropagation (JFB) and Neumann series gradient approximations [16]. Figure 2: AI carbon footprint projected to consume >2% of global electricity demand [3, 15, 28, 25], amounting to >10% of global electricity demand for data centers and infrastructure. As AI demand grows, as shown in Fig. 2 [3, 15, 28, 25], high-performance computing (HPC) is becoming critical due to economic pressures from the growth of data and AI infrastructure [29]. Low-memory acceleration techniques, like Anderson extrapolation, will be key to increasing HPC-based AI computational efficiency. This study investigates matrix-free Anderson extrapolation on GPUs, emphasizing gains from advanced computing architectures compared to CPUs. Our goal is to maximize computational efficiency while reducing iterations to convergence by reusing previous iterations to avoid unnecessary gradient calculations, gaining benefits expected from second-order methods (e.g., [32]) without manipulating Hessian matrices. The environmental impact of AI is rapidly growing [3, 15, 28, 25]. By 2030, AI is projected to account for 2% of global electricity consumption. We aim to reduce this impact by up to 90%, saving 160 terawatt-hours per year by 2030. The carbon footprint of AI exceeds the 500-megaton annual benchmark set by initiatives like Bill Gates’ Breakthrough Energy [14]. Efficiency-enhancing technologies like GPU and Anderson acceleration can reduce AI’s carbon emissions by 60 gigatons per year by 2030, as shown in Fig. 2. 1.1 Leveraging extrapolation for AI and HPC advances Anderson extrapolation, a windowing technique for accelerating nonlinear fixed point iterations, is widely applied in fields like density functional theory, kinetic theory, and climate spin-up. It is well-suited for distributed memory parallelization and GPU implementation. It is a staple of major open-source large-scale solver libraries, including PETSc [11, 12], SUNDIALS [23], Trilinos [19, 22, 21, 20], and deal.II [13, 4, 5, 6]. It can be applied to machine learning training, smoothing out standard forward iterations and achieving superior accuracy in training and testing error. Benchmarking results on CIFAR10 show expected robustness benefits and allow characterization of the temporal advantages or disadvantages from the higher cost per iteration, where a small residual minimization step is applied at each new function evaluation. Figure 3: Mathematical formulation and vector representation. Adapted from Y. He & H. De Sterck. ""Linear Asymptotic Convergence Analysis of Anderson Acceleration, with Krylov Formulation in the Linear Case"" Copper Mountain Conference (2022), ICERM Workshop (2023). Available at: https://www.bilibili.com/video/BV1Wa411i77y/ and https://icerm.brown.edu/video_archive/?play=3320 Figure 4: Deep equilibrium neural network model architecture (Source: NeurIPS Tutorial, 2020 [34]). f⁢(z,x)=norm⁢(ReLU⁢(z+norm⁢(x+W2∗(norm⁢(ReLU⁢(W1∗z))))))𝑓𝑧𝑥normReLU𝑧norm𝑥subscript𝑊2normReLUsubscript𝑊1𝑧f(z,x)=\mathrm{norm}(\mathrm{ReLU}(z+\mathrm{norm}(x+W_{2}*(\mathrm{norm}(% \mathrm{ReLU}(W_{1}*z))))))italic_f ( italic_z , italic_x ) = roman_norm ( roman_ReLU ( italic_z + roman_norm ( italic_x + italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∗ ( roman_norm ( roman_ReLU ( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∗ italic_z ) ) ) ) ) ) ""norm"" here is a group norm, representing a statistical normalization [31]. 1.2 Balancing memory and stability Fundamental tradeoffs exist between memory capacity, memory bandwidth, communication cost, and algorithmic characteristics of stability and convergence rate. The tradeoffs are generally resolved to minimize time to solution. GPUs attain high memory bandwidth advantages over CPUs at the cost of smaller memory capacity. Anderson extrapolation promotes fewer, more expensive steps, reusing cached state-vector data. In distributed memory implementations, it produces convergence with fewer interprocessor communication steps. It has tuning parameters such as window size and damping that can be tuned to application and architecture. We are assessing its utility in machine learning more broadly at a time of emergent CPU-GPU superchips. 1.3 Deep equilibrium neural network models Deep equilibrium models (DEQs) are the continuum limit of explicit neural networks as the number of layers approaches infinity [26], approximating many explicit layers with a single, implicit layer with exponentially fewer parameters using a backward pass including the output. This reduces the inverse problem in parameter space to a fixed-point iteration problem, enabling the usage of nonlinear, vector-to-vector mapping techniques to compute the fixed-point iterations that converge to the deep equilibrium state parameters by minimizing the loss function. With gains in memory and acceleration, DEQs are fit for large-scale computer vision and natural language processing tasks and benefit more from matrix-vector operation-optimized computing architectures like GPUs and CPU-GPU superchips. The standard approach using forward iteration for fixed-point iteration problems often does not efficiently converge to the fixed point and suffers from initially slow error reduction and local minimum trapping in nonlinear problems like deep neural networks. Vector-to-vector mapping techniques like Anderson extrapolation outperform standard forward iteration by combining information from previous iterations to span a searchable subspace to extrapolate the next iteration, enhancing convergence rates at the expense of memory usage in each iteration. DEQs represent any neural network at arbitrary depths and connectivities with a single implicit layer consuming vastly fewer parameters with faster forward passes for accelerated training and inferences. The implicit function theorem shows how gradients can be computed in the DEQ framework, facilitating backpropagation through the equilibrium state [8, 34]. DEQs provide a framework for accelerating deep learning, extending the capacity of deep networks within a single-layer architecture through fixed-point computations and advanced root-finding algorithms. Their amenability to convergence acceleration with techniques like Anderson positions DEQs as a robust method to reduce computation needed to build state-of-the-art models and scale up beyond current computational limitations."
Computational Bottlenecks of TrainingSmall-scale Large Language Models,"While large language models (LLMs) dominate the AI landscape, Small-scale large Language Models (SLMs) are gaining attention due to cost and efficiency demands from consumers. However, there is limited research on the training behavior and computational requirements of SLMs. In this study, we explore the computational bottlenecks of training SLMs (up to 2B parameters) by examining the effects of various hyperparameters and configurations, including GPU type, batch size, model size, communication protocol, attention type, and the number of GPUs. We assess these factors on popular cloud services using metrics such as loss per dollar and tokens per second 111We use average dollar cost ratios of cloud instance types based on publicly available pricing (Appx. A).. Our findings aim to support the broader adoption and optimization of language model training for low-resource AI research institutes.","Large Language Models (LLMs) are becoming increasingly popular in various fields due to their performance on a variety of tasks [6, 18, 8, 20, 5]. However, deploying large models widely such as on mobile hardware and edge devices is challenging due to the large memory and compute requirements [11, 12, 10]. These constraints have driven a growing interest in smaller language models (such as ≤2⁢Babsent2𝐵\leq 2B≤ 2 italic_B parameters) as a viable alternative [24, 16, 23]. Recent work refer to these models as Small-scale large Language Models (SLMs) which can work well in environments where cost-efficiency and resource limitations are of significant concern, as well as on servers where the reduced cost of inference will be a dominant factor to attract and retain customers. SLMs have demonstrated substantial potential in achieving competitive results despite their smaller size. Techniques such as pruning, distillation, and quantization have been employed to enhance their performance [2, 3, 17], allowing SLMs to perform on par with, and in some cases surpass, much larger models [4]. For example, Gemma-2B outperforms the largest OPT-175B [25], challenging the notion that sheer model size is the primary determinant of effectiveness. In addition to on par accuracy, SLMs can meet consumer demands for fast, efficient, and cost-effective AI without sacrificing task performance, making them increasingly attractive for organizations with limited computational budgets, such as small businesses and academic institutions. While prior work mostly focused on optimizing SLMs for inference [15], relatively little attention has been paid to their training dynamics. This gap is significant, as the computational and infrastructure demands of training LLMs may not translate to SLMs. Given the diverse range of hardware configurations available on cloud platforms—such as GPU type, batch size, and communication protocols—there is a need for a systematic analysis of how these factors impact the training efficiency of SLMs, particularly when measured in terms of practical metrics such as loss per dollar and tokens per second. Our findings indicate that for smaller models, more affordable options like A100-40GB GPUs and Distributed Data Parallel (DDP) can be utilized without sacrificing performance. For larger models, advanced configurations, such as A100-80GB and H100-80GB GPUs paired with Flash Attention (FA) and Fully Sharded Data Parallel (FSDP), are necessary to handle larger batch sizes and prevent memory-related issues. Recent advancements in the field underscore the importance of scaling AI systems not only for state-of-the-art performance but also for practical applications in real-world environments. The emerging trend toward SLMs suggests that a re-evaluation of hardware and computation strategies is essential. The contribution of this paper is to address the need for such evaluation, providing a systematic study on the computational bottlenecks and cost-efficiency of training SLMs up to 2B parameters on various cloud infrastructure and setups. We find that 1) FlashAttention is significantly more important for SLMs than LLMs, 2) Expensive hardware, e.g., H100-80GB and A100-80GB, is not necessarily cost effective for SLM training, 3) DDP is the best distributed training scheme for SLMs, and 4) Maximizing GPU memory utilization is not cost-optimal for SLM training."
Gradient Descent Efficiency Index,"Gradient descent is a widely used iterative algorithm for finding local minima in multivariate functions. However, the final iterations often either overshoot the minima or make minimal progress, making it challenging to determine an optimal stopping point. This study introduces a new efficiency metric, Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, designed to quantify the effectiveness of each iteration. The proposed metric accounts for both the relative change in error and the stability of the loss function across iterations. This measure is particularly valuable in resource-constrained environments, where costs are closely tied to training time. Experimental validation across multiple datasets and models demonstrates that Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT provides valuable insights into the convergence behavior of gradient descent, complementing traditional performance metrics. The index has the potential to guide more informed decisions in the selection and tuning of optimization algorithms in machine learning applications and be used to compare the “effectiveness” of models relative to each other.","In the field of machine learning, optimizing the training process of models is crucial for achieving high performance while minimizing computational resources. Gradient descent [1] is a widely used optimization algorithm due to its simplicity and effectiveness in finding local minima of differentiable functions. However, the efficiency of gradient descent can diminish with large datasets and prolonged training periods, where additional iterations provide negligible improvements. This raises the need for a robust mechanism to identify the optimal stopping point, ensuring efficient use of computational resources. Fig. 1 is a contour plot that illustrates how each step taken in gradient descent towards a local minimum is smaller than the previous one. This approach quickly returns diminishing results, making the last few steps cost more computationally than they yield in accuracy. Figure 1: Contour plot of cost b,w with path of gradient descent J⁢(w,b)𝐽𝑤𝑏J(w,b)italic_J ( italic_w , italic_b ). In Fig. 2, notice how the first 100 steps exponentially decrease (or logarithmically increase) the cost. However, if you zoom out to 100,000 steps, the curve effectively flattens out before 10,000 steps in this particular example. Figure 2: Comparison of cost with different domain restrictions. The “Gradient Descent Efficiency Index” is a novel ratio between training parameters that includes the relative change in gradient norm, the initial learning rate, the learning decay rate, absolute change in coefficients, and the number of iterations. Note that throughout this paper, I will use the name “Gradient Descent Efficiency Index”, the short form “GDEI”, and the function Eksubscript𝐸𝑘E_{k}italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT interchangeably."
Generative Diffusion Models for Sequential Recommendations,Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).,"Recommender systems are algorithms that suggest items to users by analyzing various forms of input data. Their primary goal is to enhance the customer experience through personalized recommendations, often based on prior implicit feedback. These systems track user behaviors, such as purchase history, viewing habits, and browsing activity, to model user preferences. Sequential Recommendation, a specific type of recommendation, is particularly relevant for applications where user behavior is naturally sequential. It focuses on predicting the next item a user will interact with by considering the order of previous interactions. Mainstream solutions to Sequential Recommendation (SR) [2] represent items with fixed vectors, which have a limited ability to capture the latent aspects of items and the diversity of user preferences. Generative models like Generative Adversarial Networks (GANs) [3] and Variational Auto-Encoders (VAEs) [4] have been widely applied in personalized recommendations, using adversarial training and encoder-decoder architectures, respectively, to model user behavior and preferences. However, Diffusion Models have shown significant advantages over GANs and VAEs, such as greater stability and higher generation quality in various tasks. Diffusion Models (DMs) [5, 6, 7] have achieved state-of-the-art results in image synthesis tasks [7, 8, 9, 10, 11]. These models alleviate the trade-off between stability and quality by gradually corrupting images in a forward process and iteratively learning to reconstruct them. DMs progressively corrupt 𝐱0subscript𝐱0\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with random noise and then recover 𝐱0subscript𝐱0\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT from the corrupted state 𝐱Tsubscript𝐱𝑇\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT step by step. This forward process creates a tractable posterior [6], enabling the iterative modeling of complex distributions through flexible neural networks in the reverse generation process. The objectives of recommender models align well with DMs, as recommendation essentially involves inferring future interaction probabilities from noisy historical interactions, where noise represents false-positive and false-negative items [12, 13]. This makes DMs a promising approach for accurately modeling complex interaction patterns with strong representational ability. Despite their success in other domains, applying diffusion models to recommender systems remains underexplored. We further explore diffusion models for sequential recommendation (SR) by extending the method introduced by Li et al. (2023). Our work proposes significant enhancements to the existing architecture, resulting in a new model, DiffuRecSys111https://youtu.be/bEpDfAAGL2I. Specifically, our contributions are as follows: • Enhancing the Diffusion Recommender Model: We incorporate cross-attention mechanisms within the Approximator of the model architecture. The model isn’t just learning temporal dependencies (the sequential order of items) but also more complex relationships between past interactions and the target item by focusing on the most relevant past events. • Incorporation of Offset Noise: We introduce offset noise into the diffusion process to increase model robustness and effectively handle variability in user interactions. • Comprehensive Experimental Validation: We conduct extensive experiments across three datasets under various settings, demonstrating improvements of DiffuRec with our extensions over standard baselines."
,,"Deep generative models – for example VAEs, GANs, normalizing flows, diffusion models, and flow matching – have recently made great progress in probability density learning (DL), with flow matching achieving highest accuracy at the moment. Besides generative accuracy, human interpretability of the learned representation is highly desirable, and disentangled representation learning (DRL) is a key tool for this, see Bengio et al., (2014) and Wang et al., (2024). Intuitively, DRL means that each latent variable should effect only a single, distinct semantic property of the generated data instances. We consider the problem of measuring if and to what degree a given model has actually learned a disentangled representation. Most prior work addresses this question in a supervised setting, where the true generative factors are known (see Related Work). Since this assumption is often violated in the real world, we instead focus on the unsupervised case. That is, we do not ask if the model has learned the (unknown) true factors, but only if it has learned any disentangled representation at all. The learned representation might be close to the true one, if certain identifiability conditions are fulfilled (see Related Work), but this is beyond the scope of our paper. Our work rests on the manifold hypothesis which states that data in a D𝐷Ditalic_D-dimensional space often reside near a manifold ℳℳ\mathcal{M}caligraphic_M of much lower dimension d≪Dmuch-less-than𝑑𝐷d\ll Ditalic_d ≪ italic_D. Variations along the manifold correspond to semantically important differences between data instances, whereas off-manifold variations are considered as unimportant or noise. This is familiar from PCA, where one interprets directions of high data variability as important, whereas directions of low variability are irrelevant. PCA achieves this under the assumption that the manifold ℳℳ\mathcal{M}caligraphic_M is a linear subspace, and DRL seeks to generalize this to non-linear models. If the important dimensions indeed span the manifold ℳℳ\mathcal{M}caligraphic_M, the representation is called aligned. Our method clearly highlights Alignment by assigning a high manifold entropy to the important features and low to unimportant ones, see fig. 1. Moreover, it allows sorting of latent variables by importance so that the cut-off between important and irrelevant can be adjusted later according to the needs of an application, analogous to PCA’s ordering by variance. Figure 1: The two moons distribution illustrates how manifold entropic metrics quantify DRL in terms of alignment and disentanglement. (Top left) The latent prior and a Cartesian grid spanned by the latent variables Zcsubscript𝑍𝑐Z_{c}italic_Z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and Zdsubscript𝑍𝑑Z_{d}italic_Z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue). The latent distribution is mapped to data space by three generative models with equal accuracy, but vastly different representations. This can be seen by the differences in the transformed grid spanned by the manifold random variables 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and 𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue) in the top row, and the corresponding values of our metrics manifold entropy H⁢(𝑿c)𝐻subscript𝑿𝑐H({\boldsymbol{X}}_{c})italic_H ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ), H⁢(𝑿d)𝐻subscript𝑿𝑑H({\boldsymbol{X}}_{d})italic_H ( bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) and manifold mutual information ℐ⁢(𝑿c,𝑿d)ℐsubscript𝑿𝑐subscript𝑿𝑑\mathcal{I}({\boldsymbol{X}}_{c},{\boldsymbol{X}}_{d})caligraphic_I ( bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) in the bottom row. The total entropy of the distribution (gray) is the signed sum of the three terms. (A) The latent manifolds are entangled (and thus not interpretable), and our metric indicates this by high mutual information (brown). (B) The latent manifolds are locally orthogonal everywhere and have low mutual information. However, alignment is inconsistent (𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT aligns with the upper moon, 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT with the lower), resulting in comparable manifold entropy of both variables. (C) The representation is disentangled and aligned. The manifold entropy is high for the important variable 𝑿csubscript𝑿𝑐{\boldsymbol{X}}_{c}bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT (orange) and low for the noise variable 𝑿dsubscript𝑿𝑑{\boldsymbol{X}}_{d}bold_italic_X start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (blue), and their mutual information is small. Disentanglement, i.e. the statistical independence between latent factors, has been addressed by Independent Component Analysis (ICA, Comon,, 1994) under the assumption of a linear data-generating process (DGP), 𝒙=𝑨⁢𝒔𝒙𝑨𝒔{\boldsymbol{x}}=\boldsymbol{A}\boldsymbol{s}bold_italic_x = bold_italic_A bold_italic_s. When linearity holds, the true generative factors 𝒔𝒔\boldsymbol{s}bold_italic_s are identifiable if they are independent and follow a non-Gaussian distribution. However, identifiability is generally lost for non-linear DGPs 𝒙=Φ⁢(𝒔)𝒙Φ𝒔{\boldsymbol{x}}=\Phi(\boldsymbol{s})bold_italic_x = roman_Φ ( bold_italic_s ), (Hyvärinen and Pajunen,, 1999). Finding conditions on 𝒔𝒔\boldsymbol{s}bold_italic_s to restore identifiability is a major focus of current research, see (Hyvarinen et al.,, 2023) for a recent survey. Alternatively, one can restrict the class of permitted mixing functions ΦΦ\Phiroman_Φ, and this approach primarily inspired the present work. Independent Mechanism Analysis (IMA, Gresele et al.,, 2022) postulates, for example, that the Jacobian of ΦΦ\Phiroman_Φ should be orthogonal in every point, and Principal Component Flows (Cunningham et al., 2022b, ) realize this idea by adding a loss term to normalizing flow training that encourages orthogonality. It can be shown that orthogonality of the Jacobian is equivalent to minimizing the mutual information between the image of the corresponding features in data space after mapping them through the decoder (see fig. 1 and Appendix). This is crucial: In contrast to supervised disentanglement metrics, which are usually defined with respect to the encoder, meaningful unsupervised metrics must be defined in terms of the decoder mapping from latent to data space. Specifically, we make the following contributions: • We propose a set of information-theoretic metrics for DRL defined on the decoder of a generative model. • We introduce Alignment as an important complementary condition to Disentanglement in the IMA framework • We show the usefulness of our metrics at dissecting generative models in order to reveal and quantitatively measure their behaviours."
Robust Time Series Causal Discovery for Agent-Based Model Validation,"Agent-Based Model (ABM) validation is crucial as it helps ensuring the reliability of simulations, and causal discovery has become a powerful tool in this context. However, current causal discovery methods often face accuracy and robustness challenges when applied to complex and noisy time series data, which is typical in ABM scenarios. This study addresses these issues by proposing a Robust Cross-Validation (RCV) approach to enhance causal structure learning for ABM validation. We develop RCV-VarLiNGAM and RCV-PCMCI, novel extensions of two prominent causal discovery algorithms. These aim to reduce the impact of noise better and give more reliable causal relation results, even with high-dimensional, time-dependent data. The proposed approach is then integrated into an enhanced ABM validation framework, which is designed to handle diverse data and model structures. The approach is evaluated using synthetic datasets and a complex simulated fMRI dataset. The results demonstrate greater reliability in causal structure identification. The study examines how various characteristics of datasets affect the performance of established causal discovery methods. These characteristics include linearity, noise distribution, stationarity, and causal structure density. This analysis is then extended to the RCV method to see how it compares in these different situations. This examination helps confirm whether the results are consistent with existing literature and also reveals the strengths and weaknesses of the novel approaches. By tackling key methodological challenges, the study aims to enhance ABM validation with a more resilient valuation framework presented. These improvements increase the reliability of model-driven decision making processes in complex systems analysis.","Chapter 1 Introduction 1.1 Agent-Based Models: An Overview In the modern era, Agent-Based Models (ABMs) fall under the class of modelling and simulation techniques which are increasingly being used in domains such as theoretical economics, finance, social sciences, and epidemiology. It involves a bottom-up approach by putting together individual agents and their interactions so that various phenomena may be synthesised and then examined. The detailed methodologies of such models allow them to understand complexity and draw futuristic conclusions. Recent literature stresses the significant advantages of agent-based models (ABMs) over traditional economic models. Fagiolo (2019) pinpoints the major strengths of ABMs: their capacity to provide comprehensive narratives of interactions among agents with network structures, incomplete information learning processes, and competition in imperfect markets—and the flexibility they provide in validating both model inputs and outputs [1]. This characteristic has gained much attention and prompted much research activity recently. 1.2 The Importance of ABM Validation To successfully implement ABMs in reality, the “validation” of this model could be the decisive factor for its ability to truly reflect real-world reality. The validation process consists of comparing the model output with the actual data obtained in the real world to make sure it is reliable and effective. This process is utilised to establish the credibility of the model and the reliability of the predictions made. Validation is particularly challenging in fields with complex interactions and non-linear dynamics, such as finance. As Windrum et al. (2007) pointed out, significant effort is still needed to realise consistent and satisfactory techniques of ABM method implementation to real-world financial data [2]. A key component in ABM validation is to find the cause-and-effect mechanisms from data. Besides highlighting the importance of correlational testing, causal matching between the ABM outputs and real-world data has recently been emphasized in validation. These approaches aim to understand and explain the origins and propagation of observed phenomena in financial systems [3]. The details of such causal discovery methods and their application in ABM validation will be further discussed in the following chapters. 1.3 Challenges to Address Indeed, in spite of continuous progress in the ABM validation techniques, there are still the most significant challenges in such complex systems applications: 1. Insufficient Robustness of Time Series Causal Discovery Methods: The causal discovery approaches that are commonly used today, such as VAR-LiNGAM and PCMCI, can be quite susceptible to noise and variations in the data. This could create inconsistencies when the method is applied to different subsets of the same dataset or datasets with slightly different characteristics. For ABM validation purposes, the robustness of these techniques should be enhanced. False causality in this regard could be as harmful as wrong conclusions drawn up about the ABM system’s validity and, consequently, about the underlying system mechanisms. 2. Lack of Comprehensive Understanding of Dataset Characteristics’ Impact: While previous studies have examined specific dataset characteristics, there is a lack of comprehensive understanding of how various dataset properties collectively affect the performance of causal discovery methods in ABM validation. The absence of a systematic comparison across a wide range of dataset types (e.g., linear vs. non-linear, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary, sparse vs. dense causal structures) hinders our ability to select appropriate validation techniques and interpret results accurately. This deficiency stands as a huge constraint in the area of creating functional ABM validation processes as a solution to the wide spectrum of complicated problems. 3. Limitations in Existing ABM Validation Frameworks: Even though much progress has been made in ABM Validation, for instance, the framework proposed by Guerini et al. (2017) [3], current approaches still face several key limitations: (a) Insufficient Dataset Property Analysis: Existing frameworks often lack comprehensive tests for some important dataset properties. For instance, some of these may overlook important characteristics such as linearity and stationarity, which are essential for understanding the nature of the data and selecting appropriate modelling techniques. (b) Limited Options for Causal Discovery Methods: Most current frameworks rely on a single or limited set of causal discovery methods. This limitation may prevent us from obtaining optimal performance when dealing with different dataset characteristics or different priorities (such as accuracy vs. efficiency). The lack of method diversity limits the framework’s adaptability to various scenarios and data types. (c) Narrow Range of Performance Metrics: The existing validation framework typically focuses on basic similarity tests or a limited set of performance metrics. This may cause to failure to capture the full range of model performance, especially in complex financial systems where causal relationships can be intricate. Addressing these challenges is crucial for advancing ABM validation in complex systems like financial markets. Overcoming these limitations will lead to more reliable models, enhancing decision-making processes and informing policy decisions. These challenges necessitate innovative approaches in causal discovery and model validation techniques applicable to a wide range of complex systems. 1.4 Novel Approaches and Contributions This research addresses the challenges mentioned above by utilizing several new approaches and providing the following key contributions: 1. Robust Cross-Validated (RCV) Causal Discovery Method (Chapter 3): We introduce a novel approach to enhance the robustness of existing causal discovery methods. By applying cross-validation techniques to causal discovery algorithms such as VAR-LiNGAM and PCMCI, we aim to mitigate the sensitivity of these methods to noise and data variations to improve the consistency and reliability of causal structure identification in complex time series data. 2. Comprehensive Experimental Evaluation and Analysis (Chapter 4): We present a thorough empirical analysis of our proposed methods and existing approaches. This evaluation covers a wide range of characteristics, including linear vs. non-linear relationships, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary behaviour, and sparse vs. dense causal structures. We also examine the scalability of methods with varying numbers of variables and time series lengths. We ran experimental evaluations on both synthetic datasets with controlled properties and a complex simulated fMRI dataset, so as to provide insights into method performance under various conditions. 3. Context-Aware ABM Validation Framework (Chapter 5): Extending the work done by Guerini, we develop an enhanced ABM validation framework that addresses the weaknesses of existing one. In this framework, the user could choose a suitable method of causal inference based on the property of data sets or other validation needs, such as efficiency or accuracy. Another improvement we feature is to include a more comprehensive set of performance metrics for assessing the causal relations to get a more precise evaluation of model performance. This framework is designed to pre-process datasets and ensure their uniformity, analyse dataset attributes, run user-dependent or driven causal structure detection, and enhance validation evaluations. These contributions enhance ABM validation in complex systems by improving causal discovery methods and offering a flexible validation framework. This research aims to increase the accuracy and reliability of ABMs in capturing real-world dynamics across various domains. 1.5 Report Structure The remainder of this report is structured as follows: Chapter 2 provides the foundational background and reviews related work, covering Agent-Based Models in finance, existing ABM validation techniques, causal discovery methods and the current limitations. Chapter 3 introduces our novel Robust Cross-Validated (RCV) Causal Discovery Approach, detailing its theoretical foundations and implementation. Chapter 4 presents our experimental evaluation and analysis, including synthetic dataset generation, comparative analysis of causal discovery methods, and an application to a complex simulated fMRI dataset that mimics real-world neuroimaging data. Chapter 5 describes our Context-Aware ABM Validation Framework, explaining how it integrates improved causal discovery methods and enhances overall validation reliability. Finally, Chapter 6 concludes the report, summarizing key findings, discussing implications for ABM validation in complex systems and suggesting future research directions."
An Auditing Test to Detect Behavioral Shift in Language Models,"As language models (LMs) approach human-level performance, a comprehensive understanding of their behavior becomes crucial. This includes evaluating capabilities, biases, task performance, and alignment with societal values. Extensive initial evaluations, including red teaming and diverse benchmarking, can establish a model’s behavioral profile. However, subsequent fine-tuning or deployment modifications may alter these behaviors in unintended ways. We present a method for continual Behavioral Shift Auditing (BSA) in LMs. Our test compares model generations from a baseline model to those of the model under scrutiny and provides theoretical guarantees for change detection while controlling false positives. The test features a configurable tolerance parameter that adjusts sensitivity to behavioral changes for different use cases. We evaluate our approach using two case studies: monitoring changes in (a) toxicity and (b) translation performance. We find that the test is able to detect meaningful changes in behavior distributions using just hundreds of examples.","Language models (LMs) can now achieve human-level performance in a wide range of tasks, including text summarization, machine translation, coding and even acting as AI scientists: generating hypotheses and designing experiments (Achiam et al., 2023; Katz et al., 2024; Lu et al., 2024; Zhang et al., 2024). Because of this, many sectors are looking for ways to use them to improve existing systems (Kasneci et al., 2023; Felten et al., 2023). Unfortunately, one large roadblock to broad LM adoption is their propensity to generate harmful content (Weidinger et al., 2021). For example, GPT-3 has significant anti-Muslim biases (Abid et al., 2021), and GPT-4 has racial and gender biases (Zack et al., 2024). To address this, a significant effort is going into ensuring LM behavior is aligned with our societal values, spawning the field of AI alignment (Ji et al., 2023). A large portion of this effort is on developing ways to evaluate LM behavior, for example, through benchmarks (Wang et al., 2023a) and red-teaming (Perez et al., 2022). Given these evaluation techniques, how should they be used to ensure LMs stay safe? To answer this, consider the following two hypothetical settings where this question might be asked: (1) Internal Audit: At the new start-up Chasm Intellect, the LM alignment team is looking for a way to trigger an alarm when LM behavior changes. They know that LM model behavior can change unexpectedly (Li et al., 2024b). For example, fine-tuning a model that has undergone safety evaluation (e.g., GPT-3.5 Turbo (OpenAI, 2023)) can cause it to become less safe, even when using a benign fine-tuning dataset (e.g., Alpaca (Taori et al., 2023); (Qi et al., 2023)). How can the team detect meaningful changes in model behavior? (2) External Audit: A journalist has been playing around with Better geneRAtiVe language modEl (BRAVE), Chasm Intellect’s new model, and has noticed it produces highly toxic generations when asked about seemingly benign topics. Their article on this topic spurred a governmental audit of BRAVE. The government performs an initial evaluation but is worried that model behavior will change when no longer under scrutiny. How can the government regularly check the deployed model’s behavior is the same as the previously certified one? We call the general class of problems detecting changes in LM behaviors Behavioral Shift Auditing (BSA) problems. In this paper, we formalize the problem of Behavioral Shift Auditing in Language Models. We detail a test that continuously monitors behavioral shift, solely from model generations (e.g. via API calls). Under some weak assumptions, the test provably guarantees that if model generations have different behavior than those of an initial model, the test will detect it, given enough generations. At the same time, if there has not been a change, the test is guaranteed to have tight, non-asymptotic control over the false positive rate. The key insight behind our approach is that one can phrase the problem of Behavioral Shift Auditing as hypothesis testing over the relevant behavioral distribution. This framing allows our test to be applicable to any measurable aspect of model behavior, including also capabilities (e.g., dangerous capabilities (Phuong et al., 2024) or mathematical reasoning capabilities (Mishra et al., 2022a)) and biases (e.g., gender bias (Wang et al., 2023a; Kotek et al., 2023)). Using this insight, we develop a test that extends recent work on anytime-valid hypothesis testing (Pandeva et al., 2024), a state-of-the-art sequential testing method that has been successfully applied in various auditing settings (Chugg et al., 2023; Shekhar et al., 2023; Waudby-Smith et al., 2021). Our test checks for changes in model behavior distributions, comparing generations from a reference model with those of another, potentially changed model. The test has a tunable parameter that allows one to vary the strictness of the test. This allows for detecting any change in behavior, which may be more suitable for the external audit setting, to detecting a user-specified ϵitalic-ϵ\epsilonitalic_ϵ change in behavior, which could be used for the internal audit if small changes are acceptable. Similar to Pandeva et al. (2024), test performance is optimized using a learning algorithm, improving sample efficiency over prior testing methods (Lopez-Paz & Oquab, 2017; Lhéritier & Cazals, 2018; Podkopaev & Ramdas, 2024). This testing approach can complement a full evaluation when used as a warning system. Before an expensive model assessment on large-scale benchmarks (Achiam et al., 2023; Dubey et al., 2024; Zhang et al., 2024), our approach can be used to detect an initial behavior change, which can then trigger a full evaluation. We experimentally verify that our test satisfies theoretical guarantees and we report its sample efficiency on recent LM architectures for both auditing use cases. We release our code here: https://github.com/richterleo/Auditing_Test_for_LMs. Figure 1: An External Audit Example. A regulator can use the test we describe to perform an external audit: 1. The regulator initially certifies an LM by prompting and evaluating the set of generations received; 2. Later, tipped off that LM behavior may have changed, the regulator poses as a consumer and sends prompts to the model vendor, collecting the generations; 3. The regulator compares the distribution of behavior scores b⁢(⋅)𝑏⋅b(\cdot)italic_b ( ⋅ ) between the initial, certified generations and the later generations using a Behavioral Shift Auditing (BSA) test. If the distributions are sufficiently different the test triggers. Using our proposed method, the regulator can test samples sequentially without increasing the false-positive rate. The method is guaranteed to detect a change if one exists, given enough samples (more details in Section 3)."
Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression,"In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored. Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation. To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL. Technically, SCAS achieves value-aware OOD state correction, capable of correcting the agent from OOD states to high-value in-distribution states. Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS achieves excellent performance without additional hyperparameter tuning. Moreover, benefiting from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations.","Deep reinforcement learning (RL) shows promise in solving sequential decision-making problems, gaining increasing interest for real-world applications [43, 58, 64, 54, 8]. However, deploying RL algorithms in extensive scenarios poses persistent challenges, such as risk-sensitive exploration [14] and time-consuming episode collection [28]. Recent advances view offline RL as a hopeful solution to these challenges [35]. Offline RL aims to learn a policy from a fixed dataset without further interactions [33]. It can tap into existing large-scale datasets for safe and efficient learning [24, 38, 51]. In offline RL research, a well-known concern is the out-of-distribution (OOD) action issue: the evaluation of OOD actions causes extrapolation error [13], which can be exacerbated by bootstrapping and result in severe value overestimation [35]. To address this issue, a large body of work has emerged to directly or indirectly suppress OOD actions during training, employing various techniques such as policy constraint [13, 31, 11], value penalization [32, 2, 7], and in-sample learning [30, 15, 72]. (a) CQL (b) TD3BC (c) SCAS (Ours) (d) Optimal Value Figure 1: The resulting state distributions of offline RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values. Distinguished from most previous works, this paper argues that, apart from the OOD action issue, there exists an OOD state issue that also impairs performance yet has received limited attention in the field. Such an issue refers to the agent encountering states out of the offline dataset during the policy deployment phase (i.e., test phase). The occurrence of OOD states can be attributed to OOD actions, stochastic environments, and real-world perturbations. Since typical offline RL algorithms do not involve policy training in OOD states, the agent tends to behave in an uncontrolled manner once entering OOD states in the test phase. This can further exacerbate the state deviation from the offline dataset and lead to severe degradation in performance [35, 76]. In mitigating this OOD state issue, existing limited work attempts to train the policy to correct the agent from OOD states to in-distribution (ID) states [76, 23]. Technically, Zhang et al. [76] construct a dynamics model and a state transition model and align them to guide the agent to ID regions, while Jiang et al. [23] resort to an inverse dynamics model for policy constraint. However, they deal with the OOD state and OOD action issues separately, requiring extra OOD action suppression components and complex distribution modeling, which sacrifices computational efficiency and algorithmic simplicity. Moreover, correcting the agent to all ID states impartially could be problematic, especially when the dataset contains substantial suboptimal states. As a result, the performance of prior methods also leaves considerable room for improvement. In this paper, we aim to address these two fundamental OOD issues simultaneously by proposing a simple yet effective approach for offline RL. We term our method SCAS due to its integration of OOD State Correction and OOD Action Suppression. We start with solving an analytical form of a value-aware state transition distribution, which is within the dataset support but skewed toward high-value states. Then, we align it with the dynamics induced by the trained policy on perturbed states via KL divergence. This operation intends to correct the agent from OOD states to high-value ID states, a concept we refer to as value-aware OOD state correction. Through some derivations, it also eliminates the necessity of training a multi-modal state transition model. Furthermore, we show theoretically and empirically that, while designed for OOD state correction, SCAS regularization also exhibits the effect of OOD action suppression. We evaluate SCAS on the offline RL benchmarks including D4RL [10] and NeoRL [50]. SCAS achieves excellent performance with consistent hyperparameters without additional tuning. Moreover, benefiting from its OOD state correction ability, SCAS demonstrates improved robustness against environmental perturbations. To summarize, the main contributions of this work are: • We systematically analyze the underexplored OOD state issue in offline RL and propose a simple yet effective approach SCAS unifying OOD state correction and OOD action suppression. • Our approach achieves value-aware OOD state correction, which circumvents modeling complex distributions and significantly improves performance over vanilla OOD state correction methods. • Empirically111Our code is available at https://github.com/MAOYIXIU/SCAS., our approach demonstrates superior performance on standard offline RL benchmarks and enhanced robustness in perturbed environments without additional hyperparameter tuning."
Multi-Agent Reinforcement Learning with Selective State-Space Models,"The Transformer model has demonstrated success across a wide range of domains, including in Multi-Agent Reinforcement Learning (MARL) where the Multi-Agent Transformer (MAT) has emerged as a leading algorithm in the field. However, a significant drawback of Transformer models is their quadratic computational complexity relative to input size, making them computationally expensive when scaling to larger inputs. This limitation restricts MAT’s scalability in environments with many agents. Recently, State-Space Models (SSMs) have gained attention due to their computational efficiency, but their application in MARL remains unexplored. In this work, we investigate the use of Mamba, a recent SSM, in MARL and assess whether it can match the performance of MAT while providing significant improvements in efficiency. We introduce a modified version of MAT that incorporates standard and bi-directional Mamba blocks, as well as a novel ‘cross-attention’ Mamba block. Extensive testing shows that our Multi-Agent Mamba (MAM) matches the performance of MAT across multiple standard multi-agent environments, while offering superior scalability to larger agent scenarios. This is significant for the MARL community, because it indicates that SSMs could replace Transformers without compromising performance, whilst also supporting more effective scaling to higher numbers of agents. Our project page is available at https://sites.google.com/view/multi-agent-mamba.","Figure 1: Normalised mean episode returns aggregated over all tasks and environments with 95% confidence intervals for MAM, MAT, and MAPPO. Results are obtained using ten independent seeds. MAM matches the final performance of MAT, currently state-of-the-art in MARL, and exhibits greater sample efficiency. Figure 2: Mean time (seconds) per evaluation step in smacv2 tasks with increasing numbers of agents for MAM, MAT, and MAPPO. The mean time per evaluation step for MAT increases approximately quadratically, while MAM and MAPPO scale linearly in the number of agents. Figure 3: Comparison of performance metrics for MAM, MAT, and MAPPO across various tasks. Multi-Agent Reinforcement Learning (MARL) still faces significant challenges that must be overcome to unlock its full potential; one such challenge is the ability to scale to large numbers of agents while maintaining good performance. The Multi-Agent Transformer (MAT) (Wen et al., 2022) boasts state-of-the-art performance in online MARL. MAT is an encoder-decoder framework that utilises the multi-agent advantage decomposition theorem (Kuba et al., 2022) to reframe the challenge of joint policy optimisation. It converts the problem into a sequential decision-making process, simplifying the search for optimal policies across multiple agents. However, Transformers scale quadratically in sequence length (Vaswani et al., 2023). This creates a computational bottleneck for MAT as the number of agents becomes large. Recently, State-Space Models (SSMs) (Gu et al., 2022; Gupta et al., 2022; Gu et al., 2021; Gupta et al., 2022; Smith et al., 2023; Fu et al., 2023) have offered a solution to this drawback in the Transformer architecture, with the ability to scale linearly in the sequence length. Of interest in this work is Mamba (Gu and Dao, 2024)–a selective SSM which boasts fast inference and linear scaling in the sequence length whilst matching the performance of attention architectures in the natural language processing domain. The innovations of the Mamba model are its input-dependent SSM parameters and its hardware-aware parallel algorithm in recurrent mode. In this paper, we explore replacing attention in the MAT architecture with Mamba blocks. We make use of both existing vanilla and bi-directional Mamba blocks, as well as a novel ‘cross-attention’ Mamba block we designed to replace MAT’s cross-attention. We evaluate our novel architecture, which we call the Multi-Agent Mamba (MAM), on a wide range of well-known MARL benchmark environments and compare its performance to MAT. Our core contributions can be summarised as follows: • We create an extension of the vanilla Mamba block which can be used as a cross-attention replacement. • We replace the three different attention blocks in the MAT architecture with vanilla, bi-directional and cross-attentional Mamba blocks respectively. • We empirically validate that MAM performs comparably to MAT on a wide range of MARL environments while offering significantly greater efficiency."
Toward Finding Strong Pareto Optimal Policies in Multi-Agent Reinforcement Learning,[1]\fnmViet Cuong \surTa,"Multiagent reinforcement learning (MARL) is the learning framework where multiple different agents learn to make optimal decisions in an environment through the Reinforcement Learning paradigm. MARL research advances rapidly, with the recent development of MARL achieving impressive results on a wide range of learning scenarios, notably in zero-sum games [21] and fully cooperative environments [18, 27] where all agents share the same reward function. A less studied area of research in MARL is in the environments with cooperative reward structures, in which all agents have different and possibly conflicting reward schemes [15]. In these problems, there might be multiple different but optimal policies, in the sense that none of which is better than the other. This concept is known as the Pareto optimality in multi-objective optimization. It is desirable that we can find such Pareto optimal policies, as they are equivalent to the optimal policies in the common setting of single agent problems [3]. While the current MARL methods are known to find Nash equilibrium [13], such solutions can be suboptimal [17]. In this paper, we first show that in general cooperative environments, agents need to explicitly consider the optimization of other agents to achieve Pareto optimality. Such behaviors are known as altruistic learning in RL literature [9]; altruistic learners learn to act for the benefit of other agents even though those actions do not bring about any personal gain for the actors. To learn altruistically, one agent needs to optimize not only the reward of itself but also the rewards of other agents, which involves some form of multi-objective optimization. As a result, we connect the multi-objective framework to the MARL domain. Multiple Gradient Descent Algorithm (MGDA) [5] is one of the most popular gradient-based multi-objective methods. MGDA can find arbitrary solutions in the Pareto optimal Set using first-order gradient descent. However, MGDA is known to only converge to weak Pareto optimal solutions [8]. While this problem is not significant in other learning settings, we show that MARL problems can have many weak Pareto Stationary points, which can reduce the efficacy of MGDA. To this end, we identify the effect of diminishing gradient norms as a root cause to the weak Pareto convergence issue of MGDA and propose an improved version of MGDA++ based on this observation. We demonstrate both theoretically and empirically that MGDA++ can converge to strong Pareto optimal solutions. To summarize, our contributions in this paper are: • We show that to achieve Pareto optimality in MARL, agents need to consider the objective of other agents, we connect the multi-objective optimization with MARL problems, and propose to apply MGDA to the MARL problems. • We propose MGDA++, an extension of MGDA that converges to strong Pareto Solutions with bi-objective problems in convex, smooth settings both theoretically and empirically. To our knowledge, this strong Pareto convergence result in the convex setting with gradient descent is the first in the literature. • We demonstrate the effectiveness of MGDA++ with trust region methods through several cooperative scenarios in the Gridworld benchmark. Our proposed method is able to achieve better convergence solutions across different agents in comparison with other baselines."
Notes on the Mathematical Structure of GPT LLM Architectures,"When considered from a purely mathematical point of view, the building and training of a large (transformer) language model (LLM) is the construction of a function - which can be taken to be a map from some euclidean space to another - that has certain interesting properties. And therefore, from the point of view of a mathematician, it may be frustrating to find that many key papers announcing significant new LLMs seem reluctant to simply spell out the details of the function that they have constructed in plain mathematical language or indeed even in complete pseudo-code (and the latter form of this complaint appears to be one of the motivations behind a recent article of Phuong and Hutter [1]). Here, we seek to give a relatively ‘pure’ mathematical description of the architecture of a GPT-3-style LLM.","When considered from a purely mathematical point of view, the building and training of a large (transformer) language model (LLM) is the construction of a function - which can be taken to be a map from some euclidean space to another - that has certain interesting properties. And therefore, from the point of view of a mathematician, it may be frustrating to find that many key papers announcing significant new LLMs seem reluctant to simply spell out the details of the function that they have constructed in plain mathematical language or indeed even in complete pseudo-code (and the latter form of this complaint appears to be one of the motivations behind a recent article of Phuong and Hutter [1]). Here, we seek to give a relatively ‘pure’ mathematical description of the architecture of a GPT-3-style LLM. Trainable Parameters Like all such models in machine learning, the construction really initially describes a family of functions indexed by some set Θ=𝐑NΘsuperscript𝐑𝑁\Theta=\mathbf{R}^{N}roman_Θ = bold_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT called the parameter space. There is then a separate process - the training of the model - in which a particular value θ∈Θ𝜃Θ\theta\in\Thetaitalic_θ ∈ roman_Θ is selected using a training algorithm. Each dimension of ΘΘ\Thetaroman_Θ corresponds to the possible values of an individual trainable parameter. We will draw attention to such parameters as we introduce them, as opposed to attempting to give a definition of ΘΘ\Thetaroman_Θ up front. But this short note discusses only the architecture and does not describe any training algorithms."
BitPipe: Bidirectional Interleaved Pipeline Parallelismfor Accelerating Large Models Training,"With the increasing scale of models, the need for efficient distributed training has become increasingly urgent. Recently, many synchronous pipeline parallelism approaches have been proposed to improve training throughput. However, these approaches still suffer from two major issues, i.e., pipeline bubbles caused by periodic flushing and extra communication due to the increasing number of pipeline stages. To this end, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. Specifically, a hybrid scheme of fusing interleaved pipelines with bidirectional pipelines is proposed to reduce the computational time of each single micro-batch and multiply the number of devices executing simultaneously. A V-shaped schedule with eager gradient synchronization is introduced to reduce and overlap the communication between devices. Experiments conducted on up to 32 GPUs show that BitPipe improves the training throughput of GPT-style and BERT-style models by 1.05×1.05\times1.05 ×-1.28×1.28\times1.28 × compared to the state-of-the-art synchronous approaches.","Scaling the number of parameters in contemporary deep learning models has yielded remarkable the state-of-the-art (SOTA) results. Training these large models is challenging, as the limited memory and computational capacity of a single device (e.g., GPU) pose obstacles to accommodating them within realistic timeframes. For instance, training a GPT-3 175B model demands over 3,000 GiB for storing model parameters and optimizer states, requiring an impractical 288 years with a single NVIDIA V100 GPU (Kim et al. 2023; Narayanan et al. 2021b). The urgency for parallel and distributed training (e.g., data parallelism and model parallelism) has become increasingly pronounced. While data parallelism (Li et al. 2014) allows for ideal speedup, it falters when confronted with large models that exceed the capacity of a single device. Model parallelism (Dean et al. 2012; Lee et al. 2014; Wang, Huang, and Li 2019) addresses this limitation by distributing the weight parameters of a model across multiple devices, which mitigates the memory usage per device but suffers from severe resource under-utilization. Pipeline parallelism improves resource utilization, which splits a batch into smaller micro-batches and divides a model into stages within a pipeline, allowing simultaneous execution of different micro-batches across multiple devices. Pipeline parallelism can be categorized into synchronous and asynchronous schemes based on weight update semantic. Synchronous approaches flush periodically at the end of each iteration to guarantee strict optimizer semantics, which causes device idle times (also called pipeline bubbles). Asynchronous approaches do away with flushes completely by delaying weight updates, but at the expense of strict model convergence and thus are not within the scope of our work. Figure 1: Classic synchronous pipeline schedules, with 4 pipeline devices and 8 micro-batches within a training iteration. Both schedules have the same bubble overhead and weights memory consumption (Mθsubscript𝑀θM_{\uptheta}italic_M start_POSTSUBSCRIPT roman_θ end_POSTSUBSCRIPT). The activations memory consumption (Masubscript𝑀aM_{\rm a}italic_M start_POSTSUBSCRIPT roman_a end_POSTSUBSCRIPT) of the 1F1B schedule exhibits better efficiency but existing imbalance. Early synchronous approach (e.g., GPipe (Huang et al. 2019)) focuses on reducing pipeline bubbles by increasing the number of concurrent batches in the pipeline (as shown in Figure 1(a)). As a direct consequence, there is an increase in peak activation memory demands. Subsequently, encouraged by the success of the 1F1B schedule (as shown in Figure 1(b)), researchers have proposed memory-efficient approaches (e.g., DAPPLE (Fan et al. 2021) and PipeDream-Flush (Narayanan et al. 2021a)), which further adjusts the number of micro-batches injected into devices at the beginning of pipelines. Recently approaches attempt to increase the number of devices executing simultaneously (i.e., bidirectional pipeline parallelism), or to reduce the computational time of a single micro-batch (i.e., interleaved pipeline parallelism), which shows the SOTA performance. In the bidirectional approaches (Jain et al. 2020; Li and Hoefler 2021; Zhang et al. 2023), each device stores multiple pipeline stages in different directions, which decreases bubble size and achieves a more balanced activation memory consumption. On the other hand, interleaved approaches (Narayanan et al. 2021b; Lamy-Poirier 2023; Liu et al. 2023) assign multiple smaller and nonconsecutive stages to each device, which makes each bubble size smaller accordingly. Despite the promising results, the latest synchronous approaches still face two primary issues. First, the remaining bubbles still pose the largest deficiency. Due to computation dependencies in the pipeline across different devices, bubbles are inevitable. In existing approaches, as much as 50% of the time can be spent to flush the pipeline. Second, the communication overhead remains considerable even though pipeline parallelism employs point-to-point (P2P) communication. Specifically, bidirectional pipeline parallelism requires additional weight memory and data-parallel communication to reduce pipeline bubbles, while interleaved pipeline parallelism shrinks bubble size at the expense of extra P2P communication. Moreover, if the bidirectional pipeline extends to more than two pipelines, or each device in the interleaved pipeline generalizes to have more stages, the extra communication or memory usage will increase accordingly, further degrading their performance. To address the aforementioned issues, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. To the best of our knowledge, BitPipe is the first work that incorporates the interleaved schedule into bidirectional pipeline parallelism, which reduces the computational time of each single micro-batch and doubles the number of devices executing simultaneously. BitPipe transforms the looping schedule of the interleaved pipeline to a V-shaped schedule and thus mitigates the side effect of the additional communication overhead. The contributions of BitPipe are summarized as follows: • We propose a hybrid pipeline scheme of fusing interleaved pipelines with bidirectional pipelines. This design can not only improve throughput, but also achieves a harmonious balance in memory utilization. • We introduce a V-shaped schedule of partially transforming cross-device communication to local copying, alongside an eager gradient synchronization scheme, which can reduce and overlap communication between devices. • Experiments show that BitPipe can improve the end-to-end performance by up to 1.28×1.28\times1.28 × per iteration for GPT-style and BERT-style models compared to the SOTA synchronous pipeline approaches."
FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with Ferroelectric In-Memory Computing,"In scenarios with limited training data or where explainability is crucial, conventional neural network-based machine learning models often face challenges. In contrast, Bayesian inference-based algorithms excel in providing interpretable predictions and reliable uncertainty estimation in these scenarios. While many state-of-the-art in-memory computing (IMC) architectures leverage emerging non-volatile memory (NVM) technologies to offer unparalleled computing capacity and energy efficiency for neural network workloads, their application in Bayesian inference is limited. This is because the core operations in Bayesian inference, i.e., cumulative multiplications of prior and likelihood probabilities, differ significantly from the multiplication-accumulation (MAC) operations common in neural networks, rendering them generally unsuitable for direct implementation in most existing IMC designs. In this paper, we propose FeBiM, an efficient and compact Bayesian inference engine powered by multi-bit ferroelectric field-effect transistor (FeFET)-based IMC. FeBiM effectively encodes the trained probabilities of a Bayesian inference model within a compact FeFET-based crossbar. It maps quantized logarithmic probabilities to discrete FeFET states. As a result, the accumulated outputs of the crossbar naturally represent the posterior probabilities, i.e., the Bayesian inference model’s output given a set of observations. This approach enables efficient in-memory Bayesian inference without the need for additional calculation circuitry. As the first FeFET-based in-memory Bayesian inference engine, FeBiM achieves an impressive storage density of 26.32 Mb/mm2 and a computing efficiency of 581.40 TOPS/W in a representative Bayesian classification task. These results demonstrate 10.7×\times×/43.4×\times× improvement in compactness/efficiency compared to the state-of-the-art hardware implementation of Bayesian inference.","In-memory computing (IMC) has recently emerged as a promising solution to address the memory wall issues in conventional von Neumann hardware (ielmini2018memory, ). Leveraging the compactness and high energy efficiency of emerging non-volatile memory (NVM) technologies, many leading IMC accelerators have achieved impressive computing efficiency and throughput for data-intensive machine learning models, particularly neural networks (NNs) (shafiee2016isaac, ; hu2021memory, ; yan2023improving, ; jung2022crossbar, ). While conventional NN-based algorithms are widely used, they often struggle in situations where training data is insufficient or when interpretable results are needed (qayyum2020secure, ; yang2022unbox, ). As a compelling alternative, Bayesian inference is particularly well-suited in low-data scenarios, providing explainable results with uncertainty estimation (ghahramani2015probabilistic, ; burkart2021survey, ). The primary posterior calculation in Bayesian inference, which involves the cumulative product of prior and likelihood probabilities as per Bayes’ theorem, however, differs from the multiply-and-accumulate (MAC) operations common in NN workloads. This difference renders Bayesian inference usually unsuitable for direct implementation with many existing IMC designs that typically focus on NN acceleration. In traditional complementary metal-oxide-semiconductor (CMOS)-based von Neumann implementations for Bayesian inference, such as CPU (smith2020massively, ), GPU (talbot2019parallelized, ) and field-programmable gate array (FPGA) (awano2020bynqnet, ), accessing separate memory units for stored probabilities incurs significant area and energy overhead. Efforts to exploit the non-volatility and energy efficiency of emerging devices have led to the development of Bayesian inference prototypes utilizing random number generators (RNGs) built with magnetic tunnel junction (MTJ) (vodenicarevic2017low, ), memtransistor (zheng2022hardware, ) and magnetic random-access memory (MRAM) (faria2018implementing, ). These implementations, however, are limited to Bayesian inference with binary evidence/events, and do not effectively address probability storage, rather generating required probabilities on demand, which is energy-consuming. A memristor-based Bayesian machine has been proposed (harabi2023memristor, ), using near-memory stochastic computing to reduce memory access overhead. Yet, these implementations still require additional CMOS logic and multiple clock cycles for posterior calculations and complex sensing circuitry for final inference, thus compromising computing density and inference efficiency. To address the aforementioned challenges in hardware implementation of Bayesian inference, we propose FeBiM, an efficient and compact in-memory Bayesian inference engine utilizing multi-level cell (MLC) ferroelectric field-effect transistors (FeFETs). The key contributions of this work are summarized as follows: • We propose a compact crossbar array design using one FeFET per cell as probability storage unit and a compact and scalable winner-take-all (WTA) circuit for sensing. This multi-bit FeFET array enables efficient in-memory Bayesian inference in just one clock cycle, eliminating the need for extra calculation circuitry. • We introduce a novel mapping scheme that associates quantized logarithmic probabilities with discrete FeFET states. This scheme enables the output currents of the crossbar to naturally represent the posterior probabilities, i.e., the cumulative product of priors and likelihoods given a set of observations. • We thoroughly investigate the functionality, scalability and application level performance of FeBiM. In a representative Bayesian classification task, our proposed design shows a 10.7×\times×/43.4×\times× storage density/inference efficiency improvement compared to the state-of-the-art Bayesian machine. The rest of the paper is organized as follows. Section 2 reviews the basics and relevant prior works. Section 3 introduces our FeFET-based IMC design for Bayesian inference. Section 4 presents the validation, scalability investigation and application benchmarking results of FeBiM. Finally, section 5 concludes the paper."
Interpreting Neural Networks through Mahalanobis Distance,"This paper introduces a theoretical framework that connects neural network linear layers with the Mahalanobis distance, offering a new perspective on neural network interpretability. While previous studies have explored activation functions primarily for performance optimization, our work interprets these functions through statistical distance measures, a less explored area in neural network research. By establishing this connection, we provide a foundation for developing more interpretable neural network models, which is crucial for applications requiring transparency. Although this work is theoretical and does not include empirical data, the proposed distance-based interpretation has the potential to enhance model robustness, improve generalization, and provide more intuitive explanations of neural network decisions.","Neural networks have revolutionized machine learning, achieving remarkable success across diverse applications. Central to their efficacy is the use of activation functions, which introduce non-linearity and enable the modeling of complex relationships within data. While Rectified Linear Units (ReLU) have gained prominence due to their simplicity and effectiveness (Nair and Hinton, 2010), the exploration of alternative activation functions remains an open and valuable area of research (Ramachandran et al., 2018). Neural network units are often viewed as linear separators that define decision boundaries between classes (Minsky and Papert, 1969) with larger activation values suggesting stronger contributions of features to those decisions. Our work challenges this perspective, exploring how individual neurons can be understood through the lens of statistical distance measures. Clustering techniques use distance measures. They aim to minimize the distance between data points and feature prototypes, with smaller values indicating stronger membership to the feature or cluster (MacQueen, 1967a). We explore the intersection between these perspectives on activation interpretations, leveraging the distance-minimization approach of clustering techniques to lay the groundwork for novel neural network designs based on statistical distance measures. This paper establishes a novel connection between neural network architectures and the Mahalanobis distance, a statistical measure that accounts for the covariance structure of data (Mahalanobis, 1936). We present a robust mathematical framework that bridges neural networks with this statistical distance measure and lays the groundwork for future research into neural network interpretability and design. Our key contributions are: 1. We establish a mathematical connection between neural network linear layers and the Mahalanobis distance, demonstrating how Absolute Value (Abs) activations facilitate distance-based interpretations. 2. We analyze the solution space that neural networks are likely to learn when approximating Mahalanobis distance, exploring the effects of non-uniqueness in whitening transformations and the role of Abs-activated linear nodes. 3. We discuss the broader implications of this framework for neural network design and interpretability, laying the groundwork for more interpretable models."
COAT:CompressingOptimizer states andActivation for Memory-Efficient FP8Training,"FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54× compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43× end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine’s speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT.","Foundation Models (FMs), such as Large Language Models (LLM) and Vision Language Models (VLM), have made significant breakthroughs in various tasks such as reasoning, understanding, and summarization (Dubey et al., 2024; Adler et al., 2024; Team et al., 2024; Lin et al., 2024). However, the training of such models, which often comprise billions of parameters, demands substantial computational resources and memory. This presents substantial challenges, making the training of these foundation models very challenging (Smith et al., 2022; Hoffmann et al., 2022). Low-precision training has emerged as a promising approach to make FMs training more efficient (Micikevicius et al., 2017; Wang et al., 2018; Zhu et al., 2020; Xi et al., 2023; Wortsman et al., 2023; Xi et al., 2024). By quantizing tensors used in deep neural networks into lower precision, low-precision training effectively speed up the training process and reduce the memory footprint. Currently, BF16 training (Kalamkar et al., 2019; Micikevicius et al., 2017) is the most prevalent low-precision method, and is widely adopted in large-scale training frameworks like DeepSpeed (Rasley et al., 2020) and Megatron-LM (Shoeybi et al., 2019). With the advent of Nvidia’s H100 GPU (NVIDIA, 2024a), FP8 training Micikevicius et al. (2022) is emerging as the next-generation low-precision technique. Compared to BF16, FP8 training has the potential to (1) double the speed and (2) halve the memory footprint. To achieve practical speedup, Transformer Engine (NVIDIA, 2024b) performs matrix multiplications in FP8 precision, leading to faster training. Transformer Engine’s memory footprint can be further improved by reducing optimizer states, gradients, weights, and activations to lower precision. As illustrated in Figure 1, FP8-LM (Peng et al., 2023) advances this by further quantizing the gradients, weight master copy, and first-order momentum into FP8. This reduces memory and communication overhead, partially improving memory efficiency. However, they do not tackle the memory consumption of activations and still leave the optimizer’s second-order momentum in higher precision. The memory problem of activations becomes even more critical when optimizer, gradient, and weights are sharded across multiple GPUs using ZeRO or FSDP. Besides, second-order momentum is more sensitive to quantization than first-order momentum (Fishman et al., 2024), and activations’ large spikes also make them hard to quantize to FP8 (Yang et al., 2024). This potential accuracy degradation makes them missing a crucial opportunity to optimize memory further. In this work, we propose COAT: Compressing Optimizer states and Activations for memory-efficient FP8 Training to address the aforementioned issue. COAT significantly reduces the overall memory footprint by quantizing optimizer states and activations into FP8. For optimizer states, we observe that FP8 format’s representation range is under-utilized when quantizing them, as illustrated in Figure 2(a). To address this, we introduce a novel Dynamic Range Expansion method which adjusts the distribution of optimizer states to better fit within the FP8 range, thereby minimizing quantization error. For activations, we propose Mixed-Granularity Activation Quantization to achieve efficient and accurate quantization. We apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers. Per-tensor quantization for matrix multiplications is more efficient and better suited for TensorCores, while fine-grained quantization helps maintain accuracy. These two approaches tackle high memory consumption while ensuring minimal performance degradation. We provide an overview of COAT in Figure 1(b) for demonstration. We demonstrate the accurate performance of COAT on a wide range of tasks, including LLM pretraining, LLM fine-tuning, and VLM training. COAT achieves nearly lossless performance on all of these tasks. For efficiency results, COAT achieves 1.54×1.54\times1.54 × end-to-end memory reduction compared with BF16, and 1.43×1.43\times1.43 × end-to-end training speed up on Llama 7B, 13B, and 30B models compared to BF16. COAT also doubles the batch size in all realistic distributed training settings, which is crucial for higher speedup and support for longer context length, leading to a more efficient training process for large-scale models."
Golden Ratio-Based Sufficient Dimension Reduction,"Many machine learning applications deal with high dimensional data. To make computations feasible and learning more efficient, it is often desirable to reduce the dimensionality of the input variables by finding linear combinations of the predictors that can retain as much original information as possible in the relationship between the response and the original predictors. We propose a neural network based sufficient dimension reduction method that not only identifies the structural dimension effectively, but also estimates the central space well. It takes advantages of approximation capabilities of neural networks for functions in Barron classes and leads to reduced computation cost compared to other dimension reduction methods in the literature. Additionally, the framework can be extended to fit practical dimension reduction, making the methodology more applicable in practical settings.","The curse of dimensionality poses significant challenges to statistical analysis when dealing with a large number of variables [1]. Under the supervised learning framework, sufficient dimension reduction (SDR) has emerged as a useful tool that bridges the gap between high dimensionality and traditional statistical modeling. However, current state-of-the-art statistical methods for SDR in the literature often presume the structural dimension, which is generally not the case in practice. Additionally, these methods may not be computationally feasible for handling large sample sizes or high dimensionality efficiently, which limits their usage in many real-world applications. Numerous methods have been proposed on SDR in the past decades, see e.g., [2]. For classical methods such as sliced inverse regression (SIR) [3], minimum average variance estimation (MAVE) method [4], and sliced average variance estimation (SAVE) [5], a main class of estimators for the central space is based on the inverse conditional moments of X|Yconditional𝑋𝑌X|Yitalic_X | italic_Y, where Y∈ℝ𝑌ℝY\in\mathbb{R}italic_Y ∈ blackboard_R and X∈ℝp𝑋superscriptℝ𝑝X\in\mathbb{R}^{p}italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT are the response and p𝑝pitalic_p-dimensional predictor in a regression analysis, respectively. Slicing the continuous response y𝑦yitalic_y is often used to facilitate the estimation. However, this imposes some strong probabilistic structure on X𝑋Xitalic_X. Moreover, selecting the number of slices remains an open and challenging question [6]. [7] proposed a cumulative slicing estimation methodology for SDR and developed three methods: cumulative mean estimation (CUME), cumulative directional regression (CUDR), and cumulative variance estimation (CUVE). [8] introduced a fused estimation procedure (fSIR), with observed performance improvement in some situations. [9] implemented a sliced inverse regression in an online fashion that first constructs an online estimate for the kernel matrix and then performs online singular value decomposition. [10] established consistency of estimation of the dimension reduction space in a high-dimensional setting. In a more recent work, [6] proposed an aggregate inverse mean estimation (AIME) procedure that may substantially improve estimation accuracy compared to the previous methods. It incorporates the cumulative slicing scheme into the aggregate SDR idea proposed by [11] and is much less sensitive to linearity condition violations with the localization step before aggregation. [12] proposed a real-time approach for SDR that uses a principal least squares support vector machines approach to estimate the central subspace more accurately. Their method updates the estimator efficiently as new data is collected, starting from an initial estimator obtained with currently available data. [13] proposed a method that first estimates the expectiles through kernel expectile regression and then carries out dimension reduction based on random projections of the regression expectiles. Several methods in the literature are extended under these frameworks [14, 15, 16]. There are also neural network approaches to SDR for tackling classification problems [17, 18, 19]. For regression problems, [20] proposed a nonlinear SDR method and [21] proposed a stochastic neural network that is computationally feasible to handle large scale data. [21] has proposed an algorithm that is able to obtain structural dimension, although no theoretical understanding is provided (different from this work). In this paper, we propose a golden ratio-based neural network for SDR (GRNN-SDR), a novel approach that utilizes neural networks to capture complex functional forms that are previously inaccessible with the traditional nonparametric regression tools. Our algorithm incorporates the golden ratio to dynamically search for the structural dimension, which significantly reduces computation time and complexity. Theoretical basis have demonstrated the generalization ability of multi-layer neural networks [22, 23]. Under proper conditions, we establish theoretical results that demonstrate that our approach leads to the true structural dimension with high probability. Compared to most of the existing methods, which typically presume the structural dimension to estimate the central space, extensive numerical results show that our proposed method is able to obtain the true or practical structural dimension effectively without prior knowledge. Extensive experiment comparisons show that our method estimate the central space with higher accuracy in most cases and demonstrate higher stability when the true dimensionality is not small. Furthermore, our algorithmic complexity under a fixed neural network structure is O⁢(N)𝑂𝑁O(N)italic_O ( italic_N ), where N𝑁Nitalic_N is the sample size, offering a promising solution to the challenges in SDR."
A Stock Price Prediction Approach Based on Time Series Decomposition and Multi-Scale CNN using OHLCT Images,"Stock price fluctuations are influenced by a variety of factors, including macroeconomic conditions, government policies and market sentiment, which together make price movements complex and difficult to be predicted. Despite many studies aimed at enhancing stock price prediction models, the challenges such as data noise, model overfitting and lack of interpretability are still encountered. To address these issues and improve prediction accuracy, this paper proposes a novel method, named Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), for predicting stock price movements in the China A-share market.","With the rapid development of financial markets and the acceleration of globalization, stock investment has become an essential avenue for many investors to achieve wealth appreciation [1, 2, 3]. However, the complexity and uncertainty of the stock market make accurately predicting stock trends a challenging task. The China A-share market, being one of the largest and most dynamic globally, has attracted significant quantitative analysis and research due to the growing impact of economic globalization [4, 5, 6]. Traditional stock analysis methods heavily reliant on financial data and macroeconomic indicators and often fall short in capturing the dynamic and non-linear relationships within the market [7, 8, 9, 10]. Therefore, quantitative trading has emerged as a pivotal component of modern financial strategies. By leveraging computational techniques, it enables the execution of trading strategies devoid of emotion-based decision-making, thereby uncovering patterns that may elude human analysts [11]. The potential quantitative trading in stock market forecasting has been well-documented [12]. This progress has led to the development of advanced deep models that can handle the inherent complexity, nonlinearity, and noise of financial markets, making the construction of stock trend prediction models using machine learning, deep learning, and big data techniques, which becomes a hot research topic in finance [13, 14, 15, 16, 17, 18, 19]. Despite significant advancements, many deep learning models for stock price prediction still grapple with challenges such as data noise, model overfitting, and insufficient interpretability [20]. Baek et al. [21] pointed out that the limited number of training data points often leads to overfitting in deep neural network models. Ito et al. [22] aggregated several simple, interpretable weak algorithms and assessed the importance of each weak algorithm in improving overall interpretability. In order to tackle the data noise problem, the research conducted by Liu et al. [23] employed sparse autoencoders with one-dimensional (1D) residual convolutional networks to denoise the stock prices. Convolutional neural networks (CNNs) have been employed for stock market prediction due to their efficient feature extraction capabilities [24, 25, 26]. However, traditional CNN approaches often fail to fully utilize time series information, resulting in suboptimal performance under complex market conditions [27]. Additionally, when considering multiple stocks, these models typically focus on individual stock information and neglect the inter-stock correlations that significantly influence price fluctuations [28]. Jiang et al. [29] utilized the opening prices, highest price, lowest price, closing price, and volume (OHLCV) images as inputs to predict the probability of stock price increasing or decreasing, achieving a significant accuracy. Their experimental results showed that 5-day feature maps in CNNs yield significantly better accuracy than 20-day and 60-day maps, suggesting that longer time series of stock feature maps make it challenging for CNN models to identify critical feature points, leading to local feature overfitting. Converting sequences to image features can lose the advantage of utilizing more historical data, whereas shorter image features risk sacrificing significant historical information, increasing prediction inaccuracies. Inspired by these studies, we first propose two novel methods. The first method involves replacing trading volume with stock turnover rate, which eliminates the impact of trading volume caused by ex-rights events, resulting in more stable features. The second method introduces an integration between time separator and OHLCT (Opening price, Highest price, Lowest price, Closing price, and Turnover rate), resulting a new image feature as input, named as TS-OHLCT. Specifically, we incorporate the weekend time information as separators into OHLCT images to help CNNs capture trading temporal information and learn the effects of stock trading cycles. Furthermore, two new architectures are proposed. The first is a Multi-Scale Residual Convolutional Neural Network (MSR-CNN) designed to address the overfitting problem in long sequence images. The second architecture is a Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), which addresses the problem that using only image features makes it difficult to learn information about stock price fluctuations. By integrating sequence data, SMSFR-CNN better captures stock price trends in the A-share market. We observed that traditional CNN methods often exhibit local feature overfitting and convergence issues in stock image feature extraction, with prediction performance gradually deteriorating as the time of sequence feature maps increases. This observation aligns with the findings of [29]. To address these mentioned issues, we decomposed long sequences of stock image features into multiple time periods according to different time scales and assign different feature weights based on their importance (with higher weights for features closer to the current trading day). This significantly reduces overfitting and enables CNNs to learn long sequence image features better. Additionally, considering that investors more concern about the magnitude of price fluctuations rather than the probability of price increasing or decreasing, and given that it is difficult for the image features to effectively capture the magnitude of price changes, the proposed model integrates time series information as an extra features. Our approach utilizes the CNN component to learn time series features and concatenates them with image features, simultaneously predicting both the magnitude and probability of stock price movements. This method effectively incorporates regression labels into the existing framework. Our experimental results indicate that this approach significantly improves the accuracy of stock price trend predictions, reduces the search space for image features, stabilizes and accelerates the convergence process. The paper’s major contributions can be summarized as follows: 1. This is the first time that historical open, high, low, close prices and turnover-rates are incorporated into images, which are separated by weekends and combined with time-specific information. 2. The long sequences of stock image features are decomposed into multiple time periods according to different time scales and assigned different feature weights based on their importance (with higher weights for features closer to the current trading day). 3. Combining time series with image features using CNN improves prediction accuracy, reduces the search space, stabilizes and accelerates the convergence process. 4. Comprehensive comparison experiments between different methods on 4,454 A-share stocks are provided, and the majority of A-share stocks are considered in our experiments. 5. Our proposed method, SMSFR-CNN, outperforms other advanced methods in terms of positive predictive value (PPV) and negative predictive value (NPV). The remainder of the paper is structured as follows. The related works of CNN methods in stock prediction are introduced in Section 2. Section 3 presents the dataset that serves as the basis for our study along with a detailed description of the innovative and predictive model developed in this paper. In Section 4, we describe and analyze the experimental settings and results. Finally, we provide a summary of the conclusions and main contributions of the paper in Section 5. We also highlight the significance of our results and propose potential works for future research."
Applying sparse autoencoders to unlearn knowledge in language models,"We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it language models. We demonstrate that individual interpretable biology-related SAE features can be used to unlearn biology-related knowledge with minimal side-effects. Our results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective. We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique. Current SAE quality or intervention techniques would need to improve to make SAE-based unlearning comparable to the existing fine-tuning based techniques.","Current and future language models may learn inaccurate information, produce toxic or malicious outputs, or possess dangerous capabilities that we would like to remove before deployment, such as advanced bio-weapons knowledge (Ji et al., 2023; Li et al., 2024). However, we do not yet know how to precisely and robustly remove knowledge or unlearn capabilities in these language models. The goal of this work is to investigate whether sparse autoencoders (SAEs) can be used to perform unlearning in an interpretable way. Recent work on unlearning has typically focused on fine-tuning based methods that have been applied in a variety of contexts to unlearn concepts in language models (e.g. Li et al., 2024; Zou et al., 2024; Eldan & Russinovich, 2023), going beyond prior work that aimed to unlearn specific training data points in neural networks (Bourtoule et al., 2020). While relatively successful, these fine-tuning approaches are opaque and we lack insight into what exactly is happening in the model (Łucki et al., 2024). Existing methods for removing specific facts from language models offer interpretable solutions (e.g. Meng et al., 2023), however these approaches are limited to fact-level unlearning. Having an interpretable method for unlearning is important as it can allow a higher level of confidence that the model has actually unlearned the knowledge, rather than superficially or temporarily hiding the capability to discuss a given topic. One possibility is to use sparse autoencoders to try to unlearn knowledge in an interpretable way. Sparse autoencoders (SAEs) use an unsupervised method to learn sparse reconstruction of language model activations (e.g. Ng, 2011; Bricken et al., 2023; Cunningham et al., 2023; Templeton et al., 2024; Marks et al., 2024; Gao et al., 2024). SAEs have been shown to find interpretable features in language models. SAEs appear to be a promising approach to help understand complex, abstract features that are used by language models (Templeton et al., 2024). Whether SAEs can be used to make systematic, predictable, interpretable interventions in language models in a variety of contexts remains an open question. Our work makes two key contributions: First, we attempt to develop a method for unlearning knowledge in language models in an interpretable way. Second, we apply SAEs to the task of unlearning knowledge, extending their use beyond previous work. Our approach aims to work towards more transparent and verifiable knowledge removal at a broader scale. Figure 1: An outline of how we use SAE features to intervene in the model. Selected feature activations fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are set to a negative value −c𝑐-c- italic_c when fi>0subscript𝑓𝑖0f_{i}>0italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0."
Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware Neuron Management,s m \IfBooleanF#1▷▷\triangleright▷ #2,"Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of applications (chatgpt, ; gpt4, ; llm-application-medical, ; llm-application-education, ; llm-application-finance, ; llm-application-engineer, ). Comprising millions or even billions of parameters (bert, ; opt, ; gpt3, ; llama2, ; palm, ; mistral, ; gemini, ), these models demand substantial computational and memory resources, typically available only in state-of-the-art data centers. Nonetheless, there is an increasing demand for deploying LLMs on resource-constrained devices, such as smartphones (llm-mobile-1, ; llm-mobile-2, ; llm-mobile-3, ; llm-mobile-4, ; llm-mobile-5, ; llm-mobile-6, ). On one hand, stringent privacy regulations necessitate local data processing to protect user information. On the other hand, LLMs on smartphones facilitate customization based on user habits, enabling enhanced personalization. Given the limited DRAM capacity of devices, LLMs on smartphones are typically constrained to models specially designed for mobile deployment (phi3, ; minicpm, ; gemini, ). Although these models are lightweight, the reduction in parameters inevitably leads to a compromise in their capabilities (scaling-law, ). As an alternative, many recent works (powerinfer-2, ; llm-flash, ; deja-vu, ; powerinfer, ; sparsity-mobile-survey-1, ; sparsity-mobile-survey-2, ) explore the exploitation of inherent sparsity within LLMs to address memory limitations. Specially, rather than pruning model parameters, these methods selectively activate a subset of model parameters based on the input while maintaining the original performance. By transferring only the activated parameters to DRAM for computation, larger and more powerful LLMs can be stored in external flash memory, effectively surpassing DRAM limitations of smartphones. Table 1. Breakdown of average inference latency per token when offloading 50% model parameters to flash memory. Model Compute Load Total Load Ratio OPT-350M 34 ms 87 ms 121 ms 71.9% OPT-1.3B 84 ms 273 ms 357 ms 76.5% OPT-6.7B 387 ms 1883 ms 2270 ms 82.9% Llama2-7B 450 ms 10982 ms 11432 ms 96.1% Mistral-7B 355 ms 15126 ms 15481 ms 97.7% Figure 1. Bandwidth utilization on smartphones is heavily constrained by IOPS. Ripple alleviates this bottleneck and boosts bandwidth with neuron co-activation linking. However, the efficiency of this LLM inference paradigm is significantly hindered by I/O overheads. Since different inference requests generally activate distinct sets of model parameters, frequent I/O operations are generated to swap parameters between DRAM and flash memory. As shown in Table 1, even when only half of the model parameters reside in flash memory, 71.9%-97.7% of the inference latency arises from I/O operations. More critically, the scattered activation of model parameters induces numerous small-grained read accesses, limiting transfer efficiency due to constraints in Input/Output Operations Per Second (IOPS) (iops, ). As depicted in Figure 1, this IOPS bottleneck severely restricts on-device bandwidth utilization across various LLMs. Building on these insights, this paper proposes Ripple, a novel approach to accelerating LLM inference on smartphones through I/O optimizations. While previous works (deja-vu, ; powerinfer, ) primarily focus on computation efficiency under activation sparsity, they tend to exacerbate the existing I/O overhead bottlenecks. Fewer studies (powerinfer-2, ; llm-flash, ) explore mitigating I/O overhead through enhanced caching strategies to minimize data loading. However, without directly improving bandwidth utilization, overall efficiency remains suboptimal. Orthogonal to these methods, Ripple addresses the primary bottleneck in LLM inference by maximizing bandwidth utilization via the effective reduction of I/O operations. The design of Ripple is rooted in Neuron Co-Activation, a property prevalent in activation sparsity yet underexplored in current works. Specially, neurons in LLMs exhibit strong correlations in their activation patterns. When processing real-world datasets, the activation of an individual neuron is consistently linked to the activation of a stable group of others. Given the efficiency of continuous reads, which enable the retrieval of larger data blocks with a single request, Ripple introduces a key insight: Why not establish links between neurons that are frequently co-activated in flash memory, facilitating continuous read access to reduce IOPS? However, this is not a low-hanging fruit, as both neuron co-activation patterns and storage hardware characteristics exhibit inherent complexity, complicating their effective alignment. Our comprehensive analysis identifies that three critical technical challenges must be tackled: (1) Extensive Search Space. The vast number of neurons in LLMs leads to an exponentially large space of possible neuron linking combinations. Identifying the optimized neuron linking that maximizes global benefits is exceedingly difficult and infeasible through brute-force enumeration alone. (2) Random Activation Variation. Owing to varying model inputs, the activation of model parameters exhibits intrinsic randomness. Although optimized placement strategies can spatially co-locate activated neurons, access to these neurons remains hindered by discontinuities caused by randomness. (3) Misaligned Cache Strategy. Storing frequently activated neurons in memory is critical for minimizing transfer workload. However, storing neurons individually leads to fragmentation in their placement within flash memory, potentially disrupting continuous access. To this end, Ripple employs a two-stage solution that performs hierarchical optimizations both offline and online. (1) In the Offline Phase, Ripple clusters neurons exhibiting high co-activation correlation and reorganizes their placement in flash memory. To address Challenge (1), we abstract the problem into a complete graph, reformulating it as the discovery of the globally optimal Hamiltonian Path. By leveraging graph-theoretic techniques, we propose a greedy algorithm that efficiently searches for optimized placement based on observed neuron co-activation patterns. (2) In the Online Phase, Ripple performs fine-grained refinements based on optimized neuron placement, further enhancing access continuity. To tackle Challenge (2), we devise an IOPS-friendly access collapse technique. By strategically incorporating additional neurons between two separate neuron links, we improve read access continuity with negligible overhead. In response to Challenge (3), we design a linking-aligned in-memory caching policy. Rather than individually caching the hottest neurons, we account for their interlinking relationships, ensuring efficient access patterns. We evaluate Ripple on three smartphones with distinct hardware configurations, benchmarking a diverse range of LLMs varying in structures and scales. The results demonstrate that Ripple significantly boosts on-device bandwidth, achieving improvements of up to 4.32×4.32\times4.32 ×. Moreover, this bandwidth optimization yields substantial reductions in I/O latency during inference, offering speedups of up to 5.93×5.93\times5.93 × when compared to state-of-the-art solutions. To the best of our knowledge, Ripple is the first to accelerate LLM inference on smartphones by enhancing I/O bandwidth through optimized neuron placement in flash memory. Ripple effectively bridges the performance gap between flash memory and DRAM, enabling LLM inference to exceed DRAM limitations on smartphones. Our contributions can be summarized as follows: • We identify the primary bottleneck in LLM inference on smartphones as IOPS, attributing it to the inherent misalignment between scattered activation patterns and storage hardware characteristics. • We notably exploit neuron co-activation to mitigate the IOPS bottleneck, pioneering the optimization of neuron placement in flash memory for enhancing bandwidth efficiency on smartphones. • We conduct extensive evaluations on various representative LLMs and hardware, achieving substantial improvements over state-of-the-art solutions."
"Datasheet for “Coordinated Reply Attacks in Influence Operations:
Characterization and Detection”",Motivation,
A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution Generalization to Adaptation,"Distribution shifts on graphs – the discrepancies in data distribution between training and employing a graph machine learning model – are ubiquitous and often unavoidable in real-world scenarios. These shifts may severely deteriorate model performance, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph machine learning under distribution shifts, aiming to train models to achieve satisfactory performance on out-of-distribution (OOD) test data. In our survey, we provide an up-to-date and forward-looking review of deep graph learning under distribution shifts. Specifically, we cover three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation. We begin by formally formulating the problems and discussing various types of distribution shifts that can affect graph learning, such as covariate shifts and concept shifts. To provide a better understanding of the literature, we systematically categorize the existing models based on our proposed taxonomy and investigate the adopted techniques behind. We also summarize commonly used datasets in this research area to facilitate further investigation. Finally, we point out promising research directions and the corresponding challenges to encourage further study in this vital domain. Additionally, we provide a continuously updated reading list at https://github.com/kaize0409/Awesome-Graph-OOD.","Driven by the prevalence of graph-structured data in numerous real-world scenarios, growing attention has been paid to graph machine learning, which effectively captures the relationships and dependencies among entities within graphs. In particular, Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations on graphs through message-passing [1, 2, 3], and they have demonstrated remarkable success across diverse applications, such as social networks, physics problems, and traffic networks [4, 5, 6]. While graph machine learning has achieved notable success, most of the existing efforts assume that test data follows the same distribution as training data, which is often invalid in the wild. When confronted with Out-Of-Distribution (OOD) samples, the performance of graph machine learning methods may substantially degrade, limiting their efficacy in high-stake graph applications such as finance and healthcare [7]. Although numerous transfer learning methods have been proposed to address distribution shifts for Euclidean data [8, 9, 10], their direct application to graph data is challenging. This is due to the interconnected nature of entities on graphs, which violates the independent and identically distributed (IID) assumption inherent in traditional transfer learning methods. Moreover, the various types of graph shifts introduce new challenges. These shifts occur across different modalities including features, structures, and labels, and can manifest in various forms such as variations in graph sizes, subgraph densities, and homophily [11]. Given these obstacles, increasing research efforts have been dedicated to improving the reliability of graph machine learning against distribution shifts, concentrating on three main scenarios: graph OOD generalization [7, 11], training-time graph OOD adaptation [12, 13], and test-time graph OOD adaptation [14, 15]. The primary distinction between graph OOD generalization and adaptation methods lies in their assumptions regarding the availability of target data. Graph OOD generalization methods typically assume the unavailability of target data during model training and aim to enhance the model’s generalization performance on any potential unseen test distribution. In contrast, both training-time and test-time adaptation methods assume the availability of target data and aim to improve model performance on this specific target. However, they differ in their assumptions about the source data and in how they utilize knowledge of the source distribution. Training-time adaptation assumes that both the source and target graphs are available simultaneously, allowing model adaptation to start from scratch during the training process. On the other hand, test-time adaptation typically assumes access to a model pre-trained on the source graph, rather than the source graph itself, and begins adapting the model to the target data from this pre-trained state. Although graph OOD generalization, training-time OOD adaptation, and test-time OOD adaptation are closely related, there is currently no unified framework that comprehensively discusses deep graph learning under distribution shifts across all three scenarios. With recent progress on graph OOD learning, an up-to-date and forward-looking review of this field is urgently needed. In this survey, we provide, to the best of our knowledge, the first unified and systematic review of the literature on deep graph learning under distribution shifts. We start by formally formulating the problems and discussing different types of graph distribution shifts in graph machine learning. Next, our new taxonomy is proposed, classifying existing methods into three categories based on the model learning scenario: (1) graph OOD generalization, where the generalizability is enhanced through strategic design of the model when training on source data, (2) training-time graph OOD adaptation, where the adaptation is performed when jointly training the model based on both source and target data [16, 17], and (3) test-time graph OOD adaptation, where the adaptation happens when adjusting a pre-trained source model to the target data [18, 19]. To deepen our understanding of these approaches, we further classify existing methods within each of the three categories into model-centric and data-centric strategies. Model-centric approaches focus on the learning process or the architecture of the graph model itself, enhancing the model’s inherent ability to generalize or adapt to distribution shifts by refining its structure, training objectives, or learning mechanisms. In contrast, data-centric approaches emphasize the manipulation of input graphs, improving model performance by addressing the data directly, either through preprocessing techniques or data augmentation strategies. Within each subline of research, we elaborate on the detailed techniques for enhancing the generalizability or adaptability under distribution shifts on graphs. Additionally, we provide a summary of the datasets utilized in these studies, highlighting their characteristics and relevance to the challenges posed by distribution shifts. Based on the current progress on graph OOD learning, at the end we also point out several promising research directions in this evolving field. Differences between this survey and existing ones. Despite the urgent need for an overview of graph learning under distribution shifts, existing surveys have primarily focused on subfields within this area, rather than providing a comprehensive overview from multiple scenarios. Until now, there have been several surveys in related areas, but exhibiting distinct focuses, including graph OOD generalization [7, 20], graph domain adaptation [21, 22], trustworthy graph learning [23] related to distribution shifts. Our work distinguishes itself from existing ones in the following aspects: (1) Main focus. Our survey centers on the challenges and solutions for graph learning under distribution shifts, while [23] analyzes OOD issues from a trustworthy perspective and does not delve into the methodological aspects. Conversely, [20] examines graph machine learning from a causal perspective, which is narrower than our broad examination. (2) Taxonomy. We provide a comprehensive categorization of existing methods and summarize them, whereas related work, such as [24], lacks such summaries. Other surveys like [21] and [22] primarily focus on domain adaptation without addressing the broader scope of graph OOD learning. Additionally, we provide coverage of the most recent advancements and discussions in this field. Survey structure. The general organization of this survey is presented as follows: Section 2 introduces the notations and preliminaries. Sections 3, 4 and 5 review graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation, respectively. Each section discusses model-centric and data-centric approaches within its scenario, further detailing the techniques associated with each category. Furthermore, Section 6 provides a comprehensive summary of the datasets used in the literature, highlighting popular graph datasets for evaluation and their relevance to the challenges posed by distribution shifts. Section 7 explores promising future research directions and the associated challenges. Finally, Section 8 presents the conclusion of this survey."
Spatioformer: A Geo-encoded Transformer for Large-Scale Plant Species Richness Prediction,"Earth observation data have shown promise in predicting species richness of vascular plants (α𝛼\alphaitalic_α-diversity), but extending this approach to large spatial scales is challenging because geographically distant regions may exhibit different compositions of plant species (β𝛽\betaitalic_β-diversity), resulting in a location-dependent relationship between richness and spectral measurements. In order to handle such geolocation dependency, we propose Spatioformer, where a novel geolocation encoder is coupled with the transformer model to encode geolocation context into remote sensing imagery. The Spatioformer model compares favourably to state-of-the-art models in richness predictions on a large-scale ground-truth richness dataset (HAVPlot) that consists of 68,170 in-situ richness samples covering diverse landscapes across Australia. The results demonstrate that geolocational information is advantageous in predicting species richness from satellite observations over large spatial scales. With Spatioformer, plant species richness maps over Australia are compiled from Landsat archive for the years from 2015 to 2023. The richness maps produced in this study reveal the spatiotemporal dynamics of plant species richness in Australia, providing supporting evidence to inform effective planning and policy development for plant diversity conservation. Regions of high richness prediction uncertainties are identified, highlighting the need for future in-situ surveys to be conducted in these areas to enhance the prediction accuracy.","Australia is home to a large and diverse range of plant species, with over 21,000 known native species of vascular plants and 93% of these being endemic [1, 2]. The richness of plant species, also known as α𝛼\alphaitalic_α-diversity, is highly important in maintaining the functioning of ecosystems, such as habitat provision, carbon sequestration, and water cycling [3, 4, 5, 6]. However, anthropogenic interference, such as deforestation, overgrazing, and urbanisation, has resulted in a decline in plant species richness [7, 8]. In response, conservation activities have been initialised and conducted across the country aiming to preserve plant diversity [9, 10, 11]. Accurate and up-to-date maps of plant species richness will strongly support effective planning and policy-making for these activities [12, 13]. Earth observation (EO) data provide rapid and near-real-time estimates of changes in land surface conditions across large regions [14, 15, 16, 17]. This makes remote sensing imagery a favourable data source for plant species richness modelling, as compared with another widely adopted approach where environmental variables, such as temperature, precipitation, soil texture, and topographic heterogeneity, are used as richness predictors [18]. The reason is that environmental variables drive mainly the environmental potential of plant habitats (i.e., the capacity to sustain a certain level of richness), rather than represent the actual conditions on the ground like those observed by remote sensing satellites. For example, deforestation, floods, and bushfires could cause reduction in richness [19], but such reduction might not be reflected by environmental variables. Therefore, environmental variables are often aimed at predicting the natural patterns in diversity in a pre-intensification reference state, while remote sensing data are more valuable for monitoring actual changes in those patterns. Australia covers an area of over seven million square kilometres. As a result of the relatively large geographical extents, versatile types of plant habitats are found across the country, differing in their inventories of plant species present that have been shaped by a variety of factors such as biogeographic history, climate, and geography [20]. To understand the spatiotemporal distribution of plant species richness, perseverant in-situ field surveys have been conducted over the past several decades. Via various survey campaigns, a wealth of 219,552 richness samples have been gathered across the country as of the year 2022 [18]. These samples represent a broad range of landscapes across the continent, and therefore present a unique opportunity to unravel the potentially intricate relationship between richness measurements and satellite observations. Nevertheless, geographically distant regions may exhibit distinct assemblages of plant species with differed compositional properties (i.e., β𝛽\betaitalic_β-diversity [21]), making it challenging to model richness over large spatial scales. Due to spatial variations in plant species composition, a location with a set of plant species would be expected to display quite different spectral features in remote sensing imagery from another location with a dissimilar plant composition, even if the two locations sustain the same richness of species [22, 23]. Through statistical regression analysis for two regions in southeast Australia, previous studies [13, 24] suggested that the relationship between plant species richness and hyper/multispectral satellite observations is region-specific. To account for the location dependency, we need a model that is capable of taking in geolocation context when mapping plant species richness over large spatial scales. The transformer model, first introduced in [25], is built upon the self-attention mechanism [26]. The model attends effectively to information of high importance in the input data, as the self-attention module is capable of capturing intricate data structures and dependencies [26, 27]. Initially proposed for language tasks, the transformer model has shown promise for image understanding due to its superior ability over Convolutional Neural Networks (CNNs) in capturing global dependencies between different regions of an image [28]. As a seminal work on applying transformer to image data, the Vision Transformer (ViT) model [28] first divides an image into non-overlapping patches, followed by projecting each patch into a feature vector which is then fed into the self-attention module, with state-of-the-art performance being achieved on benchmark datasets. Given remote sensing imagery captures rich features in the spectral dimension, the SpectralFormer model [29] was developed to effectively embed the spectral information. This model was later extended in [30], where FactoFormer, a factorised transformer, was introduced for the joint learning of spectral and spatial features. These studies have demonstrated the effectiveness of transformer in processing remote sensing images (e.g., [30, 29]), but to advance the model’s application to remote sensing images recorded over large spatial scales, geolocational information could be leveraged. Unlike many types of imagery whose semantics are independent of the location where they are recorded, remote sensing images are intrinsically associated with geolocations [31, 32]. Considering that the composition of plant species is location-specific, incorporating geolocation context could be helpful in modelling the location-dependent relationship between richness and remote sensing imagery. Geo-coordinates provide geographical priors that supplement geolocation context to the image data [33, 32, 34, 31, 35, 36, 37]. While a straightforward way to utilise geolocational features is to concatenate the original longitude and latitude coordinates into the model, this approach has shown to yield almost no gain in performance [34]. To deal with this problem, a geo-feature extraction approach was proposed in [34] for CNN models, where the geo-coordinates were projected into a higher dimensional feature space with a geolocation encoder, whose outputs were then merged into those of a CNN-based image network. It was observed that, by leveraging geolocation context, a 7% increase in accuracy was achieved for an image data set spanning over the continental United States [34]. This geo-encoded CNN model was later applied to global-scale vegetation canopy height mapping with satellite imagery [38], where the geolocations served as a prior. Other state-of-the-art geolocation encoders include Space2Vec [39], Sphere2Vec [40], PE-GNN [41], and a more recent algorithm that is based on the spherical harmonic basis functions [42]. Multi-scale sinusoidal functions are favoured in building these encoders (e.g., [39, 42]), thanks to their merits of being bounded in value, infinitely extended in space, and possessing a multi-resolution scalability. Geolocation encoding is demonstrated to be effective in many large-scale geospatial problems, such as animal species categorisation [38, 40], water quality prediction [43], event/activity recognition [44], and remote sensing scene classification [31, 40]. In this study, we aim to predict the spatiotemporal distribution of plant species richness in Australia from EO imagery with geolocation context being taken into account. Considering that the relationship between plant species richness and remote sensing imagery varies from one location to another due to differences in vegetation composition, we propose Spatioformer, where a novel geolocation encoder is coupled with the transformer model in order to incorporate the geolocation context. The performance of Spatioformer in richness mapping is compared with a CNN model, a ViT model, and the FactoFormer model where the geolocational information is not encoded. Through quantitative analyses, we seek to address primarily the following important questions: (1) Does Spatioformer perform better than state-of-the-art algorithms in predicting plant species richness over large spatial scales? (2) What are the spatial patterns of plant species richness in Australia inferred from remote sensing evidence? (3) Where future in-situ surveys should be conducted as suggested by the mapping results? The rest of the paper is organised as follows. Section II describes the study area and the datasets used for modelling, including the ground-truth samples of plant species richness and satellite imagery. Section III introduces the methods with a focus on the proposed Spatioformer model. This model is developed with the aim to capture the location-dependent relationships between plant species richness and remote sensing imagery over large spatial scales. Section IV describes the experimental settings for training and validation of the Spatioformer model. Section V presents the results of applying Spatioformer to plant richness mapping across Australia, and discusses the implications of these findings for biodiversity conservation and future research directions. Finally, Section VI concludes the paper."
CHESTNUT: A QoS Dataset for Mobile Edge Environments,"Quality of Service (QoS) is an important metric to measure the performance of network services. Nowadays, it is widely used in mobile edge environments to evaluate the quality of service when mobile devices request services from edge servers. QoS usually involves multiple dimensions, such as bandwidth, latency, jitter, and data packet loss rate. However, most existing QoS datasets, such as the common WS-Dream dataset, focus mainly on static QoS metrics of network services and ignore dynamic attributes such as time and geographic location. This means they should have detailed the mobile device’s location at the time of the service request or the chronological order in which the request was made. However, these dynamic attributes are crucial for understanding and predicting the actual performance of network services, as QoS performance typically fluctuates with time and geographic location. To this end, we propose a novel dataset that accurately records temporal and geographic location information on quality of service during the collection process, aiming to provide more accurate and reliable data to support future QoS prediction in mobile edge environments.","I original dataset description To create a high-quality dataset for predicting Quality of Service (QoS) in mobile edge environments, this study utilized two real-world datasets from Shanghai. One dataset is from the Shanghai Johnson Taxi, containing information such as the longitude, latitude, moving direction, and speed of the taxis on a specific day, which was used to simulate user mobile datasets. The other dataset is from Shanghai Telecom [1, 2, 3], providing the longitude and latitude of the base stations. Data from June 2014 was used to simulate the generation of edge server datasets. I-A Shanghai Johnson Taxi Dataset The Shanghai johnson taxi dataset is an important resource for traffic research, containing real-time GPS and business status information from taxis in Shanghai. Each record in the dataset includes various fields: the vehicle ID, control word (where A indicates normal and M indicates alarm), business status (0 for normal and 1 for alarm), passenger status (0 for occupied and 1 for unoccupied), top light status (with values ranging from 0 for operation to 5 for out of service), road type (0 for ground road and 1 for express road), brake status (0 for no braking and 1 for braking), meaningless fields, reception date, GPS timestamp, longitude, latitude, speed, direction, the number of satellites, and additional meaningless fields. This dataset enables the analysis of urban traffic flow, travel patterns, and traffic management strategies. In this paper, we use only the gps time, latitude, longitude, speed, and direction of the cab to generate motion information about the mobile user. I-B Shanghai Telecom Dataset This study utilized a telecommunications dataset provided by Shanghai Telecom, comprising over 7.2 million records of 9,481 mobile phones accessing the Internet through 3,233 base stations over six months.111http://sguangwang.com/TelecomDataset.html The dataset includes six parameters: month, date, start time, end time, base station location (latitude and longitude), and user ID used within Shanghai Telecom. This dataset can be used to evaluate solutions in mobile edge computing, such as edge server deployment, service migration, and service recommendation. In our research, we need to simulate the information of edge servers on base stations based on this dataset. Furthermore, considering the substantial volume of data, we only counted the data for one month and only focused on the geographic location information of Shanghai base stations."
Enhancing Exchange Rate Forecasting with Explainable Deep Learning Models,"Accurate exchange rate prediction is fundamental to financial stability and international trade, positioning it as a critical focus in economic and financial research. Traditional forecasting models often falter when addressing the inherent complexities and non-linearities of exchange rate data. This study explores the application of advanced deep learning models, including LSTM, CNN, and transformer-based architectures, to enhance the predictive accuracy of the RMB/USD exchange rate. Utilizing 40 features across 6 categories, the analysis identifies TSMixer as the most effective model for this task. A rigorous feature selection process emphasizes the inclusion of key economic indicators, such as China-U.S. trade volumes and exchange rates of other major currencies like the euro-RMB and yen-dollar pairs. The integration of grad-CAM visualization techniques further enhances model interpretability, allowing for clearer identification of the most influential features and bolstering the credibility of the predictions. These findings underscore the pivotal role of fundamental economic data in exchange rate forecasting and highlight the substantial potential of machine learning models to deliver more accurate and reliable predictions, thereby serving as a valuable tool for financial analysis and decision-making.","Since the dissolution of the Bretton Woods system, the adoption of a floating exchange rate regime has introduced significant challenges in risk management for market participants. The volatility of the RMB/USD exchange rate, particularly during periods of trade tensions, has heightened the uncertainty faced by those engaged in the foreign exchange market. The People’s Bank of China’s reform of the exchange rate fixing mechanism on August 11, 2015, further increased the marketization of the RMB exchange rate, leading to greater exchange rate volatility and increased foreign exchange risk. This has underscored the critical need for accurate exchange rate forecasting and effective risk management strategies. China’s growing role in the global supply chain, especially after its accession to the World Trade Organization (WTO), and the deepening economic ties between China and the United States, have made the RMB/USD exchange rate a focal point of global economic stability. Debates over the valuation of the RMB, particularly during periods of significant trade surpluses with the U.S., have led to multiple rounds of discussions on exchange rate policy. The RMB exchange rate reform in 2005 and the subsequent rounds of monetary policy adjustments by the Federal Reserve, especially during the 2007 financial crisis, further complicated the dynamics of the RMB/USD exchange rate. The “8.11 Exchange Rate Reform” in 2015, which introduced a more flexible exchange rate mechanism, and the intensified trade frictions since 2018, have put additional depreciation pressure on the RMB, making accurate forecasting of the RMB/USD exchange rate increasingly important [1, 2, 3, 4, 5]. Previous research on exchange rate forecasting has primarily focused on theoretical and quantitative models. Theoretical models often emphasize the equilibrium state of exchange rates, which can be difficult to achieve or maintain in practice, making short- to medium-term predictions particularly challenging. Quantitative models focus on the exchange rate’s own dynamics while often neglecting other critical influencing factors. Moreover, these models have struggled to produce consistent results across different studies. In recent years, there has been a notable shift towards using big data approaches in forecasting models, bypassing the need for complex mathematical modeling and allowing for more flexible model forms without predefined structures. The inherent complexity and non-linearity of exchange rate data have led to applying non-linear methods, such as chaos theory, non-parametric methods, and machine learning techniques, which have shown potential to improve forecasting accuracy. Studies like those of LeBaron and others have demonstrated that methods such as kernel ridge regression can significantly enhance the prediction of financial volatility, although some researchers, such as Mourer, have found that these methods do not always outperform simple autoregressive models in all contexts. Since the end of the Bretton Woods system, the transition to a floating exchange rate regime has posed significant challenges for managing risk, especially regarding the RMB/USD exchange rate. China’s integration into the global economy post-WTO accession, along with its deepening economic ties with the U.S., has made this exchange rate crucial for global economic stability. The 2015 reform of China’s exchange rate mechanism increased market-driven fluctuations, further intensified by trade tensions, which has heightened volatility. This evolving landscape has led to debates over the RMB’s valuation and spurred numerous policy discussions. Consequently, precise and reliable exchange rate forecasting has become essential for effective risk management in this volatile environment. Traditional exchange rate forecasting models, both theoretical and quantitative, have struggled with consistency and often neglect critical influencing factors. For example, theoretical models may fail to account for the real-time impact of policy changes and global economic shifts, while quantitative models may not fully capture the non-linear dynamics and intricate interactions of the variables involved. Recently, there has been a shift toward big data and machine learning approaches, which offer flexibility and improved accuracy in handling the complex, non-linear nature of exchange rate data [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]. While some methods like kernel ridge regression have shown promise, their performance varies across different contexts, highlighting the ongoing challenges in exchange rate prediction. Machine learning models, particularly deep learning models, have increasingly been applied to predicting time series and economic variables [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]. Despite their advantages in handling complex, non-linear data without requiring explicit assumptions about the underlying data distribution, these models are often criticized for their “black box” nature and lack of interpretability. Recent advancements, such as the application of Grad-CAM and attention mechanisms, have begun to address these issues, making it possible to visualize model predictions and understand the underlying decision-making processes. In related financial applications, researchers have combined advanced models like XGBoost with data balancing techniques including SMOTE to enhance predictive performance, as demonstrated by Chang et al. in their fraud detection study [60]. Their work not only showcased the effectiveness of this approach in fraud detection but also laid a foundation for developing robust models in various financial domains, including potentially exchange rate prediction [60]. However, applying these interpretability techniques has been mostly limited to fields like image recognition and natural language processing, with relatively few studies applying them to economic forecasting. Given the challenges of traditional models and the potential of machine learning approaches, this study seeks to explore the use of advanced deep learning models, including CNNs, RNNs, MLPs, and transformer-based architectures, for predicting the RMB/USD exchange rate. By incorporating a comprehensive set of features—drawn from economic indicators, trade data, and other currency pairs—and employing advanced feature selection techniques, this research aims to enhance predictive accuracy, identify the most relevant factors influencing exchange rate fluctuations, and enhance the interpretability of the model predictions. Contributions of This Study • Application of Deep Learning Models: This study provides an initial analysis of the effectiveness of deep learning models in exchange rate prediction, using MSE and MAE as key metrics to identify the best-performing models. • Enhancement of Predictive Performance: To improve the accuracy of machine learning models, this study employs various techniques, including feature selection, to reduce redundancy and retain the most relevant subset of features for exchange rate forecasting. • Analysis of Influential Factors Over Time: By applying attention mechanisms, this study enhances the interpretability of machine learning models, offering insights into how different factors influence exchange rate predictions across different periods. This analysis aims to uncover which aspects of economic data the models prioritize during the prediction process, thereby providing a more nuanced understanding of the underlying dynamics."
SHAP zero Explains All-order Feature Interactions in Black-box Genomic Models with Near-zero Query Cost,"With the rapid growth of black-box models in machine learning, Shapley values have emerged as a popular method for model explanations due to their theoretical guarantees. Shapley values locally explain a model to an input query using additive features. Yet, in genomics, extracting biological knowledge from black-box models hinges on explaining nonlinear feature interactions globally to hundreds to thousands of input query sequences. Herein, we develop SHAP zero, an algorithm that estimates all-order Shapley feature interactions with a near-zero cost per queried sequence after paying a one-time fee for model sketching. SHAP zero achieves this by establishing a surprisingly underexplored connection between the Shapley interactions and the Fourier transform of the model. Explaining two genomic models, one trained to predict guide RNA binding and the other to predict DNA repair outcomes, we demonstrate that SHAP zero achieves orders of magnitude reduction in amortized computational cost compared to state-of-the-art algorithms. SHAP zero reveals all microhomologous motifs that are predictive of DNA repair outcome, a finding previously inaccessible due to the combinatorial space of possible high-order feature interactions.","Shapley values have emerged as a theoretically robust method for explaining the local additive features of an input query to black-box models [1, 2, 3]. However, extracting biological knowledge from the emerging models in genomics demands a more global understanding, which requires explaining the nonlinear interactions among features and doing so for hundreds to thousands of input query sequences. Shapley value explanations have a high computational cost, with an exact calculation requiring an exponential number of model evaluations in the input dimension [4]. The cost is higher for nonlinear feature interactions and grows exponentially with a polynomial function of the input dimension. With the increasing growth of complex and large-scale models in genomics [5], often with only proprietary access, there is an urgent need for algorithms that can explain the nonlinear high-order interactions in these black-box models at scale–not just in a handful of sequences but among a host of query sequences. Consider the problem of explaining a black-box model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) that takes in a length-n𝑛nitalic_n DNA sequence 𝐱∈ℤ4n={A,T,C,G}n𝐱superscriptsubscriptℤ4𝑛superscript𝐴𝑇𝐶𝐺𝑛\mathbf{x}\in\mathbb{Z}_{4}^{n}=\{A,T,C,G\}^{n}bold_x ∈ blackboard_Z start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT = { italic_A , italic_T , italic_C , italic_G } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and outputs a real number. One popular method of explaining the prediction of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to the input query sequence 𝐱𝐱\mathbf{x}bold_x is using SHapley Additive exPlanations (SHAP) [6]. SHAP explains the output of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) by assigning a so-called SHAP value IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) to the ithsuperscript𝑖thi^{\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT nucleotide in 𝐱𝐱\mathbf{x}bold_x, where i𝑖iitalic_i belongs to the feature set D={1,2,…,n}𝐷12…𝑛D=\{1,2,\dots,n\}italic_D = { 1 , 2 , … , italic_n } (Fig. 1a): IS⁢V⁢(i)=∑T⊆D\{i}|T|!⁢(|D|−|T|−1)!|D|!⁢[vT∪{i}⁢(𝐱T∪{i})−vT⁢(𝐱T)].superscript𝐼𝑆𝑉𝑖subscript𝑇\𝐷𝑖𝑇𝐷𝑇1𝐷delimited-[]subscript𝑣𝑇𝑖subscript𝐱𝑇𝑖subscript𝑣𝑇subscript𝐱𝑇I^{SV}(i)=\sum_{T\subseteq D\backslash\{i\}}\frac{|T|!\,(|D|-|T|-1)!}{|D|!}% \left[v_{T\cup\{i\}}(\mathbf{x}_{T\cup\{i\}})-v_{T}(\mathbf{x}_{T})\right].italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) = ∑ start_POSTSUBSCRIPT italic_T ⊆ italic_D \ { italic_i } end_POSTSUBSCRIPT divide start_ARG | italic_T | ! ( | italic_D | - | italic_T | - 1 ) ! end_ARG start_ARG | italic_D | ! end_ARG [ italic_v start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT ) - italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ] . (1) IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ) computes the marginal contribution of the ithsuperscript𝑖thi^{\text{th}}italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT nucleotide to the value function vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) for all the subsets T𝑇Titalic_T of features in D\{i}\𝐷𝑖D\backslash\{i\}italic_D \ { italic_i }, where the value function is the output of f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to 𝐱Tsubscript𝐱𝑇\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT computed by marginalizing the absent nucleotides in D\T\𝐷𝑇D\backslash Titalic_D \ italic_T (Fig. 1a). SHAP requires computing vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) for every subset of features in T⊆D𝑇𝐷T\subseteq Ditalic_T ⊆ italic_D, which means evaluating f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) using a number of samples that grows exponentially with n𝑛nitalic_n (hereafter called sample complexity) and paying a computational cost that also grows exponentially with n𝑛nitalic_n (hereafter called computational complexity). Faithful Shapley Interaction index IF⁢S⁢I⁢(T)superscript𝐼𝐹𝑆𝐼𝑇I^{FSI}(T)italic_I start_POSTSUPERSCRIPT italic_F italic_S italic_I end_POSTSUPERSCRIPT ( italic_T ) (Faith-Shap) generalizes SHAP values defined for a single nucleotide i𝑖iitalic_i to an interaction defined over a set T⊆D𝑇𝐷T\subseteq Ditalic_T ⊆ italic_D (Equation (34)) [7]. Computing Faith-Shap requires evaluating the model for all possible subsets of features in D𝐷Ditalic_D, and then marginalizing over their power set with an exact computational cost that grows exponentially with a polynomial function of sequence length p⁢o⁢l⁢y⁢(n)𝑝𝑜𝑙𝑦𝑛poly(n)italic_p italic_o italic_l italic_y ( italic_n ). Figure 1: Schematic of the flowchart for the exact computation of SHapley Additive exPlanations (SHAP) and our approximation algorithm SHAP zero. a, Computing SHAP values exactly requires exponential model evaluations with sequence length n𝑛nitalic_n, per query sequence. This illustration shows computing one such term in IS⁢V⁢(i=1)superscript𝐼𝑆𝑉𝑖1I^{SV}(i=1)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i = 1 ), which measures the marginal contribution of the nucleotide T at site i=1𝑖1i=1italic_i = 1 to the set T={3,4,5}𝑇345T=\{3,4,5\}italic_T = { 3 , 4 , 5 } by evaluating the model for all four possible nucleotides at D\(T∪{i})={2}\𝐷𝑇𝑖2D\backslash(T\cup\{i\})=\{2\}italic_D \ ( italic_T ∪ { italic_i } ) = { 2 } in the sequence 𝐱T∪{i}subscript𝐱𝑇𝑖{\bf x}_{T\cup\{i\}}bold_x start_POSTSUBSCRIPT italic_T ∪ { italic_i } end_POSTSUBSCRIPT and all 42=16superscript42164^{2}=164 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 16 possible nucleotide combinations at D\T={1,2}\𝐷𝑇12D\backslash T=\{1,2\}italic_D \ italic_T = { 1 , 2 } in the sequence 𝐱Tsubscript𝐱𝑇{\bf x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. b, In contrast, SHAP zero cleverly samples the sequence space and resolves a sparse bipartite graph to approximate the original model with its top-s𝑠sitalic_s Fourier coefficients. c, For each query sequence 𝐱𝐱{\bf x}bold_x, SHAP zero maps the top-s𝑠sitalic_s Fourier coefficients into the Möbius transform M⁢[𝐤]𝑀delimited-[]𝐤M[{\bf k}]italic_M [ bold_k ], where k is the feature interaction vector. The illustration shows one such sequence’s Möbius transform, composed of all the permutations of ℓ=3ℓ3\ell=3roman_ℓ = 3 nucleotides from the original sequence. d, SHAP zero computes IS⁢V⁢(i=1)superscript𝐼𝑆𝑉𝑖1I^{SV}(i=1)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i = 1 ) using a weighted sum of the Möbius coefficients with k1>0subscript𝑘10k_{1}>0italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0 (Equation (2)). SHAP zero amortizes the cost of finding the Fourier transform over the future explanations. The algorithms to approximately compute SHAP values are either stochastic estimators [6, 8, 9, 10, 11, 12] or model-based approximators [6, 13, 14, 15, 16, 17, 18]. Stochastic estimators, such as KernelSHAP [6], randomly subsample the feature subsets (T𝑇Titalic_T,vT⁢(𝐱T)subscript𝑣𝑇subscript𝐱𝑇v_{T}(\mathbf{x}_{T})italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )) and approximately solve a weighted-least squares problem to estimate IS⁢V⁢(i)superscript𝐼𝑆𝑉𝑖I^{SV}(i)italic_I start_POSTSUPERSCRIPT italic_S italic_V end_POSTSUPERSCRIPT ( italic_i ). These algorithms require many model evaluations in practice and impose an undesirable trade-off between sample complexity and accuracy [19]. Model-based approximators, such as DeepSHAP [6], take advantage of the model architecture (e.g., neural networks) to estimate Shapley values. These methods are often faster but still require many model evaluations and only work for white-box models. Algorithms to compute Shapley interactions in black-box models [20, 7, 21, 22], such as SHAP-IQ [20], subsample high-order interactions for efficiency but similar to stochastic estimators they need many model evaluations. To empirically demonstrate how current SHAP algorithms scale, let us consider TIGER [23], a recent model that predicts the efficiency of CRISPR-Cas13d guide RNA from their sequence and context. To explain which region in the guide RNA is the most determinant of efficiency, we estimated SHAP values of 1038103810381038 input query sequences. Finding the KernelSHAP [6] values of all the nucleotides in these sequences took about one day on our single NVIDIA RTX A6000 machine. Worse yet, finding only up to 3rdsuperscript3rd3^{\text{rd}}3 start_POSTSUPERSCRIPT rd end_POSTSUPERSCRIPT order feature interactions using SHAP-IQ took more than 81 days—revealing a severe scalability issue in current explainability algorithms. We posit that the model evaluations needed to estimate the SHAP values of an input query sequence have information that can be used to estimate the SHAP values for a new query sequence. Therefore, instead of independently evaluating the model for each query sequence, one can “recycle” the model evaluations to slash the sample and computational cost of explaining the model. Taking this idea to the extreme, we propose to do initial query-agnostic model evaluations to sketch the model and then use the sketch for model explanation. How can a model be sketched to be efficiently mapped to SHAP values and Shapley interactions? We discover a surprisingly underexplored connection between SHAP values and interactions and the model’s Fourier transform, enabling us to sketch the model and use the sketch for fast Shapley explanations. The Fourier transform provides a global sketching of the model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) irrespective of the query sequence or even the training distribution. Moreover, since Fourier is an orthonormal basis, it enables fast and sample-efficient algorithms for sketching black-box models in genomics that are compressible or near-sparse in the Fourier domain [24, 25, 26, 27]. Herein, we develop SHAP zero, an algorithm that estimates SHAP values and Shapley interactions with a near-zero additional cost per new query sequence after paying an initial up-front cost for model sketching (Fig. 1b). In developing SHAP zero, we make three distinct and interconnected contributions: First, we build on the existing algorithms in sparse Fourier transforms [27, 28] and develop a method to sketch a black-box model f⁢(𝐱)𝑓𝐱f({\bf x})italic_f ( bold_x ) with sequence input (defined over q𝑞qitalic_q alphabets) and real-valued output, in terms of its top-s𝑠sitalic_s Fourier coefficients with a sample complexity of 𝒪⁢(s⁢n2)𝒪𝑠superscript𝑛2\mathcal{O}(sn^{2})caligraphic_O ( italic_s italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and a computational complexity of 𝒪⁢(s⁢n3)𝒪𝑠superscript𝑛3\mathcal{O}(sn^{3})caligraphic_O ( italic_s italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). Second, we establish a mathematical formalism to map from (i) the Fourier transform to a transform from algebraic geometry called the Möbius transform and (ii) from the Möbius transform to Shapley-based explanations. The Möbius transform [29, 30] enables us to map the top-s𝑠sitalic_s Fourier coefficients of the model f⁢(𝐱)𝑓𝐱f(\mathbf{x})italic_f ( bold_x ) to Shapley-based explanations in 𝒪⁢(s2⁢(2⁢q)ℓ)𝒪superscript𝑠2superscript2𝑞ℓ\mathcal{O}(s^{2}(2q)^{\ell})caligraphic_O ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 2 italic_q ) start_POSTSUPERSCRIPT roman_ℓ end_POSTSUPERSCRIPT ) time per query sequence, where ℓ≤5ℓ5\ell\leq 5roman_ℓ ≤ 5 caps the maximum order of model interactions in practice [31]. Third, we conduct large-scale experiments to explain two genomic models, TIGER [23] and inDelphi [32], with SHAP zero. We demonstrate that SHAP zero estimates all-order feature interactions with an amortized computational cost up to 1000-fold faster than current algorithms. SHAP zero reveals the GC content of the seed region in TIGER and microhomologous motifs in inDelphi as predictive high-order features, a task previously inaccessible due to the combinatorial space of possible feature interactions."
